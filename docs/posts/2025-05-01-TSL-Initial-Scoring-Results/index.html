<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2025-05-01">
<meta name="description" content="A detailed breakdown of my manual evaluation of three TinyStories language models (1M, 8M, and 28M parameters) across six capability categories. I share scoring methodology, surprising findings about emergent reasoning in small models, and comparisons to the original TinyStories paper. This analysis establishes baselines both for my own model training project and my eventual LLM Judge, and reveals how different capabilities scale with model size.">

<title>Vishal Bakshi’s Blog - Initial Manual Scoring Results for TinyStories Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi’s Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#recap" id="toc-recap" class="nav-link active" data-scroll-target="#recap">Recap</a></li>
  <li><a href="#evaluation-categories" id="toc-evaluation-categories" class="nav-link" data-scroll-target="#evaluation-categories">Evaluation Categories</a>
  <ul class="collapse">
  <li><a href="#foundational-language-capabilities" id="toc-foundational-language-capabilities" class="nav-link" data-scroll-target="#foundational-language-capabilities">Foundational language capabilities</a></li>
  <li><a href="#emergent-capabilities" id="toc-emergent-capabilities" class="nav-link" data-scroll-target="#emergent-capabilities">Emergent capabilities</a></li>
  <li><a href="#story-related-capabilities" id="toc-story-related-capabilities" class="nav-link" data-scroll-target="#story-related-capabilities">Story-related capabilities</a></li>
  </ul></li>
  <li><a href="#baseline-models" id="toc-baseline-models" class="nav-link" data-scroll-target="#baseline-models">Baseline Models</a></li>
  <li><a href="#generation-script" id="toc-generation-script" class="nav-link" data-scroll-target="#generation-script">Generation Script</a></li>
  <li><a href="#eval-prompts" id="toc-eval-prompts" class="nav-link" data-scroll-target="#eval-prompts">Eval Prompts</a></li>
  <li><a href="#scoring-methodology" id="toc-scoring-methodology" class="nav-link" data-scroll-target="#scoring-methodology">Scoring Methodology</a></li>
  <li><a href="#overall-results" id="toc-overall-results" class="nav-link" data-scroll-target="#overall-results">Overall Results</a></li>
  <li><a href="#scores-by-category" id="toc-scores-by-category" class="nav-link" data-scroll-target="#scores-by-category">Scores by Category</a></li>
  <li><a href="#scoring-by-criteria" id="toc-scoring-by-criteria" class="nav-link" data-scroll-target="#scoring-by-criteria">Scoring by Criteria</a>
  <ul class="collapse">
  <li><a href="#emergent-capabilities-creativity-factual-knowledge-and-reasoning" id="toc-emergent-capabilities-creativity-factual-knowledge-and-reasoning" class="nav-link" data-scroll-target="#emergent-capabilities-creativity-factual-knowledge-and-reasoning">Emergent Capabilities: Creativity, Factual Knowledge and Reasoning</a></li>
  <li><a href="#foundational-language-capabilities-grammar-and-context-tracking" id="toc-foundational-language-capabilities-grammar-and-context-tracking" class="nav-link" data-scroll-target="#foundational-language-capabilities-grammar-and-context-tracking">Foundational Language Capabilities: Grammar and Context-Tracking</a></li>
  <li><a href="#story-related-capabilities-plot" id="toc-story-related-capabilities-plot" class="nav-link" data-scroll-target="#story-related-capabilities-plot">Story-Related Capabilities: Plot</a></li>
  </ul></li>
  <li><a href="#comparison-to-tinystories-paper" id="toc-comparison-to-tinystories-paper" class="nav-link" data-scroll-target="#comparison-to-tinystories-paper">Comparison to TinyStories Paper</a>
  <ul class="collapse">
  <li><a href="#creativity" id="toc-creativity" class="nav-link" data-scroll-target="#creativity">Creativity</a></li>
  <li><a href="#grammar" id="toc-grammar" class="nav-link" data-scroll-target="#grammar">Grammar</a></li>
  <li><a href="#context-tracking-consistency" id="toc-context-tracking-consistency" class="nav-link" data-scroll-target="#context-tracking-consistency">Context-Tracking (Consistency)</a></li>
  <li><a href="#plot" id="toc-plot" class="nav-link" data-scroll-target="#plot">Plot</a></li>
  </ul></li>
  <li><a href="#observations-from-manual-scoring" id="toc-observations-from-manual-scoring" class="nav-link" data-scroll-target="#observations-from-manual-scoring">Observations From Manual Scoring</a></li>
  <li><a href="#exciting-discoveries" id="toc-exciting-discoveries" class="nav-link" data-scroll-target="#exciting-discoveries">Exciting Discoveries</a></li>
  <li><a href="#process-improvements" id="toc-process-improvements" class="nav-link" data-scroll-target="#process-improvements">Process Improvements</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Initial Manual Scoring Results for TinyStories Models</h1>
  <div class="quarto-categories">
    <div class="quarto-category">LLM</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">TinyScaleLab</div>
  </div>
  </div>

<div>
  <div class="description">
    A detailed breakdown of my manual evaluation of three TinyStories language models (1M, 8M, and 28M parameters) across six capability categories. I share scoring methodology, surprising findings about emergent reasoning in small models, and comparisons to the original TinyStories paper. This analysis establishes baselines both for my own model training project and my eventual LLM Judge, and reveals how different capabilities scale with model size.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<p>In this post, I’m going to analyze the initial manual scoring results for my baseline models’ text generations given my 150 evaluation prompts across six scoring categories and 18 criteria. A quick recap of what I’ve done so far:</p>
<ul>
<li>Defined scoring criteria</li>
<li>Curated a set of eval prompts based on each scoring category</li>
<li>Created a fast HTML app where I can perform my scoring activities</li>
</ul>
<p>The raw scores analyzed in this blog post can be found in <a href="https://github.com/vishalbakshi/TinyScaleLab">my TinyScaleLab repo</a>.</p>
</section>
<section id="evaluation-categories" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-categories">Evaluation Categories</h2>
<p>I have six scoring categories that I’m evaluating my models on:</p>
<section id="foundational-language-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="foundational-language-capabilities">Foundational language capabilities</h3>
<ul>
<li>Grammar</li>
<li>Context-Tracking (Consistency)</li>
</ul>
</section>
<section id="emergent-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="emergent-capabilities">Emergent capabilities</h3>
<ul>
<li>Factual Knowledge</li>
<li>Reasoning</li>
<li>Creativity</li>
</ul>
</section>
<section id="story-related-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="story-related-capabilities">Story-related capabilities</h3>
<ul>
<li>Plot</li>
</ul>
<p>My goal was to generate prompts that either isolate (Factual Knowledge, Reasoning, Context-Tracking) or elicit opportunities to exhibit (Plot, Creativity) scoring categories. I wanted to make the job easier first for myself, and then use that as a proxy of making the job of the LLM judge easier to evaluate scoring categories in a focused way.</p>
</section>
</section>
<section id="baseline-models" class="level2">
<h2 class="anchored" data-anchor-id="baseline-models">Baseline Models</h2>
<p>I’ve chosen three models as my baseline because they’re similar in size to the models that I’m going to be training in this project:</p>
<ul>
<li>TinyStories-1M (~3.7 million parameters)</li>
<li>TinyStories-8M (~20 million parameters)</li>
<li>TinyStories-28M (~60 million parameters)</li>
</ul>
</section>
<section id="generation-script" class="level2">
<h2 class="anchored" data-anchor-id="generation-script">Generation Script</h2>
<p>I’m using a pretty standard generation script. Things I want to highlight:</p>
<ul>
<li>Making sure the padding side is left so that we’re not generating tokens based on padding tokens</li>
<li><code>model.eval()</code> and <code>torch.no_grad()</code> are things that I always make sure to do so that it’s somewhat deterministic when it’s expected to be deterministic</li>
<li>I’m doing <code>do_sample=False</code> and <code>num_beams=5</code> because that was published by the authors as their parameters for generation</li>
<li>I have a minimum and maximum length, which I’ll talk about at the end about how I think that might change moving forward</li>
</ul>
</section>
<section id="eval-prompts" class="level2">
<h2 class="anchored" data-anchor-id="eval-prompts">Eval Prompts</h2>
<p>My current eval prompts set includes:</p>
<ul>
<li>25 unique prompts for Reasoning</li>
<li>25 unique prompts for Factual Knowledge</li>
<li>25 prompts each for Context-Tracking, Plot and Creativity (with some overlap)</li>
<li>25 prompts for Grammar (5 prompts sampled from the other 5 categories)</li>
</ul>
<p>That’s 150 total prompts.</p>
</section>
<section id="scoring-methodology" class="level2">
<h2 class="anchored" data-anchor-id="scoring-methodology">Scoring Methodology</h2>
<p>I have six categories across 18 criteria, evaluating generations from three models on 150 prompts each. The scores that I’m providing for each criteria are either 0, 0.5, and 1.0, taken from the Tiny Stories paper (though they didn’t quite use it the same way I’m using it), in Section 4.2 (Figures 9/10/11) where they use scoring levels success (green), failure (red), and partial success (yellow).</p>
</section>
<section id="overall-results" class="level2">
<h2 class="anchored" data-anchor-id="overall-results">Overall Results</h2>
<p>First, let’s look at the average value across all categories and criteria for each model:</p>
<table class="table">
<thead>
<tr class="header">
<th>model_name</th>
<th>score_value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>roneneneldan/TinyStories-1M</td>
<td>0.25</td>
</tr>
<tr class="even">
<td>roneneneldan/TinyStories-8M</td>
<td>0.49</td>
</tr>
<tr class="odd">
<td>roneneneldan/TinyStories-28M</td>
<td>0.61</td>
</tr>
</tbody>
</table>
<p>As I would expect, as model size increases, the average score value increases. The 1M parameter model (which actually has 3.7M parameters) has an average score of 0.25. The 8M parameter model (which is closer to 20M parameters) has an average score of about 0.5. And the 28M parameter model (which has about 60M parameters) has an average score of 0.61.</p>
<p>A parameter count <em>increase</em> of 4x (16.3M increase from 3.7M to 20M) yields an overall mean score <em>increase</em> of 1x (0.25 to 0.50). A parameter count <em>increase</em> of 2x (40M increase from 20M to 60M) yields an overall mean score <em>increase</em> of 25% (0.49 to 0.61). There are decreasing gains overall when increasing parameter count. For a 125M parameter model (that I’m planning to train), I would expect &lt;10% increase from a mean overall score of 0.61.</p>
</section>
<section id="scores-by-category" class="level2">
<h2 class="anchored" data-anchor-id="scores-by-category">Scores by Category</h2>
<p>Next, let’s look at how these models are doing for each of the categories overall:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>1M</th>
<th>8M</th>
<th>28M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Context-Tracking</td>
<td>0.14</td>
<td>0.51</td>
<td>0.63</td>
</tr>
<tr class="even">
<td>Creativity</td>
<td>0.12</td>
<td>0.16</td>
<td>0.32</td>
</tr>
<tr class="odd">
<td>Factual Knowledge</td>
<td>0.08</td>
<td>0.32</td>
<td>0.40</td>
</tr>
<tr class="even">
<td>Grammar</td>
<td>0.59</td>
<td>0.82</td>
<td>0.86</td>
</tr>
<tr class="odd">
<td>Plot</td>
<td>0.10</td>
<td>0.42</td>
<td>0.60</td>
</tr>
<tr class="even">
<td>Reasoning</td>
<td>0.20</td>
<td>0.44</td>
<td>0.70</td>
</tr>
</tbody>
</table>
<p>Some interesting things to point out:</p>
<p>The highest category by score for my 1M parameter model is grammar, by farL 0.59. That’s about three times as large as any other category. This is in line with what I read in the TinyStories paper, that grammar appears first as a capability.</p>
<p>The worst categories, even for the largest model that I tested, were Creativity and Factual Knowledge. Creativity in particular was the lowest scoring, and this also tracks with the TinyStories paper, because they had shown that creativity only really appears at large hidden dimension sizes. And even then, the maximum value of creativity (8s and 9s out of 10) was only available for models like GPT-4.</p>
<p>Factual Knowledge was also significantly lower than the other four categories.</p>
<p>The other category I want to highlight is Reasoning. The Reasoning score for the smallest model is 0.2, it doubles to 0.44 at 8M, and then it goes up by another 60 percent to 0.7 for the 28M model. That’s pretty solid! 70%, 7 out of 10. So, if we were talking about school grades, a 70 percent is passing. Very cool to see reasoning potential, even for the tiniest model evaluated.</p>
<p>In every case, there is an increase as we go from 1M to 8M to 28M model name. In some cases, the jump comes later, such as for Creativity. In most cases, the jump happens between the 1M and 8M models.</p>
</section>
<section id="scoring-by-criteria" class="level2">
<h2 class="anchored" data-anchor-id="scoring-by-criteria">Scoring by Criteria</h2>
<p>Now let’s look at each criteria for each category:</p>
<section id="emergent-capabilities-creativity-factual-knowledge-and-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="emergent-capabilities-creativity-factual-knowledge-and-reasoning">Emergent Capabilities: Creativity, Factual Knowledge and Reasoning</h3>
<table class="table">
<colgroup>
<col style="width: 78%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Factual Knowledge</th>
<th>1M</th>
<th>8M</th>
<th>28M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Completion contains only correct factual information relevant to the prompt</td>
<td>0.08</td>
<td>0.32</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 79%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Reasoning</th>
<th>1M</th>
<th>8M</th>
<th>28M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Completion demonstrates correct logical reasoning relevant to the prompt</td>
<td>0.2</td>
<td>0.44</td>
<td>0.7</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 73%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Creativity</th>
<th>1M</th>
<th>8M</th>
<th>28M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Character behavioral and emotional responses are innovative</td>
<td>0.00</td>
<td>0.04</td>
<td>0.22</td>
</tr>
<tr class="even">
<td>The completion contains unique details to the story world</td>
<td>0.02</td>
<td>0.12</td>
<td>0.34</td>
</tr>
<tr class="odd">
<td>The completion creates fresh situations</td>
<td>0.00</td>
<td>0.08</td>
<td>0.20</td>
</tr>
<tr class="even">
<td>The completion offers unexpected or novel elements</td>
<td>0.48</td>
<td>0.42</td>
<td>0.50</td>
</tr>
</tbody>
</table>
<p>Factual Knowledge and Reasoning only had one criteria each. For Factual Knowledge, I was assessing if the completion contains only correct factual information relevant to the prompt. For Reasoning, I was assessing if the completion demonstrates correct logical reasoning relevant to the prompt.</p>
<p>For Creativity, note that the smallest model performs well for the criteria “The completion offers unexpected or novel elements.” Since I was isolating Grammar, Plot and Context-Tracking from Creativity, the tiniest model could deviate from Plot/Context and still get a high score for this criterion, making it the lowest bar to cross. For the other three Creativity criteria, the 1M model has negligible skill.</p>
</section>
<section id="foundational-language-capabilities-grammar-and-context-tracking" class="level3">
<h3 class="anchored" data-anchor-id="foundational-language-capabilities-grammar-and-context-tracking">Foundational Language Capabilities: Grammar and Context-Tracking</h3>
<table class="table">
<colgroup>
<col style="width: 73%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Grammar</th>
<th>1M</th>
<th>8M</th>
<th>28M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Age-appropriate vocabulary usage</td>
<td>1.00</td>
<td>1.00</td>
<td>0.98</td>
</tr>
<tr class="even">
<td>Dialogue formatting and punctuation</td>
<td>1.00</td>
<td>0.96</td>
<td>0.98</td>
</tr>
<tr class="odd">
<td>Proper use of pronouns and referents</td>
<td>0.56</td>
<td>0.88</td>
<td>0.90</td>
</tr>
<tr class="even">
<td>Sentence structure logic, clarity and completion</td>
<td>0.14</td>
<td>0.62</td>
<td>0.70</td>
</tr>
<tr class="odd">
<td>Tense consistency throughout the completion</td>
<td>0.26</td>
<td>0.66</td>
<td>0.74</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 79%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Context-Tracking</th>
<th>1M</th>
<th>8M</th>
<th>28M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Completion maintains complete coherence with prompt</td>
<td>0.20</td>
<td>0.62</td>
<td>0.64</td>
</tr>
<tr class="even">
<td>Correctly references/tracks all objects, characters, and their attributes</td>
<td>0.20</td>
<td>0.52</td>
<td>0.68</td>
</tr>
<tr class="odd">
<td>Maintains consistent narrative flow</td>
<td>0.02</td>
<td>0.40</td>
<td>0.56</td>
</tr>
</tbody>
</table>
<p>For Grammar, the age-appropriate vocabulary usage was the easiest to score. These models don’t really generate anything that’s not within the TinyStories dataset.</p>
<p>Sentence structure, logic, clarity, and completion had the biggest jump from 1M to 8M, going from 0.14 to 0.62. That matches my experiencing scoring: the small model had terrible structure, logic, clarity, and completion in its completions.</p>
<p>For context tracking, I was looking at three criteria. The biggest jump is from 2% to 40% for maintaining a consistent narrative flow. The medium-sized models were definitely not perfect, but was much better at following the narrative flow of the story.</p>
</section>
<section id="story-related-capabilities-plot" class="level3">
<h3 class="anchored" data-anchor-id="story-related-capabilities-plot">Story-Related Capabilities: Plot</h3>
<table class="table">
<colgroup>
<col style="width: 81%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Plot</th>
<th>1M</th>
<th>8M</th>
<th>28M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Conflicts are addressed rather than abandoned</td>
<td>0.00</td>
<td>0.42</td>
<td>0.60</td>
</tr>
<tr class="even">
<td>The pacing is appropriate (not too rushed or dragging)</td>
<td>0.00</td>
<td>0.14</td>
<td>0.24</td>
</tr>
<tr class="odd">
<td>The story has a clear beginning, middle, and end appropriate to age level</td>
<td>0.26</td>
<td>0.50</td>
<td>0.72</td>
</tr>
<tr class="even">
<td>The story maintains focus on the central conflict/theme without random diversions</td>
<td>0.12</td>
<td>0.64</td>
<td>0.84</td>
</tr>
</tbody>
</table>
<p>For Plot, I found the pacing to be the worst category across all models. This checks out with my experience as I was grading these stories - I didn’t really get a sense that there was a well-paced story. Either it was dragging and repeating itself slightly, or it was just one or two sentences and insufficient.</p>
<p>For “conflicts are addressed” we go from 0% to 42% from 1M to 8M. The smallest model simply ignored or abandoned conflicts that were in the premise and the prompt. The other big jump is for “focusing on the central theme” - the smallest to medium model had almost a 3x jump, and then there was still a considerable 30% jump from the medium to large model.</p>
</section>
</section>
<section id="comparison-to-tinystories-paper" class="level2">
<h2 class="anchored" data-anchor-id="comparison-to-tinystories-paper">Comparison to TinyStories Paper</h2>
<p>I’m going to revisit the targets that I established from Figure 4 of the TinyStories paper, where they showed the different scores based on hidden dimension and number of layers. I matched that up with the three models that I’m testing:</p>
<section id="creativity" class="level3">
<h3 class="anchored" data-anchor-id="creativity">Creativity</h3>
<p>The TinyStories paper reported:</p>
<ul>
<li>1M: 0.47</li>
<li>8M: 0.65</li>
<li>28M: 0.69</li>
</ul>
<p>My scores:</p>
<ul>
<li>1M: 0.12</li>
<li>8M: 0.16</li>
<li>28M: 0.32</li>
</ul>
<p>This was really interesting - I was expecting my assessment to be maybe a little lenient, but it turns out that’s not the case. My scores were significantly lower. The 28M parameter model (which is actually 60M) got 70% for creativity in the paper, while mine was at 30%. I might have to change that criteria over the course of this project, or it might turn out that for creativity, I have a stricter judge.</p>
</section>
<section id="grammar" class="level3">
<h3 class="anchored" data-anchor-id="grammar">Grammar</h3>
<p>TinyStories:</p>
<ul>
<li>1M: 0.61</li>
<li>8M: 0.77</li>
<li>28M: 0.83</li>
</ul>
<p>My scores:</p>
<ul>
<li>1M: 0.59</li>
<li>8M: 0.82</li>
<li>28M: 0.86</li>
</ul>
<p>This matched out pretty well! 61%/59%, 77%/82%, and 83%/86%. The most common baseline capability matches between the targets and my relatively rough evaluation, so thumbs up!</p>
</section>
<section id="context-tracking-consistency" class="level3">
<h3 class="anchored" data-anchor-id="context-tracking-consistency">Context-Tracking (Consistency)</h3>
<p>TinyStories:</p>
<ul>
<li>1M: 0.45</li>
<li>8M: 0.80</li>
<li>28M: 0.90</li>
</ul>
<p>My scores:</p>
<ul>
<li>1M: 0.14</li>
<li>8M: 0.51</li>
<li>28M: 0.63</li>
</ul>
<p>Similar to creativity, it turns out that my criteria or my judging is a lot stricter than the GPT-4 evaluator used in the paper. The highest score in the paper was 90%, whereas mine was 63%. I’m not too worried about this - I would much rather be stricter than not. However, I’ll be open to changing my approach later on if that turns out to be a problem.</p>
</section>
<section id="plot" class="level3">
<h3 class="anchored" data-anchor-id="plot">Plot</h3>
<p>TinyStories: - 1M: 0.44 - 8M: 0.72 - 28M: 0.73</p>
<p>My scores: - 1M: 0.10 - 8M: 0.42 - 28M: 0.60</p>
<p>The 28M scores for Plot are in the same range but medium-sized and small model scores are significantly different.</p>
<p>Factual Knowledge and Reasoning were not quantitatively assessed in the TinyStories paper in the way that these other scores were listed, so I don’t have those reference points for my evaluation.</p>
</section>
</section>
<section id="observations-from-manual-scoring" class="level2">
<h2 class="anchored" data-anchor-id="observations-from-manual-scoring">Observations From Manual Scoring</h2>
<p>After manually scoring 450 stories, I have some observations:</p>
<ol type="1">
<li><p>Judging quality improves (and changes) over time</p>
<ul>
<li>Implicit judging criteria surfaces over time.</li>
<li>By the time I was doing the last hundred, I realized that I was a lot more definitive in giving 0s, 0.5s, and 1s.</li>
<li>Thee largest model likely has the strictest scores (it was graded last).</li>
</ul></li>
<li><p>Phrasing of scoring criteria improved</p>
<ul>
<li>I wanted to be able to answer the question as fast as I could (450 stories to get through!) with a quick yes, no, maybe (1, 0, 0.5).</li>
<li>Initially, some of the criteria were phrased as questions, requiring more cognitive work. I expect that rephrasing the criteria as statements will also ease the “cognitive load” for my LLM judge.</li>
</ul></li>
<li><p>I identified one duplicate prompt and replaced it</p></li>
<li><p>Pros and cons of isolating scoring categories</p>
<ul>
<li>I scored each category in isolation.</li>
<li>More times than not, I found this very liberating—I could assess Creativity without worrying about Context-Tracking or Plot.</li>
<li>However, language is very difficult to compartmentalize. If something’s not factually correct, it will be a distraction when assessing Reasoning. If the context is not being tracked, it makes it harder to assess plot.</li>
<li>Regardless, I thought this isolation of scoring categories overall benefited my approach</li>
</ul></li>
</ol>
</section>
<section id="exciting-discoveries" class="level2">
<h2 class="anchored" data-anchor-id="exciting-discoveries">Exciting Discoveries</h2>
<p>The main takeaway for me, which was very cool to see, is that Reasoning and Factual Knowledge capabilities exists even for the smallest model. The 1M model scored 20% on Reasoning - that’s not nothing!</p>
<p>The fact that there are non-zero values for these tiny models is really mind-blowing to me. It’s really exciting because there’s potential. We can do something with this, especially as these are just pre-trained models—we haven’t fine-tuned them yet. What can we do with this Reasoning and Factual Knowledge capability? That’s what really excites me moving forward.</p>
</section>
<section id="process-improvements" class="level2">
<h2 class="anchored" data-anchor-id="process-improvements">Process Improvements</h2>
<p>When generating completions for my Reasoning and Factual Knowledge, I want to remove the <code>min_length</code> parameter for <code>model.generate()</code> because I don’t want there to be a minimum generation length when the answer can be a few tokens, forcing the model to uneccesarily elongate the story. However, I won’t make this change for my LLM judge as I want to compare its scores with mine for the same prompt/completion pairs.</p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>With a full eval set scored, I can now move on to prompt engineering an LLM judge (I’ll be using Gemini 2.5 Flash and Claude Haiku 3.5). My goal is for a 90%+ alignment between my scores and the LLM judge before I choose to use it for future experiments.</p>
<p>Follow along this project (and others) in my <a href="https://www.youtube.com/@vishal_learner">YouTube channel!</a>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>