<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-07-25">
<meta name="description" content="In this blog post I think out loud as I attempt to understand pieces of the math presented in the rsLoRA paper.">

<title>vishal bakshi - Paper Math: rsLoRA</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#definition-3.1" id="toc-definition-3.1" class="nav-link" data-scroll-target="#definition-3.1">Definition 3.1</a></li>
  <li><a href="#theorem-3.2" id="toc-theorem-3.2" class="nav-link" data-scroll-target="#theorem-3.2">Theorem 3.2</a></li>
  <li><a href="#gradient-of-loss-mathcall-with-respect-to-adapters-a-and-b" id="toc-gradient-of-loss-mathcall-with-respect-to-adapters-a-and-b" class="nav-link" data-scroll-target="#gradient-of-loss-mathcall-with-respect-to-adapters-a-and-b">Gradient of Loss <span class="math inline">\(\mathcal{L}\)</span> with Respect to Adapters <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></a></li>
  <li><a href="#adapter-value-after-n-updates" id="toc-adapter-value-after-n-updates" class="nav-link" data-scroll-target="#adapter-value-after-n-updates">Adapter Value after <span class="math inline">\(n\)</span> Updates</a></li>
  <li><a href="#deriving-stable-rank" id="toc-deriving-stable-rank" class="nav-link" data-scroll-target="#deriving-stable-rank">Deriving Stable Rank</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Math: rsLoRA</h1>
  <div class="quarto-categories">
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">machine learning</div>
    <div class="quarto-category">paper math</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    In this blog post I think out loud as I attempt to understand pieces of the math presented in the rsLoRA paper.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 25, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this notebook, I’ll work through Definition 3.1 and the Theorem 3.2 proof provided in Appendix A of the <a href="https://arxiv.org/abs/2312.03732">rsLoRA paper</a>. Note that the purpose of this blog post is to help me think out loud—I have a lot of gaps in my understanding of matrix calculus that I need to remediate before I can derive some of the core equations in the paper. This post doesn’t provide those derivations.</p>
</section>
<section id="definition-3.1" class="level2">
<h2 class="anchored" data-anchor-id="definition-3.1">Definition 3.1</h2>
<p>An adapter <span class="math inline">\(\gamma_rBA\)</span> is <strong>rank-stabilized</strong> if the following two conditions hold:</p>
<ol type="1">
<li><p>If the inputs to the adapter are iid such that the <span class="math inline">\(m\)</span>’th moment is <span class="math inline">\(\Theta_r(1)\)</span> in each entry, then the <span class="math inline">\(m\)</span>’th moment of the outputs of the adapter is also <span class="math inline">\(\Theta_r(1)\)</span> in each entry.</p></li>
<li><p>If the gradient of the loss with respect to the adapter outputs are <span class="math inline">\(\Theta_r(1)\)</span> in each entry, then the loss gradients into the input of the adapter are also <span class="math inline">\(\Theta_r(1)\)</span> in each entry.</p></li>
</ol>
<p>I’ll define the following keywords from those two conditions:</p>
<ul>
<li>iid: independently and identically distributed</li>
<li><span class="math inline">\(\Theta_r(1)\)</span>: Big Theta notation, which specifies the upper and lower bounds of complexity of an algorithm. A notation of <span class="math inline">\(\Theta(1)\)</span> means the function or algorithm is upper bound and lower bound by a constant, meaning that as the number of inputs increases, the function stays constant (represented by the <span class="math inline">\(1\)</span>).</li>
<li>moments: quantitative measures related to the shape of the function’s graph (1st moment of a function is its mean, 2nd moment is variance, 3rd moment is skewness, 4th moment is Kurtosis).</li>
</ul>
<p>Condition 1 says that rank-stable adapters are those where IF the inputs to it are iid and have, on average, constant moments (like mean and variance) as the number of inputs increase, then the outputs of the adapter have constant moments on average as well.</p>
<p>Condition 2 says that the gradients of the inputs and outputs of rank-stable adapters, with respect to the loss, are of constant size as the number of inputs and outputs increase.</p>
<p><a href="https://huggingface.co/blog/damjan-k/rslora">This HuggingFace community article</a> puts it nicely and succinctly (emphasis mine):</p>
<blockquote class="blockquote">
<p>In the work Rank-Stabilized LoRA (rsLoRA), it is proven theoretically, by examining the learning trajectory of the adapters in the limit of large rank <span class="math inline">\(r\)</span>, that <strong>that to not explode or diminish the magnitude of the activations and gradients through each adapter</strong> one must set <span class="math inline">\(\gamma_r \in \Theta(\frac{1}{\sqrt{r}})\)</span></p>
</blockquote>
<p><a href="https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-theta-notation#:~:text=When%20we%20say,%3A">This Khan Academy</a> gives a nice example of <span class="math inline">\(\Theta(n)\)</span> from which you can imagine what <span class="math inline">\(\Theta(1)\)</span> would look like (instead of upper bound being <span class="math inline">\(k_2 \cdot n\)</span> it would be <span class="math inline">\(k_2 \cdot 1\)</span>; instead of a lower bound of <span class="math inline">\(k_1 \cdot n\)</span> it would be <span class="math inline">\(k_1 \cdot 1\)</span>. In other words, two horizontal lines of constant value as <span class="math inline">\(n\)</span> increases).</p>
</section>
<section id="theorem-3.2" class="level2">
<h2 class="anchored" data-anchor-id="theorem-3.2">Theorem 3.2</h2>
<p>I’ll restate Theorem 3.2 here for reference:</p>
<p>Let the LoRA adapters be of the form <span class="math inline">\(\gamma_rBA\)</span>, where <span class="math inline">\(B \in \mathbb{R}^{d_1 \times r}\)</span>, <span class="math inline">\(A \in \mathbb{R}^{r \times d_2}\)</span> are initialised such that <span class="math inline">\(B\)</span> is initially <span class="math inline">\(0_{d_1 \times r}\)</span>, entries of <span class="math inline">\(A\)</span> are iid with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma_A\)</span> not depending on <span class="math inline">\(r\)</span>, and <span class="math inline">\(\gamma_r \in \mathbb{R}\)</span> such that <span class="math inline">\(\gamma_r \rightarrow 0\)</span> as <span class="math inline">\(r \rightarrow \infty\)</span>.</p>
<p>In expectation over initialization, assuming the inputs to the adapter are iid distributed such that the <span class="math inline">\(m\)</span>’th moment is <span class="math inline">\(\Theta_r(1)\)</span> in each entry, we have that the <span class="math inline">\(m\)</span>’th moment of the outputs of the adapter is <span class="math inline">\(\Theta_r(1)\)</span> in each entry if and only if:</p>
<p><span class="math display">\[\gamma_r \in \Theta_r(\frac{1}{\sqrt{r}})\]</span></p>
<p>In expectation over initialization, assuming the loss gradient to the adapter outputs are <span class="math inline">\(\Theta_r(1)\)</span> in each entry, we have that the loss gradients into the input of the adapter are <span class="math inline">\(\Theta_r(1)\)</span> in each entry if and only if:</p>
<p><span class="math display">\[\gamma_r \in \Theta_r(\frac{1}{\sqrt{r}})\]</span></p>
<p>In particular, the above holds at any point in the learning trajectory if the assumptions do, and unless <span class="math inline">\(\gamma_r \in \Theta_r(\frac{1}{\sqrt{r}})\)</span>, there is unstable or collapsing learning for <span class="math inline">\(r\)</span> large enough.</p>
</section>
<section id="gradient-of-loss-mathcall-with-respect-to-adapters-a-and-b" class="level2">
<h2 class="anchored" data-anchor-id="gradient-of-loss-mathcall-with-respect-to-adapters-a-and-b">Gradient of Loss <span class="math inline">\(\mathcal{L}\)</span> with Respect to Adapters <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></h2>
<p>Let <span class="math inline">\(f(x) = \gamma_rBAx\)</span>, and <span class="math inline">\(\mathcal{L}(f(x))\)</span> denote the loss.</p>
<p>Let <span class="math inline">\(B_n\)</span>,<span class="math inline">\(A_n\)</span>, denote <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span> after the <span class="math inline">\(n\)</span>’th SGD update on input <span class="math inline">\(x_n\)</span> with learning rate <span class="math inline">\(\eta\)</span>.</p>
<p>Recall that <span class="math inline">\(B_0=0_{d \times r}\)</span>, and see that for <span class="math inline">\(v_n = \nabla_{f(x_n)}\mathcal{L}(f(x_n))\)</span>:</p>
<p><span class="math display">\[\nabla_{B_n}\mathcal{L} = \gamma_r v_n x_n^T A_n^T\]</span></p>
<p><span class="math display">\[\nabla_{A_n}\mathcal{L} = \gamma_r B_n^T v_n x_n^T\]</span></p>
<p>This comes from the chain rule. Since <span class="math inline">\(\mathcal{L}\)</span> is a function of <span class="math inline">\(f(x)\)</span>, and since <span class="math inline">\(f(x)\)</span> involves <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span>, the gradient of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(B\)</span> or <span class="math inline">\(A\)</span> is written as:</p>
<p><span class="math display">\[\nabla_{B_n}\mathcal{L} = \nabla_{f(x_n)}\mathcal{L} \cdot \nabla_{B_n}\mathcal{f(x_n)}\]</span></p>
<p><br></p>
<p><span class="math display">\[\nabla_{A_n}\mathcal{L} = \nabla_{f(x_n)}\mathcal{L} \cdot \nabla_{A_n}\mathcal{f(x_n)}\]</span></p>
<p>In each case, <span class="math inline">\(\nabla_{f(x_n)}\mathcal{L}\)</span> is given to us as <span class="math inline">\(v_n\)</span>.</p>
<p>When plugging in the matrix-vector product <span class="math inline">\(BAx\)</span> into <a href="https://www.matrixcalculus.org/">matrixcalculus.org</a> I get the following result for the partial derivatives of <span class="math inline">\(BAx\)</span> with respect to <span class="math inline">\(B\)</span> or <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[\frac{\partial}{\partial B}(B \cdot A \cdot x) = (A \cdot x)^T \otimes \mathbb{I}\]</span></p>
<p><br></p>
<p><span class="math display">\[\frac{\partial}{\partial A}(B \cdot A \cdot x) = x^T \otimes B\]</span></p>
<p>Where the <span class="math inline">\(\otimes\)</span> symbol is the tensor product or <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker Product</a> where you multiple each element of the first matrix by the second matrix—a very differently shaped result than anything in the rsLoRA proof</p>
<blockquote class="blockquote">
<p>If <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix and <span class="math inline">\(B\)</span> is a <span class="math inline">\(p \times q\)</span> matrix, then the Kronecker product <span class="math inline">\(A \otimes B\)</span> is <strong>the <span class="math inline">\(pm \times qn\)</span> block matrix</strong>.</p>
</blockquote>
<p><a href="https://math.stackexchange.com/a/1866845">This StackExchange solution’s first half</a> also shows something similar but without enough explanation for me to understand it.</p>
<p>Based on this it’s unclear to me how <span class="math inline">\(\nabla_{B_n}\mathcal{f(x_n)}\)</span> results in <span class="math inline">\(x_n^TA_n^T\)</span> and <span class="math inline">\(\nabla_{A_n}\mathcal{f(x_n)}\)</span> results in <span class="math inline">\(B_n^Tx_n^T\)</span>. I spent 5-6 hours googling, looking on YouTube and prompting Claude/ChatGPT and didn’t come away with much (other than confirming that I don’t know matrix calculus).</p>
<p>That being said, we can still look at the shapes of each matrix or vector and see how the different tranposing and placement of variables makes it all work.</p>
<p>In the rsLoRA paper:</p>
<p><br></p>
<p><span class="math inline">\(B\)</span> has the dimensions <span class="math inline">\(d_1 \times r\)</span></p>
<p><span class="math inline">\(A\)</span> has the dimensions <span class="math inline">\(r \times d_2\)</span></p>
<p><br></p>
<p>Therefore, the matrix product <span class="math inline">\(BA\)</span> which is (<span class="math inline">\(d_1 \times r\)</span>) <span class="math inline">\(\times\)</span> (<span class="math inline">\(r \times d_2\)</span>), has the dimension <span class="math inline">\(d_1 \times d_2\)</span> (the <span class="math inline">\(r\)</span>’s cancel out due to matrix multiplication).</p>
<p><br></p>
<p>Since <span class="math inline">\(x\)</span> is multiplied by <span class="math inline">\(BA\)</span> to get <span class="math inline">\(BAx\)</span>, and assuming <span class="math inline">\(x\)</span> is a vector, it has the dimensions <span class="math inline">\(d_2 \times 1\)</span> (since the first dimension of <span class="math inline">\(x\)</span> has to be equal to the last dimension of <span class="math inline">\(BA\)</span>).</p>
<p>Putting it all together, <span class="math inline">\(f(x) = BAx\)</span> has the dimensions:</p>
<p><br></p>
<p><span class="math inline">\((d_1 \times r) \times (r \times d_2) \times (d_2 \times 1) = d_1 \times 1\)</span></p>
<p><br></p>
<p>Note that the <span class="math inline">\(r\)</span>’s cancel out as do the <span class="math inline">\(d_2\)</span>’s.</p>
<p>I’ll now a similar dimensional analysis of the gradients of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>:</p>
<p><br></p>
<p><span class="math display">\[\nabla_{B_n}\mathcal{L} = \gamma_r v_n x_n^T A_n^T\]</span></p>
<p>The gradient <span class="math inline">\(\nabla_{B_n}\mathcal{L}\)</span> must have the same dimensions as <span class="math inline">\(B\)</span>, (<span class="math inline">\(d_1 \times r\)</span>), so that we can make the gradient update to each element of <span class="math inline">\(B\)</span>. Similarly, <span class="math inline">\(v_n\)</span> has to have the same dimensions as <span class="math inline">\(f(x)\)</span>, (<span class="math inline">\(d_1 \times 1\)</span>).</p>
<p><br></p>
<p><span class="math display">\[(d_1 \times r) = (d_1 \times 1) \times (1 \times d_2) \times (d_2 \times r)\]</span></p>
<p><br></p>
<p>The <span class="math inline">\(1\)</span>’sand the <span class="math inline">\(d_2\)</span>’s cancel out in the matrix multiplication, so we get:</p>
<p><br></p>
<p><span class="math display">\[(d_1 \times r) = (d_1 \times r)\]</span></p>
<p>The dimensions match.</p>
<p>Similarly for the gradient of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(A\)</span>, the dimensions of the gradient must equal the dimensions of <span class="math inline">\(A\)</span> (in order to do the gradient update):</p>
<p><br></p>
<p><span class="math display">\[\nabla_{A_n}\mathcal{L} = \gamma_r B_n^T v_n x_n^T\]</span></p>
<p><br></p>
<p><span class="math display">\[(r \times d_2) = (r \times d_1) \times (d_1 \times 1) \times(1 \times d_2)\]</span></p>
<p><br></p>
<p><span class="math display">\[(r \times d_2) = (r \times d_2)\]</span></p>
<p><br></p>
<p>The dimensions match.</p>
<p>The <span class="math inline">\(d_1\)</span>’s and the <span class="math inline">\(1\)</span>’s cancel out due to matrix multiplication.</p>
</section>
<section id="adapter-value-after-n-updates" class="level2">
<h2 class="anchored" data-anchor-id="adapter-value-after-n-updates">Adapter Value after <span class="math inline">\(n\)</span> Updates</h2>
<p>I struggled with this section so the following is just me thinking out loud and may not help clarify your understanding.</p>
<p>As a reminder, here are the expressions for the gradient of Loss with respect to the adapters <span class="math inline">\(A_n\)</span> and <span class="math inline">\(B_n\)</span> (where <span class="math inline">\(n\)</span> is the number of gradient updates during training):</p>
<p><br></p>
<p><span class="math display">\[\nabla_{B_n}\mathcal{L} = \gamma_r v_n x_n^T A_n^T\]</span></p>
<p><br></p>
<p><span class="math display">\[\nabla_{A_n}\mathcal{L} = \gamma_r B_n^T v_n x_n^T\]</span></p>
<p><br></p>
<p>After <span class="math inline">\(n \ge 1\)</span> SGD updates (in each update, we are subtracting from the adapter the learning rate times the gradient of the Loss with respect to the adapter) the two adapters <span class="math inline">\(B_n\)</span> and <span class="math inline">\(A_n\)</span> look like:</p>
<p><span class="math display">\[B_n = (-\eta \gamma_r \sum_{k=0}^{n-1}v_kx_k^T + \mathcal{O}_r(\gamma_r^2))A_0^T\]</span></p>
<p><span class="math display">\[A_n =A_0(1 + \mathcal{O}_r(\gamma_r^2))\]</span></p>
<p>Note that <span class="math inline">\(B\)</span> is initialized as a 0-matrix so there’s no <span class="math inline">\(B_0\)</span> in their expression.</p>
<p>Three observations:</p>
<ul>
<li>Though the gradient of Loss wrt <span class="math inline">\(B_n\)</span> contains an <span class="math inline">\(A_n\)</span> term (<span class="math inline">\(\gamma_rv_nx_n^TA_n^T\)</span>), the expression here for <span class="math inline">\(B_n\)</span> contains an <span class="math inline">\(A_0\)</span> term.</li>
<li>The expression here for <span class="math inline">\(A_n\)</span> does not include the gradient terms (<span class="math inline">\(\gamma_r v_nx_n^TA_n^T\)</span>).</li>
<li>There is an <span class="math inline">\(\mathcal{O}_r(\gamma_r^2)\)</span> term in both the <span class="math inline">\(B_n\)</span> and <span class="math inline">\(A_n\)</span> expressions.</li>
</ul>
<p>From those observations I’m coming to the following three conclusions (with the help of Claude):</p>
<ul>
<li>The term <span class="math inline">\(\mathcal{O}_r(\gamma_r^2)\)</span>, which is in Big-O notation, represents some term(s) that has an upper bound of <span class="math inline">\(\gamma_r^2\)</span> (in other words, some constant term). I’m not sure what term they actually represent—maybe some error term? I don’t know.</li>
<li><span class="math inline">\(A_n\)</span> is a function of <span class="math inline">\(A_0\)</span> and the constant term <span class="math inline">\(\mathcal{O}_r(\gamma_r^2)\)</span>. I wonder if that’s because <span class="math inline">\(A_0\)</span> is initialized as a normal (Gaussian) matrix and since it’s a rank-stable adapter, it doesn’t deviate that much from that normal distribution? Again, not sure. Additionally, in their <span class="math inline">\(B_n\)</span> expression they multiply by <span class="math inline">\(A_0^T\)</span> instead of the <span class="math inline">\(A_n^T\)</span> term in the gradient—maybe an indication that <span class="math inline">\(A_n\)</span> doesn’t deviate much from <span class="math inline">\(A_0\)</span>? Not sure.</li>
</ul>
<p>A supplementary graphic I created to try to illustrate my thinking + confusion:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="1.png" class="lightbox" title="Graphic showing the relationship between the gradient and the adapter value for B_n" data-gallery="quarto-lightbox-gallery-1"><img src="1.png" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">Graphic showing the relationship between the gradient and the adapter value for B_n</figcaption><p></p>
</figure>
</div>
<p>UPDATE: A fastai study group member provided the following insight which now clearly explains why <span class="math inline">\(B_n\)</span> is written in terms of <span class="math inline">\(A_0\)</span>. It’s because the derative of <span class="math inline">\(A_n\)</span> has a <span class="math inline">\(B_n\)</span> term in it (<span class="math inline">\(\nabla_{A_n} = \gamma_rB_n^Tv_nX_n^T\)</span>) and one step after initialization (<span class="math inline">\(n=1\)</span>), <span class="math inline">\(B_1\)</span> is written in terms of <span class="math inline">\(A_0\)</span>:</p>
<p><span class="math inline">\(n=0\)</span>:</p>
<p><span class="math display">\[B_n = 0\]</span> <span class="math display">\[A_n = A_0\]</span></p>
<p><br></p>
<p><span class="math display">\[\nabla_{B_0}\mathcal{L}=\gamma_rv_0x_0^TA_0^T\]</span></p>
<p><span class="math display">\[\nabla_{A_0}\mathcal{L}=\gamma_rB_0^Tv_0x_0^T = 0\]</span></p>
<p><span class="math inline">\(n=1\)</span>:</p>
<p><span class="math display">\[B_1 = 0 - \gamma_rv_0x_0^TA_0^T\]</span></p>
<p><span class="math display">\[A_1 = A_0 - 0 = A_0\]</span></p>
<p><br></p>
<p><span class="math display">\[\nabla_{B_1}\mathcal{L}=\gamma_rv_1x_1^TA_1^T = \gamma_rv_1x_1^TA_0^T\]</span></p>
<p><span class="math display">\[\nabla_{A_1}\mathcal{L}=\gamma_rB_1^Tv_1x_1^T = \gamma_r(\gamma_rv_0x_0^TA_0^T)^Tv_1x_1^T\]</span></p>
<p><br></p>
<p>Notice how <span class="math inline">\(B_1\)</span> has an <span class="math inline">\(A_0\)</span> term in it. Similarly <span class="math inline">\(\nabla_{A_1}\mathcal{L}\)</span> is in terms of <span class="math inline">\(A_0\)</span> as well since <span class="math inline">\(B_1\)</span> is in terms of <span class="math inline">\(A_0\)</span>.</p>
</section>
<section id="deriving-stable-rank" class="level2">
<h2 class="anchored" data-anchor-id="deriving-stable-rank">Deriving Stable Rank</h2>
<p>Let’s just take their expressions of <span class="math inline">\(B_n\)</span> and <span class="math inline">\(A_n\)</span> for granted and continue with the derivation of the stable rank condition:</p>
<p><span class="math display">\[B_n = (-\eta \gamma_r \sum_{k=0}^{n-1}v_kx_k^T + \mathcal{O}_r(\gamma_r^2))A_0^T\]</span></p>
<p><span class="math display">\[A_n =A_0(1 + \mathcal{O}_r(\gamma_r^2))\]</span></p>
<p>Then <span class="math inline">\(\gamma_rB_nA_n\)</span> is:</p>
<p><span class="math display">\[\gamma_rB_nA_n = -\gamma_r^2\eta\sum_{k=0}^{n-1}v_kx^T_kA^T_0A_0 + \mathcal{O}_r(\gamma_r^3)A_0^TA_0\]</span></p>
<p>To try and derive this result, I’ll start with <span class="math inline">\(B_n\)</span> and expand <span class="math inline">\(B_n\)</span> by multiplying the terms inside the parentheses by <span class="math inline">\(A_0^T\)</span>:</p>
<p><br></p>
<p><span class="math display">\[B_n = \big(-\eta \gamma_r \sum_{k=0}^{n-1}v_kx_k^T + \mathcal{O}_r(\gamma_r^2)\big)A_0^T = -\eta \gamma_r \sum_{k=0}^{n-1}v_kx_k^TA_0^T + \mathcal{O}_r(\gamma_r^2)A_0^T\]</span></p>
<p>I’ll then expand <span class="math inline">\(A_n\)</span> by multiplying the terms inside the parentheses by <span class="math inline">\(A_0\)</span>:</p>
<p><br></p>
<p><span class="math display">\[A_n =A_0(1 + \mathcal{O}_r(\gamma_r^2)) = A_0 + A_0\mathcal{O}_r(\gamma_r^2)\]</span></p>
<p>Then, I’ll write out the full multiplication of <span class="math inline">\(\gamma_rB_nA_n\)</span>:</p>
<p><span class="math display">\[\gamma_rB_nA_n = \big[ \gamma_r \big] \times \big[-\eta \gamma_r \sum_{k=0}^{n-1}v_kx_k^TA_0^T + \mathcal{O}_r(\gamma_r^2)A_0^T\big] \times \big[ A_0 + A_0\mathcal{O}_r(\gamma_r^2) \big]\]</span></p>
<p><span class="math inline">\(B_n\)</span> is getting multiplied by two terms, <span class="math inline">\(A_0\)</span> and <span class="math inline">\(A_0\mathcal{O}_r(\gamma_r^2)\)</span>. Doing that multiplication and expanding it out:</p>
<p><span class="math display">\[\gamma_rB_nA_n = \big[ \gamma_r \big] \times \big[ -\eta \gamma_r \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \mathcal{O}_r(\gamma_r^2)A_0^TA_0 + -\eta \gamma_r \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\mathcal{O}_r(\gamma_r^2) + \mathcal{O}_r(\gamma_r^2)A_0^TA_0\mathcal{O}_r(\gamma_r^2) \big]\]</span></p>
<p>Now I’ll multiple the <span class="math inline">\(\gamma_r\)</span> term at the start into the giant second term:</p>
<p><span class="math display">\[\gamma_rB_nA_n = -\eta \gamma_r^2 \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \gamma_r\mathcal{O}_r(\gamma_r^2)A_0^TA_0 + -\eta \gamma_r^2 \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\mathcal{O}_r(\gamma_r^2) + \gamma_r\mathcal{O}_r(\gamma_r^2)A_0^TA_0\mathcal{O}_r(\gamma_r^2)\]</span></p>
<p>Next, I’ll highlight terms where <span class="math inline">\(\gamma_r\)</span> is multiplied by <span class="math inline">\(\mathcal{O}_r(\gamma_r^2)\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="2.png" class="lightbox" title="Graphic showing highlighted gamma_r terms" data-gallery="quarto-lightbox-gallery-2"><img src="2.png" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">Graphic showing highlighted gamma_r terms</figcaption><p></p>
</figure>
</div>
<p>The first highlighted term, <span class="math inline">\(\gamma_r\mathcal{O}_r(\gamma_r^2)\)</span> becomes <span class="math inline">\(\mathcal{O}_r(\gamma_r^3)\)</span>.</p>
<p><br></p>
<p>The second (<span class="math inline">\(\gamma_r^2\)</span>) and third (<span class="math inline">\(\mathcal{O}_r(\gamma_r^2)\)</span>) highlighted terms multiply to become <span class="math inline">\(\mathcal{O}_r(\gamma_r^4)\)</span>.</p>
<p><br></p>
<p>The fourth (<span class="math inline">\(\gamma_r\mathcal{O}_r(\gamma_r^2)\)</span>) and fifth (<span class="math inline">\(\mathcal{O}_r(\gamma_r^2)\)</span>) highlighted terms multiply to become <span class="math inline">\(\mathcal{O}_r(\gamma_r^5)\)</span>.</p>
<p>Rewriting the expression with those simplifications:</p>
<p><span class="math display">\[\gamma_rB_nA_n = -\eta \gamma_r^2 \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \mathcal{O}_r(\gamma_r^3)A_0^TA_0 + -\eta \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\mathcal{O}_r(\gamma_r^4) + A_0^TA_0\mathcal{O}_r(\gamma_r^5)\]</span></p>
<p>According to what I understood from prompting Claude, the last two terms, <span class="math inline">\(-\eta \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\mathcal{O}_r(\gamma_r^4) + A_0^TA_0\mathcal{O}_r(\gamma_r^5)\)</span> are encompassed by the earlier <span class="math inline">\(\mathcal{O}_r(\gamma_r^3)\)</span> term. The reason being that since <span class="math inline">\(\gamma_r\)</span> goes to <span class="math inline">\(0\)</span> (as <span class="math inline">\(r\)</span> goes to <span class="math inline">\(\infty\)</span>) as stated at the beginning of Theorem 3.2, the term <span class="math inline">\(\mathcal{O}_r(\gamma_r^3)\)</span>, where <span class="math inline">\(\gamma_r^3\)</span> the upper bound, will always be larger than <span class="math inline">\(\mathcal{O}_r(\gamma_r^4)\)</span> or <span class="math inline">\(\mathcal{O}_r(\gamma_r^5)\)</span>.</p>
<p>As <span class="math inline">\(\gamma_r\)</span> goes to <span class="math inline">\(0\)</span>, <span class="math inline">\(\gamma_r^3 \gt \gamma_r^4 \gt \gamma_r^5\)</span>.</p>
<p><br></p>
<p>So with the <span class="math inline">\(\mathcal{O}_r(\gamma_r^4)\)</span> and <span class="math inline">\(\mathcal{O}_r(\gamma_r^5)\)</span> getting swallowed by the <span class="math inline">\(\mathcal{O}_r(\gamma_r^3)\)</span> term, rewriting the expression gives us:</p>
<p><span class="math display">\[\gamma_rB_nA_n = -\eta \gamma_r^2 \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \mathcal{O}_r(\gamma_r^3)A_0^TA_0\]</span></p>
<p>Which is the expression in Equation (8) of the rsLoRA paper.</p>
<p>Next they define the expectation of the initialiation <span class="math inline">\(A_0\)</span> as:</p>
<p><span class="math display">\[E_{A_0}(A_0^TA_0) = r\sigma_AI_{d \times d}\]</span></p>
<p>and replace <span class="math inline">\(A_0^TA_0\)</span> with <span class="math inline">\(r\sigma_AI_{d \times d}\)</span> in the expression of <span class="math inline">\(\gamma_rB_nA_n\)</span>:</p>
<p><span class="math display">\[\gamma_rB_nA_n = -\eta \gamma_r^2 \sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \mathcal{O}_r(\gamma_r^3)A_0^TA_0 = -\eta \gamma_r^2 \sum_{k=0}^{n-1}v_kx_k^Tr\sigma_AI_{d \times d} + \mathcal{O}_r(\gamma_r^3)r\sigma_AI_{d \times d}\]</span></p>
<p>I think the last term, <span class="math inline">\(\mathcal{O}_r(\gamma_r^3)r\sigma_AI_{d \times d}\)</span> gets simplified to <span class="math inline">\(\mathcal{O}_r(\gamma_r^3)\)</span>, and multipling by the identity matrix <span class="math inline">\(I_{d \times d}\)</span> is like multiplying by <span class="math inline">\(1\)</span>, so we end up with Equation (9) in the rsLoRA paper:</p>
<p><br></p>
<p><span class="math display">\[\gamma_rB_nA_n = -\gamma_r^2 r\sigma_A \eta\sum_{k=0}^{n-1}v_kx_k^T + \mathcal{O}_r(\gamma_r^3)\]</span></p>
<p>I’m very fuzzy on the final steps, but taking a shot at explaining how I understand it:</p>
<p>Condition 1 of Definition 3.1 states:</p>
<ol type="1">
<li>If the inputs to the adapter are iid such that the <span class="math inline">\(m\)</span>’th moment is <span class="math inline">\(\Theta_r(1)\)</span> in each entry, then the <span class="math inline">\(m\)</span>’th moment of the outputs of the adapter is also <span class="math inline">\(\Theta_r(1)\)</span> in each entry.</li>
</ol>
<p>The forward pass through the adapters is <span class="math inline">\(\gamma_rB_nA_nx_n\)</span>.</p>
<p>The <span class="math inline">\(m\)</span>’th moment of the iid inputs is represented by the expression:</p>
<p><span class="math display">\[E_x((x_k^Tx_n)^m) \in \Theta_r(1)\]</span></p>
<p>Where does the term <span class="math inline">\(E_x((x_k^Tx_n)^m)\)</span> comes from? I think it comes from Equation (11). First I’ll write Equation (9) again for reference:</p>
<p><span class="math display">\[\gamma_rB_nA_n = -\gamma_r^2 r\sigma_A \eta\sum_{k=0}^{n-1}v_kx_k^T + \mathcal{O}_r(\gamma_r^3)\]</span></p>
<p>The forward pass multiplies Equation (9) by the new input <span class="math inline">\(x_n\)</span> to get something like this (not shown in the paper, my assumption):</p>
<p><span class="math display">\[\gamma_rB_nA_nx_n = -\gamma_r^2 r\sigma_A \eta\sum_{k=0}^{n-1}v_kx_k^Tx_n + \mathcal{O}_r(\gamma_r^3)\]</span></p>
<p>Note that we now have a <span class="math inline">\(x_k^Tx_n\)</span> term inside the summation.</p>
<p>Let’s look at just the left-hand side of Equation (11) now:</p>
<p><span class="math display">\[E_{x,A_0}((\gamma_rB_nA_nx_n)^m)\]</span></p>
<p>This is the expression for <span class="math inline">\(m\)</span>’th moment of the forward pass (if I understand correctly).</p>
<p>Looking at the whole Equation (11):</p>
<p><span class="math display">\[E_{x,A_0}((\gamma_r B_n A_n x_n)^m) = (-\gamma_r^2r\sigma_A\eta)^m\sum_{k=0}^{n-1}v_k^mE_x((x_k^Tx_n)^m) + \Theta_r((\gamma_r^3r)^m) \in \Theta_r((\gamma_r^2r)^m)\]</span></p>
<p>Everything on the right-hand side of the equation is raised to the power of <span class="math inline">\(m\)</span>:</p>
<ul>
<li><span class="math inline">\((-\gamma_r^2r\sigma_A\eta)^m\)</span></li>
<li><span class="math inline">\(v_k^m\)</span></li>
<li><span class="math inline">\((x_k^Tx_n)^m\)</span></li>
<li><span class="math inline">\(\Theta_r((\gamma_r^3r)^m)\)</span></li>
</ul>
<p>The expected value is taken of <span class="math inline">\(x\)</span> and <span class="math inline">\(A_0\)</span>. We already took care of the expectation of <span class="math inline">\(A_0\)</span> earlier with the term <span class="math inline">\(r\sigma_A\)</span>. Equation (11) takes care of the expectation of <span class="math inline">\(x\)</span> with the term <span class="math inline">\(E_x((x_k^Tx_n)^m)\)</span>. At least that’s my understanding.</p>
<p>Finally the stuff at the end:</p>
<p><span class="math display">\[\in \Theta_r((\gamma_r^2r)^m)\]</span></p>
<p>Is saying that this expected value <span class="math inline">\(E_{x,A_0}\)</span> is in the set of values bound above and below by <span class="math inline">\((\gamma_r^2r)^m\)</span>. Why? Well there are two <span class="math inline">\(\gamma_r\)</span> terms in <span class="math inline">\(E_{x,A_0}\)</span>:</p>
<p><br></p>
<p><span class="math inline">\((\gamma_r^2r\sigma_A\eta)^m\)</span></p>
<p>and</p>
<p><span class="math inline">\(\Theta_r((\gamma_r^3r)^m)\)</span></p>
<p><br></p>
<p>I think it’s correct to say that the term <span class="math inline">\((\gamma_r^2r\sigma_A\eta)^m\)</span> is in the set <span class="math inline">\(\Theta_r((\gamma_r^2r)^m)\)</span> (in other words it’s bound above and below by a constant times <span class="math inline">\((\gamma_r^2r)^m\)</span>).</p>
<p><span class="math inline">\(\Theta_r((\gamma_r^2r)^m)\)</span> encompasses <span class="math inline">\(\Theta_r((\gamma_r^3r)^m)\)</span> since as <span class="math inline">\(\gamma_r\)</span> goes to <span class="math inline">\(0\)</span> (an initial assumption of Theorem 3.2), <span class="math inline">\(\gamma_r^2 \gt \gamma_r^3\)</span>.</p>
<p>Definition 3.1 stated that the <span class="math inline">\(m\)</span>’th moments of the adapter output have to be in the set <span class="math inline">\(\Theta_r(1)\)</span> for the adapters to be considered rank-stable.</p>
<p>If the <span class="math inline">\(m\)</span>’th moment, <span class="math inline">\(E_{x,A_0}((\gamma_r B_n A_n x_n)^m)\)</span>, is in the set <span class="math inline">\(\Theta_r((\gamma_r^2r)^m)\)</span> (as is defined in Equation (11)) and if Definition 3.1 condition is to be satisfied, the set <span class="math inline">\(\Theta_r((\gamma_r^2r)^m)\)</span> must be equal to <span class="math inline">\(\Theta_r(1)\)</span>:</p>
<p><br></p>
<p><span class="math inline">\(\Theta_r((\gamma_r^2r)^m)\)</span> = <span class="math inline">\(\Theta_r(1)\)</span></p>
<p><br></p>
<p>Equating the terms inside the <span class="math inline">\(\Theta_r\)</span> on each side:</p>
<p><span class="math inline">\((\gamma_r^2r)^m = 1\)</span></p>
<p><br></p>
<p>Raising each side to <span class="math inline">\(\frac{1}{m}\)</span> (to get rid of the <span class="math inline">\(m\)</span> exponent on the left) gives us:</p>
<p><span class="math inline">\(\gamma_r^2r = 1\)</span></p>
<p><br></p>
<p>Dividing both sides by <span class="math inline">\(r\)</span>:</p>
<p><span class="math inline">\(\gamma_r^2 = \frac{1}{r}\)</span></p>
<p><br></p>
<p>Taking the square root of both sides:</p>
<p><span class="math inline">\(\gamma_r = \frac{1}{\sqrt{r}}\)</span></p>
<p><br></p>
<p>Which is the proof that in order for the adapters to have stable outputs the value of <span class="math inline">\(\gamma_r\)</span> must be a constant of <span class="math inline">\(\frac{1}{\sqrt{r}}\)</span> or in other words:</p>
<p><span class="math display">\[\gamma_r \in \Theta_r(\frac{1}{\sqrt{r}})\]</span></p>
<p>I don’t understand how they derived Equation (10) so I’m not going to write about it here.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","loop":true,"descPosition":"bottom","openEffect":"zoom","closeEffect":"zoom"});</script>



</body></html>