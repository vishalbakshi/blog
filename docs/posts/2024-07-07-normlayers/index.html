<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-07-07">
<meta name="description" content="In this notebook I train CNNs using different activation normalization layers and compare performance and results.">

<title>Vishal Bakshi’s Blog - Comparing CNN Performance by Varying Activation Normalization Layers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi’s Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#visualizing-activation-normalization-layer-outputs" id="toc-visualizing-activation-normalization-layer-outputs" class="nav-link" data-scroll-target="#visualizing-activation-normalization-layer-outputs">Visualizing Activation Normalization Layer Outputs</a>
  <ul class="collapse">
  <li><a href="#batchnorm2d" id="toc-batchnorm2d" class="nav-link" data-scroll-target="#batchnorm2d"><code>BatchNorm2d</code></a></li>
  <li><a href="#instancenorm2d" id="toc-instancenorm2d" class="nav-link" data-scroll-target="#instancenorm2d"><code>InstanceNorm2d</code></a></li>
  <li><a href="#groupnorm" id="toc-groupnorm" class="nav-link" data-scroll-target="#groupnorm"><code>GroupNorm</code></a></li>
  <li><a href="#localresponsenorm" id="toc-localresponsenorm" class="nav-link" data-scroll-target="#localresponsenorm"><code>LocalResponseNorm</code></a></li>
  <li><a href="#layernorm" id="toc-layernorm" class="nav-link" data-scroll-target="#layernorm"><code>LayerNorm</code></a></li>
  </ul></li>
  <li><a href="#training-models-using-different-normalization-layers" id="toc-training-models-using-different-normalization-layers" class="nav-link" data-scroll-target="#training-models-using-different-normalization-layers">Training Models Using Different Normalization Layers</a>
  <ul class="collapse">
  <li><a href="#analyzing-training-results" id="toc-analyzing-training-results" class="nav-link" data-scroll-target="#analyzing-training-results">Analyzing Training Results</a></li>
  </ul></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Comparing CNN Performance by Varying Activation Normalization Layers</h1>
  <div class="quarto-categories">
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">fastai</div>
    <div class="quarto-category">python</div>
  </div>
  </div>

<div>
  <div class="description">
    In this notebook I train CNNs using different activation normalization layers and compare performance and results.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this notebook I’ll work through the following “Further Research” prompt given at the end of Chapter 13 (Convolutional Neural Networks) of the fastai textbook:</p>
<blockquote class="blockquote">
<p>Other normalization layers are available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed and how they differ from batch normalization.</p>
</blockquote>
<p>I’ll use the following 5 normalization layers to train separate models and compare model performance and activations:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">BatchNorm2d</a></li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html">InstanceNorm</a></li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html">GroupNorm</a></li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html">Local Response Normalization</a></li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">LayerNorm</a></li>
</ul>
</section>
<section id="visualizing-activation-normalization-layer-outputs" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-activation-normalization-layer-outputs">Visualizing Activation Normalization Layer Outputs</h2>
<p>Before I get into the trainings, I’ll visualize the outputs of each normalization layer, given an input batch of MNIST images.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.callback.hook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>matplotlib.rc(<span class="st">'image'</span>, cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="e331957c-463c-4e0b-a88d-8867339f6b3a" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.MNIST)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>path.ls()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="15687680" class="" max="15683414" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.03% [15687680/15683414 00:00&lt;00:00]
    </div>
    
</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>(#2) [Path('/root/.fastai/data/mnist_png/testing'),Path('/root/.fastai/data/mnist_png/training')]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a function to change dls params</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dls(bs<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> DataBlock(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>      blocks<span class="op">=</span>(ImageBlock(cls<span class="op">=</span>PILImageBW), CategoryBlock),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>      get_items<span class="op">=</span>get_image_files,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>      splitter<span class="op">=</span>GrandparentSplitter(<span class="st">'training'</span>, <span class="st">'testing'</span>),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>      get_y<span class="op">=</span>parent_label,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>      batch_tfms<span class="op">=</span>Normalize()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  ).dataloaders(path, bs<span class="op">=</span>bs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="b5686f4b-21fc-4760-bedc-78df99006b46" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(<span class="dv">512</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dls.show_batch(max_n<span class="op">=</span><span class="dv">9</span>, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> dls.one_batch()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="batchnorm2d" class="level3">
<h3 class="anchored" data-anchor-id="batchnorm2d"><code>BatchNorm2d</code></h3>
<p>First up is <code>BatchNorm2d</code> which is the layer we use in Chapter 13 of the fastai textbook when training a CNN.</p>
<p>Here’s an image without any normalization applied (left) and the same image after <code>BatchNorm2d</code> is applied <strong>to the entire batch</strong> on the right—it looks pretty much the same! :</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_ims(a, b):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  im1 <span class="op">=</span> ax1.imshow(a.squeeze())<span class="op">;</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  ax1.set_title(<span class="st">'Original'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  im2 <span class="op">=</span> ax2.imshow(b.detach().numpy().squeeze())<span class="op">;</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  ax2.set_title(<span class="st">'After Normalization Layer Applied'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="5af52232-2289-4b88-bd70-e6e38590a5fc" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], nn.BatchNorm2d(<span class="dv">1</span>)(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Here is the mean absolute difference between the 784 pixels in both images (pre- and post-normalization)—in other words, not that different!</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mad(a,b): <span class="cf">return</span> (a<span class="op">-</span>b).<span class="bu">abs</span>().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="72a0459a-9694-45b1-cb3c-3120ccf3d883" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mad(xb[<span class="dv">0</span>], nn.BatchNorm2d(<span class="dv">1</span>)(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>TensorImageBW(0.0040, grad_fn=&lt;AliasBackward0&gt;)</code></pre>
</div>
</div>
<p>It wasn’t until I asked Claude for an explanation that I understood that this normalization occurs across the entire batch (I know—it’s right there in the name!):</p>
<p>Prompt:</p>
<blockquote class="blockquote">
<p>will the normalization be visible per image?</p>
</blockquote>
<p>Response (emphasis mine):</p>
<blockquote class="blockquote">
<p>The normalization in BatchNorm2d is not typically visible per individual image. Here’s why:</p>
<ol type="1">
<li>Channel-wise operation: BatchNorm2d normalizes across the entire batch for each channel.</li>
<li>Batch statistics: It uses mean and variance computed <strong>over all images in the batch, not per image</strong>.</li>
<li>Shared parameters: The learnable scale (gamma) and shift (beta) parameters are applied equally to all images.</li>
<li>Subtle effects: The normalization often results in subtle changes that may not be visually apparent.</li>
<li>Purpose: It’s designed to stabilize internal representations, <strong>not to visibly alter individual images</strong>.</li>
<li>Inference behavior: During inference, it uses running statistics, further reducing per-image variation.</li>
</ol>
<p>To see BatchNorm2d’s effect, you’d typically need to analyze the statistical properties of the activations across the entire batch or network, rather than looking at individual images.</p>
</blockquote>
<p>Here’s a couple other ways to realize this behavior—the mean pixel value per image in the batch is relatively unchanged after <code>BatchNorm2d</code> is applied:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_hists(a, b, label_a, label_b, xlabel):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  plt.hist(a, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span>label_a)<span class="op">;</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  plt.hist(b, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span>label_b)<span class="op">;</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(xlabel)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="0d40b863-7c12-49c5-9269-995e98cefc3d" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.mean(<span class="dv">0</span>).flatten(),</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>nn.BatchNorm2d(<span class="dv">1</span>)(xb).mean(<span class="dv">0</span>).flatten().detach().numpy(),</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After BatchNorm2d'</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Mean Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Here is the distribution of pixel values across the entire batch—again, relatively unchanged after <code>BatchNorm2d</code> is applied:</p>
<div class="cell" data-outputid="d5546e37-d99e-41d1-9271-51e51c5d7228" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.flatten(),</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>nn.BatchNorm2d(<span class="dv">1</span>)(xb).flatten().detach().numpy(),</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After BatchNorm2d'</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>After <code>BatchNorm2d</code> is applied, the average pixel value in a single image is relatively unchanged <strong>but the average pixel value across a batch is effectively 0</strong>.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_means(a, b):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> a.mean(), b.mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="85abca08-7ee0-44ff-d167-627e37fa3b3a" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>get_means(xb[<span class="dv">0</span>], nn.BatchNorm2d(<span class="dv">1</span>)(xb)[<span class="dv">0</span>]) <span class="co"># mean of single image</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>(TensorImageBW(0.0668), TensorImageBW(0.0628, grad_fn=&lt;AliasBackward0&gt;))</code></pre>
</div>
</div>
<div class="cell" data-outputid="ed5e0b49-39df-46a7-be3d-d4f885995e9f" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, nn.BatchNorm2d(<span class="dv">1</span>)(xb)) <span class="co"># mean of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>(TensorImageBW(0.0037), TensorImageBW(2.1858e-08, grad_fn=&lt;AliasBackward0&gt;))</code></pre>
</div>
</div>
<p>Interesting to note that in this case, since the standard deviation of the batch is close to 1, it doesn’t change much after <code>BatchNorm2d</code> is applied:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_stds(a, b):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> a.std(), b.std()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="48b4d186-fdf1-4bd5-ec87-a3998c43eb46" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>get_stds(xb, nn.BatchNorm2d(<span class="dv">1</span>)(xb)) <span class="co"># std of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>(TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=&lt;AliasBackward0&gt;))</code></pre>
</div>
</div>
<p>Lastly, I’ll look at the means and standard deviations of 100 batches after <code>BatchNorm2d</code> is applied:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_means_stds(norm):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  means <span class="op">=</span> []</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  stds <span class="op">=</span> []</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    xb, _ <span class="op">=</span> dls.one_batch()</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    means.append(norm(xb).mean().item())</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    stds.append(norm(xb).std().item())</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">10</span>))</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>  ax1.hist(means)<span class="op">;</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>  ax1.set_title(<span class="st">'Means'</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>  ax1.set_xlabel(<span class="st">'Value'</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>  ax1.set_ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot second histogram</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>  ax2.hist(stds)<span class="op">;</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>  ax2.set_title(<span class="st">'Standard Deviations'</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>  ax2.set_xlabel(<span class="st">'Value'</span>)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>  ax2.set_ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>  plt.tight_layout()</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As expected, the mean of the batch is effectively 0 and standard deviation is 1 (note that the x-axis for means is 1e-8 and for standard deviations is 1e-7 + 9.99996e-1):</p>
<div class="cell" data-outputid="f15b95a9-5d65-4ca7-b413-a4d445be2b34" data-execution_count="19">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plot_means_stds(nn.BatchNorm2d(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="instancenorm2d" class="level3">
<h3 class="anchored" data-anchor-id="instancenorm2d"><code>InstanceNorm2d</code></h3>
<p>Reading the PyTorch documentation, it seems like <code>InstanceNorm2d</code> is similar to <code>BatchNorm2d</code> (the formulas look the same). They note that <code>InstanceNorm2d</code> is similar to <code>LayerNorm</code> which I’ll look at later on.</p>
<p>I’ll start by instantiating the normalization layer:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>inorm <span class="op">=</span> nn.InstanceNorm2d(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The individual images effectively look the same before and after <code>InstanceNorm2d</code> is applied:</p>
<div class="cell" data-outputid="f2e78a53-7024-447f-a29e-662f16245875" data-execution_count="21">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], inorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The mean absolute difference between all 784 pixel values in each image is considerably larger than the difference using <code>BatchNorm2d</code>:</p>
<div class="cell" data-outputid="c0352af7-bab3-4ad0-9200-30eb0c7540a9" data-execution_count="22">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>mad(xb[<span class="dv">0</span>], inorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>TensorImageBW(0.0668)</code></pre>
</div>
</div>
<p>The differences between the distributions of mean pixel value in each image before/after <code>InstanceNorm2d</code> is applied seems larger than the difference in distributions before/after <code>BatchNorm2d</code> is applied.</p>
<div class="cell" data-outputid="e0971f88-cf44-41d5-e88f-362a70afa41f" data-execution_count="23">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.mean(<span class="dv">0</span>).flatten(),</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>inorm(xb).mean(<span class="dv">0</span>).flatten().detach().numpy(),</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After InstanceNorm2d'</span>,</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Mean Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>A considerable number of pixels in the batch, after <code>InstanceNorm2d</code> is applied, have more negative values than the original batch.</p>
<div class="cell" data-outputid="df3388f6-1780-4ce7-d4e8-c9e2ffbde23f" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.flatten(),</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>inorm(xb).flatten().detach().numpy(),</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After InstanceNorm2d'</span>,</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The mean of a single image is lower after <code>InstanceNorm2d</code> is applied:</p>
<div class="cell" data-outputid="ffa782c3-d35c-4913-ad9b-c9d923b62069" data-execution_count="25">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>get_means(xb[<span class="dv">0</span>], inorm(xb)[<span class="dv">0</span>]) <span class="co"># mean of single image</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>(TensorImageBW(0.0668), TensorImageBW(7.9067e-09))</code></pre>
</div>
</div>
<p>The mean of the batch is lower as well:</p>
<div class="cell" data-outputid="49084903-0ac2-48dd-badf-a61709787823" data-execution_count="26">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, inorm(xb)) <span class="co"># mean of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>(TensorImageBW(0.0037), TensorImageBW(-8.3629e-10))</code></pre>
</div>
</div>
<p>The standard deviation of a batch is 1:</p>
<div class="cell" data-outputid="26498089-1c7f-43be-b94f-af9fca072454" data-execution_count="27">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>get_stds(xb, inorm(xb)) <span class="co"># std of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>(TensorImageBW(1.0045), TensorImageBW(1.0000))</code></pre>
</div>
</div>
<p>Similar to <code>BatchNorm2d</code>, the mean and standard deviation for a batch is about 0 and 1, respectively, for <code>InstanceNorm2d</code>.</p>
<div class="cell" data-outputid="a66b3b41-6860-428b-f049-3354183c1996" data-execution_count="28">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>plot_means_stds(inorm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="groupnorm" class="level3">
<h3 class="anchored" data-anchor-id="groupnorm"><code>GroupNorm</code></h3>
<p>This normalization layer normalizes the data across the specified <code>num_groups</code>, so it’s like <code>BatchNorm2d</code> but for groups in the batch.</p>
<p>The documentation states that:</p>
<blockquote class="blockquote">
<p><code>num_channels</code> must be divisible by <code>num_groups</code></p>
</blockquote>
<p>So in the case of my MNIST black-and-white single-channel images, I would expect <code>GroupNorm</code> results to be the same as <code>BatchNorm2d</code>.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>gnorm <span class="op">=</span> nn.GroupNorm(<span class="dv">1</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The individual image remains unchanged (at least by inspection):</p>
<div class="cell" data-outputid="2a54efe7-5d11-457e-f8b8-81d7e129bd83" data-execution_count="30">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], gnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The mean average difference between pre- and post-<code>GroupNorm</code> image is larger than <code>BatchNorm2d</code>:</p>
<div class="cell" data-outputid="d71f6321-3e0d-4dc8-b7e0-9bf2db884506" data-execution_count="31">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>mad(xb[<span class="dv">0</span>], gnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>TensorImageBW(0.0668, grad_fn=&lt;AliasBackward0&gt;)</code></pre>
</div>
</div>
<p>There’s visually more variance between the mean image distribution before/after <code>GroupNorm</code> than <code>BatchNorm2d</code>:</p>
<div class="cell" data-outputid="4efb6a77-eaef-4098-c6f8-6bb4b13fead3" data-execution_count="32">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.mean(<span class="dv">0</span>).flatten(),</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>gnorm(xb).mean(<span class="dv">0</span>).flatten().detach().numpy(),</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After GroupNorm'</span>,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Mean Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>There is a considerable difference between pre- and post-<code>GroupNorm</code> distributions (by visual inspection):</p>
<div class="cell" data-outputid="fffd54ba-3257-42a2-fc9b-4d2cd0afc961" data-execution_count="33">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.flatten(),</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>gnorm(xb).flatten().detach().numpy(),</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After GroupNorm'</span>,</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The mean pixel value for the given image is effectively 0 after <code>GroupNorm</code> is applied:</p>
<div class="cell" data-outputid="2ab849a2-b0ad-4bfe-c293-fbda1b15b8d0" data-execution_count="35">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>get_means(xb[<span class="dv">0</span>], gnorm(xb)[<span class="dv">0</span>]) <span class="co"># mean of single image</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>(TensorImageBW(0.0668), TensorImageBW(2.3112e-08, grad_fn=&lt;AliasBackward0&gt;))</code></pre>
</div>
</div>
<p>As is the mean of a given batch:</p>
<div class="cell" data-outputid="9fdb1eaf-b57d-4c31-a849-c8406c66b653" data-execution_count="36">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, gnorm(xb)) <span class="co"># mean of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>(TensorImageBW(0.0037), TensorImageBW(1.9007e-10, grad_fn=&lt;AliasBackward0&gt;))</code></pre>
</div>
</div>
<p>The standard deviation of the single batch is 1:</p>
<div class="cell" data-outputid="4551e2be-16e3-40db-d3c5-0404569892af" data-execution_count="37">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>get_stds(xb, gnorm(xb)) <span class="co"># std of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>(TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=&lt;AliasBackward0&gt;))</code></pre>
</div>
</div>
<p>While the mean and standard deviation of the 100 batches is close to 0 and 1 respectively, the distributions of mean for <code>GroupNorm</code> is centered around 1e-9 while <code>BatchNorm2d</code> and <code>InstanceNorm</code> were centered around 0.</p>
<div class="cell" data-outputid="4707b058-f2e9-48b4-aada-4201099f3a78" data-execution_count="38">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>plot_means_stds(gnorm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-38-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="localresponsenorm" class="level3">
<h3 class="anchored" data-anchor-id="localresponsenorm"><code>LocalResponseNorm</code></h3>
<p>The PyTorch docs describe this normalization layer as follows:</p>
<blockquote class="blockquote">
<p>Applies local response normalization over an input signal.</p>
<p>The input signal is composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</p>
</blockquote>
<p>Given that I only have one channel (BW images) I expect the normalization effect to be similar to <code>BatchNorm2d</code>. I’ll use the default settings first:</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>lrnorm <span class="op">=</span> nn.LocalResponseNorm(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>No noticeable difference in an individual image after applying <code>LocalResponseNorm</code>:</p>
<div class="cell" data-outputid="ba1b491f-86d6-44a2-f893-86bc6f21f555" data-execution_count="40">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], lrnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-40-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The mean absolute difference is the lowest so far across all normalization layers:</p>
<div class="cell" data-outputid="2888ca38-0268-47c5-9c6e-fda73b46784a" data-execution_count="41">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>mad(xb[<span class="dv">0</span>], lrnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>TensorImageBW(0.0002)</code></pre>
</div>
</div>
<p>The distributions of mean pixels are virtually indistinguishable:</p>
<div class="cell" data-outputid="974185ea-94be-4e9b-ff84-bd1279d4a5c8" data-execution_count="42">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.mean(<span class="dv">0</span>).flatten(),</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>lrnorm(xb).mean(<span class="dv">0</span>).flatten().detach().numpy(),</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After LocalResponseNorm'</span>,</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Mean Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-42-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As are the distributions of pixel values:</p>
<div class="cell" data-outputid="9c9dce1a-9819-4604-985b-a18f3604a6dc" data-execution_count="43">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.flatten(),</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>lrnorm(xb).flatten().detach().numpy(),</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After LocalResponseNorm'</span>,</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-43-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Following suit, the mean of a single image doesn’t change much:</p>
<div class="cell" data-outputid="8fed8ef9-c5ec-42e7-9774-258125803102" data-execution_count="44">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>get_means(xb[<span class="dv">0</span>], lrnorm(xb)[<span class="dv">0</span>]) <span class="co"># mean of single image</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>(TensorImageBW(0.0668), TensorImageBW(0.0666))</code></pre>
</div>
</div>
<p>Neither does the mean of a batch:</p>
<div class="cell" data-outputid="2aa59e56-f97a-45d2-b1d3-896c0b589438" data-execution_count="46">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, lrnorm(xb)) <span class="co"># mean of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>(TensorImageBW(0.0037), TensorImageBW(0.0035))</code></pre>
</div>
</div>
<p>Nor does the standard deviation of a batch:</p>
<div class="cell" data-outputid="efb580c6-87c9-495e-ef1a-280c55321f3c" data-execution_count="47">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>get_stds(xb, lrnorm(xb)) <span class="co"># std of entire batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>(TensorImageBW(1.0045), TensorImageBW(1.0041))</code></pre>
</div>
</div>
<p>The mean value of the 100 batches (with <code>LocalResponseNorm</code> applied) varies significantly—from -0.005 to +0.015. The standard deviation is more tightly distributed around 1.0. Overall, the default values of <code>LocalResponseNorm</code> don’t show signs of significantly changing the distribution of values for the images across the batch.</p>
<div class="cell" data-outputid="15523366-6f6f-4bfc-f3dd-2e70fe21062f" data-execution_count="48">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>plot_means_stds(lrnorm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-47-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>What if I change some the default values? I’ll vary the values of the following parameters until I see a significant difference either in individual images or across batches: <code>alpha</code>, <code>beta</code> and <code>k</code>.</p>
<p>Changing the <code>alpha</code> parameter significantly changes the individual image as well as the mean and standard deviation across the batch:</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>lrnorm <span class="op">=</span> nn.LocalResponseNorm(size<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="9eb07eee-e031-4dce-b624-23baca203f6a" data-execution_count="51">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, lrnorm(xb)), <span class="op">\</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>get_stds(xb, lrnorm(xb))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>((TensorImageBW(0.0037), TensorImageBW(-0.2167)),
 (TensorImageBW(1.0045), TensorImageBW(0.3405)))</code></pre>
</div>
</div>
<div class="cell" data-outputid="766a47f6-ee25-4cb5-c755-2cdfa722fe65" data-execution_count="52">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], lrnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-50-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Changing the <code>beta</code> parameter doesn’t affect the images as much:</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>lrnorm <span class="op">=</span> nn.LocalResponseNorm(size<span class="op">=</span><span class="dv">1</span>, beta<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="ed0403b8-4ed5-4747-9a55-255e5a9a34e3" data-execution_count="54">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, lrnorm(xb)), <span class="op">\</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>get_stds(xb, lrnorm(xb))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>((TensorImageBW(0.0037), TensorImageBW(0.0015)),
 (TensorImageBW(1.0045), TensorImageBW(0.9985)))</code></pre>
</div>
</div>
<div class="cell" data-outputid="83b61385-9ce4-44ff-a3f6-040302598793" data-execution_count="55">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], lrnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-53-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Changing the <code>k</code> parameter doesn’t change individual images much but it does affect the batch statistics:</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>lrnorm <span class="op">=</span> nn.LocalResponseNorm(size<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="33b69215-0fed-4f62-fa6a-e83c5d5c082b" data-execution_count="57">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, lrnorm(xb)), <span class="op">\</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>get_stds(xb, lrnorm(xb))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>((TensorImageBW(0.0037), TensorImageBW(0.0007)),
 (TensorImageBW(1.0045), TensorImageBW(0.1786)))</code></pre>
</div>
</div>
<div class="cell" data-outputid="8ba6dbfd-4f44-42d2-c25d-c5a9b596af9b" data-execution_count="58">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], lrnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-56-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="layernorm" class="level3">
<h3 class="anchored" data-anchor-id="layernorm"><code>LayerNorm</code></h3>
<p><code>LayerNorm</code> is different from the other normalization layers in the sense that you can explicitly determine which dimensions the normalization occurs across.</p>
<p>I’ll try different sets of dimensions and plot the images and calculate image and batch statistics.</p>
<p>Setting the <code>normalized_shape</code> parameter to the last dimension (<code>[28]</code>) affects the individual image as well as the batch mean:</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>lnorm <span class="op">=</span> nn.LayerNorm(normalized_shape<span class="op">=</span>[<span class="dv">28</span>]) <span class="co"># last dimension</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="9de847a8-ed73-43d4-a56f-edea050d4963" data-execution_count="61">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, lnorm(xb)), <span class="op">\</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>get_stds(xb, lnorm(xb))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>((TensorImageBW(0.0037), TensorImageBW(5.7020e-10, grad_fn=&lt;AliasBackward0&gt;)),
 (TensorImageBW(1.0045), TensorImageBW(0.8411, grad_fn=&lt;AliasBackward0&gt;)))</code></pre>
</div>
</div>
<div class="cell" data-outputid="efe42cd1-28a8-48f4-f28c-51840b3bc0de" data-execution_count="62">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], lnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-59-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Setting <code>normalized_shape</code> to the last two dimensions (<code>[28, 28]</code>) significantly affects the batch mean (it goes to <code>0</code>) and sets the batch standard deviation to <code>1</code>. The individual image does not seem affected:</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>lnorm <span class="op">=</span> nn.LayerNorm(normalized_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]) <span class="co"># last two dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="7373c89d-0ece-4ee4-9bc5-4beaa31f4d23" data-execution_count="64">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, lnorm(xb)), <span class="op">\</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>get_stds(xb, lnorm(xb))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>((TensorImageBW(0.0037), TensorImageBW(9.5033e-10, grad_fn=&lt;AliasBackward0&gt;)),
 (TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=&lt;AliasBackward0&gt;)))</code></pre>
</div>
</div>
<div class="cell" data-outputid="2885fb6b-4ce4-41c2-c17d-fe05cc47b156" data-execution_count="65">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], lnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-62-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Setting <code>normalized_shape</code> to the last three dimensions has a similar effect:</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>lnorm <span class="op">=</span> nn.LayerNorm(normalized_shape<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>]) <span class="co"># last three dimensions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="1bced165-1fba-42f7-d6ec-851a7d62e61c" data-execution_count="70">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>get_means(xb, lnorm(xb)), <span class="op">\</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>get_stds(xb, lnorm(xb))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>((TensorImageBW(0.0037), TensorImageBW(9.5033e-10, grad_fn=&lt;AliasBackward0&gt;)),
 (TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=&lt;AliasBackward0&gt;)))</code></pre>
</div>
</div>
<div class="cell" data-outputid="d7cdd86e-c70f-4b26-ccc0-9fa161649e01" data-execution_count="71">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>plot_ims(xb[<span class="dv">0</span>], lnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-65-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>I’ll run through my helper functions for this scenario as it seems most similar to <code>BatchNorm2d</code>, <code>InstanceNorm2d</code> and <code>GroupNorm</code>.</p>
<p>The mean absolute difference between the image before/after normalization is similar to <code>BatchNorm2d</code>:</p>
<div class="cell" data-outputid="88479ab5-9b5b-4d4a-d69d-ef543128547c" data-execution_count="72">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>mad(xb[<span class="dv">0</span>], lnorm(xb)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>TensorImageBW(0.0668, grad_fn=&lt;AliasBackward0&gt;)</code></pre>
</div>
</div>
<p>There is some visually distinguishable difference between the mean pixel value in each image before/after <code>LayerNorm</code> is applied:</p>
<div class="cell" data-outputid="4ccbb982-2446-42fe-df3b-f3fbf460c54f" data-execution_count="73">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.mean(<span class="dv">0</span>).flatten(),</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>lnorm(xb).mean(<span class="dv">0</span>).flatten().detach().numpy(),</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After LayerNorm'</span>,</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Mean Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-67-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Same goes for the distributions of pixel values:</p>
<div class="cell" data-outputid="db776dbf-ce44-47e2-fa84-108c026c9ed1" data-execution_count="74">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>plot_hists(</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>    a<span class="op">=</span>xb.flatten(),</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>lnorm(xb).flatten().detach().numpy(),</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>    label_a<span class="op">=</span><span class="st">'Original'</span>,</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>    label_b<span class="op">=</span><span class="st">'After LayerNorm'</span>,</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">'Pixel Value in Image'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-68-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Across the 100 batches, the means are centered around 1e-9 (like <code>GroupNorm</code>) and the standard deviations are similarly distributed around 1.</p>
<div class="cell" data-outputid="23694a08-3104-4bed-8a5f-cbb902c5480c" data-execution_count="75">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>plot_means_stds(lnorm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-69-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="training-models-using-different-normalization-layers" class="level2">
<h2 class="anchored" data-anchor-id="training-models-using-different-normalization-layers">Training Models Using Different Normalization Layers</h2>
<p>Now that I’m a bit more familiar with the behavior of the activation normalization layers on the MNIST dataset, I’ll train models using them and compare between them their final validation accuracy as well as their <code>activation_stats</code>.</p>
<p>I’m modifying <code>simple_cnn</code> so that the <code>conv</code> function receives <code>output_shape</code> since <code>nn.LayerNorm</code> uses that as the <code>normalized_shape</code> parameter. The <code>output_shape</code> is based on 28x28 input images to the model.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_cnn():</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> sequential(</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>      conv(<span class="dv">1</span>, <span class="dv">8</span>, output_shape<span class="op">=</span>[<span class="dv">14</span>,<span class="dv">14</span>], ks<span class="op">=</span><span class="dv">5</span>),         <span class="co"># 14x14</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>      conv(<span class="dv">8</span>, <span class="dv">16</span>, output_shape<span class="op">=</span>[<span class="dv">7</span>,<span class="dv">7</span>]),              <span class="co"># 7x7</span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>      conv(<span class="dv">16</span>, <span class="dv">32</span>, output_shape<span class="op">=</span>[<span class="dv">4</span>,<span class="dv">4</span>]),             <span class="co"># 4x4</span></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>      conv(<span class="dv">32</span>, <span class="dv">64</span>, output_shape<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">2</span>]),             <span class="co"># 2x2</span></span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>      conv(<span class="dv">64</span>, <span class="dv">10</span>, output_shape<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">1</span>], act<span class="op">=</span><span class="va">False</span>),  <span class="co"># 1x1</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>      Flatten()</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(epochs<span class="op">=</span><span class="dv">5</span>, lr<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>  learn <span class="op">=</span> Learner(dls, simple_cnn(), loss_func<span class="op">=</span>F.cross_entropy, metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ActivationStats(with_hist<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> learn.no_logging(), learn.no_bar(): learn.fit_one_cycle(epochs, lr)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> learn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="5013a563-2ff4-4c6b-ff39-81fe333adfd8" data-execution_count="6">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(<span class="dv">512</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>dls.show_batch(max_n<span class="op">=</span><span class="dv">9</span>, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-72-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>I’ll define a list of all the activation normalization layers I’ll use and a <code>data</code> object that will store the training results. Note that I am not using <code>nn.InstanceNorm2d</code> since I get the following error when attempting to train with it:</p>
<pre><code>ValueError: Expected more than 1 spatial element when training, got input size torch.Size([512, 10, 1, 1])</code></pre>
<p><a href="https://discuss.pytorch.org/t/instance-norm-valueerror-expected-more-than-1-spatial-element-when-training-got-input-size-torch-size-128-512-1-1/159620/2">This PyTorch forum’s post</a> explains that for <code>InstanceNorm2d</code>, in the input <span class="math inline">\((N,C,H,W)\)</span>, <span class="math inline">\(H\)</span> and <span class="math inline">\(W\)</span> must be greater than 1. In my model the output of the final convolution will be 1x1 (for each of the 10 numbers). I don’t want to change the architecture for <code>InstanceNorm2d</code> since I won’t be able to compare apples-to-apples with the other normalization layers, so I’m deciding not to use it.</p>
<p>For <code>nn.GroupNorm</code>, I’m using <code>num_groups</code> of <code>2</code> since that was giving me the highest accuracy when compared to lower or higher values (after quickly training for 1 epoch). I’m using a <code>size</code> of <code>2</code> for <code>nn.LocalResponseNorm</code> since it gave higher validation accuracy than lower or higher values (after training for 1 epoch).</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>norms <span class="op">=</span> [</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>    nn.BatchNorm2d,</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>    nn.GroupNorm,</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    nn.LocalResponseNorm,</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    nn.LayerNorm</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'BatchNorm2d'</span>: <span class="va">None</span>,</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GroupNorm'</span>: <span class="va">None</span>,</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'LocalResponseNorm'</span>: <span class="va">None</span>,</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'LayerNorm'</span>: <span class="va">None</span></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For each normalization layer, I’ll train 10 models for 5 epochs each, storing the <code>activation_stats.layer_stats</code>, <code>activation_stats.hist</code> and <code>learn.recorder.values[-1][-1]</code> (final accuracy).</p>
<div class="cell" data-outputid="04df5bdd-722c-4f4d-fa2c-89dc191228a1" data-execution_count="13">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> norm <span class="kw">in</span> norms:</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>  layer_stats <span class="op">=</span> L()</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>  hists <span class="op">=</span> L()</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>  accs <span class="op">=</span> L()</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> conv(ni, nf, output_shape, ks<span class="op">=</span><span class="dv">3</span>, act<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [nn.Conv2d(ni, nf, stride<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span>ks, padding<span class="op">=</span>ks<span class="op">//</span><span class="dv">2</span>)]</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm.<span class="va">__name__</span> <span class="kw">in</span> [<span class="st">'BatchNorm2d'</span>, <span class="st">'InstanceNorm2d'</span>]: normlayer <span class="op">=</span> norm(nf)</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm.<span class="va">__name__</span> <span class="op">==</span> <span class="st">'GroupNorm'</span>: normlayer <span class="op">=</span> norm(<span class="dv">1</span>, nf)</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm.<span class="va">__name__</span> <span class="op">==</span> <span class="st">'LocalResponseNorm'</span>: normlayer <span class="op">=</span> norm(<span class="dv">2</span>)</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm.<span class="va">__name__</span> <span class="op">==</span> <span class="st">'LayerNorm'</span>: normlayer <span class="op">=</span> nn.LayerNorm([nf] <span class="op">+</span> output_shape)</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>    layers.append(normlayer)</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> act: layers.append(nn.ReLU())</span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a>    learn <span class="op">=</span> fit()</span>
<span id="cb99-20"><a href="#cb99-20" aria-hidden="true" tabindex="-1"></a>    layer_stats.append(learn.activation_stats.layer_stats(<span class="op">-</span><span class="dv">2</span>))</span>
<span id="cb99-21"><a href="#cb99-21" aria-hidden="true" tabindex="-1"></a>    hists.append(learn.activation_stats.hist(<span class="op">-</span><span class="dv">4</span>))</span>
<span id="cb99-22"><a href="#cb99-22" aria-hidden="true" tabindex="-1"></a>    accs.append(learn.recorder.values[<span class="op">-</span><span class="dv">1</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb99-23"><a href="#cb99-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-24"><a href="#cb99-24" aria-hidden="true" tabindex="-1"></a>  data[norm.<span class="va">__name__</span>] <span class="op">=</span> {<span class="st">'layer_stats'</span>: layer_stats, <span class="st">'hists'</span>: hists, <span class="st">'accs'</span>: accs}</span>
<span id="cb99-25"><a href="#cb99-25" aria-hidden="true" tabindex="-1"></a>  save_pickle(<span class="st">'/content/data.pkl'</span>, data)</span>
<span id="cb99-26"><a href="#cb99-26" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>norm<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss"> ---- done.'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this
  warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>BatchNorm2d ---- done.
GroupNorm ---- done.
LocalResponseNorm ---- done.
LayerNorm ---- done.</code></pre>
</div>
</div>
<section id="analyzing-training-results" class="level3">
<h3 class="anchored" data-anchor-id="analyzing-training-results">Analyzing Training Results</h3>
<section id="final-validation-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="final-validation-accuracy">Final Validation Accuracy</h4>
<p>Time to look at the results! I’ll start by comparing the median, mean, std, min and max final validation accuracy between the different activation normalization layers.</p>
<p>Across 10 training runs (for 5 epochs each) <code>BatchNorm2d</code> had the highest median (0.9924) and mean (0.99227) final validation accuracy as well as the lowest standard deviation (0.000495). <code>LocalResponseNorm</code> had one training run with a final validation accuracy of 0.1135!</p>
<div class="cell" data-outputid="23c5f41c-0494-4767-d672-c14e83c8314d" data-execution_count="37">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>accs <span class="op">=</span> pd.DataFrame({key: data[key][<span class="st">'accs'</span>] <span class="cf">for</span> key <span class="kw">in</span> data})</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>accs.describe().loc[[<span class="st">'mean'</span>, <span class="st">'50%'</span>, <span class="st">'std'</span>, <span class="st">'min'</span>, <span class="st">'max'</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">


  <div id="df-75c9aeb9-f566-46db-a56d-f64c6623caa0" class="colab-df-container">
    <div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>BatchNorm2d</th>
      <th>GroupNorm</th>
      <th>LocalResponseNorm</th>
      <th>LayerNorm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>0.992270</td>
      <td>0.990560</td>
      <td>0.883270</td>
      <td>0.990400</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.992400</td>
      <td>0.990700</td>
      <td>0.971700</td>
      <td>0.990500</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.000495</td>
      <td>0.000799</td>
      <td>0.270729</td>
      <td>0.000523</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.991100</td>
      <td>0.989100</td>
      <td>0.113500</td>
      <td>0.989400</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.992800</td>
      <td>0.991500</td>
      <td>0.979300</td>
      <td>0.991000</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-75c9aeb9-f566-46db-a56d-f64c6623caa0')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-75c9aeb9-f566-46db-a56d-f64c6623caa0 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-75c9aeb9-f566-46db-a56d-f64c6623caa0');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-25a27059-4300-4fc5-8e21-8bab2a2e77d6">
  <button class="colab-df-quickchart" onclick="quickchart('df-25a27059-4300-4fc5-8e21-8bab2a2e77d6')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-25a27059-4300-4fc5-8e21-8bab2a2e77d6 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div>
</div>
</section>
<section id="activation-stats" class="level4">
<h4 class="anchored" data-anchor-id="activation-stats">Activation Stats</h4>
<p>Next, I’ll look at the <code>activation_stats</code> across the training runs for each of the four normalization layers.</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_stats_avg(layer_stats_list, titles, super_title, labels):</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>    fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(super_title, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [<span class="st">'red'</span>, <span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'orange'</span>]</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, title <span class="kw">in</span> <span class="bu">enumerate</span>(titles):</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, layer_stats <span class="kw">in</span> <span class="bu">enumerate</span>(layer_stats_list):</span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> layer_stats.itemgot(i)</span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(y[<span class="dv">0</span>]))</span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a>            y_median <span class="op">=</span> np.median(np.stack(y), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a>            axs[i].plot(x, y_median, color<span class="op">=</span>colors[j], alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="dv">1</span>, label<span class="op">=</span>labels[j])</span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a>        axs[i].set_title(<span class="ss">f"Median </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss"> Activations Across 10 Trainings"</span>)</span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a>        axs[i].set_xlabel(<span class="st">"Batch"</span>)</span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a>        axs[i].set_ylabel(<span class="ss">f"</span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb103-18"><a href="#cb103-18" aria-hidden="true" tabindex="-1"></a>        axs[i].legend()</span>
<span id="cb103-19"><a href="#cb103-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-20"><a href="#cb103-20" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb103-21"><a href="#cb103-21" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>LocalResponseNorm</code> (green line) has the most unstable trainings—the average mean activations for it are an order of magnitude more negative than the other three normalization layers; the mean standard deviations are an order of magnitude more positive; it has the highest mean %-near-zero activations, with almost 100% activations near zero on average by the end of the training run.</p>
<div class="cell" data-outputid="5063d3e4-04f1-43a7-9ac7-d8324fbb96f0" data-execution_count="55">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>layer_stats_list <span class="op">=</span> [data[key][<span class="st">'layer_stats'</span>] <span class="cf">for</span> key <span class="kw">in</span> data.keys()]</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> <span class="bu">list</span>(data.keys())</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>plot_stats_avg(layer_stats_list, [<span class="st">'Mean'</span>, <span class="st">'Std'</span>, <span class="st">'%-near-zero'</span>], <span class="st">'Activation Stats'</span>, labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-78-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>After excluding <code>LocalResponseNorm</code>, I can see the other three layers’ training results more clearly.</p>
<p>The median of mean activations for <code>BatchNorm2d</code> (red line) across 10 trainings are the closest to 0 throughout the training when compared to <code>GroupNorm</code> (blue line) or <code>LayerNorm</code> (green line). <code>BatchNorm2d</code> has the highest median standard deviations. <code>BatchNorm2d</code> has the lowest %-near-zero activations—plateauing at around 65% by the end of the training. After looking at these charts, I would say that <code>BatchNorm2d</code> has the most stable trainings.</p>
<div class="cell" data-outputid="73b3465d-7af8-4309-d0a6-c793f037be5e" data-execution_count="56">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>layer_stats_list <span class="op">=</span> [data[key][<span class="st">'layer_stats'</span>] <span class="cf">for</span> key <span class="kw">in</span> data.keys() <span class="cf">if</span> key <span class="op">!=</span> <span class="st">'LocalResponseNorm'</span>]</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>plot_stats_avg(layer_stats_list, [<span class="st">'Mean'</span>, <span class="st">'Std'</span>, <span class="st">'%-near-zero'</span>], <span class="st">'Activation Stats'</span>, [<span class="st">'BatchNorm2d'</span>, <span class="st">'GroupNorm'</span>, <span class="st">'LayerNorm'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-79-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="activation-histograms" class="level4">
<h4 class="anchored" data-anchor-id="activation-histograms">Activation Histograms</h4>
<p>Finally, I’ll look at the mean and median value of the histograms of activations across the 10 training runs for each normalization layer:</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_hist_avg(hist, super_title):</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>  fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>  fig.suptitle(super_title, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>  h_mean <span class="op">=</span> torch.stack(<span class="bu">list</span>(hist)).mean(<span class="dv">0</span>)</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>  h_median <span class="op">=</span> torch.stack(<span class="bu">list</span>(hist)).median(<span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>].imshow(h_mean, origin<span class="op">=</span><span class="st">'lower'</span>)<span class="op">;</span></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>].set_title(<span class="ss">f"Mean Activations Across 10 Trainings"</span>)</span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>].axis(<span class="st">'off'</span>)<span class="op">;</span></span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>].imshow(h_median, origin<span class="op">=</span><span class="st">'lower'</span>)<span class="op">;</span></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>].set_title(<span class="ss">f"Median Activations Across 10 Trainings"</span>)</span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>].axis(<span class="st">'off'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>BatchNorm2d</code> results in mean and median histogram values that look similar to the fastai textbook example of a stable training. The number of zero-activations decrease smoothly over the first epoch or so.</p>
<div class="cell" data-outputid="4c32276b-fa30-411d-f418-08bd8e3eb73c" data-execution_count="69">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>plot_hist_avg(data[<span class="st">'BatchNorm2d'</span>][<span class="st">'hists'</span>], <span class="st">"BatchNorm2d"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-81-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The <code>GroupNorm</code> activations are a bit more turbulent, as you can see slight striations of alternating white and grey that indicate small “collapses” of activations in the first epoch before the training stabilizes. I would consider this less stable than <code>BatchNorm2d</code>.</p>
<div class="cell" data-outputid="db6c9237-6199-4b40-dbf2-b2de1d3a1493" data-execution_count="72">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>plot_hist_avg(data[<span class="st">'GroupNorm'</span>][<span class="st">'hists'</span>], <span class="st">"GroupNorm"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-82-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><code>LocalResponseNorm</code> continues to be the worst normalization layer and the most turbulent as the training shows a clear sign of collapsing activations in the first epoch and continues to show a high number of zero-activations across the training run.</p>
<div class="cell" data-outputid="9271cf44-905e-4189-ea8c-4118bd7ea375" data-execution_count="73">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>plot_hist_avg(data[<span class="st">'LocalResponseNorm'</span>][<span class="st">'hists'</span>], <span class="st">"LocalResponseNorm"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-83-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Using <code>LayerNorm</code> results in a more stable training; it looks similar to the <code>BatchNorm2d</code> histograms.</p>
<div class="cell" data-outputid="dd110fb4-b700-477c-d0d7-15b58a343d4a" data-execution_count="74">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>plot_hist_avg(data[<span class="st">'LayerNorm'</span>][<span class="st">'hists'</span>], <span class="st">"LayerNorm"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-84-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>This exercise continued to illustrate just how rich the world of deep learning is! I feel like I just scratched the surface of understanding how different activation normalization layers behave, and even that required a considerable amount of time and effort.</p>
<p>Overall from my relatively simple experiments <code>BatchNorm2d</code> seems like the best layer to use for this data. Of course there are many hyperparameters that I didn’t exhaustively tune (other than quickly training 1-epoch models to see how changing 1 hyperparameter like <code>num_groups</code> for <code>GroupNorm</code> affected the validation accuracy) and so these results might not be optimal.</p>
<p>However, there’s something to be said about being able to just use <code>BatchNorm2d</code> without worrying about hyperparameters and getting the best results.</p>
<p>I hope you enjoyed this blog post! Follow me on Twitter <a href="https://twitter.com/vishal_learner"><span class="citation" data-cites="vishal_learner">@vishal_learner</span></a>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>