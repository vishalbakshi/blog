<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-02-19">
<meta name="description" content="A summary of research on the phi-1, phi-1.5 and phi-2 from the Textbook Are All You Need I and II series of publications by Microsoft Research.">

<title>vishal bakshi - Paper Summary: Textbooks are All You Need I &amp; II</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#main-takeaways" id="toc-main-takeaways" class="nav-link" data-scroll-target="#main-takeaways">Main Takeaways</a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a>
  <ul class="collapse">
  <li><a href="#phi-1" id="toc-phi-1" class="nav-link" data-scroll-target="#phi-1">phi-1</a></li>
  <li><a href="#phi-1.5" id="toc-phi-1.5" class="nav-link" data-scroll-target="#phi-1.5">phi-1.5</a></li>
  <li><a href="#phi-1.5-web-phi-1.5-web-only" id="toc-phi-1.5-web-phi-1.5-web-only" class="nav-link" data-scroll-target="#phi-1.5-web-phi-1.5-web-only">phi-1.5-web, phi-1.5-web-only</a></li>
  </ul></li>
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a>
  <ul class="collapse">
  <li><a href="#phi-1-1" id="toc-phi-1-1" class="nav-link" data-scroll-target="#phi-1-1">phi-1</a></li>
  <li><a href="#phi-1.5-1" id="toc-phi-1.5-1" class="nav-link" data-scroll-target="#phi-1.5-1">phi-1.5</a></li>
  </ul></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a>
  <ul class="collapse">
  <li><a href="#phi-1-2" id="toc-phi-1-2" class="nav-link" data-scroll-target="#phi-1-2">phi-1</a></li>
  <li><a href="#phi-1.5-2" id="toc-phi-1.5-2" class="nav-link" data-scroll-target="#phi-1.5-2">phi-1.5</a></li>
  </ul></li>
  <li><a href="#benchmarks" id="toc-benchmarks" class="nav-link" data-scroll-target="#benchmarks">Benchmarks</a>
  <ul class="collapse">
  <li><a href="#phi-1-3" id="toc-phi-1-3" class="nav-link" data-scroll-target="#phi-1-3">phi-1</a></li>
  </ul></li>
  <li><a href="#data-decontamination" id="toc-data-decontamination" class="nav-link" data-scroll-target="#data-decontamination">Data Decontamination</a>
  <ul class="collapse">
  <li><a href="#phi-1.5-3" id="toc-phi-1.5-3" class="nav-link" data-scroll-target="#phi-1.5-3">phi-1.5</a></li>
  </ul></li>
  <li><a href="#prompt-and-response-examples" id="toc-prompt-and-response-examples" class="nav-link" data-scroll-target="#prompt-and-response-examples">Prompt and Response Examples</a></li>
  <li><a href="#further-research" id="toc-further-research" class="nav-link" data-scroll-target="#further-research">Further Research</a></li>
  <li><a href="#phi-2" id="toc-phi-2" class="nav-link" data-scroll-target="#phi-2">phi-2</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Summary: Textbooks are All You Need I &amp; II</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper summary</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    A summary of research on the phi-1, phi-1.5 and phi-2 from the Textbook Are All You Need I and II series of publications by Microsoft Research.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 19, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this notebook I’ll provide a summary of Microsoft Research’s <a href="https://arxiv.org/pdf/2306.11644.pdf">Textbook Are All You Need</a> paper. Here’s the abstract:</p>
<blockquote class="blockquote">
<p>We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of “textbook quality” data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.</p>
</blockquote>
<p>I’ll also review the information published in <a href="https://arxiv.org/pdf/2309.05463.pdf">Textbooks Are All You Need II</a> a technical report in which they introduce phi-1.5 models trained on additional data.</p>
</section>
<section id="main-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="main-takeaways">Main Takeaways</h2>
<ul>
<li>Model improvement is obtained with <strong>data quality</strong> (instead of model size, dataset size and amount of compute).</li>
<li>High quality data means data that is <strong>diverse</strong> (wide range of concepts, skills, and scenarios; varying difficulty, complexity and style) and <strong>non-repetitive</strong>.</li>
<li>Finetuning on 180M tokens led to the largest accuracy increase including for tasks that are <strong>not</strong> featured in the finetuning dataset.</li>
<li>1.3B parameter models outperform larger models trained on larger datasets.</li>
<li>350M parameter model performs decently well.</li>
<li>Training on <strong>textbook-like data</strong> might mean model stores/accesses knowledge more efficiently than if trained on web data.</li>
</ul>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<section id="phi-1" class="level3">
<h3 class="anchored" data-anchor-id="phi-1">phi-1</h3>
<p>phi-1 uses two different datasets, one for pretraining and one for finetuning:</p>
<ul>
<li>CodeTextbook (pretraining)
<ul>
<li>The Stack+ (6B tokens): subset of The Stack and StackOverflow, filtered using a LM-based classifier</li>
<li>GPT-3.5 generated Python textbooks (&lt;1B tokens)</li>
</ul></li>
<li>CodeExercises (finetuning)
<ul>
<li>GPT-3.5 generated Python exercises and solutions (~180M tokens; function completion tasks based on natural language instructions)</li>
</ul></li>
</ul>
</section>
<section id="phi-1.5" class="level3">
<h3 class="anchored" data-anchor-id="phi-1.5">phi-1.5</h3>
<ul>
<li>CodeTextbook (7B tokens)</li>
<li>20B tokens of synthetically generated textbook-like data</li>
</ul>
<p>A couple of quotes from the paper about data:</p>
<blockquote class="blockquote">
<p>…our dataset consists almost exclusively of synthetically generated data</p>
</blockquote>
<blockquote class="blockquote">
<p>…a robust and comprehensive dataset demands more than raw computational power: it requires intricate iterations, strategic topic selection, and a deep understanding of knowledge gaps to ensure quality and diversity of the data.</p>
</blockquote>
</section>
<section id="phi-1.5-web-phi-1.5-web-only" class="level3">
<h3 class="anchored" data-anchor-id="phi-1.5-web-phi-1.5-web-only">phi-1.5-web, phi-1.5-web-only</h3>
<ul>
<li>95B tokens of filtered web data
<ul>
<li>88B from the Falcon refined web dataset.</li>
<li>7B from The Stack and Stack Overflow.</li>
</ul></li>
<li>phi-1.5-web-only trained only on filtered web data
<ul>
<li>80% NLP data sources.</li>
<li>20% code datasets.</li>
</ul></li>
<li>phi-1.5-web trained on a mix of filtered data
<ul>
<li>40%: a subset of filtered web data.</li>
<li>20%: phi-1’s code data.</li>
<li>40%: new synthetic NLP data.</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p>We speculate that the creation of synthetic datasets will become, in the near future, an important technical skill and a central topic of research in AI.</p>
</blockquote>
</section>
</section>
<section id="architecture" class="level2">
<h2 class="anchored" data-anchor-id="architecture">Architecture</h2>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Decoder-only Transformer (FlashAttention/MLP in parallel)</th>
<th style="text-align: center;">phi-1/phi-1.5</th>
<th style="text-align: center;">phi-1-small</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Parameters</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">350M</td>
</tr>
<tr class="even">
<td style="text-align: center;">Layers</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">20</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Hidden dimension</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">1024</td>
</tr>
<tr class="even">
<td style="text-align: center;">MLP inner dimension</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">4096</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Attention head count</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">16</td>
</tr>
<tr class="even">
<td style="text-align: center;">Attention head dimension</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
</tr>
</tbody>
</table>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<section id="phi-1-1" class="level3">
<h3 class="anchored" data-anchor-id="phi-1-1">phi-1</h3>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Pre-training (phi-1-base)</th>
<th style="text-align: center;">Fine-tuning (phi-1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Time</td>
<td style="text-align: center;">&lt;4 days</td>
<td style="text-align: center;">7 hours</td>
</tr>
<tr class="even">
<td style="text-align: center;">Batch size</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">256</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Max learning rate</td>
<td style="text-align: center;">1e-3</td>
<td style="text-align: center;">1e-4</td>
</tr>
<tr class="even">
<td style="text-align: center;">Warmup</td>
<td style="text-align: center;">750 steps</td>
<td style="text-align: center;">50 steps</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Weight Decay</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="even">
<td style="text-align: center;">Checkpoint</td>
<td style="text-align: center;">24k steps/8 epochs/50B tokens</td>
<td style="text-align: center;">Undisclosed</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Total steps</td>
<td style="text-align: center;">36000</td>
<td style="text-align: center;">6000</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>Both phi-1.5 and phi-1.5-web are base models pre-trained on large natural language corpora. In particular <strong>we did not perform further instruction-based finetuning to align them with human instructions</strong> (emphasis mine).</p>
</blockquote>
<p>I was really hoping for more details about their 350M model experiments (I love it when small models perform decently) but they only provided the following:</p>
<blockquote class="blockquote">
<p>…phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1</p>
</blockquote>
</section>
<section id="phi-1.5-1" class="level3">
<h3 class="anchored" data-anchor-id="phi-1.5-1">phi-1.5</h3>
<ul>
<li>Pretraining
<ul>
<li>Batch size: 2048</li>
<li>Constant learning rate: 2e-4</li>
<li>Weight decay: 0.1</li>
<li>Training tokens: 50B (80% new synthetic data, 20% phi-1 data)</li>
</ul></li>
</ul>
</section>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<section id="phi-1-2" class="level3">
<h3 class="anchored" data-anchor-id="phi-1-2">phi-1</h3>
<ul>
<li>This model is python-specific so it won’t be as performant for other programming languages.</li>
<li>Lacks domain-specific knowledge (APIs, less common packages).</li>
<li>Less robust to grammar/style variations (small changes in natural language instructions can affect performance).</li>
<li>Unclear what type of scale in model or dataset size will overcome these limitations.</li>
<li>GPT-4 should be used to generate synthetic data.</li>
</ul>
</section>
<section id="phi-1.5-2" class="level3">
<h3 class="anchored" data-anchor-id="phi-1.5-2">phi-1.5</h3>
<ul>
<li>Not immune to generating toxic content.</li>
<li>Makes some intricate mistakes when explaining code.</li>
</ul>
</section>
</section>
<section id="benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="benchmarks">Benchmarks</h2>
<section id="phi-1-3" class="level3">
<h3 class="anchored" data-anchor-id="phi-1-3">phi-1</h3>
<ul>
<li>HumanEval
<ul>
<li>A dataset of 164 hand-written coding problems.</li>
<li>Each problem includes a function signature, docstring, body and several unit tests (7.7 avg tests per problem)</li>
</ul></li>
<li>MBPP
<ul>
<li>1000 crowd-sources Python programming problems.</li>
<li>Designed for entry-level programmers.</li>
<li>Each problem has a task description, code solution and 3 automated test cases.</li>
</ul></li>
<li><span class="math inline">\(pass@k\)</span> metric
<ul>
<li><span class="math inline">\(k\)</span> generated code samples per problem.</li>
<li>problem is “solved” if any sample passes the unit tests.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig-2-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 2.1: Pass@1 accuracy on Human Eval for models and training datasets of various sizes.</figcaption><p></p>
</figure>
</div>
<p>In the figure above (Figure 2.1 in the paper) note the strong performance of the 350M parameter model trained on 26B tokens for 135 GPU hours. I would love to know more about that checkpoint.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Table2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Table 2: HumanEval and 50 unconventional coding problem scores.</figcaption><p></p>
</figure>
</div>
<p>In the table above (Table 2 from the paper) the “Score” column is graded by GPT-4 on a scale of 0 to 10 while the HumanEval column is calculated with pass@1 accuracy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="table1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Table 1: HumanEval and MBPP Pass@1 accuracy for various model and dataset sizes</figcaption><p></p>
</figure>
</div>
<p>I found Table 1 in the paper to be the most impressive framing of phi-1’s performance. It beats models that are hundreds of times larger (such as PaLM-Coder) trained on datasets thousands of times as large (such as StarCoder). As a reminder, MBPP consists of 1000 crowdsourced entry-level programming questions.</p>
</section>
</section>
<section id="data-decontamination" class="level2">
<h2 class="anchored" data-anchor-id="data-decontamination">Data Decontamination</h2>
<p>A standard contamination study will look for n-gram overlaps between the training and test sets to understand how “contaminated” the training set is with information from the test set. They only found four such cases in the paper, including one where the n-gram was the same but for a different context.</p>
<p>The authors used a “strong form” of data decontamination: embedding and syntax-based similarity Embedding similarity determines semantic similarity, while AST-based similarity determines how similar the underlying operations of the code are between two dataset items. After removing contaminated dataset items, the authors trained phi-1 on this “pruned dataset” and it performed better than StarCoder-Prompted (15.5B) for all AST-based match rate thresholds and similarity categories (similar, non-similar, total) except for one (see table 3 below).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="table3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Table 3 showing results of phi-1 (better performing overall) and StarCoder-Prompted on different training datasets with varying amounts of similar test data removed</figcaption><p></p>
</figure>
</div>
<section id="phi-1.5-3" class="level3">
<h3 class="anchored" data-anchor-id="phi-1.5-3">phi-1.5</h3>
<ul>
<li>The authors used LM-Eval Harness on 5 common sense benchmarks</li>
<li>5 standard language understanding tasks
<ul>
<li>Zero-shot accuracy LM-Eval Harness on PIQA, HellaSwag, and OpenbookQA.</li>
<li>2-shot accuracy on MMLU.</li>
<li>Exact match score on SQUAD.</li>
</ul></li>
<li>3 reasoning ability benchmarks
<ul>
<li>Zero-shot pass@1 accuracy on GSM8K for math and HumanEval/MBPP for entry-level Python coding.</li>
</ul></li>
<li>1 benchmark for toxicity (ToxiGen)
<ul>
<li>86 prompts, 34 evaluated as “fail” (bad), 47 as “pass” (good) and 4 as “did not understand”.</li>
</ul></li>
</ul>
<p>Here are the results for phi-1.5 on these benchmarks, compared to other (larger) models:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="phi-1-5-benchmarks-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Standard language understanding tasks</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="phi-1-5-benchmarks-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Standard language understanding tasks</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="phi-1-5-benchmarks-3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Math and coding tasks</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="phi-1-5-benchmarks-4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Toxicity benchmark</figcaption><p></p>
</figure>
</div>
<p>As I noted in the screenshot from my slides—I was curious to see phi-1.5-web-only results for toxicity. I would assume it would score worse than the other models.</p>
</section>
</section>
<section id="prompt-and-response-examples" class="level2">
<h2 class="anchored" data-anchor-id="prompt-and-response-examples">Prompt and Response Examples</h2>
<p>I’ve highlighted a few examples from the paper that show how the phi models behave. First, a comparison between phi-1, phi-1-base and phi-1-small responses to the same prompt asking them to code a problem involving multiple mathematical relationships. phi-1 successfully provides the right answer. phi-1-base (pretrained only) returns relevant code names and values but with absolutely no structure for the operations involved. phi-1-small gets about 80% of the solution right, with a couple of errors within the correct structure of the solution.</p>
<p><img src="ex1.png" class="img-fluid"></p>
<p>In the response below, phi-1.5 is able to take into consideration unconventional information (raining in the middle of July) and incorporate it into its story-telling response.</p>
<p><img src="ex2.png" class="img-fluid"></p>
<p>In the example below, phi-1.5 corretly generates a respone that aligns with the prompt but then continue on to generate unwanted text in the format of Exercise/Answer. I wonder if it’s following its textbook-like training data’s format.</p>
<p><img src="ex3.png" class="img-fluid"></p>
<p>Lastly, I tried phi-1.5’s code to check latency and found one error in its syntax (the use of <code>decode</code> instead of <code>encode</code>). Otherwise, the syntax matched the documentation example of Python’s subprocess module.</p>
<p><img src="ex4.png" class="img-fluid"></p>
</section>
<section id="further-research" class="level2">
<h2 class="anchored" data-anchor-id="further-research">Further Research</h2>
<ul>
<li>The authors noted that developing high quality datasets is a central direction to improve NLP and related field. High quality data means data that is:
<ul>
<li>balanced and representative for model use cases.</li>
<li>diverse and non-repetitive (inject randomness and creativity into data generation process to achieve this).</li>
<li>taking into consideration ethical/social implications, accountability, transparency, and biases (in both models and data).</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p>The open-sourcing of phi-1.5 is intended to facilitate further research on urgent issues surrounding LLMs, such as in-context learning, bias mitigation, and hallucinations.</p>
</blockquote>
<blockquote class="blockquote">
<p>Our work indicates the feasibility of achieving high-level capabilities in smaller LLMs, potentially paving the way for more efficient and environmentally sustainable AI systems.</p>
</blockquote>
<blockquote class="blockquote">
<p>Future directions include expanding our synthetic dataset to cover a broader array of topics, and to fine-tune phi-1.5 for more specific tasks</p>
</blockquote>
</section>
<section id="phi-2" class="level2">
<h2 class="anchored" data-anchor-id="phi-2">phi-2</h2>
<p>Subsequent to this paper, <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">Microsoft released phi-2</a> a 2.7B parameter language model.</p>
<blockquote class="blockquote">
<p><a href="#phi-2">Phi-2</a> demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters.</p>
</blockquote>
<blockquote class="blockquote">
<p>Phi-2 is a Transformer-based model with a next-word prediction objective, trained on 1.4T tokens from multiple passes on a mixture of Synthetic and Web datasets for NLP and coding</p>
</blockquote>
<blockquote class="blockquote">
<p>The training for Phi-2 took 14 days on 96 A100 GPUs. Phi-2 is a base model that has not undergone alignment through reinforcement learning from human feedback (RLHF), nor has it been instruct fine-tuned.</p>
</blockquote>
<p>From the <a href="https://huggingface.co/microsoft/phi-2">HuggingFace model page</a>:</p>
<blockquote class="blockquote">
<p><a href="#phi-2">Phi-2</a> was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value).</p>
</blockquote>
<p>Phi-2 performs better than Phi-1.5 across 14 different benchmarks:</p>
<p><img src="phi-2-1.png" class="img-fluid"></p>
<p>Surprisingly (or perhaps not so surprisingly based on the fact that Phi-2 contains more internet data), Phi-2 performs worse than Phi-1.5 but better than Llama2-7B for toxicity:</p>
<p><img src="phi-2-2.png" class="img-fluid"></p>
<p>Phi-2 performs better than Llama-2-7B/13B and Mistral on various benchmarks and is better than Llama-2-70B for coding:</p>
<p><img src="phi-2-3.png" class="img-fluid"></p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>I’m excited to continue using Phi-2 and look forward to finetuning it later this year. I have used it to classify sentiment for the <a href="https://huggingface.co/datasets/financial_phrasebank"><code>financial_phrasebank</code></a> dataset and have gotten comparable results to larger models (I’ll post a blog post on that project once it’s done). In general, I’m always excited to see “smaller” models perform well and I hope that thoughtful dataset curation can push the parameter size down even further, hopefully to the hundreds of millions (like the 350M phi-1-small) and still get decent results.</p>
<p>I’ll be posting more paper summaries in the coming weeks. I hope you enjoyed this blog post!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>