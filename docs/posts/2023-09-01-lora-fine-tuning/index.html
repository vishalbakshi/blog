<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2023-09-01">
<meta name="description" content="In this notebook I want to compare fine-tuning a pretrained model with and without using LoRA.">

<title>vishal bakshi - Fine-tuning a Language Model Using LoRA</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  <li><a href="#plan-of-attack" id="toc-plan-of-attack" class="nav-link" data-scroll-target="#plan-of-attack">Plan of Attack</a></li>
  <li><a href="#get_trainer-helper-function" id="toc-get_trainer-helper-function" class="nav-link" data-scroll-target="#get_trainer-helper-function"><code>get_trainer</code> Helper Function</a></li>
  <li><a href="#load-the-dataset" id="toc-load-the-dataset" class="nav-link" data-scroll-target="#load-the-dataset">Load the Dataset</a></li>
  <li><a href="#full-fine-tuning-with-eleutheraipythia-70m" id="toc-full-fine-tuning-with-eleutheraipythia-70m" class="nav-link" data-scroll-target="#full-fine-tuning-with-eleutheraipythia-70m">Full Fine-Tuning with <code>EleutherAi/pythia-70m</code></a></li>
  <li><a href="#fine-tuning-eleutheraipythia-70m-with-lora" id="toc-fine-tuning-eleutheraipythia-70m-with-lora" class="nav-link" data-scroll-target="#fine-tuning-eleutheraipythia-70m-with-lora">Fine-Tuning <code>EleutherAI/pythia-70m</code> with LoRA</a></li>
  <li><a href="#generating-text-from-the-pre-trained-tinystories-model" id="toc-generating-text-from-the-pre-trained-tinystories-model" class="nav-link" data-scroll-target="#generating-text-from-the-pre-trained-tinystories-model">Generating Text from the Pre-Trained TinyStories Model</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Fine-tuning a Language Model Using LoRA</h1>
  <div class="quarto-categories">
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">python</div>
  </div>
  </div>

<div>
  <div class="description">
    In this notebook I want to compare fine-tuning a pretrained model with and without using LoRA.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 1, 2023</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this notebook I want to compare the differences between fine-tuning a pretrained model with and without using LoRA. This exercise is a fastai community study group homework assignment.</p>
<p>Here is a comparison of the full-fine-tuning (Full FT) vs.&nbsp;LoRA fine-tuning (LoRA FT) process on the <code>EleutherAI/pythia-70m</code> model using the <code>roneneldan/TinyStoriesInstruct</code> dataset (which comes from the <a href="https://arxiv.org/abs/2305.07759">TinyStories paper</a>):</p>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Training Set</th>
<th style="text-align: left;">Validation Set</th>
<th style="text-align: left;">Perplexity</th>
<th style="text-align: left;">Batch Size</th>
<th style="text-align: left;">Epochs</th>
<th style="text-align: left;">Train Steps</th>
<th style="text-align: left;">Train Time (Minutes)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Full FT</td>
<td style="text-align: left;">70.4M</td>
<td style="text-align: left;">240k</td>
<td style="text-align: left;">60k</td>
<td style="text-align: left;">8.51</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">22500</td>
<td style="text-align: left;">100</td>
</tr>
<tr class="even">
<td style="text-align: left;">LoRA FT</td>
<td style="text-align: left;">98k</td>
<td style="text-align: left;">256k</td>
<td style="text-align: left;">64k</td>
<td style="text-align: left;">12.68</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">32000</td>
<td style="text-align: left;">120</td>
</tr>
</tbody>
</table>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li>I’ll use a small subset of the <code>roneneldan/TinyStoriesInstruct</code> dataset from HuggingFace for both trainings since when I use the full dataset I’m getting CUDA out-of-memory errors.</li>
<li>I’m referencing the following to patch together the code in this notebook:
<ul>
<li>Jeremy Howard’s <a href="https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners">Getting started with NLP for absolute beginners</a> for fundamental setup of data, model, and tokenizer.</li>
<li>HuggingFace’s <a href="https://huggingface.co/docs/transformers/tasks/language_modeling">Causal language modeling tutorial</a> for updating the tokenizer with a pad token, <code>data_collator</code> and training arguments.</li>
<li><a href="https://discuss.huggingface.co/t/how-to-sample-dataset-according-to-the-index/12940/2">This forum response</a> that shows how to select a subset of a dataset with a given set of indexes.</li>
<li>The TinyStories author’s hyperparameters as listed in <a href="https://huggingface.co/roneneldan/TinyStories-33M">their 33M parameter model page</a></li>
<li>HuggingFace’s <a href="https://huggingface.co/docs/peft/conceptual_guides/lora">LoRA Conceptual Guide</a> for steps on how to implement LoRA using <code>peft</code>.</li>
<li><a href="https://www.philschmid.de/fine-tune-flan-t5-peft#3-fine-tune-t5-with-lora-and-bnb-int-8">This blog post</a> which walks through an example LoRA training.</li>
<li><a href="https://discuss.huggingface.co/t/disable-checkpointing-in-trainer/2335/5">This forum response by Sylvain Gugger</a> which says to set <code>save_strategy</code> to <code>"no"</code> to avoid the <code>Trainer</code> creating checkpoints as I was running into errors around this.</li>
</ul></li>
</ul>
</section>
<section id="plan-of-attack" class="level2">
<h2 class="anchored" data-anchor-id="plan-of-attack">Plan of Attack</h2>
<p>In my first iteration of this exercise (see below) I manually ran multiple different trainings with different models, dataset sizes and training arguments. The code was flexible and easy to update but I through that process I re-ran a lot of cells with different values and lost track a bit exactly the order of things I was running. In this second iteration, I’ll create a helper function <code>get_trainer</code> which takes various arguments (<code>model</code>, <code>bs</code>,<code>tokz</code>, <code>train_ds</code>, etc.) and returns a HuggingFace <code>Trainer</code>. This will help clear up some of the redundancy in my code and make it a bit cleaner to read.</p>
<div id="cell-5" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:51:27.111684Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:51:27.110991Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:51:58.810690Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:51:58.809683Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:51:27.111646Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># all the imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install peft accelerate evaluate <span class="op">-</span>Uqq</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoModelForCausalLM, pipeline</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model, TaskType</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> evaluate <span class="im">import</span> load</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="get_trainer-helper-function" class="level2">
<h2 class="anchored" data-anchor-id="get_trainer-helper-function"><code>get_trainer</code> Helper Function</h2>
<p>This function prepares and returns <code>Trainer</code> object for a given model, tokenizer (and tokenize function), training/validation dataset, learning rate, batch size and number of epochs:</p>
<div id="cell-8" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:52:09.000547Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:52:09.000165Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:52:09.009015Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:52:09.007853Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:52:09.000493Z&quot;}" data-trusted="true" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr, bs, epochs):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get tokenized datasets</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    train_tok_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(tok_func, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    eval_tok_ds <span class="op">=</span> eval_ds.<span class="bu">map</span>(tok_func, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sometimes for whatever reason the datasets are not the right size so checking it here</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(train_tok_ds)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># not sure what this does but I get an error that the model didn't return a loss value without it</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    data_collator <span class="op">=</span> DataCollatorForLanguageModeling(tokenizer<span class="op">=</span>tokz, mlm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define training arguments</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        output_dir<span class="op">=</span><span class="st">"outputs"</span>,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        evaluation_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>lr,</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        lr_scheduler_type <span class="op">=</span> <span class="st">"cosine"</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        weight_decay<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        per_device_train_batch_size<span class="op">=</span>bs, </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        per_device_eval_batch_size<span class="op">=</span>bs,</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        num_train_epochs<span class="op">=</span>epochs,</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        report_to<span class="op">=</span><span class="st">'none'</span>,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        logging_steps<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        save_strategy<span class="op">=</span><span class="st">"no"</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define Trainer</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> Trainer(model, training_args, train_dataset<span class="op">=</span>train_tok_ds, eval_dataset<span class="op">=</span>eval_tok_ds,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>                  tokenizer<span class="op">=</span>tokz, data_collator<span class="op">=</span>data_collator)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trainer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="load-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="load-the-dataset">Load the Dataset</h2>
<p>As recommended in the study group, I’ll use the <code>TinyStoriesInstruct</code> dataset which comes from the paper <a href="https://arxiv.org/abs/2305.07759">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a>.</p>
<div id="cell-11" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:52:09.176847Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:52:09.176468Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:53:48.573914Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:53:48.567349Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:52:09.176817Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> load_dataset(<span class="st">"roneneldan/TinyStoriesInstruct"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-12" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:54:02.525022Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:54:02.524379Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:54:02.532843Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:54:02.531905Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:54:02.524986Z&quot;}" data-trusted="true" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>ds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 21755681
    })
    validation: Dataset({
        features: ['text'],
        num_rows: 218380
    })
})</code></pre>
</div>
</div>
</section>
<section id="full-fine-tuning-with-eleutheraipythia-70m" class="level2">
<h2 class="anchored" data-anchor-id="full-fine-tuning-with-eleutheraipythia-70m">Full Fine-Tuning with <code>EleutherAi/pythia-70m</code></h2>
<p>First, I’ll fully fine-tune an existing pretrained model on a subset of the <code>TinyStoriesInstruct</code> dataset using the <code>EleutherAI/pythia-70m</code> model. I chose this model because larger models were giving me CUDA-out-of-memory errors even for small dataset and batch sizes.</p>
<div id="cell-15" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:54:06.919076Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:54:06.918374Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:54:07.678518Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:54:07.677460Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:54:06.919038Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model_nm <span class="op">=</span> <span class="st">'EleutherAI/pythia-70m'</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>tokz <span class="op">=</span> AutoTokenizer.from_pretrained(model_nm)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>tokz.add_special_tokens({<span class="st">'pad_token'</span>: <span class="st">'[PAD]'</span>})</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tok_func(x): <span class="cf">return</span> tokz(x[<span class="st">"text"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T15:51:41.193077Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T15:51:41.192703Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T15:51:44.217286Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T15:51:44.216316Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T15:51:41.193045Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_nm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-17" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T15:51:46.633971Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T15:51:46.632910Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T15:51:46.644993Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T15:51:46.643945Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T15:51:46.633930Z&quot;}" data-trusted="true" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>GPTNeoXForCausalLM(
  (gpt_neox): GPTNeoXModel(
    (embed_in): Embedding(50304, 512)
    (layers): ModuleList(
      (0-5): 6 x GPTNeoXLayer(
        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attention): GPTNeoXAttention(
          (rotary_emb): RotaryEmbedding()
          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)
          (dense): Linear(in_features=512, out_features=512, bias=True)
        )
        (mlp): GPTNeoXMLP(
          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)
          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (embed_out): Linear(in_features=512, out_features=50304, bias=False)
)</code></pre>
</div>
</div>
<div id="cell-18" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-30T17:49:23.240785Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-30T17:49:23.240314Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-30T17:49:23.250283Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-30T17:49:23.249060Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-30T17:49:23.240749Z&quot;}" data-trusted="true" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model.num_parameters()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>70426624</code></pre>
</div>
</div>
<p>I first trained the model on a very small subset (1000 rows) for both full-finetuning and LoRA to make sure it worked, then slowly increased the training and validation size until I got the CUDA out-of-memory error.</p>
<p>For small datasets, I noticed that the validation loss started increasing after 3 epochs so I’ve kept the number of epochs at 3. With larger datasets I could try to increase the number of epochs and see if it still overfits.</p>
<p>I couldn’t figure out how to implement perplexity during training. I was getting a <code>Sizes of tensors must match except in dimension 0.</code> error when passing any function to <code>compute_metrics</code> so I calculate perplexity at the end of training instead.</p>
<p>When I tried to train the model with 240k, 220k or 200k training samples, I got the following error after 1.60, 1.75 and 1.92 epochs respectively:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="pp">RuntimeError</span>: [enforce fail at inline_container.cc:<span class="dv">471</span>] . PytorchStreamWriter failed writing <span class="bu">file</span> data<span class="op">/</span><span class="dv">9</span>: <span class="bu">file</span> write failed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I set the <code>save_strategy</code> argument in the training arguments dictionary to <code>"no"</code> and this resolved this error. However, in the future, if I wanted checkpoints during my training I would have to figure out how to resolve this error differently.</p>
<div id="cell-20" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-30T19:31:12.085700Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-30T19:31:12.085305Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-30T19:31:12.139259Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-30T19:31:12.138237Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-30T19:31:12.085669Z&quot;}" data-trusted="true" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> ds[<span class="st">'train'</span>].select(<span class="bu">range</span>(<span class="dv">240000</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>eval_ds <span class="op">=</span> ds[<span class="st">'validation'</span>].select(<span class="bu">range</span>(<span class="dv">60000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-21" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-30T19:31:13.644970Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-30T19:31:13.644617Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-30T19:31:45.055085Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-30T19:31:45.054133Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-30T19:31:13.644942Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr<span class="op">=</span><span class="fl">5e-4</span>, bs<span class="op">=</span><span class="dv">16</span>, epochs<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-22" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-30T19:31:48.449086Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-30T19:31:48.448728Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-30T21:12:05.403825Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-30T21:12:05.402409Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-30T19:31:48.449057Z&quot;}" data-trusted="true" data-execution_count="16">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>

    <div>
      
      <progress value="22500" max="22500" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [22500/22500 1:40:16, Epoch 3/3]
    </div>
    
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.385700</td>
<td>2.407521</td>
</tr>
<tr class="even">
<td>2</td>
<td>2.098300</td>
<td>2.192903</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.841100</td>
<td>2.141196</td>
</tr>
</tbody>
</table>
<p>
</p></div>
</div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>TrainOutput(global_step=22500, training_loss=2.1849648211161297, metrics={'train_runtime': 6016.472, 'train_samples_per_second': 119.671, 'train_steps_per_second': 3.74, 'total_flos': 1.64194783592448e+16, 'train_loss': 2.1849648211161297, 'epoch': 3.0})</code></pre>
</div>
</div>
<div id="cell-23" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-30T21:12:26.775479Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-30T21:12:26.775029Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-30T21:15:52.799689Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-30T21:15:52.798434Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-30T21:12:26.775443Z&quot;}" data-trusted="true" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>eval_results <span class="op">=</span> trainer.evaluate()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perplexity: </span><span class="sc">{</span>math<span class="sc">.</span>exp(eval_results[<span class="st">'eval_loss'</span>])<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="1875" max="1875" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [1875/1875 03:25]
    </div>
    
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Perplexity: 8.51</code></pre>
</div>
</div>
<p>I’ll generate some text from the pretrained model and fully fine-tuned model to see how they compare:</p>
<div id="cell-25" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-30T21:15:58.470606Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-30T21:15:58.469987Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-30T21:16:04.436121Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-30T21:16:04.434259Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-30T21:15:58.470566Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Once upon a time,"</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>model_nm, tokenizer<span class="op">=</span>tokz)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>generator(prompt, max_length <span class="op">=</span> <span class="dv">100</span>, repetition_penalty<span class="op">=</span><span class="fl">1.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Generated text:</p>
<blockquote class="blockquote">
<p>‘Once upon a time, thefirst two are not in agreement. The second is to be expected; and it would have been an easy task for them if they had known that he was going on their way from home as soon after leaving his house at night or when there were no other guests than himself who wanted him back with all of her belongings before returning into town again by midnight (and then later). But this one has never seen such things since I've lived here.”’</p>
</blockquote>
<div id="cell-27" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-30T21:16:07.850056Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-30T21:16:07.849553Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-30T21:16:11.365280Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-30T21:16:11.364080Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-30T21:16:07.850011Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>trainer.model.to(<span class="st">'cpu'</span>), tokenizer<span class="op">=</span>tokz)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>generator(prompt, max_length <span class="op">=</span> <span class="dv">100</span>, repetition_penalty<span class="op">=</span><span class="fl">1.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Generated text:</p>
<blockquote class="blockquote">
<p>‘Once upon a time, there was an old man. He had a big mustache and he loved to wear it every day. One morning when the sun came out, his eyes lit up with joy! 0He wanted to go outside but couldn't find anything else. So he decided to take off his hat and coat so that no one could see him. The old man smiled at Jimmy's face and said “I'm glad you like it”. Jimmy was happy again and thanked the old man’</p>
</blockquote>
<p>The pre-trained model as is does not generate text that resembles a story whatsoever. The fully fine-tuned model’s generated text is somewhat coherent and it resembles a story although elements of it still don’t make sense.</p>
</section>
<section id="fine-tuning-eleutheraipythia-70m-with-lora" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-eleutheraipythia-70m-with-lora">Fine-Tuning <code>EleutherAI/pythia-70m</code> with LoRA</h2>
<p>Since a LoRA model has less trainable parameters, I can increase the dataset size for the training. I’ll also see if I can train for more epochs without overfitting since I’m using more data.</p>
<div id="cell-32" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:55:04.251289Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:55:04.250930Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:55:04.306144Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:55:04.305201Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:55:04.251255Z&quot;}" data-trusted="true" data-execution_count="10">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> ds[<span class="st">'train'</span>].select(<span class="bu">range</span>(<span class="dv">256000</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>eval_ds <span class="op">=</span> ds[<span class="st">'validation'</span>].select(<span class="bu">range</span>(<span class="dv">64000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-33" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:54:14.300684Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:54:14.300312Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:54:17.348465Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:54:17.347401Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:54:14.300652Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>lora_config <span class="op">=</span> LoraConfig(task_type<span class="op">=</span>TaskType.CAUSAL_LM)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_nm)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, lora_config)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>model.print_trainable_parameters()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>trainable params: 98,304 || all params: 70,524,928 || trainable%: 0.13938901149959346</code></pre>
<div id="cell-35" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:54:17.980036Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:54:17.979664Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:54:17.988806Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:54:17.987396Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:54:17.980006Z&quot;}" data-trusted="true" data-execution_count="8">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): GPTNeoXForCausalLM(
      (gpt_neox): GPTNeoXModel(
        (embed_in): Embedding(50304, 512)
        (layers): ModuleList(
          (0-5): 6 x GPTNeoXLayer(
            (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attention): GPTNeoXAttention(
              (rotary_emb): RotaryEmbedding()
              (query_key_value): Linear(
                in_features=512, out_features=1536, bias=True
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=512, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (dense): Linear(in_features=512, out_features=512, bias=True)
            )
            (mlp): GPTNeoXMLP(
              (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)
              (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)
              (act): GELUActivation()
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (embed_out): Linear(in_features=512, out_features=50304, bias=False)
    )
  )
)</code></pre>
</div>
</div>
<div id="cell-36" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:55:08.782308Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:55:08.781937Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T19:55:40.833488Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T19:55:40.832333Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:55:08.782276Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr<span class="op">=</span><span class="fl">5e-4</span>, bs<span class="op">=</span><span class="dv">16</span>, epochs<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-37" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T19:55:43.809125Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T19:55:43.808237Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T21:56:36.480522Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T21:56:36.479395Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T19:55:43.809087Z&quot;}" data-trusted="true" data-execution_count="12">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>

    <div>
      
      <progress value="32000" max="32000" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [32000/32000 2:00:46, Epoch 4/4]
    </div>
    
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.616000</td>
<td>2.614058</td>
</tr>
<tr class="even">
<td>2</td>
<td>2.575500</td>
<td>2.570585</td>
</tr>
<tr class="odd">
<td>3</td>
<td>2.605000</td>
<td>2.547680</td>
</tr>
<tr class="even">
<td>4</td>
<td>2.493900</td>
<td>2.540338</td>
</tr>
</tbody>
</table>
<p>
</p></div>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>TrainOutput(global_step=32000, training_loss=2.621225409567356, metrics={'train_runtime': 7252.3347, 'train_samples_per_second': 141.196, 'train_steps_per_second': 4.412, 'total_flos': 2.33350953959424e+16, 'train_loss': 2.621225409567356, 'epoch': 4.0})</code></pre>
</div>
</div>
<div id="cell-38" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T21:57:08.937551Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T21:57:08.937152Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T22:00:41.420405Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T22:00:41.419361Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T21:57:08.937492Z&quot;}" data-trusted="true" data-execution_count="13">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>eval_results <span class="op">=</span> trainer.evaluate()</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perplexity: </span><span class="sc">{</span>math<span class="sc">.</span>exp(eval_results[<span class="st">'eval_loss'</span>])<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="2000" max="2000" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [2000/2000 03:32]
    </div>
    
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Perplexity: 12.68</code></pre>
</div>
</div>
<div id="cell-39" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T22:02:36.565557Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T22:02:36.565123Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T22:02:39.190400Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T22:02:39.189416Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T22:02:36.565499Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Once upon a time,"</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span>trainer.model.to(<span class="st">'cpu'</span>), tokenizer<span class="op">=</span>tokz)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>generator(prompt, max_length <span class="op">=</span> <span class="dv">100</span>, repetition_penalty<span class="op">=</span><span class="fl">1.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Generated text:</p>
<blockquote class="blockquote">
<p>“Once upon a time, there was an old man who lived in the park. He had many friends and loved to play with him every day at his house all night long! One morning he decided that it would be best for everyone else because they were so happy together as each other on their own one by another’s side of town hall or doorstep…so when something unexpected happened she started playing outside - her mommy said no but could help herself out here until someone came up close enough.. She”</p>
</blockquote>
<p>The generated text resembles a story and is a bit coherent for the first couple of sentences before it stops making sense in the second half.</p>
<p>Here is a comparison of the full-fine-tuning (Full FT) vs.&nbsp;LoRA fine-tuning (LoRA FT) process on the <code>EleutherAI/pythia-70m</code>:</p>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Training Set</th>
<th style="text-align: left;">Validation Set</th>
<th style="text-align: left;">Perplexity</th>
<th style="text-align: left;">Batch Size</th>
<th style="text-align: left;">Epochs</th>
<th style="text-align: left;">Train Steps</th>
<th style="text-align: left;">Train Time (Minutes)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Full FT</td>
<td style="text-align: left;">70.4M</td>
<td style="text-align: left;">240k</td>
<td style="text-align: left;">60k</td>
<td style="text-align: left;">8.51</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">22500</td>
<td style="text-align: left;">100</td>
</tr>
<tr class="even">
<td style="text-align: left;">LoRA FT</td>
<td style="text-align: left;">98k</td>
<td style="text-align: left;">256k</td>
<td style="text-align: left;">64k</td>
<td style="text-align: left;">12.68</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">32000</td>
<td style="text-align: left;">120</td>
</tr>
</tbody>
</table>
</section>
<section id="generating-text-from-the-pre-trained-tinystories-model" class="level2">
<h2 class="anchored" data-anchor-id="generating-text-from-the-pre-trained-tinystories-model">Generating Text from the Pre-Trained TinyStories Model</h2>
<p>The authors of the paper that this dataset comes released their fine-tuned model on HuggingFace, so I’ll use it to generate text to see how a state-of-the-art TinyStories model performs:</p>
<div id="cell-45" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T22:06:22.150040Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T22:06:22.149630Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T22:06:23.651043Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T22:06:23.650038Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T22:06:22.150009Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>model_nm <span class="op">=</span> <span class="st">"EleutherAI/gpt-neo-125M"</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>tokz <span class="op">=</span> AutoTokenizer.from_pretrained(model_nm)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>tokz.add_special_tokens({<span class="st">'pad_token'</span>: <span class="st">'[PAD]'</span>})</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tok_func(x): <span class="cf">return</span> tokz(x[<span class="st">"text"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-46" class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-08-31T22:06:25.802966Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-08-31T22:06:25.802589Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-08-31T22:06:31.895432Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-08-31T22:06:31.894425Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-08-31T22:06:25.802935Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span><span class="st">'roneneldan/TinyStories-33M'</span>, tokenizer<span class="op">=</span>tokz)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>generator(prompt, max_length <span class="op">=</span> <span class="dv">100</span>, repetition_penalty<span class="op">=</span><span class="fl">1.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Generated text:</p>
<blockquote class="blockquote">
<p>‘Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine and pick flowers. One day, she found an ancient book on her porch. It had lots of pictures inside that looked very old.opened the book and saw many words written around it. But then, she heard a loud noise coming from the house next door. She went to investigate and found out that someone had broken into their home. ran back to’</p>
</blockquote>
<p>The model is so good! It can hold a consistent, coherent theme in story format for multiple sentences.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>I’m happy to have got this all to work, as that alone was a big step in my learning process. This is the first time I have trained a causal language model using HuggingFace. One thought to close out this exercise: Would restructuring the data help? Currently the dataset has text values like “Summary:” and “Features:”, which are the prompts used by the TinyStories paper authors to generate stories using GPT-3.5 and 4. Perhaps removing these prompts from the dataset and keeping only the story text would help improve the model. I’ll explore this in a future exercise.</p>
<p>I hope you enjoyed this blog post!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>