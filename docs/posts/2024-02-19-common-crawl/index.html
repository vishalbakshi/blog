<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-02-19">
<meta name="description" content="In this blog post I summarize the discussion in the paper ‘Training Data for the Price of a Sandwich: Common Crawl’s Impact on Generative AI’ by Stefan Baack and Mozilla Insights.">

<title>vishal bakshi - Paper Summary: Training Data for the Price of a Sandwich</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#paper-summary-training-data-for-the-price-of-a-sandwich" id="toc-paper-summary-training-data-for-the-price-of-a-sandwich" class="nav-link active" data-scroll-target="#paper-summary-training-data-for-the-price-of-a-sandwich">Paper Summary: Training Data for the Price of a Sandwich</a>
  <ul class="collapse">
  <li><a href="#what-is-common-crawl-cc" id="toc-what-is-common-crawl-cc" class="nav-link" data-scroll-target="#what-is-common-crawl-cc">What is Common Crawl (CC)?</a></li>
  <li><a href="#cc-use-in-generative-ai" id="toc-cc-use-in-generative-ai" class="nav-link" data-scroll-target="#cc-use-in-generative-ai">CC use in Generative AI</a></li>
  <li><a href="#ccs-mission" id="toc-ccs-mission" class="nav-link" data-scroll-target="#ccs-mission">CC’s Mission</a></li>
  <li><a href="#ccs-data" id="toc-ccs-data" class="nav-link" data-scroll-target="#ccs-data">CC’s Data</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#crawldb" id="toc-crawldb" class="nav-link" data-scroll-target="#crawldb">CrawlDB</a></li>
  <li><a href="#harmonic-centrality-score" id="toc-harmonic-centrality-score" class="nav-link" data-scroll-target="#harmonic-centrality-score">Harmonic Centrality Score</a></li>
  </ul></li>
  <li><a href="#filtering-cc-for-ai" id="toc-filtering-cc-for-ai" class="nav-link" data-scroll-target="#filtering-cc-for-ai">Filtering CC for AI</a>
  <ul class="collapse">
  <li><a href="#types-of-filtering" id="toc-types-of-filtering" class="nav-link" data-scroll-target="#types-of-filtering">Types of filtering</a></li>
  <li><a href="#inadequacy-of-filtering-methods" id="toc-inadequacy-of-filtering-methods" class="nav-link" data-scroll-target="#inadequacy-of-filtering-methods">(In)adequacy of Filtering Methods</a></li>
  </ul></li>
  <li><a href="#cc-and-trustworthy-ai" id="toc-cc-and-trustworthy-ai" class="nav-link" data-scroll-target="#cc-and-trustworthy-ai">CC and Trustworthy AI</a>
  <ul class="collapse">
  <li><a href="#upside-of-cc" id="toc-upside-of-cc" class="nav-link" data-scroll-target="#upside-of-cc">Upside of CC</a></li>
  <li><a href="#downsides-of-cc" id="toc-downsides-of-cc" class="nav-link" data-scroll-target="#downsides-of-cc">Downsides of CC</a></li>
  <li><a href="#recommendations-for-using-cc-to-train-ai" id="toc-recommendations-for-using-cc-to-train-ai" class="nav-link" data-scroll-target="#recommendations-for-using-cc-to-train-ai">Recommendations for using CC to train AI</a></li>
  <li><a href="#recommendations-for-llm-based-end-user-products" id="toc-recommendations-for-llm-based-end-user-products" class="nav-link" data-scroll-target="#recommendations-for-llm-based-end-user-products">Recommendations for LLM-based end-user products</a></li>
  </ul></li>
  <li><a href="#the-future-of-cc" id="toc-the-future-of-cc" class="nav-link" data-scroll-target="#the-future-of-cc">The Future of CC</a>
  <ul class="collapse">
  <li><a href="#ccs-shortcomings" id="toc-ccs-shortcomings" class="nav-link" data-scroll-target="#ccs-shortcomings">CC’s Shortcomings</a></li>
  <li><a href="#recommendations-for-cc" id="toc-recommendations-for-cc" class="nav-link" data-scroll-target="#recommendations-for-cc">Recommendations for CC</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Summary: Training Data for the Price of a Sandwich</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Trustworthy AI</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    In this blog post I summarize the discussion in the paper ‘Training Data for the Price of a Sandwich: Common Crawl’s Impact on Generative AI’ by Stefan Baack and Mozilla Insights.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 19, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="paper-summary-training-data-for-the-price-of-a-sandwich" class="level1">
<h1>Paper Summary: Training Data for the Price of a Sandwich</h1>
<p><strong><em>Common Crawl’s Impact on Generative AI</em></strong><br>
<em>Paper written by Stefan Baack and Mozilla Insights</em></p>
<hr>
<p><br></p>
<p><em>Presentation by Vishal Bakshi for the cluster-of-stars fastai Study Group on February 16, 2024.</em></p>
<p>In this blog post I’ll summarize what I learned from the paper <a href="https://foundation.mozilla.org/en/research/library/generative-ai-training-data/common-crawl/">Training Data for the Price of a Sandwich: Common Crawl’s Impact on Generative AI</a> by Stefan Baack and Mozilla Insights. This blog post originally started as a presentation I gave to the cluster-of-stars fastai study group–I have reformatted it to a more narrative style. I have also added more detail and context, as well as my reactions and ponderings.</p>
<p>This blog post is split up into seven sections, closely following the paper’s structure:</p>
<ol type="1">
<li>What is Common Crawl (referred to as CC)</li>
<li>CC’s use in Generative AI</li>
<li>CC’s Mission</li>
<li>CC’s Data</li>
<li>Filtering CC for AI</li>
<li>CC and Trustworthy AI</li>
</ol>
<p>A couple of terms that may need to be defined for some readers:</p>
<ul>
<li>LLM = Large Language Model. A model with a lot of parameters (like an equation with lots of variables) that can predict the next word in a sentence (generally speaking).</li>
<li>document = a word, sentence, file, book, webpage, or any sequence of text that is used to train a language model.</li>
</ul>
<section id="what-is-common-crawl-cc" class="level2">
<h2 class="anchored" data-anchor-id="what-is-common-crawl-cc">What is Common Crawl (CC)?</h2>
<p>CC is a small (3-ish employees) nonprofit organization providing 9.5+ petabytes of freely available archive of web crawl data dating back to 2008 (250 billion web pages), with 3 to 5 billions pages added each month.</p>
<p>This data is <em>not</em> a single dataset. Instead, it’s offered as individual crawls of varying sizes. The data is extremely popular, cited in over 10,000 papers.</p>
<p>A key point that will come up again throughout this paper is that CC data <strong>is not a representative sample of the internet</strong>. More on that later.</p>
</section>
<section id="cc-use-in-generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="cc-use-in-generative-ai">CC use in Generative AI</h2>
<p>One of the most impactful concepts in this paper was that of <em>infrastructure</em> as contrasted with <em>data</em>.</p>
<p>Media studies scholar Luke Munn says:</p>
<blockquote class="blockquote">
<p>One of the things that make infrastructures so powerful is that they model their own ideals. They privilege certain logics and then operationalize them. And in this sense… they both register wider societal values and establish blueprints for how they should be carried out.</p>
</blockquote>
<p>CC has an <em>infrastructural</em> role within generative AI R&amp;D: it provides a basis from which AI builders create training datasets. Its data is never used directly, instead, AI builders filter CC data before using it in training.</p>
<p>CC data is primarily used for pre-training a model, meaning when an architecture is fed data in order to predict the next token in a sequence of tokens. During this phase, we expect the model to store patterns or associations between words in a language (like English) in the sense that given an English word or subword, it can predict the next word or subword that in a grammatically sensible way. For example, if the model is prompted “the bird is” after being pre-trained on sensible data it will likely predict the next word to be “red” or “hungry” or something sensible. This is contrasted with <em>fine-tuning</em> where a model that can generally predict the next token sensibly is then trained on domain-specific data to predict the next token of a particular domain (such as ornithology, the study of birds). So, if a model fine-tuned on ornithological data is prompted “a group of ravens is called” it will hopefully predict the next word as “unkindness.”</p>
<p>82% of the GPT-3 tokens are from CC. More accurately speaking (see below) 60% of the training data seen by GPT-3 is from CC.</p>
<p>From <a href="https://arxiv.org/pdf/2005.14165.pdf">Brown et al.&nbsp;2020</a>:</p>
<p>More boradly speaking, of the 47 LLM papers between 2019-2023 reviewed by Stefan and Mozilla Insights, 64% used filtered CC data. The top 5-most used filtered datasets were:</p>
<ul>
<li><a href="https://github.com/EleutherAI/pile-cc">Pile-CC</a> (EletheurAI)</li>
<li><a href="https://huggingface.co/datasets/c4?row=0">C4</a> (Alphabet)</li>
<li>Custom CC (meaning the AI builders filtered the data themselves)</li>
<li><a href="https://arxiv.org/abs/1911.00359">CCNet</a> (Facebook)</li>
<li><a href="https://arxiv.org/pdf/1905.12616v3.pdf">RealNews</a> (UW)</li>
</ul>
</section>
<section id="ccs-mission" class="level2">
<h2 class="anchored" data-anchor-id="ccs-mission">CC’s Mission</h2>
<p>CC’s stated mission is to provide:</p>
<blockquote class="blockquote">
<p>high quality crawl data that was previously only available to large search engine corporations [to] small startups or even individuals</p>
</blockquote>
<p>Founder Gil Elbaz said in an interview (emphasis mine):</p>
<blockquote class="blockquote">
<p>I felt like a world where many companies are bringing innovation forth, across the world…is ultimately the world that I want to live in. I started to think about creating a <strong>neutral data company</strong>…that wants to democratize access to information to provide data to other companies</p>
</blockquote>
<p>Its guiding principle is that <strong>less curation</strong> of the provided data enables more research and innovation by downstream users.</p>
<p>The authors revisit this mission later on when discussing the relationship between CC and trustworthy AI.</p>
</section>
<section id="ccs-data" class="level2">
<h2 class="anchored" data-anchor-id="ccs-data">CC’s Data</h2>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>CC aims to support “machine scale analysis” which means automated, large-scale analysis of web data across web domains, as opposed to human scale analysis where a person (or many people) ingests information with their senses and then processes and analyzes it with their brain.</p>
<p>How does CC pick which parts of the internet to crawl? CC data consists of samples of URLs from web domains sampled from the CrawlDB, which stores 25+ billion URLs (as well as a score for each URL, when it was last fetches, whether it was successfully crawled and other fields).</p>
<p>CC contains three types of data (see <a href="https://commoncrawl.org/blog/navigating-the-warc-file-format">this site</a> for examples) - <code>WARC</code> (WebARChive) files which store the raw crawl data (HTML code) - <code>WAT</code> (Web Archive Transformation) files which store computed metadata for the data stored in the <code>WARC</code> - <code>WET</code> (WARC Encapsulated Text) files which store extracted plaintext from the data stored in the <code>WARC</code></p>
<p>The CC crawling process is designed to automatically find (a pre-defined maximum number of) new URLs considered good quality CC thinks of “quality” in terms of how CC’s data represents the web as a whole as well as the quality of the URLs included in the crawls.</p>
<p>The uncertainty of CC about how their data reflects the web as a whole is due to <strong>not knowing the size of the web as a whole</strong>. As one of CC’s staff put it:</p>
<blockquote class="blockquote">
<p>the web is practically infinite.</p>
</blockquote>
<p>Earlier they mentioned that 3 to 5 billion URLs are added each month. Why not more? Because there is a tradeoff between the size of a crawl and the quality of the crawl. To expand the size of the crawl they have to include lower quality URLs, and many lower quality URLs are spam. Crawlers can get stuck in “crawler traps” which are these pockets of the internet where spam URLs are directed to one another. If a crawler gets stuck in there, potentially a majority of the crawled URLs can be spam, and the crawl data contains spammy content.</p>
<p>Here’s a screenshot of information of a CC <a href="https://data.commoncrawl.org/crawl-data/CC-MAIN-2023-50/index.html">main crawl</a> (1 TiB = 2^40 bytes, around 1100 GB):</p>
<p><a href="https://data.commoncrawl.org/crawl-data/index.html">Here</a> is a list of all their main crawls.</p>
</section>
<section id="crawldb" class="level3">
<h3 class="anchored" data-anchor-id="crawldb">CrawlDB</h3>
<p>URLs are added to CrawlDB during main crawls, discovery crawls (crawls with the sole purpose of fetching more URLs), and sitemap analyses.</p>
<blockquote class="blockquote">
<p>a Sitemap is an XML file that lists URLs for a site along with additional metadata about each URL (when it was last updated, how often it usually changes, and how important it is, relative to other URLs in the site)</p>
</blockquote>
</section>
<section id="harmonic-centrality-score" class="level3">
<h3 class="anchored" data-anchor-id="harmonic-centrality-score">Harmonic Centrality Score</h3>
<p>How are URLs scored? and thereby sampled to include in the next crawl? With the Harmonic Centrality Score.</p>
<p>The Harmonic Centrality Score measures the importance of a node in a network based on its distance all other nodes. - Shorter distance = higher score. - More direct and indirect links to a domain = higher score. - Captures how <em>accessible</em> a domain is to other web pages.</p>
<p>“High quality” implies a higher Harmonic Centrality Score. The score for a URL is increased if the URL has never been crawled before or hasn’t been crawled in awhile.</p>
</section>
</section>
<section id="filtering-cc-for-ai" class="level2">
<h2 class="anchored" data-anchor-id="filtering-cc-for-ai">Filtering CC for AI</h2>
<p>As mentioned earlier, AI builders filter CC data before using it to pre-train their models. This section goes into more detail around filtering.</p>
<section id="types-of-filtering" class="level3">
<h3 class="anchored" data-anchor-id="types-of-filtering">Types of filtering</h3>
<ul>
<li>By language (most of CC data is in English)</li>
<li>Keywords or simple heuristics (only keep lines that end in a punctuation mark, or remove documents with certain keywords)
<ul>
<li>The <a href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en">List of Dirty, Naughty, Obscene, and Otherwise Bad Words</a> used for C4 dataset is problematic because the words included in that list are not inherently “bad” or “harmful”. It depends on context. For example, here are some words included in this list of “bad words” that are not “bad” given a particular context:
<ul>
<li>“domination” (filtering out webpages that have the word “domination” will exclude pages with a discussion about domination of one group or system on another)</li>
<li>“sexuality” and similar terms are of course normal and healthy words to use in many contexts.</li>
<li>anatomical words (“penis”, “vagina”, “clitoris”, “vulva”) are all perfectly “good” words in many contexts. Furthermore, the censorship of female sexuality is perpetuated by the inclusion of those words in this “bad word” list.</li>
<li>slurs reclaimed by racial and gender/sex minorities are used in non-derogatory ways in their communities and cultures—exluding these words excludes their representation in the data.</li>
</ul></li>
</ul></li>
<li>AI classifiers (only keeps documents statistically similar to reference dataset)
<ul>
<li>Pile-CC (EletheurAI) uses an AI classifier trained on OpenWebText2 (deduplicated Reddit comments with 3+ upvotes). Most Reddit users are male and white so this is not a representative dataset of the global population. Reddit has also struggled moderating toxicity.</li>
<li>GPT-3 is pre-trained on CC filtered by using a classifier trained on WebText as a proxy for “high-quality” documents. Documents that are similar to WebText are deemed “low quality”.</li>
</ul></li>
<li>Deduplication (remove one document if it is exactly he same or similar to another—“similar” in a statistical sense)
<ul>
<li>GPT-3 is pre-trained on CC data that was filtered to remove documents with high overlap with other documents (fuzzy deduplication).</li>
</ul></li>
</ul>
</section>
<section id="inadequacy-of-filtering-methods" class="level3">
<h3 class="anchored" data-anchor-id="inadequacy-of-filtering-methods">(In)adequacy of Filtering Methods</h3>
<p>There is a fundamental unresolved conflict or dilemma: the amount of data desired is too large for manual curation but automated filtering for toxicity and bias are significantly limited. The authors discuss solutions to this later on.</p>
</section>
</section>
<section id="cc-and-trustworthy-ai" class="level2">
<h2 class="anchored" data-anchor-id="cc-and-trustworthy-ai">CC and Trustworthy AI</h2>
<p>Mozilla defines Trustworthy AI is AI that is:</p>
<blockquote class="blockquote">
<p>demonstrably worthy of trust, tech that considers accountability, agency, and individual and collective well-being…[trustworthy] AI-driven products and services are designed with human agency and accountability from the beginning</p>
</blockquote>
<p>What stood out to me from this definition were the terms accountability, agency and well-being.</p>
<section id="upside-of-cc" class="level3">
<h3 class="anchored" data-anchor-id="upside-of-cc">Upside of CC</h3>
<p>The filtered CC versions used in LLM training are inherently more auditable than any proprietary training datasets because CC data is freely accessible online.</p>
<p>LLMs open and transparent about their data typically come from outside of Big Tech (e.g., <a href="https://bigscience.huggingface.co/blog/bloom">Bloom</a>), achieving CC’s mission of making this data accessible to small startups and individuals.</p>
</section>
<section id="downsides-of-cc" class="level3">
<h3 class="anchored" data-anchor-id="downsides-of-cc">Downsides of CC</h3>
<p>While filtered CC data is more auditable than proprietary datasets, AI builders don’t necessarily take the opportunity to be transparent about their CC use. In other words, what use is this auditability if how this freely accessible data is filtered is not disclosed?</p>
<p>The size and diversity of CC makes it hard to understand what an LLM is trained on. <strong>This is reinforced by the (false) assumption among some AI builders that CC represents the “entire internet” and somehow is a proxy for representing “all human knowledge”.</strong> CC staff explicitly state:</p>
<blockquote class="blockquote">
<p>Often it is claimed that Common Crawl contains the entire web, but that’s absolutely not true. Based on what I know about how many URLs exist, it’s very, very small.</p>
</blockquote>
<p>Training of generative AI on massive amounts of copyrighted material could trend towards making the internet less open and collaborative (“data revolts”: when content platforms block crawlers to protect their data). Note that CC stays within the bounds of US fair use policy for copyrighted materials. It only copies HTML code, no images or media or full copies of domains.</p>
<p><br></p>
<p>Here’s an example of how content platforms block crawlers, from NY Times’ <a href="https://www.nytimes.com/robots.txt"><code>robot.txt</code></a>:</p>
<pre><code>User-agent: CCBot
Disallow: /</code></pre>
</section>
<section id="recommendations-for-using-cc-to-train-ai" class="level3">
<h3 class="anchored" data-anchor-id="recommendations-for-using-cc-to-train-ai">Recommendations for using CC to train AI</h3>
<ul>
<li>Put more effort into filtering CC. Filter more types of problematic content (e.g., content that is racist or mysoginist).</li>
<li>Problematic content should be annotated (if it’s not filtered out). There are some models who are trained on problematic content in order to better detect it. These models will need to be trained on problematic data.</li>
<li>Consistently provide proper dataset documentation. (See “Dataset Audit Card” example on page 7 of <a href="https://arxiv.org/pdf/2006.16923.pdf">Large Datasets: A Pyrrhic Win for Computer Vision?</a>)</li>
</ul>
</section>
<section id="recommendations-for-llm-based-end-user-products" class="level3">
<h3 class="anchored" data-anchor-id="recommendations-for-llm-based-end-user-products">Recommendations for LLM-based end-user products</h3>
<ul>
<li>Better industry standards and government regulation for evaluating filtered CC versions and downstream model effects.</li>
<li>More nuanced, culturally contextual tools to evaluate profanity, racism, discrimination, etc. found in the datasets.</li>
<li>A descriptive demographic overview of the dataset content (e.g., what region and culture does this data represent?)</li>
<li>Evaluations by human moderators under fair, safe conditions (<a href="https://theguardian.com/technology/2023/aug/02/ai-chatbot-training-human-toll-content-moderator-meta-openai">‘It’s destroyed me completely’: Kenyan moderators decry toll of training of AI models</a>).</li>
<li>Evaluating the effects of individual datasets on model behavior (like EletheurAI’s <a href="https://www.eleuther.ai/projects/large-language-model-evaluation">Language Model Evaluation Harness</a>).</li>
<li>Trustworthy intermediaries who filter CC for various purposes (e.g., subject matter experts or cultural experts who can curate data to match their subject or culture appropriately).</li>
</ul>
</section>
</section>
<section id="the-future-of-cc" class="level2">
<h2 class="anchored" data-anchor-id="the-future-of-cc">The Future of CC</h2>
<section id="ccs-shortcomings" class="level3">
<h3 class="anchored" data-anchor-id="ccs-shortcomings">CC’s Shortcomings</h3>
<ul>
<li>CC is not a “neutral data” organization as its samples are not representative of the web and because <em>the web is not representative of all people</em> (about 40% or 3 billion people in the world do not have internet access). I would go further and say that there is no such thing as neutral data, even raw data is not neutral because data collection, and the environment within which data is collected is not neutral.</li>
<li>CC’s lack of transparency (around its data governance) is at odds with its self-image as a public resource. For a long time, there was almost no public communication from CC outside of its mailing list (which mostly dealt with technical questions) and its blog (mostly dedicated to announcing new crawl data).</li>
</ul>
</section>
<section id="recommendations-for-cc" class="level3">
<h3 class="anchored" data-anchor-id="recommendations-for-cc">Recommendations for CC</h3>
<ul>
<li>Add a Terms of Use to the data. If AI builders want to use your data, they should have to document their filtering methodology, and take approaches to better filter (or annotate) their data for problematic, biased and harmful content.</li>
<li>CC should conduct more curated, values-oriented crawls so that digitally marginalized communities are more included. Since a URL’s quality is determined by its Harmonic Centrality Score, and since that score is determined by how accessible the URL is to other URLs, URLs from communities without socioeconomic power and/or resources will not be deemed “accessible” as such and will be scored low. Additionally, many communities will post popular links to Facebook, but because it doesn’t allow crawlers, CC won’t get to see that URL.</li>
<li>Add a community-driven approach to identify relevant content for crawls. Let the people themselves tell you directly what content matters to them and represents their interests and cultures.</li>
<li>Provide quality and toxicity evaluations, or language labeling.</li>
<li>Create a formal way to make requests about crawls.</li>
<li>Provide educational resources about the limitations of CC data.</li>
<li>Foster discussions of filtering and data analysis tools.</li>
<li>Increase the number and diversity of high-quality datasets curated by humans equitably. In other words, it’s okay if these datasets are small if they are high quality and there are a lot of them.</li>
</ul>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>I really enjoyed this paper. I came away from it inspired and empowered. If we can put our heads together and expand the filtered CC data space to include more intentional and representative data about cultures, topics and ideologies that are either ignored or filtered out in the most popular datasets today, we can reshape how LLMs predict the next token.</p>
<p>As always, I hope you enjoyed this blog post!</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>