<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2025-07-16">
<meta name="description" content="A comprehensive technical deep dive into the ColBERTv1 paper, exploring the late interaction architecture that enables BERT-level retrieval effectiveness with 100x faster query processing through independent encoding, offline indexing, and the MaxSim operation. Includes detailed code walkthroughs, query augmentation analysis, and architectural comparisons that explain how ColBERT bridges the gap between retrieval quality and computational efficiency.">

<title>Revisiting ColBERTv1 : A Return to First Principles ‚Äì Vishal Bakshi's Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9c1ae87ad5063dce4f793ccd314a7566.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi‚Äôs Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#main-takeaways-omars-thread" id="toc-main-takeaways-omars-thread" class="nav-link" data-scroll-target="#main-takeaways-omars-thread">Main Takeaways (Omar‚Äôs Thread)</a></li>
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#background-neural-rankers" id="toc-background-neural-rankers" class="nav-link" data-scroll-target="#background-neural-rankers">Background: Neural Rankers</a></li>
  <li><a href="#how-does-colbert-compare-to-previous-architectures" id="toc-how-does-colbert-compare-to-previous-architectures" class="nav-link" data-scroll-target="#how-does-colbert-compare-to-previous-architectures">How does ColBERT compare to previous architectures?</a></li>
  <li><a href="#the-colbert-architecture" id="toc-the-colbert-architecture" class="nav-link" data-scroll-target="#the-colbert-architecture">The ColBERT Architecture</a></li>
  <li><a href="#encoding-queries-and-documents" id="toc-encoding-queries-and-documents" class="nav-link" data-scroll-target="#encoding-queries-and-documents">Encoding Queries and Documents</a>
  <ul class="collapse">
  <li><a href="#query-augmentation" id="toc-query-augmentation" class="nav-link" data-scroll-target="#query-augmentation">Query Augmentation</a></li>
  <li><a href="#mask-token-embeddings-are-not-meaningless" id="toc-mask-token-embeddings-are-not-meaningless" class="nav-link" data-scroll-target="#mask-token-embeddings-are-not-meaningless"><code>MASK</code> token embeddings are not meaningless</a></li>
  <li><a href="#sorting-documents-by-length-for-batching" id="toc-sorting-documents-by-length-for-batching" class="nav-link" data-scroll-target="#sorting-documents-by-length-for-batching">Sorting Documents by Length for Batching</a></li>
  </ul></li>
  <li><a href="#maxsim" id="toc-maxsim" class="nav-link" data-scroll-target="#maxsim">MaxSim</a></li>
  <li><a href="#offline-indexing" id="toc-offline-indexing" class="nav-link" data-scroll-target="#offline-indexing">Offline Indexing</a></li>
  <li><a href="#experimental-evaluation" id="toc-experimental-evaluation" class="nav-link" data-scroll-target="#experimental-evaluation">Experimental Evaluation</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#ablation-studies" id="toc-ablation-studies" class="nav-link" data-scroll-target="#ablation-studies">Ablation Studies</a></li>
  <li><a href="#indexing-throughpout-footprint" id="toc-indexing-throughpout-footprint" class="nav-link" data-scroll-target="#indexing-throughpout-footprint">Indexing Throughpout &amp; Footprint</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Revisiting ColBERTv1 : A Return to First Principles</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ColBERT</div>
    <div class="quarto-category">information retrieval</div>
  </div>
  </div>

<div>
  <div class="description">
    A comprehensive technical deep dive into the ColBERTv1 paper, exploring the late interaction architecture that enables BERT-level retrieval effectiveness with 100x faster query processing through independent encoding, offline indexing, and the MaxSim operation. Includes detailed code walkthroughs, query augmentation analysis, and architectural comparisons that explain how ColBERT bridges the gap between retrieval quality and computational efficiency.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div id="cell-1" class="cell">
<details class="code-fold">
<summary>pip installs</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install transformers<span class="op">==</span><span class="fl">4.49.0</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install ragatouille</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>imports</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> colbert.indexing.collection_encoder <span class="im">import</span> CollectionEncoder</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> colbert.infra <span class="im">import</span> ColBERTConfig</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> colbert.modeling.checkpoint <span class="im">import</span> Checkpoint</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> colbert.modeling.tokenization.utils <span class="im">import</span> _split_into_batches, _sort_by_length, _insert_prefix_token</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> inspect <span class="im">import</span> getsource</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> Checkpoint(<span class="st">"answerdotai/answerai-colbert-small-v1"</span>, ColBERTConfig())</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>ce <span class="op">=</span> CollectionEncoder(config<span class="op">=</span>ColBERTConfig(), checkpoint<span class="op">=</span>checkpoint)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<ul>
<li><a href="#main-takeaways-omars-thread">Main takeaways (Omar‚Äôs thread)</a></li>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#background-neural-rankers">Background: Neural Rankers</a></li>
<li><a href="#how-does-colbert-compare-to-previous-architectures">How does ColBERT compare to previous architectures?</a></li>
<li><a href="#the-colbert-architecture">The ColBERT architecture</a></li>
<li><a href="#encoding-queries-and-documents">Encoding Queries and Documents</a></li>
<li><a href="#maxsim">MaxSim</a></li>
<li><a href="#offline-indexing">Offline Indexing</a></li>
<li><a href="#experimental-evaluation">Experimental Evaluation</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#ablation-studies">Ablation Studies</a></li>
<li><a href="#indexing-throughpout--footprint">Indexing Throughput &amp; Footprint</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</section>
<section id="main-takeaways-omars-thread" class="level2">
<h2 class="anchored" data-anchor-id="main-takeaways-omars-thread">Main Takeaways (Omar‚Äôs Thread)</h2>
<p>Instead of coming up with my own takeaways, I‚Äôm going to do something different this time, where I‚Äôm going to walk through Omar‚Äôs thread from 2023, where he himself summarizes the main takeaways from the ColBERT v1 paper.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Progress on dense retrievers is saturating.<br><br>The best retrievers in 2024 will apply new forms of late interaction, i.e.&nbsp;scalable attention-like scoring for multi-vector embeddings.<br><br>Aüßµon late interaction, how it works efficiently, and why/where it's been shown to improve quality <a href="https://t.co/2XG33TtM9R">pic.twitter.com/2XG33TtM9R</a>
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804963760976092?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I think it‚Äôs important to highlight that he talks about new forms of late interaction as ‚Äúscalable attention-like scoring for multi-vector embeddings‚Äù. From my understanding of ColBERT, the attention-like scoring mechanism refers to the BERT contextualization of meaning of tokens. As the query or the document goes through BERT, it passes through the attention mechanism, and tokens attend to each other. So no single query token or document token is isolated; it exists in the context of the entire query or the entire document that it‚Äôs in, respectively. The ‚Äúmulti-vector embeddings‚Äù part of his tweet is referring to this idea: we don‚Äôt compress an entire document or an entire query into a single vector, but instead have more than one vector representing different dimensions of meaning of the text.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Say you have 1M documents. With infinite GPUs, what would your retriever look like?<br><br>Maybe a cross-encoder? Finetune a large LM to take &lt;query,doc&gt; pairs. Run it 1M times to get a score for all docs.<br><br>Expressive! Given a query, the LM can pay attention to every detail in the doc! <a href="https://t.co/P4t7bYe9dT">pic.twitter.com/P4t7bYe9dT</a>
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804965942013978?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Something I want to highlight is that you still have to fine-tune a language model to become a retriever because the language model itself has this general knowledge about language and the relationship between words but it doesn‚Äôt have explicitly the capability of accurately producing a score that measures the relevancy of one body of text to another. So fine-tuning brings out that implicit skill that is in the latent space of the model into an actionable task. If we had infinite GPUs, we would want to get the relationship of every query token to every document token encoded. We would want to do this during training and we would want to do this during inference. That‚Äôs why he says ‚Äúexpressive‚Äù - this is the ultimate expressiveness or maximum expressiveness that you can achieve between query and document tokens. Your query tokens are no longer just contextualized within the query; they are contextualized within the query and every single document. That‚Äôs a really powerful expressive way of capturing meaning between two bodies of text to determine if they are related to each other to some level.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
But cross-encoders are expensive: must re-run each doc thru the LM <em>for every new query</em>.<br><br>Most retrievers are <em>single-vector</em> encoders: Cram each doc into a vector in advance; match queries/docs with a dot-product.<br><br>Scalable! We can apply dot-product search at the billion scale! <a href="https://t.co/lvLp6vSd8K">pic.twitter.com/lvLp6vSd8K</a>
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804967942697408?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>On the other side of the spectrum, we have the least expressive encoding but it is also more scalable: single vector encoders. You‚Äôre cramming each document into a vector and then you‚Äôre matching the queries and documents with a dot product.</p>
<p>You‚Äôre getting limited contextualization because of the limited expressiveness of what the tokens mean because everything is expressed by a single vector. But this is the most scalable.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
A huge burden on these bi-encoders: They must create <em>one</em> vector that captures <em>every</em> question you may ask about <em>any</em> content in the doc<br><br>Repeatedly been shown to be really fragile (especially OOD) and data-inefficient.<br><br>Can we build far more effective, yet scalable, encoders?
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804970144710981?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I think the key here is that the motivation is not just to build more effective encoders but effective <em>and</em> scalable encoders.</p>
<p>A brief aside on the limitations of single vector representations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video"><img src="1.png" class="img-fluid figure-img" alt="The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video"></a></p>
<figcaption>The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video</figcaption>
</figure>
</div>
<p>There is a great talk by Antoine Chaffin published recently called <a href="https://youtu.be/1x3k0V2IITo?feature=shared">‚ÄúLate Interaction Beats Single Vectors for RAG Introduction to PyLate‚Äù</a> which is part of Hamel Husain‚Äôs AI Evals course. He talks about how pooling is the intrinsic flaw of dense models: the pooling operation compresses <code>n</code> tokens into a single one, because of this selective behavior is learned during training through data, it gets more extreme with longer context because you have to compress more, and the compressed representation learns one notion of similarity. This is what Omar means by the ‚Äúhuge burden‚Äù on the bi-encoders. They have to compress a lot of information into a single representation.</p>
<p>If you think about it, during training, which Antoine is talking about here (and he also had a good thread on Twitter, which I can‚Äôt find because Twitter‚Äôs search is horrible) is that as you‚Äôre training a single vector representation, small changes in the query will result in wholesale changes of the single vector that represents the entire document.</p>
<p>So for example if you have a query about actors the document embedding will be trained on expressing that one notion of movies. If you have a separate query about the plot now the document encoding has to represent that different notion of similarity with one vector.</p>
<p>Antoine says in his thread that because of this you get a very noisy training experience for the document encodings because they‚Äôre constantly being tossed around left and right to match different notions of similarity with each query in the training step. They can‚Äôt match all of the notions because they are a single vector representation or compression of multiple tokens.</p>
<p>Contrast this with the late interaction setup where you have one representation for each token. Now, when you are training, the token embedding in the document that corresponds to the query about the actors gets modified and adjusted to result in a better relevance score. Later on in training, a query about the plot is going to activate the token in the document about the plot, and the query about the visual effects will activate the visual effect tokens in the document, and so on. So, you get this fine-grained, nuanced representation aligning with fine-grained, nuanced meaning in queries during training. The benefit of this is that n each step of training you can have a new, nuanced gradient update to the weights that produce these granular representations.</p>
<p>Back to Omar‚Äôs thread.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Late interaction (ColBERT) is simple.<br><br>Let's <em>not</em> cram docs into a vector. Instead, let's make the attention (interaction) scalable.<br><br>How? Build a <em>late</em> interaction fn.<br><br>(1) Applied after, not during, encoding.<br>(2) Pruning-capable, i.e.&nbsp;scales better than linear. This is key! <a href="https://t.co/m7fS15SdIE">pic.twitter.com/m7fS15SdIE</a>
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804972455768167?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>If we want efficiency, we can‚Äôt get the full effect of attention. We can‚Äôt get all query tokens and all document tokens attending to each other. And we can get an interaction between query token embeddings and document token embeddings that is done efficiently if it is applied after, not during, encoding. The ‚Äúlate‚Äù in ‚Äúlate interaction‚Äù is what allows the pruning capability, which we‚Äôll talk about later in the presentation.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Is there a function like this that preserves the retrieval precision of BERT attention?<br><br>Oddly, yes and it's incredibly simple: just aggregate (sum) the MaxSim from query tokens to doc tokens.<br><br>Basically, softly locate each query vector in the doc (w dot product), and aggregate.
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804974485811685?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The key aspect of MaxSim is it takes the maximum similarity. Unlike something like average similarity, it doesn‚Äôt care about all tokens; it cares about the document token that has the maximum similarity with the given query token. This eliminates all but one document token from final consideration, which is what allows pruning capability to unlock. Because the interaction between query and document token embeddings happens after they‚Äôre encoded, you can encode queries and documents separately.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
It was a last-ditch run on a Sunday night (3 Nov'19) after complex scoring failed.<br><br>I spent weeks looking for a "bug". ColBERT w cheap scoring rivaled BERT-large cross-encoders with 10,000x more FLOPs?!<br><br>Called it ColBERT as a pun: late show / late interaction<br><br>log scale latency: <a href="https://t.co/4MGmzAYHYG">pic.twitter.com/4MGmzAYHYG</a>
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804976406802703?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Incredible that events like these happen, and I think happen quite frequently.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
There's a little-known trick that was essential for ColBERT's results: Query Augmentation.<br><br>ColBERT appends [MASK] tokens to the query encoder to allow BERT to create more query vectors that aren't there!<br><br>Is it the earliest form of a scratchpad / chain of thought? From Nov 2019! <a href="https://t.co/2rIoMn1jxP">pic.twitter.com/2rIoMn1jxP</a>
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804978667434048?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This is a super interesting concept that we‚Äôll look at in detail later on.</p>
<p>As another aside‚Äîone of the reasons I was motivated to re-read the ColBERT V1 paper was this thread below by Antoine.</p>
<blockquote class="twitter-tweet blockquote" data-theme="dark">
<p lang="en" dir="ltr">
I am starting to be more and more convinced that MaxSim generalize very well to long documents but struggles on longer query, most probably due to the asymmetry<br>Larger documents are bound by the number of query tokens, but larger queries might get noisy<br>Either it is a query‚Ä¶
</p>
‚Äî Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1942909502723883381?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>If you have a document with 1k tokens and a query with 32 tokens, at most 32 document tokens will pass the maximum similarity threshold, hence ‚Äúlarger documents are bound by the number of query tokens.‚Äù As your query gets larger the meaning of tokens becomes diverse, this may create noise in the meaning expressed in the query. Query tokens with vastly different meanings will have maximum similarity with document tokens that are vastly different in meaning as well.</p>
<p>The conversation continues with:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Yes, that is also why it was not a big issue with query expansion because all the queries had the same number of tokens<br>but with longer queries and no query exp, meh
</p>
‚Äî Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1943002350274179544?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>slm tokens makes a really interesting point. Let‚Äôs assume that for each of the 32 query tokens, you find a document token that has a cosine similarity of 1. You add them up, and the maximum similarity is 32, the length of the query. That‚Äôs what they‚Äôre saying by the maximum MaxSim is the length of the query. Assuming that as your query gets longer, the meaning of the tokens starts to vary, and potentially the maximum similarity between the query and document tokens starts to vary.</p>
<p>You can imagine that you could have a very long query where one or more tokens are kind of obscure and may be on the fringes of the intent and meaning of the whole query, and potentially dissimilar to any document in the collection. These will potentially have a very small maximum similarity with some document token. In this situation, you can imagine that you have a very high variance because some query tokens will have high maximum similarities, some will have low, and you get this kind of noisy distribution of similarities across the query.</p>
<p>They go on to say that normalization can‚Äôt be something like the mean over query tokens, but has to be related to the distribution. Why does Antoine say that there is no query expansion? We‚Äôll see that in the next tweet.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
btw let's rule out query expansion because I am training models using flash attention and so there isn't any query expansion (I keep forgetting ffs üò≠)
</p>
‚Äî Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1942969244183834786?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This will make more sense after we look at query expansion/augmentation later in this blog post. Basically, query expansion relies on masked tokens containing semantic meaning, but, IIUC, Flash Attention negates masked tokens, so it nullifies the interaction between masked tokens and other tokens.</p>
<p>Here‚Äôs a tweet from Antoine 9 months ago that explains this:</p>
<blockquote class="twitter-tweet blockquote" data-theme="dark">
<p lang="en" dir="ltr">
</p><p>Funny insight:<br><br>ColBERT query expansion works by adding tokens that are not attended but attend to the others <br><br>This works in the OG attention implementation with attention mask (as the attention values are computed for these tokens, their contributions are just masked out for the other tokens, but their representations are computed w.r.t the unmasked tokens)</p>
<p>However, with Flash Attention, masked tokens embeddings are just zeros, meaning the contribution to MaxSim is always zeros and these tokens are not used, as if there was no query expansion at all</p>
This might explain Jina-ColBERT results of attending vs not attending to those (which seems contrary to our results): if they activated FA during the tests, the comparaison is actually no query expansion vs query expansion with attending, not not attending vs attending
<p></p>
‚Äî Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1862059400271389138?ref_src=twsrc%5Etfw">November 28, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Here is a slide from a previous video that I made titled <a href="https://youtu.be/u_v6HHyv4No?feature=shared">‚ÄúUnderstanding Eager Bidirectional Attention via the Attention Mask‚Äù</a>. In this case, we have 16 tokens, including four masked tokens. The large 16x16 tensor at the bottom is the attention scores tensor. The masked tokens are correctly not being attended to, but they do attend to other tokens. The last four columns are set to negative infinity because they are masked tokens, so their attention scores will be zero. But the last four rows do contain some 1s. The attention scores will not be zero for those rows and columns where we have 1s. Masked tokens are not being attended to, but they do attend to other tokens, and therefore they do have an attention score, and therefore they will have hidden states in the embedding space.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="16x16 attention mask"><img src="2.png" class="img-fluid figure-img" alt="16x16 attention mask"></a></p>
<figcaption>16x16 attention mask</figcaption>
</figure>
</div>
<p>Back to Omar‚Äôs thread.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
OK, but how can ColBERT search 100M docs in ~100 milliseconds?<br><br>Late interaction is pruning-capable: it only needs to "touch" &lt; 0.1% of the documents to find the top-K.<br><br>This is by design: it's composed of monotonic functions (Max/Sum), which enable some neat algorithmic tricks.
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804980840182079?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
We can decompose late interaction into dozens of tiny nearest-neighbor searches, at the token level.<br><br>We'll only fetch &amp; score docs in which at least one token in close to (at least one token in) the query.<br><br>Otherwise, we can prove the score will be too small, and we can skip it! <a href="https://t.co/6vBZp1U0Ku">pic.twitter.com/6vBZp1U0Ku</a>
</p>
‚Äî Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804982949917032?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>As shown in the diagram, we have clusters of document token embeddings, clustered by some vector-similarity indexing process. For each query token embedding, we find the closest few clusters to it. We perform our MaxSim operation between the query tokens and all those clustered documents‚Äô tokens. With this initial clustering step, we‚Äôre filtering out low-relevance documents using nearest neighbor search from the start.</p>
<p>With these main takeaways under our belt, in the following sections we‚Äôll walk through each part of the ColBERTv1 paper in detail. I‚Äôll provide excerpts from the paper in block quotes (highlighted emphasis mine) and then my thoughts after that.</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a <mark>late interaction architecture that independently encodes the query and the document using BERT</mark> and then employs a <mark>cheap yet powerful interaction step that models their fine-grained similarity.</mark> By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to <mark>pre-compute document representations offine</mark>, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT‚Äôs <mark>pruning-friendly interaction mechanism</mark> enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT‚Äôs effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing <mark>two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query</mark>.</p>
</blockquote>
<p>The key part about late interaction is that the architecture independently encodes the query and document. This allows you to index document representations offline which allows you to delay the query-document interaction until query time.</p>
<p>What took me some unpacking is the line: &gt; ColBERT‚Äôs pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection.</p>
<p>IIUC, the pruning-friendliness of ColBERT is unlocked by the fact that the interaction mechanism uses <strong>maximum</strong> similarity, and because of the nature of maximum (i.e.&nbsp;only 1 token can satisfy maximum) you can ignore low-similarity documents. Vector similarity indexes group together documents by similarity, so if a cluster of documents is not close to the query token embedding in question, it can be ignored completely.</p>
</section>
<section id="background-neural-rankers" class="level2">
<h2 class="anchored" data-anchor-id="background-neural-rankers">Background: Neural Rankers</h2>
<p>On the terms ‚Äúranker‚Äù vs.&nbsp;‚Äúretriever‚Äù and what that brings up or me:</p>
<p>The terms Ranker and Retriever give me different mental images.</p>
<p>When I think of Ranker, I think of you already having some passages that are deemed relevant and you‚Äôre ranking them, bringing the best ones to the top.</p>
<p>When I think of Retriever, the mental image I have is that you have this collection of data documents, a corpus of text where you have irrelevant and relevant passages all mixed together. The retriever then goes in, sifts through this text, and finds the relevant passages.</p>
<p>It‚Äôs been a bit of an adjustment for me using these two as synonyms. So that‚Äôs something I just want to keep in mind as I‚Äôm reading literature is that Ranker and Retriever should give me the same mental image, but they don‚Äôt.</p>
<blockquote class="blockquote">
<p><mark>By computing deeply-contextualized semantic representations of query-document pairs, these LMs help bridge the pervasive vocabulary mismatch [21, 42] between documents and queries [30].</mark></p>
</blockquote>
<p>I wanted to highlight this sentence from the background section because I thought it was getting to the core of something about BERT that I didn‚Äôt really know. We‚Äôll see in a bit. But first‚Äì</p>
<p>The following excerpt is from the paper Modeling and Solving Term Mismatch for Full-Text Retrieval Which is reference [42], written in 2012:</p>
<blockquote class="blockquote">
<p>Even though modern retrieval systems typically use a multitude of features to rank documents, the backbone for search ranking is usually the standard tf.idf retrieval models.</p>
<p>This thesis addresses a limitation of the fundamental retrieval models, the term mismatch problem, which happens when query terms fail to appear in the documents that are relevant to the query. The term mismatch problem is a long standing problem in information retrieval.</p>
</blockquote>
<p>I haven‚Äôt read the full thesis, but it does make sense that for keyword-based search, the query term failing to appear in the document that is relevant to the query would be a major problem as it‚Äôs frequency in the document would be 0.</p>
<p>Another paper referenced on this vocabulary mismatch problem is ‚ÄúUnderstanding the Behaviors of BERT and Ranking‚Äù where they say:</p>
<blockquote class="blockquote">
<p>The observations suggest that, BERT‚Äôs pre-training on surrounding contexts favors text sequence pairs that are closer in their semantic meaning.</p>
</blockquote>
<p>So, it seems like even in the embedding space, the term mismatch problem is present. Another excerpt from the same paper:</p>
<blockquote class="blockquote">
<p>[BERT] prefers semantic matches between paraphrase tokens</p>
</blockquote>
<p>Here‚Äôs Figure 2 from the same paper where each point on the chart corresponds to one query-passage pair with a random regular term removed from the passage:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure 2 from ‚ÄúUnderstanding the Behaviors of BERT and Ranking‚Äù"><img src="3.png" class="img-fluid figure-img" alt="Figure 2 from ‚ÄúUnderstanding the Behaviors of BERT and Ranking‚Äù"></a></p>
<figcaption>Figure 2 from ‚ÄúUnderstanding the Behaviors of BERT and Ranking‚Äù</figcaption>
</figure>
</div>
<p>The x-axis is the original ranking score, and the y-axis is the score after the term is removed. One takeaway they had in the paper is that BERT in general has extreme scores. It either scores 1 or 0. But that‚Äôs not the main take away here when it comes to the concept of query-document-term mismatch. In the bottom right corner of the BERT chart, we can see that there are query-passage pairs with a high orginal ranking score and a low score after a term is removed. The original ranking of 1.0 drops to a ranking of 0.0. This is evidence that the query document term mismatch problem occurs in semantic space as well. If you remove a term that‚Äôs semantically similar in the query to the document, then BERT will not recognize the similarity between the two and will give the pair a low ranking score.</p>
<p>Let‚Äôs continue a little bit more into the background of neural rankers, but now in the context of how does ColBERT compare to these previous neural architectures?</p>
</section>
<section id="how-does-colbert-compare-to-previous-architectures" class="level2">
<h2 class="anchored" data-anchor-id="how-does-colbert-compare-to-previous-architectures">How does ColBERT compare to previous architectures?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure 2 from ColBERTv1 paper"><img src="4.png" class="img-fluid figure-img" alt="Figure 2 from ColBERTv1 paper"></a></p>
<figcaption>Figure 2 from ColBERTv1 paper</figcaption>
</figure>
</div>
<p>The small rectangles in this graphic represent words, subwords or tokens. The wider rectangle represent large dimension vectors or representations.</p>
<p>Representation-based Similarity (figure 2a) calculates a single cosine similarity score between a single query embedding and a single document embedding. Query-Document interaction (2b) feeds an interaction matrix with similarity scores between every pair of query-document tokens to a neural net which produces a single final similarity score. BERT (2c), all-to-all interaction, attends each token in the query to all other tokens in the query, and each token in the document to all other tokens in the document, contextualizing each token with all other query/document tokens. From the <a href="https://arxiv.org/pdf/1901.04085">Passage Re-Ranking with BERT paper</a>:</p>
<blockquote class="blockquote">
<p>We use a BERT_LARGE model as a binary classification model, that is, we use the [CLS] vector as input to a single layer neural network to obtain the probability of the passage being relevant</p>
</blockquote>
<p>Late interaction (2d), ColBERT, combines the best of both worlds: the offline computation of representation-based similarity and the richness/granularity of interaction-based similarity. Query tokens attend to each other during encoding, document tokens attend to each other during (offline) encoding; during interaction, each query token interacts with all document tokens and the document token with the maximum similarity is selected; these maximum similarities are summed across all query tokens, giving you one score per document. Not all documents in the collection need to be considered; vector similarity indexes naturally group relevant documents together. Searching for document token embeddings in clusters close to the query token embeddings reduces the number of candidates considered.</p>
<p>These architectural differences of ColBERT give it a ton of advantages:</p>
<blockquote class="blockquote">
<p>As Figure 1 illustrates, ColBERT can serve queries in tens or few hundreds of milliseconds. For instance, when used for reranking as in ‚ÄúColBERT (re-rank)‚Äù, it delivers over 170√ó speedup (and requires 14,000√ó fewer FLOPs) relative to existing BERT-based models, while being more effective than every non-BERT baseline (¬ß4.2 &amp; 4.3). ColBERT‚Äôs indexing‚Äîthe only time it needs to feed documents through BERT‚Äîis also practical: it can index the MS MARCO collection of 9M passages in about 3 hours using a single server with four GPUs (¬ß4.5), retaining its effectiveness with a space footprint of as little as few tens of GiBs.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="5.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure 1 from the ColBERTv1 paper"><img src="5.png" class="img-fluid figure-img" alt="Figure 1 from the ColBERTv1 paper"></a></p>
<figcaption>Figure 1 from the ColBERTv1 paper</figcaption>
</figure>
</div>
<p>Figure 1 shows that ColBERT has comparable performance to BERT Large and BERT Base but a 100x faster query latency. This is not an insignificant difference, as they say that ‚Äúraising query response times by as little as 100ms is known to impact user experience and even measurably diminish revenue‚Äù. So, it follows that a decrease from 10,000 milliseconds to 100s of milliseconds is going to measurably increase revenue.</p>
<p>One related paper I briefly want to focus on is this 2018 paper by Zamani et al, which introduces SNRM (emphasis mine):</p>
<blockquote class="blockquote">
<p>In 2018, Zamani et al.&nbsp;[41] introduced SNRM, a representationfocused IR model that encodes each query and each document as a single, sparse high-dimensional vector of ‚Äúlatent terms‚Äù. By producing a <mark>sparse-vector representation for each document</mark>, SNRM is able to use a traditional IR inverted index for representing documents, allowing fast end-to-end retrieval. Despite highly promising results and insights, SNRM‚Äôs effectiveness is substantially outperformed by the state of the art on the datasets with which it was evaluated (e.g., see [18, 38]). <mark>While SNRM employs sparsity to allow using inverted indexes</mark>, we relax this assumption and compare a (dense) BERT-based representation-focused model against our late-interaction ColBERT in our ablation experiments</p>
</blockquote>
<p>I fed the SNRM paper to Claude Sonnet 4 and asked it to give me an example of sparsity allows the use of an inverted file index:</p>
<blockquote class="blockquote">
<p><strong>Simple SNRM example:</strong></p>
<p><strong>Training produces latent terms like:</strong> - Latent term 47: ‚Äúsports concepts‚Äù - Latent term 203: ‚Äútechnology concepts‚Äù</p>
<p><strong>Document representations:</strong> - Doc A (about basketball): [0, 0, ‚Ä¶, 0.8, 0, ‚Ä¶, 0, 0, ‚Ä¶] (high weight at position 47) - Doc B (about AI): [0, 0, ‚Ä¶, 0, 0, ‚Ä¶, 0.9, 0, ‚Ä¶] (high weight at position 203)</p>
<p><strong>Inverted index:</strong> - Latent term 47 ‚Üí [Doc A: 0.8, Doc C: 0.3] - Latent term 203 ‚Üí [Doc B: 0.9, Doc D: 0.5]</p>
<p><strong>Query time:</strong> Sports query activates latent term 47 ‚Üí quickly finds Doc A and C without checking every document.</p>
</blockquote>
<p>Let‚Äôs say we have two document representations: document A which is about basketball and document B which is about AI. SNRM generates sparse representations, so a number of values are zero. Let‚Äôs say that we have a latent term 47 that‚Äôs about sports concepts and a latent term 203 that‚Äôs about technology concepts. The inverted index for latent term 47 is going to store the value of that term in doc A (which is 0.8) and in doc D (let‚Äôs say that‚Äôs 0.3). For the latent term 203 (which is technology concepts), the inverted index will store 0.9 for doc B (which is the highest position value) and 0.5 for doc D (which would be some relatively low position). At query time, a sports query activates the latent term for 47, and because that‚Äôs efficiently stored in the inverted index, it‚Äôs a quick lookup and you don‚Äôt have to check every document.</p>
<p>So now that we have a sense of where the ColBERT architecture falls in the context of previous work, we can now dive into the ColBERT architecture itself.</p>
</section>
<section id="the-colbert-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-colbert-architecture">The ColBERT Architecture</h2>
<blockquote class="blockquote">
<p>delaying the query‚Äìdocument interaction can facilitate cheap neural re-ranking (i.e., through pre-computation) and even support practical end-to-end neural retrieval (i.e., through pruning via vector-similarity search)</p>
</blockquote>
<p>ColBERT balances neural retrieval quality and cost, benefiting both re-ranking and end-to-end retrieval. The delayed query-document interaction enables offline document indexing. At query time, you only encode the query and run MaxSim operations. For end-to-end retrieval, this same offline indexing allows vector similarity clustering‚Äîinstead of searching all documents, you query the closest clusters, dramatically reducing candidates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="6.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d"><img src="6.png" class="img-fluid figure-img" alt="Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d"></a></p>
<figcaption>Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d</figcaption>
</figure>
</div>
<p>The general architecture of ColBERT, comprises of: a query encoder fQ (shown in green), a document encoder fD (shown in blue), and the late interaction mechanism S(shown in gray). Given a query q and document d, fQ encodes q into a bag of embeddings Eq while fD encodes d into another bag Ed. Each embeddings in Eq and Ed is contextualized based on the other terms in q or d, respectively.</p>
<p>Before we look at the Late Interaction Mechanism, let‚Äôs look closer at what is involved during the encoding of queries and documents.</p>
</section>
<section id="encoding-queries-and-documents" class="level2">
<h2 class="anchored" data-anchor-id="encoding-queries-and-documents">Encoding Queries and Documents</h2>
<blockquote class="blockquote">
<p>We share a single BERT model among our query and document encoders but distinguish input sequences that correspond to queries and documents by prepending a special token [Q] to queries and another token [D] to documents.</p>
</blockquote>
<blockquote class="blockquote">
<p>Given BERT‚Äôs representation of each token, our encoder passes the contextualized output representations through a linear layer with no activations. This layer serves to control the dimension of ColBERT‚Äôs embeddings, producing m-dimensional embeddings for the layer‚Äôs output size m. As we discuss later in more detail, we typically ¬Äx m to be much smaller than BERT‚Äôs ¬Äxed hidden dimension.</p>
</blockquote>
<blockquote class="blockquote">
<p>While ColBERT‚Äôs embedding dimension has limited impact on the efficiency of query encoding, this step is crucial for controlling the space footprint of documents</p>
</blockquote>
<p>A quick note about embedding dimension: there are models such as <a href="https://huggingface.co/answerdotai/answerai-colbert-small-v1">answerai-colbert-small-v1</a> where the embedding dimension is as small as 96.</p>
<p>Here‚Äôs the desription of the query encoder:</p>
<blockquote class="blockquote">
<p><strong>Query Encoder.</strong> Given a textual query q, we tokenize it into its BERT-based WordPiece [35] tokens q1, q2‚Ä¶ql . We prepend the token [Q] to the query. We place this token right after BERT‚Äôs sequence-start token [CLS]. If the query has fewer than a pre-defined number of tokens Nq , <mark>we pad it with BERT‚Äôs special [mask] tokens up to length Nq</mark> (otherwise, we truncate it to the first Nq tokens). This padded sequence of input tokens is then passed into BERT‚Äôs deep transformer architecture, which computes a contextualized representation of each token.</p>
</blockquote>
<p>Here is a key contribution of this paper, that I am going to do a dive into next:</p>
<blockquote class="blockquote">
<p>We denote the padding with masked tokens as <strong>query augmentation</strong>, a step that allows BERT to produce query-based embeddings at the positions corresponding to these masks. Query augmentation is intended to serve as a <mark>soft, differentiable mechanism for learning to expand queries with new terms or to re-weigh existing terms based on their importance for matching the query</mark>. As we show in ¬ß4.4, this operation is essential for ColBERT‚Äôs effectiveness.</p>
</blockquote>
<section id="query-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="query-augmentation">Query Augmentation</h3>
<p>Query augmentation is the idea that mask tokens carry semantic meaning, so padding short queries up to some fixed length expands the queries with these new semantically relevant terms, adding more nuance to help match similar terms in documents. In this side quest, I want to understand just how semantically similar these mask token embeddings are to the non-mask query tokens. I‚Äôll start by digging into the code in the repo which takes text and converts it to embeddings.</p>
<p>Here‚Äôs how the <code>Searcher</code> encodes the query, where it uses <code>queryFromText</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode(<span class="va">self</span>, text: TextQueries, full_length_search<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    queries <span class="op">=</span> text <span class="cf">if</span> <span class="bu">type</span>(text) <span class="kw">is</span> <span class="bu">list</span> <span class="cf">else</span> [text]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    bsize <span class="op">=</span> <span class="dv">128</span> <span class="cf">if</span> <span class="bu">len</span>(queries) <span class="op">&gt;</span> <span class="dv">128</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.checkpoint.query_tokenizer.query_maxlen <span class="op">=</span> <span class="va">self</span>.config.query_maxlen</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.checkpoint.queryFromText(queries, bsize<span class="op">=</span>bsize, to_cpu<span class="op">=</span><span class="va">True</span>, full_length_search<span class="op">=</span>full_length_search)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="cell-88" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="37d9aab6-94cb-4de6-fcf1-f5a8e2827de4" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>checkpoint.query_tokenizer.query_maxlen <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> checkpoint.queryFromText([<span class="st">"this is a short query"</span>], bsize<span class="op">=</span><span class="dv">1</span>, to_cpu<span class="op">=</span><span class="va">True</span>, full_length_search<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast() if self.activated else NullContextManager()</code></pre>
</div>
</div>
<div id="cell-89" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f66d2856-acc2-454d-911b-3f3b98324759" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Q.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>torch.Size([1, 32, 96])</code></pre>
</div>
</div>
<p>Note that even though there are less than 32 tokens in <code>"this is a short query"</code>, the norm of all <code>Q</code> embeddings is <code>1.0</code>. This is because ColBERT adds <code>[MASK]</code> tokens to pad the query to a 32-token length, which we‚Äôll see next.</p>
<div id="cell-91" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b21a9f43-389e-4837-bb58-ef047821bddb" data-execution_count="199">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>Q.norm(dim<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="199">
<pre><code>tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])</code></pre>
</div>
</div>
<p><code>checkpoint.queryFromText</code> uses <code>QueryTokenizer.tokenizer</code> which does the following:</p>
<p>It first tokenizes the text</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tokenize with max_length - 1 to add the marker id afterwards</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>obj <span class="op">=</span> <span class="va">self</span>.tok(batch_text, padding<span class="op">=</span><span class="st">'max_length'</span>, truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span>(max_length <span class="op">-</span> <span class="dv">1</span>)).to(DEVICE)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'input_ids'</span>], <span class="va">self</span>.Q_marker_token_id)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'attention_mask'</span>], <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And then replaces the padding token with the mask token.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># postprocess for the [MASK] augmentation</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ids[ids <span class="op">==</span> <span class="va">self</span>.pad_token_id] <span class="op">=</span> <span class="va">self</span>.mask_token_id</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Looking at that concretely:</p>
<div id="cell-94" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>obj <span class="op">=</span> checkpoint.query_tokenizer.tok([<span class="st">"this is a short query"</span>], padding<span class="op">=</span><span class="st">'max_length'</span>, truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>                return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span>(<span class="dv">32</span> <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'input_ids'</span>], checkpoint.query_tokenizer.Q_marker_token_id)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'attention_mask'</span>], <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-95" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6d429500-8377-4845-c844-08099e97adcc" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>ids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]])</code></pre>
</div>
</div>
<div id="cell-96" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="14e03a64-ea80-4a01-db59-1d2dc156355a" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>ids[ids <span class="op">==</span> checkpoint.query_tokenizer.pad_token_id] <span class="op">=</span> checkpoint.query_tokenizer.mask_token_id</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>ids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103]])</code></pre>
</div>
</div>
<div id="cell-97" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="7d24afd2-b0ea-4f04-a1bf-ad826c24ce00" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>checkpoint.query_tokenizer.tok.decode([<span class="dv">103</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>'[MASK]'</code></pre>
</div>
</div>
<div id="cell-98" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:71}}" data-outputid="900df54d-3ce7-428a-d888-c7cd3023214d" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>checkpoint.query_tokenizer.tok.decode(ids[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>'[CLS] [unused0] this is a short query [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'</code></pre>
</div>
</div>
<p>This replacement of pad tokens with mask tokens is critical because <code>queryFromText</code> calls <code>query</code> which is defined as:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> query(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    input_ids, attention_mask <span class="op">=</span> input_ids.to(<span class="va">self</span>.device), attention_mask.to(<span class="va">self</span>.device)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.bert(input_ids, attention_mask<span class="op">=</span>attention_mask)[<span class="dv">0</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> <span class="va">self</span>.linear(Q)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.tensor(<span class="va">self</span>.mask(input_ids, skiplist<span class="op">=</span>[]), device<span class="op">=</span><span class="va">self</span>.device).unsqueeze(<span class="dv">2</span>).<span class="bu">float</span>()</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> Q <span class="op">*</span> mask</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.nn.functional.normalize(Q, p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It actually creates its own <code>mask</code> to multiply <code>Q</code> by‚Äîit doesn‚Äôt use <code>attention_mask</code>.</p>
<p>Looking at <code>mask</code>:</p>
<div id="cell-102" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f57d4111-5908-4afa-8094-80164a8f80af" data-execution_count="20">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(getsource(checkpoint.mask))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    def mask(self, input_ids, skiplist):
        mask = [[(x not in skiplist) and (x != self.pad_token) for x in d] for d in input_ids.cpu().tolist()]
        return mask
</code></pre>
</div>
</div>
<p>If the token is <code>not in skiplist</code> and <code>!= self.pad_token</code> it gets a <code>1</code> in the <code>mask</code>. Since we swapped pad tokens with MASK tokens, they get a <code>1</code>.</p>
<div id="cell-104" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c8d1cd3a-ee95-43f9-f43f-0ceffc6c2aaa" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>ids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103]])</code></pre>
</div>
</div>
<div id="cell-105" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="99bb7805-01f9-43be-f484-4bfd3e7a088c" data-execution_count="24">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>torch.tensor(checkpoint.mask(ids, skiplist<span class="op">=</span>[])).<span class="bu">float</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</code></pre>
</div>
</div>
<p>As we can see, the <code>mask</code> is all <code>1</code>s, so <code>Q</code> remains unchanged.</p>
<p>As an aside, I‚Äôve been thinking about how RAGatouille sets the maximum query length based on the full query length (instead of fixing to 32):</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">isinstance</span>(query, <span class="bu">str</span>):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    query_length <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(query.split(<span class="st">" "</span>)) <span class="op">*</span> <span class="fl">1.35</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>._upgrade_searcher_maxlen(query_length, base_model_max_tokens)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> [<span class="va">self</span>._search(query, k, pids)]</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    longest_query_length <span class="op">=</span> <span class="bu">max</span>([<span class="bu">int</span>(<span class="bu">len</span>(x.split(<span class="st">" "</span>)) <span class="op">*</span> <span class="fl">1.35</span>) <span class="cf">for</span> x <span class="kw">in</span> query])</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>._upgrade_searcher_maxlen(longest_query_length, base_model_max_tokens)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> <span class="va">self</span>._batch_search(query, k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I think the following note about <code>full_length_search</code> in the ColBERT repo is related but I‚Äôm not currently sure:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Full length search is only available for single inference (for now)</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Batched full length search requires far deeper changes to the code base</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span>(full_length_search <span class="op">==</span> <span class="va">False</span> <span class="kw">or</span> (<span class="bu">type</span>(batch_text) <span class="op">==</span> <span class="bu">list</span> <span class="kw">and</span> <span class="bu">len</span>(batch_text) <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> full_length_search:</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize each string in the batch</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    un_truncated_ids <span class="op">=</span> <span class="va">self</span>.tok(batch_text, add_special_tokens<span class="op">=</span><span class="va">False</span>).to(DEVICE)[<span class="st">'input_ids'</span>]</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the longest length in the batch</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    max_length_in_batch <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> un_truncated_ids)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the max length</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    max_length <span class="op">=</span> <span class="va">self</span>.max_len(max_length_in_batch)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Max length is the default max length from the config</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    max_length <span class="op">=</span> <span class="va">self</span>.query_maxlen</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="mask-token-embeddings-are-not-meaningless" class="level3">
<h3 class="anchored" data-anchor-id="mask-token-embeddings-are-not-meaningless"><code>MASK</code> token embeddings are not meaningless</h3>
<p>So what meaning is embedded for the MASK token in the semantic space? To (lightly) explore this, I‚Äôll calculate the cosine similarity between the non-MASK and MASK tokens.</p>
<div id="cell-110" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1d97c78d-80f4-426d-9feb-7aaa3f8dcd15" data-execution_count="25">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>ids[<span class="dv">0</span>][:<span class="dv">7</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>tensor([  101,     1,  2023,  2003,  1037,  2460, 23032])</code></pre>
</div>
</div>
<div id="cell-111" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="55585a26-af0a-4b64-8453-102cc78da962" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>checkpoint.query_tokenizer.tok.decode(ids[<span class="dv">0</span>][:<span class="dv">7</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>'[CLS] [unused0] this is a short query'</code></pre>
</div>
</div>
<div id="cell-112" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="9e6e1ed8-eca4-4a74-b2da-f98270723158" data-execution_count="32">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>checkpoint.query_tokenizer.tok.decode([<span class="dv">2460</span>, <span class="dv">23032</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>'short query'</code></pre>
</div>
</div>
<div id="cell-113" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="620147a3-0f6b-4a41-cd88-120864057e16" data-execution_count="27">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>ids[<span class="dv">0</span>][<span class="dv">8</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor([103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103,
        103, 103, 103, 103, 103, 103, 103, 103, 103, 103])</code></pre>
</div>
</div>
<div id="cell-114" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:53}}" data-outputid="91f261b2-0a15-4150-cf9d-58c6933c3069" data-execution_count="28">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>checkpoint.query_tokenizer.tok.decode(ids[<span class="dv">0</span>][<span class="dv">8</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>'[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'</code></pre>
</div>
</div>
<p>Gathering the 96-dimensional embeddings for my non-MASK tokens.</p>
<div id="cell-116" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="249a17b0-4f81-4171-84a9-dc10719f7f0a" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>Qnm <span class="op">=</span> Q[<span class="dv">0</span>][:<span class="dv">7</span>]</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>Qnm.shape, Qnm.unsqueeze(<span class="dv">0</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(torch.Size([7, 96]), torch.Size([1, 7, 96]))</code></pre>
</div>
</div>
<p>Gathering the 96-dimensional embeddings for my MASK tokens.</p>
<div id="cell-118" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f68d9c1b-3b29-4da5-e522-dcd4e2a19cee" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>Qm <span class="op">=</span> Q[<span class="dv">0</span>][<span class="dv">7</span>:]</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>Qm.shape, Qm.unsqueeze(<span class="dv">1</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(torch.Size([25, 96]), torch.Size([25, 1, 96]))</code></pre>
</div>
</div>
<p>Taking the cosine similarity between the non-MASK and MASK tokens, we see that the MASK tokens (rows) are considerably similar in meaning (and in one case exactly the same) to the non-MASK tokens (columns)!</p>
<div id="cell-120" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fbf9c790-8b74-49db-e9a9-248b313bf574" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>torch.nn.functional.cosine_similarity(Qnm.unsqueeze(<span class="dv">0</span>), Qm.unsqueeze(<span class="dv">1</span>), dim<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>tensor([[1.0000, 0.9709, 0.9150, 0.9461, 0.9672, 0.7417, 0.6532],
        [0.9793, 0.9566, 0.9038, 0.9301, 0.9495, 0.7177, 0.6400],
        [0.9804, 0.9585, 0.9053, 0.9320, 0.9517, 0.7195, 0.6380],
        [0.9814, 0.9604, 0.9059, 0.9340, 0.9534, 0.7198, 0.6394],
        [0.9812, 0.9598, 0.9065, 0.9342, 0.9533, 0.7220, 0.6390],
        [0.9814, 0.9601, 0.9073, 0.9355, 0.9537, 0.7223, 0.6341],
        [0.9816, 0.9612, 0.9068, 0.9367, 0.9549, 0.7222, 0.6354],
        [0.9809, 0.9604, 0.9050, 0.9357, 0.9535, 0.7212, 0.6333],
        [0.9823, 0.9623, 0.9064, 0.9385, 0.9562, 0.7228, 0.6365],
        [0.9826, 0.9629, 0.9070, 0.9389, 0.9567, 0.7249, 0.6373],
        [0.9818, 0.9614, 0.9062, 0.9387, 0.9554, 0.7248, 0.6390],
        [0.9819, 0.9608, 0.9044, 0.9387, 0.9550, 0.7241, 0.6388],
        [0.9821, 0.9620, 0.9032, 0.9394, 0.9561, 0.7252, 0.6372],
        [0.9830, 0.9655, 0.9048, 0.9428, 0.9601, 0.7278, 0.6390],
        [0.9836, 0.9690, 0.9081, 0.9459, 0.9632, 0.7273, 0.6392],
        [0.9860, 0.9761, 0.9165, 0.9517, 0.9717, 0.7343, 0.6475],
        [0.9850, 0.9848, 0.9299, 0.9542, 0.9805, 0.7550, 0.6576],
        [0.9805, 0.9872, 0.9360, 0.9545, 0.9827, 0.7667, 0.6658],
        [0.9780, 0.9878, 0.9385, 0.9544, 0.9833, 0.7705, 0.6693],
        [0.9774, 0.9879, 0.9389, 0.9543, 0.9832, 0.7701, 0.6699],
        [0.9764, 0.9887, 0.9397, 0.9549, 0.9839, 0.7709, 0.6731],
        [0.9762, 0.9887, 0.9394, 0.9548, 0.9838, 0.7714, 0.6736],
        [0.9763, 0.9885, 0.9396, 0.9548, 0.9837, 0.7715, 0.6738],
        [0.9776, 0.9881, 0.9392, 0.9551, 0.9837, 0.7709, 0.6713],
        [0.9772, 0.9887, 0.9400, 0.9555, 0.9840, 0.7714, 0.6736]])</code></pre>
</div>
</div>
<p>It‚Äôs interesting to note that the first MASK token (first row) has a cosine similarity of 1 with the first non-MASK token (the <code>[CLS]</code> tokens, first column). Other interesting observations:</p>
<ul>
<li>the second non-MASK token (<code>[unused0]</code>, second column) is more similar to the last MASK token than most of the other MASK tokens.</li>
<li>In general, the MASK tokens are much less similar to the last two non-MASK token (<code>short query</code>, 6th and 7th columns) than they are to the first five non-MASK tokens.</li>
</ul>
<p>I think this is enough evidence to show that the MASK tokens carry semantic meaning important to the query.</p>
<p>Returning to the paper, let‚Äôs see what they have to say about the document encoder:</p>
<blockquote class="blockquote">
<p><strong>Document Encoder</strong>. Our document encoder has a very similar architecture. We first segment a document d into its constituent tokens d1, d2‚Ä¶dm, to which we prepend BERT‚Äôs start token [CLS] followed by our special token [D] that indicates a document sequence. <mark>Unlike queries, we do not append [mask] tokens to documents</mark>. After passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list. This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness. In summary, given q = q0, q1‚Ä¶ql and d = d0, d1‚Ä¶dn , we compute the bags of embeddings Eq and Ed in the following manner, where # refers to the [mask] tokens:</p>
<p>Eq := Normalize( CNN( BERT(‚Äú[Q]q0q1‚Ä¶ql ##‚Ä¶#‚Äù) ) ) (1)</p>
<p>Ed := Filter( Normalize( CNN( BERT(‚Äú[D]d0d1‚Ä¶dn‚Äù) ) ) ) (2)</p>
</blockquote>
<p>I want to highlight something they say about how they encode their documents:</p>
<blockquote class="blockquote">
<p>When batching, <mark>we pad all documents to the maximum length of a document within the batch</mark>. To make capping the sequence length on a per-batch basis more effective, our indexer proceeds through documents in groups of B (e.g., B = 100,000) documents. <mark>It sorts these documents by length</mark> and then feeds batches of b (e.g., b = 128) <mark>documents of comparable length</mark> through our encoder.</p>
</blockquote>
<p>So let‚Äôs look at some of the code. In the <code>CollectionEncoder</code> class, which is what‚Äôs used to encode documents and queries, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_encoder.py#L26">they call <code>docFromText</code></a> and they pass to it the passages which are currently strings:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> passages_batch <span class="kw">in</span> batch(passages, <span class="va">self</span>.config.index_bsize <span class="op">*</span> <span class="dv">50</span>):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    embs_, doclens_ <span class="op">=</span> <span class="va">self</span>.checkpoint.docFromText(</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        passages_batch,</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>        ...)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Inside <code>docFromText</code>, the document tokenizer‚Äôs <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L138"><code>tensorize</code> method is called</a>, and you pass to it the documents which are still strings:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> bsize:</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    text_batches, reverse_indices <span class="op">=</span> <span class="va">self</span>.doc_tokenizer.tensorize(</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>        docs, bsize<span class="op">=</span>bsize</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And then <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/doc_tokenization.py#L48">inside <code>DocTokenizer.tensorize</code></a>, you first convert the text into tokens. And then you pass those tokens into the <code>_sort_by_length</code> helper method:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tensorize(<span class="va">self</span>, batch_text, bsize<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">type</span>(batch_text) <span class="kw">in</span> [<span class="bu">list</span>, <span class="bu">tuple</span>], (<span class="bu">type</span>(batch_text))</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    obj <span class="op">=</span> <span class="va">self</span>.tok(batch_text, padding<span class="op">=</span><span class="st">'longest'</span>, truncation<span class="op">=</span><span class="st">'longest_first'</span>,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>                    return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span>(<span class="va">self</span>.doc_maxlen <span class="op">-</span> <span class="dv">1</span>)).to(DEVICE)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    ids <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'input_ids'</span>], <span class="va">self</span>.D_marker_token_id)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'attention_mask'</span>], <span class="dv">1</span>)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> bsize:</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        ids, mask, reverse_indices <span class="op">=</span> _sort_by_length(ids, mask, bsize)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        batches <span class="op">=</span> _split_into_batches(ids, mask, bsize)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> batches, reverse_indices</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ids, mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And finally, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/utils.py#L40">inside the <code>_sort_by_length</code> method</a>, it sums the mask in the last dimension, which is the sequence length dimension. Then it sorts it, grabs those indices, and returns the tokens of the passages in order. Using those indices:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _sort_by_length(ids, mask, bsize):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ids.size(<span class="dv">0</span>) <span class="op">&lt;=</span> bsize:</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ids, mask, torch.arange(ids.size(<span class="dv">0</span>))</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> mask.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>).sort().indices</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    reverse_indices <span class="op">=</span> indices.sort().indices</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ids[indices], mask[indices], reverse_indices</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>They‚Äôre summing the mask across the sequence length dimension (<code>mask.sum(-1)</code>). The mask contains 1s where you have non-padding tokens and 0s where you have padding tokens. So the sum of the mask across a sequence is the number of non-padding tokens in it. Sorting by this sum sorts the sequences by non-padding token length in ascending order.</p>
<p>Let‚Äôs look at this concretely through code.</p>
</section>
<section id="sorting-documents-by-length-for-batching" class="level3">
<h3 class="anchored" data-anchor-id="sorting-documents-by-length-for-batching">Sorting Documents by Length for Batching</h3>
<p>To better understand how ColBERT sorts documents by length for batching, I‚Äôm going to walk through a toy example using the internal methods provided in the repo.</p>
<p>I‚Äôll start by intentionally creating a list of passages of four different lengths: 40, 60, 80, and 100</p>
<div id="cell-133" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> []</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">128</span>):</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">32</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">32</span> <span class="kw">and</span> i <span class="op">&lt;</span> <span class="dv">64</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">64</span> <span class="kw">and</span> i <span class="op">&lt;</span> <span class="dv">96</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">96</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">40</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I now pass the passages with a bat size of 32 into the Checkpoints.DocFromText method, and as a result, I get encoded documents where the bat size is 128, the maximum document length is 103, and the embedding dimension is 96 because I‚Äôm using answerai-colbert-small-v1.</p>
<div id="cell-135" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="68ad41b5-0de8-47c5-a676-37ecdf8bc6db" data-execution_count="5">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> ce.checkpoint.docFromText(docs<span class="op">=</span>passages, bsize<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast() if self.activated else NullContextManager()</code></pre>
</div>
</div>
<div id="cell-136" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1414b4c2-4f36-407c-b617-2b1e08a96169" data-execution_count="6">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>res[<span class="dv">0</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>torch.Size([128, 103, 96])</code></pre>
</div>
</div>
<p>If we look one level deeper, inside docfromtext it calls the docTokenizer‚Äôs tensorize method which converts the string of text into tokens</p>
<div id="cell-138" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>text_batches, reverse_indices <span class="op">=</span> ce.checkpoint.doc_tokenizer.tensorize(passages, bsize<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-139" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1880f14a-a499-48af-a232-1866041700f1" data-execution_count="8">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>text_batches[<span class="dv">0</span>][<span class="dv">0</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>torch.Size([32, 103])</code></pre>
</div>
</div>
<div id="cell-140" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="eb4ab00f-7d27-4e08-f589-29937da66e0c" data-execution_count="10">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>text_batches[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([ 101,    2, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037,  102,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0], device='cuda:0')</code></pre>
</div>
</div>
<p>Taking a look at the number of tokens in each of the batch items.</p>
<div id="cell-142" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1c30db69-0973-428c-8334-db7e12190cce" data-execution_count="11">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">96</span>]:</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    obj <span class="op">=</span> ce.checkpoint.doc_tokenizer.tok(passages[i], padding<span class="op">=</span><span class="st">'longest'</span>, truncation<span class="op">=</span><span class="st">'longest_first'</span>,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>                       return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(obj[<span class="st">'input_ids'</span>].shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1, 102])
torch.Size([1, 82])
torch.Size([1, 62])
torch.Size([1, 42])</code></pre>
</div>
</div>
<p><code>tensorize</code> adds the <code>[Q]</code> or <code>[D]</code> token, so that‚Äôs why the first batch item only has 102 tokens, whereas after Tensorize, it has 103 tokens.</p>
<p>Before I run the rest of the sorting code, I‚Äôm going to shuffle the passages so that we can see if sorting actually takes place.</p>
<div id="cell-145" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>random.shuffle(passages)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-146" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1d2981e8-e3eb-4cf4-9e2d-477bc4d8b4f4" data-execution_count="14">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> o <span class="kw">in</span> passages[:<span class="dv">10</span>]: <span class="bu">print</span>(<span class="bu">len</span>(o))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>80
120
80
80
160
80
160
160
120
200</code></pre>
</div>
</div>
<p>When passing all of the passages to the <code>.tok</code> method, the tokenized batch has a number of tokens equal to the largest, longest passage. All 128 passages are tokenized up to a length of 102.</p>
<div id="cell-148" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="91d5a4ed-20e5-4e49-c275-63186d596371" data-execution_count="15">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>obj <span class="op">=</span> ce.checkpoint.doc_tokenizer.tok(passages, padding<span class="op">=</span><span class="st">'longest'</span>, truncation<span class="op">=</span><span class="st">'longest_first'</span>,</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>                       return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>obj[<span class="st">'input_ids'</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>torch.Size([128, 102])</code></pre>
</div>
</div>
<p>Here‚Äôs the step where we add the prefixed tokens for the documents, which is <code>[D]</code>.</p>
<div id="cell-150" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'input_ids'</span>], ce.checkpoint.doc_tokenizer.D_marker_token_id)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'attention_mask'</span>], <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-151" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e9702bae-2d9f-4243-d67e-6493a571f295" data-execution_count="17">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>ids.shape, mask.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Looking at the number of non-zero tokens in the batch (i.e., the non-padding tokens), we can see that our batch is still currently unsorted</p>
<div id="cell-153" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fbe6f4d0-b7ee-4ccb-a64a-91cba295e447" data-execution_count="18">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(torch.count_nonzero(ids[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(63)
tensor(43)
tensor(43)
tensor(83)
tensor(43)
tensor(83)
tensor(83)
tensor(63)
tensor(103)</code></pre>
</div>
</div>
<p>Looking at the sum of the masks, we can see that the sum of masks is equal to the number of non-padding tokens in the batch item.</p>
<div id="cell-155" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="51eeac0c-0838-41fa-8d40-413f43fc1f82" data-execution_count="19">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>): <span class="bu">print</span>(<span class="bu">sum</span>(mask[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(63)
tensor(43)
tensor(43)
tensor(83)
tensor(43)
tensor(83)
tensor(83)
tensor(63)
tensor(103)</code></pre>
</div>
</div>
<p>Alright, here‚Äôs the main part where it sorts by length the batches.</p>
<div id="cell-157" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="54ca1995-bd89-4a39-955d-bc69bdc07331" data-execution_count="20">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>ids, mask, reverse_indices <span class="op">=</span> _sort_by_length(ids, mask, bsize<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>ids.shape, mask.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Looking at the lengths of non-zero values in ids and mask. The items are now sorted by token length in increasing order.</p>
<div id="cell-159" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="942cc8ed-af36-4b59-838a-ccf99c63b13f" data-execution_count="21">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(torch.count_nonzero(ids[i]), <span class="bu">sum</span>(mask[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)</code></pre>
</div>
</div>
<p>It then splits it into batches of 32. Note that all batches are padded up to the maximum document length.</p>
<div id="cell-161" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>batches <span class="op">=</span> _split_into_batches(ids, mask, bsize<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-162" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5388ec21-f3f5-4c15-c667-582abdd8b2f1" data-execution_count="23">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>): <span class="bu">print</span>(batches[i][<span class="dv">0</span>].shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 103])
torch.Size([32, 103])
torch.Size([32, 103])
torch.Size([32, 103])</code></pre>
</div>
</div>
<p>Looking at the lengths of non-zero values in each batch we can see that the batches are now sorted by length of passage.</p>
<div id="cell-164" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1424fa48-3507-4b53-8f82-52ccbd0ad231" data-execution_count="24">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"------ Batch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(torch.count_nonzero(batches[i][<span class="dv">0</span>][j]), <span class="bu">sum</span>(batches[i][<span class="dv">1</span>][j]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>------ Batch 0
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
------ Batch 1
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
------ Batch 2
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
------ Batch 3
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)</code></pre>
</div>
</div>
<p>Now let‚Äôs go a layer deeper and look at <code>_sort_by_length</code> to see how the sorting actually happens. I‚Äôll reinstantiate the passages and shuffle them to make sure the sorting actually happens.</p>
<div id="cell-166" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> []</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">128</span>):</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">32</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">32</span> <span class="kw">and</span> i <span class="op">&lt;</span> <span class="dv">64</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">64</span> <span class="kw">and</span> i <span class="op">&lt;</span> <span class="dv">96</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">96</span>: passages.append(<span class="st">"a "</span> <span class="op">*</span> <span class="dv">40</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-167" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>random.shuffle(passages)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I‚Äôll also tokenize the passages and insert the prefix tokens.</p>
<div id="cell-169" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b456ac02-a0ec-4535-e5c3-35adcb8a43e7" data-execution_count="27">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>obj <span class="op">=</span> ce.checkpoint.doc_tokenizer.tok(passages, padding<span class="op">=</span><span class="st">'longest'</span>, truncation<span class="op">=</span><span class="st">'longest_first'</span>,</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>                       return_tensors<span class="op">=</span><span class="st">'pt'</span>, max_length<span class="op">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>obj[<span class="st">'input_ids'</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>torch.Size([128, 102])</code></pre>
</div>
</div>
<div id="cell-170" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'input_ids'</span>], ce.checkpoint.doc_tokenizer.D_marker_token_id)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> _insert_prefix_token(obj[<span class="st">'attention_mask'</span>], <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-171" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d8ef3186-088e-46b3-c0fe-2fb711f56713" data-execution_count="29">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>ids.shape, mask.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Checking to make sure that my batch is shuffled</p>
<div id="cell-173" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9d2b5d99-f4b1-40a3-f4f4-6610581b7912" data-execution_count="30">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(torch.count_nonzero(ids[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(103)
tensor(103)
tensor(43)
tensor(83)
tensor(83)
tensor(103)
tensor(63)
tensor(83)
tensor(43)
tensor(43)</code></pre>
</div>
</div>
<p>To sort the batch items by length of non-padding tokens, they sum the mask across the last dimension, which is the number of tokens.</p>
<div id="cell-175" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="351136c3-ea11-4339-ddd8-297058bc2d4d" data-execution_count="31">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>mask.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>torch.Size([128, 103])</code></pre>
</div>
</div>
<p>So, what you get here is basically the number of non-padding tokens in each of the 128 items.</p>
<div id="cell-177" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9b928051-0462-461c-907c-d1f39e828bd9" data-execution_count="32">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>mask.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>tensor([103, 103,  43,  83,  83, 103,  63,  83,  43,  43, 103,  83,  43,  83,
         63,  83,  43, 103,  43,  63, 103, 103,  43,  43, 103,  63,  83, 103,
        103, 103, 103,  63,  43,  83,  63,  83,  63, 103,  43,  43,  63,  63,
        103,  43, 103,  43,  63, 103,  43,  43,  63,  63,  83,  63, 103, 103,
         83,  63, 103,  83,  43, 103, 103,  63,  83,  43, 103,  83,  83, 103,
        103,  63,  63,  83,  43,  83,  83,  43,  83,  43, 103,  83, 103,  43,
         83,  83,  43,  63,  63,  63, 103,  63,  43, 103,  83,  63,  63,  43,
         63, 103,  83,  83,  83, 103,  63,  63,  63,  83,  83,  43,  43,  63,
        103,  83,  63,  83,  43,  83,  63,  63,  43,  43,  43,  63, 103,  43,
         83,  43])</code></pre>
</div>
</div>
<p>Then they sort it and get the indices.</p>
<div id="cell-179" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c32564d4-1263-41dd-fa9e-a2e6326b6039" data-execution_count="33">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> mask.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>).sort().indices</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>indices</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>tensor([ 49,  22,  23, 110, 109,  32,  38,  43,  45,  48,  39,  97,  60,  92,
         65,  86,  83,  74,  79,  77, 121, 127,   2, 125, 122,   8,   9, 116,
        120,  12,  18,  16,  63,  51,  53,  98,  96,  57,  95,  25, 114,  91,
         31,  89,   6,  88,  87, 123,  71,  72,  19, 119,  34, 106,  36, 105,
         14,  50,  40,  41, 104, 118,  46, 111,  64, 107, 126,  81, 108, 115,
         84,  85,  94, 102, 101, 113, 117, 100,  15,  33,  35,  26,  52,  56,
         59,  13,  11,   7,  67,  76,  78,  75,   3,  73,   4,  68,  17,   1,
         20,  21,  24, 124, 112,  27,  28,  29,  10,   5,  93,  80,  82,  70,
         69,  66,   0,  90,  62,  61,  30,  58,  55,  54,  99,  47,  44, 103,
         42,  37])</code></pre>
</div>
</div>
<div id="cell-180" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="110c7559-8e92-4cda-b255-2b6360b377d5" data-execution_count="37">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>mask[<span class="dv">49</span>].<span class="bu">sum</span>(), mask[<span class="dv">37</span>].<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>(tensor(43), tensor(103))</code></pre>
</div>
</div>
<p>If we look at the first index, the corresponding mask sum is 43, which is the smallest non-padding token length. And if we look at the last index, the sum of the mask is 103, which is the largest.</p>
<p>And then they sort these indices and then get the indices of that sort</p>
<div id="cell-183" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4d611ab4-29ea-4cbd-e22a-4833bcd1c8b5" data-execution_count="38">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>reverse_indices <span class="op">=</span> indices.sort().indices</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>reverse_indices</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor([114,  97,  22,  92,  94, 107,  44,  87,  25,  26, 106,  86,  29,  85,
         56,  78,  31,  96,  30,  50,  98,  99,   1,   2, 100,  39,  81, 103,
        104, 105, 118,  42,   5,  79,  52,  80,  54, 127,   6,  10,  58,  59,
        126,   7, 124,   8,  62, 123,   9,   0,  57,  33,  82,  34, 121, 120,
         83,  37, 119,  84,  12, 117, 116,  32,  64,  14, 113,  88,  95, 112,
        111,  48,  49,  93,  17,  91,  89,  19,  90,  18, 109,  67, 110,  16,
         70,  71,  15,  46,  45,  43, 115,  41,  13, 108,  72,  38,  36,  11,
         35, 122,  77,  74,  73, 125,  60,  55,  53,  65,  68,   4,   3,  63,
        102,  75,  40,  69,  27,  76,  61,  51,  28,  20,  24,  47, 101,  23,
         66,  21])</code></pre>
</div>
</div>
<div id="cell-184" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="8b07ff83-1783-4921-e11e-131109581342" data-execution_count="39">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>indices[<span class="dv">114</span>], indices[<span class="dv">97</span>], indices[<span class="dv">22</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(tensor(0), tensor(1), tensor(2))</code></pre>
</div>
</div>
<p>Looking at the first three values of <code>reverse_indices</code>: the reverse indices‚Äô first value corresponds to the original index of 0, the reverse indices‚Äô second value corresponds to the original index of 1, and the reverse indices‚Äô third value corresponds to the original index of 2.</p>
<p>Finally, using <code>indices</code> to index into <code>ids</code></p>
<div id="cell-187" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2208b96c-fa28-4691-82b7-da4f0c41477a" data-execution_count="40">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>ids[indices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([[ 101,    2, 1037,  ...,    0,    0,    0],
        [ 101,    2, 1037,  ...,    0,    0,    0],
        [ 101,    2, 1037,  ...,    0,    0,    0],
        ...,
        [ 101,    2, 1037,  ..., 1037, 1037,  102],
        [ 101,    2, 1037,  ..., 1037, 1037,  102],
        [ 101,    2, 1037,  ..., 1037, 1037,  102]])</code></pre>
</div>
</div>
<p>We can see that <code>ids[indices]</code> is sorted.</p>
<div id="cell-189" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5f05b8f3-8f3a-4a69-fa8a-0b19dfbf5175" data-execution_count="43">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(torch.count_nonzero(ids[indices][i<span class="op">*</span><span class="dv">32</span><span class="op">+</span>j]))</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(43)
tensor(43)
tensor(43)
tensor(43)
------------------------------
tensor(63)
tensor(63)
tensor(63)
tensor(63)
tensor(63)
------------------------------
tensor(83)
tensor(83)
tensor(83)
tensor(83)
tensor(83)
------------------------------
tensor(103)
tensor(103)
tensor(103)
tensor(103)
tensor(103)
------------------------------</code></pre>
</div>
</div>
<p>And that is what they mean by ‚Äúsorting documents by length and feeding the batches with documents of comparable length to the encoder‚Äù.</p>
<p>So that‚Äôs how queries and documents are encoded. We can now look at the interaction between the two at query time, which is the MaxSim operator.</p>
</section>
</section>
<section id="maxsim" class="level2">
<h2 class="anchored" data-anchor-id="maxsim">MaxSim</h2>
<blockquote class="blockquote">
<p>Using Eq and Ed , ColBERT computes the relevance score between q and d via late interaction, which we define as a summation of maximum similarity (MaxSim) operators. In particular, we find the maximum cosine similarity of each v ‚àà Eq with vectors in Ed , and combine the outputs via summation.</p>
</blockquote>
<p><span class="math display">\[S_{q,d} := \sum_{i \in [\|E_q\|]} \max_{j \in [\|E_d\|]} E_{qi} \cdot E_{dj}^T\]</span></p>
<p>Looking at the equation‚Äîwe iterate through the queries, for each query we iterate through the document tokens and calculate the cosine similarity. we keep the maximum and sum it to S. Note that cosine similarity can be implemented as dot product because the embeddings are normalized. Another way to put it, taken from <a href="https://www.mixedbread.com/blog/maxsim-cpu#:~:text=For%20each%20candidate%20document%2C%20MaxSim%20iterates%20through%20every%20token%20within%20the%20query%2C%20and%20compares%20its%20similarity%20to%20every%20token%20within%20the%20document%2C%20before%20keeping%20the%20maximum%20value%20for%20each%20query%20token%20(hence%20the%20Max)%20and%20summing%20them%20up%20to%20produce%20a%20document%2Dlevel%20score.">Ben Clavie‚Äôs recent maxsim-cpu release blog post</a>:</p>
<blockquote class="blockquote">
<p>For each candidate document, MaxSim iterates through every token within the query, and compares its similarity to every token within the document, before keeping the maximum value for each query token (hence the Max) and summing them up to produce a document-level score.</p>
</blockquote>
<p>Beautiful.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="7.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Example of a MaxSim calculation between a query and a document"><img src="7.png" class="img-fluid figure-img" alt="Example of a MaxSim calculation between a query and a document"></a></p>
<figcaption>Example of a MaxSim calculation between a query and a document</figcaption>
</figure>
</div>
<p>Here is an example of the summation of MaxSim between query and document token embeddings. In this example, we have four query tokens and eight document tokens. For the first query token q1, the highest cosine similarity is with the fifth document token d5. d1 has the maximum cosine similarity for that q2, d2 for q3, and d8 for q4. Adding up these maximum cosine similarities, we get a final relevance score of 3.1. Since there are four tokens, the maximum possible MaxSim value is 4.0.</p>
<blockquote class="blockquote">
<p>this interaction mechanism softly searches for each query term tq ‚Äîin a manner that reflects its context in the query‚Äîagainst the document‚Äôs embeddings, quantifying the strength of the ‚Äúmatch‚Äù via the largest similarity score between tq and a document term td . Given these term scores, it then estimates the document relevance by summing the matching evidence across all query terms.</p>
</blockquote>
<p>The query tokens have passed through a transformer model and as such have passed through an attention mechanism so that all tokens attend to all other tokens. So the query itself now has interdependent relationships across tokens. When we‚Äôre searching for one token and looking to find the closest document, we‚Äôre not just looking to find the closest document to that token in isolation, we‚Äôre trying to find the closest document to that token within the context of the entire query. Some contextualized query token embeddings will find strong matches in certain documents, but what we‚Äôre looking for is the document for which the total maximum similarity for all query tokens is the largest. You can imagine that as a query gets very long, and the words in the query drift farther apart in meaning, the MaxSim values (before summation) for a document will have high variance.</p>
<blockquote class="blockquote">
<p>more sophisticated matching is possible with other choices such as deep convolution and attention layers (i.e., as in typical interaction-focused models),</p>
</blockquote>
<p>This reminds me of the <a href="https://arxiv.org/abs/2502.05364">Hypencoder paper</a> where they use a neural net for each query that takes as input a document embeddings and outputs a scalar relevance score. This is motivated by the fact that inner product (which is what cosine similarity is) is a linear operation and can thus only linearly separate two groups of vectors (such as embeddings). When your embedding dimension is much smaller than the number of vectors that you have, you can‚Äôt separate two groups linearly. In our case, our embedding dimension may be 96 and the number of vectors could be in the millions. Mathematically, you cannot linearly separate such a high number of vectors when they‚Äôre in a relatively low-dimensional space.</p>
<p>So you need a complex function because a line doesn‚Äôt work, and anytime you need a complex function where it‚Äôs more squiggly than a line, a neural net is a good choice!</p>
<p>However, the simplicity of MaxSim has two benefits:</p>
<blockquote class="blockquote">
<p>First, it stands out as a particularly cheap interaction mechanism, as we examine its FLOPs in ¬ß4.2. Second, and more importantly, it is amenable to highly-efficient pruning for top-k retrieval, as we evaluate in ¬ß4.3. This enables using vector-similarity algorithms for skipping documents without materializing the full interaction matrix or even considering each document in isolation. Other cheap choices (e.g., a summation of average similarity scores, instead of maximum) are possible; however, many are less amenable to pruning.</p>
</blockquote>
<p>ColBERT‚Äôs MaxSim mechanism enables efficient pruning: document tokens are clustered by similarity in vector indexes. At query time, each query token searches only the nearest clusters, skipping irrelevant documents. The ‚Äúmaximum‚Äù aggregation makes this possible‚Äîyou only need the best matches, not exhaustive comparison across all documents.</p>
<p>We‚Äôre going to take a look at this paragraph, and then we‚Äôre going to look at the code that corresponds to it. Note that this is for a single query, which is what I‚Äôm going to focus on.</p>
<blockquote class="blockquote">
<p>Given a query q, we compute its bag of contextualized embeddings Eq (Equation 1) and, concurrently, gather the document representations into a 3-dimensional tensor D consisting of k document matrices. We pad the k documents to their maximum length to facilitate batched operations, and move the tensor D to the GPU‚Äôs memory. On the GPU, <mark>we compute a batch dot-product of Eq and D</mark>, possibly over multiple mini-batches. ¬åe output materializes a 3-dimensional tensor that is a collection of cross-match matrices between q and each document. To compute the score of each document, <mark>we reduce its matrix across document terms via a max-pool (i.e., representing an exhaustive implementation of our MaxSim computation) and reduce across query terms via a summation.</mark> Finally, we sort the k documents by their total scores.</p>
</blockquote>
<p>So let‚Äôs first look at the higher level class which is the <code>IndexScorer</code>. <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L188C9-L189C75">In <code>score_pids</code></a>, if the query size is 1 (which it is in our case), it‚Äôs going to pass the query and the documents to <code>colbert_score_packed</code>.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> Q.size(<span class="dv">0</span>) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> colbert_score_packed(Q, D_packed, D_mask, config), pids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Inside <code>colbert_score_packed</code>, it removes the unit batch axis of the queries, and makes sure that q and d both have two dimensions. Then it performs the dot product between the two, and we can do this instead of explicitly calling cosine similarity because q and d are both normalized embeddings. The dot product results in a <code>scores</code> tensor that has size <code>number of document tokens x number of query tokens</code>. It then passes these scores into the <code>StridedTensor</code> and it gets back <code>scores_padded</code> and <code>scores_mask</code> which are then passed to <code>colbert_score_reduce</code>.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> colbert_score_packed(Q, D_packed, D_lengths, config<span class="op">=</span>ColBERTConfig()):</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a><span class="co">        Works with a single query only.</span></span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>    use_gpu <span class="op">=</span> config.total_visible_gpus <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_gpu:</span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>        Q, D_packed, D_lengths <span class="op">=</span> Q.cuda(), D_packed.cuda(), D_lengths.cuda()</span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> Q.squeeze(<span class="dv">0</span>) <span class="co"># removes the unit batch axis</span></span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-13"><a href="#cb109-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> Q.dim() <span class="op">==</span> <span class="dv">2</span>, Q.size()                     <span class="co"># num query tokens x emb dim</span></span>
<span id="cb109-14"><a href="#cb109-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> D_packed.dim() <span class="op">==</span> <span class="dv">2</span>, D_packed.size()       <span class="co"># num doc tokens   x emb dim</span></span>
<span id="cb109-15"><a href="#cb109-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-16"><a href="#cb109-16" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> D_packed <span class="op">@</span> Q.to(dtype<span class="op">=</span>D_packed.dtype).T  <span class="co"># num doc tokens x num query tokens</span></span>
<span id="cb109-17"><a href="#cb109-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-18"><a href="#cb109-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_gpu <span class="kw">or</span> config.interaction <span class="op">==</span> <span class="st">"flipr"</span>:</span>
<span id="cb109-19"><a href="#cb109-19" aria-hidden="true" tabindex="-1"></a>        scores_padded, scores_mask <span class="op">=</span> StridedTensor(scores, D_lengths, use_gpu<span class="op">=</span>use_gpu).as_padded_tensor()</span>
<span id="cb109-20"><a href="#cb109-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-21"><a href="#cb109-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> colbert_score_reduce(scores_padded, scores_mask, config)</span>
<span id="cb109-22"><a href="#cb109-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb109-23"><a href="#cb109-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ColBERT.segmented_maxsim(scores, D_lengths)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>scores_padded</code> has shape <code>number of documents x maximum number of tokens in the documents x number of query tokens</code>. So if we have three documents, a maximum of 13 document tokens, and 32 query tokens, <code>scores_padded</code> has shape 3 x 13 x 32.</p>
<p>Finally, <code>colbert_score_reduce</code> is called which takes the maximum of <code>scores_padded</code> across the second dimension (number of document tokens) to leave us with one score for each query token per document.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> colbert_score_reduce(scores_padded, D_mask, config: ColBERTConfig):</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>    D_padding <span class="op">=</span> <span class="op">~</span>D_mask.view(scores_padded.size(<span class="dv">0</span>), scores_padded.size(<span class="dv">1</span>)).<span class="bu">bool</span>()</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a>    scores_padded[D_padding] <span class="op">=</span> <span class="op">-</span><span class="dv">9999</span></span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores_padded.<span class="bu">max</span>(<span class="dv">1</span>).values</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># flipr code removed for brevity</span></span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Taking <code>scores.sum(-1)</code>, the summation across the query token dimension, leaves us with one score per document, our desired result.</p>
<blockquote class="blockquote">
<p>Relative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in fact, its cost is dominated by the cost of gathering and transferring the pre-computed embeddings. To illustrate, ranking k documents via typical BERT rankers requires feeding BERT k different inputs each of length l = |q| + |di | for query q and documents di , where attention has quadratic cost in the length of the sequence. In contrast, ColBERT feeds BERT only a single, much shorter sequence of length l = |q|. Consequently, ColBERT is not only cheaper, it also scales much better with k as we examine in ¬ß4.2.</p>
</blockquote>
<p>So, what is involved in that cost of gathering and transferring the pre-computed embeddings? We will look at what they say about offline indexing next.</p>
</section>
<section id="offline-indexing" class="level2">
<h2 class="anchored" data-anchor-id="offline-indexing">Offline Indexing</h2>
<blockquote class="blockquote">
<p>Instead of applying MaxSim between one of the query embeddings and all of one document‚Äôs embeddings, we can use fast vector-similarity data structures to efficiently conduct this search between the query embedding and all document embeddings across the full collection. For this, we employ an off-the-shelf library for large-scale vector-similarity search, namely faiss [15] from Facebook. In particular, at the end of offline indexing (¬ß3.4), we maintain a mapping from each embedding to its document of origin and then index all document embeddings into faiss.</p>
</blockquote>
<p>The current implementation in the repo uses a more efficient indexing system, the PLAID index, as opposed to what is written here (indexing ‚Äúall document embeddings into faiss‚Äù and ‚Äúmantain a mapping from each embeddings to its document of origin‚Äù). Instead, the PLAID index uses residual compression with centroids, maintains an Inverted File (IVF) structure that maps centroids to passage IDs, and stores embeddings as compressed residuals relative to centroids.</p>
<p>There‚Äôs a lot to unpack in the following section:</p>
<blockquote class="blockquote">
<p>Subsequently, when serving queries, we use a two-stage procedure to retrieve the top-k documents from the entire collection. Both stages rely on ColBERT‚Äôs scoring: the first is an approximate stage aimed at filtering while the second is a refinement stage. For the first stage, we concurrently issue Nq vector-similarity queries (corresponding to each of the embeddings in Eq ) onto our faiss index. This retrieves the top-k‚Äô (e.g., k‚Äô = k/2) matches for that vector over all document embeddings. We map each of those to its document of origin, producing Nq √ó k‚Äô document IDs, only K ‚â§ Nq √ó k‚Äô of which are unique. These K documents likely contain one or more embeddings that are highly similar to the query embeddings. For the second stage, we refine this set by exhaustively re-ranking only those K documents in the usual manner described in ¬ß3.5. In our faiss-based implementation, we use an IVFPQ index (‚Äúinverted file with product quantization‚Äù). This index partitions the embedding space into P (e.g., P = 1000) cells based on k-means clustering and then assigns each document embedding to its nearest cell based on the selected vector-similarity metric. For serving queries, when searching for the top-k‚Äô matches for a single query embedding, only the nearest p (e.g., p = 10) partitions are searched. To improve memory efficiency, every embedding is divided into s (e.g., s = 16) sub-vectors, each represented using one byte. Moreover, the index conducts the similarity computations in this compressed domain, leading to cheaper computations and thus faster search.</p>
</blockquote>
<ul>
<li>There are NQ query token embeddings, and for each one, we find the top K‚Äô document IDs.</li>
<li>NQ x k‚Äô (say 32 x 500) documents will include some duplicates, Meaning that some documents will contain document token embeddings that are close to more than one query token. Removing those duplicates will give us a K number of documents, which is less than the number of query embeddings x k‚Äô.</li>
<li>This first stage has greatly reduced the number of documents in consideration. You go from all documents in consideration, which could be tens of millions, down to just NQ x k‚Äô or fewer documents in consideration. This K documents are then re-ranked according to the MaxSim computation across query token embeddings that we saw earlier.</li>
<li>The late interaction architecture allows for pruning, as exhibited by the IVFPQ index. That index starts by partitioning the embedding space into some number of clusters, where that number of clusters is much much less than the number of token embeddings. It assigns each document token embedding to its nearest cluster based on whatever similarity metric is being used. This reduces the number of potential candidates that are close to a query token. When we search for a document similar to a given query, you only search the nearest p partitions, and p is small, say 10. And so think about this: you started out with maybe tens of millions of documents, you‚Äôve narrowed that down to a thousand clusters of documents, and then you‚Äôre now narrowing that even further down to only 10 clusters that are considered for a single query token. That we can index these documents offline before the interaction takes place and organize them into clusters is what allows this pruning to take place.</li>
<li>The last thing I‚Äôll say about this section is that the compression that they‚Äôre explaining here, where they divide every embedding into sub-vectors each represented using one byte, is now replaced with the PLAID compression where they quantize the residual embeddings into n bits.</li>
</ul>
<p>So with that, we have pretty much covered all of the conceptual foundations of ColBERT. To understand the impact of those foundations, we‚Äôll now look at the experimental evaluation section from the paper.</p>
</section>
<section id="experimental-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="experimental-evaluation">Experimental Evaluation</h2>
<p>And we‚Äôll explore the four research questions that the ColBERT authors have put forth in this section.</p>
<ul>
<li>RQ1: In a typical re-ranking setup, how well can ColBERT bridge the existing gap between highly-efficient and highly-effective neural models?</li>
<li>RQ2: Beyond re-ranking, can ColBERT effectively support end-to-end retrieval directly from a large collection?</li>
<li>RQ3: What does each component of ColBERT (e.g., late interaction, query augmentation) contribute to its quality?</li>
<li>RQ4: What are ColBERT‚Äôs indexing-related costs in terms of offline computation and memory overhead?</li>
</ul>
<p>Some training details to prepare the ColBERT retriever: they fine-tune ColBERT models on the MS MARCO and TREC CAR datasets with a learning rate of 3e-6 and a batch size of 32. They fix the number of embeddings per query at 32, meaning that they have 32 tokens per query, and the embedding dimension is 128. The model is trained on a triple of query, positive document and negative document. ColBERT is used to produce a score for each document individually, and is optimized via pairwise softmax cross-entropy loss over the computed scores of the positive and negative document.</p>
<p>Here are two lines from the <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L74-L119C21"><code>train</code> function</a> where you can see the loss method, which is cross-entropy loss, and that the labels are just zeros because the document that is positive is first in the batch item (the zero-eth index):</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> torch.zeros(config.bsize, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>DEVICE)</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.CrossEntropyLoss()(scores, labels[:scores.size(<span class="dv">0</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We‚Äôll now dig into the results of their evaluation of the ColBERT architecture vs.&nbsp;existing methods to address the four research questions listed.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="8.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Table 1"><img src="8.png" class="img-fluid figure-img" alt="Table 1"></a></p>
<figcaption>Table 1</figcaption>
</figure>
</div>
<p>Here are the results for the first type of evaluation where Colbert and other architectures are used to re-rank The top 1000 results produced by BM25, which is full text search. There are three notable takeaways from this table:</p>
<ol type="1">
<li>ColBERT beats non-BERT-based models in terms of retrieval metric MRR@10 and is comparable to BERT models.</li>
<li>ColBERT is three orders of magnitude faster than the more performant BERT models and is comparable in latency to the non-BERT neural rankers.</li>
<li>Except for KNRM, ColBERT requires 11x to 48600x fewer FLOPs per query.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="9.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure 4"><img src="9.png" class="img-fluid figure-img" alt="Figure 4"></a></p>
<figcaption>Figure 4</figcaption>
</figure>
</div>
<p>They also compared ColBERT to BERT-base trained on retrieval. In this comparison, they increased the number of documents considered for re-ranking, calculated the FLOPs required to perform the re-ranking and then calculated the retrieval performance. The purple line at the top shows BERT-base, the green line at the bottom shows ColBERT. ColBERT for each value of k (number of documents reranked) requires fewer FLOPs and is comparable in retrieval performance. Most importantly, ColBERT scales much better than BERT-base as the number of document candidates considered increases from 10 to 2000. ColBERT stays within the same order of magnitude for FLOPs whereas BERT-base FLOPs increase by two orders of magnitude.</p>
<p>Next, we‚Äôll look at their full retrieval results where they retrieve the top 1000 documents from the 8.8 million document MS Marco Corpus.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Table 2"><img src="10.png" class="img-fluid figure-img" alt="Table 2"></a></p>
<figcaption>Table 2</figcaption>
</figure>
</div>
<p>What immediately jumps off the page here is the MRR improvement that ColBERT provides, twice that of the Anserini BM25 method, which is an excellent baseline. However, ColBERT has about 5-8 times the latency of these other methods‚Äìthat could be justifiable given the increased improvement. ColBERT (end-to-end) has the best Recall across all methods.</p>
<p>Next, we‚Äôll look at some of the ablation studies that they performed, which to me are the most exciting results.</p>
</section>
<section id="ablation-studies" class="level2">
<h2 class="anchored" data-anchor-id="ablation-studies">Ablation Studies</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="11.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure 5"><img src="11.png" class="img-fluid figure-img" alt="Figure 5"></a></p>
<figcaption>Figure 5</figcaption>
</figure>
</div>
<p>This figure is really interesting, and there‚Äôs a lot to unpack here. I think it serves as a really comprehensive summary of the main architectural decisions that they‚Äôve made in this work.</p>
<p>Models A through E are used in a re-ranking setting. The first comparison we‚Äôll look at is between model A and model D. Model A is a BERT model, and it takes the CLS token embedding representation for query and document and performs an inner product between them to calculate similarity for re-ranking, which achieves an MRR@10 of about 0.26 which is 6 points fewer than a 5-layer ColBERT model. So, fine grained, token-wise embedding interaction with ColBERT is yielding better results than a single vector interaction with BERT. This is a confirmation of the fundamental concept behind late interaction.</p>
<p>The second comparison is between model B and model D. Model B is using average similarity, and model D is using the MaxSim operator. Model D again has about a 6 point increase in MRR. This validates the second fundamental concept behind late interaction: the MaxSim operator.</p>
<p>The third comparison is between Model C and Model D. In Model C, the query is <strong>not</strong> padded to 32 tokens with <code>[MASK]</code> tokens. Model D uses query augmentation (it pads to 32 with <code>[MASK]</code> tokens) and has a 2 point increase in MRR showing that these <code>[MASK]</code> tokens, which carry semantic meaning in embedding space, improve the model‚Äôs ability to find relevant documents given a query.</p>
<p>The final comparison is between Model E and Model F. Model E is ColBERT used as a re-ranker for the top 1000 documents retrieved by full-text search. Model F is ColBERT used for end-to-end retrieval using a vector similarity index to cluster documents before retrieval. Using ColBERT end-to-end gives another boost to performance.</p>
<p>Something to keep in mind is that BERT requires you to pass in the query and document embedding one pair at a time. For one query, you have to do a thousand forward passes if you have a thousand documents that you want to compare it to. Whereas ColBERT, because of late interaction, can utilize vector similarity indexes because documents are indexed offline, and the interaction calculation is much quicker because you are considering fewer candidate documents‚Äîthe documents that are close to the query token embeddings via the clusters created by the indexing process.</p>
</section>
<section id="indexing-throughpout-footprint" class="level2">
<h2 class="anchored" data-anchor-id="indexing-throughpout-footprint">Indexing Throughpout &amp; Footprint</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="12.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure 6"><img src="12.png" class="img-fluid figure-img" alt="Figure 6"></a></p>
<figcaption>Figure 6</figcaption>
</figure>
</div>
<p>I‚Äôll start by showing figure 6, where it shows that on top of basic ColBERT indexing, adding these optimizations increases the throughput, which means it increases the number of documents that are processed each minute. The two that I‚Äôll highlight here is that length-based bucketing, which we saw in detail, and per-batch maximum sequence length, where they pad all items in the batch to the maximum document length, both improve the throughput.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="13.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Table 4"><img src="13.png" class="img-fluid figure-img" alt="Table 4"></a></p>
<figcaption>Table 4</figcaption>
</figure>
</div>
<p>This table shows the space footprint and MRR@10 for different settings, dimensions, and bytes per dimension. The most space-effcient setting, re-ranking with cosine similarity with 24-dimensional vectors stored as 2-byte floats, which takes up 27 GB, is only 1% worse in MRR@10 than the most space-consuming one which takes up 286 GB.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The body of the ColBERTv1 paper is only about 9 pages long, but it is incredibly information dense. What I thought would be a 1-day foray turned into a 6-day deep dive. I found it helpful to interleave twitter conversations with core concepts from the paper, as casual conversations are often more accessible, and equally impressive as formal work.</p>
<p>As a new canonical ColBERT maintainer I wanted to ground myself in the first principles of late interaction. There are three key elements involved:</p>
<ol type="1">
<li>Independent encoding of queries and documents.</li>
<li>Offline document indexing.</li>
<li>the MaxSim operation.</li>
</ol>
<p>Encoding queries and documents separately allows for offline document indexing, and delays the interaction to the end of the architecture. Offline indexing and MaxSim both unlock pruning in their own ways. Vector-similarity indexes, through clustering, eliminate low-relevance documents from consideration before the interaction takes place. MaxSim eliminates low-relevance tokens during the interaction.</p>
<p>MaxSim is further enhanced by query augmentation, as meaningful <code>[MASK]</code> tokens are introduced in the query to improve the chance of matching relevant terms in the document.</p>
<p>That the most space-efficient setting is only 1% less performant than the most space-consuming setting foreshadows the compression opportunities realized in the PLAID paper.</p>
<p>With these foundations reinforced, I‚Äôll revisit the ColBERTv2 and PLAID papers next, and will continue to concretely witness the concepts at play in the repo‚Äôs codebase.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/vishalbakshi\.github\.io\/blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>