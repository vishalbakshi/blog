<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2025-09-14">
<meta name="description" content="Analysis of ColBERT indexing differences (using bitsandbytes tolerances) between versions where torch.allclose returns False. This analysis also led to multiple deep dives that are linked as separate blog posts.">

<title>Re-evaluating colbert-ai Index Artifacts Between PyTorch Versions with Precision-Based torch.allclose Tolerances – Vishal Bakshi's Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9c1ae87ad5063dce4f793ccd314a7566.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#comparing-all-consecutive-versions" id="toc-comparing-all-consecutive-versions" class="nav-link" data-scroll-target="#comparing-all-consecutive-versions">Comparing All Consecutive Versions</a></li>
  <li><a href="#root-cause-for-index-artifact-difference-between-consecutive-pytorch-versions" id="toc-root-cause-for-index-artifact-difference-between-consecutive-pytorch-versions" class="nav-link" data-scroll-target="#root-cause-for-index-artifact-difference-between-consecutive-pytorch-versions">Root Cause For Index Artifact Difference Between Consecutive PyTorch Versions</a>
  <ul class="collapse">
  <li><a href="#bertmodel-forward-pass-for-any-input_ids" id="toc-bertmodel-forward-pass-for-any-input_ids" class="nav-link" data-scroll-target="#bertmodel-forward-pass-for-any-input_ids">2.0.1 –&gt; 2.1.0: <code>BertModel</code> Forward Pass for Any <code>input_ids</code></a></li>
  <li><a href="#bertmodel-forward-pass-for-some-batch-sizes" id="toc-bertmodel-forward-pass-for-some-batch-sizes" class="nav-link" data-scroll-target="#bertmodel-forward-pass-for-some-batch-sizes">2.4.1 –&gt; 2.5.0: <code>BertModel</code> Forward Pass for Some Batch Sizes</a></li>
  <li><a href="#difference-in-torch.nn.functional.normalize-output" id="toc-difference-in-torch.nn.functional.normalize-output" class="nav-link" data-scroll-target="#difference-in-torch.nn.functional.normalize-output">2.7.1 –&gt; 2.8.0: Difference in <code>torch.nn.functional.normalize</code> Output</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#torch2.0.1-vs-torch2.1.0" id="toc-torch2.0.1-vs-torch2.1.0" class="nav-link" data-scroll-target="#torch2.0.1-vs-torch2.1.0"><code>torch==2.0.1</code> vs <code>torch==2.1.0</code></a></li>
  </ul></li>
  <li><a href="#core-difference-bertmodel-forward-pass" id="toc-core-difference-bertmodel-forward-pass" class="nav-link" data-scroll-target="#core-difference-bertmodel-forward-pass">Core Difference: <code>BertModel</code> Forward Pass</a></li>
  <li><a href="#peculiar-finding-different-intermediate-codes-artifact-yields-identical-final-ivf.pid.pt-artifact" id="toc-peculiar-finding-different-intermediate-codes-artifact-yields-identical-final-ivf.pid.pt-artifact" class="nav-link" data-scroll-target="#peculiar-finding-different-intermediate-codes-artifact-yields-identical-final-ivf.pid.pt-artifact">Peculiar Finding: Different Intermediate <code>codes</code> Artifact Yields Identical Final <code>ivf.pid.pt</code> Artifact</a></li>
  <li><a href="#torch2.4.1-vs-torch2.5.0" id="toc-torch2.4.1-vs-torch2.5.0" class="nav-link" data-scroll-target="#torch2.4.1-vs-torch2.5.0"><code>torch==2.4.1</code> vs <code>torch==2.5.0</code></a>
  <ul class="collapse">
  <li><a href="#final-index-artifacts-1" id="toc-final-index-artifacts-1" class="nav-link" data-scroll-target="#final-index-artifacts-1">Final Index Artifacts</a></li>
  <li><a href="#intermediate-index-artifacts-1" id="toc-intermediate-index-artifacts-1" class="nav-link" data-scroll-target="#intermediate-index-artifacts-1">Intermediate Index Artifacts</a></li>
  </ul></li>
  <li><a href="#core-difference-something-in-bertmodel" id="toc-core-difference-something-in-bertmodel" class="nav-link" data-scroll-target="#core-difference-something-in-bertmodel">Core Difference: Something in <code>BertModel</code></a></li>
  <li><a href="#torch2.7.1-vs-torch2.8.0" id="toc-torch2.7.1-vs-torch2.8.0" class="nav-link" data-scroll-target="#torch2.7.1-vs-torch2.8.0"><code>torch==2.7.1</code> vs <code>torch==2.8.0</code></a>
  <ul class="collapse">
  <li><a href="#final-index-artifacts-2" id="toc-final-index-artifacts-2" class="nav-link" data-scroll-target="#final-index-artifacts-2">Final Index Artifacts</a></li>
  <li><a href="#intermediate-index-artifacts-2" id="toc-intermediate-index-artifacts-2" class="nav-link" data-scroll-target="#intermediate-index-artifacts-2">Intermediate Index Artifacts</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Re-evaluating <code>colbert-ai</code> Index Artifacts Between PyTorch Versions with Precision-Based <code>torch.allclose</code> Tolerances</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ColBERT</div>
  </div>
  </div>

<div>
  <div class="description">
    Analysis of ColBERT indexing differences (using bitsandbytes tolerances) between versions where <code>torch.allclose</code> returns <code>False</code>. This analysis also led to multiple deep dives that are linked as separate blog posts.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 14, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I recently learned that it’s best practice to use different <code>torch.allclose</code> tolerances based on the precision of the floating point value. As a reminder, <code>torch.allclose</code> uses absolute and relative tolerances as follows:</p>
<p><code>∣input_i − other_i∣ ≤ atol + rtol × ∣other_i∣</code></p>
<p><a href="https://github.com/bitsandbytes-foundation/bitsandbytes/blob/39dd8471c1c0677001d0d20ba2218b14bf18fd00/tests/test_optim.py#L189">bitsandbytes</a> uses the following heuristic:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> dtype <span class="op">==</span> torch.float32:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    atol, rtol <span class="op">=</span> <span class="fl">1e-6</span>, <span class="fl">1e-5</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> dtype <span class="op">==</span> torch.bfloat16:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    atol, rtol <span class="op">=</span> <span class="fl">1e-3</span>, <span class="fl">1e-2</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>: <span class="co"># float16</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    atol, rtol <span class="op">=</span> <span class="fl">1e-4</span>, <span class="fl">1e-3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Full-precision (<code>float32</code>) has the lowest tolerance, followed by half-precision (<code>float16</code>) and then <code>bfloat16</code>. I’ve been using default tolerances in all my <code>torch.allclose</code> calls, regardless of precision (<code>atol</code> = <code>1e-08</code>, <code>rtol</code> = <code>1e-05</code>). Comparing these with bitsandbytes’ tolerances, these default tolerances are:</p>
<ul>
<li>float32: 100x smaller for <code>atol</code> and the same for <code>rtol</code></li>
<li>float16: 10_000x smaller for <code>atol</code> and 100x smaller for <code>rtol</code></li>
<li>bfloat16: 100_000x smaller for <code>atol</code> and 1000x smaller for <code>rtol</code></li>
</ul>
<p>As you can see, the bitsandbytes tolerances are much more forgiving for lower precision, which intuitively makes sense.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Two Goals of this Blog Post
</div>
</div>
<div class="callout-body-container callout-body">
<p>The first question I’ll explore in this post: how does changing my <code>torch.allclose</code> tolerances affect index artifact comparison? In other words, are there tensors between PyTorch versions whose difference is larger than <code>atol + rtol × ∣other_i∣</code> when using bitsandbytes’ more forgiving tolerances?</p>
<p>The second question I’ll answer: when <code>torch.allclose</code> fails, what is the root cause?</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>I use the full (69_199 documents) <a href="https://huggingface.co/datasets/UKPLab/dapr">UKPLab/DAPR/ConditionalQA</a> dataset in this exercise. In previous blog posts I used a 1000-document subset.</p>
</div>
</div>
</section>
<section id="comparing-all-consecutive-versions" class="level2">
<h2 class="anchored" data-anchor-id="comparing-all-consecutive-versions">Comparing All Consecutive Versions</h2>
<p>In this section I’ll document tensor shape mismatches and <code>torch.allclose</code> values (with default tolerances in the “Default” column and bitsandbytes tolerances in the “bnb” column) for tensor index artifacts between consecutive PyTorch versions from 1.13.1 (the version pinned in the latest <code>colbert-ai</code> release) to 2.8.0 (the latest PyTorch version available as of 9/14/2025).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">PyTorch Version A</th>
<th style="text-align: center;">PyTorch Version B</th>
<th style="text-align: center;">All Shapes Match</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">bnb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1.13.1</td>
<td style="text-align: center;">2.0.0</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.0.0</td>
<td style="text-align: center;">2.0.1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.0.1</td>
<td style="text-align: center;">2.1.0</td>
<td style="text-align: center;">No (11/12 Match)</td>
<td style="text-align: center;"><code>False</code> (0/12 Match)</td>
<td style="text-align: center;"><code>False</code> (2/12 Match)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.1.0</td>
<td style="text-align: center;">2.1.1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.1.1</td>
<td style="text-align: center;">2.1.2</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.1.2</td>
<td style="text-align: center;">2.2.0</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.2.0</td>
<td style="text-align: center;">2.2.1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.2.1</td>
<td style="text-align: center;">2.2.2</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.2.2</td>
<td style="text-align: center;">2.3.0</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.3.0</td>
<td style="text-align: center;">2.3.1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.3.1</td>
<td style="text-align: center;">2.4.0</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.4.0</td>
<td style="text-align: center;">2.4.1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.4.1</td>
<td style="text-align: center;">2.5.0</td>
<td style="text-align: center;">No (11/12 Match)</td>
<td style="text-align: center;"><code>False</code> (0/12 Match)</td>
<td style="text-align: center;"><code>False</code> (2/12 Match)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.5.0</td>
<td style="text-align: center;">2.5.1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.5.1</td>
<td style="text-align: center;">2.6.0</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.6.0</td>
<td style="text-align: center;">2.7.0</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.7.0</td>
<td style="text-align: center;">2.7.1</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.7.1</td>
<td style="text-align: center;">2.8.0</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;"><code>False</code> (8/12 Match)</td>
<td style="text-align: center;"><code>False</code> (9/12 Match)</td>
</tr>
</tbody>
</table>
<p>In the three version comparisons where <code>torch.allclose</code> failed using default <code>atol</code> and <code>rtol</code> values, using bitsandbytes values yielded the same overall result (not all tensors match) but with two more matches for 2.0.1 –&gt; 2.1.0 and 2.4.1 –&gt; 2.5.0, and one more match for 2.7.1 –&gt; 2.8.0.</p>
<p>Here’s my <code>_close</code> function to handle comparisons between tensors <code>a</code> and <code>b</code>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _close(a, b, default<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    gtype <span class="op">=</span> a.dtype</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gtype <span class="kw">in</span> [torch.uint8, torch.int32, torch.int64]:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> a.shape <span class="op">==</span> b.shape: <span class="cf">return</span> torch.equal(a,b)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> default:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> gtype <span class="op">==</span> torch.float32:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            atol, rtol <span class="op">=</span> <span class="fl">1e-6</span>, <span class="fl">1e-5</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> gtype <span class="op">==</span> torch.bfloat16:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            atol, rtol <span class="op">=</span> <span class="fl">1e-3</span>, <span class="fl">1e-2</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            atol, rtol <span class="op">=</span> <span class="fl">1e-4</span>, <span class="fl">1e-3</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        atol, rtol <span class="op">=</span> <span class="fl">1e-8</span>, <span class="fl">1e-5</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.allclose(a, b, rtol<span class="op">=</span>rtol, atol<span class="op">=</span>atol)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="root-cause-for-index-artifact-difference-between-consecutive-pytorch-versions" class="level2">
<h2 class="anchored" data-anchor-id="root-cause-for-index-artifact-difference-between-consecutive-pytorch-versions">Root Cause For Index Artifact Difference Between Consecutive PyTorch Versions</h2>
<p>There were three consecutive PyTorch versions which broke index artifact reproducibility in <code>colbert-ai</code>. Listed below are the tensors that failed <code>torch.equal</code> (for integers) or <code>torch.allclose</code> (with bitsandbytes’ tolerances):</p>
<ul>
<li>2.0.1 –&gt; 2.1.0
<ul>
<li><code>ivf.pid.pt</code> (<code>ivf</code>: unique passage IDs (pids) per centroid ID, <code>ivf_lengths</code>: number of pids per centroid id)</li>
<li><code>codes.pt</code> (centroid ID mapped to doc token IDs)</li>
<li><code>residuals.pt</code> (distance between centroids and doc token embeddings)</li>
<li><code>centroids.pt</code> (centroids of clustered sample doc token embeddings <code>local_sample_embs</code>)</li>
<li><code>bucket_cutoffs</code> (the quantization bins)</li>
</ul></li>
<li>2.4.1 –&gt; 2.5.0
<ul>
<li><code>ivf.pid.pt</code> (<code>ivf</code> and <code>ivf_lengths</code>)</li>
<li><code>codes.pt</code></li>
<li><code>residuals.pt</code></li>
<li><code>centroids.pt</code></li>
<li><code>bucket_cutoffs</code></li>
</ul></li>
<li>2.7.1 –&gt; 2.8.0
<ul>
<li><code>residuals.pt</code></li>
</ul></li>
</ul>
<p>In the following sections I’ll detail the root cause for index artifact divergence.</p>
<section id="bertmodel-forward-pass-for-any-input_ids" class="level3">
<h3 class="anchored" data-anchor-id="bertmodel-forward-pass-for-any-input_ids">2.0.1 –&gt; 2.1.0: <code>BertModel</code> Forward Pass for Any <code>input_ids</code></h3>
<p>The first critical intermediate indexing tensor created is <a href="https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/collection_indexer.py#L137"><code>local_sample_embs</code></a>. This is a sample of document token embeddings used to calculate centroids. The sample passages are passed to <code>Checkpoint.docFromText</code>, which calls <code>Checkpoint.doc</code>, which ultimately calls <code>Checkpoint.bert</code>.</p>
<p><code>sample_pids</code>, the sample of passage IDs selected for encoding, were identical between 2.0.1 and 2.1.0, but <code>local_sample_embs</code> did not pass <code>torch.allclose</code> (with bnb tolerances). This was the smell that led me to compare the <code>Checkpoint.bert</code> model layer outputs between PyTorch versions using <code>register_forward_hook</code>. I tried a variety of input tokens (different batches of passages, random text, single letter strings) and in all cases, model layer outputs between PyTorch versions failed <code>torch.allclose</code>. I thus concluded that something in PyTorch changed between 2.0.1 and 2.1.0 to cause this. You can read more details of this exploration in <a href="https://vishalbakshi.github.io/blog/posts/2025-09-13-colbert-maintenance/">another blog post</a>.</p>
<p>To confirm that the <code>local_sample_embs</code> divergence caused the divergence in downstream index artifacts, I replaced the <code>local_sample_embs</code> in the <code>torch==2.1.0</code> install with <code>local_sample_embs</code> from the <code>torch==2.0.1</code> install and the final index artifacts passed <code>torch.allclose</code>. Interestingly, even though all final index artifacts were similar, the intermediate <code>codes.pt</code> (centroid ID mapped to doc token IDs) was not. I did a deep dive in <a href="https://vishalbakshi.github.io/blog/posts/2025-09-09-colbert-maintenance/">a separate blog post</a> where I discovered that using <code>Tensor.sort</code> results in different sort indices in <code>torch==2.0.1</code> and <code>torch==2.1.0</code>.</p>
</section>
<section id="bertmodel-forward-pass-for-some-batch-sizes" class="level3">
<h3 class="anchored" data-anchor-id="bertmodel-forward-pass-for-some-batch-sizes">2.4.1 –&gt; 2.5.0: <code>BertModel</code> Forward Pass for Some Batch Sizes</h3>
<p>I saw a similar result when changing the <code>colbert-ai</code> PyTorch version from 2.4.1 to 2.5.0: identical <code>sample_pids</code>, diverging <code>local_sample_embs</code>. In this case, however, not all <code>input_ids</code> caused a divergence between PyTorch versions. Specifically, inputs of the following batch sizes resulted in model layer outputs passing <code>torch.allclose</code>: 71, 72, 70, 73, 68, 66, 115, 64, 63, 62, 61, 67, 69. And the following batches <em>failed</em> <code>torch.allclose</code>: 79, 78, 77, 194, 82, 80, 90, 86, and 83. I concluded that something in PyTorch changed between 2.4.1 and 2.5.0 which made the <code>BertModel</code> forward pass have <em>batch variance</em>. Interestingly, it was at this time that I read the excellent Thinking Machines’ <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/#:~:text=As%20it%20turns%20out%2C%20our%20request%E2%80%99s%20output%20does%20depend%20on%20the%20parallel%20user%20requests.%20Not%20because%20we%E2%80%99re%20somehow%20leaking%20information%20across%20batches%20%E2%80%94%20instead%2C%20it%E2%80%99s%20because%20our%20forward%20pass%20lacks%20%E2%80%9Cbatch%20invariance%E2%80%9D%2C%20causing%20our%20request%E2%80%99s%20output%20to%20depend%20on%20the%20batch%20size%20of%20our%20forward%20pass">blog post about LLM non-determinism</a>.</p>
</section>
<section id="difference-in-torch.nn.functional.normalize-output" class="level3">
<h3 class="anchored" data-anchor-id="difference-in-torch.nn.functional.normalize-output">2.7.1 –&gt; 2.8.0: Difference in <code>torch.nn.functional.normalize</code> Output</h3>
<p>When comparing the 2.7.1 and 2.8.0 index artifacts, all artifacts but <code>residuals.pt</code> passed <code>torch.allclose</code> with bnb tolerances. <code>residuals.pt</code> are the difference between the document token embeddings and the centroids](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/codecs/residual.py#L176):</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>residuals_ <span class="op">=</span> batch <span class="op">-</span> centroids_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>batch</code> not only passes <code>torch.allclose</code> between PyTorch versions, but also passes <code>torch.equal</code>. Whereas <code>centroids_</code> only passes <code>torch.allclose</code>. Looking deeper at how <code>centroids_</code> are calculated, they are <a href="https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/collection_indexer.py#L306-L308">normalized and then stored in half precision</a>. The pre-norm centroids pass <code>torch.equal</code> between PyTorch versions but the post-norm centroids do not. Additionally, testing this on random values, the pre-norm tensors are equal between PyTorch versions but the post-norm tensors are not. You can read more details on this in <a href="https://vishalbakshi.github.io/blog/posts/2025-09-14-colbert-maintenance/">another blog post</a>.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In all three cases, when changing PyTorch versions, <code>colbert-ai</code> indexing functionality does not break, but reproducibility does. To recap the root causes:</p>
<ul>
<li><code>torch==2.0.1</code> –&gt; <code>torch==2.1.0</code>: <code>BertModel</code> forward pass outputs diverge <strong>for any inputs</strong> + <code>Tensor.sort</code> indices order changes.</li>
<li><code>torch==2.4.1</code> –&gt; <code>torch==2.5.0</code>: <code>BertModel</code> forward pass outputs diverge <strong>depending on batch size</strong>.</li>
<li><code>torch==2.7.1</code> –&gt; <code>torch==2.8.0</code>: <code>torch.nn.functional.normalize</code> outputs diverge.</li>
</ul>
<p>I don’t think these root causes can be addressed in the <code>colbert-ai</code> codebase as they seem to be purely PyTorch changes. However, I’m documenting them here (and will link this blog post in the next <code>colbert-ai</code> release notes) as users will experience index artifact changes when using different PyTorch versions.</p>
<p>Next up: comparing and documenting search and training artifacts across PyTorch versions.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<p>In this section I’ll detail final and intermediate index tensor artifact comparisons between PyTorch versions where <code>torch.allclose</code> was <code>False</code> using default tolerances. I’ll also document integer tensor artifacts separately with <code>torch.equal</code> for tensors (which I was embarrassingly until now comparing with <code>torch.allclose</code>, 🤦) and <code>==</code> for non-tensors.</p>
<section id="torch2.0.1-vs-torch2.1.0" class="level3">
<h3 class="anchored" data-anchor-id="torch2.0.1-vs-torch2.1.0"><code>torch==2.0.1</code> vs <code>torch==2.1.0</code></h3>
<section id="final-index-artifacts" class="level4">
<h4 class="anchored" data-anchor-id="final-index-artifacts">Final Index Artifacts</h4>
<p>Using the more lenient bitsandbytes tolerances, <mark><code>avg_residual.pt</code> and <code>bucket_weights.pt</code> pass <code>torch.allclose</code></mark> while <code>bucket_cutoffs</code> and <code>centroids</code> do not.</p>
<section id="integer-tensors" class="level6">
<h6 class="anchored" data-anchor-id="integer-tensors">Integer Tensors</h6>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;"><code>torch.equal</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>codes.pt</code></td>
<td style="text-align: center;">centroid id mapped to doc token embeddings</td>
<td style="text-align: center;"><code>torch.int32</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>residuals.pt</code></td>
<td style="text-align: center;">difference between centroid and doc token embeddings</td>
<td style="text-align: center;"><code>torch.uint8</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>ivf.pid.pt</code> (ivf)</td>
<td style="text-align: center;">unique pids per centroid id</td>
<td style="text-align: center;"><code>torch.int32</code></td>
<td style="text-align: center;"><mark>shape mismatch</mark></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>ivf.pid.pt</code> (ivf_lengths)</td>
<td style="text-align: center;">number of pids per centroid id</td>
<td style="text-align: center;"><code>torch.int64</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
</tbody>
</table>
</section>
<section id="float-tensors" class="level5">
<h5 class="anchored" data-anchor-id="float-tensors">Float Tensors</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">bnb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>avg_residual.pt</code></td>
<td style="text-align: center;">Average difference between centroids and doc token embeddings</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>buckets.pt</code> (<code>bucket_cutoffs</code>)</td>
<td style="text-align: center;">The quantization bins</td>
<td style="text-align: center;"><code>torch.float32</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>buckets.pt</code> (<code>bucket_weights</code>)</td>
<td style="text-align: center;">The quantization values for each bin</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>centroids.pt</code></td>
<td style="text-align: center;">Centroids of clustered sample doc token embeddings</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="intermediate-index-artifacts" class="level4">
<h4 class="anchored" data-anchor-id="intermediate-index-artifacts">Intermediate Index Artifacts</h4>
<p>“Intermediate” artifacts are tensors saved in the middle of the indexing pipeline by adding <code>torch.save</code> calls in <code>/colbert/indexing/collection_indexer.py</code> or <code>/colbert/modeling/checkpoint.py</code>.</p>
<section id="integer-tensors-1" class="level5">
<h5 class="anchored" data-anchor-id="integer-tensors-1">Integer Tensors</h5>
<p>Some of the intermediate artifacts are not tensors so the equality column I’m titling “Equal” instead of <code>torch.equal</code>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Equal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sample_pids.pt</code></td>
<td style="text-align: center;">A sample of passage ids used to calculate centroids</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>num_passages.pt</code></td>
<td style="text-align: center;">Number of sampled passages</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>doclens.pt</code></td>
<td style="text-align: center;">List of number of tokens per document</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
</section>
<section id="float-tensors-1" class="level5">
<h5 class="anchored" data-anchor-id="float-tensors-1">Float Tensors</h5>
<p>Using the more lenient bitsandbytes tolerances, none of the <code>torch.allclose</code> calls pass.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">bnb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>local_sample_embs.pt</code></td>
<td style="text-align: center;">Embeddings of sample document passages used to calculate centroids</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>sample.pt</code></td>
<td style="text-align: center;">95% of the values from <code>local_sample_embs.half()</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>sample_heldout.pt</code></td>
<td style="text-align: center;">5% of the values from <code>local_sample_embs.half()</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>batches.pt</code></td>
<td style="text-align: center;">1 batch of encoded passages</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>D.pt</code></td>
<td style="text-align: center;">sorted and reshaped <code>batches</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
</section>
<section id="core-difference-bertmodel-forward-pass" class="level2">
<h2 class="anchored" data-anchor-id="core-difference-bertmodel-forward-pass">Core Difference: <code>BertModel</code> Forward Pass</h2>
<p>Swapping the <code>local_sample_embs.pt</code> and <code>embs_{chunk_idx}.pt</code> tensors in the <code>torch==2.1.0</code> ColBERT install with the ones generated in the <code>torch==2.0.1</code> install resolves all final index artifacts discrepancies, even when using default tolerances. This led me to uncover that the core difference between 2.0.1 and 2.1.0 is the <code>BertModel</code> forward pass. The intermediate and final <code>BertModel</code> layer outputs all fail <code>torch.allclose</code> (for both sets of tolerances), no matter what the input tokens are (I tried different batch sizes and also a single letter <code>"a"</code>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What does “Swapping” Mean?
</div>
</div>
<div class="callout-body-container callout-body">
<p>“Swapping” means loading the tensor right before it’s saved:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> SWAP <span class="op">==</span> <span class="st">'True'</span>: local_sample_embs <span class="op">=</span> torch.load(<span class="ss">f"</span><span class="sc">{</span>SWAP_ROOT<span class="sc">}</span><span class="ss">/local_sample_embs.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>torch.save(local_sample_embs, <span class="ss">f"</span><span class="sc">{</span>ROOT<span class="sc">}</span><span class="ss">/local_sample_embs.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>torch.save(local_sample_embs.half(), os.path.join(<span class="va">self</span>.config.index_path_, <span class="ss">f'sample.</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>rank<span class="sc">}</span><span class="ss">.pt'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> SWAP <span class="op">==</span> <span class="st">'True'</span>: embs <span class="op">=</span> torch.load(<span class="ss">f"</span><span class="sc">{</span>SWAP_ROOT<span class="sc">}</span><span class="ss">/embs_</span><span class="sc">{</span>chunk_idx<span class="sc">}</span><span class="ss">.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>torch.save(embs, <span class="ss">f"</span><span class="sc">{</span>ROOT<span class="sc">}</span><span class="ss">/embs_</span><span class="sc">{</span>chunk_idx<span class="sc">}</span><span class="ss">.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>torch.save(doclens, <span class="ss">f"</span><span class="sc">{</span>ROOT<span class="sc">}</span><span class="ss">/doclens.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.saver.save_chunk(chunk_idx, offset, embs, doclens) <span class="co"># offset = first passage index in chunk</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="peculiar-finding-different-intermediate-codes-artifact-yields-identical-final-ivf.pid.pt-artifact" class="level2">
<h2 class="anchored" data-anchor-id="peculiar-finding-different-intermediate-codes-artifact-yields-identical-final-ivf.pid.pt-artifact">Peculiar Finding: Different Intermediate <code>codes</code> Artifact Yields Identical Final <code>ivf.pid.pt</code> Artifact</h2>
<p>Even after swapping <code>local_sample_embs.pt</code> and <code>embs</code>, the intermediate <code>codes</code> (not shown in table above) between PyTorch versions did not pass <code>torch.allclose</code> (even with the more lenient bitsandbytes tolerances).</p>
</section>
<section id="torch2.4.1-vs-torch2.5.0" class="level2">
<h2 class="anchored" data-anchor-id="torch2.4.1-vs-torch2.5.0"><code>torch==2.4.1</code> vs <code>torch==2.5.0</code></h2>
<section id="final-index-artifacts-1" class="level3">
<h3 class="anchored" data-anchor-id="final-index-artifacts-1">Final Index Artifacts</h3>
<section id="integer-tensors-2" class="level4">
<h4 class="anchored" data-anchor-id="integer-tensors-2">Integer Tensors</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;"><code>torch.equal</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>codes.pt</code></td>
<td style="text-align: center;">centroid id mapped to doc token embeddings</td>
<td style="text-align: center;"><code>torch.int32</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>residuals.pt</code></td>
<td style="text-align: center;">difference between centroid and doc token embeddings</td>
<td style="text-align: center;"><code>torch.uint8</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>ivf.pid.pt</code> (ivf)</td>
<td style="text-align: center;">unique pids per centroid id</td>
<td style="text-align: center;"><code>torch.int32</code></td>
<td style="text-align: center;"><mark>shapes mismatch</mark></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>ivf.pid.pt</code> (ivf_lengths)</td>
<td style="text-align: center;">number of pids per centroid id</td>
<td style="text-align: center;"><code>torch.int64</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
</tbody>
</table>
</section>
<section id="float-tensors-2" class="level4">
<h4 class="anchored" data-anchor-id="float-tensors-2">Float Tensors</h4>
<p>With bnb tolerances, <code>avg_residual.pt</code> and <code>bucket_weights</code> pass <code>torch.allclose</code> between PyTorch versions.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">bnb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>avg_residual.pt</code></td>
<td style="text-align: center;">Average difference between centroids and doc token embeddings</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>buckets.pt</code> (<code>bucket_cutoffs</code>)</td>
<td style="text-align: center;">The quantization bins</td>
<td style="text-align: center;"><code>torch.float32</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>buckets.pt</code> (<code>bucket_weights</code>)</td>
<td style="text-align: center;">The quantization values for each bin</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>centroids.pt</code></td>
<td style="text-align: center;">Centroids of clustered sample doc token embeddings</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="intermediate-index-artifacts-1" class="level3">
<h3 class="anchored" data-anchor-id="intermediate-index-artifacts-1">Intermediate Index Artifacts</h3>
<section id="integer-tensors-3" class="level4">
<h4 class="anchored" data-anchor-id="integer-tensors-3">Integer Tensors</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Equal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sample_pids.pt</code></td>
<td style="text-align: center;">A sample of passage ids used to calculate centroids</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>num_passages.pt</code></td>
<td style="text-align: center;">Number of sampled passages</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>doclens.pt</code></td>
<td style="text-align: center;">List of number of tokens per document</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
</section>
<section id="float-tensors-3" class="level4">
<h4 class="anchored" data-anchor-id="float-tensors-3">Float Tensors</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">bnb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>local_sample_embs.pt</code></td>
<td style="text-align: center;">Embeddings of sample document passages used to calculate centroids</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>sample.pt</code></td>
<td style="text-align: center;">95% of the values from <code>local_sample_embs.half()</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>sample_heldout.pt</code></td>
<td style="text-align: center;">5% of the values from <code>local_sample_embs.half()</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>batches.pt</code></td>
<td style="text-align: center;">1 batch of encoded passages</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>D.pt</code></td>
<td style="text-align: center;">sorted and reshaped <code>batches</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>batches.pt</code> did not pass <code>torch.allclose</code> for a 1000-document subset as the final batch item had 8 items and .</p>
</div>
</div>
</section>
</section>
</section>
<section id="core-difference-something-in-bertmodel" class="level2">
<h2 class="anchored" data-anchor-id="core-difference-something-in-bertmodel">Core Difference: Something in <code>BertModel</code></h2>
<p>Swapping the <code>local_sample_embs.pt</code> and <code>embs_{chunk_idx}.pt</code> tensors in the <code>torch==2.5.0</code> ColBERT install with the ones generated in the <code>torch==2.4.1</code> install resolves all final <em>and intermediate</em> index artifacts discrepancies, even when using the smaller default tolerances. However, it’s unclear what is causing the divergence in the <code>BertModel</code>.</p>
<p>When sampling and embedding just the first 1000 passages (with <code>checkpoint.bert</code>), <a href="https://vishalbakshi.github.io/blog/posts/2025-08-26-colbert-maintenance/#:~:text=Mixed%20precision%20(,between%20PyTorch%20versions.">the <code>BertModel</code> intermediate dense layer outputs different tensors between PyTorch versions 2.4.1 and 2.5.0 when using mixed precision (for small batch sizes)</a> <mark>this divergence also seems to be related to the number of tokens</mark>. When embedding the full dataset (69_199 passages), the third batch of 1600 passages caused a divergence in <code>BertModel</code> layer outputs.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What does “Swapping” Mean?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>“Swapping” means loading the tensor right before it’s saved:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> SWAP <span class="op">==</span> <span class="st">'True'</span>: local_sample_embs <span class="op">=</span> torch.load(<span class="ss">f"</span><span class="sc">{</span>SWAP_ROOT<span class="sc">}</span><span class="ss">/local_sample_embs.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>torch.save(local_sample_embs, <span class="ss">f"</span><span class="sc">{</span>ROOT<span class="sc">}</span><span class="ss">/local_sample_embs.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>torch.save(local_sample_embs.half(), os.path.join(<span class="va">self</span>.config.index_path_, <span class="ss">f'sample.</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>rank<span class="sc">}</span><span class="ss">.pt'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> SWAP <span class="op">==</span> <span class="st">'True'</span>: embs <span class="op">=</span> torch.load(<span class="ss">f"</span><span class="sc">{</span>SWAP_ROOT<span class="sc">}</span><span class="ss">/embs_</span><span class="sc">{</span>chunk_idx<span class="sc">}</span><span class="ss">.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>torch.save(embs, <span class="ss">f"</span><span class="sc">{</span>ROOT<span class="sc">}</span><span class="ss">/embs_</span><span class="sc">{</span>chunk_idx<span class="sc">}</span><span class="ss">.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>torch.save(doclens, <span class="ss">f"</span><span class="sc">{</span>ROOT<span class="sc">}</span><span class="ss">/doclens.pt"</span>) <span class="co"># ADDED BY VISHAL</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.saver.save_chunk(chunk_idx, offset, embs, doclens) <span class="co"># offset = first passage index in chunk</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="torch2.7.1-vs-torch2.8.0" class="level2">
<h2 class="anchored" data-anchor-id="torch2.7.1-vs-torch2.8.0"><code>torch==2.7.1</code> vs <code>torch==2.8.0</code></h2>
<p>Using the more lenient bitsandbytes tolerances, ALL <code>torch.allclose</code> calls pass. <mark>It’s interesting to note that while <code>centroids.pt</code> (floats) passes <code>torch.allclose</code>, <code>residuals.pt</code> (integers) is not equal across PyTorch versions.</mark></p>
<section id="final-index-artifacts-2" class="level3">
<h3 class="anchored" data-anchor-id="final-index-artifacts-2">Final Index Artifacts</h3>
<section id="integer-tensors-4" class="level4">
<h4 class="anchored" data-anchor-id="integer-tensors-4">Integer Tensors</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;"><code>torch.equal</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>codes.pt</code></td>
<td style="text-align: center;">centroid id mapped to doc token embeddings</td>
<td style="text-align: center;"><code>torch.int32</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>residuals.pt</code></td>
<td style="text-align: center;">difference between centroid and doc token embeddings</td>
<td style="text-align: center;"><code>torch.uint8</code></td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>ivf.pid.pt</code> (ivf)</td>
<td style="text-align: center;">unique pids per centroid id</td>
<td style="text-align: center;"><code>torch.int32</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>ivf.pid.pt</code> (ivf_lengths)</td>
<td style="text-align: center;">number of pids per centroid id</td>
<td style="text-align: center;"><code>torch.int64</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
</section>
<section id="float-tensors-4" class="level4">
<h4 class="anchored" data-anchor-id="float-tensors-4">Float Tensors</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">bnb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>avg_residual.pt</code></td>
<td style="text-align: center;">Average difference between centroids and doc token embeddings</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>buckets.pt</code> (<code>bucket_cutoffs</code>)</td>
<td style="text-align: center;">The quantization bins</td>
<td style="text-align: center;"><code>torch.float32</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>buckets.pt</code> (<code>bucket_weights</code>)</td>
<td style="text-align: center;">The quantization values for each bin</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>centroids.pt</code></td>
<td style="text-align: center;">Centroids of clustered sample doc token embeddings</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>False</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>When using default tolerances, <a href="https://vishalbakshi.github.io/blog/posts/2025-09-02-colbert-maintenance/#inspecting-.half-behavior:~:text=The%20half%20precision%20random%20tensors%20(before%20normalization)%20are%20identical%20between%20torch%20versions%20but%20the%20half%20precision%20normalized%20tensors%20are%20not.">the normalized half-precision centroids cause the floating-point error</a>.</p>
</div>
</div>
</section>
</section>
<section id="intermediate-index-artifacts-2" class="level3">
<h3 class="anchored" data-anchor-id="intermediate-index-artifacts-2">Intermediate Index Artifacts</h3>
<p>All of my intermediate index artifacts pass <code>torch.allclose</code> regardless of which tolerances are used.</p>
<section id="integer-tensors-5" class="level4">
<h4 class="anchored" data-anchor-id="integer-tensors-5">Integer Tensors</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Equal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sample_pids.pt</code></td>
<td style="text-align: center;">A sample of passage ids used to calculate centroids</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>num_passages.pt</code></td>
<td style="text-align: center;">Number of sampled passages</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>doclens.pt</code></td>
<td style="text-align: center;">List of number of tokens per document</td>
<td style="text-align: center;"><code>int</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
</section>
<section id="float-tensors-5" class="level4">
<h4 class="anchored" data-anchor-id="float-tensors-5">Float Tensors</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">Default</th>
<th style="text-align: center;">bnb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>local_sample_embs.pt</code></td>
<td style="text-align: center;">Embeddings of sample document passages used to calculate centroids</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>sample.pt</code></td>
<td style="text-align: center;">95% of the values from <code>local_sample_embs.half()</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>sample_heldout.pt</code></td>
<td style="text-align: center;">5% of the values from <code>local_sample_embs.half()</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>batches.pt</code></td>
<td style="text-align: center;">1 batch of encoded passages</td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>D.pt</code></td>
<td style="text-align: center;">sorted and reshaped <code>batches</code></td>
<td style="text-align: center;"><code>torch.float16</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/vishalbakshi\.github\.io\/blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>