<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2025-07-27">
<meta name="description" content="In this blog post, I walk through the PyTorch 2.0 Release Notes items where I’m estimating there will be some kind of impact to ColBERT, which is currently dependent on torch==1.13.1.">

<title>Analyzing PyTorch 2.0 Release Notes for ColBERT Dependency Impact – Vishal Bakshi’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9c1ae87ad5063dce4f793ccd314a7566.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#full-release-notes-analysis" id="toc-full-release-notes-analysis" class="nav-link" data-scroll-target="#full-release-notes-analysis">Full Release Notes Analysis</a></li>
  <li><a href="#backwards-incompatible-changes" id="toc-backwards-incompatible-changes" class="nav-link" data-scroll-target="#backwards-incompatible-changes">Backwards Incompatible Changes</a>
  <ul class="collapse">
  <li><a href="#pr-92731" id="toc-pr-92731" class="nav-link" data-scroll-target="#pr-92731">PR #92731</a></li>
  <li><a href="#pr-92306" id="toc-pr-92306" class="nav-link" data-scroll-target="#pr-92306">PR #92306</a></li>
  <li><a href="#pr-88913" id="toc-pr-88913" class="nav-link" data-scroll-target="#pr-88913">PR #88913</a></li>
  </ul></li>
  <li><a href="#bug-fixes" id="toc-bug-fixes" class="nav-link" data-scroll-target="#bug-fixes">Bug Fixes</a>
  <ul class="collapse">
  <li><a href="#pr-92810" id="toc-pr-92810" class="nav-link" data-scroll-target="#pr-92810">PR #92810</a></li>
  <li><a href="#pr-92315" id="toc-pr-92315" class="nav-link" data-scroll-target="#pr-92315">PR #92315</a></li>
  <li><a href="#pr-93095" id="toc-pr-93095" class="nav-link" data-scroll-target="#pr-93095">PR #93095</a></li>
  <li><a href="#pr-85596" id="toc-pr-85596" class="nav-link" data-scroll-target="#pr-85596">PR #85596</a></li>
  <li><a href="#pr-86492" id="toc-pr-86492" class="nav-link" data-scroll-target="#pr-86492">PR #86492</a></li>
  <li><a href="#pr-88898" id="toc-pr-88898" class="nav-link" data-scroll-target="#pr-88898">PR #88898</a></li>
  <li><a href="#pr-90149" id="toc-pr-90149" class="nav-link" data-scroll-target="#pr-90149">PR #90149</a></li>
  <li><a href="#prs-86956-86958" id="toc-prs-86956-86958" class="nav-link" data-scroll-target="#prs-86956-86958">PRs #86956, #86958</a></li>
  <li><a href="#prs-94119-86240-91520-94442-94386" id="toc-prs-94119-86240-91520-94442-94386" class="nav-link" data-scroll-target="#prs-94119-86240-91520-94442-94386">PRs #94119, #86240, #91520, #94442, #94386</a></li>
  <li><a href="#prs-91120-94464" id="toc-prs-91120-94464" class="nav-link" data-scroll-target="#prs-91120-94464">PRs #91120, #94464</a></li>
  <li><a href="#pr-94484" id="toc-pr-94484" class="nav-link" data-scroll-target="#pr-94484">PR #94484</a></li>
  <li><a href="#prs-91120-94464-1" id="toc-prs-91120-94464-1" class="nav-link" data-scroll-target="#prs-91120-94464-1">PRs #91120, #94464</a></li>
  <li><a href="#prs-91786-94662" id="toc-prs-91786-94662" class="nav-link" data-scroll-target="#prs-91786-94662">PRs #91786, #94662</a></li>
  <li><a href="#prs-94259-94278-95145-95762-95905" id="toc-prs-94259-94278-95145-95762-95905" class="nav-link" data-scroll-target="#prs-94259-94278-95145-95762-95905">PRs #94259, #94278, #95145, #95762, #95905</a></li>
  <li><a href="#pr-87853" id="toc-pr-87853" class="nav-link" data-scroll-target="#pr-87853">PR #87853</a></li>
  <li><a href="#pr-93322" id="toc-pr-93322" class="nav-link" data-scroll-target="#pr-93322">PR #93322</a></li>
  <li><a href="#pr-89310" id="toc-pr-89310" class="nav-link" data-scroll-target="#pr-89310">PR #89310</a></li>
  <li><a href="#pr-90411" id="toc-pr-90411" class="nav-link" data-scroll-target="#pr-90411">PR #90411</a></li>
  <li><a href="#pr-89759" id="toc-pr-89759" class="nav-link" data-scroll-target="#pr-89759">PR #89759</a></li>
  <li><a href="#pr-86288" id="toc-pr-86288" class="nav-link" data-scroll-target="#pr-86288">PR #86288</a></li>
  <li><a href="#pr-85408" id="toc-pr-85408" class="nav-link" data-scroll-target="#pr-85408">PR #85408</a></li>
  </ul></li>
  <li><a href="#improvements" id="toc-improvements" class="nav-link" data-scroll-target="#improvements">Improvements</a>
  <ul class="collapse">
  <li><a href="#pr-56398" id="toc-pr-56398" class="nav-link" data-scroll-target="#pr-56398">PR #56398</a></li>
  <li><a href="#pr-86309" id="toc-pr-86309" class="nav-link" data-scroll-target="#pr-86309">PR #86309</a></li>
  <li><a href="#pr-87022" id="toc-pr-87022" class="nav-link" data-scroll-target="#pr-87022">PR #87022</a></li>
  <li><a href="#pr-90914" id="toc-pr-90914" class="nav-link" data-scroll-target="#pr-90914">PR #90914</a></li>
  <li><a href="#pr-85926" id="toc-pr-85926" class="nav-link" data-scroll-target="#pr-85926">PR #85926</a></li>
  <li><a href="#pr-91846" id="toc-pr-91846" class="nav-link" data-scroll-target="#pr-91846">PR #91846</a></li>
  <li><a href="#pr-92334" id="toc-pr-92334" class="nav-link" data-scroll-target="#pr-92334">PR #92334</a></li>
  <li><a href="#pr-89137" id="toc-pr-89137" class="nav-link" data-scroll-target="#pr-89137">PR #89137</a></li>
  <li><a href="#pr-90028" id="toc-pr-90028" class="nav-link" data-scroll-target="#pr-90028">PR #90028</a></li>
  <li><a href="#pr-85692" id="toc-pr-85692" class="nav-link" data-scroll-target="#pr-85692">PR #85692</a></li>
  <li><a href="#pr-89172" id="toc-pr-89172" class="nav-link" data-scroll-target="#pr-89172">PR #89172</a></li>
  <li><a href="#pr-91436" id="toc-pr-91436" class="nav-link" data-scroll-target="#pr-91436">PR #91436</a></li>
  <li><a href="#pr-86041-93022" id="toc-pr-86041-93022" class="nav-link" data-scroll-target="#pr-86041-93022">PR #86041, #93022</a></li>
  <li><a href="#pr-93898" id="toc-pr-93898" class="nav-link" data-scroll-target="#pr-93898">PR #93898</a></li>
  <li><a href="#pr-87245" id="toc-pr-87245" class="nav-link" data-scroll-target="#pr-87245">PR #87245</a></li>
  <li><a href="#pr-87343" id="toc-pr-87343" class="nav-link" data-scroll-target="#pr-87343">PR #87343</a></li>
  <li><a href="#pr-84789" id="toc-pr-84789" class="nav-link" data-scroll-target="#pr-84789">PR #84789</a></li>
  <li><a href="#pr-86218" id="toc-pr-86218" class="nav-link" data-scroll-target="#pr-86218">PR #86218</a></li>
  <li><a href="#pr-91884" id="toc-pr-91884" class="nav-link" data-scroll-target="#pr-91884">PR #91884</a></li>
  <li><a href="#pr-91734" id="toc-pr-91734" class="nav-link" data-scroll-target="#pr-91734">PR #91734</a></li>
  <li><a href="#pr-94639" id="toc-pr-94639" class="nav-link" data-scroll-target="#pr-94639">PR #94639</a></li>
  <li><a href="#pr-91576" id="toc-pr-91576" class="nav-link" data-scroll-target="#pr-91576">PR #91576</a></li>
  </ul></li>
  <li><a href="#deprecations" id="toc-deprecations" class="nav-link" data-scroll-target="#deprecations">Deprecations</a>
  <ul class="collapse">
  <li><a href="#pr-92143" id="toc-pr-92143" class="nav-link" data-scroll-target="#pr-92143">PR #92143</a></li>
  </ul></li>
  <li><a href="#performance" id="toc-performance" class="nav-link" data-scroll-target="#performance">Performance</a>
  <ul class="collapse">
  <li><a href="#pr-93234" id="toc-pr-93234" class="nav-link" data-scroll-target="#pr-93234">PR #93234</a></li>
  <li><a href="#pr-84981" id="toc-pr-84981" class="nav-link" data-scroll-target="#pr-84981">PR #84981</a></li>
  <li><a href="#pr-94034" id="toc-pr-94034" class="nav-link" data-scroll-target="#pr-94034">PR #94034</a></li>
  <li><a href="#pr-86568" id="toc-pr-86568" class="nav-link" data-scroll-target="#pr-86568">PR #86568</a></li>
  <li><a href="#pr-92300" id="toc-pr-92300" class="nav-link" data-scroll-target="#pr-92300">PR #92300</a></li>
  <li><a href="#pr-91114" id="toc-pr-91114" class="nav-link" data-scroll-target="#pr-91114">PR #91114</a></li>
  </ul></li>
  <li><a href="#closing-thoughts" id="toc-closing-thoughts" class="nav-link" data-scroll-target="#closing-thoughts">Closing Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Analyzing PyTorch 2.0 Release Notes for ColBERT Dependency Impact</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ColBERT</div>
  </div>
  </div>

<div>
  <div class="description">
    In this blog post, I walk through the PyTorch 2.0 Release Notes items where I’m estimating there will be some kind of impact to ColBERT, which is currently dependent on torch==1.13.1.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 27, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post, I walk through the <a href="https://github.com/pytorch/pytorch/releases/tag/v2.0.0">PyTorch 2.0 Release Note</a> PRs where I’m estimating there will be some kind of impact to ColBERT as I update the torch dependency to 2.0 (ColBERT is currently dependent on torch==1.13.1). The level of detail in my analysis of a PyTorch PR is not necessarily signifying its importance. In some cases, I am using this analysis as an opportunity to get more familiar with details about the ColBERT codebase (such as the number of instances where <code>torch.cat</code> is used).</p>
</section>
<section id="full-release-notes-analysis" class="level2">
<h2 class="anchored" data-anchor-id="full-release-notes-analysis">Full Release Notes Analysis</h2>
<p>You can find my item-by-item PyTorch 2.0 release notes analysis for ColBERT in <a href="https://docs.google.com/spreadsheets/d/1sUEN7xo5-hLVoxF9NL_ibGxPaKlPzxmnMU46zf3wd-U/edit?usp=sharing">this Google Sheet</a>.</p>
<p>Overall, across 508 PyTorch PRs, I have estimated that 455 of them are not applicable to ColBERT and 42 (8.7%) have a potential impact. I was unclear if or how 11 of the PyTorch 2.0 PRs would affect ColBERT (2.6%).</p>
<p>There are 5 sections in the PyTorch 2.0 Release Notes, here’s a break down of PRs by section that will have a potential (or unclear) impact on ColBERT:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Section</th>
<th style="text-align: center;"># of PRs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Improvements</td>
<td style="text-align: center;">22</td>
</tr>
<tr class="even">
<td style="text-align: center;">Bug Fixes</td>
<td style="text-align: center;">21</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;">Backwards Incompatible Changes</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Deprecations</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>In my estimation, the improvements and bug fixes PRs in PyTorch 2.0 will only improve the performance of ColBERT. That being said, there still may be noticeable differences in indexing, search, and training artifacts which may break tests I write for before/after comparisons.</p>
<p>Fortunately, only two backward-compatible changes may affect ColBERT.</p>
<p>There are 11 subsections in the PyTorch 2.0 Release Notes, here’s a break down of PRs that will have a potential (or unclear) impact on ColBERT:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Subsection</th>
<th style="text-align: center;"># of PRs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">MPS</td>
<td style="text-align: center;">12</td>
</tr>
<tr class="even">
<td style="text-align: center;">Python API</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cuda</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td style="text-align: center;">Releng</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Distributed</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">Build</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ONNX</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td style="text-align: center;">Cpu</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cpp API</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">torch.nn API</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>I am including MPS-related PRs in this analysis just in case we consider making ColBERT compatible with MPS in the future.</p>
<p>I’ll start by analyzing breaking changes, which are likely going to be the most impactful.</p>
</section>
<section id="backwards-incompatible-changes" class="level2">
<h2 class="anchored" data-anchor-id="backwards-incompatible-changes">Backwards Incompatible Changes</h2>
<section id="pr-92731" class="level3">
<h3 class="anchored" data-anchor-id="pr-92731">PR <a href="https://github.com/pytorch/pytorch/pull/92731">#92731</a></h3>
<blockquote class="blockquote">
<p>Gradients are now set to None instead of zeros by default in <code>torch.optim.*.zero_grad()</code> and <code>torch.nn.Module.zero_grad()</code> (#92731)</p>
</blockquote>
<p>There are two lines in ColBERT where<code>zero_grad</code> is called: in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L37">colbert/utils/amp.py</a> and in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L61">colbert/training/training.py</a>. I’m not sure how this change would affect ColBERT behavior, but flagging it as something to keep in mind.</p>
</section>
<section id="pr-92306" class="level3">
<h3 class="anchored" data-anchor-id="pr-92306">PR <a href="https://github.com/pytorch/pytorch/pull/92306">#92306</a></h3>
<blockquote class="blockquote">
<p>Algorithms <code>{Adadelta, Adagrad, Adam, Adamax, AdamW, ASGD, NAdam, RAdam, RMSProp, RProp, SGD}</code> default to faster <code>foreach</code> implementation when on CUDA + differentiable=<code>False</code></p>
</blockquote>
<p>This PR adds the following lines to <code>AdamW</code>, which is used in ColBERT’s <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L60">`training.py</a>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> foreach <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    foreach <span class="op">=</span> _default_to_foreach(</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        [params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps],</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        differentiable<span class="op">=</span>differentiable)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>foreach</code> is <code>None</code> in ColBERT, as it’s not specified and that’s what it defaults to (<code>foreach: Optional[bool] = None</code>):</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> AdamW(<span class="bu">filter</span>(<span class="kw">lambda</span> p: p.requires_grad, colbert.parameters()), lr<span class="op">=</span>config.lr, eps<span class="op">=</span><span class="fl">1e-8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Since this is described as a “faster implementation”, I would expect the training time to decrease. <mark>I’ll be on the lookout for this when comparing training time benchmarks before/after upgrading to PyTorch 2.0</mark>.</p>
</section>
<section id="pr-88913" class="level3">
<h3 class="anchored" data-anchor-id="pr-88913">PR <a href="https://github.com/pytorch/pytorch/pull/88913">#88913</a></h3>
<blockquote class="blockquote">
<p>Update <code>torch.tensor</code> and <code>nn.Parameter</code> to serialize all their attributes (#88913)</p>
</blockquote>
<p><mark>It’s unclear what this PR is doing but since it’s touching the <code>nn.Parameter</code> definition, I’m flagging it.</mark></p>
</section>
</section>
<section id="bug-fixes" class="level2">
<h2 class="anchored" data-anchor-id="bug-fixes">Bug Fixes</h2>
<p>I would expect PyTorch PRs that introduce bug fixes to only positively affect ColBERT. That being said, a positive effect is still a change and can potentially impact concrete artifacts during indexing, search and training. I am planning on curating a baseline set of these artifacts before I test the upgrade to PyTorch 2.0.</p>
<section id="pr-92810" class="level3">
<h3 class="anchored" data-anchor-id="pr-92810">PR <a href="https://github.com/pytorch/pytorch/pull/92810">#92810</a></h3>
<blockquote class="blockquote">
<p>Fix SIGSEGV on a big-endian machine when reading pickle data (#92810)</p>
</blockquote>
<p>The PR states:</p>
<blockquote class="blockquote">
<p>This PR fixes SIGSEGV on a big-endian machine when reading pickle data.</p>
</blockquote>
<p>I’m not familiar with the term “big-endian” so had to look it up:</p>
<blockquote class="blockquote">
<p>A big-endian system stores the most significant byte of a word at the smallest memory address and the least significant byte at the largest. A little-endian system, in contrast, stores the least-significant byte at the smallest address. (<a href="https://en.wikipedia.org/wiki/Endianness">source</a>)</p>
</blockquote>
<p>Claude’s understanding of the cpp method affected by this PR is that it affects the <code>torch.load</code> method. There are a number of ColBERT files that use <code>torch.load</code>:</p>
<ul>
<li>colbert/utils/coalesce.py uses it to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L47">load <code>codes.pt</code></a> (centroid id for each embedding in chunk) and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L66">load <code>residuals.pt</code></a> (16-bits residual for each embedding in chunk).</li>
<li>colbert/search/index_loader.py uses it to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_loader.py#L33">load <code>ivf.pid.pt</code></a> or <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_loader.py#L36"><code>ivf.pt</code></a>.</li>
<li>colbert/utils/utils.py uses it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L44">in <code>torch_load_dnn</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L91"><code>load_checkpoint_raw</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L205"><code>load_ranking</code></a>.</li>
<li>colbert/indexing/index_manager.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/index_manager.py#L20"><code>load_index_part</code></a>.</li>
<li>colbert/indexing/codecs/residual_embeddings.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual_embeddings.py#L86"><code>load_codes</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual_embeddings.py#L93"><code>load_residuals</code></a>.</li>
<li>colbert/indexing/codecs/residual.py uses it in <code>load</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L141">centroids</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L142"><code>avg_residual</code></a>, and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L143"><code>bucket_cutoffs, bucket_weights</code></a>.</li>
<li>colbert/indexing/collection_indexer.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L256"><code>_concatenate_and_split_sample</code></a>.</li>
<li>colbert/index_updater.py uses it in <code>_load_disk_ivf</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L281"><code>ivf.pid.pt</code></a> or <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L286"><code>ivf.pt</code></a>, in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L312"><code>load_chunk_codes</code></a>, and in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L316"><code>_load_chunk_residuals</code></a>.</li>
<li>colbert/tests/index_coalesce_test.py uses it to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L57">multi-file <code>codes.pt</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L66">single-file <code>codes.pt</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L83">multi-file <code>residuals.pt</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L92">single-file <code>residuals.pt</code></a>.</li>
</ul>
</section>
<section id="pr-92315" class="level3">
<h3 class="anchored" data-anchor-id="pr-92315">PR <a href="https://github.com/pytorch/pytorch/pull/92315">#92315</a></h3>
<blockquote class="blockquote">
<p>Fix NVML visible device parsing (#92315)</p>
</blockquote>
<blockquote class="blockquote">
<p>CUDA_VISIBLE_DEVICES can contain either ordinals or UUIDs Extend the logic to be able to parse it by UUID</p>
</blockquote>
<p><mark>I don’t think this would affect any artifacts created during indexing/searching/training but would make it easier for PyTorch to identify GPUs.</mark></p>
</section>
<section id="pr-93095" class="level3">
<h3 class="anchored" data-anchor-id="pr-93095">PR <a href="https://github.com/pytorch/pytorch/pull/93095">#93095</a></h3>
<p>This PR fixes an error in <a href="https://github.com/pytorch/pytorch/issues/93006">#93006</a> when using <code>topk</code>, which is used in the following places in ColBERT:</p>
<ul>
<li><code>get_cells</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L17">colbert/search/candidate_generation.py</a></li>
<li><code>score_pids</code> in colbert/search/index_storage.py to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L127">filter centroids by the threshold</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L152">filter <code>pids</code> using pruned centroid scores</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L161">filter <code>pids</code> using full centroid scores</a></li>
<li>filter scores in <code>colbert_score_reduce</code> for the <code>"flipr"</code> interaction method: <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L146">link1</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L150">link 2</a></li>
</ul>
<p>Claude recommended also consider uses of <code>max</code> and <code>argmax</code> to be potentially impacted:</p>
<ul>
<li><code>get_cells</code> if <code>ncells==1</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L15">colbert/search/candidate_generation.py</a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L121"><code>ColBERT.score</code></a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L135"><code>colbert_score_reduce</code></a></li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCodec.compress_into_codes</code> on <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">GPU</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">CPU</a></li>
<li>colbert/search/strided_tensor.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L74">`StridedTensor.lookup</a></li>
<li>colbert/search/strided_tensor_core.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L27"><code>StridedTensorCore.__init__</code></a></li>
<li>colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L215">`Checkpoint.score</a></li>
<li>colbert/indexing/utils.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L48"><code>optimize_ivf</code></a></li>
</ul>
<p><mark>I would assume that the only impact this PR would have on PyTorch is avoiding any errors during the use of <code>topk</code> (no such errors have been reported on in the open issues).</mark></p>
</section>
<section id="pr-85596" class="level3">
<h3 class="anchored" data-anchor-id="pr-85596">PR <a href="https://github.com/pytorch/pytorch/pull/85596">#85596</a></h3>
<blockquote class="blockquote">
<p>Fix: half reduction with multiple sub-iterators (#85596)</p>
</blockquote>
<p>Fixes <a href="https://github.com/pytorch/pytorch/issues/74438">cuda low-precision reductions on large tensors produce wrong results #74438</a>:</p>
<blockquote class="blockquote">
<p>Reductions with low precision inputs (half, bfloat16) that need sub-iterators accumulate directly in output and thus truncate intermediate results</p>
</blockquote>
<p>This would fix any issues related to the use of <code>half</code> in the following ColBERT files:</p>
<ul>
<li>in <code>ResidualCodec</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L27"><code>centroids</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L35"><code>avg_residual</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L40"><code>bucket_weights</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L159">saving <code>centroids</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L172">compressing token embeddings</a>, calculating cosine similarity between token embeddings and centroids <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">on GPU</a>.</li>
<li>colbert/search/candidate_generation.py in <code>CandidateGeneration.generate_candidates</code> for queries when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L52">using the GPU</a>.</li>
<li>colbert/indexing/collection_indexer.py in <code>CollectionIndexer._sample_embeddings</code> when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L181">saving <code>local_sample_embs</code></a>, in <code>CollectionIndexer.train_kmeans</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L308">centroids on the GPU</a>, and in <code>CollectionIndexer.index</code> when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L370">saving token embeddings</a>.</li>
<li>colbert/modeling/colbert.py in <code>ColBERT.doc</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L106">document token embeddings on the GPU</a></li>
</ul>
<p><mark>If this PR fix is relevant to the use of <code>half</code> in the above files I would expect there to be numeric differences in indexing/search artifacts</mark>.</p>
</section>
<section id="pr-86492" class="level3">
<h3 class="anchored" data-anchor-id="pr-86492">PR <a href="https://github.com/pytorch/pytorch/pull/86492">#86492</a></h3>
<blockquote class="blockquote">
<p>Fixes a memory leak by making autocast cache global instead of thread-local (#86492)</p>
</blockquote>
<p>This PR adds a PyTorch test which:</p>
<blockquote class="blockquote">
<p>Verifies that the autocast cache is global. This is done by mocking out cache clearing at the end of the forward pass, running forward+backward with an explicit call to autocast in the backward, and verifying that the weight only get cast to float16 once.</p>
</blockquote>
<p>Claude’s analysis:</p>
<blockquote class="blockquote">
<p>This PyTorch enhancement directly benefits ColBERT. By making the autocast cache global, this PR provides a performance improvement when training ColBERT with mixed precision. It reduces redundant computations during the backward pass, leading to faster and more efficient training without changing the model’s functionality.</p>
</blockquote>
<p>ColBERT uses <code>torch.cuda.amp.autocast</code> in the following files:</p>
<ul>
<li>colbert/utils/amp.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L15"><code>MixedPrecisionManager.context</code></a> which is used in colbert/training/training.py during <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L96"><code>train</code></a>, and in colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L87"><code>Checkpoint.query</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L93"><code>Checkpoint.doc</code></a> to calculate query and document token embeddings, respectively.</li>
<li>colbert/distillation/scorer in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/distillation/scorer.py#L48"><code>Scorer._score_pairs</code></a>.</li>
</ul>
</section>
<section id="pr-88898" class="level3">
<h3 class="anchored" data-anchor-id="pr-88898">PR <a href="https://github.com/pytorch/pytorch/pull/88898">#88898</a></h3>
<p>Fixes PyTorch <a href="https://github.com/pytorch/pytorch/issues/88873">#88873</a>:</p>
<blockquote class="blockquote">
<p>torch_extension.py should be fixed or ninja compile will fail.</p>
</blockquote>
<p>Gemini’s analysis: Because ColBERT uses the very feature this PR is fixing (torch.utils.cpp_extension.py), the change is directly relevant. This bug fix is important for any developer or user who needs to compile and run ColBERT on a Windows machine. It ensures that ColBERT’s performance-critical custom CUDA code can be built correctly, preventing potential compilation errors.</p>
<p>ColBERT uses <code>torch.utils.cpp_extension</code> in the following files:</p>
<ul>
<li>colbert/modeling/colbert.py in <code>ColBERT.try_load_torch_extensions</code> to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L12">load <code>segmented_lookup.cpp</code></a> on CPU.</li>
<li>colbert/search/index_storage.py in <code>IndexScorer.try_load_torch_extensions</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L38">filter_pids.cpp</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L51">decompress_residuals.cpp</a>.</li>
<li>colbert/search/strided_tensor.py in <code>StridedTensor.try_load_torch_extensions</code> to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L26">load <code>segmented_lookup.cpp</code></a> on CPU.</li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCode.try_load_torch_extensions</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L103">decompress_residuals.cpp</a></li>
</ul>
<p><mark>This might be related to ColBERT <a href="https://github.com/stanford-futuredata/ColBERT/issues/371">#317</a></mark></p>
</section>
<section id="pr-90149" class="level3">
<h3 class="anchored" data-anchor-id="pr-90149">PR <a href="https://github.com/pytorch/pytorch/pull/90149">#90149</a></h3>
<blockquote class="blockquote">
<p>Fix a static initialization order fiasco in c10d (#90149)</p>
</blockquote>
<p>Gemini’s analysis: Because ColBERT’s multi-GPU functionality is built directly on the PyTorch library that this PR is fixing, this change is highly relevant. This is a crucial stability improvement that makes ColBERT’s distributed training and inference more reliable by preventing potential crashes at startup.</p>
<p><mark>If Gemini’s analysis is correct, this will make ColBERT’s multi-GPU functionality more reliable and might address related open issues.</mark></p>
</section>
<section id="prs-86956-86958" class="level3">
<h3 class="anchored" data-anchor-id="prs-86956-86958">PRs <a href="https://github.com/pytorch/pytorch/pull/86956">#86956</a>, <a href="https://github.com/pytorch/pytorch/pull/86958">#86958</a></h3>
<blockquote class="blockquote">
<p>Fix issues with non-contiguous Tensor handling (#86956, #86958)</p>
</blockquote>
<p><mark>These are both MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS</mark></p>
</section>
<section id="prs-94119-86240-91520-94442-94386" class="level3">
<h3 class="anchored" data-anchor-id="prs-94119-86240-91520-94442-94386">PRs <a href="https://github.com/pytorch/pytorch/pull/94119">#94119</a>, <a href="https://github.com/pytorch/pytorch/pull/86240">#86240</a>, <a href="https://github.com/pytorch/pytorch/pull/91520">#91520</a>, <a href="https://github.com/pytorch/pytorch/pull/94442">#94442</a>, <a href="https://github.com/pytorch/pytorch/pull/94386">#94386</a></h3>
<blockquote class="blockquote">
<p>Fix issues with ops implementation torch.median (#90326, #88807), torch.{std,var} correction argument (#91203), torch.index_select (#94117, #91064), torch.cumsum (#94119), torch.where (#86240), torch.nn.Embedding (#82809), torch.nn.Softplus (#88555), torch.nn.functional.pad (#89864), torch.max (#91520), padding functions (#91522), torch.nn.functional.upsample (#91669), pooling functions (#91519, #94348), torch.nn.{NLLLoss,SmoothL1Loss} (#94226), torch.nn.SoftPlus (#94256), torch.masked_fill (#94263), torch.fill_ (#94479), torch.median (#94489), torch.nonzero (#94442), torch.nn.BatchNorm (#94351), torch.{min,max} (#94386), torch.nn.GELU (#94529), torch.nn.LSTM (#94889), #95137),torch.nn.Conv2d(#95078),torch.nn.functional.bilinear(#94892),torch.copy_ (#95272),torch.max_pool2d(#94963),torch.div (#95769)</p>
</blockquote>
<p>ColBERT uses topk in the following files:</p>
<ul>
<li><code>get_cells</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L17">colbert/search/candidate_generation.py</a></li>
<li><code>score_pids</code> in colbert/search/index_storage.py to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L127">filter centroids by the threshold</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L152">filter <code>pids</code> using pruned centroid scores</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L161">filter <code>pids</code> using full centroid scores</a></li>
<li>filter scores in <code>colbert_score_reduce</code> for the <code>"flipr"</code> interaction method: <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L146">link1</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L150">link 2</a></li>
</ul>
<p>ColBERT uses max/argmax in the following files:</p>
<ul>
<li><code>get_cells</code> if <code>ncells==1</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L15">colbert/search/candidate_generation.py</a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L121"><code>ColBERT.score</code></a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L135"><code>colbert_score_reduce</code></a></li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCodec.compress_into_codes</code> on <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">GPU</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">CPU</a></li>
<li>colbert/search/strided_tensor.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L74">`StridedTensor.lookup</a></li>
<li>colbert/search/strided_tensor_core.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L27"><code>StridedTensorCore.__init__</code></a></li>
<li>colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L215">`Checkpoint.score</a></li>
<li>colbert/indexing/utils.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L48"><code>optimize_ivf</code></a></li>
</ul>
<p>ColBERT uses <code>torch.cumsum</code> in the following files to calculate <code>offsets</code>.:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L50">colbert/indexing/utils.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L31">colbert/search/strided_tensor_core.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L48">colbert/search/strided_tensor.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L68">colbert/search/index_storage.py</a></li>
</ul>
<p>ColBERT uses <code>torch.where</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L49">colbert/modeling/checkpoint.py</a> to pool embeddings within each cluster.</li>
</ul>
<p>ColBERT uses <code>torch.nonzero</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L351">colbert/index_updater.py</a> to construct mask of where pids to be removed appear in ivf.</li>
</ul>
<p><mark>These are all MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91120-94464" class="level3">
<h3 class="anchored" data-anchor-id="prs-91120-94464">PRs <a href="https://github.com/pytorch/pytorch/pull/91120">#91120</a>, <a href="https://github.com/pytorch/pytorch/pull/94464">#94464</a></h3>
<blockquote class="blockquote">
<p>Fix issues with torch.bool for Unary ops (#91120), scatter ops (#94464)</p>
</blockquote>
<p>Claude’s analysis: Claude: The PR fixes compatibility issues where boolean tensors needed to be cast to int8 on older macOS versions, then cast back. This would be important for ColBERT’s masking operations which rely heavily on boolean tensors for attention and padding masks.</p>
<p><mark>These are MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-94484" class="level3">
<h3 class="anchored" data-anchor-id="pr-94484">PR <a href="https://github.com/pytorch/pytorch/pull/94484">#94484</a></h3>
<blockquote class="blockquote">
<p>Properly cast torch.int64 to torch.int32 for reduction ops and raise warning. (#94484)</p>
</blockquote>
<p>Claude’s analysiss: The PR changes TORCH_CHECK (which throws an error) to TORCH_WARN_ONCE (which just warns) and automatically casts int64 to int32 for min/max operations. This would allow ColBERT to run on MPS with int64 tensors instead of failing, though with potential precision loss.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91120-94464-1" class="level3">
<h3 class="anchored" data-anchor-id="prs-91120-94464-1">PRs <a href="https://github.com/pytorch/pytorch/pull/91197">#91120</a>, <a href="https://github.com/pytorch/pytorch/pull/91514">#94464</a></h3>
<blockquote class="blockquote">
<p>Fix handling of ops taking multiple dtypes as input (#91197, #91514)</p>
</blockquote>
<p>Claude’s analysis: The PR fixes MPS scatter to handle type mismatches between source and destination tensors automatically.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91786-94662" class="level3">
<h3 class="anchored" data-anchor-id="prs-91786-94662">PRs <a href="https://github.com/pytorch/pytorch/pull/91786">#91786</a>, <a href="https://github.com/pytorch/pytorch/pull/94662">#94662</a></h3>
<blockquote class="blockquote">
<p>Fix handling of channels last for torch.cat (#91786, #94662), torch.Conv2d (#91822, #94384), torch.nn.{ELU,ReLU,Hardswish} (#94664), torch.nn.BatchNorm (#94760), torch.nn.MaxPool2d (#94877)</p>
</blockquote>
<p>ColBERT uses <code>.cat</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L45">colbert/indexing/utils.py</a> to concatenate a list of tensors (<code>unique_pids_per_centroid</code>) into a single tensor (<code>ivf</code>).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/index_manager.py#L23">colbert/indexing/index_manager.py</a> to concatenate multiple path names.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L31"></a> to calculate <code>offsets</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L39"></a> to add padding to a tensor.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/distillation/scorer.py#L60"></a> to concatenate <code>scores</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_encoder.py#L38"></a> to concatenate document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L181"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L182"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L220"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L237"></a> to concatenate <code>centroids</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L276"></a> to concatenate document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L51"></a> to concatenate <code>packed_tensor</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L152"></a> to concatenate <code>all_orders</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L155"></a> to concatenate <code>all_lengths</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L101"></a> to concatenate <code>compressed_embs.codes</code> (centroid IDs corresponding to document token embeddings).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L108"></a> to concatenate <code>compressed_embs.residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L117"></a> to concatenate <code>doclens</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L431"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L434"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L506"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L507"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L115"></a> to concatenate <code>batches</code> (of queries).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L168"></a> to concatenate document token embeddings (in order).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L169"></a> to concatenate <code>mask</code> for document token embeddings (in order).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L48"></a> to concatenate <code>code</code> chunks.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L69"></a> to concatenate <code>offsets</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L149"></a> to concatenate <code>approx_scores</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/query_tokenization.py#L88"></a> to concatenate <code>ids</code> (for query tokens).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/query_tokenization.py#L89"></a> to concatenate <code>masks</code> (for query tokens).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/utils.py#L72"></a> to concatenate prefix token.</li>
</ul>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-94259-94278-95145-95762-95905" class="level3">
<h3 class="anchored" data-anchor-id="prs-94259-94278-95145-95762-95905">PRs <a href="https://github.com/pytorch/pytorch/pull/94259">#94259</a>, <a href="https://github.com/pytorch/pytorch/pull/94278">#94278</a>, <a href="https://github.com/pytorch/pytorch/pull/95145">#95145</a>, <a href="https://github.com/pytorch/pytorch/pull/95762">#95762</a>, <a href="https://github.com/pytorch/pytorch/pull/95905">#95905</a></h3>
<blockquote class="blockquote">
<p>Fix view operations handling (#94259, #94278,#95145, #95762, #95905)</p>
</blockquote>
<p>Claude’s analysis: This PR fixes crashes in view operations when slicing with incorrect lengths, which ColBERT uses for tensor reshaping and indexing operations.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-87853" class="level3">
<h3 class="anchored" data-anchor-id="pr-87853">PR <a href="87853">#87853</a></h3>
<blockquote class="blockquote">
<p>Move incorrectly placed closing curly brace of extern “C” block (#87853)</p>
</blockquote>
<p>Gemini’s analysis: This pull request is a foundational C++ correctness fix for the PyTorch framework. Because ColBERT compiles its own C++ extensions that depend on these core headers, this change is directly beneficial. It ensures the stability and reliability of ColBERT’s own build process, preventing potential compilation failures.</p>
<p>ColBERT’s C++ extensions:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/modeling/segmented_maxsim.cpp">segmented_maxsim.cpp</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/search/filter_pids.cpp">filter_pids.cpp</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/search/decompress_residuals.cpp">decompress_residuals.cpp</a></li>
</ul>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-93322" class="level3">
<h3 class="anchored" data-anchor-id="pr-93322">PR <a href="https://github.com/pytorch/pytorch/pull/93322">#93322</a></h3>
<blockquote class="blockquote">
<p>Fix MSVC compiler error in basic_ops.h (#93322)</p>
</blockquote>
<p>Gemini’s take: This pull request is a crucial build-system and correctness fix. It directly impacts ColBERT by ensuring that its custom C++ code can be compiled successfully on Windows machines that use the affected MSVC compiler. Without this fix, users in that environment would be unable to run ColBERT. This change makes ColBERT’s build process more robust and widens its platform compatibility.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
<section id="pr-89310" class="level3">
<h3 class="anchored" data-anchor-id="pr-89310">PR <a href="https://github.com/pytorch/pytorch/pull/89310">#89310</a></h3>
<blockquote class="blockquote">
<p>Fix a bug that redefines __STDC_FORMAT_MACROS (#89310)</p>
</blockquote>
<p>Gemini’s take: This pull request provides a stability and correctness fix to the underlying PyTorch framework. Because ColBERT compiles its own C++ code that depends on these core PyTorch headers, this change is directly beneficial. It makes ColBERT’s own compilation process more reliable and prevents a potential class of build failures.</p>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-90411" class="level3">
<h3 class="anchored" data-anchor-id="pr-90411">PR <a href="https://github.com/pytorch/pytorch/pull/90411">#90411</a></h3>
<blockquote class="blockquote">
<p>Add manual cuda deps search logic (#90411)</p>
</blockquote>
<p>Gemini’s take: This PyTorch pull request adds a new mechanism to help PyTorch find its essential CUDA libraries (cuBLAS and cuDNN) on Linux systems.</p>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-89759" class="level3">
<h3 class="anchored" data-anchor-id="pr-89759">PR <a href="https://github.com/pytorch/pytorch/pull/89759">#89759</a></h3>
<blockquote class="blockquote">
<p>Workaround for NumPy builds that ship with a broken Dlpack deleter (#89759)</p>
</blockquote>
<p><mark>TBD if this improves reliability as ColBERT uses NumPy</mark>/.</p>
</section>
<section id="pr-86288" class="level3">
<h3 class="anchored" data-anchor-id="pr-86288">PR <a href="https://github.com/pytorch/pytorch/pull/86288">#86288</a></h3>
<blockquote class="blockquote">
<p>Workaround MSVC ICE due to constexpr char* template argument (#86288)</p>
</blockquote>
<p>Gemini’s take: It directly impacts ColBERT by ensuring that its custom C++ code can be compiled successfully on Windows machines that use an affected MSVC compiler.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
<section id="pr-85408" class="level3">
<h3 class="anchored" data-anchor-id="pr-85408">PR <a href="https://github.com/pytorch/pytorch/pull/85408">#85408</a></h3>
<blockquote class="blockquote">
<p>Add define to fix issue with compatibility with latest Windows SDK (#85408)</p>
</blockquote>
<p>Gemini’s take: It directly impacts ColBERT by ensuring that the underlying PyTorch framework can be successfully built on modern Windows environments.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
</section>
<section id="improvements" class="level2">
<h2 class="anchored" data-anchor-id="improvements">Improvements</h2>
<p>These PyTorch PRs are related to improvements, which could affect ColBERT by speeding things up (and therefore seeing a speed up in indexing/search/training time) or changing baseline indexing/search/training artifacts if improvements impact numeric precision.</p>
<section id="pr-56398" class="level3">
<h3 class="anchored" data-anchor-id="pr-56398">PR <a href="https://github.com/pytorch/pytorch/pull/56398">#56398</a></h3>
<blockquote class="blockquote">
<p>Set std/var correction overloads default value to None (#56398)</p>
</blockquote>
<p><mark>Unclear if and how this affects ColBERT but highlighting it since it changes code in PyTorch’s aten/src/ATen/native.</mark></p>
</section>
<section id="pr-86309" class="level3">
<h3 class="anchored" data-anchor-id="pr-86309">PR <a href="https://github.com/pytorch/pytorch/pull/86309">#86309</a></h3>
<blockquote class="blockquote">
<p>Add support for int32 indices in index/index_put ops (#86309)</p>
</blockquote>
<p><mark>I think this PR is related to <a href="https://github.com/stanford-futuredata/ColBERT/pull/180">this ColBERT PR</a> which I think is related to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L163">this line of code in <code>IndexScorer</code></a></mark>.</p>
</section>
<section id="pr-87022" class="level3">
<h3 class="anchored" data-anchor-id="pr-87022">PR <a href="https://github.com/pytorch/pytorch/pull/87022">#87022</a></h3>
<blockquote class="blockquote">
<p>Enable where to have cpu scalar args (#87022)</p>
</blockquote>
<p>ColBERT uses <code>torch.where</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L49">colbert/modeling/checkpoint.py</a> to pool embeddings within each cluster.</li>
</ul>
<p><mark>Unclear if this will affect ColBERT but there are currently no open issues related to <code>torch.where</code></mark>.</p>
</section>
<section id="pr-90914" class="level3">
<h3 class="anchored" data-anchor-id="pr-90914">PR <a href="https://github.com/pytorch/pytorch/pull/90914">#90914</a></h3>
<blockquote class="blockquote">
<p>Add support for NumPy scalars to torch.tensor.asarray (#90914)</p>
</blockquote>
<p><mark>Found <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L198">1 use of <code>asarray</code></a> but it doesn’t deal with a scalar so probably won’t be affected</mark>.</p>
</section>
<section id="pr-85926" class="level3">
<h3 class="anchored" data-anchor-id="pr-85926">PR <a href="https://github.com/pytorch/pytorch/pull/85926">#85926</a></h3>
<blockquote class="blockquote">
<p>Enable out variant of torch.max(#85926)</p>
</blockquote>
<p><mark>Unclear what this PR does but highlighting it since ColBERT uses <code>torch.max</code></mark>.</p>
</section>
<section id="pr-91846" class="level3">
<h3 class="anchored" data-anchor-id="pr-91846">PR <a href="https://github.com/pytorch/pytorch/pull/91846">#91846</a></h3>
<blockquote class="blockquote">
<p>Implement faster gradient clipping using foreach function (#91846)</p>
</blockquote>
<p>ColBERT uses <code>torch.nn.utils.clip_grad_norm_</code> in two lines:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">colbert/utils/amp.py#L26</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L31">colbert/utils/amp.py#L31</a></li>
</ul>
<p><mark>IIUC this won’t affect ColBERT since it doesn’t set <code>foreach</code> in <code>torch.nn.utils.clip_grad_norm_</code>.</mark></p>
</section>
<section id="pr-92334" class="level3">
<h3 class="anchored" data-anchor-id="pr-92334">PR <a href="https://github.com/pytorch/pytorch/pull/92334">#92334</a></h3>
<blockquote class="blockquote">
<p>Enable DDP to handle custom dataclass forward outputs (#92334)</p>
</blockquote>
<p><mark>ColBERT does use DistributedDataParallel (<a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L56">in <code>train</code></a>) butit’s not being passed a custom dataclass, it’s being passed a <code>colbert</code> model so I don’t think this PR applies.</mark></p>
</section>
<section id="pr-89137" class="level3">
<h3 class="anchored" data-anchor-id="pr-89137">PR <a href="https://github.com/pytorch/pytorch/pull/89137">#89137</a></h3>
<blockquote class="blockquote">
<p>Skip collective communications for NO_SHARD in clip_grad_norm_ (#89137)</p>
</blockquote>
<p><mark>ColBERT doesn’t use FullyShardedDataParallel but it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">does use <code>torch.nn.utils.clip_grad_norm</code></a> so not sure if this PyTorch PR affects it</mark>.</p>
</section>
<section id="pr-90028" class="level3">
<h3 class="anchored" data-anchor-id="pr-90028">PR <a href="https://github.com/pytorch/pytorch/pull/90028">#90028</a></h3>
<blockquote class="blockquote">
<p>Apply the “largest” dtype across all parameters/gradients as defined by PyTorch’s type promotion semantics for the total norm returned in clip_grad_norm_ for low prec grads (#90028)</p>
</blockquote>
<p><mark>ColBERT doesn’t use FullyShardedDataParallel but it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">does use <code>torch.nn.utils.clip_grad_norm</code></a> so not sure if this PyTorch PR affects it</mark>.</p>
</section>
<section id="pr-85692" class="level3">
<h3 class="anchored" data-anchor-id="pr-85692">PR <a href="https://github.com/pytorch/pytorch/pull/85692">#85692</a></h3>
<blockquote class="blockquote">
<p>Set CUDA_MODULE_LOADING to LAZY when not set by the user (#85692)</p>
</blockquote>
<p><mark>Unclear exactly what this does but it relates to the CUDA_MODULE_LOADING env var which is not set in ColBERT</mark></p>
</section>
<section id="pr-89172" class="level3">
<h3 class="anchored" data-anchor-id="pr-89172">PR <a href="https://github.com/pytorch/pytorch/pull/89172">#89172</a></h3>
<blockquote class="blockquote">
<p>Add an option to disable reduced precision reductions for BF16 GEMM (#89172)</p>
</blockquote>
<p><mark>Unclear exactly what this does, but in the PR they mentioned it improves H100 usage, so I’ll keep that in mind.</mark></p>
</section>
<section id="pr-91436" class="level3">
<h3 class="anchored" data-anchor-id="pr-91436">PR <a href="https://github.com/pytorch/pytorch/pull/91436">#91436</a></h3>
<blockquote class="blockquote">
<p>Add an env variable to disable addmm_cuda_lt kernel (#91436)</p>
</blockquote>
<p><mark>Unclear what this does, but it’s adding a variable, so it’s a new feature.</mark></p>
</section>
<section id="pr-86041-93022" class="level3">
<h3 class="anchored" data-anchor-id="pr-86041-93022">PR <a href="https://github.com/pytorch/pytorch/pull/86041">#86041</a>, <a href="https://github.com/pytorch/pytorch/pull/93022">#93022</a></h3>
<blockquote class="blockquote">
<p>Clean up flatbuffer lib dependency and fixed its test to match pkl models (#86041, #93022)</p>
</blockquote>
<p><mark>I am not sure what these PRs are doing. The title refers “pkl models” which ColBERT doesn’t use to my knowledge.</mark></p>
</section>
<section id="pr-93898" class="level3">
<h3 class="anchored" data-anchor-id="pr-93898">PR <a href="https://github.com/pytorch/pytorch/pull/93898">#93898</a></h3>
<blockquote class="blockquote">
<p>Type corrections to avoid unnecessary static_casts (#93898)</p>
</blockquote>
<p><mark>Unclear what this PR does but it touches a lot of what seem to be core files so I’m flagging it</mark>.</p>
</section>
<section id="pr-87245" class="level3">
<h3 class="anchored" data-anchor-id="pr-87245">PR <a href="https://github.com/pytorch/pytorch/pull/87245">#87245</a></h3>
<blockquote class="blockquote">
<p>Integrate all ONNX operators with a new JitScalarType API (#87245)</p>
</blockquote>
<p><mark>It’s onnx related, which ColBERT doesn’t use, but it also says: “this PR addresses not only the issue above, but the entire family of issues related to torch._C.Value.type() parsing when scalarType() or dtype() is not available.”</mark></p>
</section>
<section id="pr-87343" class="level3">
<h3 class="anchored" data-anchor-id="pr-87343">PR <a href="https://github.com/pytorch/pytorch/pull/87343">#87343</a></h3>
<blockquote class="blockquote">
<p>Add share_from_this to torch::jit::Graph (#87343)</p>
</blockquote>
<p><mark>Is ONNX related, but unclear if it affects anything else?</mark></p>
</section>
<section id="pr-84789" class="level3">
<h3 class="anchored" data-anchor-id="pr-84789">PR <a href="https://github.com/pytorch/pytorch/pull/84789">#84789</a></h3>
<blockquote class="blockquote">
<p>Use optional op to keep None in results for ONNX internal tests (#84789)</p>
</blockquote>
<p><mark>Is ONNX related, but unclear if it affects anything else?</mark></p>
</section>
<section id="pr-86218" class="level3">
<h3 class="anchored" data-anchor-id="pr-86218">PR <a href="https://github.com/pytorch/pytorch/pull/86218">#86218</a></h3>
<blockquote class="blockquote">
<p>Add fp16 support for torch.nn.Linear (#89774), torch.nn.GELU (#86218)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91884" class="level3">
<h3 class="anchored" data-anchor-id="pr-91884">PR <a href="https://github.com/pytorch/pytorch/pull/91884">#91884</a></h3>
<blockquote class="blockquote">
<p>Add support for empty Tensors in torch.bitwise_not (#87286), torch.nn.LayerNorm (#94212), many backward functions (#94343), torch.nn.functional.hardswish (#94342), torch.topk (#91884), torch.arange (#94485), torch.linal.inv (#94551),</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91734" class="level3">
<h3 class="anchored" data-anchor-id="pr-91734">PR <a href="https://github.com/pytorch/pytorch/pull/91734">#91734</a></h3>
<blockquote class="blockquote">
<p>Add support for reduction ops on multiple axis at a time (#91734)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-94639" class="level3">
<h3 class="anchored" data-anchor-id="pr-94639">PR <a href="https://github.com/pytorch/pytorch/pull/94639">#94639</a></h3>
<blockquote class="blockquote">
<p>Add support for k greater than 16 for torch.topk (#94639)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91576" class="level3">
<h3 class="anchored" data-anchor-id="pr-91576">PR <a href="https://github.com/pytorch/pytorch/pull/91576">#91576</a></h3>
<blockquote class="blockquote">
<p>Simplify OpenMP detection in CMake (#91576)</p>
</blockquote>
<p>Claude’s take: While ColBERT should continue working, there could be subtle performance or compilation differences depending on how PyTorch’s simplified OpenMP detection affects the runtime compilation of ColBERT’s C++ extensions, particularly in multi-threaded scenarios.</p>
<p><mark>Unclear what this PR is doing but flagging it as it might improve performance as Claude states.</mark></p>
</section>
</section>
<section id="deprecations" class="level2">
<h2 class="anchored" data-anchor-id="deprecations">Deprecations</h2>
<p>These are PRs I would think would have a significant impact if applicable.</p>
<section id="pr-92143" class="level3">
<h3 class="anchored" data-anchor-id="pr-92143">PR <a href="https://github.com/pytorch/pytorch/pull/92143">#92143</a></h3>
<blockquote class="blockquote">
<p>Deprecate tensor.mT,tensor.T,tensor.mH,tensor.H on 0D-tensors (#92143)</p>
</blockquote>
<p><mark>There are five instances where .T is used, but pretty sure none of these are 0-D tensors, will confirm</mark>:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L13">colbert/search/candidate_generation.py#L13</a>: cosine similarity between centroids and query token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L43">colbert/search/candidate_generation.py#L43</a>: used in <code>generate_candidate_scores</code> which uses <code>lookup_eids</code> which I can’t find anywhere else in the codebase.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L195">colbert/modeling/colbert.py#L195</a>: Cosine similarity between query and document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">colbert/indexing/codecs/residual.py#L215</a>: Cosine similarity between centroids and document token embeddings (GPU).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">colbert/indexing/codecs/residual.py#L217</a>: Cosine similarity between centroids and document token embeddings (CPU).</li>
</ul>
</section>
</section>
<section id="performance" class="level2">
<h2 class="anchored" data-anchor-id="performance">Performance</h2>
<p>Similar to improvements, I would only expect this set of PRs to improve ColBERT performance, keeping an eye on how different artifacts changed because of that.</p>
<section id="pr-93234" class="level3">
<h3 class="anchored" data-anchor-id="pr-93234">PR <a href="https://github.com/pytorch/pytorch/pull/93234">#93234</a></h3>
<blockquote class="blockquote">
<p>Improve performance for functional.multi_head_attention_forward() (#93234, #89847)</p>
</blockquote>
<p><mark>ColBERT uses BERT, which has its own attention implementation, so this likely wouldn’t impact it unless the BERT model specifically uses <code>torch.nn.functional.multi_head_attention_forward</code> or <code>torch.nn.MultiheadAttention</code>.</mark></p>
</section>
<section id="pr-84981" class="level3">
<h3 class="anchored" data-anchor-id="pr-84981">PR <a href="https://github.com/pytorch/pytorch/pull/84981">#84981</a></h3>
<blockquote class="blockquote">
<p>Use atomicAdd for bfloat16 in Ampere and above (#84981)</p>
</blockquote>
<p>Gemini’s take: This pull request directly accelerates a fundamental operation used during the training of ColBERT. By replacing a slow, emulated function with a fast, hardware-native instruction, this change leads to a noticeable increase in training speed for anyone training ColBERT with bfloat16 mixed precision on an Ampere or newer GPU.</p>
<p><mark>If Gemini is correct, then I will see a speedup in training.</mark></p>
</section>
<section id="pr-94034" class="level3">
<h3 class="anchored" data-anchor-id="pr-94034">PR <a href="https://github.com/pytorch/pytorch/pull/94034">#94034</a></h3>
<blockquote class="blockquote">
<p>Add various performance fixes to c++ STL usage (#94034)</p>
</blockquote>
<p>Gemini’s take: The changes in this PR touch several core PyTorch components that are critical to ColBERT’s operation:</p>
<ul>
<li>Autograd Engine (function.h): Every gradient calculation during training will benefit from these optimizations.</li>
<li>CUDA Communication (comm.cpp): The code that handles broadcasting and gathering tensors across GPUs for multi-GPU training and inference is made more efficient.</li>
<li>Mixed Precision (autocast_mode.h): The logic for automatic mixed precision, which is key for training ColBERT efficiently, is also slightly optimized.</li>
</ul>
<p><mark>If Gemini is correct, then I will see a speed up in all aspects of ColBERT. </mark></p>
</section>
<section id="pr-86568" class="level3">
<h3 class="anchored" data-anchor-id="pr-86568">PR <a href="https://github.com/pytorch/pytorch/pull/86568">#86568</a></h3>
<blockquote class="blockquote">
<p>Add fmsub to vectorization primitives (#86568)</p>
</blockquote>
<p>Gemini’s take: This pull request is a CPU-specific performance optimization. It adds support for the fmsub (fused multiply-subtract) instruction to PyTorch’s CPU vectorization library. This allows PyTorch to perform the operation (a * b) - c in a single, faster instruction on modern CPUs that support it (e.g., via AVX or NEON).</p>
<p><mark>I’m pretty sure ColBERT doesn’t use multiply-subtract, but keeping it in here just in case it comes up. </mark></p>
</section>
<section id="pr-92300" class="level3">
<h3 class="anchored" data-anchor-id="pr-92300">PR <a href="https://github.com/pytorch/pytorch/pull/92300">#92300</a></h3>
<blockquote class="blockquote">
<p>Fix biasadd OMP perf issue for the packed MKL SGEMM (#92300)</p>
</blockquote>
<p>Gemini’s take: This pull request is a CPU-specific performance optimization. It fixes a parallelization issue within the Intel MKL (Math Kernel Library) backend for linear layers. This change improves the efficiency of adding a bias term to the output of a matrix multiplication when running on a CPU.</p>
<p><mark>If Gemini is correct, I would expect a speedup on CPU. </mark></p>
</section>
<section id="pr-91114" class="level3">
<h3 class="anchored" data-anchor-id="pr-91114">PR <a href="https://github.com/pytorch/pytorch/pull/91114">#91114</a></h3>
<blockquote class="blockquote">
<p>Increase performance of torch.add{cmul,cdiv,mm}(#94214, #94534)torch.multinomial (#86342), faster op launch time (#86437), torch.linear (#91114), view handling (#91743, #94218), convolutions(#94661), scatter/gather (#94663)</p>
</blockquote>
<p>Gemini’s take: While the Adam optimizer used by ColBERT does use the addcdiv operation, this is executed on the GPU via CUDA, not MPS. This pull request is a performance optimization for the torch.nn.Linear layer, but it is exclusively for the MPS (Metal Performance Shaders) backend.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>Based on my analysis, I’m optimistic about upgrading ColBERT from torch==1.13.1 to 2.0. The upgrade should deliver concrete benefits with reasonable testing overhead. Performance-wise, I’ll be watching for training time improvements from the faster <code>foreach</code> optimizer implementations and expect speedups across all aspects of ColBERT from C++ optimizations and CUDA improvements. For validation, I’ll need to check for numeric differences in indexing/search artifacts from half-precision bug fixes and benchmark retrieval quality metrics after reindexing to avoid regressions. The reliability improvements should make ColBERT’s multi-GPU functionality more reliable and might address related open issues. Plus there are fixes for operations like <code>topk</code> and <code>torch.load</code> that ColBERT uses extensively. Most MPS-related changes will only affect ColBERT if we choose future compatibility, so they’re not immediate concerns but good to have.</p>
<p>My next step will be to establish training time benchmarks and indexing/retrieval/training baseline artifacts so that I can concretely monitor even subtle performance/behavior changes when using <code>torch==2.0</code> in my development branch.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/vishalbakshi\.github\.io\/blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>