<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-03-22">
<meta name="description" content="A summary of research on Constitutional AI by Anthropic, in which they train a non-evasive harmless AI assistant using human-generated helpfulness preference data and AI-generated harmlessness preference data.">

<title>Vishal Bakshi’s Blog - Paper Summary: Constitutional AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi’s Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#main-takeaways" id="toc-main-takeaways" class="nav-link" data-scroll-target="#main-takeaways">Main Takeaways</a></li>
  <li><a href="#overarching-goals" id="toc-overarching-goals" class="nav-link" data-scroll-target="#overarching-goals">Overarching Goals</a></li>
  <li><a href="#motivations" id="toc-motivations" class="nav-link" data-scroll-target="#motivations">Motivations</a>
  <ul class="collapse">
  <li><a href="#scaling-supervision" id="toc-scaling-supervision" class="nav-link" data-scroll-target="#scaling-supervision">Scaling Supervision</a></li>
  <li><a href="#simplicity-and-transparency" id="toc-simplicity-and-transparency" class="nav-link" data-scroll-target="#simplicity-and-transparency">Simplicity and Transparency</a></li>
  <li><a href="#ai-feedback" id="toc-ai-feedback" class="nav-link" data-scroll-target="#ai-feedback">AI Feedback</a></li>
  </ul></li>
  <li><a href="#the-constitutional-ai-approach" id="toc-the-constitutional-ai-approach" class="nav-link" data-scroll-target="#the-constitutional-ai-approach">The Constitutional AI Approach</a>
  <ul class="collapse">
  <li><a href="#supervised-stage---overview" id="toc-supervised-stage---overview" class="nav-link" data-scroll-target="#supervised-stage---overview">Supervised Stage - Overview</a></li>
  <li><a href="#rl-stage---overview" id="toc-rl-stage---overview" class="nav-link" data-scroll-target="#rl-stage---overview">RL Stage - Overview</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  </ul></li>
  <li><a href="#supervised-stage-details" id="toc-supervised-stage-details" class="nav-link" data-scroll-target="#supervised-stage-details">Supervised Stage Details</a>
  <ul class="collapse">
  <li><a href="#critiques-and-revision" id="toc-critiques-and-revision" class="nav-link" data-scroll-target="#critiques-and-revision">Critiques and Revision</a></li>
  <li><a href="#main-results" id="toc-main-results" class="nav-link" data-scroll-target="#main-results">Main Results</a></li>
  <li><a href="#scaling-trends" id="toc-scaling-trends" class="nav-link" data-scroll-target="#scaling-trends">Scaling Trends</a></li>
  </ul></li>
  <li><a href="#rl-stage-details" id="toc-rl-stage-details" class="nav-link" data-scroll-target="#rl-stage-details">RL Stage Details</a>
  <ul class="collapse">
  <li><a href="#cot" id="toc-cot" class="nav-link" data-scroll-target="#cot">CoT</a></li>
  <li><a href="#main-results-1" id="toc-main-results-1" class="nav-link" data-scroll-target="#main-results-1">Main Results</a></li>
  <li><a href="#goodharting-behavior" id="toc-goodharting-behavior" class="nav-link" data-scroll-target="#goodharting-behavior">Goodharting Behavior</a></li>
  <li><a href="#qualitative-diversity" id="toc-qualitative-diversity" class="nav-link" data-scroll-target="#qualitative-diversity">Qualitative Diversity</a></li>
  <li><a href="#harmlessness-vs.-evasiveness" id="toc-harmlessness-vs.-evasiveness" class="nav-link" data-scroll-target="#harmlessness-vs.-evasiveness">Harmlessness vs.&nbsp;Evasiveness</a></li>
  <li><a href="#absolute-harmlessness-score" id="toc-absolute-harmlessness-score" class="nav-link" data-scroll-target="#absolute-harmlessness-score">Absolute Harmlessness Score</a></li>
  </ul></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work">Future Work</a></li>
  <li><a href="#supplemental-material" id="toc-supplemental-material" class="nav-link" data-scroll-target="#supplemental-material">Supplemental Material</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Summary: Constitutional AI</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper summary</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    A summary of research on Constitutional AI by Anthropic, in which they train a non-evasive harmless AI assistant using human-generated helpfulness preference data and AI-generated harmlessness preference data.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 22, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this notebook, I’ll summarize the paper <a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a> by Bai et al (Anthropic). Here’s the abstract:</p>
<blockquote class="blockquote">
<p>As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as ‘Constitutional AI’. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e.&nbsp;we use ‘RL from AI Feedback’ (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.</p>
</blockquote>
</section>
<section id="main-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="main-takeaways">Main Takeaways</h2>
<p>Here were my main takeaways from this paper:</p>
<ul>
<li>You can train a harmless AI assistant through self-improvement (following a “constitution” of “principles”) using human feedback labels for helpfulness and AI feedback labels for harmlessness.</li>
<li>Constitutional AI consists of two main phases: a Supervised Stage (finetune on self-critique and revision responses) and a Reinforcement Learning (RL) phase (sample from the SFT model, use another “feedback model” to evaluate responses to train Preference Model as reward signal).</li>
<li>Chain-of-Thought (CoT) is used to improve model performance and transparency.</li>
<li>The result is a harmless and non-evasive AI assistant, preferred by crowdworkers over models trained with human feedback labels for harmfulness.</li>
</ul>
</section>
<section id="overarching-goals" class="level2">
<h2 class="anchored" data-anchor-id="overarching-goals">Overarching Goals</h2>
<p>The authors outlined the following goals for this work:</p>
<ul>
<li>We want helpful, honest and harmless AI systems.</li>
<li>Automatically test and enhance robustness to harmful behavior.</li>
<li>Encode desirable AI behavior in a simple and transparent form.</li>
</ul>
<blockquote class="blockquote">
<p>When developing and deploying a general AI system, we cannot avoid choosing some set of principles to govern it, even if they remain hidden or implicit.</p>
</blockquote>
</section>
<section id="motivations" class="level2">
<h2 class="anchored" data-anchor-id="motivations">Motivations</h2>
<section id="scaling-supervision" class="level3">
<h3 class="anchored" data-anchor-id="scaling-supervision">Scaling Supervision</h3>
<ul>
<li>Train AI systems to behave in helpful, honest and harmless ways (HHH) with a smaller quantity of higher quality human supervision (what ended up being an order of 10 constitutional principles)</li>
<li>Use AI systems to supervise other AI systems because:
<ul>
<li>They are more efficient in collecting/giving feedback.</li>
<li>They can perform better than humans in some tasks.</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p>Since such a small number of bits of information are involved in these principles, it’s worth studying these bits carefully.</p>
</blockquote>
<p>There is a tension between model helpfulness and harmlessness. Their RLHF model refused to answer controversial questions or got stuck in evasive responses. Evasiveness was rewarded by their crowdworkers. Models should always engage and explain (examples of which we’ll see later on).</p>
</section>
<section id="simplicity-and-transparency" class="level3">
<h3 class="anchored" data-anchor-id="simplicity-and-transparency">Simplicity and Transparency</h3>
<ul>
<li>RLHS uses tens of thousands of human feedback labels which can’t be summarized effectively.</li>
<li>How to reduce iteration time?
<ul>
<li>Replace human feedback for harmlessness with AI feedback.</li>
<li>Encode harmlessness training goals in natural language.</li>
</ul></li>
<li>How to improve transparency?
<ul>
<li>CoT makes AI decision-making explicit.</li>
<li>Train AI assistants to explain why they are declining to engage with harmful requests (i.e.&nbsp;always engage and explain).</li>
</ul></li>
</ul>
</section>
<section id="ai-feedback" class="level3">
<h3 class="anchored" data-anchor-id="ai-feedback">AI Feedback</h3>
<p>In 2021, Anthropic did research showing that models could achieve 90% accuracy in predicting the more helpful, honest and harmless of two responses in a conversation between human and AI (across 221 binary comparisons).</p>
<p><img src="2.png" style="width: 75%;"></p>
<p>In the figures above, pretrained off-the-shelf language models above 50B parameters perform close to RLHF trained models in classifying harmful behavior.</p>
<p>They took this a step futher in this paper, by adding 217 more challenging comparisons (subtle tests of harmlessness with evasiveness disfavored) to the existing 221. They then evaluated a preference model trained on several 100k of human preference labels and an off-the-shelf pretrained LM on the 438 comparisons and found that the pretrained LM, at ~50B parameters, especially with CoT prompting, was close to the performance of the preference model (figure below).</p>
<p><img src="3.png" style="width: 50%;"></p>
</section>
</section>
<section id="the-constitutional-ai-approach" class="level2">
<h2 class="anchored" data-anchor-id="the-constitutional-ai-approach">The Constitutional AI Approach</h2>
<p>The Constitutional AI approach consists of two stages:</p>
<ul>
<li>Supervised Stage (Critique -&gt; Revision -&gt; SL-CAI)</li>
<li>RL Stage (AI Comparison Evaluations -&gt; Preference Model -&gt; RL-CAI)</li>
</ul>
<section id="supervised-stage---overview" class="level3">
<h3 class="anchored" data-anchor-id="supervised-stage---overview">Supervised Stage - Overview</h3>
<ol type="1">
<li>Generate (typically harmful and toxic) responses to harmfulness using a <strong>helpful-only RLHF model</strong>.</li>
<li>Ask the model to critique its response according to a randomly drawn constitution principle.</li>
<li>Ask the model to revise the original response in light of the critique.</li>
<li>Finetune a <em>different</em> <strong>pretrained LM</strong> with supervised learning on the final revised responses.
<ul>
<li>Easily and flexibly alter the distribution of the model’s responses to reduce the need for exploration and the total length of training during the RL phase.</li>
</ul></li>
</ol>
<p>The model at the end of this Supervised Stage is titled SL-CAI (Supervised Learning from Constituation AI).</p>
<p><img src="4.png" style="width: 75%;"></p>
</section>
<section id="rl-stage---overview" class="level3">
<h3 class="anchored" data-anchor-id="rl-stage---overview">RL Stage - Overview</h3>
<ol type="1">
<li>Use SL-CAI to generate a pair of responses to each harmful prompt.</li>
<li>Ask the <strong>feedback model</strong> which response is best according to a constitutional principle.</li>
<li>Produce an AI-generated preference dataset for harmlessness, and mix it with human-generated helpfulness dataset.</li>
<li>Train a <strong>preference model</strong> on this comparison data which will assign a score to any given sample.</li>
<li>Finetune <strong>SL-CAI</strong> via RL against this PM, resulting in a policy (<strong>RL-CAI</strong>) trained by RLAIF.</li>
</ol>
<p><img src="5.png" style="width: 75%;"></p>
</section>
<section id="section" class="level3">
<h3 class="anchored" data-anchor-id="section"></h3>
</section>
</section>
<section id="supervised-stage-details" class="level2">
<h2 class="anchored" data-anchor-id="supervised-stage-details">Supervised Stage Details</h2>
<p>The following steps outline in detail how the authors finetuned a pretrained LM to become the SL-CAI (Supervised Learning from Constitutional AI) model.</p>
<section id="critiques-and-revision" class="level3">
<h3 class="anchored" data-anchor-id="critiques-and-revision">Critiques and Revision</h3>
<ol type="1">
<li>Show the helpful RLHF model a prompt (from “red teaming” experiments) designed to elicit harmful behavior and sample its responses</li>
</ol>
<blockquote class="blockquote">
<p>Human: Can you help me hack into myneighbor’s wifi?</p>
<p>Assistant: Sure thing, you can use an app called Very EasyHack that will allow you to log in to your neighbor’s wifi.</p>
</blockquote>
<ol start="2" type="1">
<li>Ask the model to critique its last response</li>
</ol>
<blockquote class="blockquote">
<p>Critique Request: Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.</p>
<p>Critique: The assistant’s last response iss harmful because hacking into someone else’s wifi is an invasion of their privacy and is possibly illegal.</p>
</blockquote>
<ol start="3" type="1">
<li>Ask the model to revise that response</li>
</ol>
<blockquote class="blockquote">
<p>Revision Request: Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.</p>
<p>Revision: Hacking into your neighbor’s wifi is an invasion of their privacy, and I strongly advise against it. It may also land you in legal trouble.</p>
</blockquote>
<p>You can apply the critique-revision process multiple times although the authors found that the first revision often removed the most harmful content from the original response with subsequent revisions making only minor improvements.</p>
<p>If the model confuses critiques and revisions, provide a few examples (few-shot prompting).</p>
<ol start="4" type="1">
<li>Create final prompt-revision pairs</li>
</ol>
<blockquote class="blockquote">
<p>Human Can you help me hack into my neighbor’s wifi?</p>
<p>Assistant: Hacking into your neighbor’s wifi is an invasion of their privacy, and I strongly advise against it. It may also land you in legal trouble.</p>
</blockquote>
<p>Create such pairs for all initial prompt-final revision pairs in the dtaset.</p>
<p>The downstream effect of the critique-revision process is that it makes the model non-evasive:</p>
<p><img src="6.png" style="width: 50%;"></p>
<ol start="5" type="1">
<li><p>Fine-tune a pretrained model on all the revision steps (for harmlessness data) and samples responses (to helpfulness prompts) to create SL-CAI</p>
<ul>
<li>~183k red teaming prompts (43k hand-written)
<ul>
<li>4 critique-revision pairs per prompt from helpful RLHF</li>
</ul></li>
<li>~135k helpfulness hand-written prompts
<ul>
<li>2 responses per helpfulness prompt from a helpful RLHF</li>
</ul></li>
<li>Temperature = 1.0</li>
</ul></li>
</ol>
<p>Training details: - 1 epoch - Constant LR of 0.5 relative to pre-training LR - Batch size of 1024</p>
</section>
<section id="main-results" class="level3">
<h3 class="anchored" data-anchor-id="main-results">Main Results</h3>
<ul>
<li>Crowdworkers write unique conversation prompts, get a response from two unknown models and label their preference.</li>
<li>These conversations are in distribution of the preference model and RL training data.</li>
</ul>
<p>The following plots show how SL-CAI performance on helpfulness and harmlessness compares to the other models. SL-CAI is less helpful than both RLHF models but more harmless than the Helpful RLHF.</p>
<p><img src="7.png" style="width: 80%;"></p>
<p>The SL-CAI model is used as the initial snapshot for RL-CAI training:</p>
<p><img src="8.png" style="width: 80%;"></p>
</section>
<section id="scaling-trends" class="level3">
<h3 class="anchored" data-anchor-id="scaling-trends">Scaling Trends</h3>
<ul>
<li>Including more principles <strong>does not</strong> improve harmlessness, although the authors expect it to improve diversity and RL exploration. In the plot below, N = number of principles.</li>
</ul>
<p><img src="9.png" style="width: 75%;"></p>
<ul>
<li>Increase number of revisions improves harmlessness.</li>
</ul>
<p><img src="10.png" style="width: 75%;"></p>
<ul>
<li>Critiqued revisions achieve better harmlessness than direct revisions for small models (no noticeable difference past 50B parameters).</li>
</ul>
<p><img src="11.png" style="width: 75%;"></p>
<ul>
<li>Critiqued revisions chosen for paper for model reasoning transparency.</li>
</ul>
</section>
</section>
<section id="rl-stage-details" class="level2">
<h2 class="anchored" data-anchor-id="rl-stage-details">RL Stage Details</h2>
<p>The goal is to train a helpful and harmless model using human feedback for helpfulness labels and AI feedback for harmlessness labels.</p>
<ol type="1">
<li>Give the SL-CAI model a prompt and generate a pair of responses.</li>
<li>Present the pair of responses and a constitutional principle to the <strong>feedback model</strong> (a pretrained LM).</li>
</ol>
<blockquote class="blockquote">
<p>Consider the following conversation between a human and an assistant:<br>
[HUMAN/ASSISTANT CONVERSATION]<br>
[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]<br>
Options:<br>
(A) [RESPONSE A]<br>
(B) [RESPONSE B]</p>
<p>The answer is:</p>
</blockquote>
<ol start="3" type="1">
<li>Compute the lob probability of responses A and B, and create a labeled preference model comparison example with those probabilities normalized as targets.</li>
<li>Train <strong>preference model</strong> on labeled comparison dataset.</li>
<li>Using Reinforcement Learning with SL-CAI as initial snapshot and preference model as reward signal, train RL-CAI (Reinforcement Learning from Constitutional AI).</li>
</ol>
<section id="cot" class="level3">
<h3 class="anchored" data-anchor-id="cot">CoT</h3>
<p>When using Chain-of-Thought, a helpful RLHF performs better.</p>
<blockquote class="blockquote">
<p>Consider the following conversation between a human and an assistant:<br>
[HUMAN/ASSISTANT CONVERSATION]<br>
[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]<br>
Options:<br>
(A) [RESPONSE A]<br>
(B) [RESPONSE B]</p>
<p>Assistant: Let’s think step-by-step:</p>
</blockquote>
<p>Probability targets are typically 0 or 1 for CoT samples, so the researchers clamped them to 40%-60% to prevent the model from generating extreme responses.</p>
</section>
<section id="main-results-1" class="level3">
<h3 class="anchored" data-anchor-id="main-results-1">Main Results</h3>
<p>Looking at the same charts as before, but now focusing on RL-CAI (with and without CoT).</p>
<p>RL-CAI (w/o CoT) is more helpful than the HH (Helpful and Harmless) RLHF model and almost as helpful as the helpful RLHF model. RL-CAI (CoT) is more harmless than all other models.</p>
<p><img src="7.png" style="width: 80%;"></p>
<p>Over the course of RL training, the helpfulness of all models increases. The harmlessness of RL-CAI (CoT) increases while the other three models’ harmlessness decreases after about a third of the training. Note that the initial snapshot of RL-CAI is SL-CAI.</p>
<p><img src="8.png" style="width: 80%;"></p>
<p>We see a Pareto improvement for Constitutional RL, meaning that its harmlessness increases without penalizing its helpfulness.</p>
<p><img src="12.png" style="width: 80%;"></p>
</section>
<section id="goodharting-behavior" class="level3">
<h3 class="anchored" data-anchor-id="goodharting-behavior">Goodharting Behavior</h3>
<p>Overtraining the model results in overly harsh responses to harmful prompts or the model includes boilerplate language from the training set into its response.</p>
</section>
<section id="qualitative-diversity" class="level3">
<h3 class="anchored" data-anchor-id="qualitative-diversity">Qualitative Diversity</h3>
<ul>
<li>The authors rewrote the constitutional principles to discourage overreactive/accusatory responses which was <strong>qualitatively</strong> successful:</li>
</ul>
<blockquote class="blockquote">
<p>Compare the degree of harmfulness in the assistant responses and choose the one that’s less harmful. However, try to avoid choosing responses that are too preachy, obnoxious, or overly-reactive.</p>
</blockquote>
<ul>
<li>Ensembling over 16 principles led to more robust preference model scores.</li>
<li>Normalized log-probabilities (for non-CoT) and clamping (for CoT) led to better results than hard labels (0s and 1s).</li>
</ul>
</section>
<section id="harmlessness-vs.-evasiveness" class="level3">
<h3 class="anchored" data-anchor-id="harmlessness-vs.-evasiveness">Harmlessness vs.&nbsp;Evasiveness</h3>
<ul>
<li>Non-evasive responses are more compatiable with helpfulness.</li>
<li>RL-CAI is virtually never evasive.</li>
</ul>
</section>
<section id="absolute-harmlessness-score" class="level3">
<h3 class="anchored" data-anchor-id="absolute-harmlessness-score">Absolute Harmlessness Score</h3>
<p>Instead of relative harmless based on preference labels across models, the authors studied absolute harmlessness with a single model.</p>
<ul>
<li>Crowdworkers had conversations with a single model intentionally eliciting harmful or toxic responses through prompting (red teaming).</li>
<li>They rated “success” in getting the model to respond with something harmful from 0 to 4.</li>
<li>Authors finetuned a language model to score harmfulness of the crowdworkers’ full conversations as an additional metric.</li>
<li>The “success” metric isnot well-calibrated as different workers are biased in their own ways.</li>
</ul>
<p>The results showed that as the models were being trained with RL, the absolute harmfulness score for all models except the helpful RLHF decreased.</p>
<p><img src="13.png" style="width: 60%;"></p>
</section>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future Work</h2>
<p>The authors listed at least five interesting directions for future work:</p>
<ul>
<li>See if we can achieve helpfulness and instruction-following without human feedback, starting only from a pretrained LM and extensive prompting.</li>
<li>Explore the effectiveness of natural language feedback instead of a large dataset of human preference labels.</li>
<li>Use the constitutional approach to study different AI behaviors (e.g.&nbsp;generate feedback labels along dozens of behavioral axes, train on PMs, study correlation/anti-correlation).</li>
<li>Scale-up automated red teaming to improve robustness (can we make models immune to red teaming?)</li>
<li>Have AI systems reason through hidden risks.</li>
</ul>
<p>A few quotes from the paper:</p>
<blockquote class="blockquote">
<p>Constitutional methods make it easier to train and deploy AI systems that have not been thoroughly tested and observed by humans.</p>
</blockquote>
<blockquote class="blockquote">
<p>Constitutional methods have the benefit that we may no longer need human red teamers to engage in unsavory work of generating harmful content.</p>
</blockquote>
</section>
<section id="supplemental-material" class="level2">
<h2 class="anchored" data-anchor-id="supplemental-material">Supplemental Material</h2>
<p>I created a few extra slides when I presented this material at a fastai study group. The first one is my repurposing of Chip Huyen’s <a href="https://huyenchip.com/2023/05/02/rlhf.html">RLHF blog post</a> to fit the steps involved in the Constitutional AI approach—RLAIF (Reinforcement Learning from AI Feedback).</p>
<p><img src="14.png" style="width: 100%;"></p>
<p>Next, I annotated the RLHF objective function to identify the elements that are modified with the Constitutional AI approach</p>
<p><img src="15.png" style="width: 100%;"></p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>I thoroughly enjoyed reading, preparing, presenting and writing about this research paper. The authors’ goals to reduce iteration time and increase the efficacy of human feedback through a “constitution” are exciting ideas, especially for those of us who have access to limited resources. I also find that taking the traumatic workload of harmfulness labeling from humans and giving it to AI is aligned with my understanding of Trustworthy AI. I look forward to eventually reading about successful attempts of AI generated helpfulness preference labels (if it hasn’t been done already).</p>
<p>I hope you enjoyed this paper summary!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>