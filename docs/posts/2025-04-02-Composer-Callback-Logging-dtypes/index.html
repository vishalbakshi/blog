<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2025-04-02">
<meta name="description" content="I write a custom Composer callback (with lots of Claude’s help!) to log data types of different entities during mixed precision LoRA fine-tuning. When the model is in fp32, all entities except activations are in fp32 (activations are in bf16).">

<title>Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry – Vishal Bakshi’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9c1ae87ad5063dce4f793ccd314a7566.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#composer-callback-walkthrough" id="toc-composer-callback-walkthrough" class="nav-link" data-scroll-target="#composer-callback-walkthrough">Composer Callback Walkthrough</a></li>
  <li><a href="#helper-functions" id="toc-helper-functions" class="nav-link" data-scroll-target="#helper-functions">Helper Functions</a></li>
  <li><a href="#model-in-fp32-master_weights_dtypenone" id="toc-model-in-fp32-master_weights_dtypenone" class="nav-link" data-scroll-target="#model-in-fp32-master_weights_dtypenone">Model in fp32 (<code>master_weights_dtype==None</code>)</a>
  <ul class="collapse">
  <li><a href="#data-types-by-lora_layer" id="toc-data-types-by-lora_layer" class="nav-link" data-scroll-target="#data-types-by-lora_layer">Data Types by <code>lora_layer</code></a></li>
  <li><a href="#data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights" id="toc-data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights" class="nav-link" data-scroll-target="#data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights">Data Types by <code>entity</code> (Activations, Gradients, Loss, Optimizer States and Weights)</a></li>
  <li><a href="#data-types-by-composer-training-step" id="toc-data-types-by-composer-training-step" class="nav-link" data-scroll-target="#data-types-by-composer-training-step">Data Types by Composer Training Step</a></li>
  </ul></li>
  <li><a href="#model-in-bf16-master_weights_dtypebfloat16" id="toc-model-in-bf16-master_weights_dtypebfloat16" class="nav-link" data-scroll-target="#model-in-bf16-master_weights_dtypebfloat16">Model in bf16 (<code>master_weights_dtype==bfloat16</code>)</a>
  <ul class="collapse">
  <li><a href="#data-type-by-lora_layer" id="toc-data-type-by-lora_layer" class="nav-link" data-scroll-target="#data-type-by-lora_layer">Data Type by <code>lora_layer</code></a></li>
  <li><a href="#data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights-1" id="toc-data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights-1" class="nav-link" data-scroll-target="#data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights-1">Data Types by <code>entity</code> (Activations, Gradients, Loss, Optimizer States and Weights)</a></li>
  <li><a href="#data-type-by-composer-training-step" id="toc-data-type-by-composer-training-step" class="nav-link" data-scroll-target="#data-type-by-composer-training-step">Data Type by Composer Training Step</a></li>
  </ul></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry</h1>
  <div class="quarto-categories">
    <div class="quarto-category">python</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    I write a custom Composer callback (with lots of Claude’s help!) to log data types of different entities during mixed precision LoRA fine-tuning. When the model is in fp32, all entities except activations are in fp32 (activations are in bf16).
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 2, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In a <a href="https://vishalbakshi.github.io/blog/posts/2025-03-30-Composer-Callback/">previous blog post</a> I shared my first couple of iterations of custom Composer callback used to log data types of different entities (activations, gradients, weights, optimizer states, and loss) during training with LLM-Foundry. In this blog post I’ll share my final callback iteration’s code, some lessons I learned along the way (i.e.&nbsp;LLaMA’s self-attention module doesn’t have positional arguments!) and analyze the logging results to observe entity data types throughout the training loop.</p>
</section>
<section id="composer-callback-walkthrough" class="level2">
<h2 class="anchored" data-anchor-id="composer-callback-walkthrough">Composer Callback Walkthrough</h2>
<p>The data types of entities (activations, gradients, weights, loss, and optimizer states) are logged during training with a custom Composer callback <code>DtypeLogger</code> passed to the Composer <code>Trainer</code>. This callback was built up and tested event-by-event using Claude. There is one event handler in the callback for each Composer event from <code>&lt;FIT_START&gt;</code> to <code>&lt;BATCH_END&gt;</code>:</p>
<pre><code># &lt;INIT&gt;
# &lt;BEFORE_LOAD&gt;
# &lt;AFTER_LOAD&gt;
# &lt;FIT_START&gt;
for epoch in range(NUM_EPOCHS):
    # &lt;EPOCH_START&gt;
    while True:
        # &lt;BEFORE_DATALOADER&gt;
        batch = next(dataloader)
        if batch is None:
            break
        inputs, targets = batch
        # &lt;AFTER_DATALOADER&gt;

        # &lt;BATCH_START&gt;

        # &lt;BEFORE_FORWARD&gt;
        outputs = model.forward(inputs)
        # &lt;AFTER_FORWARD&gt;

        # &lt;BEFORE_LOSS&gt;
        loss = model.loss(outputs, targets)
        # &lt;AFTER_LOSS&gt;

        # &lt;BEFORE_BACKWARD&gt;
        loss.backward()
        # &lt;AFTER_BACKWARD&gt;

        optimizer.step()
        optimizer.zero_grad()

        # &lt;BATCH_END&gt;
    # &lt;EPOCH_END&gt;</code></pre>
<p>There are four explicit logging functions:</p>
<ul>
<li><code>_log_model_weight_dtypes</code></li>
<li><code>_log_gradient_dtypes</code></li>
<li><code>_log_optimizer_state_dtypes</code></li>
<li><code>_log_loss_dtype</code></li>
</ul>
<p>Additionally, activations are logged using <code>register_forward_hook</code> for all modules except self-attention (more on that below). Self-attention inputs are logged using a monkey-patched forward pass.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DtypeLogger(Callback):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, save_path<span class="op">=</span><span class="st">"/model-checkpoints/dtype_tracking"</span>, log_interval<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.save_path <span class="op">=</span> Path(save_path)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dtype_logs <span class="op">=</span> {<span class="st">'log'</span>: {}}</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log_interval <span class="op">=</span> log_interval</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit_start(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"fit_start"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._save_logs()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> epoch_start(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"epoch_start"</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._save_logs()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_dataloader(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"before_dataloader"</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_dataloader(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"after_dataloader"</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> batch_start(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"batch_start"</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_forward(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"before_forward"</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Clear old hooks</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>                hook.remove()</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get the model</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> state.model.model.base_model.model</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>            transformer_model <span class="op">=</span> model.model  <span class="co"># This is the transformer part</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>            batch_id <span class="op">=</span> state.timestamp.batch.value</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store original forward methods to restore later</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.original_forward_methods <span class="op">=</span> {}</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook_fn(layer_name, module_name):</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>                <span class="kw">def</span> _hook(module, inputs, outputs):</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Log input activation dtype</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="bu">isinstance</span>(inputs, <span class="bu">tuple</span>) <span class="kw">and</span> <span class="bu">len</span>(inputs) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"forward:</span><span class="sc">{</span>module_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">:activation_input"</span>] <span class="op">=</span> <span class="bu">str</span>(inputs[<span class="dv">0</span>].dtype)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Log output activation dtype</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="bu">isinstance</span>(outputs, torch.Tensor):</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"forward:</span><span class="sc">{</span>module_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">:activation_output"</span>] <span class="op">=</span> <span class="bu">str</span>(outputs.dtype)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">elif</span> <span class="bu">isinstance</span>(outputs, <span class="bu">tuple</span>) <span class="kw">and</span> <span class="bu">len</span>(outputs) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"forward:</span><span class="sc">{</span>module_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">:activation_output"</span>] <span class="op">=</span> <span class="bu">str</span>(outputs[<span class="dv">0</span>].dtype)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> _hook</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Monkey patch self-attention modules</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> layer_idx, layer <span class="kw">in</span> <span class="bu">enumerate</span>(transformer_model.layers):</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store the original forward method</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>                original_forward <span class="op">=</span> layer.self_attn.forward</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.original_forward_methods[layer_idx] <span class="op">=</span> original_forward</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Define a closure to capture the current layer_idx</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>                <span class="kw">def</span> make_patched_forward(layer_idx, orig_forward):</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>                    <span class="kw">def</span> patched_forward(self_attn, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># Log the hidden_states dtype</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> <span class="st">'hidden_states'</span> <span class="kw">in</span> kwargs <span class="kw">and</span> <span class="bu">hasattr</span>(kwargs[<span class="st">'hidden_states'</span>], <span class="st">'dtype'</span>):</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>                            <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"forward:self_attn:layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">:activation_input"</span>] <span class="op">=</span> <span class="bu">str</span>(kwargs[<span class="st">'hidden_states'</span>].dtype)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># Call the original method as a bound method</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># This ensures 'self_attn' is correctly passed as 'self'</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">return</span> orig_forward.<span class="fu">__get__</span>(self_attn, <span class="bu">type</span>(self_attn))(<span class="op">**</span>kwargs)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> patched_forward</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Replace the forward method</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>                layer.self_attn.forward <span class="op">=</span> make_patched_forward(layer_idx, original_forward).<span class="fu">__get__</span>(layer.self_attn, <span class="bu">type</span>(layer.self_attn))</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Register hook for lm_head</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(model, <span class="st">'lm_head'</span>):</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(model.lm_head.register_forward_hook(hook_fn(<span class="st">"output"</span>, <span class="st">"lm_head"</span>)))</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Register hook for embedding layer</span></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hooks.append(transformer_model.embed_tokens.register_forward_hook(hook_fn(<span class="st">"embeddings"</span>, <span class="st">"embed_tokens"</span>)))</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Register hooks for each transformer layer</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> layer_idx, layer <span class="kw">in</span> <span class="bu">enumerate</span>(transformer_model.layers):</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Self-attention components - we still register hooks for outputs</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.self_attn.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"self_attn"</span>)))</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.self_attn.q_proj.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"q_proj"</span>)))</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.self_attn.k_proj.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"k_proj"</span>)))</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.self_attn.v_proj.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"v_proj"</span>)))</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.self_attn.o_proj.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"o_proj"</span>)))</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>                <span class="co"># MLP components</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.mlp.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"mlp"</span>)))</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.mlp.gate_proj.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"gate_proj"</span>)))</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.mlp.up_proj.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"up_proj"</span>)))</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.mlp.down_proj.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"down_proj"</span>)))</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Layer norms</span></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.input_layernorm.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"input_layernorm"</span>)))</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(layer.post_attention_layernorm.register_forward_hook(hook_fn(<span class="ss">f"layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">"</span>, <span class="st">"post_attention_layernorm"</span>)))</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Final layer norm</span></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hooks.append(transformer_model.norm.register_forward_hook(hook_fn(<span class="st">"final"</span>, <span class="st">"norm"</span>)))</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_forward(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"after_forward"</span>)</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Restore original forward methods</span></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(<span class="va">self</span>, <span class="st">'original_forward_methods'</span>):</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>                model <span class="op">=</span> state.model.model.base_model.model</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a>                transformer_model <span class="op">=</span> model.model</span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> layer_idx, original_forward <span class="kw">in</span> <span class="va">self</span>.original_forward_methods.items():</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>                    transformer_model.layers[layer_idx].self_attn.forward <span class="op">=</span> original_forward</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.original_forward_methods <span class="op">=</span> {}</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Clear hooks</span></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>                hook.remove()</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_loss(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"before_loss"</span>)</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_loss(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"after_loss"</span>)</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_loss_dtype(state, <span class="st">"after_loss"</span>)</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_backward(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"before_backward"</span>)</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_backward(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Log gradient dtypes as before</span></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_gradient_dtypes(state, <span class="st">"after_backward"</span>)</span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Track weight dtypes before optimizer step</span></span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"before_optim_step"</span>)</span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Log optimizer state dtypes</span></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_optimizer_state_dtypes(state, <span class="st">"optimizer_step"</span>)</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> batch_end(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> state.timestamp.batch.value <span class="op">%</span> <span class="va">self</span>.log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Track weight dtypes after optimizer step to detect precision changes</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"after_optim_step"</span>)</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._save_logs()</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> epoch_end(<span class="va">self</span>, state: State, logger: Logger) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._log_model_weight_dtypes(state, <span class="st">"epoch_end"</span>)</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._save_logs()</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log_model_weight_dtypes(<span class="va">self</span>, state: State, event_name: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> state.model</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>            name <span class="op">=</span> name.removeprefix(<span class="st">"model.base_model.model.model."</span>)</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"</span><span class="sc">{</span>event_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">:weights"</span>] <span class="op">=</span> <span class="bu">str</span>(param.dtype)</span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log_gradient_dtypes(<span class="va">self</span>, state: State, event_name: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> state.model</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>            name <span class="op">=</span> name.removeprefix(<span class="st">"model.base_model.model.model."</span>)</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="va">self</span>.dtype_logs[<span class="st">'log'</span>][<span class="ss">f"</span><span class="sc">{</span>event_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">:gradients"</span>] <span class="op">=</span> <span class="bu">str</span>(param.grad.dtype)</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>: <span class="va">self</span>.dtype_logs[<span class="st">'log'</span>][<span class="ss">f"</span><span class="sc">{</span>event_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">:gradients"</span>] <span class="op">=</span> <span class="st">"None"</span></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log_loss_dtype(<span class="va">self</span>, state: State, event_name: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(state, <span class="st">'loss'</span>) <span class="kw">and</span> <span class="bu">hasattr</span>(state.loss, <span class="st">'dtype'</span>):</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"</span><span class="sc">{</span>event_name<span class="sc">}</span><span class="ss">:loss"</span>] <span class="op">=</span> <span class="bu">str</span>(state.loss.dtype)</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _log_optimizer_state_dtypes(<span class="va">self</span>, state: State, event_name: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(state, <span class="st">'optimizers'</span>) <span class="kw">and</span> state.optimizers <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Handle single optimizer or list of optimizers</span></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>            optimizers <span class="op">=</span> state.optimizers <span class="cf">if</span> <span class="bu">isinstance</span>(state.optimizers, <span class="bu">list</span>) <span class="cf">else</span> [state.optimizers]</span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> opt_idx, optimizer <span class="kw">in</span> <span class="bu">enumerate</span>(optimizers):</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Get optimizer state dict</span></span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>                opt_state <span class="op">=</span> optimizer.state_dict()</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Check if 'state' exists in the optimizer state dict</span></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">'state'</span> <span class="kw">in</span> opt_state:</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> param_id, param_state <span class="kw">in</span> opt_state[<span class="st">'state'</span>].items():</span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> state_name, state_value <span class="kw">in</span> param_state.items():</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">if</span> <span class="bu">isinstance</span>(state_value, torch.Tensor):</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>                                <span class="co"># Store dtype of optimizer state tensors (momentum buffers, etc.)</span></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>                                key <span class="op">=</span> <span class="ss">f"optimizer_</span><span class="sc">{</span>opt_idx<span class="sc">}</span><span class="ss">_param_</span><span class="sc">{</span>param_id<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>state_name<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>                                <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"</span><span class="sc">{</span>event_name<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">:optimizer_states"</span>] <span class="op">=</span> <span class="bu">str</span>(state_value.dtype)</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _save_logs(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>        os.makedirs(<span class="va">self</span>.save_path, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>        log_file <span class="op">=</span> <span class="va">self</span>.save_path <span class="op">/</span> <span class="st">"dtype_logs.json"</span></span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(log_file, <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>            json.dump(<span class="va">self</span>.dtype_logs, f, indent<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The most involved event handler is <code>before_forward</code> which involves creating a hook function (<code>hook_fn</code>) passed to PyTorch’s <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook"><code>register_forward_hook</code></a> which exposes the positional inputs and outputs of a module’s <code>forward</code> pass. The hook function modifies <code>self.dtype_logs</code> directly by storing the data type string of inputs and outputs. <code>hook_fn</code> is used for all modules except self attention.</p>
<p>Self attention <a href="https://github.com/huggingface/transformers/issues/29247#issuecomment-1965894085">cannot utilize <code>register_forward_hook</code></a> because the <a href="https://github.com/huggingface/transformers/blob/bf41e54fc8242dafa31bf6203e3d505bcb907119/src/transformers/models/llama/modeling_llama.py#L345">LlamaDecoderLayer</a> does not call self attention forward pass with any positional arguments:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>hidden_states, self_attn_weights <span class="op">=</span> <span class="va">self</span>.self_attn(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    hidden_states<span class="op">=</span>hidden_states,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    attention_mask<span class="op">=</span>attention_mask,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    position_ids<span class="op">=</span>position_ids,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    past_key_value<span class="op">=</span>past_key_value,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    output_attentions<span class="op">=</span>output_attentions,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    use_cache<span class="op">=</span>use_cache,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    cache_position<span class="op">=</span>cache_position,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    position_embeddings<span class="op">=</span>position_embeddings,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>kwargs,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Contrast this with how the forward pass of other modules are called with positional arguments only:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># self attention sublayers</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>query_states <span class="op">=</span> <span class="va">self</span>.q_proj(hidden_states).view(hidden_shape).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>key_states <span class="op">=</span> <span class="va">self</span>.k_proj(hidden_states).view(hidden_shape).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>value_states <span class="op">=</span> <span class="va">self</span>.v_proj(hidden_states).view(hidden_shape).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>attn_output <span class="op">=</span> <span class="va">self</span>.o_proj(attn_output)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># mlp sublayers</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>down_proj <span class="op">=</span> <span class="va">self</span>.down_proj(<span class="va">self</span>.act_fn(<span class="va">self</span>.gate_proj(x)) <span class="op">*</span> <span class="va">self</span>.up_proj(x))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># non-self attention modules</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> <span class="va">self</span>.input_layernorm(hidden_states)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> <span class="va">self</span>.post_attention_layernorm(hidden_states)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> <span class="va">self</span>.mlp(hidden_states)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> <span class="va">self</span>.norm(hidden_states)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Since self-attention inputs can’t be captured by a hook I had to monkey patch its forward pass to log its inputs’ data type:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer_idx, layer <span class="kw">in</span> <span class="bu">enumerate</span>(transformer_model.layers):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the original forward method</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    original_forward <span class="op">=</span> layer.self_attn.forward</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.original_forward_methods[layer_idx] <span class="op">=</span> original_forward</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define a closure to capture the current layer_idx</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> make_patched_forward(layer_idx, orig_forward):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> patched_forward(self_attn, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Log the hidden_states dtype</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'hidden_states'</span> <span class="kw">in</span> kwargs <span class="kw">and</span> <span class="bu">hasattr</span>(kwargs[<span class="st">'hidden_states'</span>], <span class="st">'dtype'</span>):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.dtype_logs[<span class="st">"log"</span>][<span class="ss">f"forward:self_attn:layer_</span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">:activation_input"</span>] <span class="op">=</span> <span class="bu">str</span>(kwargs[<span class="st">'hidden_states'</span>].dtype)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Call the original method as a bound method</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># This ensures 'self_attn' is correctly passed as 'self'</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> orig_forward.<span class="fu">__get__</span>(self_attn, <span class="bu">type</span>(self_attn))(<span class="op">**</span>kwargs)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> patched_forward</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the forward method</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    layer.self_attn.forward <span class="op">=</span> make_patched_forward(layer_idx, original_forward).<span class="fu">__get__</span>(layer.self_attn, <span class="bu">type</span>(layer.self_attn))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>patched_forward</code> receives positional arguments <code>*args</code> (of which there are none) and keyword arguments <code>**kwargs</code> (all of the arguments to the self-attention forward) and logs the data types of the inputs to self-attention (<code>hidden_states</code>) as <code>self_attn_input</code> before returning the outputs of the original forward pass.</p>
<p>A key line is <code>orig_forward.__get__(self_attn, type(self_attn))(**kwargs)</code>. As Claude’s comment mentions, this is to avoid using <code>orig_forward(self_attn, **kwargs)</code> which was causing the following error because the first argument, <code>self_attn</code>, was being interpreted as <code>hidden_states</code> whereas it was intended to represent <code>self</code>:</p>
<pre><code>TypeError: LlamaFlashAttention2.forward() got multiple values for argument 'hidden_states'</code></pre>
<p>In short, when you call <code>__get__(obj, type)</code> on a function it will bind that function as a method to the given object, thus no longer requiring you to pass in <code>self</code> as an argument. This is critical because <code>self_attn.forward</code> <em>has no positional arguments</em>. We can then pass in the keyword arguments to the bound method <code>orig_forward.__get__(self_attn, type(self_attn))(**kwargs)</code>, and let the model continue using self-attention correctly. See the <a href="https://docs.python.org/3/howto/descriptor.html#functions-and-methods:~:text=To%20recap%2C%20functions%20have%20a%20__get__()%20method%20so%20that%20they%20can%20be%20converted%20to%20a%20method%20when%20accessed%20as%20attributes.%20The%20non%2Ddata%20descriptor%20transforms%20an%20obj.f(*args)%20call%20into%20f(obj%2C%20*args).%20Calling%20cls.f(*args)%20becomes%20f(*args).">Descriptor Guide in the Python docs</a> for more information.</p>
</section>
<section id="helper-functions" class="level2">
<h2 class="anchored" data-anchor-id="helper-functions">Helper Functions</h2>
<div id="cell-14" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-15" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parse_index(string):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract structured information from parameter names"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    info <span class="op">=</span> {</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">'layer_number'</span>: <span class="va">None</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'module'</span>: <span class="va">None</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'layer_name'</span>: <span class="va">None</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'lora_layer'</span>: <span class="va">None</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">'training_step'</span>: <span class="va">None</span>,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'entity'</span>: <span class="va">None</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># layer = string.split(":")[1]</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># info["layer"] = layer</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    layer_number_match <span class="op">=</span> re.search(<span class="vs">r'layers</span><span class="ch">\.</span><span class="kw">(</span><span class="dv">\d</span><span class="op">+</span><span class="kw">)</span><span class="vs">'</span>, string)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> layer_number_match: info[<span class="st">'layer_number'</span>] <span class="op">=</span> <span class="bu">int</span>(layer_number_match.group(<span class="dv">1</span>))</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    modules <span class="op">=</span> [</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">"embed_tokens"</span>,</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">"input_layernorm"</span>,</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">"self_attn"</span>,</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"post_attention_layernorm"</span>,</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"mlp"</span>,</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">"norm"</span>,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">"lm_head"</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    module_match <span class="op">=</span> re.search(<span class="vs">r'</span><span class="kw">(</span><span class="vs">mlp</span><span class="cf">|</span><span class="vs">self_attn</span><span class="cf">|</span><span class="vs">input_layernorm</span><span class="cf">|</span><span class="vs">post_attention_layernorm</span><span class="cf">|</span><span class="vs">embed_tokens</span><span class="cf">|</span><span class="vs">norm</span><span class="cf">|</span><span class="vs">lm_head</span><span class="kw">)</span><span class="vs">'</span>, string)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> module_match: info[<span class="st">'module'</span>] <span class="op">=</span> <span class="bu">str</span>(modules.index(module_match.group(<span class="dv">1</span>))).zfill(<span class="dv">2</span>) <span class="op">+</span> <span class="st">'_'</span> <span class="op">+</span> module_match.group(<span class="dv">1</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    layer_name_match <span class="op">=</span> re.search(<span class="vs">r'</span><span class="kw">(</span><span class="vs">q_proj</span><span class="cf">|</span><span class="vs">k_proj</span><span class="cf">|</span><span class="vs">v_proj</span><span class="cf">|</span><span class="vs">o_proj</span><span class="cf">|</span><span class="vs">gate_proj</span><span class="cf">|</span><span class="vs">up_proj</span><span class="cf">|</span><span class="vs">down_proj</span><span class="kw">)</span><span class="vs">'</span>, string)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> layer_name_match: info[<span class="st">'layer_name'</span>] <span class="op">=</span> layer_name_match.group(<span class="dv">1</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    lora_match <span class="op">=</span> re.search(<span class="vs">r'</span><span class="kw">(</span><span class="vs">base_layer</span><span class="cf">|</span><span class="vs">lora_A</span><span class="cf">|</span><span class="vs">lora_B</span><span class="kw">)</span><span class="vs">'</span>, string)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> lora_match: info[<span class="st">'lora_layer'</span>] <span class="op">=</span> lora_match.group(<span class="dv">1</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: info[<span class="st">'lora_layer'</span>] <span class="op">=</span> <span class="st">"Not a LoRA Layer"</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    training_steps <span class="op">=</span> [</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">"fit_start"</span>,</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">"epoch_start"</span>,</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">"before_dataloader"</span>,</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">"after_dataloader"</span>,</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">"batch_start"</span>,</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        <span class="st">"before_forward"</span>,</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        <span class="st">"forward"</span>,</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">"after_forward"</span>,</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"before_loss"</span>,</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">"after_loss"</span>,</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"before_backward"</span>,</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"after_backward"</span>,</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        <span class="st">"before_optim_step"</span>,</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="st">"optimizer_step"</span>,</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        <span class="st">"after_optim_step"</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    training_step <span class="op">=</span> string.split(<span class="st">":"</span>)[<span class="dv">0</span>]</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    info[<span class="st">'training_step'</span>] <span class="op">=</span> <span class="bu">str</span>(training_steps.index(training_step)).zfill(<span class="dv">2</span>) <span class="op">+</span> <span class="st">'_'</span> <span class="op">+</span> training_step</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    info[<span class="st">'entity'</span>] <span class="op">=</span> string.split(<span class="st">":"</span>)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> info</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _df(url):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    dtype_data <span class="op">=</span> json.loads(requests.get(url).text)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(dtype_data).reset_index()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.rename(columns<span class="op">=</span>{<span class="st">"index"</span>: <span class="st">"index"</span>, <span class="st">"log"</span>: <span class="st">"dtype"</span>})</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    parsed_info <span class="op">=</span> df[<span class="st">'index'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: parse_index(x))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'layer_number'</span>] <span class="op">=</span> parsed_info.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'layer_number'</span>])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'module'</span>] <span class="op">=</span> parsed_info.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'module'</span>])</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'layer_name'</span>] <span class="op">=</span> parsed_info.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'layer_name'</span>])</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'lora_layer'</span>] <span class="op">=</span> parsed_info.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'lora_layer'</span>])</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'training_step'</span>] <span class="op">=</span> parsed_info.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'training_step'</span>])</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'entity'</span>] <span class="op">=</span> parsed_info.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="st">'entity'</span>])</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-in-fp32-master_weights_dtypenone" class="level2">
<h2 class="anchored" data-anchor-id="model-in-fp32-master_weights_dtypenone">Model in fp32 (<code>master_weights_dtype==None</code>)</h2>
<p>In this case, <code>master_weights_dtype</code> is not provided in the training YAML file.</p>
<div id="cell-19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:206}}" data-outputid="73fb1c7d-f091-413b-90af-cf9fb0c6eb51" data-execution_count="5">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://gist.githubusercontent.com/vishalbakshi/9ade8d501629d4c30e8aecfa1c6f67cf/raw/0c162e2305002fbe57fd2570ade302c3659140a1/dtypes_logs_1ba_fp32.json"</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> _df(url)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div id="df-f6bb70e8-9110-47f0-85d1-46d907c311a6" class="colab-df-container">
    <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">index</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th">layer_number</th>
<th data-quarto-table-cell-role="th">module</th>
<th data-quarto-table-cell-role="th">layer_name</th>
<th data-quarto-table-cell-role="th">lora_layer</th>
<th data-quarto-table-cell-role="th">training_step</th>
<th data-quarto-table-cell-role="th">entity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>fit_start:embed_tokens.weight:weights</td>
<td>torch.float32</td>
<td>NaN</td>
<td>00_embed_tokens</td>
<td>None</td>
<td>Not a LoRA Layer</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>fit_start:layers.0.self_attn.q_proj.base_layer...</td>
<td>torch.float32</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>q_proj</td>
<td>base_layer</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>fit_start:layers.0.self_attn.q_proj.lora_A.def...</td>
<td>torch.float32</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>q_proj</td>
<td>lora_A</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>fit_start:layers.0.self_attn.q_proj.lora_B.def...</td>
<td>torch.float32</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>q_proj</td>
<td>lora_B</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>fit_start:layers.0.self_attn.k_proj.base_layer...</td>
<td>torch.float32</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>k_proj</td>
<td>base_layer</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-f6bb70e8-9110-47f0-85d1-46d907c311a6')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-f6bb70e8-9110-47f0-85d1-46d907c311a6 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-f6bb70e8-9110-47f0-85d1-46d907c311a6');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-b878e7a4-6a3d-484e-8565-4873e2d61a8a">
  <button class="colab-df-quickchart" onclick="quickchart('df-b878e7a4-6a3d-484e-8565-4873e2d61a8a')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-b878e7a4-6a3d-484e-8565-4873e2d61a8a button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div>
</div>
<section id="data-types-by-lora_layer" class="level3">
<h3 class="anchored" data-anchor-id="data-types-by-lora_layer">Data Types by <code>lora_layer</code></h3>
<p>All LoRA layer entities are in fp32.</p>
<div id="cell-22" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:366}}" data-outputid="d3494bca-3fbd-4a42-ae1b-5087d3ef6618" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df.groupby([<span class="st">'lora_layer'</span>, <span class="st">'dtype'</span>])[<span class="st">'dtype'</span>].count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">dtype</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">lora_layer</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="4" data-quarto-table-cell-role="th" data-valign="top">Not a LoRA Layer</td>
<td data-quarto-table-cell-role="th">None</td>
<td>62</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>331</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>2339</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.int64</td>
<td>1</td>
</tr>
<tr class="odd">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">base_layer</td>
<td data-quarto-table-cell-role="th">None</td>
<td>210</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>2520</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">lora_A</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>2730</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">lora_B</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>2730</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> int64</label>
</div>
</div>
</section>
<section id="data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights" class="level3">
<h3 class="anchored" data-anchor-id="data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights">Data Types by <code>entity</code> (Activations, Gradients, Loss, Optimizer States and Weights)</h3>
<p>Every entity except activations are in fp32. Some parameters don’t have gradients because we are training with LoRA.</p>
<div id="cell-25" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:429}}" data-outputid="78343b50-69b7-4e1e-f2eb-58f2a3fbc781" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>df.groupby([<span class="st">'entity'</span>, <span class="st">'dtype'</span>])[<span class="st">'dtype'</span>].count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">dtype</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">entity</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="3" data-quarto-table-cell-role="th" data-valign="top">activation_input</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>60</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>272</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.int64</td>
<td>1</td>
</tr>
<tr class="even">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">activation_output</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>271</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>62</td>
</tr>
<tr class="even">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">gradients</td>
<td data-quarto-table-cell-role="th">None</td>
<td>272</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>420</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">loss</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">optimizer_states</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>1260</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>8304</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> int64</label>
</div>
</div>
</section>
<section id="data-types-by-composer-training-step" class="level3">
<h3 class="anchored" data-anchor-id="data-types-by-composer-training-step">Data Types by Composer Training Step</h3>
<div id="cell-27" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:773}}" data-outputid="98def9cf-d2c3-4003-a11c-1293b9879460" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>df.groupby([<span class="st">'training_step'</span>, <span class="st">'entity'</span>, <span class="st">'dtype'</span>])[<span class="st">'dtype'</span>].count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">dtype</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">training_step</th>
<th data-quarto-table-cell-role="th">entity</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">00_fit_start</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">01_epoch_start</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">02_before_dataloader</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">03_after_dataloader</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">04_batch_start</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">05_before_forward</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="odd">
<td rowspan="5" data-quarto-table-cell-role="th" data-valign="top">06_forward</td>
<td rowspan="3" data-quarto-table-cell-role="th" data-valign="top">activation_input</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>60</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>272</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.int64</td>
<td>1</td>
</tr>
<tr class="even">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">activation_output</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>271</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>62</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">07_after_forward</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">08_before_loss</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="even">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">09_after_loss</td>
<td data-quarto-table-cell-role="th">loss</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">10_before_backward</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="odd">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">11_after_backward</td>
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">gradients</td>
<td data-quarto-table-cell-role="th">None</td>
<td>272</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>420</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12_before_optim_step</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13_optimizer_step</td>
<td data-quarto-table-cell-role="th">optimizer_states</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>1260</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14_after_optim_step</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>692</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> int64</label>
</div>
</div>
</section>
</section>
<section id="model-in-bf16-master_weights_dtypebfloat16" class="level2">
<h2 class="anchored" data-anchor-id="model-in-bf16-master_weights_dtypebfloat16">Model in bf16 (<code>master_weights_dtype==bfloat16</code>)</h2>
<p>I also logged data types after setting <code>master_weights_dtype</code> in the training YAML to <code>bfloat16</code>.</p>
<div id="cell-30" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:206}}" data-outputid="5b638922-40c0-42a9-9c0e-8b907d105abd" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://gist.githubusercontent.com/vishalbakshi/ec91a59754633611fd8eb33b59031243/raw/5b83a7ebd5759cf6bd2db2369edf1c73e1fb67cf/dtypes_logs_1ba_bf16.json"</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> _df(url)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<div id="df-6f3e8532-20df-451d-8e93-caa52d279f3f" class="colab-df-container">
    <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">index</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th">layer_number</th>
<th data-quarto-table-cell-role="th">module</th>
<th data-quarto-table-cell-role="th">layer_name</th>
<th data-quarto-table-cell-role="th">lora_layer</th>
<th data-quarto-table-cell-role="th">training_step</th>
<th data-quarto-table-cell-role="th">entity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>fit_start:embed_tokens.weight:weights</td>
<td>torch.bfloat16</td>
<td>NaN</td>
<td>00_embed_tokens</td>
<td>None</td>
<td>Not a LoRA Layer</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>fit_start:layers.0.self_attn.q_proj.base_layer...</td>
<td>torch.bfloat16</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>q_proj</td>
<td>base_layer</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>fit_start:layers.0.self_attn.q_proj.lora_A.def...</td>
<td>torch.bfloat16</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>q_proj</td>
<td>lora_A</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>fit_start:layers.0.self_attn.q_proj.lora_B.def...</td>
<td>torch.bfloat16</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>q_proj</td>
<td>lora_B</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>fit_start:layers.0.self_attn.k_proj.base_layer...</td>
<td>torch.bfloat16</td>
<td>0.0</td>
<td>02_self_attn</td>
<td>k_proj</td>
<td>base_layer</td>
<td>00_fit_start</td>
<td>weights</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-6f3e8532-20df-451d-8e93-caa52d279f3f')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-6f3e8532-20df-451d-8e93-caa52d279f3f button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-6f3e8532-20df-451d-8e93-caa52d279f3f');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-159646a6-3c88-49de-8770-5f4464ad1b49">
  <button class="colab-df-quickchart" onclick="quickchart('df-159646a6-3c88-49de-8770-5f4464ad1b49')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-159646a6-3c88-49de-8770-5f4464ad1b49 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div>
</div>
<section id="data-type-by-lora_layer" class="level3">
<h3 class="anchored" data-anchor-id="data-type-by-lora_layer">Data Type by <code>lora_layer</code></h3>
<p>Interestingly, setting <code>master_weights_dtype</code> makes all LoRA layers bfloat16 but some non-LoRA layers’ entities are still in fp32.</p>
<div id="cell-33" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:366}}" data-outputid="b18ff791-dbf5-40a8-fcfe-402ed510d645" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df.groupby([<span class="st">'lora_layer'</span>, <span class="st">'dtype'</span>])[<span class="st">'dtype'</span>].count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">dtype</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">lora_layer</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="4" data-quarto-table-cell-role="th" data-valign="top">Not a LoRA Layer</td>
<td data-quarto-table-cell-role="th">None</td>
<td>62</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>2249</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>421</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.int64</td>
<td>1</td>
</tr>
<tr class="odd">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">base_layer</td>
<td data-quarto-table-cell-role="th">None</td>
<td>210</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>2520</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">lora_A</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>2730</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">lora_B</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>2730</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> int64</label>
</div>
</div>
</section>
<section id="data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights-1" class="level3">
<h3 class="anchored" data-anchor-id="data-types-by-entity-activations-gradients-loss-optimizer-states-and-weights-1">Data Types by <code>entity</code> (Activations, Gradients, Loss, Optimizer States and Weights)</h3>
<p>All floating point values are in bfloat16 except for the loss and some of the optimizer states. I’m not sure why some optimizer states are in bf16, even though it says in the <a href="https://docs.mosaicml.com/projects/composer/en/latest/notes/numerics.html#automatic-mixed-precision-amp-training">Composer docs</a>:</p>
<blockquote class="blockquote">
<p>Store the weights and perform the optimizer step in single precision, enabling the weight update to be done more precisely.</p>
</blockquote>
<div id="cell-36" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:397}}" data-outputid="17283b63-7034-448d-f2a6-b51857b9d320" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>df.groupby([<span class="st">'entity'</span>, <span class="st">'dtype'</span>])[<span class="st">'dtype'</span>].count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">dtype</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">entity</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">activation_input</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>332</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.int64</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">activation_output</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>333</td>
</tr>
<tr class="even">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">gradients</td>
<td data-quarto-table-cell-role="th">None</td>
<td>272</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>420</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">loss</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>1</td>
</tr>
<tr class="odd">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">optimizer_states</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>840</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>420</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>8304</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> int64</label>
</div>
</div>
</section>
<section id="data-type-by-composer-training-step" class="level3">
<h3 class="anchored" data-anchor-id="data-type-by-composer-training-step">Data Type by Composer Training Step</h3>
<div id="cell-38" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:742}}" data-outputid="168d15bc-1303-40c0-cc01-7a529ed5cb5d" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>df.groupby([<span class="st">'training_step'</span>, <span class="st">'entity'</span>, <span class="st">'dtype'</span>])[<span class="st">'dtype'</span>].count()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">dtype</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">training_step</th>
<th data-quarto-table-cell-role="th">entity</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">00_fit_start</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">01_epoch_start</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">02_before_dataloader</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">03_after_dataloader</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">04_batch_start</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">05_before_forward</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="odd">
<td rowspan="3" data-quarto-table-cell-role="th" data-valign="top">06_forward</td>
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">activation_input</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>332</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.int64</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">activation_output</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>333</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">07_after_forward</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">08_before_loss</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="even">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">09_after_loss</td>
<td data-quarto-table-cell-role="th">loss</td>
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">10_before_backward</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="odd">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">11_after_backward</td>
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">gradients</td>
<td data-quarto-table-cell-role="th">None</td>
<td>272</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>420</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12_before_optim_step</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
<tr class="even">
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">13_optimizer_step</td>
<td rowspan="2" data-quarto-table-cell-role="th" data-valign="top">optimizer_states</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>840</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">torch.float32</td>
<td>420</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">14_after_optim_step</td>
<td data-quarto-table-cell-role="th">weights</td>
<td data-quarto-table-cell-role="th">torch.bfloat16</td>
<td>692</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> int64</label>
</div>
</div>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>I absolutely loved this exercise. I learned a ton about callbacks, data types during mixed precision training, and Python fundamentals. Working with LLM-Foundry has opened up a whole universe of learning opportunities as I try to better understand what’s going on under the hood. It’s a gift that keeps giving!</p>
<p>I’m trying to grow <a href="https://www.youtube.com/@vishal_learner">my YouTube channel</a> so please give it a visit and subscribe if you want to stay in the loop.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/vishalbakshi\.github\.io\/blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>