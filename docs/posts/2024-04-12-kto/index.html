<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-04-12">
<meta name="description" content="Exploring the math from the Kahneman Tversky Optimization paper to better understand it.">

<title>vishal bakshi - Paper Math: KTO (Kahneman Tversky Optimization)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#section-4.1.-derivation" id="toc-section-4.1.-derivation" class="nav-link" data-scroll-target="#section-4.1.-derivation">Section 4.1. Derivation</a>
  <ul class="collapse">
  <li><a href="#kto-loss-function-mathcall_ktopi_theta-pi_ref-mathbbe_x-ysimmathcaldwy1---v_ktox-ybeta" id="toc-kto-loss-function-mathcall_ktopi_theta-pi_ref-mathbbe_x-ysimmathcaldwy1---v_ktox-ybeta" class="nav-link" data-scroll-target="#kto-loss-function-mathcall_ktopi_theta-pi_ref-mathbbe_x-ysimmathcaldwy1---v_ktox-ybeta">KTO Loss Function: <span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}[w(y)(1 - v_{KTO}(x, y;\beta))]\]</span></a></li>
  <li><a href="#understanding-z_ref" id="toc-understanding-z_ref" class="nav-link" data-scroll-target="#understanding-z_ref">Understanding <span class="math inline">\(z_{ref}\)</span></a></li>
  </ul></li>
  <li><a href="#section-4-implementation" id="toc-section-4-implementation" class="nav-link" data-scroll-target="#section-4-implementation">Section 4: Implementation</a></li>
  <li><a href="#proposition-3.5-proof" id="toc-proposition-3.5-proof" class="nav-link" data-scroll-target="#proposition-3.5-proof">Proposition 3.5 Proof</a>
  <ul class="collapse">
  <li><a href="#reward-function-r_theta" id="toc-reward-function-r_theta" class="nav-link" data-scroll-target="#reward-function-r_theta">Reward function <span class="math inline">\(r_\theta\)</span></a></li>
  <li><a href="#reference-point-distributions-q_xx-q_yyx" id="toc-reference-point-distributions-q_xx-q_yyx" class="nav-link" data-scroll-target="#reference-point-distributions-q_xx-q_yyx">Reference point distributions <span class="math inline">\(Q_x(X'), Q_y(Y'|X')\)</span></a></li>
  <li><a href="#value-function-v_f" id="toc-value-function-v_f" class="nav-link" data-scroll-target="#value-function-v_f">Value function <span class="math inline">\(v_f\)</span></a></li>
  <li><a href="#dpo-loss" id="toc-dpo-loss" class="nav-link" data-scroll-target="#dpo-loss">DPO Loss</a></li>
  </ul></li>
  <li><a href="#proposition-4.1" id="toc-proposition-4.1" class="nav-link" data-scroll-target="#proposition-4.1">Proposition 4.1</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Math: KTO (Kahneman Tversky Optimization)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper math</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    Exploring the math from the Kahneman Tversky Optimization paper to better understand it.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 12, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this notebook I’ll walk through some of the math involved in the research paper <a href="https://arxiv.org/pdf/2402.01306">KTO: Model Alignment as Prospect Theoretic Optimization</a>. Here’s the abstract:</p>
<blockquote class="blockquote">
<p>Kahneman &amp; Tversky’s prospect theory tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases – the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being human-aware loss functions (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences – only a binary signal of whether an output is desirable or undesirable for a given input. This makes it far easier to use in the real world, where preference data is scarce and expensive.</p>
</blockquote>
<p>The key difference between KTO and other methods (such as RLHF or DPO) is that instead of using preference data, KTO uses a binary signal of <em>desirable</em> or <em>undesirable</em>, as seen in the dataset example on the <a href="https://huggingface.co/docs/trl/main/en/kto_trainer#expected-dataset-format">HuggingFace KTO Trainer page</a> (where <code>True</code> = desired and <code>False</code> = undesired):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>kto_dataset_dict <span class="op">=</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"prompt"</span>: [</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Hey, hello"</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"How are you"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"What is your name?"</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"What is your name?"</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Which is the best programming language?"</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Which is the best programming language?"</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Which is the best programming language?"</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"completion"</span>: [</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"hi nice to meet you"</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"leave me alone"</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"I don't have a name"</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"My name is Mary"</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Python"</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">"C++"</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Java"</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"label"</span>: [</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">True</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">False</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">False</span>,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">True</span>,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">True</span>,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">False</span>,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">False</span>,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the following sections I walk through some of the math in the paper to get a better understanding of the concepts behind them.</p>
</section>
<section id="section-4.1.-derivation" class="level2">
<h2 class="anchored" data-anchor-id="section-4.1.-derivation">Section 4.1. Derivation</h2>
<section id="kto-loss-function-mathcall_ktopi_theta-pi_ref-mathbbe_x-ysimmathcaldwy1---v_ktox-ybeta" class="level3">
<h3 class="anchored" data-anchor-id="kto-loss-function-mathcall_ktopi_theta-pi_ref-mathbbe_x-ysimmathcaldwy1---v_ktox-ybeta">KTO Loss Function: <span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}[w(y)(1 - v_{KTO}(x, y;\beta))]\]</span></h3>
<p>Where:</p>
<p><span class="math display">\[w(y) = \begin{cases}
      \lambda_D &amp; \text{if } y \sim y_{desirable}|x \\
      \lambda_U &amp; \text{if } y \sim y_{undesirable}|x
   \end{cases}\]</span></p>
<p><span class="math display">\[v_{KTO}(x,y;\beta) = \begin{cases}
      \sigma(r_{KTO}(x,y) - z_{ref}) &amp; \text{if } y \sim y_{desirable}|x \\
      \sigma(z_{ref} -r_{KTO}(x,y)) &amp; \text{if } y \sim y_{undesirable}|x
   \end{cases}\]</span></p>
<p><span class="math display">\[z_{ref}= \mathbb{E}_{x'\sim\mathcal{D}}[\beta KL(\pi_\theta(y'|x')||\pi_{ref}(y'|x'))]\]</span></p>
<p><span class="math display">\[r_{KTO}(x,y)=\beta\log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}\]</span></p>
<section id="kto-loss-for-desirable-outputs" class="level4">
<h4 class="anchored" data-anchor-id="kto-loss-for-desirable-outputs">KTO Loss for Desirable Outputs</h4>
<p><span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_D\big(1 - \sigma(r_{KTO} - z_{ref})\big)\big]\]</span></p>
<p>Here is what <span class="math inline">\(1-\sigma(x)\)</span> looks like:</p>
<p><img src="1.png" style="width:100%;"></p>
<p>As the term <span class="math inline">\(r_{KTO} - z_{ref}\)</span> increases (i.e.&nbsp;the reward for desirable outputs increases while KL divergence stays the same or decreases), loss decreases. From the paper:</p>
<blockquote class="blockquote">
<p>Intuitively, KTO works because if the model increases the reward of a desirable example in a generic way, then the KL penalty will also rise and no progress will be made on the loss.</p>
</blockquote>
<p>I think “generic way” has a negative connotation in this statement, meaning that the model is not increasing the reward in the specific way that <span class="math inline">\(\pi_{ref}\)</span>, the supervised fine-tune reference model, was trained to generate outputs in (for whatever the use case is—helpful, honest, harmless, etc.).</p>
<p>From the paper:</p>
<blockquote class="blockquote">
<p>We do not back-propagate through the KL term, as it makes training much more stable. This means that the KL term purely serves to control <strong>how saturated the loss is.</strong> (emphasis mine)</p>
</blockquote>
<p>ChatGPT:</p>
<blockquote class="blockquote">
<p>When the loss for a deep learning model is described as “saturated,” it typically means that the model has reached a point where further training does not significantly decrease the loss anymore. In other words, the model has learned as much as it can from the available data, and additional training iterations are unlikely to improve its performance significantly.</p>
</blockquote>
<p>I think it’s correct to say that <span class="math inline">\(r_{KTO}\)</span> is like the KL divergence between the policy being trained and the reference policy across all input/output training data pairs, while <span class="math inline">\(z_{ref}\)</span> is the KL divergence between the policy being trained and the reference policy across all reference data pairs. I think it’s also correct to continue that logic to say that as <span class="math inline">\(r_{KTO}\)</span> increases, the policy being trained diverges from the reference policy (on training data), and <span class="math inline">\(z_{ref}\)</span> keeps that divergence in check (if the policy being trained diverges too far from the reference policy on the reference data, the loss increases or stays the same).</p>
</section>
<section id="kto-loss-for-undesirable-outputs" class="level4">
<h4 class="anchored" data-anchor-id="kto-loss-for-undesirable-outputs">KTO Loss for Undesirable Outputs</h4>
<p><span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_U\big(1 - \sigma(z_{ref} - r_{KTO})\big)\big]\]</span></p>
<p>This has the same <span class="math inline">\(1-\sigma(x)\)</span> form. If the reward for an UNdesirable output increases while the KL term <span class="math inline">\(z_{ref}\)</span> stays the same, the loss will increase.</p>
</section>
</section>
<section id="understanding-z_ref" class="level3">
<h3 class="anchored" data-anchor-id="understanding-z_ref">Understanding <span class="math inline">\(z_{ref}\)</span></h3>
<p>From the paper:</p>
<blockquote class="blockquote">
<p>Rather than having just one dispreferred generation <span class="math inline">\(y_l|x\)</span> as the reference point, we assume that humans judge the quality of <span class="math inline">\((x,y)\)</span> in relation to all input-output pairs they have seen.</p>
</blockquote>
<p>The phrase “rather than having just one dispreferred generation <span class="math inline">\(y_l|x\)</span> as the reference point” I think is referring to the DPO loss function, specifically the second term inside log sigmoid:</p>
<p><span class="math display">\[\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\log\sigma\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\big]\]</span></p>
<p>Which in the KTO loss function is captured by <span class="math inline">\(z_{ref}\)</span> across all <span class="math inline">\(y'\)</span> outputs, not just <span class="math inline">\(y_l\)</span>:</p>
<p><span class="math display">\[z_{ref}= \mathbb{E}_{x'\sim\mathcal{D}}[\beta KL(\pi_\theta(y'|x')||\pi_{ref}(y'|x'))]\]</span></p>
<p>I may be taking that phrase from the paper too literally, so I may be wrong about this.</p>
<p>From the paper:</p>
<blockquote class="blockquote">
<p>we write the reference point to be the expected reward under the optimal policy, not just for generation following <span class="math inline">\(x\)</span> but following any input <span class="math inline">\(x': \mathbb{E}_{x' \sim \mathcal{D}, y \sim \pi^*}\big[ r^*(x',y')\big]\)</span>. Under the assumption that the expected value of the partition function across <span class="math inline">\(x'\)</span> is zero, this simplifies to the KL divergence between <span class="math inline">\(\pi^*\)</span> and <span class="math inline">\(\pi_{ref}\)</span> scaled by <span class="math inline">\(\beta\)</span>.</p>
</blockquote>
<p>Where</p>
<p><span class="math display">\[r^*(x,y) = \beta\log\frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta\log Z(x)\]</span></p>
<p>becomes:</p>
<p><span class="math display">\[z_{ref}= \mathbb{E}_{x'\sim\mathcal{D}}[\beta KL(\pi_\theta(y'|x')||\pi_{ref}(y'|x'))]\]</span></p>
<p>and I think it’s correct to rewrite <span class="math inline">\(z_{ref}\)</span> as the following (ChatGPT agrees):</p>
<p><span class="math display">\[z_{ref}= \mathbb{E}_{x'\sim\mathcal{D}}\big[\beta \log\frac{\pi_\theta(y'|x')}{\pi_{ref}(y'|x'))}\big]\]</span></p>
<p>Last thing about <span class="math inline">\(z_{ref}\)</span>:</p>
<p>the expectation is across <span class="math inline">\(x'\)</span> only (i.e.&nbsp;<span class="math inline">\(\mathbb{E}_{x' \sim \mathcal{D}}\)</span>). From ChatGPT:</p>
<blockquote class="blockquote">
<p>this expression is dependent on <span class="math inline">\(y'\)</span>. Both <span class="math inline">\(\pi_\theta(y'|x')\)</span> and <span class="math inline">\(\pi_{ref}(y'|x')\)</span> are conditional probability distributions where the probability of <span class="math inline">\(y'\)</span> depends on <span class="math inline">\(x'\)</span>. So, the KL divergence between these two distributions also depends on <span class="math inline">\(y'\)</span> indirectly through <span class="math inline">\(x'\)</span>. However, <span class="math inline">\(z_{ref}\)</span> itself does not directly depend on <span class="math inline">\(y'\)</span>, as it represents the expected value of the KL divergence over all possible values of <span class="math inline">\(x'\)</span>.</p>
</blockquote>
</section>
</section>
<section id="section-4-implementation" class="level2">
<h2 class="anchored" data-anchor-id="section-4-implementation">Section 4: Implementation</h2>
<p>From the “Implementation” subsection in section 4. Kahneman-Tversky Optimization:</p>
<blockquote class="blockquote">
<p>In practice, we estimate the KL term by matching inputs <span class="math inline">\(x'\)</span> with unrelated outputs <span class="math inline">\(y'_U\)</span> in a batch of size <span class="math inline">\(m\)</span> and then calculating:</p>
</blockquote>
<p><span class="math display">\[max\big( 0, \frac{1}{m}\sum\log\frac{\pi_\theta(y'_U|x')}{\pi_{ref}(y'_U|x')}\big)\]</span></p>
<p>In the <a href="https://github.com/huggingface/trl/blob/995f1174da89da4dc0ad04c45de11d67b6d06274/trl/trainer/dpo_trainer.py#L894">TRL library</a> they have the following comment which I believe refers to the above excerpt:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a> <span class="co"># As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The code implementation of KTO loss:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># eqn (7) of the HALOs paper</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>chosen_KL <span class="op">=</span> (policy_chosen_logps <span class="op">-</span> reference_chosen_logps).mean().clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>rejected_KL <span class="op">=</span> (policy_rejected_logps <span class="op">-</span> reference_rejected_logps).mean().clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>chosen_logratios <span class="op">=</span> policy_chosen_logps <span class="op">-</span> reference_chosen_logps</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>rejected_logratios <span class="op">=</span> policy_rejected_logps <span class="op">-</span> reference_rejected_logps</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> torch.cat(</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="dv">1</span> <span class="op">-</span> F.sigmoid(<span class="va">self</span>.beta <span class="op">*</span> (chosen_logratios <span class="op">-</span> rejected_KL)),</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="dv">1</span> <span class="op">-</span> F.sigmoid(<span class="va">self</span>.beta <span class="op">*</span> (chosen_KL <span class="op">-</span> rejected_logratios)),</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I find that nomenclature used in the paper a bit confusing since elsewhere in the paper they use the subscript <span class="math inline">\(U\)</span> to represent “undesirable” but here they use it to mean “unrelated”. After looking at the code, I think by “unrelated” they mean that when calculating desired loss they use the undesirable KL and vice versa.</p>
<p>Note that <code>chosen_KL</code> is just the (clamped) mean of <code>chosen_logratios</code>. As is <code>rejected_KL</code> to <code>rejected_logratios</code>.</p>
<p>The first loss in <code>losses</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="op">-</span> F.sigmoid(<span class="va">self</span>.beta <span class="op">*</span> (chosen_logratios <span class="op">-</span> rejected_KL)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Corresponds to KTO loss for desired outputs:</p>
<p><span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_D\big(1 - \sigma(r_{KTO} - z_{ref})\big)\big]\]</span></p>
<p>Although I don’t know why they are not multiplying by <span class="math inline">\(\lambda_D\)</span>.</p>
<p>The second loss in <code>losses</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="op">-</span> F.sigmoid(<span class="va">self</span>.beta <span class="op">*</span> (chosen_KL <span class="op">-</span> rejected_logratios))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Corresponds to the KTO loss for undesired outputs:</p>
<p><span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_U\big(1 - \sigma(z_{ref} - r_{KTO})\big)\big]\]</span></p>
<p>Again, I don’t know why they are not multiplying by <span class="math inline">\(\lambda_U\)</span>.</p>
<p>Also, I’m not sure why they are concatenating a <code>0</code> to the two loss function in <code>losses</code>.</p>
</section>
<section id="proposition-3.5-proof" class="level2">
<h2 class="anchored" data-anchor-id="proposition-3.5-proof">Proposition 3.5 Proof</h2>
<p>For a loss to be a HALO (human-aware loss function) it needs to be expressible as:</p>
<p><span class="math display">\[f(x,y;\theta) = t(v_f(r_\theta(x,y) - \mathbb{E}_{x'\sim Q'_x, y' \sim Q'_y}[r_\theta(x',y')]))\]</span></p>
<p>with a parameterized reward function <span class="math inline">\(r_\theta\)</span>, reference point distributions <span class="math inline">\(Q_x(X'), Q_y(Y'|X')\)</span>, value function <span class="math inline">\(v_f\)</span> and a negative affine function <span class="math inline">\(t\)</span>.</p>
<section id="reward-function-r_theta" class="level3">
<h3 class="anchored" data-anchor-id="reward-function-r_theta">Reward function <span class="math inline">\(r_\theta\)</span></h3>
<p>The reward function <span class="math inline">\(r_\theta\)</span> needs to satisfy the following expression:</p>
<p><span class="math display">\[\forall(x_1,y_1), (x_2,y_2) \in \mathcal{X} \times \mathcal{Y}, \; r_\theta(x_1,y_1) &gt; r_\theta(x_2,y_2) \iff (x_1,y_1) \succ_{r_\theta} (x_2,y_2)\]</span></p>
<p>This expression reads as (ChatGPT):</p>
<p>For all pairs of points <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2, y_2)\)</span> belonging to sets <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> respectively, the value of the function <span class="math inline">\(r_\theta\)</span> applied to the first pair <span class="math inline">\((x_1, y_1)\)</span> is greater than the value of the function <span class="math inline">\(r_\theta\)</span>applied to the second pair <span class="math inline">\((x_2, y_2)\)</span> if and only if the first pair <span class="math inline">\((x_1, y_1)\)</span> is preferred to the second pair <span class="math inline">\((x_2, y_2)\)</span> according to the relation <span class="math inline">\(\succ_{r_\theta}\)</span>.</p>
<p>Explanation of symbols:</p>
<p><span class="math inline">\(\forall\)</span>: for all</p>
<p><span class="math inline">\(\in\)</span>: in</p>
<p><span class="math inline">\(\iff\)</span>: if and only if</p>
<p><span class="math inline">\(\succ\)</span>: succeeds operator (indicates <a href="https://en.wikipedia.org/wiki/Preference_(economics)">preference</a>)</p>
</section>
<section id="reference-point-distributions-q_xx-q_yyx" class="level3">
<h3 class="anchored" data-anchor-id="reference-point-distributions-q_xx-q_yyx">Reference point distributions <span class="math inline">\(Q_x(X'), Q_y(Y'|X')\)</span></h3>
<p>In section 3.2 of the paper they define a reference point as:</p>
<blockquote class="blockquote">
<p>input-output pairs sampled from the distributions <span class="math inline">\(Q_x, Q_y\)</span>.</p>
</blockquote>
<p>From what I understand, the <span class="math inline">\('\)</span> in <span class="math inline">\(X'\)</span> and <span class="math inline">\(Y'\)</span> indicates that it is a different, reference input and output (respectively) from the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> used as the training dataset.</p>
<p>ChatGPT:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are fixed values or points in the domain, while <span class="math inline">\(x'\)</span> and <span class="math inline">\(y'\)</span> are variables representing points randomly sampled from the distributions <span class="math inline">\(Q_{x'}\)</span>​ and <span class="math inline">\(Q_{y'}\)</span>​ respectively. These samples are used to calculate the expected value <span class="math inline">\(\mathbb{E}\)</span> over those distributions.</p>
</blockquote>
</section>
<section id="value-function-v_f" class="level3">
<h3 class="anchored" data-anchor-id="value-function-v_f">Value function <span class="math inline">\(v_f\)</span></h3>
<p>The value function is defined as this expression:</p>
<p><span class="math display">\[v_f: \mathbb{R} \to \mathbb{R}\]</span></p>
<p>Which can be read as (ChatGPT):</p>
<blockquote class="blockquote">
<p>“The function <span class="math inline">\(v_f\)</span> maps real numbers to real numbers.”</p>
<p>Here, <span class="math inline">\(\mathbb{R}\)</span> represents the set of real numbers, and the notation <span class="math inline">\(v_f: \mathbb{R} \to \mathbb{R}\)</span> specifies that the function <span class="math inline">\(v_f\)</span>​ takes inputs from the set of real numbers and produces outputs that are also real numbers.</p>
</blockquote>
<p>The value function must be monotonic non-decreasing and concave in <span class="math inline">\((0, \infty)\)</span>.</p>
<p><strong>Monotonic</strong>: varying in such a way that it either never decreases or never increases.</p>
<p><strong>Non-decreasing</strong>: self-explanatory (the function never decreases)</p>
<p><strong>Concave</strong>: A concave function is one in which the slope is continually decreasing (note that the slope is decreasing, not the function) <a href="https://study.com/academy/lesson/concave-definition-shape-function.html">source</a>.</p>
<p><strong><span class="math inline">\((0, \infty)\)</span></strong>: the set of all real numbers greater than 0 but not including 0 itself, extending infinitely towards positive infinity</p>
<p><br></p>
<p>Example of a function that is monotonic non-decreasing and concave in <span class="math inline">\((0, \infty)\)</span> is <span class="math inline">\(log(x)\)</span>:</p>
<p><img src="2.png" style="width:100%;"></p>
</section>
<section id="dpo-loss" class="level3">
<h3 class="anchored" data-anchor-id="dpo-loss">DPO Loss</h3>
<p>The DPO loss function is in the form:</p>
<p><span class="math display">\[\mathcal{L}_{DPO}(\pi_\theta, \pi_{ref}) = \mathbb{E}\big[-\log\sigma\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\big]\]</span></p>
<p>This is expressible as:</p>
<p><span class="math display">\[t(v_f(r_\theta(x,y) - \mathbb{E}_{x'\sim Q'_x, y' \sim Q'_y}[r_\theta(x',y')]))\]</span></p>
<p>Mapping the DPO loss to this expression:</p>
<p><span class="math inline">\(r_\theta\)</span> is the DPO reward <span class="math inline">\(\beta\log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}\)</span> which is passed as an input into the value function <span class="math inline">\(v_f\)</span> which in this case is <span class="math inline">\(\log\sigma\)</span> (monotonic non-decreasing and concave everywhere) the output of which is passed into <span class="math inline">\(t\)</span> which takes the negative of the input, <span class="math inline">\(t(x) = -x\)</span>.</p>
<p>I didn’t understand the following line in the paper:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Q_x\)</span> places all mass on <span class="math inline">\(x\)</span> and <span class="math inline">\(Q_y\)</span> places all mass on the dispreferred output <span class="math inline">\(y_l\)</span> for <span class="math inline">\(x\)</span> such that <span class="math inline">\(y \succ y_l\)</span>.</p>
</blockquote>
<p>Which ChatGPT explains as:</p>
<blockquote class="blockquote">
<p>In the context of reinforcement learning and optimization, <span class="math inline">\(Q_x\)</span> and <span class="math inline">\(Q_y\)</span> represent probability distributions. When it’s stated that “<span class="math inline">\(Q_x\)</span> places all mass on <span class="math inline">\(x\)</span>” and “<span class="math inline">\(Q_y\)</span> places all mass on the dispreferred output <span class="math inline">\(y_l\)</span> for <span class="math inline">\(x\)</span> such that <span class="math inline">\(y \succ y_l\)</span>,” it means that these distributions are highly concentrated on specific values.</p>
<p>Let’s break it down:</p>
<p><span class="math inline">\(Q_x\)</span> is a distribution over inputs <span class="math inline">\(x\)</span>. Saying that it places all mass on <span class="math inline">\(x\)</span> means that it assigns a probability of 1 to a particular input <span class="math inline">\(x\)</span> and 0 to all other possible inputs. In other words, <span class="math inline">\(Q_x\)</span> is a degenerate distribution that only considers a single input <span class="math inline">\(x\)</span>.</p>
<p>Similarly, <span class="math inline">\(Q_y\)</span> is a distribution over outputs <span class="math inline">\(y\)</span>. However, in this case, it places all mass on the dispreferred output <span class="math inline">\(y_l\)</span> for a given input <span class="math inline">\(x\)</span>. This means that for every input <span class="math inline">\(x\)</span>, <span class="math inline">\(Q_y\)</span> assigns a probability of 1 to the dispreferred output <span class="math inline">\(y_l\)</span> and 0 to all other possible outputs. Additionally, it’s mentioned that <span class="math inline">\(y \succ y_l\)</span>, which indicates that the preferred output <span class="math inline">\(y\)</span> is preferred over <span class="math inline">\(y_l\)</span>.</p>
</blockquote>
<p>If that is correct, then I would add the following to complete the construction of DPO loss as the HALO expression:</p>
<p>Since <span class="math inline">\(y \succ y_l\)</span>, I interpret that <span class="math inline">\(y\)</span> in <span class="math inline">\(r_\theta(x,y)\)</span> corresponds to <span class="math inline">\(y_w\)</span> and is represented in the DPO loss function by the term <span class="math inline">\(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}\)</span>.</p>
<p>Then because <span class="math inline">\(\mathbb{E}_{x'\sim Q'_x, y' \sim Q'_y}[r_\theta(x',y')]\)</span> is subracted from <span class="math inline">\(r_\theta(x,y)\)</span> in the generic HALO expression, it corresponds to <span class="math inline">\(\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\)</span> in the DPO loss function.</p>
<p>I am not confident about this last part. I’m also not going to do a walkthrough of the SLiC and PPO-Clip loss functions since I haven’t read those papers.</p>
</section>
</section>
<section id="proposition-4.1" class="level2">
<h2 class="anchored" data-anchor-id="proposition-4.1">Proposition 4.1</h2>
<p>KTO does not learn from undesirable examples with sufficiently high rewards or desirable examples with sufficiently low rewards.</p>
<p>In this section they provide the derivative of KTO loss without derivation. I will try to derive it from the KTO loss. Here is the final form of the derivative in the paper:</p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{KTO}(\pi_\theta. \pi_{ref}) = \mathbb{E}_{x, y \sim \mathcal{D}}\big[ \lambda(y) \sigma(z) \sigma(-z) \nabla \beta \log \pi_\theta(y|x)\big]\]</span></p>
<p>Where</p>
<p><span class="math display">\[z = r_{KTO}(x,y) - z_{ref}\]</span></p>
<p><br></p>
<p><span class="math display">\[\lambda(y) = \begin{cases}
      -\lambda_D &amp; \text{if } y \text{ is desirable} \\
      \lambda_U &amp; \text{if } y \text{ is undesirable}
   \end{cases}\]</span></p>
<p><strong>I’ll start by rewriting the <em>desirable</em> KTO loss function with <span class="math inline">\(z\)</span> as defined above:</strong></p>
<p><span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_D\big(1 - \sigma(r_{KTO} - z_{ref})\big)\big] = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_D\big(1 - \sigma(z)\big)\big]\]</span></p>
<p>The derivative of $_D(1-(z)) with respect to <span class="math inline">\(\theta\)</span> I’ll write as:</p>
<p><span class="math display">\[\frac{dz}{d\theta} \times \frac{d}{dz}\lambda_D(1-\sigma(z))\]</span></p>
<p>Starting with the rightmost term, pulling out the constant <span class="math inline">\(\lambda_D\)</span>, given that the derivative of 1 with respect to <span class="math inline">\(z\)</span> is 0, multiplying by -1 (chain rule), and given the property of sigmoid that <span class="math inline">\(\sigma(-z) = 1 - \sigma(z)\)</span>:</p>
<p><span class="math display">\[\frac{d}{dz}\lambda_D(1-\sigma(z)) = \lambda_D\frac{d}{dz}(1-\sigma(z)) = -\lambda_D\frac{d}{dz}\sigma(-z) = -\lambda_D\big[\sigma(z)(1 - \sigma(z))\big] = -\lambda_D\sigma(z)\sigma(-z)\]</span></p>
<p>I’ll do <span class="math inline">\(\frac{dz}{d\theta}\)</span> next:</p>
<p><span class="math display">\[\frac{dz}{d\theta} = \frac{d}{d\theta}\big(r_{KTO}(x,y) - z_{ref}\big) = \frac{d}{d\theta}\big(\beta\log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} -  \mathbb{E}_{x'\sim\mathcal{D}}[\beta KL(\pi_\theta(y'|x')||\pi_{ref}(y'|x'))]\big)\]</span></p>
<p>They mention that they don’t backpropagate through the KL term so I think I can interpret that as meaning the KL term’s gradient is 0. We’re left with:</p>
<p><span class="math display">\[\frac{d}{d\theta}\big(\beta\log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}\big) = \beta\frac{d}{d\theta}\big(log\pi_\theta(y|x) - \log\pi_{ref}(y|x)\big)\]</span></p>
<p>The reference model is not changing (i.e it’s not parameterized by <span class="math inline">\(\theta\)</span> and is not being trained) so its derivative with respect to <span class="math inline">\(\theta\)</span> is 0. That leaves us with:</p>
<p><span class="math display">\[\beta\frac{d}{d\theta}log\pi_\theta(y|x)\]</span></p>
<p>Multiplying by <span class="math inline">\(\frac{d}{dz}\sigma(z)\)</span> to get the complete <span class="math inline">\(\frac{d}{d\theta}\sigma(z)\)</span>:</p>
<p><span class="math display">\[\frac{d}{d\theta}\sigma(z) = -\lambda_D\sigma(z)\sigma(-z)\beta\frac{d}{d\theta}log\pi_\theta(y|x)\]</span></p>
<p>Putting it back in the full form of the loss gradient:</p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{KTO}(\pi_\theta. \pi_{ref}) = \mathbb{E}_{x, y \sim \mathcal{D}}\big[ -\lambda_D \sigma(z) \sigma(-z) \beta\frac{d}{d\theta}log\pi_\theta(y|x)\big]\]</span></p>
<p>Which seems equivalent to the gradient of the KTO loss provided in the paper (although I’m not sure why they have <span class="math inline">\(\beta\)</span> inside the gradient symbol <span class="math inline">\(\nabla\)</span>):</p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{KTO}(\pi_\theta. \pi_{ref}) = \mathbb{E}_{x, y \sim \mathcal{D}}\big[ \lambda(y) \sigma(z) \sigma(-z) \nabla \beta \log \pi_\theta(y|x)\big]\]</span></p>
<p><strong>Next, I’ll derive the gradient of the loss function for <em>undesirable</em> <span class="math inline">\(y\)</span> values, starting with the loss function:</strong></p>
<p><span class="math display">\[\mathcal{L}_{KTO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_U\big(1 - \sigma(z_{ref} - r_{KTO})\big)\big] = \mathbb{E}_{x, y\sim\mathcal{D}}\big[\lambda_U\big(1 - \sigma(-z)\big)\big]\]</span></p>
<p>Where</p>
<p><span class="math display">\[z = r_{KTO}(x,y) - z_{ref}\]</span></p>
<p>so <span class="math display">\[-z = z_{ref} - r_{KTO}(x,y)\]</span></p>
<p>The derivative of <span class="math inline">\(\lambda_U(1-\sigma(-z))\)</span> with respect to <span class="math inline">\(\theta\)</span> I’ll write as:</p>
<p><span class="math display">\[\frac{dz}{d\theta} \times \frac{d}{dz}\lambda_U(1-\sigma(-z))\]</span></p>
<p>The derivative of <span class="math inline">\(\lambda_U(1-\sigma(-z))\)</span> with respect to <span class="math inline">\(z\)</span> involves pulling out the constant <span class="math inline">\(\lambda_U\)</span>, pulling out the constant -1 from <span class="math inline">\(-\sigma(z)\)</span>, multiplying by -1 (chain rule), and the derivative of 1 with respect to <span class="math inline">\(\theta\)</span> going to 0:</p>
<p><span class="math display">\[\frac{d}{dz}\lambda_U(1-\sigma(-z)) = \frac{d}{dz}\lambda_U(1-\sigma(-z)) = -1 \times -1 \times \lambda_U\frac{d}{dz}\sigma(-z) = \lambda_U\frac{d}{dz}\sigma(-z)\]</span></p>
<p>Given that <span class="math inline">\(\sigma(-z) = 1 - \sigma(z)\)</span>:</p>
<p><span class="math display">\[\lambda_U\frac{d}{dz}\sigma(-z) = \lambda_U\sigma(-z)(1 - \sigma(-z))= \lambda_U\sigma(-z)\big[1 - (1-\sigma(z))\big] = \lambda_U\sigma(-z)\big[1-1+\sigma(z)\big] = \lambda_U\sigma(-z)\sigma(z)\]</span></p>
<p>The derivative of <span class="math inline">\(z\)</span> with respect to <span class="math inline">\(\theta\)</span> is the same as before:</p>
<p><span class="math display">\[\beta\frac{d}{d\theta}log\pi_\theta(y|x)\]</span></p>
<p>Multiplying the two derivative terms together:</p>
<p><span class="math display">\[\lambda_U\sigma(-z)\sigma(z)\beta\frac{d}{d\theta}log\pi_\theta(y|x)\]</span></p>
<p>Plugging it all back to get the loss gradient function for <strong>undesired</strong> outputs:</p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{KTO}(\pi_\theta. \pi_{ref}) = \mathbb{E}_{x, y \sim \mathcal{D}}\big[ \lambda_U \sigma(z) \sigma(-z) \nabla \beta \log \pi_\theta(y|x)\big]\]</span></p>
<p>Comparing that to the loss derivative for <strong>desired</strong> outputs, the difference is the minus sign:</p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{KTO}(\pi_\theta. \pi_{ref}) = \mathbb{E}_{x, y \sim \mathcal{D}}\big[ -\lambda_D \sigma(z) \sigma(-z) \beta\frac{d}{d\theta}log\pi_\theta(y|x)\big]\]</span></p>
<p>The intuition behind the minus sign given in the paper:</p>
<blockquote class="blockquote">
<p>if <span class="math inline">\(y\)</span> is desirable, then <span class="math inline">\(\lambda(y)\)</span> is negative and we push up the probability of <span class="math inline">\(\pi_\theta(y|x)\)</span> to minimize the loss; we do the opposite if <span class="math inline">\(y\)</span> is undesirable.</p>
</blockquote>
<p>Proposition 4.1 states that “KTO does not learn from undesirable examples with sufficiently high rewards or desirable examples with sufficiently low rewards” and the paper explains that:</p>
<blockquote class="blockquote">
<p>As <span class="math inline">\(z\)</span> tends to <span class="math inline">\(\pm\infty\)</span>, the gradient will tend to zero since either <span class="math inline">\(\sigma(-z)\)</span> or <span class="math inline">\(\sigma(z)\)</span> will tend to zero. Since <span class="math inline">\(z\)</span> is increasing in the reward, this means that sufficiently large and sufficiently small rewards will yield a gradient of zero.</p>
</blockquote>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>