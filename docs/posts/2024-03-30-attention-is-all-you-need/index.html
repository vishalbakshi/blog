<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-03-30">
<meta name="description" content="A summary of research introducing the Transformer architecture and a code walkthrough for the Encoder and Decoder.">

<title>vishal bakshi - Paper Summary: Attention is All You Need</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#sequence-modeling-review" id="toc-sequence-modeling-review" class="nav-link" data-scroll-target="#sequence-modeling-review">Sequence Modeling Review</a>
  <ul class="collapse">
  <li><a href="#recurrent-neural-nets-rnns" id="toc-recurrent-neural-nets-rnns" class="nav-link" data-scroll-target="#recurrent-neural-nets-rnns">Recurrent Neural Nets (RNNs)</a></li>
  <li><a href="#long-short-term-memory-lstm" id="toc-long-short-term-memory-lstm" class="nav-link" data-scroll-target="#long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</a></li>
  </ul></li>
  <li><a href="#transformer-architecture-overview" id="toc-transformer-architecture-overview" class="nav-link" data-scroll-target="#transformer-architecture-overview">Transformer Architecture Overview</a></li>
  <li><a href="#code-overview-encoder" id="toc-code-overview-encoder" class="nav-link" data-scroll-target="#code-overview-encoder">Code Overview: Encoder</a>
  <ul class="collapse">
  <li><a href="#input-embedding" id="toc-input-embedding" class="nav-link" data-scroll-target="#input-embedding">Input Embedding</a></li>
  <li><a href="#positional-encodings" id="toc-positional-encodings" class="nav-link" data-scroll-target="#positional-encodings">Positional Encodings</a></li>
  <li><a href="#attention-mechanism" id="toc-attention-mechanism" class="nav-link" data-scroll-target="#attention-mechanism">Attention Mechanism</a></li>
  </ul></li>
  <li><a href="#code-overview-decoder" id="toc-code-overview-decoder" class="nav-link" data-scroll-target="#code-overview-decoder">Code Overview: Decoder</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Summary: Attention is All You Need</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper summary</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    A summary of research introducing the Transformer architecture and a code walkthrough for the Encoder and Decoder.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 30, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this notebook I’ll provide a summary of the <a href="">Attention is All You Need</a> paper. I’ll also heavily reference the fantastic code walkthroughs by CodeEmporium on YouTube for the <a href="https://youtu.be/g3sEsBGkLU0?feature=shared">Encoder</a> and <a href="https://youtu.be/MqDehUoMk-E?feature=shared">Decoder</a>.</p>
<p>Other resources that were critical to my understanding of this paper:</p>
<ul>
<li>Benjamin Warner’s two-part blog post on creating a transformer from scratch (<a href="https://benjaminwarner.dev/2023/07/01/attention-mechanism">Attention mechanism</a> and the <a href="https://benjaminwarner.dev/2023/07/28/rest-of-the-transformer">rest of the transformer</a>).</li>
<li><a href="https://maximilian-weichart.de/posts/rnn-1/">Introduction to RNNs</a> by Max Weichart.</li>
<li><a href="">The Illustrated Transformer</a> and <a href="https://maximilian-weichart.de/posts/rnn-1/">The Illustrated GPT-2</a> by Jay Alammar.</li>
<li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> by Christopher Olah.</li>
<li><a href="https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder">Understanding Encoder and Decoder LLMs</a> by Sebastian Raschka.</li>
<li><a href="https://txt.cohere.com/sentence-word-embeddings/">What are Word and Sentence Embeddings?</a> by Cohere.</li>
<li><a href="https://www.youtube.com/watch?v=4Bdc55j80l8">Illustrated Guide to Transformers Neural Network: A step by step explanation</a> by the AI Hacker.</li>
</ul>
</section>
<section id="sequence-modeling-review" class="level2">
<h2 class="anchored" data-anchor-id="sequence-modeling-review">Sequence Modeling Review</h2>
<p>Before getting into the details of the Transformer architecture introduced in this paper, I’ll do a short overview of the main type of architecture (RNN) that the Transformer is improving upon. Most importantly, the Transformer is improves upon the dependencies between tokens in long sequences.</p>
<section id="recurrent-neural-nets-rnns" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-nets-rnns">Recurrent Neural Nets (RNNs)</h3>
<p>In Max’s post he provides the following illustration of RNNs, where the inputs are recursively passed through the hidden laye and at each iteration, the hidden layer state from the previous step is incorporated in the current state’s calculation. In this way, RNNs store information about the previous step in the next step.</p>
<p>The <code>forward</code> pass in Max’s post is given as:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, X, i<span class="op">=</span><span class="dv">0</span>, h<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  l1 <span class="op">=</span> lin(X[i], <span class="va">self</span>.w1)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  h <span class="op">=</span> relu(l1 <span class="op">+</span> h<span class="op">*</span><span class="va">self</span>.w2)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span> <span class="op">!=</span> <span class="bu">len</span>(X)):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.foward(X, i<span class="op">+</span><span class="dv">1</span>, h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The hidden state from the previous iteration <code>h</code> is multiplied by a trainable weight <code>w2</code>, added to the output of the linear function <code>lin(X[i], self.w1)</code> and passed through a non-linearity (in this case a ReLU) to get the current state <code>h</code>. Until the end of the input sequence is reached, the forward pass continues to recursively incorporate previous state information into the current input’s calculation.</p>
<p><img src="2.png" style="width:100%;"></p>
<blockquote class="blockquote">
<p>The study of RNNs highlights how, in the basic RNN architecture, as the time instants considered increase, the product chain determined by backpropagation through time <strong>tends to zero</strong> or <strong>tends to extremely large values</strong>. In the first case, we have a <strong>vanishing gradient</strong>, in the second case an <strong>exploding gradient</strong>. (<a href="https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c">source</a>).</p>
</blockquote>
</section>
<section id="long-short-term-memory-lstm" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h3>
<p>To combat this training instability for long sequences, the LSTM network is used. This is an RNN architecutre capable of learning long-term dependencies with long-term memory (cell state C in the diagram) and short-term memory (hidden state H in the diagram).</p>
<p><img src="3.png" style="width:100%;"></p>
<p>(<a href="https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c">source</a>).</p>
<p>The LSTM uses past information (H) and new information (X) to <strong>update long-term memory (C)</strong>. It then uses C to <strong>update H</strong>, and the cycle continues for the next input sequence.</p>
<p>In the diagram below, at the bottom left, the Hidden state from the previous step, <span class="math inline">\(\textbf{H}_{t-1}\)</span> is combined with the new Input of the current step <span class="math inline">\(\textbf{X}_t\)</span> and goes into the different gates for different purposes (Forget gate, Input gate, Candidate memory, and Output gate).</p>
<p>The <span class="math inline">\(+\)</span> operator is the combining of long-term memory from the previous step <span class="math inline">\(\textbf{C}_{t-1}\)</span> with the output of the Candidate memory <span class="math inline">\(\tilde{C_t}\)</span>.</p>
<p>Finally, the Output gate <span class="math inline">\(\textbf{O}_t\)</span> combines the long-term memory <span class="math inline">\(\textbf{C}_t\)</span> with the sigmoid output of <span class="math inline">\(\textbf{H}_{t-1}\)</span> and <span class="math inline">\(\textbf{X}_t\)</span> to create the hidden state for the current step <span class="math inline">\(\textbf{H}_t\)</span>.</p>
<p>The hidden state and long-term memory are then used in the next input step.</p>
<p><img src="4.png" style="width:100%;"></p>
</section>
</section>
<section id="transformer-architecture-overview" class="level2">
<h2 class="anchored" data-anchor-id="transformer-architecture-overview">Transformer Architecture Overview</h2>
<ul>
<li>Does not use recurrence.</li>
<li>Relies entirely on the <strong>attention mechanism</strong> for global dependencies.</li>
<li>Allows for parallelization (as opposed to sequential processing).</li>
</ul>
<blockquote class="blockquote">
<p>The Transformer achieves better <a href="">BLUE scores</a> than previous state-of-the-art (SOTA) models on the English-to-German and English-to-French machine translation tests at a fraction of the training cost.</p>
</blockquote>
<p><img src="5.png" style="width:100%;"></p>
<p>Here are the current English-to-German SOTA results:</p>
<p><img src="6.png" style="width:100%;"></p>
<p><a href="https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german">source</a></p>
<p>And the current English-to-French SOTA results:</p>
<p><img src="7.png" style="width:100%;"></p>
<p><a href="https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-french">source</a></p>
<p>Here is the paper’s beautiful diagram (with my annotations) of an Encoder-Decoder Transformer architecture:</p>
<p><img src="8.png" style="width:100%;"></p>
<p>The inputs (numericalized tokens) pass through the Input Embedding which projects these numbers into a much larger number of dimensions, dimensions in which different information about the tokens will be learned through training. In this paper they use a dimension of 512 (referred to as the “hidden dimension”). This value is a hyperparameter and different architectures use different numbers of hidden dimensions.</p>
<p>The output of this Embedding is passed through a Positional Encoding step which quantitatively stores information about the position of each token. Since Transformers don’t explicitly express position as sequence modeling does, we have to implicitly express position in this way.</p>
<p>The inputs, after going through the Embedding and Positional Encoding, now enter the <strong>Encoder</strong> which is a type of <strong>Transformer Block</strong> containing Mult-Head Attention, Add &amp; Norm layers and a Feed Forward Network.</p>
<p>The outputs follow a similar path, first through an Output Embedding, then Positional Encoding, and then a <strong>Decoder</strong> which is another type of <strong>Transform Block</strong>. A Transformer can be Encoder-only, Decoder-only or Encoder-Decoder. In this paper they focus on Encoder-Decoder Transformers, where information learned in the Encoder is used in the Decoder in a process call cross-attention that we’ll look into shortly. In this paper, they have 6 Encoder blocks and 6 Decoder blocks. The number of blocks can be varied (i.e.&nbsp;it’s a hyperparameter).</p>
<p>The outputs of the Decoder pass through a final linear layer and then a softmax layer (transformed into 0 to 1.0 probabilities).</p>
<blockquote class="blockquote">
<p>The encoder receives the input text that is to be translated, and the decoder generates the translated text. (<a href="https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder">source</a>)</p>
</blockquote>
<blockquote class="blockquote">
<p>Fundamentally, both encoder- and decoder-style architectures use the same self-attention layers to enocde word tokens. However, the main difference is that <strong>encoders are designed to learn embeddings</strong> that can be used for various predictive modeling tasks such as classification. In contrast, <strong>decoders are designed to generate new texts</strong>, for example, answering user queries. (<a href="https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder">source</a>)</p>
</blockquote>
<blockquote class="blockquote">
<p>The encoder part in the original transformer…<strong>is responsible for understanding and extracting the relevant information from the input text</strong>. It then outputs a continuous representation (embedding) of the input text that is passed to the decoder. Finally, <strong>the decoder generates the translated text</strong> (target language) based on the continuous representation received from the encoder. (<a href="https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder">source</a>)</p>
</blockquote>
<p>From Benjamin Warner’s post:</p>
<blockquote class="blockquote">
<p>If we use the first sentence in this post and assume each word is a token:</p>
<p>Transformers are everywhere.</p>
<p>then the “Transformers” token would predict “are”, and “are” would predict “everywhere.”</p>
<p>To create our inputs (line 1) we’ll drop the last token and to create the labels (line 2) we’ll remove the first token:</p>
<ol type="1">
<li>Transformers are</li>
<li>are everywhere</li>
</ol>
</blockquote>
</section>
<section id="code-overview-encoder" class="level2">
<h2 class="anchored" data-anchor-id="code-overview-encoder">Code Overview: Encoder</h2>
<p>In this section I’ll walk through some of the code presented in the YouTube video by CodeEmporium.</p>
<p>I’ll start by defining some constants that I’ll use throughout. <code>d_model</code> is the hidden dimension hyperparameter.</p>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>max_sequence_length <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">10_000</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>context_size <span class="op">=</span> <span class="dv">200</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In a real scenarios, the inputs (<code>tokens</code>) would be numericalized tokens corresponding to a real natural language dataset. In this example, I’ll use random integers.</p>
<div class="cell" data-outputid="0dbd523c-439d-4a03-98e0-148d9dfb8eb6" data-execution_count="102">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> torch.randint(<span class="dv">0</span>, vocab_size, (batch_size, max_sequence_length))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>tokens.shape, tokens.<span class="bu">min</span>(), tokens.<span class="bu">max</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="102">
<pre><code>(torch.Size([64, 200]), tensor(0), tensor(9999))</code></pre>
</div>
</div>
<p>I have 64 batches of 200 tokens each, where each token is an integer from 0 to 10_000.</p>
<section id="input-embedding" class="level3">
<h3 class="anchored" data-anchor-id="input-embedding">Input Embedding</h3>
<p>The input Embedding is a PyTorch object which takes an integer and returns a tensor of a given dimension (in this case 512).</p>
<div class="cell" data-outputid="4947c29e-1a86-46b0-b8a2-ccb14279e91f" data-execution_count="103">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>vocab_embed <span class="op">=</span> nn.Embedding(vocab_size, d_model)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>vocab_embed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="103">
<pre><code>Embedding(10000, 512)</code></pre>
</div>
</div>
<p>When I pass a tensor integer, I get in return a 512 dimension tensor filled with float values.</p>
<div class="cell" data-outputid="33cc5ecd-f487-47dd-93bf-ac77576741d6" data-execution_count="104">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>vocab_embed(torch.tensor([<span class="dv">4</span>])).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="104">
<pre><code>torch.Size([1, 512])</code></pre>
</div>
</div>
<div class="cell" data-outputid="77a63ecb-d4ec-47cd-e918-a6ccadda8829" data-execution_count="105">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>vocab_embed(torch.tensor([<span class="dv">4</span>]))[<span class="dv">0</span>][:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="105">
<pre><code>tensor([-0.0996,  1.2077, -0.8627, -0.4755,  0.5210], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>When I pass my batched <code>tokens</code> to the Embedding, I get back a batched set of 512 float values:</p>
<div class="cell" data-outputid="95b2b9c4-8398-43d6-d44f-a97913517acf" data-execution_count="106">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>token_embs <span class="op">=</span> vocab_embed(tokens)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>token_embs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre><code>torch.Size([64, 200, 512])</code></pre>
</div>
</div>
<p>In other words, my tokens, which are integers that represent natural language, are now projected into 512 dimensions, dimensions in which the Embedding will learn something about the tokens and therefore about language.</p>
</section>
<section id="positional-encodings" class="level3">
<h3 class="anchored" data-anchor-id="positional-encodings">Positional Encodings</h3>
<p>The formula used in the paper for positional encodings are as follows (sine for even i values and cosine for odd):</p>
<p><span class="math display">\[PE_{(pos, 2i)} = \sin(\text{pos} / 10000^{2i/d_{model}})\]</span> <span class="math display">\[PE_{(pos, 2i+1)} = \cos(\text{pos} / 10000^{2i/d_{model}})\]</span></p>
<p>I’ll reuse the code provided in <a href="https://benjaminwarner.dev/2023/07/28/rest-of-the-transformer">Benjamin’s blog post</a>:</p>
<div class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create the positional encoding tensor of shape</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># maximum sequence length (MS) by embedding dimension (C)</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> torch.zeros(context_size, d_model, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># pre-populate the position and the div_terms</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>position <span class="op">=</span> torch.arange(context_size).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>div_term <span class="op">=</span> torch.exp(</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> (<span class="op">-</span>math.log(<span class="dv">10000</span>) <span class="op">/</span> d_model)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># even positional encodings use sine, odd cosine</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I want to make sure I understand the <code>div_term</code> since I didn’t understand it at first glance:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>torch.exp(torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> (<span class="op">-</span>math.log(<span class="dv">10000</span>) <span class="op">/</span> d_model))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Translating that to math gives us:</p>
<p><span class="math display">\[\exp\big(-2i * \ln(10000) / d_{model}\big)\]</span></p>
<p>Using the negative exponent rule: <span class="math inline">\(\exp(-a) = \frac{1}{\exp(a)}\)</span>:</p>
<p><span class="math display">\[\exp\big({\frac{-2i * ln(10000)}{d_{model}}}\big) = \frac{1}{\exp \big( \frac{2i * ln(10000)}{d_{model}}\big)}\]</span></p>
<p>Using the power of a power rule: <span class="math inline">\(\exp(ab) = \exp(a)^b\)</span>:</p>
<p><span class="math display">\[\frac{1}{\exp \big( \frac{2i * ln(10000)}{d_{model}}\big)} = \frac{1}{\exp\big(\ln(10000)\big)^{2i/d_{model}}}\]</span></p>
<p>The term <span class="math inline">\(\exp(\ln(10000))\)</span> equals just <span class="math inline">\(10000\)</span>:</p>
<p><span class="math display">\[\frac{1}{\exp\big(\ln(10000)\big)^{2i/d_{model}}} = \frac{1}{10000^{2i/d_{model}}}\]</span></p>
<p>Which is the same as the divison term in the paper’s math formula.</p>
<div class="cell" data-outputid="b562b045-e99c-4adb-bba1-1a149789594c" data-execution_count="108">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>pe[<span class="dv">0</span>][:<span class="dv">5</span>], pe[<span class="dv">1</span>][:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre><code>(tensor([0., 1., 0., 1., 0.]),
 tensor([0.8415, 0.5403, 0.8219, 0.5697, 0.8020]))</code></pre>
</div>
</div>
<p>I’ll add the positional encoding to the embedded tokens—note that here PyTorch uses broadcasting to “copy” <code>pe</code> over each of the 64 batches.</p>
<div class="cell" data-outputid="92ff5f5e-fb66-4617-c466-3ee1102a263a" data-execution_count="109">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>pe.shape, token_embs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="109">
<pre><code>(torch.Size([200, 512]), torch.Size([64, 200, 512]))</code></pre>
</div>
</div>
<div class="cell" data-outputid="628f707e-e9ba-4c65-ba49-6a78e8fbf6b3" data-execution_count="110">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>token_embs <span class="op">=</span> token_embs <span class="op">+</span> pe</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>token_embs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="110">
<pre><code>torch.Size([64, 200, 512])</code></pre>
</div>
</div>
<p>In CodeEmporium’s implementation, at this point <code>token_embs</code> is passed through a <code>Dropout</code> layer, so I’ll do the same:</p>
<div class="cell" data-outputid="4bf64501-6e43-44c5-e535-f2e7b911c870" data-execution_count="111">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>embed_drop <span class="op">=</span> nn.Dropout(<span class="fl">0.1</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> embed_drop(token_embs)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="111">
<pre><code>torch.Size([64, 200, 512])</code></pre>
</div>
</div>
</section>
<section id="attention-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="attention-mechanism">Attention Mechanism</h3>
<p>At this point, the inputs are now ready to enter the attention mechanism. Before we do that, I’ll save the current state of the inputs in a variable so that later on I can add it to the output of the attention mechanism.</p>
<div class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>residual_x <span class="op">=</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The particular flavor of attention used at this point is Scaled Dot-Product Attention across multiple heads. Here’s the steps taken in Scaled Dot-Product Attention:</p>
<p><img src="9.png" style="width:100%;"></p>
<p>Where <span class="math inline">\(Q\)</span> (query), <span class="math inline">\(K\)</span> (key) and <span class="math inline">\(V\)</span> (value) are matrices (initially of random numbers) that consist of learned weights during training.</p>
<p>The first step is the matrix multiplication of <span class="math inline">\(Q\)</span> and <span class="math inline">\(K^T\)</span>, followed by scaling that result by the square root of the dimension <span class="math inline">\(d_k\)</span>. The encoder doesn’t have a mask (the decoder does). Finally, the softmax is taken of that scaled dot product and its output matrix multiplied with <span class="math inline">\(V\)</span>.</p>
<p>Here’s a conceptual understanding of attention from the <a href="https://jalammar.github.io/illustrated-gpt2/">Illustrated GPT-2</a>:</p>
<p><img src="10.png" style="width:100%;"></p>
<p>And here’s a visualization of attention values between tokens:</p>
<p><img src="11.png" style="width:100%;"></p>
<p>Before we get into the code for attention, here is a visualization of Mult-Head Attention, where the Scaled Dot-Product Attention occurs simultaneously across multiple heads, displaying the parallelization capability of Transformers:</p>
<p><img src="12.png" style="width:100%;"></p>
<p>We’ll go bottom-up in the diagram:</p>
<ul>
<li>Create Q, K, V matrices. Split them across <span class="math inline">\(h\)</span> heads.</li>
<li>Perform Scaled Dot-Product Attention.</li>
<li>Concatenate them from <span class="math inline">\(h\)</span> heads.</li>
<li>Pass them through a final Linear layer.</li>
</ul>
<p>Both Benjamin and CodeEmporium created a single Linear layer and then split them into <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>, so I’ll do the same. A reminder (to myself and the reader) that these are weight matrices that will be used eventually to multiply by the inputs.</p>
<div class="cell" data-outputid="fcaea4bf-91bb-4805-8d51-56206a952bbb" data-execution_count="113">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>qkv_layer <span class="op">=</span> nn.Linear(d_model, <span class="dv">3</span> <span class="op">*</span> d_model)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>qkv_layer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre><code>Linear(in_features=512, out_features=1536, bias=True)</code></pre>
</div>
</div>
<p>Passing the inputs through this linear layer gives us the matrices:</p>
<div class="cell" data-outputid="e69fbc95-a72c-4d21-d006-3b3ce1016c17" data-execution_count="114">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>qkv <span class="op">=</span> qkv_layer(x)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>qkv.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="114">
<pre><code>torch.Size([64, 200, 1536])</code></pre>
</div>
</div>
<p>Next, we project the <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span> combined matrix across 8 heads</p>
<div class="cell" data-outputid="53aea929-2095-4fbb-e364-838d30780957" data-execution_count="115">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>head_dim <span class="op">=</span> d_model <span class="op">//</span> num_heads</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>qkv <span class="op">=</span> qkv.reshape(batch_size, max_sequence_length, num_heads, <span class="dv">3</span> <span class="op">*</span> head_dim)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>qkv.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="115">
<pre><code>torch.Size([64, 200, 8, 192])</code></pre>
</div>
</div>
<p>This splits the 1536 values into 8 sets of 192.</p>
<p>In CodeEmporium’s code, they swap the middle two dimensions so it’s broadcastable with tensors later on</p>
<div class="cell" data-outputid="8b699017-98c7-4d58-f9d0-cd6559eefae6" data-execution_count="116">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>qkv <span class="op">=</span> qkv.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>qkv.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="116">
<pre><code>torch.Size([64, 8, 200, 192])</code></pre>
</div>
</div>
<p>We then split <code>qkv</code> into three separate matrices, each with 200 x 64 values on each of the 8 heads:</p>
<div class="cell" data-outputid="8473b3a4-112e-457b-ebeb-19a68117225d" data-execution_count="117">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>q, k, v <span class="op">=</span> qkv.chunk(<span class="dv">3</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>q.shape, k.shape, v.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="117">
<pre><code>(torch.Size([64, 8, 200, 64]),
 torch.Size([64, 8, 200, 64]),
 torch.Size([64, 8, 200, 64]))</code></pre>
</div>
</div>
<p>Finally, we can create the attention matrix. First we perform the scaled dot-product between <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span></p>
<div class="cell" data-outputid="284129bf-3ccd-4946-ded0-036d91bc4cb0" data-execution_count="118">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>d_k <span class="op">=</span> torch.tensor(q.shape[<span class="op">-</span><span class="dv">1</span>]) <span class="co"># 64</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>scaled_dot_product <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>)) <span class="op">/</span> torch.sqrt(d_k)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>scaled_dot_product.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="118">
<pre><code>torch.Size([64, 8, 200, 200])</code></pre>
</div>
</div>
<p>Note that when <span class="math inline">\(K\)</span> is transposed, the last two dimensions are swapped to allow for correct matrix multiplication dimension order.</p>
<div class="cell" data-outputid="a711b3a6-3d6d-4882-be3e-d3d2e41d985c" data-execution_count="52">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>q.shape, k.shape, k.transpose(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>(torch.Size([64, 8, 200, 64]),
 torch.Size([64, 8, 200, 64]),
 torch.Size([64, 8, 64, 200]))</code></pre>
</div>
</div>
<p>The dimension of 64 matches between <span class="math inline">\(Q\)</span> and <span class="math inline">\(K^T\)</span> after <code>.transpose(-1 ,-2)</code> swaps the last two dimensions of <span class="math inline">\(K\)</span>.</p>
<p>One thing I noticed in both Benjamin and CodeEmporium’s code is that they define attention as the output of passing the scaled dot-product through softmax. This is the “attention matrix” I’ve seen referred to in places. The paper defines attention as the product of the matrix multiplication between that softmax output and the <span class="math inline">\(V\)</span> (values) matrix.</p>
<p><img src="13.png" style="width:100%;"></p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> F.softmax(scaled_dot_product, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="e58df8a7-d850-4a54-91cc-e8282e8bd944" data-execution_count="93">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>attention.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="93">
<pre><code>torch.Size([64, 8, 200, 200])</code></pre>
</div>
</div>
<p><code>dim</code> is set to <code>-1</code> so that the values in the last dimension are between 0 and 1.</p>
<div class="cell" data-outputid="cb0fb0ba-05a6-4de0-aee3-1bc2f3bfe8e4" data-execution_count="89">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>attention[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>].shape, attention[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>].<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>(torch.Size([200]), tensor(1., grad_fn=&lt;SumBackward0&gt;))</code></pre>
</div>
</div>
<p><code>attention</code>’s final dimensions are of size 200 x 200, representing weights corresponding to the relationship between each of the 200 tokens.</p>
<div class="cell" data-outputid="93492e91-1fbc-4434-95bd-ff4b3f119004" data-execution_count="90">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.matmul(attention, v)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>v.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>torch.Size([64, 8, 200, 64])</code></pre>
</div>
</div>
<p>From Benjamin’s post:</p>
<blockquote class="blockquote">
<p>Next we matrix multiply the Attention weights with our value matrix <span class="math inline">\(V\)</span> which applies the Attention weights to our propagating token embeddings</p>
</blockquote>
<div class="cell" data-outputid="ae602d87-64c7-407b-8a92-bfc7f48c0130" data-execution_count="91">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>torch.Size([64, 200, 512])</code></pre>
</div>
</div>
<div class="cell" data-outputid="e2efde1c-ace5-47bd-913c-7609a4d2a85d" data-execution_count="92">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> attention <span class="op">@</span> v</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>torch.Size([64, 8, 200, 64])</code></pre>
</div>
</div>
<p>Next, we have to concatenate across the 8 heads:</p>
<div class="cell" data-outputid="e595a3fc-bf0d-4ed0-b206-dee32d5486cd" data-execution_count="94">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.reshape(batch_size, max_sequence_length, num_heads <span class="op">*</span> head_dim)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>torch.Size([64, 200, 512])</code></pre>
</div>
</div>
<p>Now the 64 dimensions across 8 heads are concatenated to get back to the embedding size of 512. We still maintain the 64 batches and 200 sequence length.</p>
<p>The last step before the attention mechanism is fully complete is to pass these values through a linear layer:</p>
<div class="cell" data-outputid="c2ba3590-9b0a-44ea-febb-4f67c36d3537" data-execution_count="95">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>linear_layer <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> linear_layer(x)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>torch.Size([64, 200, 512])</code></pre>
</div>
</div>
<p>The linear layer maintains the dimension (512 in, 512 out).</p>
<p><code>x</code> then passes through a Dropout layer and a Layer Normalization layer. Note that <code>residual_x</code> is added to <code>x</code> before the sum is passed through the Layer Normalization.</p>
<p>I won’t walk through the details of Layer Normalization, but CodeEmporium provides the following code that I’ll highlight the following few lines from:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> x.mean(dim<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>], keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>var <span class="op">=</span> ((x <span class="op">-</span> mean) <span class="op">**</span> <span class="dv">2</span>).mean(dim<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>], keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> (var <span class="op">+</span> <span class="fl">1e-5</span>).sqrt()</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> gamma <span class="op">*</span> y <span class="op">+</span> beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Where <code>gamma</code> and <code>beta</code> are learnable <code>nn.Parameter</code> weights. Note that the values are normalized (resulting in <code>y</code>) and then normalization occurs across all samples (<code>gamma * y + beta</code>).</p>
<p><code>x</code> is stored as <code>residual_x</code> to add on later, and then <code>x</code> goes through a Feed Forward Network (a non-linearity, in this case a GELU, and a Dropout layer sandwiched between two linear layers), and then through another Dropout layer and Layer Normalization (where <code>residual_x</code> is added to <code>x</code>).</p>
</section>
</section>
<section id="code-overview-decoder" class="level2">
<h2 class="anchored" data-anchor-id="code-overview-decoder">Code Overview: Decoder</h2>
<p>There are some similarities and some differences between the Encoder and the Decoder. Note that in CodeEmporium’s implementation, the Decoder contains Self Attention and Encoder-Decoder Attention (also called <a href="https://benjaminwarner.dev/2023/07/01/attention-mechanism#cross-attention">Cross Attention in Benjamin’s post</a>).</p>
<p><img src="14.png" style="width:100%;"></p>
<p>The first main difference is that what goes into the Decoder are the outputs (the inputs shifted by one token).</p>
<p>In the Decoder, attention is masked. Only the current token and previous output tokens are “visible” to the model. Future tokens are masked. How does this masking take place? Here’s CodeEmporium’s code:</p>
<p>Start by creating a 200 x 200 tensor full of negative infinity (negative infinity is used so that when you take the softmax of it, it goes to 0)</p>
<div class="cell" data-outputid="696ce332-24a3-4906-8b07-f4638762ab64" data-execution_count="100">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.full([max_sequence_length, max_sequence_length], <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],
        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],
        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],
        ...,
        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],
        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],
        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])</code></pre>
</div>
</div>
<p>Keep the upper triangle as <code>-inf</code> and make everything else 0 with <code>torch.triu</code>:</p>
<div class="cell" data-outputid="65c248a7-9792-4a0c-a427-9234184d5f7c" data-execution_count="101">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.triu(mask, diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],
        [0., 0., -inf,  ..., -inf, -inf, -inf],
        [0., 0., 0.,  ..., -inf, -inf, -inf],
        ...,
        [0., 0., 0.,  ..., 0., -inf, -inf],
        [0., 0., 0.,  ..., 0., 0., -inf],
        [0., 0., 0.,  ..., 0., 0., 0.]])</code></pre>
</div>
</div>
<p>Now, when the mask is added to the scaled dot-product, the upper triangle will go to <code>-inf</code> (since anything plus <code>-inf</code> is <code>-inf</code>). Taking the softmax of that to get the attention matrix will result in a matrix with an upper triangle of zeros:</p>
<div class="cell" data-outputid="c05aa863-952e-4670-b570-ca43e3f27cf1" data-execution_count="122">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>scaled_dot_product <span class="op">=</span> scaled_dot_product <span class="op">+</span> mask</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>scaled_dot_product[<span class="dv">0</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>tensor([[-9.5668e-01,        -inf,        -inf,  ...,        -inf,
                -inf,        -inf],
        [-3.3439e-01, -1.0772e+00,        -inf,  ...,        -inf,
                -inf,        -inf],
        [-2.8391e-01,  2.7374e-02,  7.6844e-01,  ...,        -inf,
                -inf,        -inf],
        ...,
        [ 3.8771e-04, -2.7279e-01,  3.2622e-01,  ..., -1.5672e-01,
                -inf,        -inf],
        [ 9.1237e-01,  9.8978e-01,  8.4105e-02,  ...,  7.8569e-01,
         -1.8654e-03,        -inf],
        [-8.3207e-01, -1.6773e-01, -8.6295e-01,  ..., -3.1891e-01,
         -7.7460e-01, -8.4962e-01]], grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-outputid="c35ccf8d-b114-4270-e8b4-f3942beadfa6" data-execution_count="125">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>F.softmax(scaled_dot_product, dim<span class="op">=-</span><span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="125">
<pre><code>tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.6776, 0.3224, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1912, 0.2610, 0.5477,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0049, 0.0037, 0.0068,  ..., 0.0042, 0.0000, 0.0000],
        [0.0073, 0.0079, 0.0032,  ..., 0.0065, 0.0029, 0.0000],
        [0.0031, 0.0060, 0.0030,  ..., 0.0052, 0.0033, 0.0030]],
       grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
<p>Cross Attention works differently—the “Cross” in Cross Attention is talking about the relationship between the Encoder and Decoder. Specifically, the <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span> weights are applied to the Encoder outputs and the <span class="math inline">\(Q\)</span> weights are applied to the Decoder outputs. The rest of the process (scaled dot product, softmax, concatenation, linear layer) are the same as before (with the addition of adding the <code>mask</code> to the scaled dot product).</p>
<p>After passing the through Cross Attention, the outputs go through Dropout and Layer Normalization, then a Feed Forward Network, and then through another Dropout and Layer Normalization step. The inputs to the Layer Normalization call are the <code>residual_x</code> plus <code>x</code>, which is said to stabilize the training process.</p>
<p>Finally, the outputs go through a final linear layer which projects the outputs to the vocabulary size and then a final softmax call which converts those logits to probabilities per vocabulary token (in other words, answering the question: what are the probabilities that the next token will be each token in the vocabulary?)</p>
<p>There are a lot of details that I have left out of this post for brevity so to get the full Transformers code experience, see Benjamin’s <a href="https://github.com/warner-benjamin/commented-transformers">commented-transformers</a> repository, and CodeEmporium’s <a href="https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Transformer_Encoder_EXPLAINED!.ipynb">Encoder</a>/<a href="https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Transformer_Decoder_EXPLAINED!.ipynb">Decoder</a> notebooks.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>I was pleasantly surprised at how understandable the code is for the Transformer architecture. The paper does a great job of helping the reader visualizing the concepts in play, especially the process involved in calculating scaled dot-product attention across multiple heads. The number excellent resources available that I’ve referenced throughout this blog post are also essential to understanding the code and concepts involved.</p>
<p>On a personal note, I recall going to a presentation on this paper a few years ago and leaving feeling so incredibly lost, and that maybe I wouldn’t understand how this critical architecture actually works—like I had hit a wall of complexity that I wouldn’t be able to overcome. Reading this paper, understanding it, presenting on it and writing this blog post felt like redemption for me. I obviously couldn’t have done it without the excellent resources I’ve linked above.</p>
<p>As always, I hope you enjoyed this paper summary!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>