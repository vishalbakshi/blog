<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-04-26">
<meta name="description" content="A summary of research benchmarking reward models.">

<title>vishal bakshi - Paper Summary: RewardBench</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#main-takeaways" id="toc-main-takeaways" class="nav-link" data-scroll-target="#main-takeaways">Main Takeaways</a></li>
  <li><a href="#scoring-method" id="toc-scoring-method" class="nav-link" data-scroll-target="#scoring-method">Scoring Method</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a>
  <ul class="collapse">
  <li><a href="#chat" id="toc-chat" class="nav-link" data-scroll-target="#chat">Chat</a></li>
  <li><a href="#chat-hard" id="toc-chat-hard" class="nav-link" data-scroll-target="#chat-hard">Chat Hard</a></li>
  <li><a href="#safety" id="toc-safety" class="nav-link" data-scroll-target="#safety">Safety</a></li>
  <li><a href="#reasoning" id="toc-reasoning" class="nav-link" data-scroll-target="#reasoning">Reasoning</a></li>
  <li><a href="#prior-sets" id="toc-prior-sets" class="nav-link" data-scroll-target="#prior-sets">Prior Sets</a></li>
  <li><a href="#distribution-of-source-completions" id="toc-distribution-of-source-completions" class="nav-link" data-scroll-target="#distribution-of-source-completions">Distribution of Source Completions</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#leaderboard-from-the-paper" id="toc-leaderboard-from-the-paper" class="nav-link" data-scroll-target="#leaderboard-from-the-paper">Leaderboard (from the paper)</a></li>
  <li><a href="#leaderboard-as-of-4262024" id="toc-leaderboard-as-of-4262024" class="nav-link" data-scroll-target="#leaderboard-as-of-4262024">Leaderboard as of 4/26/2024</a></li>
  <li><a href="#scaling-trends" id="toc-scaling-trends" class="nav-link" data-scroll-target="#scaling-trends">Scaling Trends</a></li>
  <li><a href="#b-models" id="toc-b-models" class="nav-link" data-scroll-target="#b-models">7B Models</a></li>
  <li><a href="#chat-hard-1" id="toc-chat-hard-1" class="nav-link" data-scroll-target="#chat-hard-1">Chat Hard</a></li>
  <li><a href="#safety-1" id="toc-safety-1" class="nav-link" data-scroll-target="#safety-1">Safety</a></li>
  <li><a href="#distribution-of-model-accuracy-by-dataset" id="toc-distribution-of-model-accuracy-by-dataset" class="nav-link" data-scroll-target="#distribution-of-model-accuracy-by-dataset">Distribution of Model Accuracy by Dataset</a></li>
  <li><a href="#prompt-length-distribution-by-dataset" id="toc-prompt-length-distribution-by-dataset" class="nav-link" data-scroll-target="#prompt-length-distribution-by-dataset">Prompt Length Distribution by Dataset</a></li>
  <li><a href="#dpo-vs-classifier-rms" id="toc-dpo-vs-classifier-rms" class="nav-link" data-scroll-target="#dpo-vs-classifier-rms">DPO vs Classifier RMs</a></li>
  </ul></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work">Future Work</a></li>
  <li><a href="#rewardbench-result-analysis" id="toc-rewardbench-result-analysis" class="nav-link" data-scroll-target="#rewardbench-result-analysis">RewardBench Result Analysis</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Summary: RewardBench</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper summary</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    A summary of research benchmarking reward models.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 26, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post I’ll summarize the research paper <a href="https://arxiv.org/pdf/2403.13787">RewardBench: Evaluating Reward Models for Language Modeling</a>. Here’s the abstract:</p>
<blockquote class="blockquote">
<p>Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g.&nbsp;bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO), and on a spectrum of datasets. We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.</p>
</blockquote>
</section>
<section id="main-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="main-takeaways">Main Takeaways</h2>
<ul>
<li>A good reward function, and therefore a good reward model (RM) stably assigns credit to the classes of good or bad content.</li>
<li>Reward models potentially provide a glimpse into how human values map onto language models.</li>
<li>Reward model output distributions vary across models.</li>
<li>DPO policies (compared to classifier RMs) fail to generalize to popular preference data test sets and present a higher variance in performance.</li>
<li>Data subsets with low ceilings indicate opportunities to improve preference datasets and modeling methods while subsets with high variability indicate opportunities for improving best practices.</li>
<li>RewardBench if a framework to which we can add new models.</li>
</ul>
</section>
<section id="scoring-method" class="level2">
<h2 class="anchored" data-anchor-id="scoring-method">Scoring Method</h2>
<p><img src="1.png" style="width:80%;"></p>
<blockquote class="blockquote">
<p>Each data point consists of a prompt and two completions (chosen and rejected). For each prompt, the score of the reward model is computed. The prompt is then categorized as a win if the score of the prompt with the verified chosen completion is higher than that of the verified rejected completion.</p>
</blockquote>
</section>
<section id="datasets" class="level2">
<h2 class="anchored" data-anchor-id="datasets">Datasets</h2>
<p>The authors define the following subsets on which they evaluate reward model performance:</p>
<ul>
<li>Chat</li>
<li>Chat Hard</li>
<li>Safety</li>
<li>Reasoning</li>
<li>Prior Sets</li>
</ul>
<p>The first four subsets are curated (some are modified) from existing benchmark evaluation sets. The last subset (Prior Sets) consists of unmodified existing evaluation sets.</p>
<section id="chat" class="level3">
<h3 class="anchored" data-anchor-id="chat">Chat</h3>
<p>The Chat subset includes prompts curated from two benchmarks:</p>
<ul>
<li>AlpacaEval
<ul>
<li>Easy: 100 prompt-chosen-rejected trios.</li>
<li>Length: 95 prompt-chosen-rejected trios.</li>
<li>Hard: 95 manually verified prompt-chosen-rejected trios.</li>
</ul></li>
<li>MT Bench
<ul>
<li>Easy: 28 manually verified prompt-chosen-rejected trios.</li>
<li>Medium: 40 manually verified prompt-chosen-rejected trios.</li>
</ul></li>
</ul>
<p>The AlpacaEval Length subset is designed to differentiate between other Chat subsets by having notably different model capabilities with the same <strong>average</strong> length.</p>
<section id="alpacaeval" class="level4">
<h4 class="anchored" data-anchor-id="alpacaeval">AlpacaEval</h4>
<ul>
<li><a href="https://klu.ai/glossary/alpaca-eval">AlpacaEval</a> is an automated tool for evaluation instruction-folliwng language models against the AlpacaFarm dataset.</li>
<li><a href="https://github.com/tatsu-lab/alpaca_eval/tree/main">AlpacaEval 2.0</a> with length-controlled win-rates has a spearman correlation of 0.98 with ChatBot Arena.</li>
<li><a href="https://chat.lmsys.org/?leaderboard">ChatBot Arena</a> is a crowdsourced open platform for LLM evals with 700,000+ human pairwise comparisons to rank LLMs.</li>
</ul>
<p>“length-controlled” is a way to account for the bias that LLMs have towards longer responses (i.e.&nbsp;they prefer responses that are longer).</p>
<p>Here is an example prompt-chosen-rejected tri from the alpacaeval-easy dataset. The chosen-model is the model that generated the chosen response and rejected-model is the model that generated the rejected response.</p>
<p><img src="2.png" style="width:100%;"></p>
<p>Here a trio from the alpacaeval-length dataset, where the prompt lengths for chosen and rejected responses are similar.</p>
<p><img src="3.png" style="width:100%;"></p>
<p>Here’s a trio from alpacaeval-length with different prompt lengths for the chosen and rejected response showing that there is variation in prompt lengths even though the average length across the full dataset between chosen and rejected responses is similar.</p>
<p><img src="4.png" style="width:100%;"></p>
</section>
<section id="mt-bench" class="level4">
<h4 class="anchored" data-anchor-id="mt-bench">MT Bench</h4>
<ul>
<li><a href="https://arxiv.org/pdf/2306.05685">MT Bench</a> is designed to test multi-turn conversation (AlpacaEval was single-turn only) and instruction-following ability across 8 categories of user prompts: writing, roleplay, extraction, reasining, math, coding, knowledge I (STEM), and knowledge II (humanities/social science).</li>
</ul>
<p>Here’s a trio from the mt-bench-easy dataset where claude-v1 correctly counts the specific words while raven-14b deviates from the instruction.</p>
<p><img src="5.png" style="width:100%;"></p>
</section>
</section>
<section id="chat-hard" class="level3">
<h3 class="anchored" data-anchor-id="chat-hard">Chat Hard</h3>
<ul>
<li>MT Bench
<ul>
<li>Hard: 37 manually verified prompt-chosen-rejected trios.</li>
</ul></li>
<li>LLMBar
<ul>
<li>Natural: 100 manually verified prompt-chosen-rejected trios.</li>
<li>Adversarial
<ul>
<li>Neighbor: 134 trios.</li>
<li>GPT4Inst: 92 trios.</li>
<li>GPTOut: 47 trios.</li>
<li>Manual: 46 trios.</li>
</ul></li>
</ul></li>
</ul>
<section id="llmbar" class="level4">
<h4 class="anchored" data-anchor-id="llmbar">LLMBar</h4>
<p>The <a href="https://arxiv.org/pdf/2310.07641">LLMBar dataset</a> contains responses to prompts that are preferred and dispreferred.</p>
<ul>
<li>Natural
<ul>
<li>Instances from existing subject human-preference datasets, filtered/modified for objective preference.</li>
</ul></li>
<li>Adversarial
<ul>
<li>Dispreffered output deviates from the instruction but often has good superficial qualities and may thus distract the evaluator.</li>
</ul></li>
</ul>
<p>Here’s a trio where the rejected response deviates from the instruction but is coherent, uses good grammar, and has a conclusive response. When the model is listed as “unknown” it means that both a human and an AI model were involved in the response generation.</p>
<p><img src="6.png" style="width:100%;"></p>
</section>
</section>
<section id="safety" class="level3">
<h3 class="anchored" data-anchor-id="safety">Safety</h3>
<ul>
<li>Refusals
<ul>
<li>Dangerous: 100 trios.</li>
<li>Offensive: 100 trios.</li>
</ul></li>
<li>Do Not Answer
<ul>
<li>136 trios.</li>
</ul></li>
<li>XSTest
<ul>
<li>Should Respond
<ul>
<li>250 manually selected trios.</li>
</ul></li>
<li>Should Refuse
<ul>
<li>154 manually selected trios.</li>
</ul></li>
</ul></li>
</ul>
<section id="refusals" class="level4">
<h4 class="anchored" data-anchor-id="refusals">Refusals</h4>
<p>An in-development refusals dataset at AI2, where the chosen response is a refusal and the rejected is harmful text is of either dangerous or offensive nature.</p>
<p>The following refusals-dangerous trio involves a medical scenario where the model should refuse to respond.</p>
<p><img src="7.png" style="width:100%;"></p>
<p>In the following trio, the rejected model is tricked by the user prompt that the heist is for a novel and it provides a helpful but dangerous response.</p>
<p><img src="8.png" style="width:100%;"></p>
</section>
<section id="do-not-answer" class="level4">
<h4 class="anchored" data-anchor-id="do-not-answer">Do Not Answer</h4>
<ul>
<li><a href="https://arxiv.org/pdf/2308.13387">A dataset</a> curated and filtered to consist of only instructions responsible language models should now follow.</li>
</ul>
<p><img src="9.png" style="width:100%;"></p>
<p>The following trio from the donotanswer dataset shows how the model should refuse to respond to the prompt.</p>
<p><img src="10.png" style="width:100%;"></p>
</section>
<section id="xstest" class="level4">
<h4 class="anchored" data-anchor-id="xstest">XSTest</h4>
<p>The prompts from this XSTest subset were designed to test for exaggerated safety behavior by containing certain, seemingly unsafe, trigger words while actually being safe to comply to.</p>
<p>In the following trio from the xstest-should-respond dataset, the model tricks the rejected model into thinking the prompt is offensive by using the phrase “not be allowed to lead public companies”.</p>
<p><img src="11.png" style="width:100%;"></p>
</section>
</section>
<section id="reasoning" class="level3">
<h3 class="anchored" data-anchor-id="reasoning">Reasoning</h3>
<ul>
<li>HumanEvalPack (164 prompts each)
<ul>
<li>CPP</li>
<li>Go</li>
<li>JavaScript</li>
<li>Java</li>
<li>Rust</li>
<li>Python</li>
</ul></li>
<li>PRM (Process Reward Model) Math
<ul>
<li>Filtered/select answers from the PRM800k dataset.</li>
</ul></li>
</ul>
<section id="humanevalpack" class="level4">
<h4 class="anchored" data-anchor-id="humanevalpack">HumanEvalPack</h4>
<ul>
<li><a href="https://arxiv.org/pdf/2308.07124">HumanEvalPack</a> expands the HumanEval benchmark to 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust)</li>
<li>Handwritten <a href="https://arxiv.org/pdf/2107.03374">HumanEval</a> measures functional correctness for synthesizing python programs from docstrings.</li>
</ul>
<p>Here’s a trio from the hep-python dataset:</p>
<p><img src="12.png" style="width:100%;"></p>
</section>
<section id="prm-math" class="level4">
<h4 class="anchored" data-anchor-id="prm-math">PRM Math</h4>
<ul>
<li><a href="https://arxiv.org/pdf/2305.20050">PRM800k</a> is an 800k step-level labels over 75000 solutions.</li>
</ul>
<p><img src="13.png" style="width:100%;"></p>
<p>A trio from the math-prm dataset where the chosen response is human-generated.</p>
<p><img src="14.png" style="width:100%;"></p>
</section>
</section>
<section id="prior-sets" class="level3">
<h3 class="anchored" data-anchor-id="prior-sets">Prior Sets</h3>
<ul>
<li>Anthropic
<ul>
<li>Helpful</li>
<li>Harmless*</li>
<li>HHH</li>
</ul></li>
<li>MT Bench
<ul>
<li>GPT-4*</li>
<li>Human*</li>
</ul></li>
<li>Stanford Human Preferences (SHP)</li>
<li>OpenAI’s Learning to Summarize</li>
</ul>
<p>* Not used in the RewardBench leaderboard.</p>
<p>Here’s a trio from the Anthropic Helpful dataset where the chosen model provides a helpful response to the prompt and rejecte response is not helpful.</p>
<p><img src="15.png" style="width:100%;"></p>
<p>A trio from Anthropic HHH where the chosen model provides an honest response “I’m not sure…if I had to guess…” while the rejected model provides a confident incorrect response.</p>
<p><img src="16.png" style="width:100%;"></p>
<p>A trio from the Helpful subset of Anthropic HHH where the chosen model gives pros for each vacation destination whereas the rejected model gives a non-committal response.</p>
<p><img src="17.png" style="width:100%;"></p>
<p>A trio from SHP where the chosen response is more detailed and thorough than the reject response (in my opinion). My personal opinion is that neither explain the concept at a five year old level.</p>
<p><img src="18.png" style="width:100%;"></p>
<p>A trio from Learning to Summarize. In my opinion, both responses are similar.</p>
<p><img src="19.png" style="width:100%;"></p>
<hr>
<p>Here is a summary of the datasets used for the RewardBench leaderboard, including a description of how chosen-rejected responses are generated.</p>
<p><img src="20.png" style="width:100%;"></p>
</section>
<section id="distribution-of-source-completions" class="level3">
<h3 class="anchored" data-anchor-id="distribution-of-source-completions">Distribution of Source Completions</h3>
<p>The chosen and rejected responses are generated by a variety of models (including humans and both humans and models—“unknown”). Humans are the most common source of the response, with “unknown”, GPT-3.5-turbo, GPT-4 and Llama-2-70b-chat rounding out the top five overall and for chosen responses. Two Mistral-7B variants are 4th and 5th most used for rejected responses.</p>
<p><img src="21.png" style="width:100%;"></p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="leaderboard-from-the-paper" class="level3">
<h3 class="anchored" data-anchor-id="leaderboard-from-the-paper">Leaderboard (from the paper)</h3>
<p>The following table shows the top-20 models in terms of average performance (accuracy) on the five subsets of Chat, Chat Hard, Safety, Reasoning and Prior Sets. Note that 15 of the top 20 are DPO models, with 5 Sequence Classifiers. A random model would have an accuracty of 50% when chosing the preferred response. The highlighted accuracies are the highest in each column.</p>
<p><img src="22.png" style="width:100%;"></p>
</section>
<section id="leaderboard-as-of-4262024" class="level3">
<h3 class="anchored" data-anchor-id="leaderboard-as-of-4262024">Leaderboard as of 4/26/2024</h3>
<p>The current leaderboard (at the time I made this presentation) had many new models involved. A Starling variant was still in the top 5, while Allen AI’s tulu-2-dpo-70B had dropped to 11th place.</p>
<p><img src="23.png" style="width:100%;"></p>
<p>When sorting by Prior Sets (descending) you can see the paper’s findings in action—none of the top 20 models were trained by DPO.</p>
<p><img src="24.png" style="width:100%;"></p>
</section>
<section id="scaling-trends" class="level3">
<h3 class="anchored" data-anchor-id="scaling-trends">Scaling Trends</h3>
<p>The authors found that DPO trained models followed scaling laws (accuracy generally increased with model size). Whereas in the Qwen1.5 family (not DPO trained) for example, the accuracy actually regresses on Prior Sets as model size increases.</p>
<p><img src="25.png" style="width:100%;"></p>
</section>
<section id="b-models" class="level3">
<h3 class="anchored" data-anchor-id="b-models">7B Models</h3>
<p>The Zephyr-7B variants performed well in Chat Hard and Reasoning.</p>
<p><img src="26.png" style="width:100%;"></p>
</section>
<section id="chat-hard-1" class="level3">
<h3 class="anchored" data-anchor-id="chat-hard-1">Chat Hard</h3>
<p>On Chat Hard some models performed worse than random.</p>
<p><img src="27.png" style="width:100%;"></p>
</section>
<section id="safety-1" class="level3">
<h3 class="anchored" data-anchor-id="safety-1">Safety</h3>
<p>The authors highlighted three model behaviors for safety:</p>
<ul>
<li>Top section: models refuse or respond when they should.</li>
<li>Middle: Models always refuse (low accuracy on Should Respond).</li>
<li>Bottom: Models always respond (low accuracy on Should Refuse).</li>
</ul>
<p><img src="28.png" style="width:100%;"></p>
</section>
<section id="distribution-of-model-accuracy-by-dataset" class="level3">
<h3 class="anchored" data-anchor-id="distribution-of-model-accuracy-by-dataset">Distribution of Model Accuracy by Dataset</h3>
<p>The highlighted distributions show how for some datasets, none of the models had an accuracy anywhere close to 100%, showing an opportunity to improve these datasets and modeling methods.</p>
<p><img src="29.png" style="width:100%;"></p>
</section>
<section id="prompt-length-distribution-by-dataset" class="level3">
<h3 class="anchored" data-anchor-id="prompt-length-distribution-by-dataset">Prompt Length Distribution by Dataset</h3>
<p>The authors showed the variation in (and average of) prompt lengths across the various datasets. Note that the AlpacaEval Length prompt lengths vary a lot although the average prompt length of chosen and rejected responses are close. For most of the other datasets the averages are either close (HumanEvalPack) or the chosen responses had a shorter prompt (LLMBar).</p>
<p><img src="30.png" style="width:100%;"></p>
</section>
<section id="dpo-vs-classifier-rms" class="level3">
<h3 class="anchored" data-anchor-id="dpo-vs-classifier-rms">DPO vs Classifier RMs</h3>
<ul>
<li>This is an understudied field.</li>
<li>DPO model availability due to low compute requirements.</li>
<li>DPOs perform well on all subsets except Prior Sets.</li>
<li>Lack of documentation on reference models restricts DPO evaluation because using the “wrong” reference model leads to lower DPO performance.</li>
<li>DPOs regularize with KL.</li>
<li>DPOs are trained for multiple epochs (Sequence Classifiers usually trained for 1 epoch).</li>
</ul>
</section>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future Work</h2>
<ul>
<li>Explore reference free DPO model impacts on inference.</li>
</ul>
<p>The image below shows going from log probability ratio to probability ratio when the reference model is removed.</p>
<p><img src="31.png" style="width:100%;"></p>
<ul>
<li>Analyze hyperparamters’ role in DPO and RM classifier performance.</li>
<li>Incorporate generative reward modeling scores into leaderboard (already done in the current leaderboard).</li>
<li>Improve datasets with model accuracy ceilings under 100%.</li>
<li>Improve RMs to reduce variance (especially for challenging tasks).</li>
<li>Ablate base models and fine-tuning recipes to find the best RMs.</li>
<li>Identify a practical RM output distribution for downstream RL training.</li>
</ul>
</section>
<section id="rewardbench-result-analysis" class="level2">
<h2 class="anchored" data-anchor-id="rewardbench-result-analysis">RewardBench Result Analysis</h2>
<p>I couldn’t get the <a href="https://github.com/allenai/reward-bench/tree/main">rewardbench repo</a> to install locally in time for the presentation so I’ve copy/pasted the critical functions to get the datasets in <a href="https://colab.research.google.com/drive/1HENcTJbbBS4GxLIJI0L80cxdiOTnkdmw?usp=sharing">this notebook</a> in order to create custom visualizations of the model results.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>I found this to be another inspiring paper, especially with the rich content in the Future Work section on how much this field needs to be studied. Getting a glimpse into how human values map onto language models is a fascinating frontier to explore.</p>
<p>I hope you enjoyed this paper summary!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>