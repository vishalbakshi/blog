<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-04-05">
<meta name="description" content="Exploring the math from the Direct Preference Optimization paper to better understand it.">

<title>vishal bakshi - Paper Math: DPO (Direct Preference Optimization)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vishal bakshi</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#section-3-preliminaries" id="toc-section-3-preliminaries" class="nav-link" data-scroll-target="#section-3-preliminaries">Section 3: Preliminaries</a>
  <ul class="collapse">
  <li><a href="#reward-modelling-phase" id="toc-reward-modelling-phase" class="nav-link" data-scroll-target="#reward-modelling-phase">Reward Modelling Phase</a></li>
  <li><a href="#rl-fine-tuning-phase" id="toc-rl-fine-tuning-phase" class="nav-link" data-scroll-target="#rl-fine-tuning-phase">RL Fine-Tuning Phase</a></li>
  </ul></li>
  <li><a href="#appendix-a.1-deriving-the-optimum-of-the-kl-constrained-reward-maximization-objective" id="toc-appendix-a.1-deriving-the-optimum-of-the-kl-constrained-reward-maximization-objective" class="nav-link" data-scroll-target="#appendix-a.1-deriving-the-optimum-of-the-kl-constrained-reward-maximization-objective">Appendix A.1: Deriving the Optimum of the KL-Constrained Reward Maximization Objective</a></li>
  <li><a href="#appendix-a.2-deriving-the-dpo-objective-under-the-bradley-terry-model" id="toc-appendix-a.2-deriving-the-dpo-objective-under-the-bradley-terry-model" class="nav-link" data-scroll-target="#appendix-a.2-deriving-the-dpo-objective-under-the-bradley-terry-model">Appendix A.2 Deriving the DPO Objective Under the Bradley-Terry Model</a></li>
  <li><a href="#appendix-a.4-deriving-the-gradient-of-the-dpo-objective" id="toc-appendix-a.4-deriving-the-gradient-of-the-dpo-objective" class="nav-link" data-scroll-target="#appendix-a.4-deriving-the-gradient-of-the-dpo-objective">Appendix A.4 Deriving the Gradient of the DPO Objective</a></li>
  <li><a href="#appendix-b-dpo-implementation-details-and-hyperparameters" id="toc-appendix-b-dpo-implementation-details-and-hyperparameters" class="nav-link" data-scroll-target="#appendix-b-dpo-implementation-details-and-hyperparameters">Appendix B: DPO Implementation Details and Hyperparameters</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Math: DPO (Direct Preference Optimization)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper math</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    Exploring the math from the Direct Preference Optimization paper to better understand it.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post I’ll walk through some of the math involved in the research paper <a href="https://arxiv.org/pdf/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>.</p>
<p>The abstract:</p>
<blockquote class="blockquote">
<p>While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.</p>
</blockquote>
<p>DPO involves preference data, a reference model and the parameterized model (i.e.&nbsp;the model being trained).</p>
<p>I found this section of the paper provide the most intuition about why DPO works:</p>
<p><img src="1.png" style="width:100%;"></p>
<p>As the model gets more likely to pick the preferred response, the gradient increases. Conversely, as the implicit reward (log probability ratio of parameterized model and reference model) for rejected responses increases, the gradient increases. I think of these two terms contrasting each other, keeping the gradient from vanishing or exploding.</p>
<p>In the next few sections of this blog post, I’ll walk through parts of the paper where I found that math either challenging (and used Claude or ChatGPT to help me figure it out) and/or particulary helpful for my understanding of the concepts. In the final section, I do my best to connect the code implementation of DPO loss with the math formulas provided in the paper. Overall I found this process very rewarding and built my confidence to take on understanding the mathy parts of the paper moving forward.</p>
</section>
<section id="section-3-preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="section-3-preliminaries">Section 3: Preliminaries</h2>
<section id="reward-modelling-phase" class="level3">
<h3 class="anchored" data-anchor-id="reward-modelling-phase">Reward Modelling Phase</h3>
<p>Bradley-Terry model stipulates that the human preference distribution <span class="math inline">\(p^*\)</span> can be written as:</p>
<p><strong>Equation (1):</strong> <span class="math display">\[ p^*(y_1 \succ y_2|x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))}\]</span></p>
<p>Where <span class="math inline">\(r^*(y, x)\)</span> is a latent reward model we don’t have access to.</p>
<p>(I don’t know if it’s mathematically correct to say this but <span class="math inline">\(p^*\)</span> function looks like softmax).</p>
<p><span class="math inline">\(\succ\)</span> = “succeeds” symbol. A couple of definitions I found online, seems pretty straightforward:</p>
<blockquote class="blockquote">
<p>The term successor and predecessor in Math means that they come directly after or before the particular number respectively. Both successor and predecessor basically applied only to the whole numbers. The successor is termed as after numbers whereas the predecessor is termed as before numbers.</p>
<p>Successor in Math refers to the after the term of a particular term while the predecessor in Math refers to them before the term of a particular term. We can find the successor of a whole number by adding one to the particularly given number whereas to find the predecessor of a whole number we will subtract one from the particularly given number. In this article, we will study what are successor and predecessor, successor meaning, predecessor meaning, how to find the successor of a given number, how to find the predecessor of a given number, etc. <a href="https://www.vedantu.com/maths/what-are-the-successor-and-predecessor">Source</a>.</p>
</blockquote>
<hr>
<blockquote class="blockquote">
<p>The succeeds operator (≻) is a generalization of idea behind some numbers being bigger than others.</p>
<p>The succeeds operator (≻) is like 10&gt;4 , except that it applies to objects other than numbers. (<a href="https://math.stackexchange.com/questions/644731/whats-the-meaning-of-succ-operator#:~:text=The%20succeeds%20operator%20(%E2%89%BB)%20is,a%20fruit%20than%20f2.">Source</a>)</p>
</blockquote>
<hr>
<blockquote class="blockquote">
<p>A preference relation expresses the consumer’s feelings between pairs of objects in X . We denote the preference relation by <span class="math inline">\(\succeq\)</span> and imagine that for every pair x and y from X , the consumer is willing to say that either x <span class="math inline">\(\succeq\)</span> y , meaning x is at least as good as y, or not. <a href="https://assets.press.princeton.edu/chapters/s9890.pdf">Source</a></p>
</blockquote>
<p>Static dataset of comparisons sampled from <span class="math inline">\(p^*\)</span>:</p>
<p><span class="math display">\[\mathcal{D}\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}^N\]</span></p>
<p>Assuming we have access to <span class="math inline">\(\mathcal{D}\)</span> we can parameterize a reward model <span class="math inline">\(r_{\phi}(x,y)\)</span> and estimate parameters via maximum likelihood. Framed as a binary classification problem we have the negative log-likelihood loss:</p>
<p><strong>Equation (2)</strong>:</p>
<p><br></p>
<p><span class="math display">\[\mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\log\sigma(r_{\phi}(x, y_w) -  r_{\phi}(x, y_l)\big]\]</span></p>
<p><br></p>
<p>In the loss function above, as <span class="math inline">\(r_{\phi}(x, y_w) - r_{\phi}(x, y_l)\)</span> increases (the model assigns higher reward to the preferred response), sigmoid (<span class="math inline">\(\sigma\)</span>, blue function below) goes to 1 and <span class="math inline">\(-\log\sigma\)</span> (red function below) goes to 0.</p>
<p><img src="2.png" style="width:100%;"></p>
<p>The network <span class="math inline">\(r_{\phi}\)</span> is often initialized from the SFT model <span class="math inline">\(\pi^{SFT}(y|x)\)</span> with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value.</p>
</section>
<section id="rl-fine-tuning-phase" class="level3">
<h3 class="anchored" data-anchor-id="rl-fine-tuning-phase">RL Fine-Tuning Phase</h3>
<p>Optimization problem formulated as:</p>
<p><strong>Equation (3)</strong>:</p>
<p><span class="math display">\[\max_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}[r_\phi(x,y)]-\beta\mathbb{D}_{KL}[\pi_\theta(y|x) \;||\; \pi_{ref}(y|x)]\]</span></p>
<p><span class="math inline">\(\beta\)</span> is a parameter controlling the deviation from the base reference policy <span class="math inline">\(\pi_{ref}\)</span> (the initial SFT model <span class="math inline">\(\pi^{SFT}\)</span>).</p>
<p>The language model policy <span class="math inline">\(\pi_\theta\)</span> is also initialized to <span class="math inline">\(\pi^{SFT}\)</span>.</p>
<p>This objective is not differentiable because language generation is discrete, so the standard approach is to construct the reward function as:</p>
<p><span class="math display">\[ r(x,y) = r_\phi(x,y)-\beta(\log\pi_\theta(y|x) - \log\pi_{ref}(y|x)\]</span></p>
<p>And maximize it using PPO.</p>
</section>
</section>
<section id="appendix-a.1-deriving-the-optimum-of-the-kl-constrained-reward-maximization-objective" class="level2">
<h2 class="anchored" data-anchor-id="appendix-a.1-deriving-the-optimum-of-the-kl-constrained-reward-maximization-objective">Appendix A.1: Deriving the Optimum of the KL-Constrained Reward Maximization Objective</h2>
<p>Start with a modified version of Equation (3), where now we have any reward function <span class="math inline">\(r(x,y)\)</span> and a general non-parametric policy class <span class="math inline">\(\pi\)</span>:</p>
<p>Step 1: <span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi}[r(x,y)]-\beta\mathbb{D}_{KL}[\pi(y|x) \;||\; \pi_{ref}(y|x)]\]</span></p>
<p>Step 2:</p>
<p><span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[r(x,y)]-\beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\big]\]</span></p>
<hr>
<p>ChatGPT explanation on how to go from Step 1 to Step 2:</p>
<p><strong>Prompt:</strong></p>
<p>Explain in detail how you go from:</p>
<p>Step 1: <span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi}[r(x,y)]-\beta\mathbb{D}_{KL}[\pi(y|x) \;||\; \pi_{ref}(y|x)]\]</span></p>
<p>to Step 2: <span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[r(x,y)]-\beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\big]\]</span></p>
<p><strong>Response:</strong></p>
<p>To go from Step 1 to Step 2, we will first break down the components of the expectation terms and then manipulate them accordingly.</p>
<p>In step 1:</p>
<p><span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi}[r(x,y)]-\beta\mathbb{D}_{KL}[\pi(y|x) \;||\; \pi_{ref}(y|x)]\]</span></p>
<p>The first term represents the expected reward under the joint distribution of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> drawn from the distribution <span class="math inline">\(\mathcal{D}\)</span> and <span class="math inline">\(\pi\)</span> respectively. The second term is the Kullback-Leibler (KL) divergence between the conditional distribution <span class="math inline">\(\pi(y|x)\)</span> and a reference conditional distribution <span class="math inline">\(\pi_{ref}(y|x)\)</span>.</p>
<p>Now, to transform it into Step 2:</p>
<p><span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[r(x,y)]-\beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\big]\]</span></p>
<p>We will separate the expectation over <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and then express the KL divergence term differently.</p>
<ol type="1">
<li><strong>Separation of expectations</strong>: We start by separating the joint expectation into two nested expectations:</li>
</ol>
<p><span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[ \mathbb{E}_{y\sim\pi(y|x)}\left[r(x,y)\right]\right] - \beta\mathbb{D}_{KL}[\pi(y|x) \;||\; \pi_{ref}(y|x)]\]</span></p>
<ol start="2" type="1">
<li><strong>Transforming the KL divergence term</strong>:</li>
</ol>
<p>Next, we express the KL divergence term differently using its definition:</p>
<p><span class="math display">\[\mathbb{D}_{KL}[\pi(y|x) \;||\; \pi_{ref}(y|x)] = \mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\right]\]</span></p>
<p>Here, we are effectively taking the expectation over <span class="math inline">\(y\)</span> for the logarithm of the ratio of the two distributions.</p>
<p>After these transformations, we have successfully rewritten Step 1 into the desired form of Step 2. The key steps involve separating the expectations over <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and expressing the KL divergence term using its definition.</p>
<hr>
<p>Based on what ChatGPT provided, I’m adding one more detail that it didn’t write out, which is plugging in the KL divergence term using its definition into the main equation before simplifying it:</p>
<p><span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[ \mathbb{E}_{y\sim\pi(y|x)}\left[r(x,y)\right]\right] - \beta\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\right]\]</span></p>
<p>I think is then simplified to:</p>
<p><span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[r(x,y) - \beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\big]\]</span></p>
<hr>
<p>Next in the paper, is Step 3:</p>
<p><span class="math display">\[\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)} - \frac{1}{\beta}r(x,y)\big]\]</span></p>
<p>(Summarized) ChatGPT explanation on how to go from Step 2 to Step 3:</p>
<p><strong>Prompt:</strong></p>
<p>Explain in detail how you go from:</p>
<p>Step 2: <span class="math display">\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\big]\]</span></p>
<p>to Step 3:</p>
<p><span class="math display">\[\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)} - \frac{1}{\beta}r(x,y)\big]\]</span></p>
<p><strong>Response:</strong></p>
<p>To go from Step 2 to Step 3, we need to transform the maximization problem into a minimization problem, and rearrange the terms accordingly. Let’s break down the steps:</p>
<ol type="1">
<li><strong>Negate the Objective Function:</strong></li>
</ol>
<p>To transform a maximization problem into a minimization problem, we can simply negate the objective function. Since maximizing <span class="math inline">\(\mathbb{E}[f(x)]\)</span> is equivalent to minimizing <span class="math inline">\(−\mathbb{E}[f(x)]\)</span>, we negate the entire objective function:</p>
<p><span class="math display">\[-\big(r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}\big) = \beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)} - r(x,y) \]</span></p>
<ol start="2" type="1">
<li><strong>Reorder the terms</strong></li>
</ol>
<p>Next, we rearrange the terms in the objective function to match the desired form.</p>
<p><span class="math display">\[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)} - \frac{1}{\beta}r(x,y)\]</span></p>
<p>By dividing the second term by <span class="math inline">\(\beta\)</span>, we transform the maximization problem into a minimization problem with a positive weight on the reward term.</p>
<hr>
<p>I think what is implied in the above explanation is that you are allowed to divide the optimization objective by a constant (in this case, <span class="math inline">\(\beta\)</span>).</p>
<hr>
<p>Next in the paper is Step 4:</p>
<p><span class="math display">\[\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\big[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))} - \log Z(x)\big]\]</span></p>
<p>Where:</p>
<p><span class="math display">\[Z(x) = \sum_{y}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))\]</span></p>
<p>is the “partition function”.</p>
<p>ChatGPT didn’t have a very helpful explanation (even after multiple prompts) except for the following explanation of why <span class="math inline">\(Z(x)\)</span> is needed:</p>
<blockquote class="blockquote">
<p>We introduce a normalization constant <span class="math inline">\(Z(x)\)</span> inside the logarithm. This constant ensures that the expression inside the logarithm integrates to 1, effectively making it a probability distribution.</p>
</blockquote>
<p>I’ll try to connect the dots myself.</p>
<p>This:</p>
<p><span class="math display">\[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)} - \frac{1}{\beta}r(x,y)\]</span></p>
<p><br></p>
<p>Should be shown to be equal to this:</p>
<p><span class="math display">\[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))} - \log Z(x)\]</span></p>
<p>I’ll start with the “normalized” version:</p>
<p><span class="math display">\[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))} - \log Z(x)\]</span></p>
<p><br></p>
<p>rewrite the denominator:</p>
<p><span class="math display">\[\log\frac{\pi(y|x)}{\frac{\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))}{Z(x)}} - \log Z(x)\]</span></p>
<p><br></p>
<p>Apply the property of logarithm <span class="math inline">\(\log(\frac{a}{b}) = \log(a) - \log(b)\)</span> to the first term:</p>
<p><br></p>
<p><span class="math display">\[\log\pi(y|x) - \log\frac{\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))}{Z(x)} - \log Z(x)\]</span></p>
<p><br></p>
<p>Apply that property to the second term (putting it inside brackets to maintain the minus sign before it):</p>
<p><span class="math display">\[\log\pi(y|x) - \big[\log\big( \pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))\big) - \log Z(x) \big] - \log Z(x)\]</span></p>
<p><br></p>
<p>Minus a negative becomes a plus:</p>
<p><span class="math display">\[\log\pi(y|x) - \log \big(\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))\big) + \log Z(x) - \log Z(x)\]</span></p>
<p><br></p>
<p>The <span class="math inline">\(\log Z(x)\)</span>’s cancel out:</p>
<p><span class="math display">\[\log\pi(y|x) - \log \big(\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))\big)\]</span></p>
<p><br></p>
<p>Applying the property of logarithms <span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span>:</p>
<p><span class="math display">\[\log\pi(y|x) - \big[\log \pi_{ref}(y|x) + \log\exp(\frac{1}{\beta}r(x,y))\big]\]</span></p>
<p><br></p>
<p>Minus a positive stays a minus:</p>
<p><span class="math display">\[\log\pi(y|x) - \log \pi_{ref}(y|x) - \log\exp(\frac{1}{\beta}r(x,y))\]</span></p>
<p><br></p>
<p>Given that <span class="math inline">\(\log(\exp(x)) = x\)</span>:</p>
<p><span class="math display">\[\log\pi(y|x) - \log \pi_{ref}(y|x) - \frac{1}{\beta}r(x,y)\]</span></p>
<p><br></p>
<p>Applying the property that <span class="math inline">\(\log(a) - \log(b) = \log(\frac{a}{b})\)</span>:</p>
<p><span class="math display">\[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)} - \frac{1}{\beta}r(x,y) \]</span></p>
<p>Which is the original expression.</p>
<p>Next in the paper they define a “valid probability distribution” as:</p>
<p><span class="math display">\[\pi^*(y|x) = \frac{1}{Z(x)}\pi_{ref}(y|x)\exp\big(\frac{1}{\beta}r(x,y)\big)\]</span></p>
<p><br></p>
<p>They say it’s valid because <span class="math inline">\(\pi^*(y|x) \ge 0\)</span> for all <span class="math inline">\(y\)</span> and <span class="math inline">\(\sum_{y}\pi^*(y|x) = 1\)</span>. They don’t provide much detail. I’ll rewrite that expression with the full form of <span class="math inline">\(Z(x)\)</span>:</p>
<p><span class="math display">\[\pi^*(y|x) = \frac{\pi_{ref}(y|x)\exp\big(\frac{1}{\beta}r(x,y)\big)}{Z(x)}\]</span></p>
<p><br></p>
<p><span class="math display">\[\pi^*(y|x) = \frac{\pi_{ref}(y|x)\exp\big(\frac{1}{\beta}r(x,y)\big)}{\sum_{y}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))}\]</span></p>
<p><br></p>
<p>That looks like the softmax function, which is a valid probability distribution so I’m going with that understanding for now.</p>
<p>Next in the paper they rewrite the optimization objective with this <span class="math inline">\(\pi^*\)</span> expression:</p>
<p><span class="math display">\[\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\big[\mathbb{E}_{y\sim\pi(y|x)} \big[\log\frac{\pi(y|x)}{\pi^*(y|x)}\big] -\log Z(x) \big]\]</span></p>
<p><br></p>
<p>the first term inside the first set of brackets is KL-divergence:</p>
<p><span class="math display">\[\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\big[\mathbb{D}_{KL} \big(\pi(y|x) \; || \; \pi^*(y|x)\big) -\log Z(x) \big]\]</span></p>
<p><br></p>
<p><span class="math inline">\(Z(x)\)</span> does not depend on <span class="math inline">\(y\)</span> so minimizing the objective is dependent only on minimizing the KL-divergence term, which is minimized to 0 when the two probability distributions are identical:</p>
<p><span class="math display">\[\pi(y|x) = \pi^*(y|x) = \frac{1}{Z(x)}\pi_{ref}(y|x)\exp\big(\frac{1}{\beta}r(x,y)\big)\]</span></p>
<p><br></p>
<p>I’ll rewrite that with the full form of <span class="math inline">\(Z(x)\)</span>:</p>
<p><span class="math display">\[\pi(y|x) = \pi^*(y|x) = \frac{\pi_{ref}(y|x)\exp\big(\frac{1}{\beta}r(x,y)\big)}{\sum_{y}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))}\]</span></p>
</section>
<section id="appendix-a.2-deriving-the-dpo-objective-under-the-bradley-terry-model" class="level2">
<h2 class="anchored" data-anchor-id="appendix-a.2-deriving-the-dpo-objective-under-the-bradley-terry-model">Appendix A.2 Deriving the DPO Objective Under the Bradley-Terry Model</h2>
<p>Bradley-Terry preference model:</p>
<p><span class="math display">\[p^*(y_1 \gt y_2) = \frac{\exp(r^*(x,y_1))}{\exp(r^*(x,y_1)) + \exp(r^*(x,y_2))}\]</span></p>
<p><br></p>
<p>The unavailable ground-truth reward expressed through its optimal policy:</p>
<p><span class="math display">\[r^*(x,y)=\beta\log\frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta\log Z(x)\]</span></p>
<p><br></p>
<p>In section 4 they say that they derive this expression of reward from the following:</p>
<p><span class="math display">\[\pi_r(y|x) = \frac{1}{Z(x)}\pi_{ref}(y|x)\exp\big(\frac{1}{\beta}r(x,y)\big)\]</span></p>
<p><br></p>
<p>By taking the logarithm of each side:</p>
<p><span class="math display">\[\log\pi_r(y|x) = \log\big(\frac{1}{Z(x)}\pi_{ref}(y|x)\exp\big(\frac{1}{\beta}r(x,y)\big)\big)\]</span></p>
<p><br></p>
<p>Applying the property <span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span> to the right hand side:</p>
<p><span class="math display">\[\log\pi_r(y|x) = \log\big(\frac{1}{Z(x)}\pi_{ref}(y|x)\big) + \log\big(\exp\big(\frac{1}{\beta}r(x,y)\big)\big)\]</span></p>
<p><br></p>
<p>Applying the property <span class="math inline">\(\log(\exp(x)) = x\)</span> to the second term on the right hand side:</p>
<p><span class="math display">\[\log\pi_r(y|x) = \log\big(\frac{1}{Z(x)}\pi_{ref}(y|x)\big) + \frac{1}{\beta}r(x,y)\]</span></p>
<p><br></p>
<p>Applying the property <span class="math inline">\(\log(\frac{a}{b}) = \log(a) - \log(b)\)</span> to the first term on the right hand side:</p>
<p><span class="math display">\[\log\pi_r(y|x) = \log\pi_{ref}(y|x) - \log Z(x) + \frac{1}{\beta}r(x,y)\]</span></p>
<p><br></p>
<p>Isolating <span class="math inline">\(r(x,y)\)</span>:</p>
<p><span class="math display">\[\log\pi_r(y|x) - \log\pi_{ref}(y|x) + \log Z(x) = \frac{1}{\beta}r(x,y)\]</span></p>
<p><br></p>
<p>Rewriting the left hand side using logarithm property:</p>
<p><span class="math display">\[\log\frac{\pi_r(y|x)}{\pi_{ref}(y|x)} + \log Z(x) = \frac{1}{\beta}r(x,y)\]</span></p>
<p><br></p>
<p>Multiplying both sides by <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\beta\log\frac{\pi_r(y|x)}{\pi_{ref}(y|x)} + \beta\log Z(x) = r(x,y)\]</span></p>
<p>Which is the final result.</p>
<hr>
<p>In the paper they then substitute that for <span class="math inline">\(r^*(x,y)\)</span> in the Bradley-Terry preference model expression:</p>
<p><span class="math display">\[p^*(y_1 \gt y_2|x) = \frac{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)} + \beta\log Z(x)\big)}{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)} + \beta\log Z(x)\big) + \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)} + \beta\log Z(x)\big)}\]</span></p>
<p>They arrive at the following form without intermediate steps:</p>
<p><span class="math display">\[\frac{1}{1 + \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)} - \beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)}\]</span></p>
<p>Using ChatGPT (its response was not fully clear so I’m writing it out here with my two cents):</p>
<ol type="1">
<li><strong>Distribute the exponentials in the numerator and denominator using the property <span class="math inline">\(\exp(a + b) = \exp(a)\exp(b)\)</span></strong></li>
</ol>
<p><span class="math display">\[\frac{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)} \big)\exp \big(\beta\log Z(x)\big)}{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)\exp\big(\beta\log Z(x)\big) + \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)}\big)\big(\beta\log Z(x)\big)}\]</span></p>
<p><br></p>
<ol start="2" type="1">
<li><strong>Cancel out the common term <span class="math inline">\(\exp(\beta\log Z(x))\)</span></strong></li>
</ol>
<p><span class="math display">\[\frac{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)} \big)}{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big) + \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)}\big)}\]</span></p>
<p>This resembles the softmax function.</p>
<ol start="3" type="1">
<li><strong>Divide numerator and denominator by <span class="math inline">\(\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)\)</span></strong></li>
</ol>
<p><span class="math display">\[\frac{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)} \big) \div \exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)}{\exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big) \div \exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big) + \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)}\big) \div \exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)}\]</span></p>
<p><br></p>
<ol start="4" type="1">
<li><strong>Numerator and first term in denominator cancel out to equal 1</strong></li>
</ol>
<p><span class="math display">\[\frac{1}{1+ \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)}\big) \div \exp\big(\beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)}\]</span></p>
<p><br></p>
<ol start="5" type="1">
<li><strong>Use the property <span class="math inline">\(\exp(a) \div \exp(b) = \exp(a - b)\)</span></strong></li>
</ol>
<p><span class="math display">\[\frac{1}{1+ \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)} - \beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)}\]</span></p>
<p><br></p>
<p>This is in the form of sigmoid: <span class="math inline">\(\sigma(a) = \frac{1}{1 + e^{-a}}\)</span></p>
<p>Where <span class="math inline">\(a\)</span> in this case is the full term inside <span class="math inline">\(\exp()\)</span></p>
<p><br></p>
<ol start="6" type="1">
<li><strong>Rewrite it as sigmoid function</strong></li>
</ol>
<p><span class="math display">\[\frac{1}{1+ \exp\big(\beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)} - \beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)}=\sigma \big( \beta\log\frac{\pi^*(y_2|x)}{\pi_{ref}(y_2|x)} - \beta\log\frac{\pi^*(y_1|x)}{\pi_{ref}(y_1|x)}\big)\]</span></p>
<p><br></p>
<p>This is the per-instance loss in the following loss function (Equation 7 in the paper):</p>
<p><br></p>
<p><span class="math display">\[\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\log\sigma\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\big]\]</span></p>
</section>
<section id="appendix-a.4-deriving-the-gradient-of-the-dpo-objective" class="level2">
<h2 class="anchored" data-anchor-id="appendix-a.4-deriving-the-gradient-of-the-dpo-objective">Appendix A.4 Deriving the Gradient of the DPO Objective</h2>
<p>For the gradient of the DPO objective—I’m unclear why in this section it’s written with the <span class="math inline">\(y_l\)</span> term as the first term, minus the <span class="math inline">\(y_w\)</span> term whereas in Equation 7 it’s written as the <span class="math inline">\(y_w\)</span> term first, minus the <span class="math inline">\(y_l\)</span> term—I’m going to go with what they have written in the Equation 7. <strong>Proceed with caution as this deviates from the appendix.</strong></p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{DPO}(\pi_\theta;\pi_{ref}) = -\nabla_\theta\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\log\sigma\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\big]\]</span></p>
<p><br></p>
<p>They rewrite this as:</p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{DPO}(\pi_\theta;\pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[ \frac{\sigma'(u)}{\sigma(u)}\nabla_\theta(u)\big]\]</span></p>
<p>Where:</p>
<p><span class="math display">\[u = \beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\]</span></p>
<p>Note: their <span class="math inline">\(u\)</span> is flipped but I’m following Equation 7.</p>
<p>Using ChatGPT (simplifying its response), the following</p>
<p><span class="math display">\[\log\sigma\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\]</span></p>
<p>is rewritten as:</p>
<p><span class="math display">\[\log \sigma(u)\]</span></p>
<p><span class="math inline">\(u\)</span> is a function of <span class="math inline">\(\theta\)</span> so taking the derivative of <span class="math inline">\(\log \sigma(u)\)</span> with respect to <span class="math inline">\(\theta\)</span> requires the chain rule:</p>
<p><br></p>
<p><span class="math display">\[\frac{d}{d\theta}\log(\sigma(u)) = \frac{du}{d\theta} \times \frac{d\sigma}{du} \times \frac{d}{d\sigma}\log(\sigma(u))\]</span></p>
<p>Working right to left:</p>
<p><span class="math display">\[\frac{d}{d\sigma(u)}\log\sigma(u) = \frac{1}{\sigma(u)}\]</span></p>
<p><br></p>
<p><span class="math display">\[\frac{d\sigma}{du} = \sigma(u) \times (1 - \sigma(u))\]</span></p>
<p><br></p>
<p>Multiplying those two together, the <span class="math inline">\(\sigma(u)\)</span> terms cancel out:</p>
<p><span class="math display">\[\frac{1}{\sigma(u)} \times \sigma(u) \times (1 - \sigma(u)) = 1 - \sigma(u)\]</span></p>
<p><br></p>
<p>Using the property of sigmoid that <span class="math inline">\(\sigma(-u) = 1 - \sigma(u)\)</span> I’ll rewrite the gradient using that:</p>
<p><br></p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{DPO}(\pi_\theta;\pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[ \frac{\sigma'(u)}{\sigma(u)}\nabla_\theta(u)\big] = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[ (1-\sigma(u))\nabla_\theta(u)\big] = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[ \sigma(-u)\nabla_\theta(u)\big]\]</span></p>
<p><br></p>
<p>Plugging Equation 7 version of the full form of <span class="math inline">\(u\)</span> back in:</p>
<p><br></p>
<p><span class="math display">\[-\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[ \sigma\big( \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} - \beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} \big)\nabla_\theta\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\big]\]</span></p>
<p><br></p>
<p>I’ll look at just the <span class="math inline">\(\nabla_\theta\)</span> term:</p>
<p><span class="math display">\[\nabla_\theta\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\]</span></p>
<p><br></p>
<p>I’ll use logarithm properties to expand the logarithms:</p>
<p><br></p>
<p><span class="math display">\[\beta\nabla_\theta\big(\log\pi_\theta(y_w|x) - \log\pi_{ref}(y_w|x) - \log\pi_\theta(y_l|x) + \log\pi_{ref}(y_l|x)\big)\]</span></p>
<p><br></p>
<p>I’m pretty certain <span class="math inline">\(\pi_{ref}\)</span> is not a function of <span class="math inline">\(\theta\)</span> (i.e.&nbsp;it’s not being trained) so the gradient of that with respect to <span class="math inline">\(\theta\)</span> is 0, so we’re left with:</p>
<p><br></p>
<p><span class="math display">\[\beta\nabla_\theta\big(\log\pi_\theta(y_w|x) - \log\pi_\theta(y_l|x)\big)\]</span></p>
<p><br></p>
<p>Which gives the final form of the gradient:</p>
<p><span class="math display">\[-\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[ \beta\sigma\big( \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} - \beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} \big)\big[\nabla_\theta\log\pi_\theta(y_w|x) - \nabla_\theta\log\pi_\theta(y_l|x)\big)\big]\big]\]</span></p>
<p><br></p>
<p>I’m not sure why in their final form of the gradient, instead of <span class="math inline">\(\pi_\theta\)</span> they have just <span class="math inline">\(\pi\)</span>.</p>
<p><br></p>
<p>Substituting <span class="math inline">\(\hat{r}(x,y) = \beta\log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}\)</span> and pulling out the <span class="math inline">\(\beta\)</span></p>
<p><br></p>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{DPO}(\pi_\theta;\pi_{ref}) = -\beta\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\sigma\big( \hat{r}(x,y_l) - \hat{r}(x,y_w) \big)\big[\nabla_\theta\log\pi_\theta(y_w|x) - \nabla_\theta\log\pi_\theta(y_l|x)\big)\big]\big]\]</span></p>
<p><br></p>
<p>Which is the same as the equation in section 4, page 5 “What does the DPO update do?”.</p>
</section>
<section id="appendix-b-dpo-implementation-details-and-hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="appendix-b-dpo-implementation-details-and-hyperparameters">Appendix B: DPO Implementation Details and Hyperparameters</h2>
<p>Here is the formula (Equation 7):</p>
<p><span class="math display">\[\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\log\sigma\big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\big)\big]\]</span></p>
<p><br></p>
<p>If I expand the log terms inside the sigmoid function:</p>
<p><span class="math display">\[\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\log\sigma\big(\beta\log\pi_\theta(y_w|x)- \beta\log\pi_{ref}(y_w|x) - \beta\log\pi_\theta(y_l|x) + \beta\log\pi_{ref}(y_l|x)\big]\]</span></p>
<p><br></p>
<p>Rearranging terms to combine <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{ref}\)</span> values:</p>
<p><span class="math display">\[\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}}\big[\log\sigma\big(\beta\log\pi_\theta(y_w|x)- \beta\log\pi_\theta(y_l|x) + \beta\log\pi_{ref}(y_l|x) - \beta\log\pi_{ref}(y_w|x)\big]\]</span></p>
<p><br></p>
<p>Note that for policy model (<span class="math inline">\(\pi_\theta\)</span>) the log probabilities of the rejected responses are subtracted from the chosen responses (i.e.&nbsp;<em>chosen - rejected</em>):</p>
<p><span class="math display">\[\beta\log\pi_\theta(y_w|x)- \beta\log\pi_\theta(y_l|x)\]</span></p>
<p><br></p>
<p>But for the reference model (<span class="math inline">\(\pi_{ref}\)</span>) the log probabilities of the <strong>chosen</strong> responses are subtracted from the reject responses (i.e.&nbsp;<em>rejected - chosen</em>):</p>
<p><span class="math display">\[\beta\log\pi_{ref}(y_l|x) - \beta\log\pi_{ref}(y_w|x)\]</span></p>
<p><br></p>
<p>Here is the implementation code</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">  pi_logps: policy logprobs, shape (B,)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  ref_logps: reference model logprobs, shape (B,)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">  yw_idxs: preferred completion indices in [0, B-1], shape (T,)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">  beta: temperature controlling strength of KL penalty</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  Each pair of (yw_idxs[i], yl_idxs[i]) represents the</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">  indices of a single preference pair.</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  pi_yw_logps, pi_yl_logps <span class="op">=</span> pi_logps[yw_idxs], pi_logps[yl_idxs]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  ref_yw_logps, ref_yl_logps <span class="op">=</span> ref_logps[yw_idxs], ref_logps[yl_idxs]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  pi_logratios <span class="op">=</span> pi_yw_logps <span class="op">-</span> pi_yl_logps</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  ref_logratios <span class="op">=</span> ref_yw_logps <span class="op">-</span> ref_yl_logps</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> (pi_logratios <span class="op">-</span> ref_logratios))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  rewards <span class="op">=</span> beta <span class="op">*</span> (pi_logps <span class="op">-</span> ref_logps).detach()</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> losses, rewards</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>pi_logratios</code> is defined as the rejected responses subracted from the chosen responses (i.e.&nbsp;<em>chosen - rejected</em>):</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>pi_logratios <span class="op">=</span> pi_yw_logps <span class="op">-</span> pi_yl_logps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><br></p>
<p><code>ref_logratios</code> is also defined the same way (<em>chosen - rejected</em>):</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ref_logratios <span class="op">=</span> ref_yw_logps <span class="op">-</span> ref_yl_logps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then when calculating <code>losses</code>, <code>ref_logratios</code> is subtracted from <code>pi_logratios</code>, so the signs match:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> (pi_logratios <span class="op">-</span> ref_logratios))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>is the same as</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> ((pi_yw_logps <span class="op">-</span> pi_yl_logps) <span class="op">-</span> (ref_yw_logps <span class="op">-</span> ref_yl_logps))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which is simplified to</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> (pi_yw_logps <span class="op">-</span> pi_yl_logps <span class="op">-</span> ref_yw_logps <span class="op">+</span> ref_yl_logps))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>rearranging the terms to match the paper:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> (pi_yw_logps <span class="op">-</span> pi_yl_logps <span class="op">+</span> ref_yl_logps <span class="op">-</span> ref_yw_logps))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <a href="https://github.com/huggingface/trl/blob/9a28b3fd0505aa38798f0122ab0ff3bb795384dd/trl/trainer/dpo_trainer.py#L844">TRL library</a> has the same implementation:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>pi_logratios <span class="op">=</span> policy_chosen_logps <span class="op">-</span> policy_rejected_logps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>ref_logratios <span class="op">=</span> reference_chosen_logps <span class="op">-</span> reference_rejected_logps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> pi_logratios <span class="op">-</span> ref_logratios</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>