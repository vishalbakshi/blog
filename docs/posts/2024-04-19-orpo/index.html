<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vishal Bakshi">
<meta name="dcterms.date" content="2024-04-19">
<meta name="description" content="Summarizing the research from the Odds Ratio Preference Optimization paper and exploring its math to better understand it.">

<title>Vishal Bakshi’s Blog - Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Vishal Bakshi’s Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#main-takeaways" id="toc-main-takeaways" class="nav-link" data-scroll-target="#main-takeaways">Main Takeaways</a></li>
  <li><a href="#quick-review-dpo-and-rlhf" id="toc-quick-review-dpo-and-rlhf" class="nav-link" data-scroll-target="#quick-review-dpo-and-rlhf">Quick Review: DPO and RLHF</a></li>
  <li><a href="#quick-review-sft-without-orpo" id="toc-quick-review-sft-without-orpo" class="nav-link" data-scroll-target="#quick-review-sft-without-orpo">Quick Review: SFT without ORPO</a></li>
  <li><a href="#orpo-loss" id="toc-orpo-loss" class="nav-link" data-scroll-target="#orpo-loss">ORPO Loss</a></li>
  <li><a href="#odds-ratio" id="toc-odds-ratio" class="nav-link" data-scroll-target="#odds-ratio">Odds Ratio</a></li>
  <li><a href="#gradient-of-orpo-loss" id="toc-gradient-of-orpo-loss" class="nav-link" data-scroll-target="#gradient-of-orpo-loss">Gradient of ORPO Loss</a>
  <ul class="collapse">
  <li><a href="#deltad" id="toc-deltad" class="nav-link" data-scroll-target="#deltad"><span class="math inline">\(\delta(d)\)</span></a></li>
  <li><a href="#hd" id="toc-hd" class="nav-link" data-scroll-target="#hd"><span class="math inline">\(h(d)\)</span></a></li>
  </ul></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a>
  <ul class="collapse">
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models">Models</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a></li>
  <li><a href="#reward-model" id="toc-reward-model" class="nav-link" data-scroll-target="#reward-model">Reward Model</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#computation-efficiency" id="toc-computation-efficiency" class="nav-link" data-scroll-target="#computation-efficiency">Computation Efficiency</a></li>
  </ul></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work">Future Work</a></li>
  <li><a href="#section-4-odds-ratio-preference-optimization" id="toc-section-4-odds-ratio-preference-optimization" class="nav-link" data-scroll-target="#section-4-odds-ratio-preference-optimization">Section 4: Odds Ratio Preference Optimization</a>
  <ul class="collapse">
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link" data-scroll-target="#preliminaries">4.1 Preliminaries</a></li>
  <li><a href="#objective-function-of-orpo" id="toc-objective-function-of-orpo" class="nav-link" data-scroll-target="#objective-function-of-orpo">4.2 Objective Function of ORPO</a></li>
  <li><a href="#gradient-of-orpo" id="toc-gradient-of-orpo" class="nav-link" data-scroll-target="#gradient-of-orpo">4.3 Gradient of ORPO</a></li>
  </ul></li>
  <li><a href="#section-7.1-comparison-to-probability-ratio" id="toc-section-7.1-comparison-to-probability-ratio" class="nav-link" data-scroll-target="#section-7.1-comparison-to-probability-ratio">Section 7.1: Comparison to Probability Ratio</a></li>
  <li><a href="#trl-library-implementation" id="toc-trl-library-implementation" class="nav-link" data-scroll-target="#trl-library-implementation">TRL Library Implementation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">paper math</div>
    <div class="quarto-category">paper summary</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    Summarizing the research from the Odds Ratio Preference Optimization paper and exploring its math to better understand it.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vishal Bakshi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 19, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post I’ll provide a summary (and an exploration of some of the math) for the research paper <a href="https://arxiv.org/pdf/2403.07691">ORPO: Monolithic Preference Optimization without Reference Model</a>.</p>
<p>Here’s the abstract:</p>
<blockquote class="blockquote">
<p>While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on AlpacaEval2.0 (Figure 1), 66.19% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-<span class="math inline">\(\alpha\)</span> (7B) and Mistral-ORPO-<span class="math inline">\(\beta\)</span> (7B).</p>
</blockquote>
</section>
<section id="main-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="main-takeaways">Main Takeaways</h2>
<p>I took away 5 main points from this paper:</p>
<ul>
<li>ORPO does not use a reference model like DPO (in the KL term) or a reward model and initial SFT snapshot like RLHF.</li>
<li>Instead, ORPO directly trains a preference-aligned Supervised Fine-Tuned (SFT) model.</li>
<li>The ORPO loss includes a penalty (added to the normal causal LM Negative Log Likelihood loss) which maximizes the likelihood of generating a favored reponse.</li>
<li>ORPO consistently is preferred by a reward model against SFT and RLHF.</li>
<li>The ORPOR win rate vs.&nbsp;DPO increases as model size increases.</li>
</ul>
</section>
<section id="quick-review-dpo-and-rlhf" class="level2">
<h2 class="anchored" data-anchor-id="quick-review-dpo-and-rlhf">Quick Review: DPO and RLHF</h2>
<p>In order to better visualize how ORPO differs from DPO and RLHF I’ll provide a couple of visuals to highlight those points.</p>
<p>First, from <a href="https://huyenchip.com/2023/05/02/rlhf.html">Chip Huyen’s blog post on RLHF</a>, I’ve highlighted where the SFT model and reward model are used.</p>
<p><img src="1.png" style="width:100%;"></p>
<p>Next I’ll show the DPO loss function, where the reference model is used in the log probability ratios:</p>
<p><img src="2.png" style="width:100%;"></p>
<p>Finally, from the ORPO paper, a graphic that compares RLHF, DPO and ORPO. The odds ratio is shown on the right (it strongly adapts to chosen reponses and has a weak penalty for rejected responses).</p>
<p><img src="3.png" style="width:100%;"></p>
</section>
<section id="quick-review-sft-without-orpo" class="level2">
<h2 class="anchored" data-anchor-id="quick-review-sft-without-orpo">Quick Review: SFT without ORPO</h2>
<p>I’ll do a quick review of what the loss function looks like for SFT without ORPO. The loss function is cross entropy loss, where the log probabilities <span class="math inline">\(\log(p_i^{(k)})\)</span> of the label tokens (when <span class="math inline">\(y_i\)</span> is True) are average across the input sequence of length <span class="math inline">\(m\)</span>:</p>
<p><span class="math display">\[\mathcal{L} = -\frac{1}{m}\sum^m_{k=1}\sum^{|V|}_{i=1}y_i^{(k)}\cdot\log(p_i^{(k)})\]</span></p>
<p>This loss function is effective for domain adaptation (for next token prediction) but doesn’t have a mechanism to penalize rejected responses.</p>
<p>In the ORPO paper they studies the log probabilities of chosen and rejected responses during SFT (without ORPO) and found that both increase over the course of training. In other words, the model does not discriminate between desired and undesired tokens.</p>
<p><img src="5.png" style="width:100%;"></p>
</section>
<section id="orpo-loss" class="level2">
<h2 class="anchored" data-anchor-id="orpo-loss">ORPO Loss</h2>
<p>The ORPO loss function is an enhancement or augmentation of SFT loss:</p>
<p><span class="math display">\[\mathcal{L} = \mathbb{E}_{(x, y_w, y_l)}\big[\mathcal{L}_{SFT} + \lambda \cdot\mathcal{L}_{OR}\big]\]</span></p>
<p>In this loss function, <span class="math inline">\(\mathcal{L}_{SFT}\)</span> helps the model adapt to the specified subset of the domain. In other words, it does what good ol’ SFT does—fine-tune a model toward a given downstream task (chat, QA, reasoning, etc.).</p>
<p>The <span class="math inline">\(\mathcal{L}_{OR}\)</span> term (called relative ratio loss) helps the mode disfavor generations in the rejected responses set.</p>
<p>The term <span class="math inline">\(\lambda\)</span> weighs the relative ratio loss and affects how much the model disfavors rejected responses. An ablation study on this term is done in the appendix which I’ll talk about in a bit.</p>
<p>The relative ratio loss is defined as:</p>
<p><span class="math display">\[\mathcal{L}_{OR} = -\log\sigma\big(\log\frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)}\big)\]</span></p>
<p>This loss contains the <strong>odds ratio</strong> which I’ll talk about next.</p>
</section>
<section id="odds-ratio" class="level2">
<h2 class="anchored" data-anchor-id="odds-ratio">Odds Ratio</h2>
<p>The odds ratio is a ratio of:</p>
<ul>
<li>the odds of generating the desired output sequence <span class="math inline">\(y_w\)</span> given an input sequence <span class="math inline">\(x\)</span></li>
<li>and the odds of generating the undesired output sequence <span class="math inline">\(y_l\)</span> given an input sequence <span class="math inline">\(x\)</span>.</li>
</ul>
<p>Odds are defined as follows:</p>
<p><span class="math display">\[\textbf{odds}_\theta(y|x) = \frac{P_\theta(y|x)}{1-P_\theta(y|x)}\]</span></p>
<p>Where <span class="math inline">\(P_\theta(y|x)\)</span> is the likelihood that the model <span class="math inline">\(\theta\)</span> will generate <span class="math inline">\(y\)</span> given input <span class="math inline">\(x\)</span>.</p>
<blockquote class="blockquote">
<p>Intuitively <span class="math inline">\(\textbf{odds}_\theta(y|x)=k\)</span> implies that it is <span class="math inline">\(k\)</span> times more likely for the model <span class="math inline">\(\theta\)</span> to generate the output sequence <span class="math inline">\(y\)</span> than not generating it <span class="math inline">\(y\)</span>.</p>
</blockquote>
<p>Note that as the likelihood <span class="math inline">\(P\)</span> increases, <span class="math inline">\(1-P\)</span> decreases and the <strong>odds</strong> increase.</p>
<p>In the following visual I show how we can plot odds as <span class="math inline">\(\frac{x}{1-x}\)</span> in Desmos and see how quickly the odds increase as likelihood increases.</p>
<p><img src="6.png" style="width:100%;"></p>
<p>I also plot the relative ratio loss as a function of the odds ratio in desmos. You can see that as the odds ratio (in the graph denoted by <span class="math inline">\(x\)</span>) increases the loss function decreases. When the odds ratio increases, the numerator increases relative to the denominator. This means that the odds that the model will generated desired responses increases relative to the odds that it will generated undesired responses. In other words, as the model is trained and learns to minimize the loss, it also learns to maximize the likelihood of generating desired responses.</p>
<p><img src="7.png" style="width:100%;"></p>
<p>In the ORPO paper, they study how the log odds ratio and log probabilities of chosen and rejected responses vary over the course of training. They find that as the model is trained longer, the log odds ratio increases and after awhile, the log probabilities of rejected responses decreases (while chosen log probs continue to increase). This show the discriminating behavior of the relative ratio loss as compared to SFT loss.</p>
<p><img src="8.png" style="width:100%;"></p>
</section>
<section id="gradient-of-orpo-loss" class="level2">
<h2 class="anchored" data-anchor-id="gradient-of-orpo-loss">Gradient of ORPO Loss</h2>
<p>I’ll go through some of the intuition provided in the paper around the gradient of the ORPO loss function, which contains two factors: <span class="math inline">\(\delta(d)\)</span> and <span class="math inline">\(h(d)\)</span> which are functions of the dataset <span class="math inline">\(d\)</span> which contains inputs <span class="math inline">\(x\)</span>, desired responses <span class="math inline">\(y_w\)</span> and undesired responses <span class="math inline">\(y_l\)</span>.</p>
<p><img src="9.png" style="width:100%;"></p>
<section id="deltad" class="level3">
<h3 class="anchored" data-anchor-id="deltad"><span class="math inline">\(\delta(d)\)</span></h3>
<p>I’ll start by looking at <span class="math inline">\(\delta(d)\)</span> first: when the odds of the desired responses are relatively higher than the undesired responses, this term will converge to 0.</p>
<p>In the desmos graph below, <span class="math inline">\(x\)</span> represents the odds ratio and so <span class="math inline">\(\delta(d)\)</span> takes the form of <span class="math inline">\(\frac{1}{1+x}\)</span>:</p>
<p><img src="10.png" style="width:100%;"></p>
<p>For the following concept:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\delta(d)\)</span> will play the role of a penalty term. accelerating the parameter updates if the model is more likely to generate the rejected responses.</p>
</blockquote>
<p>I found the following rearranging of <span class="math inline">\(\delta(d)\)</span> terms more intuitive:</p>
<p><br></p>
<p><span class="math display">\[\delta(d) = \big[1 + \frac{\textbf{odds}_\theta P(y_w|x)}{\textbf{odds}_\theta P(y_l|x)}\big]^{-1}\]</span></p>
<p>I’ll replace <span class="math inline">\(1\)</span> with an equivalent fraction that has the same denominator as the odds ratio:</p>
<p><br></p>
<p><span class="math display">\[\delta(d) = \big[\frac{\textbf{odds}_\theta P(y_l|x)}{\textbf{odds}_\theta P(y_l|x)} + \frac{\textbf{odds}_\theta P(y_w|x)}{\textbf{odds}_\theta P(y_l|x)}\big]^{-1}\]</span></p>
<p>Now I can easily add together the fractions inside the brackets because they have the same denominator:</p>
<p><br></p>
<p><span class="math display">\[\delta(d) = \big[\frac{\textbf{odds}_\theta P(y_l|x) + \textbf{odds}_\theta P(y_w|x)}{\textbf{odds}_\theta P(y_l|x)}\big]^{-1}\]</span></p>
<p>Raising a fraction to the <span class="math inline">\(-1\)</span> power is the same as flipping it:</p>
<p><br></p>
<p><span class="math display">\[\delta(d) = \big[\frac{\textbf{odds}_\theta P(y_l|x)}{\textbf{odds}_\theta P(y_l|x) + \textbf{odds}_\theta P(y_w|x)}\big]\]</span></p>
<p><br></p>
<p>I find this form of <span class="math inline">\(\delta(d)\)</span> more intuitive. Here you can directly see that as the odds of generating rejected responses (the numerator) increases, the overall fraction increases and thus the gradient increases, accelerating the parameter update.</p>
</section>
<section id="hd" class="level3">
<h3 class="anchored" data-anchor-id="hd"><span class="math inline">\(h(d)\)</span></h3>
<p>The second term of the ORPO loss gradient is <span class="math inline">\(h(d)\)</span> which has the form:</p>
<p><span class="math display">\[h(d) = \frac{\nabla_\theta\log P_\theta(y_w|x)}{1-P_\theta(y_w|x)} - \frac{\nabla_\theta\log P_\theta(y_l|x)}{1-P_\theta(y_l|x)}\]</span></p>
<blockquote class="blockquote">
<p>For the chosen responses, this accelerates the model’s adaptation toward the distribution of chosen responses as the likelihood increases.</p>
</blockquote>
<p>As the likelihood of chosen reponses (<span class="math inline">\(P_\theta(y_w|x)\)</span>) increases, the denominator (<span class="math inline">\(1-P\)</span>) decreases and the first fraction increases, increasing the gradient and accelerating the model’s adaptation toward predicting chosen responses.</p>
<p>On the other hand, as the likelihood of the rejected response increases the denominator decreases and the second fraction increases, decreasing the gradient and slowing down the model’s adapation toward the rejected response distribution.</p>
<p>I’ve summarized the behavior of the two terms below:</p>
<p><img src="11.png" style="width:100%;"></p>
<p>I see these two terms, <span class="math inline">\(\delta(d)\)</span> and <span class="math inline">\(h(d)\)</span>, contrasting each other and keeping the gradients from vanishing or exploding.</p>
</section>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>In this section I’ll recap the training details provided in the paper.</p>
<section id="models" class="level3">
<h3 class="anchored" data-anchor-id="models">Models</h3>
<p>The authors trained the following models for SFT, PPO, DPO and ORPO to compare results across each other:</p>
<ul>
<li>OPT (125M to 1.3B).</li>
<li>Phi-2 (2.7B).</li>
<li>Llama 2 (7B).</li>
<li>Mistral (7B).</li>
</ul>
<p>They used the following training techniques and hyperparameters:</p>
<ul>
<li>Flash Attention 2.</li>
<li>Deep Speed Zero 2 (for OPT series and Phi-2).</li>
<li>FSDP (Fully Sharded Data Parallel) for Llama-2 and Mistral (both 7B).</li>
<li>AdamW and paged Adam optimizers.</li>
<li>Linear Warmup with Cosine Decay.</li>
<li>Input length of 1024 (HH dataset) and 2048 (UltraFeedback dataset).</li>
<li>SFT: Max LR of 1e-5, trained for 1 epoch.</li>
<li>DPO: <span class="math inline">\(\beta\)</span>=0.1, LR = 5e-6, 3 epochs.</li>
<li>ORPO: LR=8e-6, 10 epochs.</li>
</ul>
<p><img src="12.png" style="width:50%;"></p>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<ul>
<li>Anthropic’s HH-RLHF.</li>
<li>Binarized UltraFeedback.</li>
</ul>
</section>
<section id="reward-model" class="level3">
<h3 class="anchored" data-anchor-id="reward-model">Reward Model</h3>
<p>The reward model is used to score the responses from the different models. They trained OPT 250M and 1.3B on each dataset for a single epoch.</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Their ORPO model performed better than their SFT, DPO and PPO models for each model family for AlpacaEval 1.0 and 2.0 (corrected for length-bias in models).</p>
<p><img src="13.png" style="width:100%;"></p>
<p>On the MT-Bench dataset they found that:</p>
<blockquote class="blockquote">
<p>In comparison to <span class="math inline">\(\lambda\)</span> = 0.1, Mistral+ORPO (7B) with <span class="math inline">\(\lambda\)</span> = 1.0 performs worse in extraction, math and reasoning=, which are the categories that generally require deterministic answers. On the other hand, it performs better in STEM, humanities, and roleplay, which ask the generations without hard answers.</p>
</blockquote>
<p>I will note that STEM contains science, engineering and math which can contain deterministic answers so I’m not sure this distinction between the two <span class="math inline">\(\lambda\)</span> values holds.</p>
<p>On the HH-RLHF dataset, ORPO had high win-rates for 125M and 350M models against SFT and SFT+PPO, and high win-rate for 1.3B against SFT+DPO.</p>
<p><img src="14.png" style="width:100%;"></p>
<p>The trend was the same for the UltraFeedback dataset:</p>
<p><img src="15.png" style="width:100%;"></p>
<section id="computation-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="computation-efficiency">Computation Efficiency</h3>
<p>The authors found the following results as relating to computational efficiency:</p>
<ul>
<li>ORPO does not require a reference model which in RLHF and DPO is the model trained with SFT used during training to keep the parameterized model from degenerating.</li>
<li>DPO and RLHF require two SFT models: a frozen reference model (KL term) and the model being trained.</li>
<li>ORPO only has one model: the model being trained with SFT. This requires half the forward passes of DPO or RLHF.</li>
</ul>
</section>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<p>The authors highlighted that their work lacks comparisons with alignment algorithms other than PPO and DPO (such KTO or IPO). They also only trained models that are up to 7B parameters in size.</p>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future Work</h2>
<p>The authors highlighted three areas of future work:</p>
<ul>
<li>Evaluate ORPO performance on models larger than 7B parameters.</li>
<li>Evaluate the impact of ORPO on pretrained models.</li>
<li>Expand to consecutive preference alignment algorithms.</li>
</ul>
<hr>
<p>In the following sections I walk through some of the math in the paper.</p>
</section>
<section id="section-4-odds-ratio-preference-optimization" class="level2">
<h2 class="anchored" data-anchor-id="section-4-odds-ratio-preference-optimization">Section 4: Odds Ratio Preference Optimization</h2>
<section id="preliminaries" class="level3">
<h3 class="anchored" data-anchor-id="preliminaries">4.1 Preliminaries</h3>
<blockquote class="blockquote">
<p>Given an input sequence <span class="math inline">\(x\)</span>, the average log-likelihood of generating the output sequence <span class="math inline">\(y\)</span>, of length <span class="math inline">\(m\)</span> tokens, is computed as:</p>
</blockquote>
<p><span class="math display">\[\log P_\theta(y|x) = \frac{1}{m}\sum^m_{t=1} \log P_\theta(y_t|x,y_{&lt;t})\]</span></p>
<p>Excellently worded explanations from ChatGPT (emphasis mine):</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\frac{1}{m}\)</span>​: This term represents the reciprocal of the length of the output sequence <span class="math inline">\(y\)</span>. It’s the inverse of the number of tokens in the output sequence. This term normalizes the sum of log probabilities over the length of the sequence, <strong>ensuring that longer sequences don’t disproportionately influence the average log-likelihood</strong>.</p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(\log P_\theta​(y_t​∣x,y_{&lt;t}​)\)</span>: This term represents the logarithm of the probability of generating the current token <span class="math inline">\(y_t\)</span>​ given the input sequence <span class="math inline">\(x\)</span> and the preceding tokens <span class="math inline">\(y_{&lt;t}\)</span>. It measures how likely the model thinks the current token <span class="math inline">\(y_t\)</span>​ is, given the input <span class="math inline">\(x\)</span> and the previously generated tokens <span class="math inline">\(y_{&lt;t}\)</span>​.</p>
</blockquote>
<p>Here’s what <span class="math inline">\(log(x)\)</span> looks like:</p>
<p><img src="16.png" style="width:100%;"></p>
<p>The odds of generating the output sequence <span class="math inline">\(y\)</span> given an input sequence <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\textbf{odds}_\theta(y|x) = \frac{P_\theta(y|x)}{1-P_\theta(y|x)}\]</span></p>
<blockquote class="blockquote">
<p>Intuitively <span class="math inline">\(\textbf{odds}_\theta(y|x)=k\)</span> implies that it is <span class="math inline">\(k\)</span> times more likely for the model <span class="math inline">\(\theta\)</span> to generate the output sequence <span class="math inline">\(y\)</span> than not generating it.</p>
</blockquote>
<p>Writing that out:</p>
<p><span class="math display">\[\textbf{odds}_\theta(y|x)= \frac{P_\theta(y|x)}{1-P_\theta(y|x)} = k\]</span></p>
<p><br></p>
<p><span class="math display">\[\frac{P_\theta(y|x)}{1-P_\theta(y|x)} = k\]</span></p>
<p><br></p>
<p><span class="math display">\[P_\theta(y|x) = k[1-P_\theta(y|x)]\]</span></p>
<blockquote class="blockquote">
<p>it is <span class="math inline">\(k\)</span> times more likely to generate <span class="math inline">\(y\)</span> (i.e.&nbsp;<span class="math inline">\(P_\theta)\)</span>) than not generating it (<span class="math inline">\(1-P_\theta\)</span>).</p>
</blockquote>
<blockquote class="blockquote">
<p>The odds ratio of the chosen response <span class="math inline">\(y_w\)</span> over the rejected response:</p>
</blockquote>
<p><span class="math display">\[\textbf{OR}_\theta(y_w,y_l) = \frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)}\]</span></p>
<blockquote class="blockquote">
<p>indicates how much more likely it is for the model <span class="math inline">\(\theta\)</span> to generate <span class="math inline">\(y_w\)</span> than <span class="math inline">\(y_l\)</span> given input <span class="math inline">\(x\)</span>.</p>
</blockquote>
<p>If the odds ratio is some value <span class="math inline">\(r\)</span>:</p>
<p><span class="math display">\[\frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)} = r\]</span></p>
<p><br></p>
<p><span class="math display">\[\textbf{odds}_\theta(y_w|x) = r \cdot \textbf{odds}_\theta(y_l|x)\]</span></p>
<p>The odds of the model generating a chosen response is <span class="math inline">\(r\)</span> times the odds of it generating a rejected response.</p>
</section>
<section id="objective-function-of-orpo" class="level3">
<h3 class="anchored" data-anchor-id="objective-function-of-orpo">4.2 Objective Function of ORPO</h3>
<p><span class="math display">\[\mathcal{L}_{ORPO}=\mathbb{E}_{(x,y_w,y_l)}\big[ \mathcal{L}_{SFT} + \lambda \cdot \mathcal{L}_{OR}\big]\]</span></p>
<p>Where:</p>
<p><span class="math display">\[\mathcal{L}_{SFT} \text{ is the supervised fine-tuning loss (maximizes the likelihood of generating the reference tokens)}\]</span></p>
<p><br></p>
<p><span class="math display">\[\mathcal{L}_{OR} \text{ is the relative ratio loss (maximizes the odds ratio between the likelihood of generating the favored response } y_w\text{ and the disfavored response } y_l)\]</span></p>
<section id="relative-ratio-loss-mathcall_or" class="level4">
<h4 class="anchored" data-anchor-id="relative-ratio-loss-mathcall_or">Relative Ratio Loss <span class="math inline">\(\mathcal{L}_{OR}\)</span></h4>
<p><span class="math display">\[\mathcal{L}_{OR} = - \log \sigma\big(\log \frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)} \big)\]</span></p>
<p>The term inside log-sigmoid, <span class="math inline">\(\log \frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)}\)</span>, increases as the odds of generating chosen responses increases (and therefore increases as the likelihood of the model generating chosen responses increases).</p>
<p><span class="math inline">\(\mathcal{L}_{OR}\)</span>, represented below by <span class="math inline">\(-\log\sigma(x)\)</span>, decreases as x increases, meaning the loss decreases as the likelihood and the odds of the model generating chosen responses increases.</p>
<p><img src="17.png" style="width:100%;"></p>
<p>Minimizing <span class="math inline">\(\mathcal{L}_{OR}\)</span> means maximizing <span class="math inline">\(\frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)}\)</span></p>
</section>
</section>
<section id="gradient-of-orpo" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-orpo">4.3 Gradient of ORPO</h3>
<p><span class="math display">\[\nabla_\theta\mathcal{L}_{OR}=\delta(d) \cdot h(d)\]</span></p>
<p>Where:</p>
<p><span class="math display">\[\delta(d) \text{ penalizes the wrong predictions}\]</span> <span class="math display">\[h(d) \text{ contrasts between chosen and rejected responses}\]</span> <span class="math display">\[d = (x, y_w, y_l) \sim D\]</span></p>
<p>Full form of <span class="math inline">\(\delta(d)\)</span>:</p>
<p><span class="math display">\[\delta(d) = \big[1 + \frac{\textbf{odds}_\theta P(y_w|x)}{\textbf{odds}_\theta P(y_l|x)} \big]^{-1}\]</span></p>
<p><br></p>
<p>Visualizing this as <span class="math inline">\(\frac{1}{1+x}\)</span>, as the odds ratio increases (odds of generating favored responses increases) <span class="math inline">\(delta(d)\)</span> decreases.</p>
<p><img src="18.png" style="width:100%;"></p>
<p>Rewriting that by expanding out the fraction in multiple steps. First, replace <span class="math inline">\(1\)</span> with an equivalent (rejected odds divided by rejected odds):</p>
<p><span class="math display">\[\delta(d) = \big[\frac{\textbf{odds}_\theta P(y_l|x)}{\textbf{odds}_\theta P(y_l|x)}  + \frac{\textbf{odds}_\theta P(y_w|x)}{\textbf{odds}_\theta P(y_l|x)} \big]^{-1}\]</span></p>
<p>Now that they have the same denominator, add the fractions:</p>
<p><br></p>
<p><span class="math display">\[\delta(d) = \big[\frac{\textbf{odds}_\theta P(y_l|x) +\textbf{odds}_\theta P(y_w|x)}{\textbf{odds}_\theta P(y_l|x)} \big]^{-1}\]</span></p>
<p>Taking the inverse (exponent of <span class="math inline">\(-1\)</span>), flips the fraction:</p>
<p><br></p>
<p><span class="math display">\[\delta(d) = \big[\frac{\textbf{odds}_\theta P(y_l|x)}{\textbf{odds}_\theta P(y_l|x) +\textbf{odds}_\theta P(y_w|x)} \big]\]</span></p>
<p>I find this version more intuitive as it’s easier to see that as the odds of generating rejected responses increases, <span class="math inline">\(\delta(d)\)</span> <strong>increases</strong>. As the odds of generating favored responses increases, <span class="math inline">\(\delta(d)\)</span> <strong>decreases</strong>.</p>
<blockquote class="blockquote">
<p>When the odds of the favored responses are relatively higher than the disfavored responses, <span class="math inline">\(\delta(d)\)</span> will converge to 0.</p>
</blockquote>
<blockquote class="blockquote">
<p>This indicates that <span class="math inline">\(\delta(d)\)</span> will play the role of a penalty term, accelerating the parameter updates<br>
if the model is more likely to generate the rejected responses.</p>
</blockquote>
<p>Full form of <span class="math inline">\(h(d)\)</span>:</p>
<p><span class="math display">\[h(d) = \frac{\nabla_\theta\log P_\theta(y_w|x)}{1-P_\theta(y_w|x)} - \frac{\nabla_\theta\log P_\theta(y_l|x)}{1-P_\theta(y_l|x)}\]</span></p>
<blockquote class="blockquote">
<p><span class="math inline">\(h(d)\)</span> implies a weighted contrast of the two gradients from the chosen and rejected responses. Specifically, <span class="math inline">\(1-P(y|x)\)</span> in the denominators amplifies the gradients when the corresponding side of the likelihood <span class="math inline">\(P(y|x)\)</span> is low. <strong>For the chosen responses, this accelerates the model’s adaptation toward the distribution of chosen responses as the likelihood increases.</strong></p>
</blockquote>
<p>The last sentence (that I bolded) clarifies the concept (that the previous sentence muddied for me). As <span class="math inline">\(P_\theta(y_w|x)\)</span> increases, <span class="math inline">\(1 - P_\theta(y_w|x)\)</span> decreases (towards 0) and the fraction <span class="math inline">\(\frac{\nabla_\theta\log P_\theta(y_w|x)}{1-P_\theta(y_w|x)}\)</span> increases (i.e.&nbsp;“this accelerates the model’s adaptation toward the distribution of chosen responses as the likelihood increases.”).</p>
<p>Conversely, as <span class="math inline">\(P_\theta(y_l|x)\)</span> increases (the likelihood of the model generating rejected responses), <span class="math inline">\(1 - P_\theta(y_l|x)\)</span> decreases (towards 0) and the fraction <span class="math inline">\(\frac{\nabla_\theta\log P_\theta(y_l|x)}{1-P_\theta(y_l|x)}\)</span> increases. <span class="math inline">\(h(d)\)</span> gets smaller, slowing the parameter updates since the model is more likely to generate the rejected responses.</p>
<p>My takeaway from the explanation of <span class="math inline">\(\delta(d)\)</span> and <span class="math inline">\(h(d)\)</span> is that they are opposing forces that keep each other in check. If the model is more likely to generate rejected responses, <span class="math inline">\(\delta(d)\)</span> accelerates the parameter updates (i.e.&nbsp;increases the gradient) and <span class="math inline">\(h(d)\)</span> slows down the parameter updates (decreases the gradient). If the model is more likely to generate favored responses, <span class="math inline">\(\delta(d)\)</span> slows down the parameter update (decreases the gradient) and <span class="math inline">\(h(d)\)</span> accelerates the parameter update (increases the gradient). The intuition (I think) is that if either term gets too large or too small, the other term counters it so you don’t have vanishing or exploding gradients.</p>
</section>
</section>
<section id="section-7.1-comparison-to-probability-ratio" class="level2">
<h2 class="anchored" data-anchor-id="section-7.1-comparison-to-probability-ratio">Section 7.1: Comparison to Probability Ratio</h2>
<p>The following equations are slightly modified equations from the paper, defining their experiment comparing probability ratio (PR) to odds ratio (OR):</p>
<p><span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are sampled from a uniform probability distribution.</p>
<p><span class="math display">\[X_1, X_2 \sim \text{Unif}(0,1)\]</span></p>
<p><span class="math inline">\(Y_{PR}\)</span> is the log probability ratio:</p>
<p><span class="math display">\[Y_{PR} \sim \beta(\log\frac{X_1}{X_2}) = \beta(\log X_1 - \log X_2)\]</span></p>
<p><span class="math inline">\(Y_{OR}\)</span> is the log odds ratio:</p>
<p><span class="math display">\[Y_{OR} = \log(\frac{\textbf{odds}(X_1)}{\textbf{odds}(X_2)}) = \log\big(\frac{X_1}{1-X_1}\big) - \log\big(\frac{X_2}{1-X_2}\big)\]</span></p>
<p>I used Claude 3’s help in explaining the above equations and translating them to code so I could re-run the experiment in the paper:</p>
<div class="cell" data-outputid="0c9b5d4d-a523-4674-9382-46a9a2dbdb42">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample x1 and x2 from Uniform(0, 1)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50000</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50000</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability Ratio</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Set the value of the proportionality constant</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y_pr <span class="op">=</span> beta <span class="op">*</span> (np.log(x1) <span class="op">-</span> np.log(x2))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Odds Ratio</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>y_or <span class="op">=</span> np.log(x1 <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> x1)) <span class="op">-</span> np.log(x2 <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> x2))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># histogram bins</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.hist(y_pr, bins, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'PR'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.hist(y_or, bins, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'OR'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="index_files/figure-html/cell-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="index_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="trl-library-implementation" class="level2">
<h2 class="anchored" data-anchor-id="trl-library-implementation">TRL Library Implementation</h2>
<p><span class="math display">\[\mathcal{L}_{OR} = - \log \sigma\big(\log \frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)} \big)\]</span></p>
<p>Given that:</p>
<p><span class="math display">\[\textbf{odds}_\theta(y|x) = \frac{P_\theta(y|x)}{1-P_\theta(y|x)}\]</span></p>
<p>Plugging in the odds function:</p>
<p><span class="math display">\[\mathcal{L}_{OR} = - \log \sigma\big(\log \frac{\textbf{odds}_\theta(y_w|x)}{\textbf{odds}_\theta(y_l|x)} \big) = -\log\sigma(\log(\frac{\frac{P_\theta(y_w|x)}{1-P_\theta(y_w|x)}}{\frac{P_\theta(y_l|x)}{1-P_\theta(y_l|x)}}))\]</span></p>
<p>Using log property of division:</p>
<p><span class="math display">\[-\log\sigma(\log(\frac{\frac{P_\theta(y_w|x)}{1-P_\theta(y_w|x)}}{\frac{P_\theta(y_l|x)}{1-P_\theta(y_l|x)}})) = -\log\sigma(\log(\frac{P_\theta(y_w|x)}{1-P_\theta(y_w|x)}) - \log({\frac{P_\theta(y_l|x)}{1-P_\theta(y_l|x)}}))\]</span></p>
<p>Using log property of division again:</p>
<p><br></p>
<p><span class="math display">\[= -\log\sigma(\log P_\theta(y_w|x)- \log(1-P_\theta(y_w|x)) - \log P_\theta(y_l|x) + \log (1-P_\theta(y_l|x)))\]</span></p>
<p>Rewriting so it’s cleaner:</p>
<p><br></p>
<p><span class="math display">\[-\log\sigma\big(\log P_\theta(y_w|x)- \log P_\theta(y_l|x) - \big[\log(1-P_\theta(y_w|x))  - \log(1-P_\theta(y_l|x))\big]\big)\]</span></p>
<p>In the code below the components of this line:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>log_odds <span class="op">=</span> (policy_chosen_logps <span class="op">-</span> policy_rejected_logps) <span class="op">-</span> (</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>            torch.log1p(<span class="op">-</span>torch.exp(policy_chosen_logps)) <span class="op">-</span> torch.log1p(<span class="op">-</span>torch.exp(policy_rejected_logps))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>correspond to the following math as follows:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>policy_chosen_logps <span class="op">-</span> policy_rejected_logps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[\log P_\theta(y_w|x)- \log P_\theta(y_l|x)\]</span></p>
<p><br> <br></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>torch.log1p(<span class="op">-</span>torch.exp(policy_chosen_logps)) <span class="op">-</span> torch.log1p(<span class="op">-</span>torch.exp(policy_rejected_logps))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[\big[\log(1-P_\theta(y_w|x)) - \log(1-P_\theta(y_l|x))\big]\]</span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> odds_ratio_loss(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        policy_chosen_logps: torch.FloatTensor,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        policy_rejected_logps: torch.FloatTensor,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute ORPO's odds ratio (OR) loss for a batch of policy and reference model log probabilities.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">            The losses tensor contains the ORPO loss for each example in the batch.</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">            The log odds ratio of the chosen responses over the rejected responses ratio for logging purposes.</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">            The `log(sigmoid(log_odds_chosen))` for logging purposes.</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Derived from Eqs. (4) and (7) from https://arxiv.org/abs/2403.07691 by using log identities and exp(log(P(y|x)) = P(y|x)</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        log_odds <span class="op">=</span> (policy_chosen_logps <span class="op">-</span> policy_rejected_logps) <span class="op">-</span> (</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            torch.log1p(<span class="op">-</span>torch.exp(policy_chosen_logps)) <span class="op">-</span> torch.log1p(<span class="op">-</span>torch.exp(policy_rejected_logps))</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        sig_ratio <span class="op">=</span> F.sigmoid(log_odds)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        ratio <span class="op">=</span> torch.log(sig_ratio)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> <span class="va">self</span>.beta <span class="op">*</span> ratio</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        chosen_rewards <span class="op">=</span> <span class="va">self</span>.beta <span class="op">*</span> (policy_chosen_logps.to(<span class="va">self</span>.accelerator.device)).detach()</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        rejected_rewards <span class="op">=</span> <span class="va">self</span>.beta <span class="op">*</span> (policy_rejected_logps.to(<span class="va">self</span>.accelerator.device)).detach()</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> losses, chosen_rewards, rejected_rewards, torch.mean(ratio).item(), torch.mean(log_odds).item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","loop":true,"openEffect":"zoom","closeEffect":"zoom","descPosition":"bottom"});</script>



</body></html>