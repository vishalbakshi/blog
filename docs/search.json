[
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "",
    "text": "Here’s the video walkthrough of this notebook\nIn this notebook, I’ll walkthrough the code and concepts introduced in Chapter 8 of the fastai textbook. This chapter explores the various ways fastai can handle a collaborative filtering problem."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "What is Collaborative Filtering?",
    "text": "What is Collaborative Filtering?\nIn a situation where two variables have some numeric relationship, such as users rating movies, collaborative filtering is a solution for predicting ratings that are blank, based on existing data.\nA machine learning model for collaborative filtering implicitly learns the answers to the following questions:\n\nWhat types of movies do users like?\nWhat are characteristics of each movie?\n\n\nLatent Factors\nFor the movie rating example, latent factors are the “types of movies” users like and “characteristics” of each movie. Latent factors are not explicitly categorical, they are numeric values, but they represent the implicit categories of each variable.\nThe reason that they are implicit categories, is that the model learns the ideal latent factors as it trains on the dataset, observing patterns between users and their movie ratings."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "MovieLens 100K Dataset",
    "text": "MovieLens 100K Dataset\nThe dataset used to train the collaborative filtering model is a subset (100,000 rows in length) of the full MovieLens dataset which is 25 million rows.\n\n# a first look at the data\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\n\n\nThe dataset lists users, movies, ratings and a timestamp.\n\n# load the data into a DataFrame\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nBefore we get into the training, I want to familiarize myself with how the data is structured. There are 943 unique users and 1682 unique movies.\n\n# how many unique users and movies are there?\nlen(ratings['user'].unique()), len(ratings['movie'].unique())\n\n(943, 1682)\n\n\nThe movie IDs are a consecutive range from 1 to 1682 and the user IDs are a consecutive range from 1 to 943. The movie ratings range from 1 to 5.\n\n# are movie IDs consecutive?\n(ratings['movie'].sort_values().unique() == np.array(range(1,1683))).sum()\n\n1682\n\n\n\n# are user IDs consecutive?\n(ratings['user'].sort_values().unique() == np.array(range(1,944))).sum()\n\n943\n\n\n\n# what is the range of ratings?\nratings['rating'].min(), ratings['rating'].max()\n\n(1, 5)\n\n\nTo visualize the problem we are trying to solve with collaborative filtering the book recommended that we observe a cross-tabulation of the data because then we can see that what we are trying to predict are the null values between user and movie, and what we are training our model on are the non-null ratings in the dataset.\nThe model will learn something about user 2 and movie 2 in order to predict what rating that user would give that movie. That “something” the model will learn are the latent factors for users and latent factors for movies.\n\n# view crosstab of users and movies with rating values\nct = pd.crosstab(ratings['user'], ratings['movie'], ratings['rating'],aggfunc='mean')\nct\n\n\n\n\n\n  \n    \n      movie\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      ...\n      1643\n      1644\n      1645\n      1646\n      1647\n      1648\n      1649\n      1650\n      1651\n      1652\n      1653\n      1654\n      1655\n      1656\n      1657\n      1658\n      1659\n      1660\n      1661\n      1662\n      1663\n      1664\n      1665\n      1666\n      1667\n      1668\n      1669\n      1670\n      1671\n      1672\n      1673\n      1674\n      1675\n      1676\n      1677\n      1678\n      1679\n      1680\n      1681\n      1682\n    \n    \n      user\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      5.0\n      3.0\n      4.0\n      3.0\n      3.0\n      5.0\n      4.0\n      1.0\n      5.0\n      3.0\n      2.0\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n      3.0\n      4.0\n      5.0\n      4.0\n      1.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      2.0\n      4.0\n      1.0\n      3.0\n      3.0\n      5.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      3.0\n      4.0\n      3.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      939\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      940\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      5.0\n      3.0\n      NaN\n      NaN\n      4.0\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      941\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      942\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      943\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      4.0\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n943 rows × 1682 columns\n\n\n\nInstead of movie IDs, we can get the movie titles into a DataFrame and add that column to our ratings DataFrame by merging the two.\nThe movie titles are in the u.item file, which is pipe-delimited, with latin-1 encoding. The u.item file has 24 columns, but we only want the first two which have the movie id and the title.\n\n# get movie titles\nmovies = pd.read_csv(\n    path/'u.item', \n    delimiter='|', \n    encoding='latin-1', \n    usecols=(0,1), \n    names=('movie', 'title'), \n    header=None)\n\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nThe movies DataFrame and the ratings DataFrame are merged using the movie column as the key to match title in movies to movie ID in ratings. By default, pandas uses as the key whichever column name exists in both DataFrames.\n\n# get the user ratings by title\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n\n\nDoes this change the uniqueness of the data? Yes it actually does! There are 1682 unique movie IDs but there are only 1664 unique movie titles. 18 movies have associated with it duplicate titles.\n\n# how many unique titles and movies are there?\nlen(ratings['title'].unique()), len(ratings['movie'].unique())\n\n(1664, 1682)\n\n\nThe .duplicated DataFrame method takes a list of columns for the subset parameter, finds values in those columns that are duplicated, and returns a boolean Series with a True value at indexes with duplicates. I use that as a mask and pass it to the movies DataFrame to view those duplicate titles.\n\n# 18 movies have duplicate titles\nmovies[movies.duplicated(subset=['title'])]\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      267\n      268\n      Chasing Amy (1997)\n    \n    \n      302\n      303\n      Ulee's Gold (1997)\n    \n    \n      347\n      348\n      Desperate Measures (1998)\n    \n    \n      499\n      500\n      Fly Away Home (1996)\n    \n    \n      669\n      670\n      Body Snatchers (1993)\n    \n    \n      679\n      680\n      Kull the Conqueror (1997)\n    \n    \n      864\n      865\n      Ice Storm, The (1997)\n    \n    \n      880\n      881\n      Money Talks (1997)\n    \n    \n      1002\n      1003\n      That Darn Cat! (1997)\n    \n    \n      1256\n      1257\n      Designated Mourner, The (1997)\n    \n    \n      1605\n      1606\n      Deceiver (1997)\n    \n    \n      1606\n      1607\n      Hurricane Streets (1998)\n    \n    \n      1616\n      1617\n      Hugo Pool (1997)\n    \n    \n      1624\n      1625\n      Nightwatch (1997)\n    \n    \n      1649\n      1650\n      Butcher Boy, The (1998)\n    \n    \n      1653\n      1654\n      Chairman of the Board (1998)\n    \n    \n      1657\n      1658\n      Substance of Fire, The (1996)\n    \n    \n      1679\n      1680\n      Sliding Doors (1998)\n    \n  \n\n\n\n\nfastai has a built-in constructor for DataLoaders specific to collaborative filtering. I pass it the ratings DataFrame, specify that the items are the titles, and that I want 64 rows in each batch.\n\n# create DataLoaders\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      297\n      Indian Summer (1996)\n      4\n    \n    \n      1\n      934\n      Grease (1978)\n      4\n    \n    \n      2\n      846\n      Money Train (1995)\n      2\n    \n    \n      3\n      479\n      Crumb (1994)\n      3\n    \n    \n      4\n      499\n      Local Hero (1983)\n      4\n    \n    \n      5\n      455\n      Adventures of Priscilla, Queen of the Desert, The (1994)\n      3\n    \n    \n      6\n      943\n      Rumble in the Bronx (1995)\n      4\n    \n    \n      7\n      374\n      Dead Poets Society (1989)\n      1\n    \n    \n      8\n      533\n      Deer Hunter, The (1978)\n      3\n    \n    \n      9\n      846\n      Vanya on 42nd Street (1994)\n      2\n    \n  \n\n\n\n\nMatrices for Latent Factors\nWe need the model to find relationships between users and movies. And we need to give the model something concrete and numeric to represent those relationships. We will give it latent factor matrices.\nIn this example, they have chosen to use 5 latent factors for movies and 5 latent factors for users. We represent these latent factors by creating a matrix of random values.\nThe user latent factors will have 944 rows, one for each user including a null user, and 5 columns, one for each latent factor. The movies latent factors will have 1665, one for each movie including a null movie, and 5 columns.\n\n# user and title classes contain '#na#'\nL(dls.classes['title']),L(dls.classes['user'])\n\n((#1665) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...],\n (#944) ['#na#',1,2,3,4,5,6,7,8,9...])\n\n\n\n# define dimensions for users and movies latent factor matrices\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\nn_users, n_movies, n_factors\n\n(944, 1665, 5)\n\n\n\n# build users and movies latent factor matrices\nuser_factors = torch.randn(n_users,n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\nuser_factors.shape, movie_factors.shape\n\n(torch.Size([944, 5]), torch.Size([1665, 5]))\n\n\n\n\nEmbeddings instead of One Hot Encoded Matrix Multiplication\nOkay so how do we use these latent factor matrices?\nFor each row in our batch, we have a user ID and a movie ID. We need to get the latent factors for each of those and calculate the dot product, in order to predict our rating. As it adjusts the latent factors during the training loop, the predictions will get better.\nI’ll grab one batch from the dls DataLoaders object and illustrate an example prediction calculation. Each independent variable, x, is a tensor with [user, movie]. Each dependent variable, y, is a tensor with [rating].\n\nx,y = dls.one_batch()\nx.shape, y.shape\n\n(torch.Size([64, 2]), torch.Size([64, 1]))\n\n\n\nx[0], y[0]\n\n(tensor([466, 614]), tensor([3], dtype=torch.int8))\n\n\n\ntype(x[0]), type(y[0])\n\n(torch.Tensor, torch.Tensor)\n\n\nI determined the order of the x values by looking at the maximum value in each column, specifying axis=0. The movie IDs go up to 1644, so a max value of 1608 means that the movie is the second value in each tensor.\n\nx.max(axis=0)\n\ntorch.return_types.max(values=tensor([ 935, 1642]), indices=tensor([24,  3]))\n\n\nI get the latent factors for the user and movie in the first batch item.\n\nu = user_factors[x[0][0]]\nu\n\ntensor([-0.6595, -0.3355,  1.0491,  1.1764,  0.8750])\n\n\n\nm = movie_factors[x[0][1]]\nm\n\ntensor([-0.1751, -0.5016,  0.6298,  0.2370, -0.7902])\n\n\nI calculate the dot product of the two vectors, which is the sum of the element-wise product.\n\npred = (u * m).sum()\npred\n\ntensor(0.5320)\n\n\nI pass it through sigmoid_range to get a value between 0 and 5. Sigmoid outputs a value between 0 and 1, and sigmoid_range scales and shifts that function to fit the specified range. The output is the prediction for the rating that this user would give this movie.\n\npred = sigmoid_range(pred, 0, 5)\npred\n\ntensor(3.1497)\n\n\nSince the prediction and the target are a single numeric value, we’ll use Mean Squared Error loss. For a single value, the loss is the squared error. For a batch, the mean would be calculated.\n\nloss = (pred - y[0].item()) ** 2\nloss\n\ntensor(0.0224)\n\n\n\n\nBuilding a Collaborative Filtering Model from Scratch\nI’ll create a DotProduct class which builds an Embedding to store latent factor matrices for users and movies, and calculates the prediction in its forward method using the dot product of the user and movie latent factors.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\nI’ll illustrate how this model operates by coding through the calculation of predictions for one batch.\nI create Embeddings for users and movies. Their shape corresponds to the number of users and movies and the number of latent factors.\n\nuser_factors = Embedding(n_users, n_factors)\nmovie_factors = Embedding(n_movies, n_factors)\nuser_factors, movie_factors\n\n(Embedding(944, 5), Embedding(1665, 5))\n\n\nThe weight attribute holds the latent factor values, which are parameters whose gradient can be calculated. The values are from a normal distribution with mean 0 and variance 1.\n\nuser_factors.weight.shape\n\ntorch.Size([944, 5])\n\n\n\nuser_factors.weight\n\nParameter containing:\ntensor([[-0.0066, -0.0111,  0.0091,  0.0056,  0.0075],\n        [-0.0138, -0.0014,  0.0189,  0.0028, -0.0166],\n        [ 0.0149,  0.0053, -0.0153,  0.0078, -0.0119],\n        ...,\n        [-0.0051, -0.0117,  0.0170, -0.0102, -0.0044],\n        [ 0.0037, -0.0084,  0.0042, -0.0049,  0.0186],\n        [ 0.0199, -0.0194,  0.0044,  0.0012,  0.0084]], requires_grad=True)\n\n\nThe user_factors weight has the shape 944 rows by 5 columns. You can see that they are a tensor with requires_grad equals True.\n\nmovie_factors.weight\n\nParameter containing:\ntensor([[-0.0017, -0.0051,  0.0065,  0.0050, -0.0095],\n        [-0.0065, -0.0158,  0.0062, -0.0145, -0.0087],\n        [ 0.0067,  0.0111,  0.0059, -0.0003,  0.0061],\n        ...,\n        [-0.0012,  0.0002, -0.0088, -0.0022, -0.0152],\n        [-0.0053, -0.0058, -0.0074, -0.0033, -0.0171],\n        [ 0.0030,  0.0031, -0.0037, -0.0023,  0.0157]], requires_grad=True)\n\n\nThe movie_factors weight has the shape 1665 rows by 5 columns. And here you can see it is a tensor as well with requires_grad equals True.\n\nmovie_factors.weight.shape\n\ntorch.Size([1665, 5])\n\n\nIn my batch, the 0th column of the dependent variable x holds user indexes. I pass that to user_factors and receive a tensor with those users’ latent factors. Column index 1 holds the movie indexes, I pass that to movie_factors and receive a tensor with those movies’ latent factors.\n\nusers = user_factors(x[:,0])\nusers[:5]\n\ntensor([[ 0.0029,  0.0042, -0.0093,  0.0023, -0.0053],\n        [ 0.0029,  0.0008,  0.0193,  0.0082,  0.0117],\n        [-0.0025,  0.0070, -0.0144, -0.0193,  0.0086],\n        [ 0.0103,  0.0028,  0.0172,  0.0110,  0.0084],\n        [-0.0087, -0.0109,  0.0062, -0.0018, -0.0012]],\n       grad_fn=<SliceBackward>)\n\n\n\nmovies = movie_factors(x[:,1])\nmovies[:5]\n\ntensor([[ 0.0011, -0.0009,  0.0114,  0.0017,  0.0033],\n        [ 0.0049, -0.0019,  0.0175,  0.0027, -0.0014],\n        [-0.0047, -0.0026,  0.0032,  0.0028, -0.0146],\n        [-0.0103, -0.0024,  0.0057, -0.0141, -0.0080],\n        [ 0.0099,  0.0113,  0.0022,  0.0123,  0.0096]],\n       grad_fn=<SliceBackward>)\n\n\nI take the dot product and pass it through a sigmoid_range and the calculate the predictions for the batch.\n\npreds = sigmoid_range((users * movies).sum(dim=1), 0, 5.5)\npreds, preds.shape\n\n(tensor([2.7498, 2.7505, 2.7497, 2.7497, 2.7497, 2.7501, 2.7495, 2.7506, 2.7499,\n         2.7502, 2.7503, 2.7506, 2.7501, 2.7498, 2.7497, 2.7507, 2.7498, 2.7497,\n         2.7499, 2.7500, 2.7499, 2.7500, 2.7502, 2.7501, 2.7502, 2.7499, 2.7500,\n         2.7499, 2.7499, 2.7501, 2.7503, 2.7497, 2.7500, 2.7498, 2.7497, 2.7496,\n         2.7502, 2.7502, 2.7501, 2.7498, 2.7501, 2.7502, 2.7500, 2.7501, 2.7506,\n         2.7500, 2.7498, 2.7499, 2.7501, 2.7502, 2.7502, 2.7501, 2.7498, 2.7501,\n         2.7501, 2.7499, 2.7499, 2.7499, 2.7498, 2.7502, 2.7499, 2.7498, 2.7494,\n         2.7499], grad_fn=<AddBackward0>), torch.Size([64]))\n\n\nThat’s what the DotProduct model will return. I can then take the mean squared error and calculate the loss value, which I can then call backward on, to calculate the gradients for the latent factors. I can then multiply them by the learning rate, and add them to the weights, and repeat the training loop.\n\nloss = ((preds - y) ** 2).mean()\nloss\n\ntensor(2.0156, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\n\n\nuser_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\nmovie_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nWe see here that the gradients are small but not all of them are zero:\n\nuser_factors.weight.grad.sum(), movie_factors.weight.grad.sum()\n\n(tensor(-0.0108), tensor(-0.0026))\n\n\n\nTraining the Model\nThere are five different models that I will build, from simple to complex.\nModel 1 will not use sigmoid_range for the dot product prediction calculation. Instead, I will get a value that is normally distributed with a mean of zero and a variance of 1. Model 2 will pass the predictions through sigmoid_range to get an output between 0 and 5.5. Model 3 will add a bias parameter to the dot product prediction, so that we can establish some baseline rating of each movie that’s independent of a particular latent factor. Model 4 will introduce weight decay in order to better generalize our model, and in Model 5 I’ll implement a custom class instead of using the built-in PyTorch Embedding class.\n\n\n\nModel\nsigmoid_range\nBias\nWeight Decay\nCustom Embedding\n\n\n\n\n1\nN\nN\nN\nN\n\n\n2\nY\nN\nN\nN\n\n\n3\nY\nY\nN\nN\n\n\n4\nY\nY\nY\nN\n\n\n5\nY\nY\nY\nY\n\n\n\n\nModel 1: No sigmoid_range\nThe DotProduct class will initialize an Embedding for users and movies with random values from a normal distribution with mean 0 and variance 1. In the forward method, called during the prediction step of the training loop, the latent factors for the batch’s users and movies are accessed from the corresponding Embedding and the dot product is calculated and returned as the prediction.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return (users * movies).sum(dim=1)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.339355\n      1.274582\n      00:06\n    \n    \n      1\n      1.039260\n      1.061794\n      00:06\n    \n    \n      2\n      0.945793\n      0.954731\n      00:06\n    \n    \n      3\n      0.826578\n      0.871694\n      00:06\n    \n    \n      4\n      0.752750\n      0.858231\n      00:06\n    \n  \n\n\n\nThe average prediction error on the validation ratings is the square root of the final validation loss.\n\nmath.sqrt(learn.recorder.final_record[1])\n\n0.9264077107145671\n\n\nHere are some predictions on the validation set ratings:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      554\n      37\n      4\n      4.181926\n    \n    \n      1\n      561\n      811\n      4\n      3.008064\n    \n    \n      2\n      503\n      298\n      5\n      4.451642\n    \n    \n      3\n      380\n      581\n      4\n      3.474877\n    \n    \n      4\n      666\n      422\n      4\n      4.054492\n    \n    \n      5\n      444\n      933\n      2\n      3.940120\n    \n    \n      6\n      368\n      1612\n      3\n      2.864129\n    \n    \n      7\n      537\n      457\n      3\n      2.955165\n    \n    \n      8\n      224\n      1535\n      1\n      2.940819\n    \n  \n\n\n\n\n\nModel 2 - with sigmoid_range\nIn this model, I’ll force the predictions to fall within the range of actual ratings. The book recommends, based on what they’ve experienced, using a maximum rating for predictions that is slightly larger than the maximum ground truth rating. The range of predictions we’ll use is 0 to 5.5.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.009657\n      0.977510\n      00:07\n    \n    \n      1\n      0.881551\n      0.891973\n      00:06\n    \n    \n      2\n      0.686247\n      0.853506\n      00:07\n    \n    \n      3\n      0.487054\n      0.857519\n      00:06\n    \n    \n      4\n      0.374934\n      0.862651\n      00:06\n    \n  \n\n\n\n\nmath.sqrt(learn.recorder.final_record[1])\n\n\nlearn.show_results()\n\nThe valid loss starts off lower, but by the end of the training, I don’t see an improvement. In fact, the valid loss starts to increase at the end. This is an indication that overfitting is taking place. This will be addressed in Model 4.\n\n\nModel 3 - Add bias\nBut first, let’s look at Model 3, which adds a bias parameter to the predictions, which provides a baseline rating for each movie independent of the weights related to the different latent factors.\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\n\nIn addition to initializing new Embeddings for bias, the dot product is first kept in matrix form by passing keepdim=True to the sum method. The biases are then added on afterwhich the result is passed through sigmoid_range. Here’s a illustrative example for how keepdim=True affects the dot product:\n\na = Tensor([[1,2,3], [4,5,6]])\n(a * a).sum(dim=1).shape, (a * a).sum(dim=1, keepdim=True).shape\n\n(torch.Size([2]), torch.Size([2, 1]))\n\n\nThe first tensor is a 2-vector, whereas the second tensor is a 2 x 1 matrix.\n\nLet’s train Model 3 and view the results!\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.952214\n      0.912603\n      00:07\n    \n    \n      1\n      0.825348\n      0.845193\n      00:07\n    \n    \n      2\n      0.618910\n      0.852381\n      00:07\n    \n    \n      3\n      0.406514\n      0.875637\n      00:07\n    \n    \n      4\n      0.293729\n      0.882840\n      00:07\n    \n  \n\n\n\nThe initial validation loss is lower than the previous trainings, but Model 3 is overfitting even more than Model 2. It’s time to introduce weight decay.\n\n\nModel 4 - Use Weight Decay\nSmaller weights lead to a smoother function which corresponds to fewer inflection points, leading to a better generalization of the model. Larger weights lead to a sharper function, corresponding to more inflection points which overfit the training data.\nThe text uses the basic example of a parabola to illustrate. As the weight a increases, the function becomes narrower, with a sharper trough.\n\nHTML('<iframe src=\"https://www.desmos.com/calculator/uog6rvyubg\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>')\n\n\n\n\nWe will intentionally increase the gradients so that the weights are stepped with larger increments toward a smaller value.\nThis corresponds to an intentionally larger loss value calculated for each batch:\nloss_with_wd = loss + wd * (parameters**2).sum()\nThe parameters are squared to ensure a positive value. Taking the derivative of the loss function means taking the derivative of the following function:\nwd * (parameters**2).sum()\nWhich results in:\n2 * wd * parameters\nInstead of multiplying by 2, we can just use twice the weight decay value as wd. I’ll use a weight decay of 0.1 as they do in the text.\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.939321\n      0.937423\n      00:07\n    \n    \n      1\n      0.851871\n      0.855584\n      00:07\n    \n    \n      2\n      0.720202\n      0.815807\n      00:07\n    \n    \n      3\n      0.630149\n      0.806268\n      00:07\n    \n    \n      4\n      0.491224\n      0.807063\n      00:07\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      877\n      1538\n      3\n      3.708797\n    \n    \n      1\n      601\n      1285\n      1\n      2.832736\n    \n    \n      2\n      292\n      1147\n      2\n      3.675529\n    \n    \n      3\n      132\n      400\n      4\n      3.504542\n    \n    \n      4\n      405\n      1614\n      1\n      1.930779\n    \n    \n      5\n      655\n      458\n      3\n      3.274209\n    \n    \n      6\n      453\n      60\n      4\n      3.864617\n    \n    \n      7\n      629\n      1498\n      4\n      3.598821\n    \n    \n      8\n      724\n      580\n      4\n      3.921505\n    \n  \n\n\n\n\n\nModel 5 - custom Embedding class\nThe final model I’ll train does not have a fundamentally different component than the other four. Instead of using the built-in Embedding PyTorch class, the text has us write our own class.\nOptimizers get a module’s parameters by calling the parameters method. We have to wrap parameters in nn.Parameter for them to be recognized as such. This class also calls requires_grad_ for us.\nI’ll replace each Embedding with a tensor filled with random values from a normal distribution with mean 0 and variance 0.01.\n\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0,0.01))\n\n\ntorch.zeros(3,4)\n\ntensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n\n\ntorch.zeros(3,4).normal_(0,0.01)\n\ntensor([[-0.0134,  0.0098, -0.0124, -0.0032],\n        [ 0.0056,  0.0071,  0.0005,  0.0014],\n        [-0.0236, -0.0024, -0.0060,  0.0017]])\n\n\nI redefine the DotProductBias model using the create_params method instead of Embedding, and train the model.italicized text\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = create_params([n_users, n_factors])\n    self.user_bias = create_params([n_users])\n    self.movie_factors = create_params([n_movies, n_factors])\n    self.movie_bias = create_params([n_movies])\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors[x[:,0]]\n    movies = self.movie_factors[x[:,1]]\n    res = (users * movies).sum(dim=1)\n    res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.934845\n      0.933218\n      00:08\n    \n    \n      1\n      0.841111\n      0.859618\n      00:08\n    \n    \n      2\n      0.730065\n      0.820388\n      00:08\n    \n    \n      3\n      0.599684\n      0.807086\n      00:08\n    \n    \n      4\n      0.484760\n      0.807552\n      00:08\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      487\n      6\n      3\n      2.994869\n    \n    \n      1\n      54\n      349\n      3\n      2.511689\n    \n    \n      2\n      501\n      1252\n      4\n      4.130728\n    \n    \n      3\n      244\n      861\n      4\n      2.314526\n    \n    \n      4\n      322\n      1501\n      5\n      3.823119\n    \n    \n      5\n      537\n      1294\n      2\n      3.124064\n    \n    \n      6\n      193\n      1530\n      3\n      2.546681\n    \n    \n      7\n      581\n      286\n      5\n      3.062707\n    \n    \n      8\n      450\n      154\n      4\n      4.161049\n    \n  \n\n\n\nI get similar results as before!\n\n\n\nInterpreting Embeddings and Biases\nI’ll save this model so that the embedding and bias analyses I perform can be recreated.\n\nlearn = load_learner(\"/content/gdrive/MyDrive/fastai-course-v4/dot_product_bias.pkl\")\nmodel = learn.model\n\nBias represents a baseline rating of a movie regardless of how well the latent factors of the movie match the latent factors of the user. Low bias values correspond to movies that people didn’t enjoy, even if it matched their preferences.\nWhat were the 5 generally least liked movies?\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nmovie_bias = model.movie_bias\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Robocop 3 (1993)',\n 'Showgirls (1995)',\n 'Kansas City (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)']\n\n\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'L.A. Confidential (1997)',\n 'As Good As It Gets (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Good Will Hunting (1997)']\n\n\n\nVisualizing Embeddings\nThe embeddings are a 50-dimensional matrix of latent factors. I’ll use Principal Component Analysis (PCA) to extract the two most descriptive dimensions and plot the latent factor values. I’ll also calculate the distance from 0 of each movie so that I can filter for outliers in order to reduce the number of data points on the plot, and help me understand what these latent factors may be.\n\n!pip install fbpca\nimport fbpca\n\nCollecting fbpca\n  Downloading https://files.pythonhosted.org/packages/a7/a5/2085d0645a4bb4f0b606251b0b7466c61326e4a471d445c1c3761a2d07bc/fbpca-1.0.tar.gz\nBuilding wheels for collected packages: fbpca\n  Building wheel for fbpca (setup.py) ... done\n  Created wheel for fbpca: filename=fbpca-1.0-cp37-none-any.whl size=11376 sha256=719b80446eeb8f157c99e298adb61b0978c0ae279ade82e500dbe37902c447e4\n  Stored in directory: /root/.cache/pip/wheels/53/a2/dd/9b66cf53dbc58cec1e613d216689e5fa946d3e7805c30f60dc\nSuccessfully built fbpca\nInstalling collected packages: fbpca\nSuccessfully installed fbpca-1.0\n\n\nI grab the movie_factors from the trained model, bring it over the the .cpu(), .detach() it from the gradients and convert it to a .numpy() array.\n\nmovie_embeddings = model.movie_factors.cpu().detach().numpy()\n\nI pass those embeddings to the fbpca.pca method and get back the rank-2 approximation.\n\nU, s, Va = fbpca.pca(movie_embeddings, k=2)\n\nI then create a DataFrame from the U matrix which is an m x k (1665 movies x 2 components) matrix. I also create a column with the calculated distance from 0 of each movie, based on the 2-component coordinates, and a column specifying which quadrant the movie is in (First, Second, Third or Fourth).\nMy distance function receives each DataFrame row, and returns the square root of the sum of squares of the two coordinates.\nMy quadrant function received each row and based on the sign of the x or 0 column and the y or 1 column, determines which quadrant that movie lies in.\nI apply both functions to the DataFrame and specify axis=1 so that I can access the column names.\n\n# helper functions\ndef distance(row):\n  return np.sqrt(row[0]**2 + row[1]**2)\n\ndef quadrant(row):\n  if (row[0] > 0 and row[1] > 0):\n    return \"First Quadrant\"\n  elif (row[0] < 0 and row[1] > 0):\n    return \"Second Quadrant\"\n  elif (row[0] < 0 and row[1] < 0):\n    return \"Third Quadrant\"\n  elif (row[0] > 0 and row[1] < 0):\n    return \"Fourth Quadrant\"\n  else:\n    return \"Center\"\n\n\n# create DataFrame from PCA output\ndef pca_to_df(U):\n  df = pd.DataFrame(data=U)\n\n  # calculate the distance of each Embedding from 0\n  df[2] = df.apply(lambda x: np.sqrt(x[0]**2 + x[1]**2), axis=1)\n\n  # identify which quadrant the movie is in\n  df[3] = df.apply(lambda x: quadrant(x), axis=1)\n\n  return df\n\nI’ll import the DataFrame I created from my original PCA output so that I can recreate the corresponding plots.\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movie_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.010150\n      0.004517\n      0.011110\n      First Quadrant\n    \n    \n      1\n      0.025090\n      -0.000186\n      0.025091\n      Fourth Quadrant\n    \n    \n      2\n      -0.005773\n      0.025443\n      0.026090\n      Second Quadrant\n    \n    \n      3\n      0.015933\n      -0.021972\n      0.027141\n      Fourth Quadrant\n    \n    \n      4\n      -0.056279\n      -0.013351\n      0.057841\n      Third Quadrant\n    \n  \n\n\n\n\nWhen I plot the first two columns as x,y coordinates, I can see that the movies are spread out quite evenly across those latent factors.\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06691494d0>\n\n\n\n\n\nI’m going to plot the farthest points from the origin and label them on the plot in order to get a sense of what these two latent factors may represent.\nFor each quadrant, I grab the indexes for the rows with the 5 largest distances and create a DataFrame from that and plot that data.\n\ndef plot_top_5(df):\n  # get the 5 points farthest from 0 from each quadrant\n  idxs = np.array([])\n  for quad in df['3'].unique():\n    idxs = np.append(idxs, df[df['3']==quad]['2'].sort_values(ascending=False).index[:5].values)\n  plot_df = df.loc[idxs]\n\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_top_5(df)\n\n\n\n\nThe first quadrant seems to represent comedies (Ed, Ready to Wear, The Stupids) although Barb Wire is not a comedy.\nThe second quadrant has movies with some more dark and disturbing elements to it.\nThe third quadrant has movies which are drama and I guess share some theme of gaining freedom?\nFinally, the fourth quadrant seems to have drama and action movies which have a distinct American storyline to them. I haven’t seen all of these movies, but Dirty Dancing and The American President seem like anomalies in this group.\nI should also take a look at other movies that do not fall on the extreme ends of each quadrant. For example, which movies fall close to the vertical and horizontal axes?\n\ndef plot_close_to_axes(df):\n  # get 5 points closes to each axis \n  idxs = np.array([])\n  for key in ['0', '1']:\n    idxs = np.append(idxs, df[np.abs(df[key]) < 0.0002].index.values)\n  plot_df = df.loc[idxs]\n  plot_df = plot_df.drop_duplicates()\n\n  # set figure size\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n\n  # annotate with movie titles\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_close_to_axes(df)\n\n\n\n\nThe latent factor corresponding to the vertical axis seems to represent drama (positive values) and romance (negative values) whereas the horizontal axis represents elements mystery (negative values) and comedy (positive values). However, I could just be focusing on genre whereas there are other features of a movie these may represent. Unfortunately, I’m not a movie buff, so the need for a domain expert is evident here!\n\n\nRobust PCA\nAlthough it’s out of scope for this chapter and my understanding, I’d like to at least experiment with using Robust PCA for visualizing embeddings. Robust PCA is an algorithm which decomposes a matrix M into two components: a low rank matrix L and a sparse matrix S such that M = L + S. From what I understand, the sparse matrix S contains anomalies or outliers or “corrupted” data, whereas L contains a more accurate representation of the original data. For example, if an image has some additional noise added to it, the original image matrix M can be decomposed into a noise-less “clean” image L and a sparse noise matrix S. Another example is if you have an image with a background (such as a landscape with lawn, sidewalks, buildings) and a foreground (people walking on the sidewalk) passing that image through the RPCA algorithm would yield a background matrix L with the lawn, sidewalk and buildings and a foreground sparse matrix S with the people. Since my movie_embeddings matrix may contain anomalies which would affect the effectiveness and accuracy of my 2-component PCA approximation, I will pass it through a RPCA algorithm and calculate the 2-component approximation on the low-rank L and sparse S matrices and compare the results with what I calculated above.\nThe following algorithm is from Rachel Thomas’ lesson on RPCA as part of her Computational Linear Algebra course.\n\nfrom scipy import sparse\nfrom sklearn.utils.extmath import randomized_svd\nimport fbpca\n\n\nTOL=1e-9\nMAX_ITERS=3\n\n\ndef converged(Z, d_norm):\n    err = np.linalg.norm(Z, 'fro') / d_norm\n    print('error: ', err)\n    return err < TOL\n\n\ndef shrink(M, tau):\n    S = np.abs(M) - tau\n    return np.sign(M) * np.where(S>0, S, 0)\n\n\ndef _svd(M, rank): return fbpca.pca(M, k=min(rank, np.min(M.shape)), raw=True)\n\n\ndef norm_op(M): return _svd(M, 1)[1][0]\n\n\ndef svd_reconstruct(M, rank, min_sv):\n    u, s, v = _svd(M, rank)\n    s -= min_sv\n    nnz = (s > 0).sum()\n    return u[:,:nnz] @ np.diag(s[:nnz]) @ v[:nnz], nnz\n\n\ndef pcp(X, maxiter=10, k=10): # refactored\n    m, n = X.shape\n    trans = m<n\n    if trans: X = X.T; m, n = X.shape\n        \n    lamda = 1/np.sqrt(m)\n    op_norm = norm_op(X)\n    Y = np.copy(X) / max(op_norm, np.linalg.norm( X, np.inf) / lamda)\n    mu = k*1.25/op_norm; mu_bar = mu * 1e7; rho = k * 1.5\n    \n    d_norm = np.linalg.norm(X, 'fro')\n    L = np.zeros_like(X); sv = 1\n    \n    examples = []\n    \n    for i in range(maxiter):\n        print(\"rank sv:\", sv)\n        X2 = X + Y/mu\n        \n        # update estimate of Sparse Matrix by \"shrinking/truncating\": original - low-rank\n        S = shrink(X2 - L, lamda/mu)\n        \n        # update estimate of Low-rank Matrix by doing truncated SVD of rank sv & reconstructing.\n        # count of singular values > 1/mu is returned as svp\n        L, svp = svd_reconstruct(X2 - S, sv, 1/mu)\n        \n        # If svp < sv, you are already calculating enough singular values.\n        # If not, add 20% (in this case 240) to sv\n        sv = svp + (1 if svp < sv else round(0.05*n))\n        \n        # residual\n        Z = X - L - S\n        Y += mu*Z; mu *= rho\n        \n        examples.extend([S[140,:], L[140,:]])\n        \n        if m > mu_bar: m = mu_bar\n        if converged(Z, d_norm): break\n    \n    if trans: L=L.T; S=S.T\n    return L, S, examples\n\n\nL, S, examples = pcp(movie_embeddings)\n\n\nL.shape, S.shape\n\nI’ll calculate 2-component PCA for the L and S matrices and plot those to see how they compare to the plots above.\n\nU_L, _, _ = fbpca.pca(L, k=2)\nU_S, _, _ = fbpca.pca(S, k=2)\n\nI exported the outputs to CSV for repeatability, so I’ll import them in again:\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_L_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.009254\n      -0.003454\n      0.009877\n      Fourth Quadrant\n    \n    \n      1\n      0.032954\n      0.008637\n      0.034067\n      First Quadrant\n    \n    \n      2\n      -0.014662\n      -0.047128\n      0.049356\n      Third Quadrant\n    \n    \n      3\n      0.012602\n      0.029182\n      0.031787\n      First Quadrant\n    \n    \n      4\n      -0.037841\n      0.008742\n      0.038838\n      Second Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06668a58d0>\n\n\n\n\n\nThe scatter plot for the 2-component PCA result seems much more evenly distributed across the quadrants.\nI take the 5 farthest movies from each quadrant and plot those separately.\n\nplot_top_5(df)\n\n\n\n\nHere are the patterns I observe. Again, someone who has watched these movies and is not just reading online descriptions of them would see themes and patterns that I would not.\n\n\n\nQuadrant\nObservation\n\n\n\n\n1\nRomance/Drama movies. Fausto and Castle Freak seem out of place\n\n\n2\nMore romance movies. Top Gun and Prefontaine seem out of place\n\n\n3\nMore romance movies. The Butcher Boy seems out of place.\n\n\n4\nComedies. The Carmen Miranda documentary seems out of place.\n\n\n\nAfter making these observations, either the low-rank matrix L is a poor choice to use for this type of analysis, or my understanding these movies is too shallow to see the deeper relationships between them. With so many romance movies across the plot, I don’t think these latent factors represent genres.\nI’m not too confident the S matrix will provide more clarity, but let’s see!\n\n# import previously generated CSV\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_S_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      -0.009776\n      0.005041\n      0.010999\n      Second Quadrant\n    \n    \n      1\n      -0.021503\n      0.001765\n      0.021576\n      Second Quadrant\n    \n    \n      2\n      0.005098\n      0.024645\n      0.025166\n      First Quadrant\n    \n    \n      3\n      -0.016745\n      -0.021457\n      0.027218\n      Third Quadrant\n    \n    \n      4\n      0.056964\n      -0.023095\n      0.061467\n      Fourth Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06681c3310>\n\n\n\n\n\n\nplot_top_5(df)\n\n\n\n\nInteresting! This plot looks like the plot of the original M matrix PCA results reflected across the y-axis. Similar movies are grouped together but the latent factors are showing an inverse relationship to the original 2-components.\n\n\n\nUsing fastai.collab\nfastai comes with a built-in method to create a collaborative filtering model similar to the DotProductBias model I created.\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.932735\n      0.930009\n      00:10\n    \n    \n      1\n      0.834800\n      0.862961\n      00:11\n    \n    \n      2\n      0.746893\n      0.822192\n      00:10\n    \n    \n      3\n      0.585107\n      0.811398\n      00:10\n    \n    \n      4\n      0.490022\n      0.812597\n      00:10\n    \n  \n\n\n\nThis yields similar results to what I’ve done above. Here are this model’s results:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      541\n      332\n      1\n      2.542970\n    \n    \n      1\n      899\n      1295\n      4\n      3.555057\n    \n    \n      2\n      346\n      1492\n      3\n      3.456876\n    \n    \n      3\n      933\n      1399\n      4\n      3.380442\n    \n    \n      4\n      310\n      1618\n      5\n      4.623110\n    \n    \n      5\n      276\n      1572\n      4\n      3.636531\n    \n    \n      6\n      463\n      322\n      5\n      3.901797\n    \n    \n      7\n      130\n      408\n      4\n      3.343735\n    \n    \n      8\n      914\n      1617\n      4\n      3.076288\n    \n  \n\n\n\nThe model created is a EmbeddingDotBias model\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nThe top biases can be obtained similar to how we did it before, but with a slightly different API:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Star Wars (1977)',\n \"Schindler's List (1993)\"]\n\n\nSimilar to the distance function I created, PyTorch has a nn.CosineSimilarity function which calculates the cosine of the angle between two vectors. The smaller the angle, the closer the two points are, and the more similar they are. nn.CosineSimilarity returns the similarity (cosine of the angle) between two vectors where 1.000 means the angle is 0.\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\nsimilarity = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = similarity.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Some Folks Call It a Sling Blade (1993)'\n\n\n\n\n\nDeep Learning for Collaborative Filtering\nIn this final section, we create a Deep Learning model which can make predictions on movie ratings after training on the MovieLens dataset. The model uses Embeddings (for users and movies) which are then fed into a small neural net (with one ReLu sandwiched between two Linear layers) which outputs an activation which we normalize using sigmoid_range. The embedding matrices are sized based on a heuristic built-in to fastai with the get_emb_sz method:\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nThe model used is constructed as follows: the user and item latent factors are created using Embeddings, and the neural net is created using the nn.Sequential class. Each time a prediction is needed, the user and item matrices for one batch are concatenated and passed through the neural net. The returned activation is sent to sigmoid_range and a prediction between 0 and 5.5 is calculated.\n\nclass CollabNN(Module):\n  def __init__(self, user_sz, item_sz, y_range=(0, 5.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.item_factors = Embedding(*item_sz)\n    self.layers = nn.Sequential(\n        nn.Linear(user_sz[1]+item_sz[1], n_act),\n        nn.ReLU(),\n        nn.Linear(n_act, 1))\n    self.y_range = y_range\n\n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1))\n    return sigmoid_range(x, *self.y_range)\n\nI want to visualize the forward method, so I’ll create the model and a batch, and walkthrough the code.\n\nmodel = CollabNN(*embs)\nx,y = dls.one_batch()\ndevice = \"cpu\"\nx = x.to(device)\nmodel = model.to(device)\nembs = torch.cat((model.user_factors(x[:,0]), model.item_factors(x[:,1])), dim=1)\nembs.shape\n\ntorch.Size([64, 176])\n\n\n\nx = model.layers(embs)\nsigmoid_range(x, *model.y_range)[:5]\n\ntensor([[2.8637],\n        [2.8647],\n        [2.8624],\n        [2.8696],\n        [2.8601]], grad_fn=<SliceBackward>)\n\n\nThe fastai collab_learner, instead of using the EmbeddingDotBias model, will use a neural network if passed True for its use_nn parameter. The number and size of neural network layers can also be specified.\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.988426\n      0.984418\n      00:15\n    \n    \n      1\n      0.893442\n      0.909180\n      00:16\n    \n    \n      2\n      0.900106\n      0.877499\n      00:16\n    \n    \n      3\n      0.809255\n      0.853736\n      00:16\n    \n    \n      4\n      0.769467\n      0.853571\n      00:16\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      244\n      1305\n      4\n      4.185966\n    \n    \n      1\n      902\n      965\n      3\n      3.474954\n    \n    \n      2\n      87\n      1173\n      5\n      4.206645\n    \n    \n      3\n      759\n      333\n      5\n      4.247213\n    \n    \n      4\n      109\n      1624\n      3\n      3.726794\n    \n    \n      5\n      363\n      743\n      1\n      1.774737\n    \n    \n      6\n      756\n      1216\n      5\n      4.058509\n    \n    \n      7\n      378\n      179\n      4\n      3.192873\n    \n    \n      8\n      18\n      141\n      3\n      3.296141\n    \n  \n\n\n\n\nlearn.model\n\nEmbeddingNN(\n  (embeds): ModuleList(\n    (0): Embedding(944, 74)\n    (1): Embedding(1665, 102)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=176, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=50, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=50, out_features=1, bias=True)\n    )\n    (3): SigmoidRange(low=0, high=5.5)\n  )\n)\n\n\nThe EmbeddingNN architecture extends the TabularModel class which we will explore in Chapter 9.\nThat finishes my review of Chapter 8, I’ll be working through the “Further Research” section in upcoming blog posts and associated videos:\n\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens.\n\n\n\n\nFind three other areas where collaborative filtering is being used, and identify the pros and cons of this approach in those areas.\n\n\n\n\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forums for ideas. Note that there are more columns in the full dataset–see if you can use those too (the next chapter might give you ideas).\n\n\n\n\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "",
    "text": "In this blog post, I work through the book Effective Pandas by Matt Harrison. I’ll take notes, work through examples and end-of-chapter exercises."
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-4-series-introduction",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-4-series-introduction",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 4: Series Introduction",
    "text": "Chapter 4: Series Introduction\nRepresent the following data in pure python:\n\n\n\nArtist\nData\n\n\n\n\n0\n145\n\n\n1\n142\n\n\n2\n38\n\n\n3\n13\n\n\n\n\nseries = {\n    'index': [0, 1, 2, 3],\n    'data': [145, 142, 38, 13],\n    'name': 'songs'\n}\n\nseries\n\n{'index': [0, 1, 2, 3], 'data': [145, 142, 38, 13], 'name': 'songs'}\n\n\nThe get function below can pull items out of this data structure based on the index:\n\ndef get(series, idx):\n    value_idx = series['index'].index(idx)\n    return series['data'][value_idx]\n\n\nget(series, 1)\n\n142\n\n\nThe index method on the list returns the list element at the provided index value.\n\n[0, 1, 2, 3].index(1)\n\n1\n\n\nBelow is an example that has string values for the index:\n\nsongs = {\n    'index': ['Paul', 'John', 'George', 'Ringo'],\n    'data': [145, 142, 38, 13],\n    'name': 'songs'\n}\n\n\nget(songs, 'John')\n\n142\n\n\nCreate a Series object from a list:\n\nimport pandas as pd\n\n\nsongs2 = pd.Series([145, 142, 38, 13], name = 'counts')\nsongs2\n\n0    145\n1    142\n2     38\n3     13\nName: counts, dtype: int64\n\n\nThe series is one-dimensional. The leftmost column is the index, also called the axis. The data (145, 142, 38, 13) is also called the values of the series. A DataFrame has two axes, one for the rows and another for the columns.\n\nsongs2.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nThe default values for an index are monotonically increasing integers. The index can be string-based as well (datatype for the index is object).\n\nsongs3 = pd.Series([145, 142, 38, 13],\n                   name = 'counts',\n                   index = ['Paul', 'John', 'George', 'Ringo'])\nsongs3\n\nPaul      145\nJohn      142\nGeorge     38\nRingo      13\nName: counts, dtype: int64\n\n\n\nsongs3.index\n\nIndex(['Paul', 'John', 'George', 'Ringo'], dtype='object')\n\n\nWe can insert Python objects into a series:\n\nclass Foo:\n    pass\n\nringo = pd.Series(\n    ['Richard', 'Starkey', 13, Foo()],\n    name = 'ringo')\n\nringo\n\n0                                 Richard\n1                                 Starkey\n2                                      13\n3    <__main__.Foo object at 0x10847d960>\nName: ringo, dtype: object\n\n\nThe object data type is also used for a series with string values and values that have heterogeneous or mixed types.\nHere is a series that has NaN in it:\n\nimport numpy as np\nnan_series = pd.Series([2, np.nan],\n                       index = ['Ono', 'Clapton'])\nnan_series\n\nOno        2.0\nClapton    NaN\ndtype: float64\n\n\nfloat64 supports NaN while int64 does not. As of pandas 0.24, Int64 (nullable integer type) supports NaN.\ncount ignores NaNs, .size does not.\n\nnan_series.count()\n\n1\n\n\n\nnan_series.size\n\n2\n\n\n\nnan_series2 = pd.Series([2, None],\n                        index = ['Ono', 'Clapton'],\n                        dtype = 'Int64')\nnan_series2\n\nOno           2\nClapton    <NA>\ndtype: Int64\n\n\n\nnan_series2.count()\n\n1\n\n\n\n# convert data type\nnan_series.astype('Int64')\n\nOno           2\nClapton    <NA>\ndtype: Int64\n\n\nThe Series object behaves similarly to a NumPy array.\n\nnumpy_ser = np.array([145, 142, 38, 13])\nsongs3[1], numpy_ser[1]\n\n(142, 142)\n\n\nThey both have methods in common\n\nsongs3.mean(), numpy_ser.mean()\n\n(84.5, 84.5)\n\n\nThey both have a notion of a boolean array.\n\nmask = songs3 > songs3.median()\nmask\n\nPaul       True\nJohn       True\nGeorge    False\nRingo     False\nName: counts, dtype: bool\n\n\n\n# use mask as a filter\nsongs3[mask]\n\nPaul    145\nJohn    142\nName: counts, dtype: int64\n\n\n\n# NumPy equivalent\nnumpy_ser[numpy_ser > np.median(numpy_ser)]\n\narray([145, 142])\n\n\nIf can indicate that data is categorical.\nCategorical values:\n\nUse less memory than strings\nImpove performance\nCan have an ordering\nCan perform operations on categories\nEnforce membership on values\n\n\ns = pd.Series(['m', 'l', 'xs', 's', 'xl'], dtype = 'category')\ns\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['l', 'm', 's', 'xl', 'xs']\n\n\nBy default categories don’t have an ordering.\n\ns.cat.ordered\n\nFalse\n\n\nConvert non-categorical series to an ordered category:\n\ns2 = pd.Series(['m', 'l', 'xs', 's', 'xl'])\n\nsize_type = pd.api.types.CategoricalDtype(\n    categories=['s', 'm', 'l'], ordered = True)\n\ns3 = s2.astype(size_type)\ns3\n\n0      m\n1      l\n2    NaN\n3      s\n4    NaN\ndtype: category\nCategories (3, object): ['s' < 'm' < 'l']\n\n\n\n# can perform comparisons on ordered categories\ns3 > 's'\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\n\n# add ordering information to categorical data\ns.cat.reorder_categories(['xs', 's', 'm', 'l', 'xl'], ordered=True)\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['xs' < 's' < 'm' < 'l' < 'xl']\n\n\nFor strings and dates converted to categorical types, we can still use the str or dt attributes on them:\n\ns3.str.upper()\n\n0      M\n1      L\n2    NaN\n3      S\n4    NaN\ndtype: object\n\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\npd.Series(data=None, index=None, dtype=None, name=None, copy=False)\nCreate a series from data (sequence, dictionary or scalar)\n\n\ns.index\nAccess index of series.\n\n\ns.astype(dtype, errors='raise')\nCast a series to dtype. To ignore errors (and return original object) use errors='ignore'\n\n\ns[boolean_array]\nReturn values from s where boolean_array is True\n\n\ns.cat.ordered\nDetermine if a categorical series is ordered\n\n\ns.cat.reorder_categories(new_categories, ordered=False)\nAdd categories (potentially ordered) to the series. new_categories must include all categories.\n\n\n\n\nExercises\n\nUsing Jupyter, create a series with the temperature values for the last seven days. Filter out the vaues below the mean.\nUsing Jupyter, create a series with your favorite colors. Use a categorical type.\n\n\n# temperature series\ntemps = pd.Series([88, 84, 84, 84, 88, 95, 97 ,88])\n\ntemps[temps >= temps.mean()]\n\n5    95\n6    97\ndtype: int64\n\n\n\n# favorite colors\ncolors_series = pd.Series(['orange', 'coral', 'midnight green'], dtype = 'category')\ncolors_series\n\n0            orange\n1             coral\n2    midnight green\ndtype: category\nCategories (3, object): ['coral', 'midnight green', 'orange']"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-5-series-deep-dive",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-5-series-deep-dive",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 5: Series Deep Dive",
    "text": "Chapter 5: Series Deep Dive\n\n# analyze the US Fuel Economy data\nurl = 'https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip'\n\ndf = pd.read_csv(url)\n\ncity_mpg = df.city08\nhighway_mpg = df.highway08\n\n/var/folders/5q/_bn7l90s177_2gq7rnhssjxm0000gn/T/ipykernel_76817/221626492.py:4: DtypeWarning: Columns (68,70,71,72,73,74,76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(url)\n\n\n\ncity_mpg\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\n\nhighway_mpg\n\n0        25\n1        14\n2        33\n3        12\n4        23\n         ..\n41139    26\n41140    28\n41141    24\n41142    24\n41143    21\nName: highway08, Length: 41144, dtype: int64\n\n\nBecause the type is int64 we know that none of the values are missing.\nThe dir function lists the attributes of an object. A series has 400+ attributes:\n\nlen(dir(city_mpg))\n\n412\n\n\n\nlen(dir(highway_mpg))\n\n412\n\n\nFunctionality of series attributes:\n\nDunder methods provide many numeric operations, looping, attribute access, and index access. For the numeric operations, these return Series.\nCorresponding operator methods for many of the numeric operations allow us to tweak the behavior.\nAggregate methods and properties which reduce or aggregate the values in a series down to a single scalar value.\nConversion methods. Some of these start with .to_ and export the data to other formats.\nManipulation methods that return Series objects with the same index.\nIndexing and accessor methods and attributes that return Series or scalars.\nString manipulation methods using .str.\nDate manipulation methods using .dt.\nPlotting methods using .plot.\nCategorical manipulation methods using .cat.\nTransformation methods.\nAttributes such as .index and .dtype.\nA bunch of private attributes (130 of them) that we’ll ignore.\n\n\nExercises\n\nExplore the documentation for five attributes of a series from Jupyter.\nHow many attributes are found on the .str attribute? Look at the documentation for three of them.\nHow many attributes are found on the .dt attribute? Look at the documentation for three of them.\n\n\ncity_mpg.values\n\narray([19,  9, 23, ..., 18, 18, 16])\n\n\n\ncity_mpg.axes\n\n[RangeIndex(start=0, stop=41144, step=1)]\n\n\n\ncity_mpg.empty\n\nFalse\n\n\n\ncity_mpg.at[4]\n\n17\n\n\n\ncity_mpg.loc[1:4]\n\n1     9\n2    23\n3    10\n4    17\nName: city08, dtype: int64\n\n\n\n# 98 string attributes\nlen(dir(s2.str))\n\n98\n\n\n\ns2.str.cat(sep = \".\")\n\n'm.l.xs.s.xl'\n\n\n\ns2.str.capitalize()\n\n0     M\n1     L\n2    Xs\n3     S\n4    Xl\ndtype: object\n\n\n\ns2.str.endswith('l')\n\n0    False\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n\n\n\ndt_series = pd.Series(['2023-01-01', '2023-04-05', '2023-07-06'])\n\ndt_series = pd.to_datetime(dt_series)\ndt_series\n\n0   2023-01-01\n1   2023-04-05\n2   2023-07-06\ndtype: datetime64[ns]\n\n\n\nlen(dir(dt_series.dt))\n\n83\n\n\n\ndt_series.dt.day\n\n0    1\n1    5\n2    6\ndtype: int32\n\n\n\ndt_series.dt.day_of_year\n\n0      1\n1     95\n2    187\ndtype: int32\n\n\n\ndt_series.dt.daysinmonth\n\n0    31\n1    30\n2    31\ndtype: int32"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-6-operators-dunder-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-6-operators-dunder-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 6: Operators (& Dunder Methods)",
    "text": "Chapter 6: Operators (& Dunder Methods)\nThese are the protocols that determine how the Python language reacts to operations.\n\n2 + 4\n\n6\n\n\n\n# under the cover is\n(2).__add__(4)\n\n6\n\n\n\n(city_mpg + highway_mpg) / 2\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\nWhen you operate with two series, pandas will align the index before performing the operation. Because of index alignment, you will want to make sure that the indexes: - are unique - are common to both series\n\n# example of series with repeated and non-common indexes\ns1 = pd.Series([10, 20, 30], index=[1,2,2])\ns2 = pd.Series([35, 44, 53], index=[2,2,4], name = 's2')\n\n\ns1\n\n1    10\n2    20\n2    30\ndtype: int64\n\n\n\ns2\n\n2    35\n2    44\n4    53\nName: s2, dtype: int64\n\n\n\n# index 1 and 4 have NaN\n# index 2 has four results\ns1 + s2\n\n1     NaN\n2    55.0\n2    64.0\n2    65.0\n2    74.0\n4     NaN\ndtype: float64\n\n\nWhen you perform math operations with a scalar, pandas broadcasts the operation to all values. A numeric pandas series is a block of memory, and modern CPUs leverage a technology called Single Instruction/Multiple Data (SIMD) to apply a math operation to the block of memory.\n\n# use `fill_value` parameter to replace missing operands\ns1.add(s2, fill_value = 0)\n\n1    10.0\n2    55.0\n2    64.0\n2    65.0\n2    74.0\n4    53.0\ndtype: float64\n\n\nChaining makes the code easy to read and understand\n\n(city_mpg\n    .add(highway_mpg)\n    .div(2))\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\n\n\n\n\n\n\n\n\nMethod\nOperator\nDescription\n\n\n\n\ns.add(s2)\ns + s2\nAdds series\n\n\ns.radd(s2)\ns2 + s\nAdds series\n\n\ns.sub(s2)\ns - s2\nSubtracts series\n\n\ns.rsub(s2)\ns2 - s\nSubtracts series\n\n\ns.mul(s2)\ns * s2\nMultiplies series\n\n\ns.multiply(s2)\ns * s2\nMultiplies series\n\n\ns.rmul(s2)\ns2 * s\nMultiplies series\n\n\ns.div(s2)\ns / s2\nDivides series\n\n\ns.truediv(s2)\ns / s2\nDivides series\n\n\ns.rdiv(s2)\ns2 / s\nDivides series\n\n\ns.rtruediv(s2)\ns2 / s\nDivides series\n\n\ns.mod(s2)\ns % s2\nModulo of series division\n\n\ns.rmod(s2)\ns2 % s\nModulo of series division\n\n\ns.floordiv(s2)\ns // s2\nFloor divide series\n\n\ns.rfloordiv(s2)\ns2 // s\nFloor divide series\n\n\ns.pow(s2)\ns ** s2\nExponential power of series\n\n\ns.rpow(s2)\ns2 ** s\nExponential power of series\n\n\ns.eq(s2)\ns2 == s\nElementwise equals of series\n\n\ns.ne(s2)\ns2 != s\nElementwise not equals of series\n\n\ns.gt(s2)\ns > s2\nElementwise greater than of series\n\n\ns.ge(s2)\ns >= s2\nElementwise greater than or equals of series\n\n\ns.lt(s2)\ns < s2\nElementwise less than of series\n\n\ns.le(s2)\ns <= s2\nElementwise less than or equals of series\n\n\nnp.invert(s)\n~s\nElementwise inversion of boolean series (no pandas method)\n\n\nnp.logical_and(s, s2)\ns & s2\nElementwise logical and of boolean series (no pandas method)\n\n\nnp.logical_or(s, s2)\ns \\| s2\nElementwise logical or of boolean series (no pandas method)\n\n\n\n\nExercises\nWith a dataset of your choice:\n\nAdd a numeric series to itself.\nAdd 10 to a numeric series.\nAdd a numeric series to itself using the .add method.\nRead the documentation for the .add method.\n\n\ncity_mpg + city_mpg\n\n0        38\n1        18\n2        46\n3        20\n4        34\n         ..\n41139    38\n41140    40\n41141    36\n41142    36\n41143    32\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg + 10\n\n0        29\n1        19\n2        33\n3        20\n4        27\n         ..\n41139    29\n41140    30\n41141    28\n41142    28\n41143    26\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg.add(city_mpg)\n\n0        38\n1        18\n2        46\n3        20\n4        34\n         ..\n41139    38\n41140    40\n41141    36\n41142    36\n41143    32\nName: city08, Length: 41144, dtype: int64\n\n\n\n# experimenting with fill_value parameter\nnan_series3 = pd.Series([2, None])\nnan_series4 = pd.Series([3, None])\n\n\nnan_series3\n\n0    2.0\n1    NaN\ndtype: float64\n\n\n\nnan_series4\n\n0    3.0\n1    NaN\ndtype: float64\n\n\n\n# two corresponding NaN values stay NaN\n# even with fill_value = 0\nnan_series3.add(nan_series4, fill_value=0)\n\n0    5.0\n1    NaN\ndtype: float64"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-7-aggregate-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-7-aggregate-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 7: Aggregate Methods",
    "text": "Chapter 7: Aggregate Methods\nAggregate methods collapse the values of a series down to a scalar.\n\n# calculate the mean\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ncity_mpg.is_unique\n\nFalse\n\n\n\npd.Series([1,2,3]).is_unique\n\nTrue\n\n\n\ncity_mpg.is_monotonic_increasing\n\nFalse\n\n\n\npd.Series([1,2,3]).is_monotonic_increasing\n\nTrue\n\n\n\n# default is median (50% quantile)\ncity_mpg.quantile()\n\n17.0\n\n\n\ncity_mpg.quantile(0.9)\n\n24.0\n\n\n\n# multiple quantiles returns a Series\ncity_mpg.quantile([0.1, 0.5, 0.9])\n\n0.1    13.0\n0.5    17.0\n0.9    24.0\nName: city08, dtype: float64\n\n\nIf you want the count of values that meet some criteria, you can use the .sum method:\n\n# count of cars with mileage greater than 20\n(city_mpg\n     .gt(20)\n     .sum()\n)\n\n10272\n\n\n\n# percentage of cars with mileage greater than 20\n(city_mpg\n     .gt(20)\n     .mul(100)\n     .mean()\n)\n\n24.965973167412017\n\n\n\n(pd.Series([1,2,3,4])\n    .gt(2)\n    .mul(100)\n)\n\n0      0\n1      0\n2    100\n3    100\ndtype: int64\n\n\n\n(pd.Series([1,2,3,4])\n     .gt(2)\n     .mul(100)\n     .mean()   \n)\n\n50.0\n\n\nIf you sum up a series of boolean values, the result is the count of True values. If you take the mean of a series of boolean values, the result is the fraction of values that are True.\n.agg can perform multiple operations.\n\ncity_mpg.agg('mean')\n\n18.369045304297103\n\n\n\ndef second_to_last(s):\n    return s.iloc[-2]\n\n\ncity_mpg.agg(['mean', np.var, max, second_to_last])\n\nmean               18.369045\nvar                62.503036\nmax               150.000000\nsecond_to_last     18.000000\nName: city08, dtype: float64\n\n\nAggregation strings and descriptions:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\n'all'\nReturns True if every value is truthy.\n\n\n'any'\nReturns True if any value is truthy.\n\n\n'autocorr'\nReturns Pearson correlation of series with shifted self. Can override lag as keyword argument (default is 1).\n\n\n'corr'\nReturns Pearson correlation of series with other series. Need to specify other\n\n\n'count'\nReturns count of non-missing values.\n\n\n'cov'\nReturns covariance of series with other series. Need to specify other\n\n\n'dtype'\nType of the series.\n\n\n'dtypes'\nType of the series.\n\n\n'empty'\nTrue is no values in series.\n\n\n'hasnans'\nTrue if missing values in series.\n\n\n'idxmax'\nReturns index value of maximum value.\n\n\n'idxmin'\nReturns index value of minimum value.\n\n\n'is_monotonic'\nTrue if values always increase.\n\n\n'is_monotonic_decreasing'\nTrue if values always decrease.\n\n\n'is_monotonic_increasing'\nTrue if values always increase.\n\n\n'kurt'\nReturns “excess” kurtosis (0 is normal distribution). Values greater than 0 have more outliers than normal.\n\n\n'mad'\nReturns the mean absolute deviation.\n\n\n'max'\nReturns the maximum value.\n\n\n'mean'\nReturns the mean value.\n\n\n'median'\nReturns the median value.\n\n\n'min'\nReturns the minimum value.\n\n\n'nbytes'\nReturns the number of bytes of the data.\n\n\n'ndim'\nReturn the number of dimensions (1) of the data.\n\n\n'nunique'\nReturns the count of unique values.\n\n\n'quantile'\nReturns the median value. Can override q to specify other quantile.\n\n\n'sem'\nReturns the unbiarsed standard error.\n\n\n'size'\nReturns the size of the data.\n\n\n'skew'\nReturns the unbiased skew of the data. Negative indicates tail is on the left side.\n\n\n'std'\nReturns the standard deviation of the data.\n\n\n'sum'\nReturns the sum of the series.\n\n\n\nAggregation methods and properties:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ns.agg(func=None, axis=0, *args, **kwargs)\nReturns a scalar if func is a single aggregation function. Returns a series if a list of aggregations are passed to func.\n\n\ns.all(axis=0, bool_only=None, skipna=True, level=None)\nReturns True if every value is truthy. Otherwise False.\n\n\ns.any(axis=0, bool_only=None, skipna=True, level=None)\nReturns True if at least one value is truthy. Otherwise False.\n\n\ns.autocorr(lag=1)\nReturns Pearson correlation between s and shifted s.\n\n\ns.corr(other, method='pearson')\nReturns correlation coefficient for 'pearson', 'spearman', 'kendall', or a callable.\n\n\ns.cov(other, min_periods=None)\nReturns covariance.\n\n\ns.max(axis=None, skipna=None, level=None, numeric_only=None)\nReturns maximum value.\n\n\ns.min(axis=None, skipna=None, level=None, numeric_only=None)\nReturns minimum value.\n\n\ns.mean(axis=None, skipna=None, level=None, numeric_only=None)\nReturns mean value.\n\n\ns.median(axis=None, skipna=None, level=None, numeric_only=None)\nReturns median value.\n\n\ns.prod(axis=None, skipna=None, level=None, numeric_only=None, min_count=0)\nReturns product of s values.\n\n\ns.quantile(q=0.5, interpolation='linear')\nReturns 50% quantile by default. Returns Series if q is a list.\n\n\ns.sem(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns unbiased standard error of mean.\n\n\ns.std(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns sample standard deviation.\n\n\ns.var(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns unbiased variance.\n\n\ns.skew(axis=None, skipna=None, level=None, numeric_only=None)\nReturns unbiased skew.\n\n\ns.kurtosis(axis=None, skipna=None, level=None, numeric_only=None)\nReturns unbiased kurtosis.\n\n\ns.nunique(dropna=True)\nReturns count of unique items.\n\n\ns.count(level=None)\nReturns count of non-missing items.\n\n\ns.size\nNumber of items in series. (Property)\n\n\ns.is_unique\nTrue if all values are unique.\n\n\ns.is_monotonic\nTrue if all values are increasing.\n\n\ns.is_monotonic_increasing\nTrue if all values are increasing.\n\n\ns.is_monotonic_decreasing\nTrue if all values are decreasing.\n\n\n\n\nExercises\nWith a dataset of your choice:\n\nFind the count of non-missing values of a series.\nFind the number of entries of a series.\nFind the number of unique entries of a series.\nFind the mean value of a series.\nFind the maximum value of a series.\nUse the .agg method to find all of the above.\n\n\ncity_mpg.count()\n\n41144\n\n\n\ncity_mpg.size\n\n41144\n\n\n\ncity_mpg.nunique()\n\n105\n\n\n\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ncity_mpg.max()\n\n150\n\n\n\ncity_mpg.agg(['count', 'size', 'nunique', 'mean', 'max'])\n\ncount      41144.000000\nsize       41144.000000\nnunique      105.000000\nmean          18.369045\nmax          150.000000\nName: city08, dtype: float64"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "title": "R Shiny Census App",
    "section": "",
    "text": "In this blog post, I’ll walk through my development process for a U.S. Census data visualization web app I created using the Shiny package in R.\nYou can access the app at vbakshi.shinyapps.io/census-app."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "title": "R Shiny Census App",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackstory\nCodebase\n\napp.R\n\nWhat’s in my ui?\n\nDropdowns\nTables\nPlot\nDownload buttons\n\nWhat does my server do?\n\nGet Data\nRender Outputs\nPrepare Dynamic Text\nHandle Downloads\n\n\nprep_db.R\n\nDatabase Tables\n\nb20005\nb20005_vars\ncodes\n\nCreate Tables\nWrite to Tables\nLoad the Data\n\nget_b20005_ruca_aggregate_earnings.R\n\nGet Variable Names\nDerive RUCA Level Estimates and MOE\n\ncalculate_median.R\n\nCreate Frequency Distribution\nCalculate Weighted Total\nApproximate Standard Error\nCalculate Median Estimate Bounds\nReshape the Data\n\nformat_query_result.R\n\nExtract data.frame Objects from List\nReshape data.frame Objects\nAdd Descriptive Labels\n\nget_b20005_labels.R\n\nGet Earnings Population Estimate Labels\nGet All Labels\n\nget_b20005_tract_earnings.R\n\nGet Variable Names\nJoin Tables\n\nget_b20005_states.R\nget_design_factor.R\nmake_plot.R"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "title": "R Shiny Census App",
    "section": "Backstory",
    "text": "Backstory\nI started this project by reading the handbook Understanding and Using American Community Survey Data: What State and Local Government Users Need to Know published by the U.S. Census Bureau. I recreated the handbook’s first case study in R, in which they make comparisons across geographic areas, create custom geographic areas from census tracts and calculate margins of error for derived estimates for Minnesota Census Tract 5-year earnings estimates.\nDuring the process of recreating the derived median earnings estimate calculations, I was unable to recreate a key value from the handbook (the Standard Error for the 50% proportion, calculated to be 0.599) because I was unable to deduce the values used in the following formula referenced from page 17 of the PUMS Accuracy of the Data documentation:\n\n\n\nStandard Error equals Design Factor times square root of the product of 95 over 5B and 50 squared\n\n\nThe documentation defines B as the base, which is the calculated weighted total. I chose the value of 1.3 for the design factor DF since it corresponds to STATE = Minnesota, CHARTYP = Population, CHARACTERISTIC = Person Earnings/Income in the Design Factors CSV published by the Census Bureau.\nI called the Census Bureau Customer Help Center for assistance and was transferred to a member of the ACS Data User Support team with whom I discussed my woes. He was unable to confirm the values of the design factor DF or B, and was unable to pull up the contact information for the statistical methodology team, so I emailed him my questions. After a few email exchanges, the statistical methodology team provided the following:\n\nDF = 1.3\nB = the total population estimate for which the median is being calculated, which is 82488 for the case study calculation (Minnesota Rural Male Full Time Workers)\nThe term 95/5 is associated with the finite population correction factor (100 - f) divided by the sample fraction (f), where f = 5% (later on I note in the documentation that this 95/5 term is based on a 68% confidence interval). The data used in the handbook case study is from 5-year estimates. 1-year estimates sample 2.5% of the population, so the 5-year estimates represent a 5 * 2.5 = 12.5% sample. Instead of 95/5, the ratio becomes (100 - 12.5)/12.5 = 87.5/12.5\n\nThe updated formula is then:\n\n\n\nStandard Error equals Design Factor times square root of the product of 87.5 over 12.5B and 50 squared\n\n\nI was able to calculate the median earnings estimate (and associated standard error and margin of error) within a few percent of the values given in the handbook. This provided me with confirmation that I was ready to expand my code to calculate median earnings estimates for other subgroups."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "title": "R Shiny Census App",
    "section": "Codebase",
    "text": "Codebase\nI built this app using the R package Shiny which handles both the UI and the server. I store the data in a sqlite database and access it with queries written using the RSQLite package which uses the DBI API. The following sections break down the R scripts based on functionality. Click on the script name to navigate to that section.\n\napp.R\n\nUI and server functions to handle people inputs and plot/table/text outputs\n\nprep_db.R\n\nImport, clean, combine and then load data into the census_app_db.sqlite database\n\nget_b20005_ruca_aggregate_earnings.R\n\nQueries the database for earnings and associated margins of error for RUCA levels derived from Census Tracts\n\ncalculate_median.R\n\nDerives estimate, standard of error and margin of error of median earnings for RUCA levels\n\nformat_query_result.R\n\nFormats calculate_median query results\n\nget_b20005_labels.R\n\nQueries the database for descriptive labels of B20005 table variables\n\nget_b20005_tract_earnings.R\n\nQueries the database for Census Tract-level earnings and associated margins of error\n\nget_b20005_states.R\n\nQueries the SQLite database for a list of U.S. states\n\nget_design_factor.R\n\nQueries database for the design factor used for the median earnings estimation calculation\n\nmake_plot.R\n\nCreates a bar plot object"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "title": "R Shiny Census App",
    "section": "app.R",
    "text": "app.R\nA shiny app has three fundamental components:\nui <- (...)\nserver <- (...)\nshinyApp(ui, server,...)\nThe ui object holds all UI layout, input and output objects which define the front-end of your app. The server object holds all rendering functions which are assigned to outputs that appear on the UI. The shinyApp function takes a ui and server object (along with other arguments) and creates a shiny app object which can be run in a browser by passing it to the runApp function. Person inputs (such as selections in a dropdown) are assigned to a global input object.\n\nWhat’s in my ui?\nAll of my UI objects are wrapped within a fluidPage call which returns a page layout which “consists of rows which in turn include columns” (from the docs).\nMy app’s UI has four sections:\n\nDropdowns to select state, sex and work status for which the person using the app wants ACS 5-year earnings estimates\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\nA table with population estimates for earnings levels for each RUCA level for the selected state, sex and work status\n\nEach section has a download button so that people can get the CSV files or plot image for their own analysis and reporting. Each section is separated with markdown('---') which renders an HTML horizontal rule (<hr>).\n\nDropdowns\nDropdowns (the HTML <select> element) are a type of UI Input. I define each with an inputId which is a character object for reference on the server-side, a label character object which is rendered above the dropdown, and a list object which defines the dropdown options.\nselectInput(\n  inputId = \"...\",\n  label = \"...\",\n  choices = list(...)\n)\nIn some cases, I want the person to see a character object in the dropdown that is more human-readable (e.g. \"Large Town\") but use a corresponding input value in the server which is more computer-readable (e.g. \"Large_Town). To achieve this, I use a named character vector where the names are displayed in the dropdown, and the assigned values are assigned to the global input:\nselectInput(\n     inputId = \"ruca_level\",\n     label = \"Select RUCA Level\",\n     choices = list(\n       \"RUCA LEVEL\" = c(\n       \"Urban\" = \"Urban\", \n       \"Large Town\" = \"Large_Town\", \n       \"Small Town\" = \"Small_Town\", \n       \"Rural\" = \"Rural\"))\n     )\nIn this case, if the person selects \"Large Town\" the value assigned to input$ruca_level is \"Large_Town\".\n\n\nTables\nTables (the HTML <table> element) are a type of UI Output. I define each with an outputId for reference in the server.\ntableOutput(outputId = \"...\")\n\n\nPlot\nSimilarly, a plot (which is rendered as an HTML <img> element) is a type of UI Output. I define each with an outputId.\nplotOutput(outputId = \"...\")\n\n\nDownload Buttons\nThe download button (an HTML <a> element) is also a type of UI Output. I define each with an outputId and label (which is displayed as the HTML textContent attribute of the <a> element).\ndownloadButton(\n  outputId = \"...\",\n  label = \"...\"\n)\n\n\n\nWhat does my server do?\nThe server function has three parameters: input, output and session. The input object is a ReactiveValues object which stores all UI Input values, which are accessed with input$inputId. The output object similarly holds UI Output values at output$outputId. I do not use the session object in my app (yet).\nMy app’s server has four sections:\n\nGet data from the SQLite database\nRender table and plot outputs\nPrepare dynamic text (for filenames and the plot title)\nHandle data.frame and plot downloads\n\n\nGet Data\nThere are three high-level functions which call query/format/calculation functions to return the data in the format necessary to produce table, text, download and plot outputs:\n\nThe earnings_data function passes the person-selected dropdown options input$sex, input$work_status and input$state to the get_b20005_ruca_aggregate_earnings function to get a query result from the SQLite database. That function call is passed to format_earnings, which in turn is passed to the reactive function to make it a reactive expression. Only reactive expressions (and reactive endpoints in the output object) are allowed to access the input object which is a reactive source. You can read more about Shiny’s “reactive programming model” in this excellent article.\n\nearnings_data <- reactive(\n  format_earnings(\n    get_b20005_ruca_aggregate_earnings(\n      input$sex, \n      input$work_status, \n      input$state)))\n\nThe design_factor function passes the input$state selection to the get_design_factor function which in turn is passed to the reactive function.\n\ndesign_factor <- reactive(get_design_factor(input$state))\n\nThe median_data function passes the return values from earnings_data() and design_factor() to the calculate_median function which in turn is passed to the reactive function.\n\nmedian_data <- reactive(calculate_median(earnings_data(), design_factor()))\n\n\nRender Outputs\nI have two reactive endpoints for table outputs, and one endpoint for a plot. The table outputs use renderTable (with row names displayed) with the data.frame coming from median_data() and earnings_data(). The plot output uses renderPlot, and a helper function make_plot to create a bar plot of earnings_data() for a person-selected input$ruca_level with a title created with the helper function earnings_plot_title().\noutput$median_data <- renderTable(\n  expr = median_data(), \n  rownames = TRUE)\n  \noutput$earnings_data <- renderTable(\n  expr = earnings_data(), \n  rownames = TRUE)\n    \noutput$earnings_histogram <- renderPlot(\n  expr = make_plot(\n    data=earnings_data(), \n    ruca_level=input$ruca_level, \n    plot_title=earnings_plot_title()))\n\n\nPrepare Dynamic Text\nI created four functions that generate filenames for the downloadHandler call when the corresponding downloadButton gets clicked, one function that generates the title used to generate the bar plot, and one function which takes computer-readable character objects (e.g. \"Large_Town\") and maps it to and returns a more human-readable character object (e.g. \"Large Town\"). I chose to keep filenames more computer-readable (to avoid spaces) and the plot title more human-readable.\nget_pretty_text <- function(raw_text){\n  text_map <- c(\"M\" = \"Male\", \n  \"F\" = \"Female\",\n  \"FT\" = \"Full Time\",\n  \"OTHER\" = \"Other\",\n  \"Urban\" = \"Urban\",\n  \"Large_Town\" = \"Large Town\",\n  \"Small_Town\" = \"Small Town\",\n  \"Rural\" = \"Rural\")\n  return(text_map[raw_text])\n  }\n \nearnings_plot_title <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    get_pretty_text(input$work_status),\n    input$ruca_level,\n    \"Workers\",\n    sep=\" \"))\n  }\n\nb20005_filename <- function(){\n    return(paste(\n      input$state,\n      get_pretty_text(input$sex),\n      input$work_status,\n      \"earnings.csv\",\n      sep=\"_\"\n    ))\n  }\n  \nmedian_summary_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex), \n    input$work_status, \n    'estimated_median_earnings_summary.csv',  \n    sep=\"_\")\n  }\n  \nruca_earnings_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex),  \n    input$work_status, \n    'estimated_median_earnings_by_ruca_level.csv',  \n    sep=\"_\")\n  }\n  \nearnings_plot_filename <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    input$work_status,\n    input$ruca_level,\n    \"Workers.png\",\n    sep=\"_\"))\n  }\n\n\nHandle downloads\nI have five download buttons in my app: two which trigger a download of a zip file with two CSVs, two that downloads a single CSV, and one that downloads a single PNG. The downloadHandler function takes a filename and a content function to write data to a file.\nIn order to create a zip file, I use the zip base package function and pass it a vector with two filepaths (to which data is written using the base package’s write.csv function) and a filename. I also specify the contentType as \"application/zip\". In the zip file, one of the CSVs contains a query result from the b20005 SQLite database table with earnings data, and the other file, \"b20005_variables.csv\" contains B20005 table variable names and descriptions. In order to avoid the files being written locally before download, I create a temporary directory with tempdir and prepend it to the filename to create the filepath.\nFor the bar plot image download, I use the ggplot2 package’s ggsave function, which takes a filename, a plot object (returned from the make_plot helper function) and the character object \"png\" (for the device parameter).\noutput$download_selected_b20005_data <- downloadHandler(\n    filename = \"b20005_data.zip\",\n    content = function(fname) {\n      # Create a temporary directory to prevent local storage of new files\n      temp_dir <- tempdir()\n      \n      # Create two filepath character objects and store them in a list\n      # which will later on be passed to the `zip` function\n      path1 <- paste(temp_dir, '/', b20005_filename(), sep=\"\")\n      path2 <- paste(temp_dir, \"/b20005_variables.csv\", sep=\"\")\n      fs <- c(path1, path2)\n      \n      # Create a CSV with person-selection input values and do not add a column\n      # with row names\n      write.csv(\n        get_b20005_earnings(input$state, input$sex, input$work_status), \n        path1,\n        row.names = FALSE)\n      \n      # Create a CSV for table B20005 variable names and labels for reference\n      write.csv(\n        get_b20005_ALL_labels(),\n        path2,\n        row.names = FALSE)\n      \n      # Zip together the files and add flags to maximize compression\n      zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_all_b20005_data <- downloadHandler(\n  filename = \"ALL_B20005_data.zip\",\n  content = function(fname){\n    path1 <- \"ALL_B20005_data.csv\"\n    path2 <- \"b20005_variables.csv\"\n    fs <- c(path1, path2)\n    \n    write.csv(\n      get_b20005_earnings('ALL', 'ALL', 'ALL'),\n      path1,\n      row.names = FALSE)\n    \n    write.csv(\n      get_b20005_ALL_labels(),\n      path2,\n      row.names = FALSE)\n    \n    zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_median_summary <- downloadHandler(\n  filename = median_summary_filename(),\n  content = function(file) {\n    write.csv(median_data(), file)\n    }\n  )\n  \noutput$download_earnings_plot <- downloadHandler(\n  filename = earnings_plot_filename(),\n  content = function(file) {\n    ggsave(\n      file, \n      plot = make_plot(\n        data=earnings_data(), \n        ruca_level=input$ruca_level, \n        plot_title=earnings_plot_title()), \n        device = \"png\")\n      }\n  )\n  \noutput$download_ruca_earnings <- downloadHandler(\n  filename = ruca_earnings_filename(),\n  content = function(file) {\n    write.csv(earnings_data(), file)\n  }\n  )"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "title": "R Shiny Census App",
    "section": "prep_db.R",
    "text": "prep_db.R\nThis script is meant to be run locally, and is not deployed, as doing so would create a long delay to load the app.\n\nDatabase Tables\nThe database diagram is shown below (created using dbdiagram.io):\n\n\n\nDatabase diagram showing the database table schemas and their relationships\n\n\nI have five tables in my database:\n\n\nb20005\nHolds the data from the ACS 2015-2019 5-year detailed table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months). This includes earnings estimates and margins of errors for Male and Female, Full Time and Other workers, for earning ranges (No earnings, $1 - $2499, $2500 - $4999, …, $100000 or more). The following table summarizes the groupings of the (non-zero earnings) variables relevant to this app:\n\n\n\n\nVariable\nDemographic\n\n\n\n\nB20005_003 to B20005_025\nMale Full Time Workers\n\n\nB20005_029 to B20005_048\nMale Other Workers\n\n\nB20005_050 to B20005_072\nFemale Full Time Workers\n\n\nB20005_076 to B20005_095\nFemale Other Workers\n\n\n\n\n\n\nb20005_vars\nHas the name (e.g. B20005_003E) and label (e.g. “Estimate!!Total!!Male!!Worked full-time, year-round in the past 12 months”) for all B20005 variables. Variable names ending with an E are estimates, and those ending with M are margins of error. - ruca contains RUCA (Rural-Urban Commuting Area) codes published by the U.S. Department of Agriculture Economic Research Service which classify U.S. census tracts using measures of population density. The following table shows the code ranges relevant to this app:\n\n\n\n\nRUCA Code\nRUCA Level\n\n\n\n\n1-3\nUrban\n\n\n4-6\nLarge Town\n\n\n7-9\nSmall Town\n\n\n10\nRural\n\n\n99\nZero Population\n\n\n\n\n\n\ncodes\nolds state FIPS (Federal Information Processing Standards) codes and RUCA levels - design_factors contains Design Factors for different characteristics (e.g. Person Earnings/Income) which are used to determine “the standard error of total and percentage sample estimates”, and “reflect the effects of the actual sample design and estimation procedures used for the ACS.” (2015-2019 PUMS 5-Year Accuracy of the Data).\nIn prep_db.R, I use the DBI package, censusapi and base R functions to perform the following protocol for each table:\n\n\nLoad the Data\n\nFor tables b20005 and b20005_vars, I use the censusapi::getCensus and censusapi::listCensusMetadata repsectively to get the data\n\n# TABLE b20005_vars ------------------------------\nb20005_vars <- listCensusMetadata(\n  name = 'acs/acs5',\n  vintage = 2015,\n  type = 'variables',\n  group = 'B20005')\n  \n # TABLE b20005 ----------------------------------\n b20005 <- getCensus(\n  name = 'acs/acs5',\n  region = \"tract:*\",\n  regionin = regionin_value,\n  vintage = 2015,\n  vars = b20005_vars$name,\n  key=\"...\"\n  )\n\nFor tables codes, ruca, and design_factors I load the data from CSVs that I either obtained (in the case of the Design Factors) or created (in the case of the codes and RUCA levels)\n\n # TABLE codes ----------------------------------\nstate_codes <- read.csv(\n  \"data/state_codes.csv\",\n  colClasses = c(\n    \"character\", \n    \"character\", \n    \"character\")\n)\n\nruca_levels <- read.csv(\n  \"data/ruca_levels.csv\",\n  colClasses = c(\n    \"character\",\n    \"character\",\n    \"character\")\n)\n\n\nCreate Tables\nOnce the data is ready, I use DBI::dbExecute to run a SQLite command to create each table. The relationships shown in the image above dictate which fields create the primary key (in some cases, a compound primary key) as listed below:\n\n\n\n\n\n\n\n\nTable\nPrimary Key\nNotes\n\n\n\n\nb20005\n(state, county, tract))\nForeign key for table ruca\n\n\nb20005_vars\nname\ne.g. B20005_001E\n\n\nruca\nTRACTFIPS\nForeign key for table b20005\n\n\ncodes\n(CODE, DESCRIPTION)\ne.g. (1, \"Urban\")\n\n\ndesign_factors\n(ST, CHARACTERISTIC)\ne.g. (\"27\", \"Person Earnings/Income\")\n\n\n\n\n\nWrite to Tables\nOnce the table has been created in the database, I write the data.frame to the corresponding table with the following call:\ndbWriteTable(census_app_db, \"<table name>\", <data.frame>, append = TRUE"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_ruca_aggregate_earnings.R",
    "text": "get_b20005_ruca_aggregate_earnings.R\nThe function inside this script (with the same name), receives inputs from the server, sends queries to the database and returns the results. This process involves two steps:\n\nGet Variable Names\nThe person using the app selects Sex (M or F), Work Status (Full Time or Other) and State (50 states + D.C. + Puerto Rico) for which they want to view and analyze earnings data. As shown above, different variables in table b20005 correspond to different sexes and work statuses, and each tract for which there is all that earnings data resides in a given state.\nI first query b20005_vars to get the relevent variables names which will be used in the query to b20005, as shown below. names that end with “M” (queried with the wilcard '%M') are for margins of error and those that end with “E” (wildcard '%E') are for estimates.\nvars <- dbGetQuery(\n    census_app_db, \n    \"SELECT name FROM b20005_vars \n    WHERE label LIKE $label_wildcard \n    AND name LIKE '%M'\",\n    params=list(label_wildcard=label_wildcard))\nThe b20005_vars.label column holds long string labels (which follow a consistent pattern, which is captured by the $label_wildcard) that describe the variable’s contents. Here are a couple of examples: \n\n\n\n\n\n\n\nb20005_vars.name\nb20005_vars.label\n\n\n\n\nB20005_053E\n\"Estimate!!Total!!Female!!Worked full-time, year-round in the past 12 months!!With earnings\")\n\n\nB20005_076M\n\"Margin of Error!!Total!!Female!!Other!!With earnings!!$1 to $2,499 or loss\"\n\n\n\n\nSince the label string contains the sex and work status, I assign a label_wildcard based on the person inputs from the sex and work status UI dropdowns.\n# Prepare wildcard for query parameter `label_wildcard`\n  if (sex == 'M') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Male!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Male!!Other%\" }\n  }\n  \n  if (sex == 'F') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Female!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Female!!Other%\" }\n  }\n\n\nDerive RUCA Level Estimates and MOE\nOnce the variables are returned, the actual values are queried from b20005, grouped by RUCA level. The ACS handbook Understanding and Using American Community Survey Data: What All Data Users Need to Know shows how to calculate that margin of error for derived estimates. In our case, the margin of error for a RUCA level such as “Urban” for a given state is derived from the margin of error of individual Census Tracts using the formula below:\n\n\n\nThe MOE for a sum of estimates is the square root of the sum of MOEs squared\n\n\nTranslating this to a SQLite query:\n# Construct query string to square root of the sum of margins of error squared grouped by ruca level\nquery_string <- paste0(\n    \"SQRT(SUM(POWER(b20005.\", vars$name, \", 2))) AS \", vars$name, collapse=\",\")\nWhere vars$name is a list of variable names, and the collapse parameter converts a list or vector to a string. The beginning of that query_string looks like:\n\"SQRT(SUM(POWER(b20005.B20005_001M, 2))) AS B20005_001M, SQRT(...\"\nThe query is further built by adding the rest of the SQL statements:\nquery_string <- paste(\n    \"SELECT ruca.DESCRIPTION,\",\n    query_string,\n    \"FROM 'b20005' \n    INNER JOIN ruca \n    ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n    WHERE \n    b20005.state = $state\n    GROUP BY ruca.DESCRIPTION\"\n  )\nThe ruca.DESCRIPTION column, which contains RUCA levels (e.g. \"Urban\") is joined onto b20005 from the ruca table using the foreign keys representing the Census Tract FIPS code (TRACTFIPS for the ruca table and the concatenated field state || county || tract for b20005). The $state parameter is assigned the person-selected state input, and the columns are aggreaggated by RUCA levels (i.e. GROUP BY ruca.DESCRIPTION). Finally, the RUCA level and square root of the sum of MOEs squared are SELECTed from the joined tables.\nThe query for estimates is simpler than MOEs, because estimates only need to be summed over RUCA levels:\n# Construct a query to sum estimates grouped by ruca level\n  query_string <- paste0(\"SUM(b20005.\",vars$name, \") AS \", vars$name, collapse=\",\")\nget_b20005_ruca_aggregate_earnings returns the query result data.frames in a named list:\nreturn(list(\"estimate\" = estimate_rs, \"moe\" = moe_rs))"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "title": "R Shiny Census App",
    "section": "calculate_median.R",
    "text": "calculate_median.R\nThe procedure for calculating a median earnings data estimate is shown starting on page 17 of the Accuracy of PUMS documentation. This script follows it closely:\n\nCreate Frequency Distribution\n\nObtain the weighted frequency distribution for the selected variable. data is a data.frame with earning estimate values. The rows are the earning ranges and the columns are ruca_levels:\n\n\ncum_percent <- 100.0 * cumsum(data[ruca_level]) / sum(data[ruca_level])\n\n\nCalculate Weighted Total\n\nCalculate the weighted total to yield the base, B.\n\n\nB <- colSums(data[ruca_level])\n\n\nApproximate Standard Error\n\nApproximate the standard error of a 50 percent proportion using the formula in Standard Errors for Totals and Percentages. The design_factor is passed to this function by the server who uses the get_design_factor function explained below to query the design_factors table.\n\n\nse_50_percent <- design_factor * sqrt(87.5/(12.5*B) * 50^2)\n\n\nCalculate Median Estimate Bounds\n\nCreate the variable p_lower by subtracting the SE from 50 percent. Create p_upper by adding the SE to 50 percent.\n\n\np_lower <- 50 - se_50_percent\np_upper <- 50 + se_50_percent\n\nDetermine the categories in the distribution that contain p_lower and p_upper…\n\n\n# Determine the indexes of the cumulative percent data.frame corresponding  \n# to the upper and lower bounds of the 50% proportion estimate\ncum_percent_idx_lower <- min(which(cum_percent > p_lower))\ncum_percent_idx_upper <- min(which(cum_percent > p_upper))\n.._If p_lower and p_upper fall in the same category, follow step 6. If p_lower and p_upper fall in different categories, go to step 7…_\n\n# The median estimation calculation is handled differently based on \n# whether the upper and lower bound indexes are equal\n    if (cum_percent_idx_lower == cum_percent_idx_upper) {\n\nIf p_lower and p_upper fall in the same category, do the following:\n\n\nDefine A1 as the smallest value in that category.\n\n\n# A1 is the minimum earnings value (e.g. 30000) of the earning range \n# (e.g. 30000 to 34999) corresponding to the lower bound cumulative percent\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\n\nDefine A2 as the smallest value in the next (higher) category.\n\n\n# A2 is the minimum earnings value of the earning range above the \n# earning range corresponding to the upper bound cumulative percent\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\n\nDefine C1 as the cumulative percent of units strictly less than A1.\n\n\n# C1 is the cumulative percentage of earnings one row below the \n# lower bound cumulative percent\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\n\nDefine C2 as the cumulative percent of units strictly less than A2.\n\n\n# C2 is the cumulative percentage of the earnings below the \n# lower bound cumulative percent\nC2 <- cum_percent[cum_percent_idx_lower, ]\n\nUse the following formulas to approximate the lower and upper bounds for a confidence interval about the median:\n\n\n# the lower bound of the median \nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n      \n# the upper bound of the median\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nIf p_lower and p_upper fall in different categories, do the following:\n\n\nFor the category containing p_lower: Define A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the lower bound.\n\n\n# A1, A2, C1 and C2 are calculated using the lower bound cumulative percent\n# to calculate the lower bound of the median estimate\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\nC2 <- cum_percent[cum_percent_idx_lower, ]\nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n\nFor the category containing p_upper: Define new values for A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the upper bound.\n\n\n# A1, A2, C1 and C2 are calculated using the upper bound cumulative percent\n# to calculate the upper bound of the median estimate\nA1 <- earnings[cum_percent_idx_upper, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_upper + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_upper - 1,]\nC2 <- cum_percent[cum_percent_idx_upper,]\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nUse the lower and upper bounds approximated in steps 6 or 7 to approximate the standard error of the median. SE(median) = 1/2 X (Upper Bound – Lower Bound)\n\n\n# The median earning estimate is the average of the upper and lower bounds\n# of the median estimates calculated above in the if-else block\nmedian_earnings <- 0.5 * (lower_bound + upper_bound)\n    \n# The median SE is half the distance between the upper and lower bounds\n# of the median estimate\nmedian_se <- 0.5 * (upper_bound - lower_bound)\n\n# The 90% confidence interval critical z-score is used to calculate \n# the margin of error\nmedian_90_moe <- 1.645 * median_se\n\n\nReshape the Data\nFinally, a data.frame is returned, which will be displayed in a tableOutput element.\n\n# A data.frame will be displayed in the UI\nmedian_data <- data.frame(\n  \"Estimate\" = median_earnings,\n  \"SE\" = median_se,\n  \"MOE\" = median_90_moe\n)"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "title": "R Shiny Census App",
    "section": "format_query_result.R",
    "text": "format_query_result.R\nThe purpose of this function is to receive two data.frame objects, one for earnings estimate values, and one for the corresponding moe values, and return a single data.frame which is ready to be displayed in a tableOutput.\n\nExtract data.frame Objects from List\nSince get_b20005_ruca_aggregate_earnings returns a named list, I first pull out the estimate and moe data.frame objects:\n\n# Pull out query result data.frames from the list\nestimate <- rs[[\"estimate\"]]\nmoe <- rs[[\"moe\"]]\n\n\nReshape data.frame Objects\nThese data.frame objects have RUCA levels in the column DESCRIPTION and one column for each population estimate. For example, the estimate for Alabama Full Time Female workers looks like this:\n\n\n\n\nDESCRIPTION\n…\nB20005_053E\nB20005_054E\nB20005_055E\n…\n\n\n\n\n1\nLarge Town\n…\n149\n257\n546\n…\n\n\n2\nRural\n…\n75\n66\n351\n…\n\n\n3\nSmall Town\n…\n28\n162\n634\n…\n\n\n4\nUrban\n…\n468\n1061\n4732\n…\n\n\n5\nZero Population\n…\n0\n0\n0\n…\n\n\n\nThe moe data.frame has a similar layout.\nHowever, in the UI, I want the table to look like this:\n\n\n\nPopulation estimates for earnings levels from $1 to $2499 up to $100000 and more for Alabama Full Time Female Workers\n\n\nTo achieve this, I first transpose the estimate and moe data.frames…\n\n# Transpose the query results\ncol_names <- estimate[,\"DESCRIPTION\"]\nestimate <- t(estimate[-1])\ncolnames(estimate) <- col_names\n  \ncol_names <- moe[,\"DESCRIPTION\"]\nmoe <- t(moe[-1])\ncolnames(moe) <- col_names\n…then zip them together, keeping in mind that not all states have tracts designated with all RUCA levels:\n\n# Create a mapping to make column names more computer-readable\nformat_ruca_level <- c(\n  \"Urban\" = \"Urban\", \n  \"Large Town\" = \"Large_Town\", \n  \"Small Town\" = \"Small_Town\", \n  \"Rural\" = \"Rural\",\n  \"Zero Population\" = \"Zero_Population\")\n\n# bind together estimate and corresponding moe columns\n# some states do not have all RUCA levels\n# for example, Connecticut does not have \"Small Town\" tracts\n\n# Create empty objects\noutput_table <- data.frame(temp = matrix(NA, nrow = nrow(estimate), ncol = 0))\ncol_names <- c()\n\nfor (ruca_level in c(\"Urban\", \"Large Town\", \"Small Town\", \"Rural\")) {\n  if (ruca_level %in% colnames(estimate)) {\n    output_table <- cbind(output_table, estimate[,ruca_level], moe[,ruca_level])\n    \n    # paste \"_MOE\" suffix for MOE columns\n    col_names <- c(\n      col_names,\n      format_ruca_level[[ruca_level]],\n      paste0(format_ruca_level[[ruca_level]], \"_MOE\"))\n  }\n}\n\n# Replace old names with more computer-readable names\ncolnames(output_table) <- col_names\n\n\n\nAdd Descriptive Labels\nFinally, merge the output_table data.frame with labels (long form description of the B20005 variables) which are retrieved from the database using the get_b20005_labels function explained later on in this post. Remember that the label is delimited with \"!!\" and the last substring contains earnings ranges (e.g. “$30,000 to $34,999”):\n\n# name rows as long-form labels, by splitting them by '!!' and \n# grabbing the last chunk which has dollar ranges e.g. \n# $30000 to $34999\noutput_table <- merge(output_table, labels, by.x = 0, by.y = \"name\")\nsplit_label <- data.frame(\n  do.call(\n    'rbind', \n    strsplit(as.character(output_table$label),'!!',fixed=TRUE)))\n\nrownames(output_table) <- split_label$X6"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_labels.R",
    "text": "get_b20005_labels.R\nThis script contains two helper functions to retrieve the label column from the b20005_vars table.\n\nGet Earnings Population Estimate Labels\nThe first one, get_b20005_labels retrieves the variable name and label for earning range strings (e.g. “$30,000 to $34,999”):\n\nget_b20005_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    WHERE \n      label LIKE '%$%'\n    ORDER BY name\"\n    )\n  dbDisconnect(census_app_db)\n  return(rs)\n}\n\n\n\nGet All Labels\nThe second function, get_b20005_ALL_labels returns the whole table:\n\nget_b20005_ALL_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    ORDER BY name\"\n  )\n  dbDisconnect(census_app_db)\n  return(rs)\n}"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_tract_earnings.R",
    "text": "get_b20005_tract_earnings.R\nThis function is similar to get_b20005_ruca_aggregate_earnings but does not aggregate by RUCA level, and also includes Census Tracts that are not designated a RUCA level. The label_wildcard is constructed the same way as before.\n\nGet Variable Names\nThe variable names are obtained for both margin of error and estimates in the same query:\n\n # Get b20005 variable names (estimates and moe)\nvars <- dbGetQuery(\n  census_app_db, \n  \"SELECT name FROM b20005_vars \n  WHERE label LIKE $label_wildcard\",\n  params=list(label_wildcard=label_wildcard)\n  )\n\n\n\nJoin Tables\nThe tract-level earnings are queried with the following, using a LEFT JOIN between b20005 and ruca tables to include tracts that do not have a RUCA level.\n\n# Construct query to get tract-level earnings data\nquery_string <- paste(\n  \"SELECT ruca.DESCRIPTION,\n  b20005.state || b20005.county || b20005.tract AS TRACTFIPS,\",\n  paste0(vars$name, collapse=\",\"),\n  \"FROM b20005 \n  LEFT JOIN ruca \n  ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n  WHERE \n  b20005.state LIKE $state\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_states.R",
    "text": "get_b20005_states.R\nThis function retrieves state codes and names from the codes table, and is used to assign choices to selectInput dropdowns. \"United States\" which has a FIPS code of \"00\" is excluded because the b20005 table contains state-level data only. The query result is sorted by the state name so that the dropdown menu choices are in ascending alphabetical order.\nstates <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESCRIPTION, CODE\n  FROM codes \n  WHERE CATEGORY = 'state'\n  AND CODE <> '00'\n  ORDER BY DESCRIPTION\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "title": "R Shiny Census App",
    "section": "get_design_factor.R",
    "text": "get_design_factor.R\nThis function retrieves a single numeric Design Factor for the “Person Earnings/Income” characteristic from the design_factors table for a given state parameter:\n\nrs <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESIGN_FACTOR FROM design_factors\n  WHERE ST = $state\n  AND CHARACTERISTIC = 'Person Earnings/Income'\",\n  params = list(state=state))\n\nrs <- as.numeric(rs[1, \"DESIGN_FACTOR\"])"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "title": "R Shiny Census App",
    "section": "make_plot.R",
    "text": "make_plot.R\nThis is function creates a ggplot.bar_plot object using a given data, RUCA level, and title. The x-axis labels are rotated, both axis labels are resized, and plot title and subtitle are formatted.\n\nmake_plot <- function(data, ruca_level, plot_title){\n  # Prepare x-axis factor for `aes` parameter\n  xs <- rownames(data)\n  xs <- factor(xs, xs)\n\n  bar_plot <- ggplot(\n    data=data,\n    aes(x=xs, y=get(ruca_level))) + \n    geom_bar(stat='identity') + \n\n    theme(\n      # Rotate x-axis labels\n      axis.text.x=element_text(\n        angle = -90, \n        vjust = 0.5, \n        hjust=1, \n        size=12),\n\n      # Resize x-axis labels and move them away from axis\n      axis.title.x=element_text(vjust=-0.75,size=14),\n\n      # Resize y-axis labels\n      axis.text.y=element_text(size=12),\n      axis.title.y=element_text(size=14),\n\n      # Set plot title and subtitle font and placement\n      plot.title = element_text(size = 18, hjust=0.5, face='bold'),\n      plot.subtitle = element_text(size = 12, hjust=0.5)) +\n\n    labs(x=\"Earnings\", y=\"Population Estimate\") + \n    ggtitle(plot_title, subtitle=\"Population Estimate by Earnings Level\")\n\n  return (bar_plot)\n}"
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "",
    "text": "In this blog post, I will work through the textbook Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari. I’ll be posting excerpts (sometimes verbatim), notes (paraphrased excerpts) and my thoughts about the content (and any other related reading that I come across while understanding the textbook topics).\nStarting in Chapter 2, I recreate the plots shown in the textbook first by following their provided code on the supplemental website, and then by recreating it using tidyverse and ggplot. This process gives me practice reading and understanding other people’s code, as well as practice applying tidyverse and ggplot syntax and principles to produce a similar result with different code.\nBefore I get into the reading, I’ll note that I have not posted my solutions to exercises to honor the request of one of the authors, Aki Vehtari:\nHowever, to learn by writing, I will write about the process of doing the exercises, the results I got, and what I learned from it all."
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html#what-you-should-be-able-to-do-after-reading-and-working-through-this-book",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html#what-you-should-be-able-to-do-after-reading-and-working-through-this-book",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "What you should be able to do after reading and working through this book",
    "text": "What you should be able to do after reading and working through this book\n\nPart 1: Review key tools and concepts in mathematics, statistics and computing\n\nChapter 1: Have a sense of the goals and challenges of regression\nChapter 2: Explore data and be aware of issues of measurement and adjustment\nChapter 3: Graph a straight line and know some basic mathematical tools and probability distributions\nChapter 4: Understand statistical estimation and uncertainty assessment, along with the problems of hypothesis testing in applied statistics\nChapter 5: Simulate probability models and uncertainty about inferences and predictions\n\nPart 2: Build linear regression models, use them in real problems, and evaluate their assumptions and fit to data\n\nChapter 6: Distinguish between descriptive and causal interpretations of regression, understanding these in historical context\nChapter 7: Understand and work with simple linear regression with one predictor\nChapter 8: Gain a conceptual understanding of least squares fitting and be able to perform these fits on the computer\nChapter 9: Perform and understand probabilistic and simple Bayesian information aggregation, and be introduced to prior distributions and Bayesian inference\nChapter 10: Build, fit, and understand linear models with multiple predictors\nChapter 11: Understand the relative importance of different assumptions of regression models and be able to check models and evaluate their fit to data\nChapter 12: Apply linear regression more effectively by transforming and combining predictors\n\nPart 3: Build and work with logistic regression and generalized linear models\n\nChapter 13: Fit, understand, and display logistic regression models for binary data\nChapter 14: Build, understand and evaluate logistic regressions with interactions and other complexities\nChapter 15: Fit, understand, and display generalized linear models, including the Poisson and negative binomial regression, ordered logistic regression, and other models\n\nPart 4: Design studies and use data more effectively in applied settings\n\nChapter 16: Use probability theory and simulation to guide data-collection decisions, without falling into the trap of demanding unrealistic levels of certainty\nChapter 17: Use poststratification to generalize from sample to population, and use regression models to impute missing data\n\nPart 5: Implement and understand basic statistical designs and analyses for causal inference\n\nChapter 18: Understand assumptions underlying causal inference with a focus on randomized experiments\nChapter 19: Perform causal inference in simple setting using regressions to estimate treatments and interactions\nChapter 20: Understand the challenges of causal inference from observational data and statistical tools for adjusting for differences between treatment and control groups\nChapter 21: Understand the assumptions underlying more advanced methods that use auxiliary variables or particular data structures to identify causal effects, and be able to fit these models to data\n\nPart 6: Become aware of more advanced regression models\n\nChapter 22: Get a sense of the directions in which linear and generalized linear models can be extended to attack various classes of applied problems\n\nAppendixes\n\nAppendix A: Get started in the statistical software R, with a focus on data manipulation, statistical graphics, and fitting using regressions\nAppendix B: Become aware of some important ideas in regression workflow\n\n\nAfter working through the book, you should be able to fit, graph, understand, and evaluate linear and generalized linear models and use these model fits to make predictions and inferences about quantities of interest, including causal effects of treatments and exposures."
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-1-overview",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-1-overview",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "Chapter 1: Overview",
    "text": "Chapter 1: Overview\nAlternate title: Prediction as a unifying theme in statistics and causal inference\n\n1.1 The three challenges of statistics\n\nGeneralizing from sample to population\nGeneralizing from treatment to support group\nGeneralizing from observed measurements to the underlying constructs of interest\n\nPrediction: for new people or new items that are not in the sample, future outcomes under differently potentially assigned treatments, and underlying constructs of interest, if they could be measured exactly.\nKey skills you should learn from this book:\n\nUnderstanding regression models: mathematical models for predicting an outcome variable from a set of predictors, starting with straight-line fits and moving to various nonlinear generalizations\nConstructing regression models: with many options involving the choice of what variables to include and how to transform and constrain them\nFitting regression models to data\nDisplaying and interpreting the results\n\nInference: using mathematical models to make general claims from particular data.\n\n\n1.2 Why learn regression?\nRegression: a method that allows researchers to summarize how predictions or average values of an outcome vary across individuals defined by a set of predictors.\n\n# load the data\ndata(hibbs)\n\n# make a scatterplot\nplot(\n  hibbs$growth, \n  hibbs$vote, \n  xlab = \"Average recent growth in personal income\",\n  ylab = \"Incumbent party's vote share\")\n\n\n\n\n\n# estimate the regression: y = a + bx + error\nM1 <- stan_glm(vote ~ growth, data=hibbs)\n\n\n# make a scatter plot\nplot(\n  hibbs$growth, \n  hibbs$vote, \n  xlab = \"Average recent growth in personal income\",\n  ylab = \"Incumbent party's vote share\")\n\n# add fitted line to the graph\nabline(coef(M1), col = 'gray')\n\n\n\n\n\n# display the fitted model\nprint(M1)\n\nstan_glm\n family:       gaussian [identity]\n formula:      vote ~ growth\n observations: 16\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 46.4    1.7  \ngrowth       3.0    0.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.9    0.7   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\ny = 46.3 + 3.0x\nMAD: Median Absolute Deviations.\nsigma: the scale of the variation in the data explained by the regression model (the scatter of points above and below the regression line). The linear model predicts vote share roughly to an accuracy of 3.9 percentage points.\nSome of the most important uses of regression:\n\nPrediction: Modeling existing observations or forecasting new data.\nExploring associations: Summarizing how well one variable, or set of variables, predicts the outcome. One can use a model to explore associations, stratifications, or structural relationships between variables.\nExtrapolation: Adjusting for known differences between the sample (observed data) and a population of interest.\nCausal inference: Estimating treatment effects. A key challenge of causal inference is ensuring that treatment and control groups are similar, on average, before exposure to treatment, or else adjusting for differences between these groups.\n\n\n\n1.3 Some examples of regression\n\nEstimating public opinion from an opt-in internet survey\n\nA characteristic problem of big data: a very large sample, relatively inexpensive to collect, but not immediately representative of the larger population\n\n\n\nA randomized experiment on the effect of an educational television program\n\n\nEstimating the effects of United Nations peacekeeping, using pre-treatment variables to adjust for differences between treatment and control groups\n\nSelection bias: perhaps peacekeepers chose the easy cases, which would explain the difference in outcomes. The treatment–peacekeeping–was not randomly assigned. They had an observational study rather than an experiment, where we must do our best to adjust for pre-treatment differences between the treatment and control groups.\nCensoring: certain ranges of data cannot be observed (e.g. countries where civil war had not yet returned by the time data collection ended)\nWhen adjusting for badness, peacekeeping was performed in tougher conditions, on average. As a result, the adjustment increases the estimated beneficial effects of peacekeeping, at least during the study.\n\n\n\nEstimating the effects of gun laws, and the difficulty of inference using regression with a large number of predictors\n\nIn this sort of regression with 50 data points and 30 predictors and no prior information to guide the inference, the coefficient estimates will be hopelessly noisy and compromised by dependence among predictors.\nThe treatments are observational and not externally applied. There is no reason to believe that the big differences in gun-related deaths between states are mostly attributable to particular policies.\n\n\n\nComparing the peacekeeping and gun-control studies\n\nIn both cases, policy conclusions have been drawn from observational data, using regression modeling to adjust for differences between treatment and control groups.\nIt is more practical to perform adjustments when there is a single goal (peacekeeping study)\n\nparticular concern that the UN might be more likely to step in when the situation on the ground was not so bad\nthe data analysis found the opposite\nthe measure of badness is constructed based on particular measured variables and so it is possible that there are important unmeasured characteristics that would cause adjustments to go the other way\n\nThe gun-control model adjusts for many potential causal variables at once\n\nThe effect of each law is estimated conditional on all others being held constant (not realistic—no particular reason for their effects to add up in a simple manner)\nThe comparison is between states, which vary in systematic ways (it is not at all clear that a simple model can hope to adjust for the relevant differences)\nRegression analysis was taken naively to be able to control for variation and give valid causal inference from observational data\n\n\n\n\n\n1.4 Challenges in building, understanding, and interpreting regressions\nTwo different ways in which regression is used for causal inference: estimating a relationship and adjusting for background variables.\n\nRegression to estimate a relationship of interest\nRandomization: a design in which people—or, more generally, experimental units—are randomly assigned to treatment or control groups.\nThere are various ways to attain approximate comparability of treatment and control groups, and to adjust for known or modeled differences between the groups.\nWe assume the comparability of the groups assigned to different treatments so that a regression analysis predicting the outcome given the treatment gives us a direct estimate of the causal effect.\nIt is always possible to estimate a linear model, even if it does not fit the data.\nInteractions: treatment effects that vary as a function of other predictors in a model.\n\nExample: the relation between cancer rate and radon is different for smokers and nonsmokers. The effect of radon is assumed to be linear but with an interaction with smoking.\n\n\n\nRegression to adjust for differences between treatment and control groups\nIn most real-world causal inference problems, there are systematic differences between experimental units that receive treatment and control. In such settings it is important to adjust for pre-treatment differences between the groups, and we can use regression to do this.\nAdjusting for background variables is particularly important when there is imbalance so that the treated and control groups differ on key pre-treatment predictors.\nThe estimated treatment effect is necessarily model based.\n\n\nInterpreting coefficients in a predictive model\nA model fit to survey data: earnings = 11000 + 1500 * (height - 60) + error, with errors in the range of mean of 0 and standard deviation of 22000.\n\nPrediction: useless for forecasting because errors are too large\nExploring an association: best fit for this example, as the estimated slope is positive, can lead to further research to study reasons taller people earn more than shorter people\nSampling inference: the regression coefficients can be interpreted directly to the extent that people in the survey are a representative sample of the population of interest\nCausal inference: height is not a randomly assigned treatment. Tall and short people may differ in many other ways.\n\n\n\nBuilding, interpreting and checking regression models\nStatistical analysis cycles through four steps:\n\nModel building\nModel fitting\nUnderstanding model fits\nCriticism\n\nAccept that we can learn from statistical analysis—we can generalize from sample to population, from treatment to control, and from observed measurements to underlying constructs of interest—even while these inferences can be flawed.\nOverinterpretation of noisy data: the gun control study took existing variations among states and too eagerly attributed it to available factors.\nNo study is perfect.\nWe should recognize challenges in extrapolation and then work to adjust for them.\n\n\n\n1.5 Classical and Bayesian Inference\nThree concerns related to fitting models to data and using models to make predictions:\n\nwhat information is being used in the estimation process\nwhat assumptions are being made\nhow estimates and predictions are interpreted, in a classical or Bayesian framework\n\n\nInformation\n\nThe starting point for any regression problem is data on an outcome variable y and one or more predictors x.\nInformation should be available on what data were observed at all\nWe typically have prior knowledge coming from sources other than the data at hand. Where local data are weak it would be foolish to draw conclusions without using prior knowledge\n\n\n\nAssumptions\n\nThree sorts of assumptions that are essential to any regression model of an outcome y given predictors x.\n\nThe functional form of the relation between x and y\nWhere the data comes from (strongest assumptions tend to be simple and easy to understand, weaker assumptions, being more general, can also be more complicated)\nReal-world relevance of the measured data\n\nThe interpretation of a regression of y on x depends also on the relation between the measured x and the underlying predictors of interest, and on the relation between the measured y and the underlying outcomes of interest\n\n\n\n\n\nClassical inference\nThe traditional approach to statistical analysis is based on summarizing the information in the data, not using prior information, but getting estimates and predictions that have well-understood statistical properties, low bias and low variance.\nUnbiasedness: estimates should be true on average\nCoverage: confidence intervals should cover the true parameter value 95% of the time\nConservatism: sometimes data are weak and we can’t make strong statements, but we’d like to be able to say, at least approximately, that our estimates are unbiased and our intervals have the advertised coverage.\nIn classical statistics there should be a clear and unambiguous (“objective”) path from data to inferences, which in turn should be checkable, at least in theory, based on their frequency properties.\n\n\nBayesian inference\nIncorporates prior information into inferences, going beyond the goal of merely summarizing existing data. The analysis gives more reasonable results and can be used to make direct predictions about future outcomes and about the results of future experiments.\nThe prior distribution represents the arena over which any predictions will be evaluated.\nWe have a choice: classical inference, leading to pure summaries of data which can have limited value as predictions; or Bayesian inference, which in theory can yield valid predictions even with weak data, but relies on additional assumptions.\nAll Bayesian inferences are probabilistic and thus can be represented by random simulations. For this reason, whenever we want to summarize uncertainty in estimation beyond simple confidence intervals, and whenever we want to use regression models for predictions, we go Bayesian.\n\n\n\n1.6 Computing least squares and Bayesian regression\nIn general, we recommend using Bayesian inference for regression\n\nIf prior information is available, you can use it\nif not, Bayesian regression with weakly informative default priors still has the advantage of yielding stable estimates and producing simulations that enable you to express inferential and predictive uncertainty (estimates with uncertainties and probabilistic predictions or forecasts)\n\nBayesian regression in R:\nfit <- stan_glm(y ~ x, data=mydata)\n\n# stan_glm can run slow for large datasets, make it faster by running in optimizing mode\nfit <- stan_glm(y ~ x, data=mydata, algorithm=\"optimizing\")\nleast squares regression (classical statistics)\nfit <- lm(y ~ x, data=mydata)\nBayesian and simulation approaches become more important when fitting regularized regression and multilevel models.\n\n\n1.8 Exercises\nIt took me 8-9 days to complete the 10 exercises at the end of this chapter. A few observations:\n\nExperiment design, even with simple paper helicopters, still requires time, effort and attention to detail. Even then, a well thought out and executed plan can still end up with uninspiring results. It is therefore the process of experimentation and not the result where I learned the most. It was important to continue experimentation (as it resulted in finding a high-performing copter) even after my initial approach didn’t seem to work as I had thought it would.\nFinding good and bad examples of research is hard on your own! I relied on others existing work where they explicitly called out (and explained) good and bad research, sometimes with great detail, in order to answer exercises 1.7 and 1.8.\nThink about the problem in real life! I don’t know if my answer to 1.9 is correct, but it helped to think about the physical phenomenon that was taking place (a helicopter falling through air, pulled by gravity) in order to come up with a better model fit than the example provided. I found that thinking of physical constraints of the real world problem helped narrow the theoretical model form.\nPick something you’re interested in and is worth your while! For exercise 1.10, the dataset I have chosen is the NFL Play-by-Play dataset, as I’m already working on a separate project where I’ll be practicing writing SQL queries to analyze the data. Play-by-play data going back to 1999 will provide me with ample opportunities to pursue different modeling choices. My interest in football will keep me dedicated through the tough times."
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-2-data-and-measurement",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-2-data-and-measurement",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "Chapter 2: Data and Measurement",
    "text": "Chapter 2: Data and Measurement\nIn this book we will be:\n\nfitting lines and curves to data.\nmaking comparisons and predictions and assessing our uncertainties in the resulting inferences.\ndiscuss the assumptions underlying regression models, methods for checking these assumptions, and directions for improving fitted models.\ndiscussing the challenges of extrapolating from available data to make causal inferences and predictions for new data.\nusing computer simulations to summarize the uncertainties in our estimates and predictions.\n\n\n2.1 Examining where data comes from\nI’ll recreate the charts shown in the textbook by following the code provided in the website supplement to the book, using tidyverse where it makes sense:\n\n# load the Human Development Index data\n# start using DT to produce better table outputs\nlibrary(DT)\nhdi <- read.table(\"../../../ROS_data/HDI/data/hdi.dat\", header=TRUE)\nDT::datatable(head(hdi))\n\n\n\n\n\n\n\n# load income data via the votes data\nlibrary(foreign)\nvotes <- read.dta(\"../../../ROS_data/HDI/data/state vote and income, 68-00.dta\")\nDT::datatable(head(votes), options=list(scrollX=TRUE))\n\n\n\n\n\n\n\n# preprocess\nincome2000 <- votes[votes[,\"st_year\"]==2000, \"st_income\"]\n\n# it seems like the income dataset doesn't have DC\n# whereas HDI does\n# so we're placing an NA for DC in the income list\nstate.income <- c(income2000[1:8], NA, income2000[9:50])\nstate.abb.long <- c(state.abb[1:8], \"DC\", state.abb[9:50])\nstate.name.long <- c(state.name[1:8], \"Washington, D.C.\", state.name[9:50])\nhdi.ordered <- rep(NA, 51)\ncan <- rep(NA, 51)\n\nfor (i in 1:51) {\n  ok <- hdi[,\"state\"]==state.name.long[i]\n  hdi.ordered[i] <- hdi[ok, \"hdi\"]\n  can[i] <- hdi[ok, \"canada.dist\"]\n}\nno.dc <- state.abb.long != \"DC\"\n\n\n# plot average state income and Human Development Index\npar(mar=c(3, 3, 2.5, 1), mgp=c(1.5, .2, 0), tck=-0.01, pty='s')\nplot(\n  state.income, \n  hdi.ordered, \n  xlab=\"Average state income in 2020\",\n  ylab=\"Human Development Index\",\n  type=\"n\")\n\ntext(state.income, hdi.ordered, state.abb.long)\n\n\n\n\nThe pattern between HDI numbers and average state income is strong and nonlinear. If anyone is interested in following up on this, we suggest looking into South Carolina and Kentucky, which are so close in average income and so far apart on HDI.\n\n# plot rank of average state income and Human Development Index\npar(mar=c(3, 3, 2.5, 1), mgp=c(1.5, 0.2, 0), tck=-0.01, pty='s')\nplot(\n  rank(state.income[no.dc]), \n  rank(hdi.ordered[no.dc]),\n  xlab=\"Rank of average state income in 2000\",\n  ylab=\"Rank of Human Development Index\", \n  type=\"n\")\n\ntext(rank(state.income[no.dc]), rank(hdi.ordered[no.dc]), state.abb)\n\n\n\n\nThere is a high correlation between the ranking of HDI and state income.\nI will redo the above data processing and plotting with tidyverse and ggplot for practice:\n\n# plot average state income and Human Development Index\n# using tidyverse and ggplot\nmerged_data <- hdi %>% dplyr::left_join(\n  votes %>% filter(st_year==2000),\n  by = c(\"state\" = \"st_state\")\n) \n\np <- ggplot(\n  merged_data,\n  aes(x = st_income, y = hdi, label =  st_stateabb)\n) + theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    panel.background = element_rect(fill = 'white'),\n    aspect.ratio = 1\n  ) +\n  labs(\n    x = \"Average state income in 2020\",\n    y = \"Human Development Index\"\n  )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\np + geom_text()\n\nWarning: Removed 1 rows containing missing values (`geom_text()`).\n\n\n\n\n\nNone of the tidyverse methods I found (row_number, min_rank, dense_rank, percent_rank, cume_dist, or ntile) work quite like rank (where “ties” are averaged), so I continued using rank for this plot:\n\n# plot rank of average state income and Human Development Index\np <- ggplot(\n  merged_data %>% filter(state != 'Washington, D.C.'), \n  aes(x = rank(st_income), y = rank(hdi), label = st_stateabb),\n  ) + theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    panel.background = element_rect(fill = 'white'),\n    aspect.ratio = 1\n  ) +\n  labs(\n    x = \"Rank of average state income in 2000\",\n    y = \"Rank of Human Development Index\"\n  )\n\np + geom_text()\n\n\n\n\nThe relevance of this example is that we were better able to understand the data by plotting them in different ways.\n\nDetails of measurement can be important\nIn the code chunks below, I will recreate the plots in Figure 2.3 in the text (distribution of political ideology by income and distribution of party identification by income)., using their code provided on the supplemental website.\n\npew_data_path <- file.path(data_root_path, \"Pew/data/pew_research_center_june_elect_wknd_data.dta\")\npew_pre <- read.dta(pew_data_path)\nn <- nrow(pew_pre)\nDT::datatable(head(pew_pre), options=list(scrollX=TRUE))\n\n\n\n\n\n\n\n# weight\npop.weight0 <- pew_pre$weight\n\nhead(unique(pop.weight0))\n\n[1] 1.326923 0.822000 0.493000 0.492000 2.000000 1.800000\n\n\n\n# income (1-9 scale)\ninc <- as.numeric(pew_pre$income)\n\n# remove \"dk/refused\" income level\ninc[inc==10] <- NA\n\nlevels(pew_pre$income)\n\n [1] \"less than $10,000\" \"$10,000-$19,999\"   \"$20,000-$29,999\"  \n [4] \"$30,000-$39,999\"   \"$40,000-$49,000\"   \"$50,000-$74,999\"  \n [7] \"$75,000-$99,999\"   \"$100,000-$149,999\" \"$150,000+\"        \n[10] \"dk/refused\"       \n\nunique(inc)\n\n [1]  6  8 NA  4  7  5  9  1  3  2\n\n\n\n# I believe these are the midpoints of the income levels\nvalue.inc <- c(5,15,25,35,45,62.5,87.5,125,200)\n\n# maximum income value\nn.inc <- max(inc, na.rm = TRUE)\nn.inc\n\n[1] 9\n\n\n\n# party id\npid0 <- as.numeric(pew_pre[,\"party\"])\nlean0 <- as.numeric(pew_pre[,\"partyln\"])\n\nlevels(pew_pre[,\"party\"])\n\n[1] \"missing/not asked\" \"republican\"        \"democrat\"         \n[4] \"independent\"       \"no preference\"     \"other\"            \n[7] \"dk\"               \n\nlevels(pew_pre[,\"partyln\"])\n\n[1] \"missing/not asked\" \"lean republican\"   \"lean democrat\"    \n[4] \"other/dk\"         \n\nunique(pid0)\n\n[1]  3  2  4  7  5  6 NA\n\nunique(lean0)\n\n[1] NA  3  4  2\n\n\n\n# assigning new integers for party id\npid <- ifelse(pid0==2, 5,  # Republican\n         ifelse(pid0==3, 1,  # Democrat\n         ifelse(lean0==2, 4, # Lean Republican\n         ifelse(lean0==4, 2, # Lean Democrat\n         3)))) # Independent\n\npid.label <- c(\"Democrat\", \"Lean Dem.\", \"Independent\", \"Lean Rep.\", \"Republican\")\nn.pid <- max(pid, na.rm=TRUE)\n\n\n# ideology\nideology0 <- as.numeric(pew_pre[,\"ideo\"])\n\nlevels(pew_pre[,\"ideo\"])\n\n[1] \"missing/not asked\" \"very conservative\" \"conservative\"     \n[4] \"moderate\"          \"liberal\"           \"very liberal\"     \n[7] \"dk/refused\"       \n\nunique(ideology0)\n\n[1]  5  4  3  6  2  7 NA\n\n\n\n# assign new integers for ideology\nideology <- ifelse(ideology0==2, 5, # Very conservative\n              ifelse(ideology0==3, 4, # Conservative\n              ifelse(ideology0==6, 1, # Very liberal\n              ifelse(ideology0==5, 2, # Liberal\n              3)))) # Moderate\nideology.label <- c(\"Very liberal\", \"Liberal\", \"Moderate\", \"Conservative\", \"Very conservative\")\nn.ideology <- max(ideology, na.rm=TRUE)\n\n\n# plot settings\npar(mar=c(3,2,2.5,1), mgp=c(1.5, .7, 0), tck=-0.01)\n\n# this creates an empty plot\nplot(c(1,n.inc), c(0,1), xaxs=\"i\", yaxs=\"i\", type=\"n\", xlab=\"\", ylab=\"\", xaxt=\"n\", yaxt=\"n\")\n\n# add x-axis tick marks with empty string labels\naxis(1, 1:n.inc, rep(\"\", n.inc))\n\n# add x-axis labels\naxis(1, seq(1.5, n.inc-.5, length=3), c(\"Low income\", \"Middle income\", \"High income\"), tck=0)\n\n# add y-axis tick marks and labels\naxis(2, c(0, 0.5, 1), c(0, \"50%\", \"100%\"))\n\ncenter <- floor((1 + n.inc) / 2)\n\nincprop <- array(NA, c(n.pid + 1, n.inc))\n\nincprop[n.pid + 1,] <- 1\n\n\nfor (i in 1:n.pid) {\n  for (j in 1:n.inc) {\n    incprop[i, j] <- weighted.mean((pid<i)[inc==j], pop.weight0[inc==j], na.rm = TRUE)\n  }\n}\n\nfor (i in 1:n.pid){\n  polygon(c(1:n.inc, n.inc:1), c(incprop[i,], rev(incprop[i+1,])), col=paste(\"gray\", 40+10*i, sep=\"\"))\n  lines(1:n.inc, incprop[i,])\n  text(center, mean(incprop[c(i,i+1),center]), pid.label[i])\n}\nmtext(\"Self-declared party identification, by income\", side=3, line=1, cex=1.2)\n\n\n\n\n\npar(mar=c(3,2,2.5,1), mgp=c(1.5,.7,0), tck=-.01)\nplot(c(1,n.inc), c(0,1), xaxs=\"i\", yaxs=\"i\", type=\"n\", xlab=\"\", ylab=\"\", xaxt=\"n\", yaxt=\"n\")\naxis(1, 1:n.inc, rep(\"\",n.inc))\naxis(1, seq(1.5,n.inc-.5,length=3), c(\"Low income\", \"Middle income\", \"High income\"), tck=0)\naxis(2, c(0,.5,1), c(0,\"50%\",\"100%\"))\ncenter <- floor((1+n.inc)/2)\nincprop <- array(NA, c(n.ideology+1,n.inc))\nincprop[n.ideology+1,] <- 1\nfor (i in 1:n.ideology){\n  for (j in 1:n.inc){\n    incprop[i,j] <- weighted.mean((ideology<i)[inc==j], pop.weight0[inc==j], na.rm=TRUE)\n  }\n}\nfor (i in 1:n.ideology){\n  polygon(c(1:n.inc, n.inc:1), c(incprop[i,], rev(incprop[i+1,])), col=paste(\"gray\", 40+10*i, sep=\"\"))\n  lines(1:n.inc, incprop[i,])\n  text(center, mean(incprop[c(i,i+1),center]), ideology.label[i])\n}\nmtext(\"Self-declared political ideology, by income\", side=3, line=1, cex=1.2)\n\n\n\n\nI’ll recreate the two plots using tidyverse and ggplot. I referenced this article by the Pew Research Center which shows how to calculated weighted estimates.\n\npew_pre <- pew_pre %>%\n  mutate(\n    # create an integer column for income levels\n    inc = as.numeric(income),\n    # remove \"dk/refuse\" value for income\n    inc = case_when(\n      inc == 10 ~ NA,\n      TRUE ~ inc\n    ),\n    # set political party integer column\n    pid0 = as.numeric(party),\n    # set lean integer column\n    lean0 = as.numeric(partyln),\n    # set ideology integer column\n    ideology0 = as.numeric(ideo),\n    # re-assign party values\n    pid = case_when(\n      pid0 == 2 ~ 5, # Repubican\n      pid0 == 3 ~ 1, # Democrat\n      lean0 == 2 ~ 4, # Lean Republican\n      lean0 == 4 ~ 2, # Lean Democrat\n      is.na(pid0) ~ NA,\n      TRUE ~ 3 # Independent\n    ),\n    # reassign ideology values\n    ideology = case_when(\n      ideology0 == 2 ~ 5, # Very conservative\n      ideology0 == 3 ~ 4, # Conservative\n      ideology0 == 6 ~ 1, # Very liberal\n      ideology0 == 5 ~ 2, # Liberal\n      is.na(ideology0) ~ NA,\n      TRUE ~ 3 # Moderate\n    )\n  )\n\n# constants\nn_inc <- max(pew_pre$inc, na.rm = TRUE)\nn_pid <- max(pew_pre$pid, na.rm = TRUE)\nn_ideology <- max(pew_pre$ideology, na.rm = TRUE)\n\n\n# calculate income proportions using population weight\ninc_totals <- pew_pre %>% group_by(\n  inc\n) %>% summarize(\n  total_inc = n()\n)\n\npid_weighted_estimates <- pew_pre %>% dplyr::left_join(\n  inc_totals\n) %>% group_by(\n  inc,\n  pid\n) %>% summarise(\n  weighted_n = sum(weight)\n) %>% mutate(\n  weighted_group_size = sum(weighted_n),\n  weighted_estimate = weighted_n / weighted_group_size\n) %>% arrange(\n  inc\n)\n\nJoining with `by = join_by(inc)`\n`summarise()` has grouped output by 'inc'. You can override using the `.groups`\nargument.\n\n\n\nDT::datatable(pid_weighted_estimates)\n\n\n\n\n\n\n\nggplot(pid_weighted_estimates, aes(x=inc, y=weighted_estimate)) + \n  geom_area(aes(group = pid, fill = pid), position = position_stack(reverse = TRUE), show.legend = FALSE) +\n  annotate(\"text\", x=5, y=.2, label=\"Democrat\", color=\"white\") + \n  annotate(\"text\", x=5, y=.425, label=\"Lean Dem.\", color=\"white\") + \n  annotate(\"text\", x=5, y=.55, label=\"Independent\", color=\"white\") + \n  annotate(\"text\", x=5, y=.68, label=\"Lean Rep.\") + \n  annotate(\"text\", x=5, y=.87, label=\"Republican\") + \n  scale_x_continuous(\"Average Income Level\", breaks=c(1,5,9), labels = c(\"Low Income\", \"Middle Income\", \"High Income\"), , expand = c(0, 0)) +\n  scale_y_continuous(\"Proportion of Sample\", breaks=c(0,.5, 1), labels = c(\"0%\", \"50%\", \"100%\"), expand = c(0, 0)) +\n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  ggtitle(\"Party Identification by Income Levels\")\n\nWarning: Removed 6 rows containing non-finite values (`stat_align()`).\n\n\n\n\n\n\n# ideology data prep\nideo_weighted_estimates <- pew_pre %>% dplyr::left_join(\n  inc_totals\n) %>% group_by(\n  inc,\n  ideology\n) %>% summarise(\n  weighted_n = sum(weight)\n) %>% mutate(\n  weighted_group_size = sum(weighted_n),\n  weighted_estimate = weighted_n / weighted_group_size\n) %>% arrange(\n  inc\n)\n\nJoining with `by = join_by(inc)`\n`summarise()` has grouped output by 'inc'. You can override using the `.groups`\nargument.\n\n\n\nDT::datatable(ideo_weighted_estimates)\n\n\n\n\n\n\n\nggplot(ideo_weighted_estimates, aes(x=inc, y=weighted_estimate)) + \n  geom_area(aes(group = ideology, fill = ideology), position = position_stack(reverse = TRUE), show.legend = FALSE) +\n  annotate(\"text\", x=5, y=.03, label=\"Very Liberal\", color=\"white\", size=3) + \n  annotate(\"text\", x=5, y=.13, label=\"Liberal\", color=\"white\", size=3) + \n  annotate(\"text\", x=5, y=.4, label=\"Moderate\", color=\"white\", size=3) + \n  annotate(\"text\", x=5, y=.75, label=\"Conservative\", size=3) + \n  annotate(\"text\", x=5, y=.95, label=\"Very Conservative\", size=3) + \n  scale_x_continuous(\"Average Income Level\", breaks=c(1,5,9), labels = c(\"Low Income\", \"Middle Income\", \"High Income\"), , expand = c(0, 0)) +\n  scale_y_continuous(\"Proportion of Sample\", breaks=c(0,.5, 1), labels = c(\"0%\", \"50%\", \"100%\"), expand = c(0, 0)) +\n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  ggtitle(\"Ideology by Income Levels\")\n\nWarning: Removed 6 rows containing non-finite values (`stat_align()`).\n\n\n\n\n\nOverall, I think the plots I made look pretty similar to the textbook’s. The main bottleneck was learning how to calculate proportions based on weighted estimates. Once that took shape, I was able to figure out how to present the data visually in a way that resembles the authors’ approach.\nThe figure “Ideology by Income Levels” shows that the proportion of political liberals, moderates, and conservatives is about the same for all income levels. The figure “Party Identification by Income Levels” shows a strong relation between income and Republican partisanship, at least as of 2008. The partisanship and ideology example is a reminder that even very similar measures can answer quite different questions.\n\n\n\n2.2 Validity and reliability\nData analysis reaches a dead end if we have poor data.\nTwo reasons for discussing measurement:\n\nwe need to understand what our data actually mean, otherwise we cannot extract the right information\nlearning about accuracy, reliability and validity will set a foundation for understanding variance, correlation, and error, which will be useful in setting up linear models in the forthcoming chapters\n\nThe property of being precise enough is a combination of the properties of the scale and what we are trying to use it for.\nIn social science, the way to measure what we are trying to measure is not as transparent as it is in everyday life.\nOther times, the thing we are trying to measure is pretty straightforward, but a little bit fuzzy, and the ways to tally it up aren’t obvious.\nSometimes we are trying to measure something that we all agree has meaning, but which is subjective for every person and does not correspond to a “thing” we can count or measure with a ruler.\nAttitudes are private; you can’t just weigh them or measure their widths.\nIt can be helpful to take multiple measurements on an underlying construct of interest.\n\nValidity\nA measure is valid to the degree that it represents what you are trying to measure. Asking people how satisfied they are with some government service might not be considered a valid measure of the effectiveness of that service.\nValid measures are ones in which there is general agreement that the observations are closely related to the intended construct.\nvalidity: the property of giving the right answer on average across a wide range of plausible scenarios. To study validity in an empirical way, ideally you want settings in which there is an observable true value and multiple measurements can be taken.\n\n\nReliability\nA reliable measure is one that is precise and stable. We would hope the variability in our sample is due to real differences among people or things, and not due to random error incurred during the measurement process.\n\n\nSample selection\nselection: the idea that the data you see may be a nonrepresentative sample of a larger population you will not see.\nIn addition to selection bias, there are also biases from nonresponse to particular survey items, partially observed measurements, and choices in coding and interpretation of data.\n\n\n\n2.3 All graphs are comparisons\n\nSimple scatterplots\n\nhealth_data_path = file.path(data_root_path, \"HealthExpenditure/data/healthdata.txt\")\nhealth <- read.table(health_data_path, header=TRUE)\nDT::datatable(head(health))\n\n\n\n\n\n\n\npar(mgp=c(1.7,.5,0), tck=-.01, mar=c(3,3,.1,.1))\nggplot(data=health, aes(x=spending, y=lifespan, label=country)) +\n  geom_text() + \n  labs(x=\"Health care spending (PPP US$)\",y=\"Life expectancy (years)\") + \n  theme(\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    panel.background = element_rect(fill = 'white'),\n    aspect.ratio = 0.5\n  ) \n\n\n\n\nThe graph shows the exceptional position of the United States (highest health care spending, low life expectancy) and also shows the relation between spending and lifespan in other countries.\n\n\nDisplaying more information on the graph\nAt least in theory, you can display five variables easily with a scatterplot:\n\nx position\ny position\nsymbol\nsymbol size\nsymbol color\n\nWe prefer clearly distinguishable colors or symbols such as open circles, solid circles, crosses and dots. When a graph has multiple lines, label them directly.\n\n\nMultiple plots\nLooking at data in unexpected ways can lead to discovery.\nWe can learn by putting multiple related graphs in a single display.\nThere is no single best way to display a dataset.\nIn the code chunks below I’ll recreate plots 2.6, 2.7, 2.8 and 2.9 using their code provided in the supplemental website.\n\nallnames_path = file.path(data_root_path, \"Names/data/allnames_clean.csv\")\nallnames <- read.csv(allnames_path)\nDT::datatable(head(allnames), options=list(scrollX=TRUE))\n\n\n\n\n\n\n\n# create a vector identifying which rows have Female names (TRUE) and which have Male names (FALSE)\ngirl <- as.vector(allnames$sex) == \"F\"\n\n# Since there is a `names` base R function, I'm renaming the names vector to names_vec\nnames_vec <- as.vector(allnames$name)\n\n\n# plot data\n\n# calculate the number of characters in each name\nnamelength <- nchar(names_vec)\n\n# create a vector of last letters of each name\nlastletter <- substr(names_vec, namelength, namelength)\n\n# create a vector of first letters of each name\nfirstletter <- substr(names_vec, 1, 1)\n\n# plotting function\n# i am renaming it to names_histogram since there is an `arm` package function called `discrete.histogram`\nnames_histogram <- function(x, prob, prob2=NULL, xlab=\"x\", ylab=\"Probability\", xaxs.label=NULL, yaxs.label=NULL, bar.width=NULL, ...) {\n  if (length(x) != length(prob)) stop()\n  \n  x.values <- sort (unique(x))\n  n.x.values <- length (x.values)\n  gaps <- x.values[2:n.x.values] - x.values[1:(n.x.values-1)]\n  if (is.null(bar.width)) bar.width <- min(gaps)*.2\n  par(mar=c(3,3,2,2), mgp=c(1.7,.3,0), tck=0)\n  plot(range(x)+c(-1,1)*bar.width, c(0,max(prob)),\n    xlab=xlab, ylab=ylab, xaxs=\"i\", xaxt=\"n\",  yaxs=\"i\",\n    yaxt=ifelse(is.null(yaxs.label),\"s\",\"n\"), bty=\"l\", type=\"n\", ...)\n  if (is.null(xaxs.label)){\n    axis(1, x.values)\n  }\n  else {\n    n <- length(xaxs.label[[1]])\n    even <- (1:n)[(1:n)%%2==0]\n    odd <- (1:n)[(1:n)%%2==1]\n    axis(1, xaxs.label[[1]][even], xaxs.label[[2]][even], cex.axis=.9)\n    axis(1, xaxs.label[[1]][odd], xaxs.label[[2]][odd], cex.axis=.9)\n  }\n  if (!is.null(yaxs.label)){\n    axis(2, yaxs.label[[1]], yaxs.label[[2]], tck=-.02)\n  }\n  for (i in 1:length(x)){\n    polygon(x[i] + c(-1,-1,1,1)*bar.width/2, c(0,prob[i],prob[i],0),\n      border=\"gray10\", col=\"gray10\")\n    if (!is.null(prob2))\n      polygon(x[i] + c(-1,-1,1,1)*bar.width/10, c(0,prob2[i],prob2[i],0),\n        border=\"red\", col=\"black\")\n  }\n}\n\nfor (year in c(1900,1950,2010)){\n  # get the column X1900, X1950 and X2010 from the data.frame `allnames`\n  thisyear <- allnames[,paste(\"X\",year,sep=\"\")]\n  # create 2D empty arrays to store Male and Female last and first letter of names\n  lastletter.by.sex <- array(NA, c(26,2))\n  firstletter.by.sex <- array(NA, c(26,2))\n  # construct the 2D arrays of Male/Female last/first letter of names\n  for (i in 1:26){\n    lastletter.by.sex[i,1] <- sum(thisyear[lastletter==letters[i] & girl])\n    lastletter.by.sex[i,2] <- sum(thisyear[lastletter==letters[i] & !girl])\n    firstletter.by.sex[i,1] <- sum(thisyear[firstletter==LETTERS[i] & girl])\n    firstletter.by.sex[i,2] <- sum(thisyear[firstletter==LETTERS[i] & !girl])\n  }\n  \n  # plot the histogram for the given year\n  names_histogram(1:26, 100*(lastletter.by.sex[,2])/sum(lastletter.by.sex[,2]), xaxs.label=list(1:26,letters), yaxs.label=list(seq(0,30,10),seq(0,30,10)), xlab=\"\", ylab=\"Percentage of boys born\", main=paste(\"Last letter of boys' names in\", year), cex.axis=.9, cex.main=.9, bar.width=.8)\n  for (y in c(10,20,30)) abline (y,0,col=\"gray\",lwd=.5)\n  \n  names_histogram(1:26, 100*(lastletter.by.sex[,1])/sum(lastletter.by.sex[,1]), xaxs.label=list(1:26,letters), yaxs.label=list(seq(0,30,10),seq(0,30,10)), xlab=\"\", ylab=\"Percentage of girls born\", main=paste(\"Last letter of girls' names in\", year), cex.main=.9)\n  # adding the horizontal grid lines for the girls plots\n  for (y in c(10,20,30)) abline (y,0,col=\"gray\",lwd=.5)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ll recreate similar plots using tidyverse and ggplot.\nFirst, I’ll make the data long (instead of it’s current wide shape) so that I can aggregate the data more easily for plotting:\n\n# reshape the data to make it longer\nallnames_long <- allnames %>% rename(\n  id = 'X'\n) %>% pivot_longer(\n  starts_with(\"X\"),\n  names_to = \"year\",\n  values_to = \"name_n\"\n) %>% mutate(\n  # remove X at start of the four-digit year\n  year = str_remove(year, \"X\"),\n  # extract last letter of each name\n  last_letter = str_sub(name, -1)\n) \n\nDT::datatable(head(allnames_long))\n\n\n\n\n\n\nI then aggregate the data by year, sex and last letter, and calculate the count and percentage of last letters in each year for each sex:\n\nallnames_agg <- allnames_long %>% group_by(\n  year,\n  sex,\n  last_letter\n) %>% summarise(\n  last_letter_n = sum(name_n)\n) %>% mutate(\n  total_n = sum(last_letter_n),\n  last_letter_pct = last_letter_n / total_n\n)\n\n`summarise()` has grouped output by 'year', 'sex'. You can override using the\n`.groups` argument.\n\nDT::datatable(head(allnames_agg, n=30))\n\n\n\n\n\n\nI then plot one year’s data to make sure I’m getting the same result as the textbook:\n\nggplot(allnames_agg %>% filter(year == \"1900\", sex == \"F\"), aes(x = last_letter, y = last_letter_pct)) + \n  geom_bar(stat=\"identity\", fill=\"black\") + \n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  scale_x_discrete(\"Last Letter of Name\", expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of Total\", limits=c(0,0.4), breaks=c(0,.1,.2,.3,.4), labels=c(\"0%\", \"10%\", \"20%\", \"30%\", \"40%\"), expand = c(0, 0)) +\n  ggtitle(\"Last Letter of Girls' Names in 1900\")\n\n\n\n\nFinally I’ll plots histograms to match figures 2.6 and 2.7:\n\n# plotting function \nplot_last_letter_distribution <- function(year_arg, sex_arg) {\n  # prepare string for plot title\n  title_sex <- if(sex_arg == 'F') \"Girls'\" else \"Boys'\"\n  title_str <- paste(\"Last Letter of\", title_sex, \"in\", year_arg)\n  \n  # prep data\n  plot_data <- allnames_agg %>% filter(year == year_arg, sex == sex_arg)\n  \n  # calculate y-axis limits, break points and labels\n  # calculate max y limit to 0.05 more than the nearest 0.05\n  limits_max <- round(max(plot_data$last_letter_pct)/0.05) * 0.05 + 0.05\n  \n  # calculate y-axis breaks\n  breaks_val = seq(0, limits_max, by=0.05)\n  \n  # calculate y-axis labels\n  labels_val = c()\n  \n  for (val in breaks_val) {\n    labels_val <- append(labels_val, paste0(val*100, \"%\"))\n  }\n  \n  print(ggplot(plot_data, aes(x = last_letter, y = last_letter_pct)) + \n  geom_bar(stat=\"identity\", fill=\"black\") + \n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  scale_x_discrete(\"Last Letter of Name\", expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of Total\", limits = c(0, limits_max), breaks = breaks_val, labels = labels_val, expand = c(0, 0)) +\n  ggtitle(title_str)\n  )\n}\n\nfor (year in c(1906, 1956, 2006)) {\n  plot_last_letter_distribution(year, sex = 'M')\n}\n\n\n\n\n\n\n\n\n\n\nThe following code chunks recreate Figure 2.8 using their code provided on the supplemental website:\n\n# define full range of years in the dataset\nyrs <- 1880:2010\n\n# total number of years\nn.yrs <- length(yrs)\n\n# create empyt 3D arrays to hold last and first letter frequencies\nlastletterfreqs <- array(NA, c(n.yrs,26,2))\nfirstletterfreqs <- array(NA, c(n.yrs,26,2))\n\n# define the names of each dimension of the letter frequency 3D arrays\n# as the list of years, letters and the list c(\"girls\", \"boys\")\ndimnames(lastletterfreqs) <- list(yrs, letters, c(\"girls\",\"boys\"))\ndimnames(firstletterfreqs) <- list(yrs, letters, c(\"girls\",\"boys\"))\n\n# construct the arrays lastletterfreqs and fistletterfreqs\nfor (i in 1:n.yrs){\n  thisyear <- allnames[,paste(\"X\",yrs[i],sep=\"\")]\n  for (j in 1:26){\n    # sum of last letters for girls\n    lastletterfreqs[i,j,1] <- sum(thisyear[lastletter==letters[j] & girl])\n    \n    # sum of last letters for boys\n    lastletterfreqs[i,j,2] <- sum(thisyear[lastletter==letters[j] & !girl])\n    \n    # sum o first letters for girls\n    firstletterfreqs[i,j,1] <- sum(thisyear[firstletter==LETTERS[j] & girl])\n    \n    # sum of laster letters for boys\n    firstletterfreqs[i,j,2] <- sum(thisyear[firstletter==LETTERS[j] & !girl])\n  }\n  for (k in 1:2){\n    # percentage of each last letter (of all last letters that year)\n    lastletterfreqs[i,,k] <- lastletterfreqs[i,,k]/sum(lastletterfreqs[i,,k])\n    \n    # percentage of each first letter (of all first letters that year)\n    firstletterfreqs[i,,k] <- firstletterfreqs[i,,k]/sum(firstletterfreqs[i,,k])\n  }\n}\n\n\n# plot of percentage of last letters over time\n\n# plot settings\npar(mar=c(2,3,2,1), mgp=c(1.7,.3,0), tck=-.01)\n\n# index of letters N, Y and D\npopular <- c(14,25,4)\n\n# width and line type for all letters except N, Y and D\nwidth <- rep(.5,26)\ntype <- rep(1,26)\n\n# width and line type for N, Y and D\nwidth[popular] <- c(2,3,3)\ntype[popular] <- c(1,3,2)\n\nplot(range(yrs), c(0,41), type=\"n\", xlab=\"\", ylab=\"Percentage of all boys' names that year\", bty=\"l\", xaxt=\"n\", yaxt=\"n\", yaxs=\"i\", xaxs=\"i\")\n  axis(1, seq(1900,2000,50))\n  axis(2, seq(0,40,20), paste(seq(0,40,20), \"%\", sep=\"\"))\n  for (j in 1:26){\n    # I don't think the following two variables, `maxfreq` and `best` are used in plotting\n    maxfreq <- max(lastletterfreqs[,j,2])\n    best <- (1:n.yrs)[lastletterfreqs[,j,2]==maxfreq]\n    \n    # plotting line for each letter for all years for boys\n    lines(yrs, 100*lastletterfreqs[,j,2], col=\"black\", lwd=width[j], lty=type[j])\n  }\n  \n# plot annotations\ntext(2000, 35, \"N\")\ntext(1935, 20, \"D\")\ntext(1975, 15, \"Y\")\nmtext(\"Last letters of boys' names\", side=3, line=.5)\n\n\n\n\nNext, I’ll recreate the plot of Figure 2.8 using tidyverse and ggplot for practice:\n\n# define new linewidth vector\n\n# index of letters N, Y and D\npopular <- c(14,25,4)\n\n# width and line type for all letters except N, Y and D\nwidth <- rep(.25,26)\n\n# width and line type for N, Y and D\nwidth[popular] <- c(1,1,1)\n\nggplot(allnames_agg %>% filter(sex == \"M\"), aes(x=year, y=last_letter_pct, linetype=factor(last_letter), linewidth = factor(last_letter))) + \n  geom_line(aes(group = last_letter)) +\n  scale_linetype_manual(values = type, guide = \"none\") + \n  scale_linewidth_manual(values = width, guide = \"none\") + \n  scale_x_discrete(\"Year\", breaks = c(1900, 1950, 2000), expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of all boys' names that year\", breaks = c(0, 0.1, 0.2, 0.3, 0.4), labels = c(\"0%\", \"10%\", \"20%\", \"30%\", \"40%\"), expand = c(0, 0)) +\n  ggtitle(\"Last letters of boys' names\") + \n  theme(\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  annotate(\"text\", x = \"2000\", y = 0.35, label = \"N\") + \n  annotate(\"text\", x = \"1935\", y = 0.20, label = \"D\") + \n  annotate(\"text\", x = \"1975\", y = 0.15, label = \"Y\")\n\n\n\n\nThe following code chunks recreate Figure 2.9 using their code provided on the supplemental website:\n\n# create empty 3D array to hold yearly percentage of top ten names\ntopten_percentage <- array(NA, c(length(yrs), 2))\n\nfor (i in 1:length(yrs)){\n  # get data for the given year\n  thisyear <- allnames[,paste(\"X\",yrs[i],sep=\"\")]\n  \n  # get boys' data for this year\n  boy.totals <- thisyear[!girl]\n  \n  # calculate percentages of boys' names\n  boy.proportions <- boy.totals/sum(boy.totals)\n  \n  # get total percentage of the top ten names\n  index <- rev(order(boy.proportions))\n  popularity <- boy.proportions[index]\n  topten_percentage[i,2] <- 100*sum(popularity[1:10])\n  \n  # do the same for girls' top ten names\n  girl.totals <- thisyear[girl]\n  girl.proportions <- girl.totals/sum(girl.totals)\n  index <- rev(order(girl.proportions))\n  popularity <- girl.proportions[index]\n  topten_percentage[i,1] <- 100*sum(popularity[1:10])\n}\n\n\n# plot settings\npar(mar=c(4,2,1,0), mgp=c(1.3,.2,0), tck=-.02)\n\n# plot girls and boys top ten name percentages over time\nplot(yrs, topten_percentage[,2], type=\"l\", xaxt=\"n\", yaxt=\"n\", xaxs=\"i\", yaxs=\"i\", ylim=c(0,45), bty=\"l\", xlab=\"Year\", ylab=\"\", cex.lab=.8)\nlines(yrs, topten_percentage[,1])\naxis(1, c(1900,1950,2000), cex.axis=.8)\naxis(2, c(0,20,40), c(\"0%\",\"20%\",\"40%\"), cex.axis=.8)\ntext(1902, 35, \"Boys\", cex=.75, adj=0)\ntext(1911, 20, \"Girls\", cex=.75, adj=0)\nmtext(\"Total popularity of top ten names each year, by sex\", cex=.8)\nmtext(\"Source:  Social Security Administration, courtesy of Laura Wattenberg\", 1, 2.5, cex=.5, adj=0)\n\n\n\n\nNext, I’ll recreate the plot of Figure 2.9 using tidyverse and ggplot for practice:\n\nDT::datatable(head(allnames_long))\n\n\n\n\n\n\n\nallnames_topten_names <- allnames_long %>% group_by(\n  year,\n  sex\n) %>% mutate(\n  name_pct = name_n / sum(name_n)\n) \n\n\nDT::datatable(head(allnames_topten_names %>% arrange(year, sex, desc(name_pct))))\n\n\n\n\n\n\n\nallnames_top_ten_pct <- allnames_topten_names %>% arrange(\n  year, \n  sex, \n  desc(name_pct)\n) %>% group_by(\n  year,\n  sex\n) %>% slice(\n  1:10\n) %>% summarise(\n  top_ten_pct = sum(name_pct)\n)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\nDT::datatable(head(allnames_top_ten_pct))\n\n\n\n\n\n\n\n# plot the lines\n\nggplot(allnames_top_ten_pct, aes(x = year, y = top_ten_pct)) + \n  geom_line(aes(group = sex)) +\n  scale_x_discrete(\"Year\", breaks = c(1900, 1950, 2000), expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of top ten names that year\", breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), labels = c(\"0%\", \"10%\", \"20%\", \"30%\", \"40%\", \"50%\"), expand = c(0, 0)) +\n  ggtitle(\"Total popularity of top ten names each year, by sex\") + \n  theme(\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  annotate(\"text\", x = \"1902\", y = 0.36, label = \"Boys\") + \n  annotate(\"text\", x = \"1911\", y = 0.19, label = \"Girls\")\n\n\n\n\n\n\nGrids of plots\nRealistically, it can be difficult to read a plot with more than two colors.\nConstructing a two-way grid of plots to represent two more variables, an approach of small multiples can be more effective than trying to cram five variables onto a single plot.\nIn the following code chunks, I’ll recreate Figure 2.10 following their code provided on the supplemental website.\n\n\n\n\n# view the data\ncongress[[27]][1,]\n\n[1]   1948      1      1     -1 127802 103294\n\ncongress[[28]][1,]\n\n[1]   1950      1      1      1 134258  96251\n\ncongress[[37]][10,]\n\n[1]   1968      3      1     -1      0 140419\n\ncongress[[38]][10,]\n\n[1]   1970      3      1     -1      0 117045\n\ncongress[[47]][20,]\n\n[1]   1988      4      1     -1  86623 131824\n\ncongress[[48]][50,]\n\n[1]  1990    13    11     1 36286     0\n\n\n\n# plot settings\npar(mfrow=c(3,5), mar=c(0.1,3,0,0), mgp=c(1.7, .3, 0), tck=-.02, oma=c(1,0,2,0))\n\n# plot data for each of the selected year pairs: 1948/50, 1968/70, 1988/90\nfor (i in c(27, 37, 47)) {\n  # calculate year based on index\n  year <- 1896 + 2*(i-1)\n  \n  # get the data for the first election year\n  cong1 <- congress[[i]]\n  \n  # get data for the next election year\n  cong2 <- congress[[i+1]]\n  \n  # the second column contains the state code\n  state_code <- cong1[,2]\n  \n  # convert from state code to region index\n  region <- floor(state_code/20) + 1\n  \n  # the fourth column contains whether the election has an incumbent\n  inc <- cong1[,4]\n  \n  # the fifth and sixth columns contain democrat and republican vote counts\n  dvote1 <- cong1[,5]/(cong1[,5] + cong1[,6])\n  dvote2 <- cong2[,5]/(cong2[,5] + cong2[,6])\n  \n  # their definition of an election being \"contested\" if the \n  # proportion of dem votes is between 20 and 80 percent each election year\n  contested <- (abs(dvote1 - 0.5)) < 0.3 & (abs(dvote2 - 0.5) < 0.3)\n  \n  # plot function\n  plot(c(0, 1), c(0, 1), type=\"n\", xlab=\"\", ylab=\"\", xaxt=\"n\", yaxt=\"n\", bty=\"n\")\n  text(0.8, 0.5, paste(year,\"\\nto\\n\", year+2, sep=\"\"), cex=1.1)\n  \n  # four columns, one for each region\n  for (j in 1:4){\n    plot(c(.2, .8), c(-.4, .3), type=\"n\", xlab= \"\" , ylab=if (j==1) \"Vote swing\" else \"\", xaxt=\"n\", yaxt=\"n\", bty=\"n\", cex.lab=.9)\n    if (i==47) {\n      # x-axis labels for the final pair of elections 1988/90\n      text(c(.25, .5, .75), rep(-.4, 3), c(\"25%\", \"50%\", \"75%\"), cex=.8)\n      abline(-.35, 0, lwd=.5, col=\"gray60\")\n      segments(c(.25, .5, .75), rep(-.35, 35), c(.25, .5, .75), rep(-.37, 3), lwd=.5)\n      mtext(\"Dem. vote in election 1\", side=1, line=.2, cex=.5)\n    }\n    # y-axis for each plot\n    axis(2, c(-0.25, 0, 0.25), c(\"-25%\", \"0\", \"25%\"),  cex.axis=.8)\n    abline(0, 0)\n    # region name above the first pair of elections 1948/50\n    if (i==27) mtext(region_name[j], side=3, line=1, cex=.75)\n    \n    # plotting contested elections for the region with incumbent\n    ok <- contested & abs(inc)==1 & region==j\n    points(dvote1[ok], dvote2[ok] - dvote1[ok], pch=20, cex=.3, col=\"gray60\")\n    \n    # plotting contested elections for the region with open seats\n    ok <- contested & abs(inc)==0 & region==j\n    points(dvote1[ok], dvote2[ok] - dvote1[ok], pch=20, cex=.5, col=\"black\")\n  }\n}\n\n\n\n\nNext, I’ll recreate similar plots with tidyverse and ggplot.\n\n# get list of all .asc file paths\ncongress_data_folder <- file.path(data_root_path, \"Congress/data\")\ncongress_data_files <- list.files(congress_data_folder, full.names=TRUE, pattern = \"*.asc\")\n\n# helper function which loads data into a CSV and adds a year column\nread_congress_data <- function(fpath) {\n  year_str = str_remove(basename(fpath), \".asc\")\n  \n  out <- readr::read_table(\n    fpath,\n    col_names = c(\"state\", \"col2\", \"incumbent\", \"dem\", \"rep\"),\n    col_types = cols(state = \"i\", col2 = \"c\", incumbent = \"i\", dem = \"i\", rep = \"i\")\n  ) %>% mutate(\n    year = year_str\n  )\n  \n  return(out)\n}\n\n# read all asc files into a single data.frame\n# filter for the years in question\ncongress_data <- congress_data_files %>% map(\n  read_congress_data\n) %>% list_rbind(\n) %>% filter(\n  year %in% c(1948, 1950, 1968, 1970, 1988, 1990)\n)\n\n\n# view the data\nDT::datatable(congress_data)\n\n\n\n\n\n\nNext, I’ll create two data.frames, cong1 and cong2 with 1948, 1968, 1988 and 1950, 1970, 1990 data, respectively, then merge them row-wise in order to perform year-to-year calculations:\n\n# 1948, 1968, and 1988 data\ncong1 <- congress_data %>% filter(\n  year %in% c(1948, 1968, 1988)\n) %>% rename(\n  # rename columns to avoid duplicates later on when joining the two data.frames\n  state1 = state,\n  col21 = col2,\n  incumbent1 = incumbent,\n  dem1 = dem,\n  rep1 = rep,\n  year1 = year\n) %>% mutate(\n  # create a lookup value to join the two data.frames by\n  election = case_when(\n    year1 == 1948 ~ \"1948 to 1950\",\n    year1 == 1968 ~ \"1968 to 1970\",\n    year1 == 1988 ~ \"1988 to 1990\"\n  ),\n  lookup = paste0(state1, \"-\", col21, \"-\", election)\n)\n\n# 1950, 1970 and 1990 data\ncong2 <- congress_data %>% filter(\n  year %in% c(1950, 1970, 1990)\n) %>% rename(\n  # rename columns to avoid duplicates later on when joining the two data.frames\n  state2 = state,\n  col22 = col2,\n  incumbent2 = incumbent,\n  dem2 = dem,\n  rep2 = rep,\n  year2 = year\n) %>% mutate(\n  # create a lookup value to join the two data.frames by\n  election = case_when(\n    year2 == 1950 ~ \"1948 to 1950\",\n    year2 == 1970 ~ \"1968 to 1970\",\n    year2 == 1990 ~ \"1988 to 1990\"\n  ),\n  lookup = paste0(state2, \"-\", col22, \"-\", election)\n)\n\n\n# merge the two data.frames \ncong <- cong1 %>% dplyr::left_join(\n  cong2,\n  by = \"lookup\"\n) %>% mutate(\n  # calculate democrat vote proportion\n  dvote1 = dem1 / (dem1 + rep1),\n  dvote2 = dem2 / (dem2 + rep2),\n  # calculate difference in democrat vote proportion between election years\n  dvote_diff= dvote2 - dvote1,\n  # calculate whether an election is contested\n  # an election is contested if the dem vote in both years is \n  # between 20% and 80%\n  contested = case_when(\n    (abs(dvote1 - 0.5)) < 0.3 & (abs(dvote2 - 0.5) < 0.3) ~ 1,\n    TRUE ~ 0\n  ),\n  # standardize incumbent flag in the first election year\n  inc = abs(incumbent1),\n  # remove values that are not applicable\n  inc = na_if(inc, 9),\n  inc = na_if(inc, 3),\n  inc = as.character(inc),\n  # calculate region code\n  region = floor(state1/20) + 1\n) %>% filter(\n  # only include contested elections\n  contested == 1,\n  # only include four main regions\n  region %in% c(1,2,3,4)\n) \n\n# %>% mutate(\n#   # change region to text\n#   region = case_when(\n#     region == 1 ~ \"Northeast\",\n#     region == 2 ~ \"Midwest\",\n#     region == 3 ~ \"South\",\n#     region == 4 ~ \"West\"\n#   )\n# )\n\n\nDT::datatable(cong, options=list(scrollX=TRUE))\n\n\n\n\n\n\nFinally, I can plot the data:\n\n# labels for regions\nregion_labeller <- c(\n  \"1\" = \"Northeast\",\n  \"2\" = \"Midwest\",\n  \"3\" =  \"South\",\n  \"4\" = \"West\"\n)\n\nggplot(cong, aes(x=dvote1, y = dvote_diff)) + \n  geom_point(aes(color=inc, size=inc), show.legend = FALSE) + \n  facet_grid(rows = vars(election.x), cols = vars(region), switch = \"y\", labeller = labeller(region = region_labeller)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    strip.placement = \"outside\",\n    axis.line=element_line()\n  ) + \n  scale_color_manual(values = c(\"black\", \"grey\"), na.translate = FALSE) + \n  scale_size_manual(values = c(0.75, 0.5), na.translate = FALSE) + \n  geom_abline(slope = 0, intercept = 0) +\n  scale_y_continuous(\"Vote Swing\", breaks = c(-0.25, 0, 0.25), labels = c(\"-25%\", \"0%\", \"25%\")) +\n  scale_x_continuous(\"Dem. Vote in Election 1\", breaks = c(.25, .50, .75), labels = c(\"25%\", \"50%\", \"75%\"))\n\nWarning: Removed 38 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWhile I wasn’t able to get exactly the same output as the textbook, I’m content with the grid of plots I created with ggplot.\n\n\nApplying graphical principles to numerical displays and communication more generally\nAvoid overwhelming the reader with irrelevant material.\nDo not report numbers to too many decimal places. You should display precision in a way that respects the uncertainty and variability in the numbers being presented. It makes sense to save lots of digits for intermediate steps in computations.\nYou can often make a list or table of numbers more clear by first subtracting out the average (or for a table, row and column averages).\nA graph can almost always be made smaller than you think and still be readable. This then leaves room for more plots on a grid, which then allows more patterns to be seen at once and compared.\nNever display a graph you can’t talk about. Give a full caption for every graph (this explains to yourself and others what you are trying to show and what you have learned from each plot).\nAvoid displaying graphs that have been made simply because they are conventional.\n\n\nGraphics for understanding statistical models\nThree uses of graphs in statistical analysis:\n\nDisplays of raw data. Exploratory analysis.\nGraphs of fitted models and inferences (and simulated data).\nGraphs presenting your final results.\n\nThe goal of any graph is communication to self and others. Graphs are comparisons: to zero, to other graphs, to horizontal lines, and so forth. The unexpected is usually not an “outlier” or aberrant point but rather a systematic pattern in some part of the data.\n\n\nGraphs as comparisons\nWhen making a graph, line things up so that the most important comparisons are clearest.\n\n\nGraphs of fitted models\nIt can be helpful to graph a fitted model and data on the same plot. Another use of graphics with fitted models is to plot predicted datasets and compare them visually to actual data.\n\n\n\n2.4 Data and adjustment: trends in mortality rates\nAggregation bias: occurs when it is wrongly assumed that the trends seen in aggregated data also apply to individual data points.\nI referenced this Missouri Department of Health & Senior Services page to learn more about how to calculate age-adjusted death rates.\nIn the following code chunks I’ll recreate Figures 2.11 and 2.12 using their code provided on the supplemental website.\n\n# deaton <- read.table(root(\"AgePeriodCohort/data\",\"deaton.txt\"), header=TRUE)\nages_all <- 35:64\nages_decade <- list(35:44, 45:54, 55:64)\nyears_1 <- 1999:2013\nmort_data <- as.list(rep(NA,3))\ngroup_names <- c(\"Non-Hispanic white\", \"Hispanic white\", \"African American\")\n\nfpath_1 <- file.path(data_root_path, \"AgePeriodCohort/data\", \"white_nonhisp_death_rates_from_1999_to_2013_by_sex.txt\")\nfpath_2 <- file.path(data_root_path, \"AgePeriodCohort/data\", \"white_hisp_death_rates_from_1999_to_2013_by_sex.txt\")\nfpath_3 <- file.path(data_root_path, \"AgePeriodCohort/data\", \"black_death_rates_from_1999_to_2013_by_sex.txt\")\nmort_data[[1]] <- read.table(fpath_1, header=TRUE)\nmort_data[[2]] <- read.table(fpath_2, header=TRUE)\nmort_data[[3]] <- read.table(fpath_3, header=TRUE)\n\n\nDT::datatable(mort_data[[1]])\n\n\n\n\n\n\n\nraw_death_rate <- array(NA, c(length(years_1), 3, 3))\nmale_raw_death_rate <- array(NA, c(length(years_1), 3, 3))\nfemale_raw_death_rate <- array(NA, c(length(years_1), 3, 3))\navg_death_rate <- array(NA, c(length(years_1), 3, 3))\nmale_avg_death_rate <- array(NA, c(length(years_1), 3, 3))\nfemale_avg_death_rate <- array(NA, c(length(years_1), 3, 3))\n\n# k represents the 3 different race/ethnicity groups\nfor (k in 1:3){\n  data <- mort_data[[k]]\n  \n  # the Male column is 0 for Female, 1 for Male\n  male <- data[,\"Male\"]==1\n  \n  # j represents the 3 different age decades\n  # 35:44, 45:54, 55:64\n  for (j in 1:3){\n    # years1 is from 1999 to 2013\n    for (i in 1:length(years_1)){\n      ok <- data[,\"Year\"]==years_1[i] & data[,\"Age\"] %in% ages_decade[[j]]\n      \n      # raw death rate calculated as\n      # 100,000 * deaths / population\n      raw_death_rate[i,j,k] <- 1e5*sum(data[ok,\"Deaths\"])/sum(data[ok,\"Population\"])\n      male_raw_death_rate[i,j,k] <- 1e5*sum(data[ok&male,\"Deaths\"])/sum(data[ok&male,\"Population\"])\n      female_raw_death_rate[i,j,k] <- 1e5*sum(data[ok&!male,\"Deaths\"])/sum(data[ok&!male,\"Population\"])\n      avg_death_rate[i,j,k] <- mean(data[ok,\"Rate\"])\n      male_avg_death_rate[i,j,k] <- mean(data[ok&male,\"Rate\"])\n      female_avg_death_rate[i,j,k] <- mean(data[ok&!male,\"Rate\"])\n    }\n  }\n}\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(range(years_1), c(1, 1.1), xaxt=\"n\", yaxt=\"n\", type=\"n\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Death rate relative to 1999\", main=\"Age-adjusted death rates for non-Hispanic whites aged 45-54:\\nTrends for women and men\")\nlines(years_1, male_avg_death_rate[,2,1]/male_avg_death_rate[1,2,1], col=\"blue\")\nlines(years_1, female_avg_death_rate[,2,1]/female_avg_death_rate[1,2,1], col=\"red\")\naxis(1, seq(1990,2020,5))\naxis(2, seq(1, 1.1, .05))\ntext(2011.5, 1.075, \"Women\", col=\"red\")\ntext(2010.5, 1.02, \"Men\", col=\"blue\")\ngrid(col=\"gray\")\n\n\n\n\n\nmean_age_45_54 <- function(yr){\n  ages <- 45:54\n  ok <- births$year %in% (yr - ages)\n  return(sum(births$births[ok]*rev(ages))/sum(births$births[ok]))\n}\n\n\nnumber_of_deaths <- rep(NA, length(years_1))\nnumber_of_people <- rep(NA, length(years_1))\navg_age <- rep(NA, length(years_1))\navg_age_census <- rep(NA, length(years_1))\n\n# data for white non-hispanic death rates\ndata <- mort_data[[1]]\n\ndeath_rate_extrap_1999 <- rep(NA, length(years_1))\ndeath_rate_extrap_2013 <- rep(NA, length(years_1))\n\n# data for Males\nmale <- data[,\"Male\"]==1\n\nok_1999 <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]] \n\ndeath_rate_1999 <- (data[ok_1999 & male, \"Deaths\"] + data[ok_1999 & !male, \"Deaths\"])/(data[ok_1999 & male, \"Population\"] + data[ok_1999 & !male, \"Population\"])\n\nok_2013<- data[,\"Year\"]==2013 & data[,\"Age\"] %in% ages_decade[[2]] \n\ndeath_rate_2013 <- (data[ok_2013 & male, \"Deaths\"] + data[ok_2013 & !male, \"Deaths\"])/(data[ok_2013 & male, \"Population\"] + data[ok_2013 & !male, \"Population\"])\n\nage_adj_rate_flat <- rep(NA, length(years_1))\nage_adj_rate_1999 <- rep(NA, length(years_1))\nage_adj_rate_2013 <- rep(NA, length(years_1))\n\nok <- data[,\"Age\"] %in% ages_decade[[2]]\n\npop1999 <- data[ok & data[,\"Year\"]==1999 & male,\"Population\"] + data[ok & data[,\"Year\"]==1999 & !male,\"Population\"]\n\npop2013 <- data[ok & data[,\"Year\"]==2013 & male,\"Population\"] + data[ok & data[,\"Year\"]==2013 & !male,\"Population\"]\n\nfor (i in 1:length(years_1)){\n  ok <- data[,\"Year\"]==years_1[i] & data[,\"Age\"] %in% ages_decade[[2]]\n  \n  number_of_deaths[i] <- sum(data[ok,\"Deaths\"])\n  number_of_people[i] <- sum(data[ok,\"Population\"])\n  \n  avg_age[i] <- weighted.mean(ages_decade[[2]], data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n  \n  avg_age_census[i] <- mean_age_45_54(years_1[i])\n  \n  rates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n  \n  age_adj_rate_flat[i] <- weighted.mean(rates, rep(1,10))\n  age_adj_rate_1999[i] <- weighted.mean(rates, pop1999)\n  age_adj_rate_2013[i] <- weighted.mean(rates, pop2013)\n}\n\nfor (i in 1:length(years_1)){\n  ok <- data[,\"Year\"]==years_1[i] & data[,\"Age\"] %in% ages_decade[[2]]\n  \n  death_rate_extrap_1999[i] <- weighted.mean(death_rate_1999, data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n  \n  death_rate_extrap_2013[i] <- weighted.mean(death_rate_2013, data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n  \n}\n\n\nhead(number_of_deaths)\n\n[1] 106808 111964 117086 119812 121832 123037\n\nhead(number_of_people)\n\n[1] 27995805 28669184 29733531 29880552 30260532 30629390\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1,  number_of_deaths/number_of_people, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Mortality rate among non-Hisp whites 45-54\", main=\"So take the ratio!\", cex.main=.9)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\nAs an aside, I want to better understand how the avg_age vector is calculated:\n\n# flag for when data is from 1999 and for ages 45 to 54\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# range of ages 45 to 54\nages_decade[[2]]\n\n [1] 45 46 47 48 49 50 51 52 53 54\n\n# 1999 population for ages 45 to 54\ndata[ok & male,\"Population\"] + data[ok & !male,\"Population\"]\n\n [1] 3166393 3007083 2986252 2805975 2859406 2868751 2804957 3093631 2148382\n[10] 2254975\n\n# manually calculated weighted mean\nsum(ages_decade[[2]] * (data[ok & male,\"Population\"] + data[ok & !male,\"Population\"]) / (sum(data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])))\n\n[1] 49.25585\n\n# weighted.mean output\navg_age[1] # this equals the manually calculated weighted mean. Nice!\n\n[1] 49.25585\n\n\n\nlength(avg_age) # 15 years: 1999-2013\n\n[1] 15\n\nhead(avg_age)\n\n[1] 49.25585 49.29742 49.40278 49.34845 49.34162 49.35910\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1, avg_age, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Avg age among non-Hisp whites 45-54\", main=\"But the average age in this group is going up!\", cex.main=.9)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\n\nhead(death_rate_extrap_2013)\n\n[1] 0.003999152 0.004012133 0.004050824 0.004031180 0.004028291 0.004034446\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1, number_of_deaths/number_of_people, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Death rate for 45-54 non-Hisp whites\", main=\"Projecting backward from 2013 makes it clear that\\nall the underlying change happened between 1999 and 2005\", cex.main=.8)\nlines(years_1, death_rate_extrap_2013, col=\"green4\")\naxis(1, seq(1990,2020,5))\ntext(2003, .00395, \"Raw death rate\", cex=.8)\ntext(2001.5, .004075, \"Expected just from\\nage shift\", col=\"green4\", cex=.8)\ngrid(col=\"gray\")\n\n\n\n\n\nhead(age_adj_rate_flat)\n\n[1] 0.003908209 0.003978170 0.003964249 0.004053627 0.004072032 0.004058058\n\nhead(age_adj_rate_flat[1])\n\n[1] 0.003908209\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1, age_adj_rate_flat/age_adj_rate_flat[1], xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Age-adj death rate, relative to 1999\", main=\"Trend in age-adjusted death rate\\nfor 45-54-year-old non-Hisp whites\", cex.main=.8)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\n\nhead(age_adj_rate_1999)\n\n[1] 0.003815143 0.003889664 0.003891300 0.003976232 0.003999796 0.003987676\n\nhead(age_adj_rate_1999[1])\n\n[1] 0.003815143\n\nhead(age_adj_rate_2013)\n\n[1] 0.003972000 0.004040848 0.004021437 0.004111934 0.004129688 0.004115952\n\nhead(age_adj_rate_2013[1])\n\n[1] 0.003972\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nrng <- range(age_adj_rate_flat/age_adj_rate_flat[1], age_adj_rate_1999/age_adj_rate_1999[1], age_adj_rate_2013/age_adj_rate_2013[1])\nplot(years_1, age_adj_rate_flat/age_adj_rate_flat[1], ylim=rng, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Age-adj death rate, relative to 1999\", main=\"It doesn't matter too much what age adjustment\\nyou use for 45-54-year-old non-Hisp whites\", cex.main=.8)\nlines(years_1, age_adj_rate_1999/age_adj_rate_1999[1], lty=2)\nlines(years_1, age_adj_rate_2013/age_adj_rate_2013[1], lty=3)\ntext(2003, 1.053, \"Using 1999\\nage dist\", cex=.8)\ntext(2004, 1.032, \"Using 2013\\nage dist\", cex=.8)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\nNext, I’ll recreate the plots in Figures 2.11 and 2.12 using tidyverse and ggplot. I’ll go in the order of plots displayed in the supplemental website. They start with Figure 2.11a (Observed increase in raw mortality rates among 45-to-54-year-old non-Hispanic whites, unadjusted for age.\n\n# load death rate data for white non-hispanic individuals\nmort_data_fpath <- file.path(data_root_path, \"AgePeriodCohort/data\",\"white_nonhisp_death_rates_from_1999_to_2013_by_sex.txt\")\n\nmort_data2 <- readr::read_table(mort_data_fpath) %>% filter(\n  # get data for the age decade in question\n  Age %in% 45:54\n)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Age = col_double(),\n  Male = col_double(),\n  Year = col_double(),\n  Deaths = col_double(),\n  Population = col_double(),\n  Rate = col_double()\n)\n\nDT::datatable(mort_data2)\n\n\n\n\n\n\nI’ll look at their vector raw_death_rate to compare my recreated data.frame with.\n\nraw_death_rate[,2,1]\n\n [1] 381.5143 390.5378 393.7844 400.9698 402.6102 401.6959 408.2857 407.9969\n [9] 405.2389 412.3255 413.7259 407.1706 414.3665 410.9378 415.3878\n\n\nI’ll calculate the raw death rate for each Year. In their code, they multiple it by 100,000 to get per 100,000 deaths, but I’ll follow the textbook plot which shows it as a decimal.\n\nraw_death_rate_data <- mort_data2 %>% group_by(\n  Year\n) %>% summarize(\n  death_rate = sum(Deaths) / sum(Population)\n)\n\nDT::datatable(raw_death_rate_data)\n\n\n\n\n\n\nNext, I’ll plot the data\n\nggplot(raw_death_rate_data, aes(x = Year, y = death_rate)) + \n  geom_line() + \n  scale_y_continuous(\"Death rates among non-Hisp whites 45-54\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Raw death rates\\nfor 45-54-year-old non-Hispanic whites\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nNext, I’ll recreate the data and plots for Figure 2.12c (trends in age-adjusted death rates broken down by sex). I’ll start by calculating the average death rate for Males and Females:\n\navg_death_rate <- mort_data2 %>% group_by(\n  Year,\n  Male\n) %>% summarise(\n  avg_rate = mean(Rate)\n) %>% mutate(\n  age_adj_death_rate = case_when(\n    Male == 0 ~ avg_rate / 288.19,\n    Male == 1 ~ avg_rate / 495.37\n  ),\n  # convert Male to discrete value\n  Male = case_when(\n    Male == 0 ~ \"Women\",\n    Male == 1 ~ \"Men\"\n  )\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(avg_death_rate)\n\n\n\n\n\n\nNext, I’ll plot the age-adjusted death rates (relative to 1999) for Non-Hispanic White Males and Females aged 45 - 54 between 1999 and 2013:\n\nggplot(avg_death_rate, aes(x = Year, y = age_adj_death_rate, color = Male)) + \n  geom_line(aes(group = Male), show.legend = FALSE) + \n  scale_color_manual(values = c(\"blue\", \"red\")) + \n  annotate(\"text\", x = 2011.5, y = 1.075, label = \"Women\", color = \"red\") + \n  annotate(\"text\", x = 2010.5, y = 1.02, label= \"Men\", color = \"blue\") + \n  scale_y_continuous(\"Death rate relative to 1999\") + \n  scale_x_continuous(\"Year\") + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Age-adjusted death rates for non-Hispanic whites aged 45-54:\\nTrends for women and men\")\n\n\n\n\nLooks great! My grid lines are at different placements (5 years and 0.025 death rate) than the textbook but I’m happy with how the plot turned out. One hiccup that needed to be addressed was that the group aesthetic needs to be discrete (it was integer 1 for Male and integer 0 for Female) in order to use scale_color_manual.\nNext, I’ll recreate Figure 2.11b (increase in average age of this group as the baby boom generation moves through).\n\navg_age_data <- mort_data2 %>% group_by(\n  Year,\n  Age\n) %>% summarise(\n  total_pop = sum(Population)\n) %>% mutate(\n  # calculated weighted mean\n  weighted_age = Age * total_pop\n) %>% group_by(\n  # in order to calculate weighted mean age by Year, \n  # group only by Year\n  Year\n) %>% summarize(\n  weighted_mean_age = sum(weighted_age) / sum(total_pop)\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(avg_age_data)\n\n\n\n\n\n\nLet’s compare it with the actual avg_age values:\n\navg_age\n\n [1] 49.25585 49.29742 49.40278 49.34845 49.34162 49.35910 49.36995 49.39921\n [9] 49.43863 49.48060 49.51819 49.53985 49.62024 49.66772 49.70577\n\n\nNext, I’ll plot weighted mean age vs years:\n\nggplot(avg_age_data, aes(x = Year, y = weighted_mean_age)) + \n  geom_line() + \n  scale_y_continuous(\"Avg age among non-Hispanic whites 45-54\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"But the average age in this group is going up!\")\n\n\n\n\nNext, I’ll recreate Figure 2.11c (raw death rate, along with trend in death rate attributable by change in age distribution alone, had age-specific mortality rates been at the 2013 level throughout).\nAs an aside, I’m going to manually calculate the first value of death_rate_extrap_2013:\n\n# death rates in 2013 for ages 45:54\ndeath_rate_2013\n\n [1] 0.002607366 0.002898205 0.003235408 0.003428914 0.003845450 0.004221539\n [7] 0.004660622 0.004944160 0.005267291 0.005727139\n\n# weighted mean of 2013 death rates of ages 45:54 by 1999-2013 population of ages 45-54\ndeath_rate_extrap_2013 \n\n [1] 0.003999152 0.004012133 0.004050824 0.004031180 0.004028291 0.004034446\n [7] 0.004037951 0.004048038 0.004061568 0.004075922 0.004089161 0.004096481\n[13] 0.004124305 0.004140684 0.004153878\n\n# flag for when data is from 1999 and for ages 45 to 54\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# 1999 population for ages 45 to 54\ndata[ok & male,\"Population\"] + data[ok & !male,\"Population\"]\n\n [1] 3166393 3007083 2986252 2805975 2859406 2868751 2804957 3093631 2148382\n[10] 2254975\n\n# numerator of weighted mean\ndeath_rate_2013 * (data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n\n [1]  8255.944  8715.142  9661.744  9621.447 10995.704 12110.545 13072.845\n [8] 15295.406 11316.152 12914.555\n\n# weighted mean of death_rate_2013 and 1999 population\nsum(death_rate_2013 * (data[ok & male,\"Population\"] + data[ok & !male,\"Population\"]) / (sum(data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])))\n\n[1] 0.003999152\n\n\nI’ll first calculate the total deaths, population and death rate for each year and age value.\n\nmort_data_totals <- mort_data2 %>% group_by(\n  # calculate total death rate and total population\n  # for each year and age\n  Year,\n  Age\n) %>% summarise(\n  total_deaths = sum(Deaths),\n  total_pop = sum(Population),\n  total_death_rate = total_deaths / total_pop\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(mort_data_totals %>% filter(Year == 2013)) # matches death_rate_2013\n\n\n\n\n\n\nNext, I’ll widen the data:\n\ndeath_rate_extrap_2013_data <- mort_data_totals %>% pivot_wider(\n  names_from = Year,\n  values_from = c(total_death_rate, total_pop),\n  names_vary = \"slowest\",\n  id_cols = c(Age)\n) %>% mutate(\n  # multiply each year's age population with 2013 death rate\n  across(starts_with(\"total_pop\"), ~ .x * total_death_rate_2013, .names = \"extrap_{.col}\")\n) %>% pivot_longer(\n  # return back to a long table \n  # in order to group by Age, Year and calculate weighted mean\n  !Age,\n  names_to = c(\".value\", \"Year\"),\n  names_pattern = \"^(.*)_([0-9]{4})$\"\n) %>% group_by(\n  Year\n) %>% summarise(\n  weighted_mean_extrap = sum(extrap_total_pop) / sum(total_pop)\n) \n\nDT::datatable(death_rate_extrap_2013_data, options = list(scrollX = TRUE))\n\n\n\n\n\n\nMy calculated values match death_rate_extrap_2013. Yes!\nNext, I’ll prepare the data to recreate number_of_deaths/number_of_people in the text:\n\nyearly_total_death_rate <- mort_data2 %>% group_by(\n  # calculate total death rate and total population\n  # for each year \n  Year\n) %>% summarise(\n  total_death_rate = sum(Deaths) / sum(Population)\n) %>% mutate(\n  Year = as.character(Year)\n)\n\nDT::datatable(yearly_total_death_rate)\n\n\n\n\n\n\nNext, I’ll plot the extrapolated death rate vs. years:\n\nggplot() + \n  geom_line(data = death_rate_extrap_2013_data, aes(x = Year, y = weighted_mean_extrap, group = 1), color = \"green4\") +\n  geom_line(data = yearly_total_death_rate, aes(x = Year, y = total_death_rate, group = 1), color = \"black\") +\n  scale_y_continuous(\"Death rate for 45-54 non-Hisp whites\", expand = c(0,0)) +\n  scale_x_discrete(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Projecting backward from 2013 makes it clear that\\nall the underlying change happened between 1999 and 2005\") + \n  annotate(\"text\", x = \"2003\", y = 0.00395, label = \"Raw death rate\") +\n  annotate(\"text\", x = \"2002\", y = 0.004075, label = \"Expected just from\\nage shift\", color=\"green4\")\n\n\n\n\nThe next plot I’ll recreate is Figure 2.12a (Age-adjusted death rates among 45-to-54-year-old non-Hispanic whites, showing an increase from 1999 to 2005 and a steady pattern since 2005).\nI’ll start by looking at the vector age_adj_rate_flat:\n\nage_adj_rate_flat\n\n [1] 0.003908209 0.003978170 0.003964249 0.004053627 0.004072032 0.004058058\n [7] 0.004123289 0.004112009 0.004072423 0.004130493 0.004133025 0.004059948\n[13] 0.004105064 0.004053953 0.004083609\n\n\nThen run the calculation for the first value for 1999:\n\n# flag for data in 1999 for ages 45 to 54\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# calculation of death rates\nrates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n\nprint(rates)\n\n [1] 0.002622542 0.002929417 0.003059018 0.003371734 0.003589906 0.003767493\n [7] 0.004289549 0.004447848 0.005451079 0.005553498\n\n# non-weighted mean\nmean(rates)\n\n[1] 0.003908209\n\n# calculation of age_adj_rate_flat\nweighted.mean(rates, rep(1,10))\n\n[1] 0.003908209\n\n\nThe calculation for this metric is similar to Figure 2.12c which showed the age-adjusted death rate by sex. Instead of separating by sex, I’ll combine Men and Women deaths and populations in this death rate calculation.\n\nage_adj_rate_flat_data <- mort_data2 %>% group_by(\n  Year,\n  Age\n) %>% summarise(\n  # calculate death rate for each Year and Age\n  avg_rate = sum(Deaths) / sum(Population)\n) %>% group_by(\n  Year\n) %>% summarise(\n  # calculate average death rate across ages 45-54 for each Year\n  avg_rate = mean(avg_rate)\n) %>% mutate(\n  # adjust death rate relative to 1999\n  adj_death_rate = avg_rate / 0.00390820851324269\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(age_adj_rate_flat_data)\n\n\n\n\n\n\nNext, I’ll plot age-adjusted death rate relative to 1999 vs. Years:\n\nggplot(age_adj_rate_flat_data, aes(x = Year, y = adj_death_rate)) + \n  geom_line() +\n  scale_y_continuous(\"Age-adj death rate, relative to 1999\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Trend in age-adjusted death rate\\nfor 45-54-year-old non-Hisp whites\")\n\n\n\n\nThe final figure I’ll recreate is Figure 2.12b (comparison of two different age adjustments).\nI’ll start by looking at the vectors in question, age_adj_rate_1999 and age_adj_rate_2013.\n\nage_adj_rate_1999\n\n [1] 0.003815143 0.003889664 0.003891300 0.003976232 0.003999796 0.003987676\n [7] 0.004049536 0.004036497 0.003995087 0.004057049 0.004054927 0.003979230\n[13] 0.004023539 0.003970082 0.003999152\n\nage_adj_rate_2013\n\n [1] 0.003972000 0.004040848 0.004021437 0.004111934 0.004129688 0.004115952\n [7] 0.004185252 0.004174235 0.004133826 0.004193094 0.004197334 0.004125137\n[13] 0.004172716 0.004122541 0.004153878\n\n\nAs an aside, I’ll view the first values (1999 and 2013) from each of the two vectors respectively, following the their code.\n\n# flag for 1999 and 45-54 age range\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# average death rates\nrates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n\n# 1999 average death rates\nprint(rates)\n\n [1] 0.002622542 0.002929417 0.003059018 0.003371734 0.003589906 0.003767493\n [7] 0.004289549 0.004447848 0.005451079 0.005553498\n\n# 1999 population\nprint(pop1999)\n\n [1] 3166393 3007083 2986252 2805975 2859406 2868751 2804957 3093631 2148382\n[10] 2254975\n\n# first value of age_adj_rate_1999\nweighted.mean(rates, pop1999)\n\n[1] 0.003815143\n\n# flag for 1999 and 45-54 age range\nok <- data[,\"Year\"]==2013 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# average death rates\nrates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n\n# 2013 average death rates\nprint(rates)\n\n [1] 0.002607366 0.002898205 0.003235408 0.003428914 0.003845450 0.004221539\n [7] 0.004660622 0.004944160 0.005267291 0.005727139\n\n# 2013 population\nprint(pop2013)\n\n [1] 2576547 2629559 2692087 2905293 3017592 3054810 3099157 3182745 3199178\n[10] 3141010\n\n# first value of age_adj_rate_2013\nweighted.mean(rates, pop2013)\n\n[1] 0.004153878\n\n\nI’ll start by reusing my data.frame mort_data_totals which contains Year, Age, total_deaths, total_pop and total_death_rate.\n\nDT::datatable(mort_data_totals)\n\n\n\n\n\n\nI’ll use a similar approach to how I created death_rate_extrap_2013_data.\n\npop_1999_adj_death_rate <- mort_data_totals %>% pivot_wider(\n  names_from = Year,\n  values_from = c(total_death_rate, total_pop),\n  names_vary = \"slowest\",\n  id_cols = c(Age)\n) %>% mutate(\n  # calculate weighted mean for death rates with 1999 population\n  across(starts_with(\"total_death_rate\"), ~ sum(.x * total_pop_1999) / sum(total_pop_1999), .names = \"adj_{.col}\")\n) %>% pivot_longer(\n  # return back to a long table\n  !Age,\n  names_to = c(\".value\", \"Year\"),\n  names_pattern = \"^(.*)_([0-9]{4})$\"\n) %>% group_by(\n  Year\n) %>% dplyr::select(\n  # pull columns needed for plot\n  Year,\n  adj_total_death_rate\n) %>% distinct(\n  Year, adj_total_death_rate\n) %>% mutate(\n  # normalize death rate relative to 1999 value\n  adj_total_death_rate = adj_total_death_rate / 0.0038151430187487,\n  # make Year continuous\n  Year = as.numeric(Year)\n)\n\nDT::datatable(pop_1999_adj_death_rate, options = list(scrollX = TRUE))\n\n\n\n\n\n\nI’ll create the same data.frame for the 2013 adjusted death rates:\n\npop_2013_adj_death_rate <- mort_data_totals %>% pivot_wider(\n  names_from = Year,\n  values_from = c(total_death_rate, total_pop),\n  names_vary = \"slowest\",\n  id_cols = c(Age)\n) %>% mutate(\n  # calculate weighted mean for death rates with 1999 population\n  across(starts_with(\"total_death_rate\"), ~ sum(.x * total_pop_2013) / sum(total_pop_2013), .names = \"adj_{.col}\")\n) %>% pivot_longer(\n  # return back to a long table\n  !Age,\n  names_to = c(\".value\", \"Year\"),\n  names_pattern = \"^(.*)_([0-9]{4})$\"\n) %>% group_by(\n  Year\n) %>% dplyr::select(\n  # pull columns needed for plot\n  Year,\n  adj_total_death_rate\n) %>% distinct(\n  Year, adj_total_death_rate\n) %>% mutate(\n  # normalize death rate relative to 1999 value\n  adj_total_death_rate = adj_total_death_rate / 0.00397199977461337,\n  # make Year continuous\n  Year = as.numeric(Year)\n)\n\nDT::datatable(pop_2013_adj_death_rate, options = list(scrollX = TRUE))\n\n\n\n\n\n\nFinally, I’ll plot the three lines:\n\nggplot() + \n  geom_line(data = age_adj_rate_flat_data, aes(x = Year, y = adj_death_rate)) +\n  geom_line(data = pop_1999_adj_death_rate, aes(x = Year, y = adj_total_death_rate), linetype = \"dashed\") +\n  geom_line(data = pop_2013_adj_death_rate, aes(x = Year, y = adj_total_death_rate), linetype = \"dotted\") + \n  scale_y_continuous(\"Age-adj death rate, relative to 1999\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"It doesn't matter too much what age adjustment\\nyou use for 45-54-year-old non-Hisp whites\") + \n  annotate(\"text\", x = 2003, y = 1.053, label = \"Using 1999\\nage dist\") + \n  annotate(\"text\", x = 2004, y = 1.032, label = \"Using 2013\\nage dist\")"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "",
    "text": "An example image of a bear from the image dataset used in this chapter.\nIn Chapter 6, we learned to train an image recognition model for multi-label classification. In this notebook, I will apply those concepts to the bear classifier from Chapter 2.\nI’ll place the prompt of the “Further Research” section here and then answer each part.\nHere’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Setup",
    "text": "Setup\n\nfrom fastai.vision.all import *\n\n\nimport fastai\nimport pandas as pd\n\nfastai.__version__\n\n'2.3.0'\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nI have three different CSVs with Google Image URLs, one each for black, brown and grizzly bears. The script below, taken from the book, creates a directory for each of the three types of bears in the bears folder, and then downloads the corresponding bear type’s images into that directory.\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\nbear_types = ['black', 'grizzly', 'teddy']\nif not path.exists():\n  path.mkdir()\n  for o in bear_types:\n    dest = path/o\n    dest.mkdir(exist_ok=True)\n    download_images(f'/content/gdrive/MyDrive/fastai-course-v4/images/bears/{o}', url_file=Path(f'/content/gdrive/MyDrive/fastai-course-v4/download_{o}.csv'))\n\n\n# confirm that `get_image_files` retrieves all images\nfns = get_image_files(path)\nfns\n\n(#535) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000009.jpg')...]\n\n\n\n# verify all images\nfailed = verify_images(fns)\nfailed\n\n(#0) []\n\n\nSince I may need to move files around if they are incorrectly labeled, I’m going to prepend the filenames with the corresponding bear type.\n\nimport os\nfor dir in os.listdir(path):\n    for f in os.listdir(path/dir):\n      os.rename(path/dir/f, path/dir/f'{dir}_{f}')\n\n\nfns = get_image_files(path)\nfns\n\n(#723) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000006.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg')...]"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Single-Label Classifier",
    "text": "Single-Label Classifier\nI’ll train the single-digit classifier as we did in Chapter 2.\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# create DataLoaders\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# verify train batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# first training\n# use it to clean the data\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.367019\n      0.252684\n      0.080645\n      00:05\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.179421\n      0.175091\n      0.056452\n      00:04\n    \n    \n      1\n      0.155954\n      0.165824\n      0.048387\n      00:04\n    \n    \n      2\n      0.119193\n      0.173681\n      0.056452\n      00:04\n    \n    \n      3\n      0.098313\n      0.170383\n      0.048387\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\nInitial training: Clean the Dataset\n\n# plot highest loss images\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\nSome of these images are infographics containing text, illustrations and other non-photographic bear data. I’ll delete those using the cleaner\n\nfrom fastai.vision.widgets import *\n\n\n# view highest loss images\n# using ImageClassifierCleaner\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n# unlink images with \"<Delete>\" selected in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\n\n# move any images reclassified in the cleaner\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nAfter a few rounds of quickly training the model and using the cleaner, I was able to remove or change a couple dozen of the images. I’ll use lr.find() and re-train the model.\n\n\nSecond Training with Cleaned Dataset\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\n\n# create DataLoaders\ndls = bears.dataloaders(path)\n\n#verify validation batch\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n#verify training batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.0005754399462603033)\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\n\n# fit one cycle\nlr = 1e-3\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.405979\n      0.418305\n      0.145161\n      00:04\n    \n    \n      1\n      0.803087\n      0.214286\n      0.056452\n      00:04\n    \n    \n      2\n      0.557531\n      0.169275\n      0.048387\n      00:04\n    \n    \n      3\n      0.408410\n      0.163632\n      0.056452\n      00:04\n    \n    \n      4\n      0.321682\n      0.164792\n      0.040323\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n# show results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘t’ 8=‘h’ 9=‘e’ 10=’ ’ 11=‘m’ 12=‘o’ 13=‘d’ 14=‘e’ 15=‘l’}\nlearn.export(fname=path/'single_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Multi-Label Classifier",
    "text": "Multi-Label Classifier\nThere are three major differences between training a multi-label classification model and a single-label model on this dataset. I present them in a table here:\n\n\n\n\n\n\n\n\n\nClassification Model Type\nDependent Variable\nLoss Function\nget_y function\n\n\n\n\nSingle-label\nDecoded string\nCross Entropy (softmax)\nparent_label\n\n\nMulti-label\nOne-hot Encoded List\nBinary Cross Entropy (sigmoid with threshold)\n[parent_label]\n\n\n\n\n# create helper function\ndef get_y(o): return [parent_label(o)]\n\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_y,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# view validation batch\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18,  metrics=partial(accuracy_multi,thresh=0.95), loss_func=BCEWithLogitsLossFlat(thresh=0.5))\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of BCEWithLogitsLoss()\n\n\n\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.478340\n      0.436599\n      0.937695\n      00:51\n    \n    \n      1\n      0.289231\n      0.642520\n      0.887850\n      00:03\n    \n    \n      2\n      0.203213\n      0.394335\n      0.897196\n      00:03\n    \n    \n      3\n      0.159622\n      0.155405\n      0.959502\n      00:02\n    \n    \n      4\n      0.132379\n      0.090879\n      0.965732\n      00:02\n    \n  \n\n\n\n\n# verify results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘m’ 8=‘o’ 9=‘d’ 10=‘e’ 11=‘l’}\nlearn.export(path/'multi_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Model Inference",
    "text": "Model Inference\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images')\n\n\nImage with a Single Bear\n\n# grizzly bear image\nimg = PILImage.create(path/'test'/'grizzly_test_1.jpg')\nimg\n\n\n\n\n\n# load learners\nsingle_learn_inf = load_learner(path/'bears'/'single_label_bear_classifier.pkl')\nmulti_learn_inf = load_learner(path/'bears'/'multi_label_bear_classifier.pkl')\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n\n\n('teddy', tensor(2), tensor([1.7475e-04, 3.7727e-04, 9.9945e-01]))\n\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\n\n((#1) ['grizzly'],\n tensor([False,  True, False]),\n tensor([6.3334e-05, 1.0000e+00, 1.4841e-04]))\n\n\n\n\nImage with Two Bears\n\n# image with grizzly and black bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with grizzly and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with black and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\nImages without Bears\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/')\nimg = PILImage.create(path/'test'/'computer.jpg')\nimg\n\n\n\n\n\nsigle_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.1316, 0.1916, 0.0004]))\n\n\n\nsingle_learn_inf.predict(img)[2].sum()\n\n\n\n\ntensor(1.)\n\n\n\n# set loss function threshold to 0.9\nmulti_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.0275, 0.0196, 0.8457]))"
  },
  {
    "objectID": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "href": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "title": "Making Chicken Wings",
    "section": "",
    "text": "Background\nRestaurant-bought wings are pricey. Even fast-food bought wings, which can be miserably made some times—I once bought advertised Honey BBQ Chicken Wings which were great the first time, but the second time looked like plain wings with barbeque sauce drizzled over them–were about $20 for 16 wings. I don’t mind the price, gratefully, but I do mind consistent quality and want to avoide letdowns, so I chose to learn how to make wings. I also need to fill the gap caused by the lack of NFL and College Football during the offseason, so this is a tasty project which serves that purpose as well.\n\n\nFirst Attempt\nI made my first batch of chicken wings on Saturday, February 11, 2023. They were edible and the sauce was delicious. But they were at most a 5/10. I follow some recipe I found online after a google search which went something like:\n\nPreheat oven to 400°F\nBaste oil on both sides of the wings and season with salt and pepper\nBake for 20 minutes, flipping the wings after 10 minutes\nHeat the oven to 425°F\nBaste on the sauce and bake for 7 minutes then flip. Repeat a few times.\nBroil (500°F) for 5-10 minutes.\n\nMy sauce was made of Ray’s sugar-free BBQ sauce, mustard, soy sauce, and honey.\nI did not have a basting brush and was using a spoon to lather on the sauce. It did not work well. Also, the meat did not fall off the bones and it took more effort than worthwhile to eat them.\n\n\nSecond Attempt\nI was hungrier this time I suppose because I chose to make honey bbq drumsticks instead of wings. This time I got a silicone basting brush at Safeway. I modified my approach slightly:\n\nPreheat oven to 400°F\nActually baste this time and season the drumsticks\nBake on one side for 15 minutes, flip and bake for another 15 minutes\nRemove the drumsticks and heat the oven to 425°F\nRepeat four times, twice on each side: baste on sauce, bake for 7 minutes, remove and flip.\nAdd coconut flour to the sauce (note to self: use rice flour instead)\nRepeat two times, once on each side: baste on sauce/flour mix (gravy?), bake for 7 minutes and flip.\nBroil (500°F) for 5 minutes.\n\nUsing the basting brush allowed for a more even distribution of sauce on the drumsticks. The skin still wasn’t crispy enough, although the meat was (more) easily coming off the bones and was juicy + delicious.\nWhen I make it again, probably next weekend, I’ll switch back to chicken wings and find a new recipe which emphasizes the cripsiness of the wings.\n\n\n\nclose-up of my honey bbq chicken drumsticks with a few charred spots."
  },
  {
    "objectID": "posts/2023-06-01-practical-deep-learning-for-coders-part-1/2023_06_01_practical_deep_learning_for_coders_part_1.html",
    "href": "posts/2023-06-01-practical-deep-learning-for-coders-part-1/2023_06_01_practical_deep_learning_for_coders_part_1.html",
    "title": "vishal bakshi",
    "section": "",
    "text": "Vishal Bakshi\nThis notebook contains my notes (of course videos, example notebooks and book chapters) and exercises of Part 1 of the course Practical Deep Learning for Coders.\n\n\n\n\nThe first thing I did was to run through the lesson 1 notebook from start to finish. In this notebook, they download training and validation images of birds and forests then train an image classifier with 100% accuracy in identifying images of birds.\nThe first exercise is for us to create our own image classifier with our own image searches. I’ll create a classifier which accurately predicts an image of an alligator.\nI’ll start by using their example code for getting images using DuckDuckGo image search:\n\n# It's a good idea to ensure you're running the latest version of any libraries you need.\n# `!pip install -Uqq <libraries>` upgrades to the latest version of <libraries>\n# NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities\n!pip install -Uqq fastai fastbook duckduckgo_search timm\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nThe search_images function takes a search term and max_images maximum number of images value. It prints out a line of text that it’s \"Searching for\" the term and returns an L object with the image URL.\nThe ddg_images function returns a list of JSON objects containing the title, image URL, thumbnail URL, height, width and source of the image.\n\nsearch_object = ddg_images('alligator', max_results=1)\nsearch_object\n\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n[{'title': 'The Creature Feature: 10 Fun Facts About the American Alligator | WIRED',\n  'image': 'https://www.wired.com/wp-content/uploads/2015/03/Gator-2.jpg',\n  'thumbnail': 'https://tse4.mm.bing.net/th?id=OIP.FS96VErnOXAGSWU092I_DQHaE8&pid=Api',\n  'url': 'https://www.wired.com/2015/03/creature-feature-10-fun-facts-american-alligator/',\n  'height': 3456,\n  'width': 5184,\n  'source': 'Bing'}]\n\n\nWrapping this list in L object and calling .itemgot('image') on it extracts URL value associated with the image key in the JSON object.\n\nL(search_object).itemgot('image')\n\n(#1) ['https://www.wired.com/wp-content/uploads/2015/03/Gator-2.jpg']\n\n\nNext, they provide some code to download the image to a destination filename and view the image:\n\nurls = search_images('alligator', max_images=1)\n\nfrom fastdownload import download_url\ndest = 'alligator.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\nSearching for 'alligator'\n\n\n\n\n\nFor my not-alligator images, I’ll use images of a swamp.\n\ndownload_url(search_images('swamp photos', max_images=1)[0], 'swamp.jpg', show_progress=False)\nImage.open('swamp.jpg').to_thumb(256,256)\n\nSearching for 'swamp photos'\n\n\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n\n\n\nIn the following code, I’ll search for both terms, alligator and swamp and store the images in alligator_or_not/alligator and alligator_or_not/swamp paths, respectively.\nThe parents=TRUE argument creates any intermediate parent directories that don’t exist (in this case, the alligator_or_not directory). The exist_ok=TRUE argument suppresses the FileExistsError and does nothing.\n\nsearches = 'swamp','alligator'\npath = Path('alligator_or_not')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'swamp photo'\nSearching for 'swamp sun photo'\nSearching for 'swamp shade photo'\nSearching for 'alligator photo'\nSearching for 'alligator sun photo'\nSearching for 'alligator shade photo'\n\n\nNext, I’ll train my model using the code they have provided.\nThe get_image_files function is a fastai function which takes a Path object and returns an L object with paths to the image files.\n\ntype(get_image_files(path))\n\nfastcore.foundation.L\n\n\n\nget_image_files(path)\n\n(#349) [Path('alligator_or_not/swamp/1b3c3a61-0f7f-4dc2-a704-38202d593207.jpg'),Path('alligator_or_not/swamp/9c9141f2-024c-4e26-b343-c1ca1672fde8.jpeg'),Path('alligator_or_not/swamp/1340dd85-5d98-428e-a861-d522c786c3d7.jpg'),Path('alligator_or_not/swamp/2d3f91dc-cc5f-499b-bec6-7fa0e938fb13.jpg'),Path('alligator_or_not/swamp/84afd585-ce46-4016-9a09-bd861a5615db.jpg'),Path('alligator_or_not/swamp/6222f0b6-1f5f-43ec-b561-8e5763a91c61.jpg'),Path('alligator_or_not/swamp/a71c8dcb-7bbb-4dba-8ae6-8a780d5c27c6.jpg'),Path('alligator_or_not/swamp/bbd1a832-a901-4e8f-8724-feac35fa8dcb.jpg'),Path('alligator_or_not/swamp/45b358b3-1a12-41d4-8972-8fa98b2baa52.jpg'),Path('alligator_or_not/swamp/cf664509-8eb6-42c8-9177-c17f48bc026b.jpg')...]\n\n\nThe fastai parent_label function takes a Path object and returns a string of the file’s parent folder name.\n\nparent_label(Path('alligator_or_not/swamp/18b55d4f-3d3b-4013-822b-724489a23f01.jpg'))\n\n'swamp'\n\n\nSome image files that are downloaded may be corrupted, so they have provided a verify_images function to find images that can’t be opened. Those images are then removed (unlinked) from the path.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\n\nfailed\n\n(#1) [Path('alligator_or_not/alligator/1eb55508-274b-4e23-a6ae-dbbf1943a9d1.jpg')]\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\nI’ll train the model using their code which uses the resnet18 image classification model, and fine_tunes it for 3 epochs.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.690250\n      0.171598\n      0.043478\n      00:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.127188\n      0.001747\n      0.000000\n      00:02\n    \n    \n      1\n      0.067970\n      0.006409\n      0.000000\n      00:02\n    \n    \n      2\n      0.056453\n      0.004981\n      0.000000\n      00:02\n    \n  \n\n\n\nThe accuracy is 100%.\nNext, I’ll test the model as they’ve done in the lesson.\n\nPILImage.create('alligator.jpg').to_thumb(256,256)\n\n\n\n\n\nis_alligator,_,probs = learn.predict(PILImage.create('alligator.jpg'))\nprint(f\"This is an: {is_alligator}.\")\nprint(f\"Probability it's an alligator: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is an: alligator.\nProbability it's an alligator: 1.0000\n\n\n\n\n\nIn this section, I’ll take notes while I watch the lesson 1 video.\n\nThis is the fifth version of the course!\nWhat seemed impossible in 2015 (image recognition of a bird) is now free and something we can build in 2 minutes.\nAll models need numbers as their inputs. Images are already stored as numbers in computers. [PixSpy] allows you to (among other things) view the color of each pixel in an image file.\nA DataBlock gives fastai all the information it needs to create a computer vision model.\nCreating really interesting, real, working programs with deep learning is something that doesn’t take a lot of code, math, or more than a laptop computer. It’s pretty accessible.\nDeep Learning models are doing things that very few, if any of us, believed would be possible to do by computers in our lifetime.\nSee the Practical Data Ethics course as well.\nMeta Learning: How To Learn Deep Learning And Thrive In The Digital World.\nBooks on learning/education:\n\nMathematician’s Lament by Paul Lockhart\nMaking Learning Whole by David Perkins\n\nWhy are we able to create a bird-recognizer in a minute or two? And why couldn’t we do it before?\n\n2012: Project looking at 5-year survival of breast cancer patients, pre-deep learning approach\n\nAssembled a team to build ideas for thousands of features that required a lot of expertise, took years.\nThey fed these features into a logistic regression model to predict survival.\nNeural networks don’t require us to build these features, they build them for us.\n\n2015: Matthew D. Zeiler and Rob Fergus looked inside a neural network to see what it had learned.\n\nWe don’t give it features, we ask it to learn features.\nThe neural net is the basic function used in deep learning.\nYou start with a random neural network, feed it examples and you have it learn to recognize things.\nThe deeper you get, the more sophisticated the features it can find are.\nWhat we’re going to learn is how neural networks do this automatically.\nThis is the key difference in why we can now do things that we couldn’t previously conceive of as possible.\n\n\nAn image recognizer can also be used to classify sounds (pictures of waveforms).\nTurning time series into pictures for image classification.\nfastai is built on top of PyTorch.\n!pip install -Uqq fastai to update.\nAlways view your data at every step of building a model.\nFor computer vision algorithms you don’t need particularly big images.\nFor big images, most of the time is taken up opening it, the neural net on the GPU is must faster.\nThe main thing you’re going to try and figure out is how do I get this data into my model?\nDataBlock\n\nblocks=(ImageBlock, CategoryBlock): ImageBlock is the type of input to the model, CategoryBlock is the type of model output\nget_image_files(path) returns a list of all image files in a path.\nIt’s critical that you put aside some data for testing the accuracy of your model (validation set) with something like RandomSplitter for the splitter parameter.\nget_y tells fastai how to get the correct label for the photo.\nMost computer vision architectures need all of your inputs to be the same size, using Resize (either crop out a piece in the middle or squish the image) for the parameter item_tfms.\nDataLoaders contains iterators that PyTorch can run through to grab batches of your data to feed the training algorithm.\nshow_batch shows you a batch of input/label pairs.\nA Learner combines a model (the actual neural network that we are training) and the data we use to train it with.\nPyTorch Image Models (timm).\nresnet has already been trained to recognize over 1 million images of over 1000 different types. fastai downloads this so you can start with a neural network that can do a lot.\nfine_tune takes those pretrained weights downloaded for you and adjusts them in a carefully controlled way to teach the model differences between your dataset and what it was originally trained for.\nYou pass .predict an image, which is how you would deploy your model, returns whether it’s a bird or not as a string, integer and probability of whether it’s a bird (in this example).\n\n\nIn the code blocks below, I’ll train the different types of models presented in the video lesson.\n\n\n\nfrom fastai.vision.all import *\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs = 8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.454409\n      3.015761\n      00:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.928762\n      1.719756\n      00:02\n    \n    \n      1\n      1.649520\n      1.394089\n      00:02\n    \n    \n      2\n      1.533350\n      1.344445\n      00:02\n    \n    \n      3\n      1.414438\n      1.279674\n      00:02\n    \n    \n      4\n      1.291168\n      1.063977\n      00:02\n    \n    \n      5\n      1.174492\n      0.980055\n      00:02\n    \n    \n      6\n      1.073124\n      0.931532\n      00:02\n    \n    \n      7\n      0.992161\n      0.922516\n      00:02\n    \n  \n\n\n\n\nlearn.show_results(max_n=3, figsize=(7,8))\n\n\n\n\n\n\n\n\n\n\n\nIt’s amazing how many it’s getting correct because this model was trained in about 24 seconds using a tiny amount of data.\nI’ll take a look at the codes out of curiousity, which is an array of string elements describing different objects in view.\n\nnp.loadtxt(path/'codes.txt', dtype=str)\n\narray(['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'Tunnel',\n       'VegetationMisc', 'Void', 'Wall'], dtype='<U17')\n\n\n\n\n\n\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ADULT_SAMPLE)\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names='salary',\n                                  cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                                               'relationship', 'race'],\n                                  cont_names = ['age', 'fnlwgt', 'education-num'],\n                                  procs = [Categorify, FillMissing, Normalize])\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      workclass\n      education\n      marital-status\n      occupation\n      relationship\n      race\n      education-num_na\n      age\n      fnlwgt\n      education-num\n      salary\n    \n  \n  \n    \n      0\n      State-gov\n      Some-college\n      Divorced\n      Adm-clerical\n      Own-child\n      White\n      False\n      42.0\n      138162.000499\n      10.0\n      <50k\n    \n    \n      1\n      Private\n      HS-grad\n      Married-civ-spouse\n      Other-service\n      Husband\n      Asian-Pac-Islander\n      False\n      40.0\n      73025.003080\n      9.0\n      <50k\n    \n    \n      2\n      Private\n      Assoc-voc\n      Married-civ-spouse\n      Prof-specialty\n      Wife\n      White\n      False\n      36.0\n      163396.000571\n      11.0\n      >=50k\n    \n    \n      3\n      Private\n      HS-grad\n      Never-married\n      Sales\n      Own-child\n      White\n      False\n      18.0\n      110141.999831\n      9.0\n      <50k\n    \n    \n      4\n      Self-emp-not-inc\n      12th\n      Divorced\n      Other-service\n      Unmarried\n      White\n      False\n      28.0\n      33035.002716\n      8.0\n      <50k\n    \n    \n      5\n      ?\n      7th-8th\n      Separated\n      ?\n      Own-child\n      White\n      False\n      50.0\n      346013.994175\n      4.0\n      <50k\n    \n    \n      6\n      Self-emp-inc\n      HS-grad\n      Never-married\n      Farming-fishing\n      Not-in-family\n      White\n      False\n      36.0\n      37018.999571\n      9.0\n      <50k\n    \n    \n      7\n      State-gov\n      Masters\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      White\n      False\n      37.0\n      239409.001471\n      14.0\n      >=50k\n    \n    \n      8\n      Self-emp-not-inc\n      Doctorate\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      White\n      False\n      50.0\n      167728.000009\n      16.0\n      >=50k\n    \n    \n      9\n      Private\n      HS-grad\n      Married-civ-spouse\n      Tech-support\n      Husband\n      White\n      False\n      38.0\n      247111.001513\n      9.0\n      >=50k\n    \n  \n\n\n\nFor tabular models, there’s not generally going to be a pretrained model that already does something like what you want because every table of data is very different, so generally it doesn’t make too much sense to fine_tune a tabular model.\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.373780\n      0.365976\n      0.832770\n      00:06\n    \n    \n      1\n      0.356514\n      0.358780\n      0.833999\n      00:05\n    \n  \n\n\n\n\n\n\nThe basis of most recommendation systems.\n\nfrom fastai.collab import *\npath = untar_data(URLs.ML_SAMPLE)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n    \n  \n  \n    \n      0\n      457\n      457\n      3.0\n    \n    \n      1\n      407\n      2959\n      5.0\n    \n    \n      2\n      294\n      356\n      4.0\n    \n    \n      3\n      78\n      356\n      5.0\n    \n    \n      4\n      596\n      3578\n      4.5\n    \n    \n      5\n      547\n      541\n      3.5\n    \n    \n      6\n      105\n      1193\n      4.0\n    \n    \n      7\n      176\n      4993\n      4.5\n    \n    \n      8\n      430\n      1214\n      4.0\n    \n    \n      9\n      607\n      858\n      4.5\n    \n  \n\n\n\nThere’s actually no pretrained collaborative filtering model so we could use fit_one_cycle but fine_tune works here as well.\n\nlearn = collab_learner(dls, y_range=(0.5, 5.5))\nlearn.fine_tune(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.498450\n      1.417215\n      00:00\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.375927\n      1.357755\n      00:00\n    \n    \n      1\n      1.274781\n      1.176326\n      00:00\n    \n    \n      2\n      1.033917\n      0.870168\n      00:00\n    \n    \n      3\n      0.810119\n      0.719341\n      00:00\n    \n    \n      4\n      0.704180\n      0.679201\n      00:00\n    \n    \n      5\n      0.640635\n      0.667121\n      00:00\n    \n    \n      6\n      0.623741\n      0.661391\n      00:00\n    \n    \n      7\n      0.620811\n      0.657624\n      00:00\n    \n    \n      8\n      0.606947\n      0.656678\n      00:00\n    \n    \n      9\n      0.605081\n      0.656613\n      00:00\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      15.0\n      35.0\n      4.5\n      3.886339\n    \n    \n      1\n      68.0\n      64.0\n      5.0\n      3.822170\n    \n    \n      2\n      62.0\n      33.0\n      4.0\n      3.088149\n    \n    \n      3\n      39.0\n      91.0\n      4.0\n      3.788227\n    \n    \n      4\n      37.0\n      7.0\n      5.0\n      4.434169\n    \n    \n      5\n      38.0\n      98.0\n      3.5\n      4.380877\n    \n    \n      6\n      3.0\n      25.0\n      3.0\n      3.443295\n    \n    \n      7\n      23.0\n      13.0\n      2.0\n      3.220192\n    \n    \n      8\n      15.0\n      7.0\n      4.0\n      4.306846\n    \n  \n\n\n\nNote: RISE turnes your notebook into a presentation.\nGenerally speaking, if it’s something that a human can do reasonably quickly, even an expert human (like look at a Go board and decide if it’s a good board or not) then that’s probably something that deep learning will probably be good at. If it’s something that takes logical thought process over time, particularly if it’s not based on much data, deep learning probably won’t do that well.\nThe first neural network was built in 1957. The basic ideas have not changed much at all.\nWhat’s going on in these models?\n\nArthur Samuel in late 1950s invented Machine Learning.\nNormal program: input -> program -> results.\nMachine Learning model: input and weights (parameters) -> model -> results.\n\nThe model is a mathematical function that takes the input, multiplies them with one set of weights and adds them up, then does that again for a second set of weights, and so forth.\nIt takes all of the negative numbers and replaces them with 0.\nIt takes all those numbers as inputs to the next layer.\nAnd it repeats a few times.\n\nWeights start out as being random.\nA more useful workflow: input/weights -> model -> results -> loss -> update weights.\nThe loss is a number that says how good the results were.\nWe need a way to come up with a new set of weights that are a bit better than the current weights.\n“bit better” weights means it makes the loss a bit better.\nIf we make it a little bit better a few times, it’ll eventually get good.\nNeural nets proven to solve any computable function (i.e. it’s flexible enough to update weights until the results are good).\n“Generate artwork based on someone’s twitter bio” is a computable function.\nOnce we’ve finished the training procedure we don’t the loss and the weights can be integrated into the model.\nWe end up with inputs -> model -> results which looks like our original idea of a program.\nDeploying a model will have lots of tricky details but there will be one line of code which says learn.predict which takes an input and provides results.\nThe most important thing to do is experiment.\n\n\n\n\n\nChapter 1: Your Deep Learning Journey In this section, I’ll take notes while I read Chapter 1 in the textbook.\n\n\n\nWhat you don’t need for deep learning: lots of math, lots of data, lots of expensive computers.\nDeep learning is a computer technique to extract and transform data by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task.\n\n\n\n\n\nWarren McCulloch and Walter Pitts developed a mathematical model of an artificial neuron in 1943.\nMost of Pitt’s famous work was done while he was homeless.\nPsychologist Frank Rosenblatt further developed the artificial neuron to give it the ability to learn and built the first device that used these principles, the Mark I Perceptron, which was able to recognize simple shapes.\nMarvin Minsky and Seymour Papert wrote a book about the Perceptron and showed that using multiple layers of the devices would allow the limitations of a single layer to be addressed.\nThe 1986 book Parallel Distributed Processing (PDP) by David Rumelhart, James McClelland, and the PDP Research Group defined PDP as requiring the following:\n\nA set of processing units.\nA state of activation.\nAn output function for each unit.\nA pattern of connectivity among units.\nA propogation rule for propagating patterns of activities through the network of connectivities.\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit.\nA learning rule whereby patterns of connectivity are modified by experience.\nAn environment within which the system must operate.\n\n\n\n\n\n\nThe hardest part of deep learning is artisanal: how do you know if you’ve got enough data, whether it is in the right format, if your model is training properly, and, if it’s not, what you should do about it?\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path,\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=is_cat,\n    item_tfms=Resize(224)\n)\n\ndls.show_batch()\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:11<00:00]\n    \n    \n\n\n\n\n\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n/usr/local/lib/python3.10/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 162MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.140327\n      0.019135\n      0.007442\n      01:05\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      4.17% [1/24 00:01<00:34]\n    \n    \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.070464\n      0.024966\n      0.006766\n      01:00\n    \n  \n\n\n\nThe error rate is the proportion of images that were incorrectly identified.\nCheck this model actually works with an image of a dog or cat. I’ll download a picture from google and use it for prediction:\n\nimport ipywidgets as widgets\nuploader = widgets.FileUpload()\nuploader\n\n\n\n\n\nim = PILImage.create(uploader.data[0])\nis_cat, _, probs = learn.predict(im)\nim.to_thumb(256)\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'Is this a cat?: {is_cat}.')\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\n\nIs this a cat?: True.\nProbability it's a cat: 1.000000\n\n\n\n\n\n\nA traditional program: inputs -> program -> results.\nIn 1949, IBM researcher Arthur Samuel started working on machine learning. His basic idea was this: instead of telling the computer the exact steps required to solve a problem, show it examples of the problem to solve, and let it figure out how to solve it itself.\nIn 1961 his checkers-playing program had learned so much that it beat the Connecticut state champion.\nWeights are just variables and a weight assignment is a particular choice of values for those variables.\nThe program’s inputs are values that it processes in order to produce its results (for instance, taking image pixels as inputs, and returning the classification “dog” as a result).\nBecause the weights affect the program, they are in a sense another kind of input.\nA program using weight assignment: inputs and weights -> model -> results.\nA model is a special kind of program, on that can do many different things depending on the weights.\nWeights = parameters, with the term “weights” reserved for a particulat type of model parameter.\nLearning would become entirely automatic when the adjustment of the weights was also automatic.\nTraining a maching learning model: inputs and weights -> model -> results -> performance -> update weights.\nresults are different than the performance of a model.\nUsing a trained model as a program -> inputs -> model -> results.\nmaching learning is the training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.\n\n\n\n\n\nNeural networks is a mathematical function that can solve any problem to any level of accuracy.\nStochastic Gradient Descent (SGD) is a completely general way to update the weights of a neural network, to make it improve at any given task.\nImage classification problem:\n\nOur inputs are the images.\nOur weights are the weights in the neural net.\nOur model is a neural net.\nOur results are the values that are calculated by the neural net, like “dog” or “cat”.\n\n\n\n\n\n\nThe functional form of the model is called its architecture.\nThe weights are called parameters.\nThe predictions are calculated from the independent variable, which is the data not including the labels.\nThe results or the model are called predictions.\nThe measure of performance is called the loss.\nThe loss depends not only on the predictions, but also on the correct labels (also known as targets or the dependent variable).\nDetailed training loop: inputs and parameters -> architecture -> predictions (+ labels) -> loss -> update parameters.\n\n\n\n\n\nA model cannot be created without data.\nA model can learn to operate on only the patterns seen in the input data used to train it.\nThis learning approach creates only predictions, not recommended actions.\nIt’s not enough to just have examples of input data, we need labels for that data too.\nPositive feedback loop: the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.\n\n\n\n\n\nitem_tfms are applied to each item while batch_tfms are applied to a batch of items at a time using the GPU.\nA classification model attempts to predict a class, or category.\nA regression model is one that attempts to predict one or more numeric quantities, such as temperature or location.\nThe parameter seed=42 sets the random seed to the same value every time we run this code, which means we get the same validation set every time we run it. This way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.\nWe care about how well our model works on previously unseen images.\nThe longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is overfitting.\nOverfitting is the single most important and challenging issue when training for all machine learning practitioners, and all algorithms.\nYou should only use methods to avoid overfitting after you have confirmed that overfitting is occurring (i.e., if you have observed the validation accuracy getting worse during training)\nfastai defaults to valid_pct=0.2.\nModels using architectures with more layers take longer to train and are more prone to overfitting, on the other hand, when using more data, they can be quite a bit more accurate.\nA metric is a function that measures the quality of the model’s predictions using the validation set.\nerror_rate tells you what percentage of inputs in the validation set are being classified incorrectly.\naccuracy = 1.0 - error_rate.\nThe entire purpose of loss is to define a “measure of performance” that the training system can use to update weights automatically. A good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand.\nA model that has weights that have already been trained on another dataset is called a pretrained model.\nWhen using a pretrained model, cnn_learner will remove the last layer and replace it with one or more new layers with randomized weights. This last part of the model is known as the head.\nUsing a pretrained model for a task different from what is was originally trained for is known as transfer learning.\nThe architecture only describes a template for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains.\nTo fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of epochs).\nfit will fit a model (i.e., look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels).\nFine-Tuning: a transfer learning technique that updates the parameters of a pretrained model by training for additional epochs using a different task from that used for pretraining.\nfine_tune has a few parameters you can set, but in the default form it does two steps:\n\nUse one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\nUse the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which don’t require many changes from the pretrained weights).\n\nThe head of the model is the part that is newly added to be specific to the new dataset.\nAn epoch is one complete pass through the dataset.\n\n\n\n\n\nWhen we fine tune our pretrained models, we adapt what the last layers focus on to specialize on the problem at hand.\n\n\n\n\n\nA lot of things can be represented as images.\nSound can be converted to a spectogram.\nTimes series data can be created into an image using Gramian Angular Difference Field (GADF).\nIf the human eye can recognize categories from the images, then a deep learning model should be able to do so too.\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nLabel\nThe data that we’re trying to predict\n\n\nArchitecture\nThe template of the model that we’re trying to fit; i.e., the actual mathematical function that we’re passing the input data and parameters to\n\n\nModel\nThe combination of the architecture with a particular set of parameters\n\n\nParameters\nThe values in the model that change what task it can do and that are updated through model training\n\n\nFit\nUpdate the parameters of the model such that the predictions of the model using the input data match the target labels\n\n\nTrain\nA synonym for fit\n\n\nPretrained Model\nA model that has already been trained, generally using a large dataset, and will be fine-tuned\n\n\nFine-tune\nUpdate a pretrained model for a different task\n\n\nEpoch\nOne complete pass through the input data\n\n\nLoss\nA measure of how good the model is, chosen to drive training via SGD\n\n\nMetric\nA measurement of how good the model is using the validation set, chosen for human consumption\n\n\nValidation set\nA set of data held out from training, used only for measuring how good the model is\n\n\nTraining set\nThe data used for fitting the model; does not include any data from the validation set\n\n\nOverfitting\nTraining a model in such a way that it remembers specific features of the input data, rather than generalizing wel to data not seen during training\n\n\nCNN\nConvolutional neural network; a type of neural network that works particularly well for computer vision tasks\n\n\n\n\n\n\n\nSegmentation\nNatural language processing (see below)\nTabular (see Adults income classification above)\nCollaborative filtering (see MovieLens ratings predictor above)\nStart by using one of the cut-down dataset versions and later scale up to the full-size version. This is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and use the full dataset only when they have a good understanding of what they have to do.\n\n\n\n\n\nIf the model makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by actually having seen that particular item.\nHyperparameters: various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors.\nWe, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values and we are in danger of overfitting the validation data through human trial and error and exploration.\nThe test set can be used only to evaluate the model at the very end of our efforts.\nTraining data is fully exposed to training and modeling processes, validation data is less exposed and test data is fully hidden.\nThe test and validation sets should have enough data to ensure that you get a good estimate of your accuracy.\nThe discipline of the test set helps us keep ourselves intellectually honest.\nIt’s a good idea for you to try out a simple baseline model yourself, so you know what a really simply model can achieve.\n\n\n\n\n\nA key property of the validation and test sets is that they must be representative of the new data you will see in the future.\nAs an example, for time series data, use earlier dates for training set and later more recent dates as validation set\nThe data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\n\n\nfrom fastai.text.all import *\n\n# I'm using IMDB_SAMPLE instead of the full IMDB dataset since it either takes too long or\n# I get a CUDA Out of Memory error if the batch size is more than 16 for the full dataset\n# Using a batch size of 16 with the sample dataset works fast\ndls = TextDataLoaders.from_csv(\n    path=untar_data(URLs.IMDB_SAMPLE),\n    csv_fname='texts.csv',\n    text_col=1,\n    label_col=0,\n    bs=16)\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is\n      negative\n    \n    \n      1\n      xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot … which is highly believable to xxunk . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies\n      positive\n    \n    \n      2\n      xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n\\n xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after 20 - odd years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \\n\\n xxmaj none of this excuses him this present ,\n      negative\n    \n    \n      3\n      xxbos i really wanted to love this show . i truly , honestly did . \\n\\n xxmaj for the first time , gay viewers get their own version of the \" the xxmaj bachelor \" . xxmaj with the help of his obligatory \" hag \" xxmaj xxunk , xxmaj james , a good looking , well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ) . xxmaj the only problem is half of them are straight and xxmaj james does n't know this . xxmaj if xxmaj james picks a gay one , they get a trip to xxmaj new xxmaj zealand , and xxmaj if he picks a straight one , straight guy gets $ 25 , xxrep 3 0 . xxmaj how can this not be fun\n      negative\n    \n    \n      4\n      xxbos xxmaj many neglect that this is n't just a classic due to the fact that it 's the first 3d game , or even the first xxunk - up . xxmaj it 's also one of the first xxunk games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - xxunk gaming experience in general . xxmaj with graphics that are terribly dated today , the game xxunk you into the role of xxunk even * think * xxmaj i 'm going to attempt spelling his last name ! ) , an xxmaj american xxup xxunk . caught in an underground bunker . xxmaj you fight and search your way through xxunk in order to achieve different xxunk for the six xxunk , let 's face it , most of them are just an excuse to hand you a weapon\n      positive\n    \n    \n      5\n      xxbos xxmaj i 'm sure things did n't exactly go the same way in the real life of xxmaj homer xxmaj hickam as they did in the film adaptation of his book , xxmaj rocket xxmaj boys , but the movie \" october xxmaj sky \" ( an xxunk of the book 's title ) is good enough to stand alone . i have not read xxmaj hickam 's memoirs , but i am still able to enjoy and understand their film adaptation . xxmaj the film , directed by xxmaj joe xxmaj xxunk and written by xxmaj lewis xxmaj xxunk , xxunk the story of teenager xxmaj homer xxmaj hickam ( jake xxmaj xxunk ) , beginning in xxmaj october of 1957 . xxmaj it opens with the sound of a radio broadcast , bringing news of the xxmaj russian satellite xxmaj xxunk , the first artificial satellite in\n      positive\n    \n    \n      6\n      xxbos xxmaj to review this movie , i without any doubt would have to quote that memorable scene in xxmaj tarantino 's \" pulp xxmaj fiction \" ( xxunk ) when xxmaj jules and xxmaj vincent are talking about xxmaj mia xxmaj wallace and what she does for a living . xxmaj jules tells xxmaj vincent that the \" only thing she did worthwhile was pilot \" . xxmaj vincent asks \" what the hell is a pilot ? \" and xxmaj jules goes into a very well description of what a xxup tv pilot is : \" well , the way they make shows is , they make one show . xxmaj that show 's called a ' pilot ' . xxmaj then they show that show to the people who make shows , and on the strength of that one show they decide if they 're going to\n      negative\n    \n    \n      7\n      xxbos xxmaj how viewers react to this new \" adaption \" of xxmaj shirley xxmaj jackson 's book , which was promoted as xxup not being a remake of the original 1963 movie ( true enough ) , will be based , i suspect , on the following : those who were big fans of either the book or original movie are not going to think much of this one … and those who have never been exposed to either , and who are big fans of xxmaj hollywood 's current trend towards \" special effects \" being the first and last word in how \" good \" a film is , are going to love it . \\n\\n xxmaj things i did not like about this adaption : \\n\\n 1 . xxmaj it was xxup not a true adaption of the book . xxmaj from the xxunk i had\n      negative\n    \n    \n      8\n      xxbos xxmaj the trouble with the book , \" memoirs of a xxmaj geisha \" is that it had xxmaj japanese xxunk but underneath the xxunk it was all an xxmaj american man 's way of thinking . xxmaj reading the book is like watching a magnificent ballet with great music , sets , and costumes yet performed by xxunk animals dressed in those xxunk far from xxmaj japanese ways of thinking were the characters . \\n\\n xxmaj the movie is n't about xxmaj japan or real geisha . xxmaj it is a story about a few xxmaj american men 's mistaken ideas about xxmaj japan and geisha xxunk through their own ignorance and misconceptions . xxmaj so what is this movie if it is n't about xxmaj japan or geisha ? xxmaj is it pure fantasy as so many people have said ? xxmaj yes , but then why\n      negative\n    \n  \n\n\n\n\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.629276\n      0.553454\n      0.740000\n      00:19\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.466581\n      0.548400\n      0.740000\n      00:30\n    \n    \n      1\n      0.410401\n      0.418941\n      0.825000\n      00:30\n    \n    \n      2\n      0.286162\n      0.410872\n      0.830000\n      00:31\n    \n    \n      3\n      0.192047\n      0.405275\n      0.845000\n      00:31\n    \n  \n\n\n\n\n# view actual vs prediction\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n      category_\n    \n  \n  \n    \n      0\n      xxbos xxmaj this film sat on my xxmaj xxunk for weeks before i watched it . i xxunk a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj xxunk . \\n\\n xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" la xxmaj xxunk , \" based on a play by xxmaj arthur xxmaj xxunk , who is given an \" inspired by \" credit . xxmaj it starts from one person , a prostitute , standing on a street corner in xxmaj brooklyn . xxmaj she is picked up by a home contractor , who has sex with her on the hood of a car , but ca n't come . xxmaj he refuses to pay her . xxmaj when he 's off xxunk , she\n      positive\n      positive\n    \n    \n      1\n      xxbos xxmaj bonanza had a great cast of wonderful actors . xxmaj xxunk xxmaj xxunk , xxmaj pernell xxmaj whitaker , xxmaj michael xxmaj xxunk , xxmaj dan xxmaj blocker , and even xxmaj guy xxmaj williams ( as the cousin who was brought in for several episodes during 1964 to replace xxmaj adam when he was leaving the series ) . xxmaj the cast had chemistry , and they seemed to genuinely like each other . xxmaj that made many of their weakest stories work a lot better than they should have . xxmaj it also made many of their best stories into great western drama . \\n\\n xxmaj like any show that was shooting over thirty episodes every season , there are bound to be some weak ones . xxmaj however , most of the time each episode had an interesting story , some kind of conflict ,\n      positive\n      negative\n    \n    \n      2\n      xxbos i watched xxmaj grendel the other night and am compelled to put together a xxmaj public xxmaj service xxmaj announcement . \\n\\n xxmaj grendel is another version of xxmaj beowulf , the thousand - year - old xxunk - saxon epic poem . xxmaj the scifi channel has a growing catalog of xxunk and uninteresting movies , and the previews promised an xxunk low - budget mini - epic , but this one xxunk to let me switch xxunk . xxmaj it was xxunk , xxunk , bad . i watched in xxunk and horror at the train wreck you could n't tear your eyes away from . i reached for a xxunk and managed to capture part of what i was seeing . xxmaj the following may contain spoilers or might just save your xxunk . xxmaj you 've been warned . \\n\\n - xxmaj just to get\n      negative\n      negative\n    \n    \n      3\n      xxbos xxmaj this is the last of four xxunk from xxmaj france xxmaj i 've xxunk for viewing during this xxmaj christmas season : the others ( in order of viewing ) were the uninspired xxup the xxup black xxup tulip ( 1964 ; from the same director as this one but not nearly as good ) , the surprisingly effective xxup lady xxmaj oscar ( 1979 ; which had xxunk as a xxmaj japanese manga ! ) and the splendid xxup cartouche ( xxunk ) . xxmaj actually , i had watched this one not too long ago on late - night xxmaj italian xxup tv and recall not being especially xxunk over by it , so that i was genuinely surprised by how much i enjoyed it this time around ( also bearing in mind the xxunk lack of enthusiasm shown towards the film here and elsewhere when\n      positive\n      positive\n    \n    \n      4\n      xxbos xxmaj this is not really a zombie film , if we 're xxunk zombies as the dead walking around . xxmaj here the protagonist , xxmaj xxunk xxmaj louque ( played by an unbelievably young xxmaj dean xxmaj xxunk ) , xxunk control of a method to create zombies , though in fact , his ' method ' is to mentally project his thoughts and control other living people 's minds turning them into hypnotized slaves . xxmaj this is an interesting concept for a movie , and was done much more effectively by xxmaj xxunk xxmaj lang in his series of ' dr . xxmaj mabuse ' films , including ' dr . xxmaj mabuse the xxmaj xxunk ' ( 1922 ) and ' the xxmaj testament of xxmaj dr . xxmaj mabuse ' ( 1933 ) . xxmaj here it is unfortunately xxunk to his quest to\n      negative\n      positive\n    \n    \n      5\n      xxbos \" once upon a time there was a charming land called xxmaj france … . xxmaj people lived happily then . xxmaj the women were easy and the men xxunk in their favorite xxunk : war , the only xxunk of xxunk which the people could enjoy . \" xxmaj the war in question was the xxmaj seven xxmaj year 's xxmaj war , and when it was noticed that there were more xxunk of soldiers than soldiers , xxunk were sent out to xxunk the ranks . \\n\\n xxmaj and so it was that xxmaj fanfan ( gerard xxmaj philipe ) , caught xxunk a farmer 's daughter in a pile of hay , escapes marriage by xxunk in the xxmaj xxunk xxunk … but only by first believing his future as xxunk by a gypsy , that he will win fame and fortune in xxmaj his xxmaj\n      positive\n      positive\n    \n    \n      6\n      xxbos xxup ok , let me again admit that i have n't seen any other xxmaj xxunk xxmaj ivory ( the xxunk ) films . xxmaj nor have i seen more celebrated works by the director , so my capacity to xxunk xxmaj before the xxmaj rains outside of analysis of the film itself is xxunk . xxmaj with that xxunk , let me begin . \\n\\n xxmaj before the xxmaj rains is a different kind of movie that does n't know which genre it wants to be . xxmaj at first , it pretends to be a romance . xxmaj in most romances , the protagonist falls in love with a supporting character , is separated from the supporting character , and is ( sometimes ) united with his or her partner . xxmaj this movie 's hero has already won the heart of his lover but can not\n      negative\n      negative\n    \n    \n      7\n      xxbos xxmaj first off , anyone looking for meaningful \" outcome xxunk \" cinema that packs some sort of social message with meaningful performances and soul searching dialog spoken by dedicated , xxunk , heartfelt xxunk , please leave now . xxmaj you are wasting your time and life is short , go see the new xxmaj xxunk xxmaj jolie movie , have a good cry , go out & buy a xxunk car or throw away your conflict xxunk if that will make you feel better , and leave us alone . \\n\\n xxmaj do n't let the door hit you on the way out either . xxup the xxup incredible xxup melting xxup man is a grade b minus xxunk horror epic shot in the xxunk of xxmaj oklahoma by a young , xxup tv friendly cast & crew , and concerns itself with an astronaut who is\n      positive\n      negative\n    \n    \n      8\n      xxbos \" national xxmaj treasure \" ( 2004 ) is a thoroughly misguided xxunk - xxunk of plot xxunk that borrow from nearly every xxunk and dagger government conspiracy cliché that has ever been written . xxmaj the film stars xxmaj nicholas xxmaj cage as xxmaj benjamin xxmaj xxunk xxmaj xxunk ( how precious is that , i ask you ? ) ; a seemingly normal fellow who , for no other reason than being of a xxunk of like - minded misguided fortune hunters , decides to steal a ' national treasure ' that has been hidden by the xxmaj united xxmaj states xxunk fathers . xxmaj after a bit of subtext and background that plays laughably ( unintentionally ) like xxmaj indiana xxmaj jones meets xxmaj the xxmaj patriot , the film xxunk into one misguided xxunk after another  attempting to create a ' stanley xxmaj xxunk\n      negative\n      negative\n    \n  \n\n\n\n\nreview_text = \"I really liked the movie!\"\nlearn.predict(review_text)\n\n\n\n\n\n\n\n\n('positive', tensor(1), tensor([0.0174, 0.9826]))\n\n\n\n\n\n\n\nDo you need these for deep learning?\n\nLots of Math (FALSE).\nLots of Data (FALSE).\nLots of expensive computers (FALSE).\nA PhD (FALSE).\n\nName five areas where deep learning is now the best tool in the world\n\nNatural Language Processing (NLP).\nComputer vision.\nMedicine.\nImage generation.\nRecommendation systems.\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nMark I Perceptron.\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nA series of processing units.\nA state of activation.\nAn output function for each unit.\nA pattern of connectivity among units.\nA propagation rule for propagating patterns of activities through the network of connectivities.\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit.\nA learning rule whereby patterns of connectivity are modified by experience.\nAn environment within which the system must operate.\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\nUsing multiple layers of the device would allow limitations of one layer to be addressed—this was ignored.\nMore than two layers are needed to get practical, good perforamnce—only in the last decade has this been more widely appreciated and applied.\n\nWhat is a GPU?\n\nA Graphical Processing Unit, which can perform thousands of tasks at the same time.\n\nOpen a notebook and execute a cell containing: 1+1. What happens?\n\nDepending on the server, it may take some time for the output to generate, but running this cell will output 2.\n\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.\n\n(I did this for the notebook shared for Lesson 1).\n\nComplete the Jupyter Notebook online appendix.\n\nDone. Will reference some of it again.\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\n\nBecause it’s hard to instruct a computer clear instructions to recognize images.\n\nWhat did Samuel mean by “weight assignment”?\n\nA particular choice for weights (variables)\n\nWhat term do we normally use in deep learning for what Samuel called “weights”?\n\nParameters\n\nDraw a picture that summarizes Samuel’s view of a machine learning model\n\ninput and weights -> model -> results -> performance -> update weights/inputs\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\nBecause a deep learning model has many layers and connectivities and activations between neurons that are not intuitive to our understanding.\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\n\nUniversal approximation theorem.\n\nWhat do you need in order to train a model?\n\nLabeled data (Inputs and targets).\nArchitecture.\nInitial weights.\nA measure of performance (loss, accuracy).\nA way to update the model (SGD).\n\nHow could a feedback loop impact the rollout of a predictive policing model?\n\nThe model will end up predicting where arrests are made, not where crime is taking place, so more police officers will go to locations where more arrests are predicted and feed that data back to the model which will reinforce the prediction of arrests in those areas, continuing this feedback loop of predictions -> arrests -> predictions.\n\nDo we always have to use 224x224-pixel images with the cat recognition model?\n\nNo, that’s just the convention for image recognition models.\nYou can use larger images but it will slow down the training process (it takes longer to open up bigger images).\n\nWhat is the difference between classification and regression?\n\nClassification predicts discrete classes or categories.\nRegression predicts continuous values.\n\nWhat is a validation set? What is a test set? Why do we need them?\n\nA validation set is a dataset upon which a model’s accuracy (or metrics in general) is calculated during training, as well as the dataset upon which the performance of different hyperparameters (like batch size and learning rate) are measured.\nA test set is a dataset upon which a model’s final performance is measured, a truly unseen dataset for both the model and the practitioner\n\nWhat will fastai do if you don’t provide a validation set?\n\nSet aside a random 20% of the data as the validation set by default\n\nCan we always use a random sample for a validation set? Why or why not?\n\nNo, in situations where we want to ensure that the model’s accuracy is evaluated on data the model has not seen, we should not use a random validation set. Instead, we should create an intentional validation set. For example:\n\nFor time series data, use the most recent dates as the validation set\nFor human recognition data, use images of different people for training and validation sets\n\n\nWhat is overfitting? Provide an example.\n\nOverfitting is when a model memorizes features of the training dataset instead of learning generalizations of the features in the data. An example of this is when a model memorizes training data facial features but then cannot recognize different faces in the real world. Another example is when a model memorizes the handwritten digits in the training data, so it cannot then recognize digits written in different handwriting. Overfitting can be observed during training when the validation loss starts to increase as the training loss decreases.\n\nWhat is a metric? How does it differ from loss?\n\nA metric a measurement of how good a model is performing, chosen for human consumption. A loss is also a measurement of how good a model is performing, but it’s chosen to drive training using an optimizer.\n\nHow can pretrained models help?\n\nPretrained models are already good at recognizing many generalized features and so they can help by providing a set of weights in an architecture that are capable, reducing the amount of time you need to train a model specific to your task.\n\nWhat is the “head” of the model?\n\nThe last/top few neural network layers which are replaced with randomized weights in order to specialize your model via training on the task at hand (and not the task it was pretrained to perform).\n\nWhat kinds of features do the early layers of a CNN find? How about the later layers?\n\nEarly layers: simple features lie lines, color gradients\nLater layers: compelx features like dog faces, outlines of people\n\nAre image models useful only for photos?\n\nNo! Lots of things can be represented by images so if you can represent something (like a sound) as an image (spectogram) and differences between classes/categories are easily recognizable by the human eye, you can train an image classifier to recognize it.\n\nWhat is an architecture?\n\nA template, mathematical function, to which you pass input data to in order to fit/train a model\n\nWhat is segmentation?\n\nRecognizing different objects in an image based on pixel colors (each object is a different pixel color)\n\nWhat is y_range used for? When do we need it?\n\nIt’s used to specify the output range of a regression model. We need it when the target is a continuous value.\n\nWhat are hyperparameters?\n\nModeling choices such as network architecture, learning rates, data augmentation strategies and other higher level choices that govern the meaning of the weight parameters.\n\nWhat is the best way to avoid failures when using AI in an organization?\n\nMaking sure you have good validation and test sets to evaluate the performance of a model on real world data.\nTrying out a simple baseline model to know what level of performance such a model can achieve.\n\n\n\n\n\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?\n\nCPU vs GPU for Machine Learning\n\nCPUs process tasks in a sequential manner, GPUs process tasks in parallel.\nGPUs can have thousands of cores, processing tasks at the same time.\nGPUs have many cores processing at low speeds, CPUs have few cores processing at high speeds.\nSome algorithms are optimized for CPUs rather than GPUs (time series data, recommendation systems that need lots of memory).\nNeural networks are designed to process tasks in parallel.\n\nCPU vs GPU in Machine Learning Algorithms: Which is Better?\n\nMachine Learning Operations Preferred on CPUs\n\nRecommendation systems that involve huge memory for embedding layers.\nSupport vector machines, time-series data, algorithms that don’t require parallel computing.\nRecurrent neural networks because they use sequential data.\nAlgorithms with intensive branching.\n\nMachine Learning Operations Preferred on GPUs\n\nOperations that involve parallelism.\n\n\nWhy Deep Learning Uses GPUs\n\nNeural networks are specifically made for running in parallel.\n\n\nTry to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice.\n\nHidden Risks of Machine Learning Applied to Healthcare: Unintended Feedback Loops Between Models and Future Data Causing Model Degradation\n\nIf clinicians fully trust the machine learning model (100% adoption of the predicted label) the false positive rate (FPR) grows uncontrollably with the number of updates.\n\nRunaway Feedback Loops in Predictive Policing\n\nOnce police are deployed based on these predictions, data from observations in the neighborhood is then used to further update the model.\nDiscovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated.\nPredictive policing systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.\n\nPitfalls of Predictive Policing: An Ethical Analysis\n\nPredictive policing relies on a large database of previous crime data and forecasts where crime is likely to occur. Since the program relies on old data, those previous arrests need to be unbiased to generate unbiased forecasts.\nPeople of color are arrested far more often than white people for committing the same crime.\nRacially biased arrest data creates biased forecasts in neighborhoods where more people of color are arrested.\nIf the predictive policing algorithm is using biased data to divert more police forces towards less affluent neighborhoods and neighborhoods of color, then those neighborhoods are not receiving the same treatment as others.\n\nBias in Criminal Risk Scores Is Mathematically Inevitable, Researchers Say\n\nThe algorithm COMPAS which predicts whether a person is “high-risk” and deemed more likely to be arrested in the future, leads to being imprisoned (instead of sent to rehab) or longer sentences.\n\nCan bots discriminate? It’s a big question as companies use AI for hiring\n\nIf an older candidate makes it past the resume screening process but gets confused by or interacts poorly with the chatbot, that data could teach the algorithm that candidates with similar profiles should be ranked lower\n\nEcho chambers, rabbit holes, and ideological bias: How YouTube recommends content to real users\n\nWe find that YouTube’s algorithm pushes real users into (very) mild ideological echo chambers.\nWe found that 14 out of 527 (~3%) of our users ended up in rabbit holes.\nFinally, we found that, regardless of the ideology of the study participant, the algorithm pushes all users in a moderately conservative direction.\n\n\n\n\n\n\n\nI’m going to do things a bit differently than how I approached Lesson 1. Jeremy suggested that we first watch the video without pausing in order to understand what we’re going to do and then watch it a second time and follow along. I also want to be mindful of how long I’m running my Paperspace Gradient maching (at $0.51/hour) so that I don’t run the machine when I don’t need its GPU.\nSo, here’s how I’m going to approach Lesson 2: - Read the Chapter 2 Questionnaire so I know what I’ll be “tested” on at the end - Watch the video without taking notes or running code - Rewatch the video and take notes in this notebook - Add the Kaggle code cells to this notebook and run them in Paperspace - Read the Gradio tutorial without running code - Re-read the Gradio tutorial and follow along with my own code - Read Chapter 2 in the textbook and run code in this notebook in Paperspace - Read Chapter 2 in the textbook and take notes in this notebook (including answers to the Questionnaire)\nWith this approach, I’ll have a big picture understanding of each step of the lesson and I’ll minimize the time I’m spending running my Paperspace Gradient machine.\n\n\n\nIn this lesson we’re doing things that hasn’t been in courses like this before.\nResource: aiquizzes.com—I signed up and answered a couple of questions.\nDon’t forget the FastAI Forums\n\nClick “Summarize this Topic” to get a list of the most upvoted posts\n\nHow do we go about putting a model in production?\n\nFigure out what problem you want to solve\nFigure out how to get data for it\nGather some data\n\nUse DuckDuckGo image function\nDownload data\nGet rid of images that failed to open\n\nData cleaning\n\nBefore you clean your data, train the model\nImageClassifierCleaner can be used to clean (delete or re-label) the wrongly labeled data in the dataset\n\ncleaner orders by loss so you only need to look at the first few\n\nAlways build a model to find out what things are difficult to recognize in your data and to find the things the model can help you find that are problems in the data\n\nTrain your model again\nDeploy to HuggingFace Spaces\n\nInstall Jupyter Notebook Extensions to get features like table of contents and collapsible sections (with which you can also navigate sections using arrow keys)\nType ?? followed by function name to get source code\nType ? followed by function name to get brief info\nIf you have nbdev installed doc(<fn>) will give you link to documentation\nDifferent ways to resize an image\n\nResizeMethod.Squish (to see the whole picture with different aspect ratio)\nResizeMethod.Pad (whole image in correct aspect ratio)\n\nData Augmentation\n\nRandomResizedCrop (different bit of an image everytime)\nbatch_tfms=aug_tranforms() (images get turned, squished, warped, saturated, recolored, etc.)\n\nUse if you are training for more than 5-10 epochs\nIn memory, real-time, the image is being resized/cropped/etc.\n\n\nConfusion matrix (ClassificationInterpretation)\n\nOnly meaningful for category labels\nShows what category errors your model is making (actual vs predicted)\nIn a lot of situations this will let you know what the hard categories to classify are (e.g. breeds of pets hard to identify)\n.plot_top_losses tells us where the loss is the highest (prediction/actual/loss/probability)\n\nA loss will be bad (high) if we are wrong + confident or right + unconfident\n\n\nOn your computer, normal RAM doesn’t get filled up as it saves RAM to hard disk (swapping). GPUs don’t do swapping so do only one thing at a time so you’re not using up all the memory.\nGradio + HuggingFace Spaces\n\nHere is my Hello World HuggingFace Space!\nNext, we’ll put a deep learning model in production. In the code cells below, I will train and export a dog vs cat classifier.\n\n\n\n# import all the stuff we need from fastai\nfrom fastai.vision.all import *\nfrom fastbook import *\n\n\n# download and decompress our dataset\npath = untar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:57<00:00]\n    \n    \n\n\n\n# define a function to label our images\ndef is_cat(x): return x[0].isupper()\n\n\n# create `DataLoaders`\ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path),\n    valid_pct = 0.2,\n    seed = 42,\n    label_func = is_cat,\n    item_tfms = Resize(192))\n\n\n# view batch\ndls.show_batch()\n\n\n\n\n\n# train our model using resnet18 to keep it small and fast\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.199976\n      0.072374\n      0.020298\n      00:19\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.061802\n      0.081512\n      0.020974\n      00:20\n    \n    \n      1\n      0.047748\n      0.030506\n      0.010149\n      00:18\n    \n    \n      2\n      0.021600\n      0.026245\n      0.006766\n      00:18\n    \n  \n\n\n\n\n# export our trained learner\nlearn.export('model.pkl')\n\n\nFollowing the script in the video, as well as the git-lfs and requirements.txt in Tanishq Abraham’s tutorial, I deployed a Dog and Cat Classifier on HuggingFace Spaces.\nIf you run the training for long enough (high number of epochs) the error rate will get worse. We’ll learn why in a future lesson.\nUse fastsetup to setup your local machine with Python and Jupyter.\n\nThey recommend using mamba instead of conda as it is faster.\n\n\n\n\n\nIn the cells below, I’ll run the code provided in the Chapter 2 notebook.\n\n# prepare path and subfolder names\nbear_types = 'grizzly', 'black', 'teddy'\npath = Path('bears')\n\n\n# download images of grizzly, black and teddy bears\nif not path.exists():\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok = True)\n        results = search_images_ddg(f'{o} bear')\n        download_images(dest, urls = results)\n\n\n# view file paths\nfns = get_image_files(path)\nfns\n\n(#570) [Path('bears/grizzly/ca9c20c9-e7f4-4383-b063-d00f5b3995b2.jpg'),Path('bears/grizzly/226bc60a-8e2e-4a18-8680-6b79989a8100.jpg'),Path('bears/grizzly/2e68f914-0924-42ed-9e2e-19963fa03a37.jpg'),Path('bears/grizzly/38e2d057-3eb2-4e8e-8e8c-fa409052aaad.jpg'),Path('bears/grizzly/6abc4bc4-2e88-4e28-8ce4-d2cbdb05d7b5.jpg'),Path('bears/grizzly/3c44bb93-2ac5-40a3-a023-ce85d2286846.jpg'),Path('bears/grizzly/2c7b3f99-4c8e-4feb-9342-dacdccf60509.jpg'),Path('bears/grizzly/a59f16a6-fa06-42d5-9d79-b84e130aa4e3.jpg'),Path('bears/grizzly/d1be6dc8-da42-4bee-ac31-0976b175f1e3.jpg'),Path('bears/grizzly/7bc0d3bd-a8dd-477a-aa16-449124a1afb5.jpg')...]\n\n\n\n# get list of corrupted images\nfailed = verify_images(fns)\nfailed\n\n(#24) [Path('bears/grizzly/2e68f914-0924-42ed-9e2e-19963fa03a37.jpg'),Path('bears/grizzly/f77cfeb5-bfd2-4c39-ba36-621f117a65f6.jpg'),Path('bears/grizzly/37aa7eed-5a83-489d-b8f5-54020ba41390.jpg'),Path('bears/black/90a464ad-b0a7-4cf5-86ff-72d507857007.jpg'),Path('bears/black/f03a0ceb-4983-4b8f-a001-84a0875704e8.jpg'),Path('bears/black/6193c1cf-fda4-43f9-844e-7ba7efd33044.jpg'),Path('bears/teddy/474bdbb3-de2f-49e5-8c5b-62b4f3f50548.JPG'),Path('bears/teddy/58755f3f-227f-4fad-badc-a7d644e54296.JPG'),Path('bears/teddy/eb55dc00-3d01-4385-a7da-d81ac5211696.jpg'),Path('bears/teddy/97eadc96-dc4e-4b3f-8486-88352a3b2270.jpg')...]\n\n\n\n# remove corrupted image files\nfailed.map(Path.unlink)\n\n(#24) [None,None,None,None,None,None,None,None,None,None...]\n\n\n\n# create DataBlockfor training\nbears = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct = 0.2, seed = 42),\n    get_y = parent_label,\n    item_tfms = Resize(128)\n)\n\n\n# create DataLoaders object\ndls = bears.dataloaders(path)\n\n\n# view training batch -- looks good!\ndls.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# view validation batch -- looks good!\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# observe how images react to the \"squish\" ResizeMethod\nbears = bears.new(item_tfms = Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\nNotice how the grizzlies in the third image look abnormally skinny, since the image is squished.\n\n# observe how images react to the \"pad\" ResizeMethod\nbears = bears.new(item_tfms = Resize(128, ResizeMethod.Pad, pad_mode = 'zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\nIn these images, the original aspect ratio is maintained.\n\n# observe how images react to the transform RandomResizedCrop\nbears = bears.new(item_tfms = RandomResizedCrop(128, min_scale = 0.3))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# observe how images react to data augmentation transforms\nbears = bears.new(item_tfms=Resize(128), batch_tfms = aug_transforms(mult = 2))\ndls = bears.dataloaders(path)\n# note that data augmentation occurs on training set\ndls.train.show_batch(max_n = 8, nrows = 2, unique = True)\n\n\n\n\n\n# train the model in order to clean the data\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# train the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 100MB/s] \n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.221027\n      0.206999\n      0.055046\n      00:34\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.225023\n      0.177274\n      0.036697\n      00:32\n    \n    \n      1\n      0.162711\n      0.189059\n      0.036697\n      00:31\n    \n    \n      2\n      0.144491\n      0.191644\n      0.027523\n      00:31\n    \n    \n      3\n      0.122036\n      0.188296\n      0.018349\n      00:31\n    \n  \n\n\n\n\n# view Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model confused a grizzly for a black bear and a black bear for a grizzly bear. It didn’t confuse any of the teddy bears, which makes sense given how different they look to real bears.\n\n# view images with the highest losses\ninterp.plot_top_losses(5, nrows = 1)\n\n\n\n\n\n\n\n\n\n\n\nThe fourth image has two humans in it, which is likely why the model didn’t recognize the bear. The model correctly predicted the the third and fifth images but with low confidence (57% and 69%).\n\n# clean the training and validation sets\nfrom fastai.vision.widgets import *\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI cleaned up the images (deleting an image of a cat, another of a cartoon bear, a dog, and a blank image).\n\n# delete or move images based on the dropdown selections made in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n# create new dataloaders object\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# retrain the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.289331\n      0.243501\n      0.074074\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.225567\n      0.256021\n      0.064815\n      00:32\n    \n    \n      1\n      0.218850\n      0.288018\n      0.055556\n      00:34\n    \n    \n      2\n      0.184954\n      0.315183\n      0.055556\n      00:31\n    \n    \n      3\n      0.141363\n      0.308634\n      0.055556\n      00:31\n    \n  \n\n\n\nWeird!! After cleaning the data, the model got worse (1.8% error rate is now 5.6%). I’ll run the cleaning routine again and retrain the model to see if it makes a difference. Perhaps there are still erroneous images in the mix.\n\n# view Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis time, the model incorrectly predicted 3 grizzlies as black bears, 2 black bears as grizzlies and 1 black bear as a teddy.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# delete or move images based on the dropdown selections made in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n# create new dataloaders object\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\n# The lower right image (cartoon bear) is one that I selected \"Delete\" for\n# in the cleaner so I'm not sure why it's still there\n# I'm wondering if there's something wrong with the cleaner or how I'm using it?\ndls.show_batch()\n\n\n\n\n\n# retrain the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.270627\n      0.130137\n      0.046729\n      00:31\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.183445\n      0.078030\n      0.028037\n      00:32\n    \n    \n      1\n      0.201080\n      0.053461\n      0.018692\n      00:33\n    \n    \n      2\n      0.183515\n      0.019479\n      0.009346\n      00:37\n    \n    \n      3\n      0.144900\n      0.012682\n      0.000000\n      00:31\n    \n  \n\n\n\nI’m still not confident that this is a 100% accurate model given the bad images in the training set (such as the cartoon bear) but I’m going to go with it for now.\n\n\n\n\n\n\nUnderestimating the constraints and overestimating the capabilities of deep learning may lead to frustratingly poor results, at least until you gain some experience and can solve the problems that arise.\nOverstimating the constraints and underestimating the capabilities of deep learning may mean you do not attempt a solvable problem because you talk yourself out of it.\nThe most important thing (as you learn deep learning) is to ensure that you have a project to work on.\nThe goal is not to find the “perfect” dataset or project, but just to get started and iterate from there.\nComplete every step as well as you can in a reasonable amount of time, all the way to the end.\nComputer vision\n\nObject recognition: recognize items in an image\nObject detection: recognition + highlight the location and name of each found object.\nDeep learning algorithms are generally not good at recognizing images that are significantly different in structure or style from those used to train the model.\n\nNLP\n\nDeep learning is not good at generating correct responses.\nText generation models will always be technologically a bit ahead of models for recognizing automatically generated text.\nGoogle’s online translation system is based on deep learning.\n\nCombining text and images\n\nA deep learning model can be trained on input images with output captions written in English, and can learn to generate surprisingly appropriate captions automatically for new images (with no guarantee the captions will be correct).\nDeep learning should be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely.\n\nTabular data\n\nIf you already have a system that is using random forests or gradient boosting machines then switching to or adding deep learning may not result in any dramatic improvement.\nDeep learning greatly increases the variety of columns that you can include.\nDeep learning models generally take longer to train than random forests or gradient boosting machines.\n\nRecommendation systems\n\nA special type of tabular data (a high-cardinality categorical variable representing users and another one representing products or something similar).\nDeep learning models are good at handling high cardinality categorical variables and thus recommendation systems.\nDeep learning models do well when combining these variables with other kinds of data such as natural language, images, or additional metadata represented as tables such as user information, previous transactions, and so forth.\nNearly all machine learning approaches have th downside that they tell you only which products a particular user might like, rather than what recommendations would be helpful for a user.\n\nOther data types\n\nUsing NLP deep learning methods is the current SOTA approach for many types of protein analysis since protein chains look a lot like natural language documents.\n\nThe Drivetrain Approach\n\nDefined objective\nLevers (what inputs can we control)\nData (what inputs we can collect)\nModels (how the levers influence the objective)\n\nGathering data\n\nFor most projects you can find the data online.\nUse duckduckgo_search\n\nFrom Data to DataLoaders\n\nDataLoaders is a thin class that just stores whatever DataLoader objects you pass to it and makes them available as train and valid.\nTo turn data into a DataLoaders object we need to tell fastai four things:\n\nWhat kinds of data we are working with.\nHow to get the list of items.\nHow to label these items.\nHow to create the validation set.\n\nWith the DataBlock API you can customize every stage of the creation of your DataLoaders:\n\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\nexplanation of DataBlock\n\nblocks specifies types for independent (the thing we are using to make predictions from) and dependent (our target) variables.\nComputers don’t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time–called the seed–then you will get the exact same list each time.\nImages need to be all the same size.\nA DataLoader is a class that provides batches of a few items at a time to the GPU.\nfastai default batch size is 64 items.\nResize crops the images to fit a square shape, alternatively you can pad (ResizeMethod.Pad) or squish (ResizeMethod.Squish) the images to fit the square.\nSquishing (model learns that things look differently from how they actually are), cropping (removal of features that would allow us to perform recognition) and padding (lot of empty space which is just wasted computation) are wasteful or problematic approaches. Instead, randomly select part of the image and then crop to just that part. On each epoch, we randomly select a different part of each image (RandomResizedCrop(min_scale)).\nTraining the neural network with examples of images in which objects are in slightly different places and are slightly different sizes helps it to understand the basic concept of what an object is and how it can be represented in an image.\n\nData Augmentation\n\nrefers to creating random variations of our input data, such that they appear different but do not change the meaning of the data (rotation, flipping, perspective warping, brightness changes, and contrast changes).\naug_transforms() provides a standard set of augmentations.\nUse batch_tfms to process a batch at a time on the GPU to save time.\n\nTraining your model and using it to clean your data\n\nView confusion matrix with ClassificationInterpretation.from_learner(learn). The diagonal shows images that are classified correctly. Calculated using validation set.\nSort images by loss using interp.plot_top_losses().\nLoss is high if the model is incorrect (especially if it’s also confident) or if it’s correct but not confident.\nA model can help you find data issues more quickly.\n\nUsing the model for inference\n\nlearn.export() will export a .pkl file.\nGet predictions with learn_inf.predict(<input>). This returns three things: the predicted category in the same format you originally provided, the index of the predicted category and the probabilities for each category.\nYou can access the DataLoaders as an attribute of the Learner: learn_inf.dls.\n\nDeploying your app\n\nYou almost certainly do not need a GPU to serve your model in production.\nTo classify a few users’ images at a time, you need high-volume. If you do have this scenario, use Microsoft’s ONNX Runtime or AWS SageMaker.\nRecommended wherever possible to deploy the model itself to a server and have your mobile/edge application connect to it as a web service.\nIf your application uses sensitive data, your users may be concerned about an approach that sends that data to a remote server.\n\nHow to Avoid Disaster\n\nUnderstanding and testing the behavior of a deep learning model is much more difficult than with most other code you write.\nThe kinds of photos that people are most likely to upload to the internet are the kinds of photos that do a good job of clearly and artistically displaying their subject matter, which isn’t the kind of input this system is going to be getting in real life. We may need to do a lot of our own data collection and labeling to create a useful system.\nout-of-domain data: data that our model sees in production that is very different from what it saw during training.\ndomain shift: data that our model sees changes over time.\nDeployment process\n\nManual Process: run model in parallel, humans check all predictions.\nLimited scope deployment: careful human supervision, time or geography limited.\nGradual expansion: good reporting systems needed, consider what could go wrong.\n\nUnforeseen consequences and feedback loops\n\nYour model may change the behavior of the system it’s a part of.\nfeedback loops can result in negative implications of bias getting worse.\nA helpful exercise prior to rolling out a significant machine learning system is to consider the question “What would happen if it went really, really well?”\n\n\nQuestionnaire\n\nWhere do text models currently have a major deficiency?\n\nProviding correct or accurate information.\n\nWhat are possible negative societal implications of text generation models?\n\nThe viral spread of misinformation, which can lead to real actions and harms.\n\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a god alternative to automating a process?\n\nRun the model in parallel with a human checking its predictions.\n\nWhat kind of tabular data is deep learning particularly good at?\n\nHigh-cardinality categorical data.\n\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\n\nIt will only tell you which products a particular user might like, rather than what recommendations may be helpful for a user.\n\nWhat are the steps of the Drivetrain Approach?\n\nDefine an objective\nDetermine what inputs (levers) you can control\nCollect data\nCreate models (how the levers influence the objective)\n\nHow do the steps of the Drivetrain Approach map to a recommendation system?\n\nObjective: drive additional sales due to recommendations.\nLevel: ranking of the recommendations.\nData: must be collectd to generate recommendations that will cause new sales.\nModels: two for purchasing probabilities conditional on seeing or not seeing a recommendation, the difference between these two probabilities is a utility function for a given recommendation to a customer (low in cases when algorithm recommends a familiar book that the customer has already rejected, or a book they would have bought even without the recommendation).\n\nCreate an image recognition model using data you curate, and deploy it on the web.\n\nHere.\n\nWhat is DataLoaders?\n\nA class that creates validation and training sets/batches that are fed to the GPUS\n\nWhat four things do we need to tell fastai to create DataLoaders?\n\nWhat kinds of data we are working with (independent and dependent variables).\nHow to get the list of items.\nHow to label these items.\nHow to create the validation set.\n\nWhat does the splitter parameter to DataBlock do?\n\nSet aside a percentage of the data as the validation set.\n\nHow do we ensure a random split always gives the same validation set?\n\nSet the seed parameter to the same value.\n\nWhat letters are often used to signify the independent and dependent variables?\n\nIndependent: x\nDependent: y\n\nWhat’s the difference between crop, pad and squish resize approaches? When might you choose one over the others?\n\nCrop: takes a section of the image and resizes it to the desired size. Use when it’s not necessary to have the model traing on the whole image.\nPad: keep the image aspect ratio as is, add white/black padding to make a square. Use when it’s necessary to have the model train on the whole image.\nSquish: distorts the image to fit a square. Use when it’s not necessary to have the model train on the original aspect ratio.\n\nWhat is data augmentation? Why is it needed?\n\nData augmentation is the creation of random variations of input data through techniques like rotation, flipping, brightness changes, contrast changes, perspective warping. It is needed to help the model learn to recognize objects under different lighting/perspective conditions.\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\nWhat is the difference between item_tfms and batch_tfms?\n\nitem_tfms are transforms that are applied to each item in the set.\nbatch_tfms are transforms applied to a batch of items in the set.\n\nWhat is a confusion matrix?\n\nA matrix that shows the counts of predicted (columns) vs. actual (rows) labels, with the diagonal being correctly predicted data.\n\nWhat does export save?\n\nBoth the architecture and the parameters as a .pkl file.\n\nWhat is called when we use a model for making predictions, instead of training?\n\nInference\n\nWhat are IPython widgets?\n\ninteractive browser controls for Jupyter Notebooks.\n\nWhen would you use a CPU for deployment? When might a GPU be better?\n\nCPU: low-volume, single-user inputs for prediction.\nGPU: high-volume, multiple-user inputs for predictions.\n\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n\nRequires internet connectivity (and latency).\nSensitive data transfer may not be okay with your users.\nManaging complexity and scaling the server creates additional overhead.\n\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice?\n\nout-of-domain data: the images captured of real bears may not be represented in the model’s training or validation datasets.\nNumber of bear alerts doubles or halves after rollout of the new system in some location.\nout-of-domain data: the cameras may capture low-resolution images of the bears when the training and validation set had high resolution images.\n\nWhat is out-of-domain data?\n\nData your model sees in production that it hasn’t seen during training.\n\nWhat is domain shift?\n\nChanges in the data that our model sees in production over time.\n\nWhat are the three steps in the deployment process?\n\nManual Process\nLimited scope deployment\nGradual expansion\n\n\nFurther Research\n\nConsider how the Drivetrain Approach maps to a project or problem you’re interested in.\n\nI’ll take the example of a project I will be working on to practice what I’m learning in this book: training a deep learning model which correctly classifies the typeface from a collection of single letter.\n\nThe objective: correctly classify typeface from a collection of single letters.\nLevers: observe key features of key letters that are the “tell” of a typeface.\nData: using an HTML canvas object and Adobe Fonts, generate images of single letters of multiple fonts associated with each category of typeface.\nModels: output the probabilities of each typeface a given collection of single letters is predicted as. This allows for some flexibility in how you categorize letters based on the shared characteristics of more than one typeface that the particular font may possess.\n\n\nWhen might it be best to avoid certain types of data augmentation?\n\nIn my typeface example, it’s best to avoid perspective warping because it will change key features used to recognize a typeface.\n\nFor a project you’re interested in applying deep learning to, consider the thought experiment, “What would happen if it went really, really well?”\n\nIf my typeface classifier works really well, I imagine it would be used by people to take pictures of real-world text and learn what typeface it is. This may inspire a new wave of typeface designers. If a feedback loop was possible, and the classifier went viral, the very definition of typefaces may be affected by popular opinion. Taken a step further, a generative model may be inspired by this classifier, and a new wave of AI typeface would be launched—however this last piece is highly undesirable unless the training of the model involves appropriate licensing and attribution of the typefaces used that are created by humans. Furthermore, from what I understand from reading about typefaces, the process of creating a typeface is an amazing experience and should not be replaced with AI generators. If I created such a generative model (in part 2 of the course) and it went viral (do HuggingFace Spaces go viral? Cuz that’s where I would launch it), I would take it down.\n\nStart a blog (done!)\n\n\n\n\n\n\n\n\n\n\nHow to do a fast.ai lesson\n\nWatch lecture\nRun notebook & experiment\nReproduce results\nRepeat with different dataset\n\nfastbook repo contains “clean” folder with notebooks without markdown text.\nTwo concepts: training the model and using it for inference.\nOver 500 architectures in timm (PyTorch Image Models).\ntimm.list_models(pattern) will list models matching the pattern.\nPass string name of timm model to the Learner like: vision_learner(dls, 'timm model string', ...).\nin22 = ImageNet with 22k categories, 1k = ImageNet with 1k categories.\nlearn.predict probabilities are in the order of learn.dls.vocab.\nlearn.model contains the trained model which contains lots of nested layers.\nlearn.model.get_submodule takes a dotted string navigating through the hierarchy.\nMachine learning models fit functions to data.\nThings between dollar signs is LaTeX \"$...$\".\nGeneral form of quadratic: def quad(a,b,c,x): return a*x**2 + b*x + c\npartial from functools fixes parameters to a function.\nLoss functions tells us how good our model is.\n@interact from ipywidgets allows sliders tied to the function its above.\nMean Squared Error: def mse(preds, acts): return ((preds - acts)**2).mean()\nFor each parameter we need to know: does the loss get better when we increase or decrease the parameter?\nThe derivative is the function that tells you: if you increase the input does the output increase or decrease, and by how much?\n*params spreads out the list into its elements and passes each to the function.\n1-D (rank 1) tensor (lists of numbers), 2-D tensor (tables of numbers) 3-D tensor (layers of tables of numbers) and so on.\ntensor.requires_grad_() calculates the gradient of the values in the tensor whenever its used in calculation.\nloss.backward() calculates gradients on the inputs to the loss function.\nabc.grad attribute added after gradients are calculated.\nnegative gradient means increasing the parameter will decrease the loss.\nupdate parameters with torch.no_grad() so PyTorch doesn’t calculate the gradient (since it’s being used in a function). We don’t want the derivative of the parameter update, we only want the derivative with respect to the loss.\nAutomate the steps\n\nCalculate Mean Squared Error\nCall .backward.\nSubtract gradient * small number from the parameters\n\nAll optimizers are built on the concept of gradient descent (calculate gradients and decrease the loss).\nWe need a better function than quadratics\nRectified Linear Unit:\n\ndef rectified_linear(m,b,x):\n    y = m*x + b\n    return torch.clip(y, 0.)\n\ntorch.clip turns values less than value specified to the value specified (in this case, it turns negative values to 0.).\nAdding rectified linear functions together gives us an arbitrarily squiggly function that will match as close as we want to the data.\nReLU in 2D gives you surfaces, volumes in 3D, etc.\nWith this incredibly simple foundation you can construct an arbitrarily precise, accurate model.\nWhen you have ReLU’s getting added together, and gradient descent to optimize the parameters, and samples of inputs and outputs that you want, the computer “draws the owl” so to speak.\nDeep learning is using gradient descent to set some parameters to make a wiggly function (the addition of lots of rectified linear units or something very similar to that) that matches your data.\nWhen selecting an architecture, the biggest beginner mistake is that they jump to the highest-accuracy models.\nAt the start of the project, just use resnet18 so you can spend all of your time trying things out (data augmentation, data cleaning, different external data) as fast as possible.\nTrying better architectures is the very last thing to do.\nHow do I know if I have enough data?\n\nVast majority of projects in industry wait far too long until they train their first model.\nTrain your first model on day 1 with whatever CSV files you can hack together.\nSemi-supervised training lets you get dramatically more out of your data.\nOften it’s easy to get lots of inputs but hard to get lots of outputs (labels).\n\nUnits of parameter gradients: for each increase in parameter of 1, the gradient is the amount the loss would change by (if it stayed at that slope—which it doesn’t because it’s a curve).\nOnce you get close enough to the optimal parameter value, all loss functions look like quadratics\n\nThe slope of the loss function decreases as you approach the optimal\n\nLearning rate (a hyperparameter) is multiplied by the gradient, the product of which is subtracted from the parameters\nIf you pick a learning rate that’s too large, you will diverge; if you pick too small, it’ll take too long to train.\nhttp://matrixmultiplication.xyz/\nMatrix multiplication is the critical foundational mathematical operation in deep learning\nGPUs are good at matrix multiplication with tensor cores (multiply together two 4x4 matrices)\nUse a spreadsheet to train a deep learning model on the Kaggle Titanic dataset in which you’re trying to predict if a person survived.\n\nColumns included (convert some of them to binary categorical variables):\n\nSurvivor\nPclass\n\nConvert to Pclass_1 and Pclass_2 (both 1/0).\n\nSex\n\nConvert to Male (0/1) column.\n\nAge\n\nRemove blanks.\nNormalize (Age/Max(Age))\n\nSibSp (how many siblings they have)\nParch (# of parents/children aboard)\nFare\n\nLots of very small and very large fares, log of it has a much more even distribution. (LOG10(Fare + 1).\n\nEmbarked (which city they got on at)\n\nRemove blanks.\nConvert to Embark_S and Embark_C (both 1/0)\n\nOnes\n\nAdd a column of 1s.\n\n\nCreate random numbers for params (including Const) with =RAND() - 0.5.\nRegression\n\nUse SUMPRODUCT to calculate linear function.\nLoss of linear function is (linear function result - Survived) ^ 2.\nAverage loss = AVERAGE(individual losses).\nUser “Solver” with GRG Nonlinear Solving Method. Set Objective to minimize the cell with average loss. Change parameter variables.\n\nNeural Net\n\nTwo sets of params.\nTwo linear columns.\nTwo ReLU columns.\nAdding two linear functions together gives you a linear function, we want all those wiggles (non-linearity) so we use ReLUs.\nReLU: IF(lin1 < 0, 0, lin1)\nPreds = sum of the two ReLUs.\nLoss same as regression.\nSolver process the same as well.\n\nNeural Net (Matrix Multiplication)\n\nTranspose params into two columns.\n=MMULT(...) for Lin1 and Lin2 columns.\nKeep ReLU, Preds and Loss column the same.\nOptimize params using Solver.\nHelpful reminder to build intuition around matrix multiplication: it’s doing the same thing as the SUMPRODUCTs.\n\nDummy variables: Pclass_1, Pclass_2, etc.\n\nNext lesson: NLP\n\nIt’s about making predictions with text data which most of the time is in the form of prose.\nFirst Farsi NLP resource was created by a student of the first fastai course.\nNLP most commonly and practically used for classification.\nDocument = one or two words, a book, a wikipedia page, any length.\nClassification = figure out a category for a document.\nSentiment analysis\nAuthor identification\nLegal discovery (is this document in-scope or out-of-scope)\nOrganizing documents by topic\nTriaging inbound emails\nClassification of text looks similar to images.\nWe’re going to use a different library: HuggingFace Transformers\n\nHelpful to see how things are done in more than one library.\nHuggingFace Transformers doesn’t have the same high-level API. Have to do more stuff manually. Which is good for students at this point of the course.\nIt’s a good library.\n\nBefore the next lesson take a look at the NLP notebook and U.S. Patent to Phrase Matching data.\n\nTrying to figure out in patents whether two concepts are referring to the same thing. The document is text1, text2, and the category is similar (1) or not-similar (0).\n\nWill also talk about the two very important topics of validation sets and metrics.\n\n\n\n\n\n\n\nIn this section, I’ll train a Pets dataset classifier as done by Jeremy in this notebook.\n\nfrom fastai.vision.all import *\nimport timm\n\n\npath = untar_data(URLs.PETS)/'images'\n\n# Create DataLoaders object\ndls = ImageDataLoaders.from_name_func('.',\n                                      get_image_files(path),\n                                      valid_pct=0.2,\n                                      seed=42,\n                                      label_func=RegexLabeller(pat = r'^([^/]+)_\\d+'),\n                                      item_tfms=Resize(224))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:00<00:00]\n    \n    \n\n\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n# train using resnet34 as architecture\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 196MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.496086\n      0.316146\n      0.100135\n      01:12\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      45.65% [42/92 00:25<00:30 0.4159]\n    \n    \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.441153\n      0.315289\n      0.093369\n      01:04\n    \n    \n      1\n      0.289844\n      0.215224\n      0.069012\n      01:05\n    \n    \n      2\n      0.123374\n      0.191152\n      0.060217\n      01:03\n    \n  \n\n\n\nThe pets classifier, using resnet34 and 3 epochs, is about 94% accurate.\n\n# train using a timm architecture\n# from the convnext family of architectures\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name convnext_tiny_in22k to current convnext_tiny.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.130913\n      0.240275\n      0.085927\n      01:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.277886\n      0.193888\n      0.061570\n      01:08\n    \n    \n      1\n      0.196232\n      0.174544\n      0.055480\n      01:09\n    \n    \n      2\n      0.127525\n      0.156720\n      0.048038\n      01:07\n    \n  \n\n\n\nUsing convnext_tiny_in22k, the model is about 95.2% accurate, about a 20% decrease in error rate.\n\n# export to use in gradio app\nlearn.export('pets_model.pkl')\n\nYou can view my pets classifier gradio app here.\n\n\n\nIn this section, I’ll plot the timm model results as shown in Jeremy’s notebook.\n\nimport pandas as pd\n\n\n# load data\ndf_results = pd.read_csv(\"../../../fastai-course/data/results-imagenet.csv\")\ndf_results.head()\n\n\n\n\n\n  \n    \n      \n      model\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count\n      img_size\n      crop_pct\n      interpolation\n    \n  \n  \n    \n      0\n      eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n      90.052\n      9.948\n      99.048\n      0.952\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      1\n      eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n      89.966\n      10.034\n      99.012\n      0.988\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      2\n      eva_giant_patch14_560.m30m_ft_in22k_in1k\n      89.786\n      10.214\n      98.992\n      1.008\n      1,014.45\n      560\n      1.0\n      bicubic\n    \n    \n      3\n      eva02_large_patch14_448.mim_in22k_ft_in1k\n      89.624\n      10.376\n      98.950\n      1.050\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      4\n      eva02_large_patch14_448.mim_m38m_ft_in1k\n      89.570\n      10.430\n      98.922\n      1.078\n      305.08\n      448\n      1.0\n      bicubic\n    \n  \n\n\n\n\ntop1 = what percent of the time the model predicts the correct label with the highest probability.\ntop5 = what percent of the time the model predits the correct label with the top 5 highest probabilities.\nSource\n\n# remove additional text from model name\ndf_results['model_org'] = df_results['model']\ndf_results['model'] = df_results['model'].str.split('.').str[0]\ndf_results.head()\n\n\n\n\n\n  \n    \n      \n      model\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count\n      img_size\n      crop_pct\n      interpolation\n      model_org\n    \n  \n  \n    \n      0\n      eva02_large_patch14_448\n      90.052\n      9.948\n      99.048\n      0.952\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n    \n    \n      1\n      eva02_large_patch14_448\n      89.966\n      10.034\n      99.012\n      0.988\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n    \n    \n      2\n      eva_giant_patch14_560\n      89.786\n      10.214\n      98.992\n      1.008\n      1,014.45\n      560\n      1.0\n      bicubic\n      eva_giant_patch14_560.m30m_ft_in22k_in1k\n    \n    \n      3\n      eva02_large_patch14_448\n      89.624\n      10.376\n      98.950\n      1.050\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_in22k_ft_in1k\n    \n    \n      4\n      eva02_large_patch14_448\n      89.570\n      10.430\n      98.922\n      1.078\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_m38m_ft_in1k\n    \n  \n\n\n\n\n\ndef get_data(part, col):\n    # get benchmark data and merge with model data\n    df = pd.read_csv(f'../../../fastai-course/data/benchmark-{part}-amp-nhwc-pt111-cu113-rtx3090.csv').merge(df_results, on='model')\n    # convert samples/sec to sec/sample\n    df['secs'] = 1. / df[col]\n    # pull out the family name from the model name\n    df['family'] = df.model.str.extract('^([a-z]+?(?:v2)?)(?:\\d|_|$)')\n    # removing `resnetv2_50d_gn` and `resnet50_gn` for some reason\n    df = df[~df.model.str.endswith('gn')]\n    # not sure why the following line is here, \"in22\" was removed in cell above\n    df.loc[df.model.str.contains('in22'),'family'] = df.loc[df.model.str.contains('in22'),'family'] + '_in22'\n    df.loc[df.model.str.contains('resnet.*d'),'family'] = df.loc[df.model.str.contains('resnet.*d'),'family'] + 'd'\n    # only returns subset of families\n    return df[df.family.str.contains('^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin')]\n\n\n# load benchmark inference data\ndf = get_data('infer', 'infer_samples_per_sec')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      model\n      infer_samples_per_sec\n      infer_step_time\n      infer_batch_size\n      infer_img_size\n      param_count_x\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count_y\n      img_size\n      crop_pct\n      interpolation\n      model_org\n      secs\n      family\n    \n  \n  \n    \n      12\n      levit_128s\n      21485.80\n      47.648\n      1024\n      224\n      7.78\n      76.526\n      23.474\n      92.872\n      7.128\n      7.78\n      224\n      0.900\n      bicubic\n      levit_128s.fb_dist_in1k\n      0.000047\n      levit\n    \n    \n      13\n      regnetx_002\n      17821.98\n      57.446\n      1024\n      224\n      2.68\n      68.746\n      31.254\n      88.536\n      11.464\n      2.68\n      224\n      0.875\n      bicubic\n      regnetx_002.pycls_in1k\n      0.000056\n      regnetx\n    \n    \n      15\n      regnety_002\n      16673.08\n      61.405\n      1024\n      224\n      3.16\n      70.278\n      29.722\n      89.528\n      10.472\n      3.16\n      224\n      0.875\n      bicubic\n      regnety_002.pycls_in1k\n      0.000060\n      regnety\n    \n    \n      17\n      levit_128\n      14657.83\n      69.849\n      1024\n      224\n      9.21\n      78.490\n      21.510\n      94.012\n      5.988\n      9.21\n      224\n      0.900\n      bicubic\n      levit_128.fb_dist_in1k\n      0.000068\n      levit\n    \n    \n      18\n      regnetx_004\n      14440.03\n      70.903\n      1024\n      224\n      5.16\n      72.398\n      27.602\n      90.828\n      9.172\n      5.16\n      224\n      0.875\n      bicubic\n      regnetx_004.pycls_in1k\n      0.000069\n      regnetx\n    \n  \n\n\n\n\n\n# plot the data\nimport plotly.express as px\nw,h = 1000, 800\n\ndef show_all(df, title, size):\n    return px.scatter(df,\n                      width=w,\n                      height=h,\n                      size=df[size]**2,\n                      title=title,\n                      x='secs',\n                      y='top1',\n                      log_x=True,\n                      color='family',\n                      hover_name='model_org',\n                      hover_data=[size]\n                     )\n\nshow_all(df, 'Inference', 'infer_img_size')\n\n\n                                                \n\n\n\n# plot a subset of the data\nsubs = 'levit|resnetd?|regnetx|vgg|convnext.*|efficientnetv2|beit|swin'\n\ndef show_subs(df, title, size, subs):\n    df_subs = df[df.family.str.fullmatch(subs)]\n    return px.scatter(df_subs,\n                      width=w,\n                      height=h,\n                      size=df_subs[size]**2,\n                      title=title,\n                      trendline='ols',\n                      trendline_options={'log_x':True},\n                      x='secs',\n                      y='top1',\n                      log_x=True,\n                      color='family',\n                      hover_name='model_org',\n                      hover_data=[size])\n\nshow_subs(df, 'Inference', 'infer_img_size', subs)\n\n\n                                                \n\n\n\n# plot inference speed vs parameter count\npx.scatter(df,\n           width=w,\n           height=h,\n           x='param_count_x',\n           y='secs',\n           log_x=True,\n           log_y=True,\n           color='infer_img_size',\n           hover_name='model_org',\n           hover_data=['infer_samples_per_sec', 'family']\n)\n\n\n                                                \n\n\n\n# repeat plots for training data\ntdf = get_data('train', 'train_samples_per_sec')\nshow_all(tdf, 'Training', 'train_img_size')\n\n\n                                                \n\n\n\n# subset of training data\nshow_subs(tdf, 'Training', 'train_img_size', subs)\n\n\n                                                \n\n\n\n\n\nIn this section, I’ll recreate the content in Jeremy’s notebook here, where he walks through a quadratic example of training a function to match the data.\nA neural network layer:\n\nMultiplies each input by a number of values. These values are known as parameters.\nAdds them up for each group of values.\nReplaces the negative numbers with zeros.\n\n\n# helper functions\nfrom ipywidgets import interact\nfrom fastai.basics import *\n\n\n# helper functions\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\nIn the plot_function definition, I’ll look into why [:,None] is added after torch.linspace(min, max, 100)\n\ntorch.linspace(-1, 1, 10), torch.linspace(-1, 1, 10).shape\n\n(tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n          0.7778,  1.0000]),\n torch.Size([10]))\n\n\n\ntorch.linspace(-1, 1, 10)[:,None], torch.linspace(-1, 1, 10)[:,None].shape\n\n(tensor([[-1.0000],\n         [-0.7778],\n         [-0.5556],\n         [-0.3333],\n         [-0.1111],\n         [ 0.1111],\n         [ 0.3333],\n         [ 0.5556],\n         [ 0.7778],\n         [ 1.0000]]),\n torch.Size([10, 1]))\n\n\n[:, None] adds a dimension to the tensor.\nNext he fits a quadratic function to data:\n\ndef f(x): return 3*x**2 + 2*x + 1\n\nplot_function(f, '$3x^2 + 2x + 1$')\n\n\n\n\nIn order to simulate “finding” or “learning” the right model fit, he creates a general quadratic function:\n\ndef quad(a, b, c, x): return a*x**2 + b*x + c\n\nand uses partial to make new quadratic functions:\n\ndef mk_quad(a, b, c): return partial(quad, a, b, c)\n\n\n# recreating original quadratic with mk_quad\nf2 = mk_quad(3, 2, 1)\nplot_function(f2)\n\n\n\n\n\nf2\n\nfunctools.partial(<function quad at 0x148c6d000>, 3, 2, 1)\n\n\n\nquad\n\n<function __main__.quad(a, b, c, x)>\n\n\nNext he simulates noisy measurements of the quadratic f:\n\n# `scale` parameter is the standard deviation of the distribution\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n\n# noise function matches quadratic x + x^2 (with noise) + constant noise\ndef add_noise(x, mult, add): return x * (1+noise(x, mult)) + noise(x,add)\n\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:, None]\ny = add_noise(f(x), 0.15, 1.5)\n\n\n# values match Jeremy's\nx[:5], y[:5]\n\n(tensor([[-2.0000],\n         [-1.7895],\n         [-1.5789],\n         [-1.3684],\n         [-1.1579]]),\n tensor([[11.8690],\n         [ 6.5433],\n         [ 5.9396],\n         [ 2.6304],\n         [ 1.7947]], dtype=torch.float64))\n\n\n\nplt.scatter(x, y)\n\n<matplotlib.collections.PathCollection at 0x148e16320>\n\n\n\n\n\n\n# overlay data with variable quadratic\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x, y)\n    plot_function(mk_quad(a, b, c), ylim=(-3,13))\n\n\n\n\nImportant note changing sliders: only after changing b and c values do you realize that a also needs to be changed.\nNext, he creates a measure for how well the quadratic fits the data, mean absolute error (distance from each data point to the curve).\n\ndef mae(preds, acts): return (torch.abs(preds-acts)).mean()\n\n\n# update interactive plot\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\nIn a neural network we’ll have tens of millions or more parameters to fit and thousands or millions of data points to fit them to, which we can’t do manually with sliders. We need to automate this process.\nIf we know the gradient of our mae() function with respect to our parameters, a, b and c, then that means we know how adjusting a parameter will change the function. If, say, a has a negative gradient, then we know increasing a will decrease mae(). So we find the gradient of the parameters with respect to the loss function and adjust our parameters a bit in the opposite direction of the gradient sign.\nTo do this we need a function that will take the parameters as a single vector:\n\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n\n\n# testing it out\n# should equal 2.4219\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.4219, dtype=torch.float64)\n\n\n\n# pick an arbitrary starting point for our parameters\nabc = torch.tensor([1.1, 1.1, 1.1])\n\n# tell pytorch to calculate its gradients\nabc.requires_grad_()\n\n# calculate loss\nloss = quad_mae(abc)\nloss\n\ntensor(2.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n\n\n\n# calculate gradients\nloss.backward()\n\n# view gradients\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\n\n# increase parameters to decrease loss based on gradient sign\nwith torch.no_grad():\n    abc -= abc.grad*0.01\n    loss = quad_mae(abc)\n\nprint(f'loss={loss:.2f}')\n\nloss=2.40\n\n\nThe loss has gone down from 2.4219 to 2.40. We’re moving in the right direction.\nThe small number we multiply gradients by is called the learning rate and is the most important hyper-parameter to set when training a neural network.\n\n# use a loop to do a few more iterations\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.40\nstep=1; loss=2.36\nstep=2; loss=2.30\nstep=3; loss=2.21\nstep=4; loss=2.11\nstep=5; loss=1.98\nstep=6; loss=1.85\nstep=7; loss=1.72\nstep=8; loss=1.58\nstep=9; loss=1.46\n\n\nThe loss continues to decrease. Here are our parameters and their gradients at this stage:\n\nabc\n\ntensor([1.9634, 1.1381, 1.4100], requires_grad=True)\n\n\n\nabc.grad\n\ntensor([-13.4260,  -1.0842,  -4.5000])\n\n\nA neural network can approximate any computable function, given enough parameters using two key steps:\n\nMatrix multiplication.\nThe function \\(max(x,0)\\), which simply replaces all negative numbers with zero.\n\nThe combination of a linear function and \\(max\\) is called a rectified linear unit and can be written as:\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y, 0.)\n\n\nplot_function(partial(rectified_linear, 1, 1))\n\n\n\n\n\n# we can do the same thing using PyTorch\nimport torch.nn.functional as F\ndef rectified_linear2(m,b,x): return F.relu(m*x+b)\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\nCreate an interactive ReLU:\n\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m, b), ylim=(-1,4))\n\n\n\n\nObserve what happens when we add two ReLUs together:\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))\n\n\n\n\nCreating a triple ReLU function to fit our data:\n\ndef triple_relu(m1,b1,m2,b2,m3,b3,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x) + rectified_linear(m3,b3,x)\n\ndef mk_triple_relu(m1,b1,m2,b2,m3,b3): return partial(triple_relu, m1,b1,m2,b2,m3,b3)\n\n@interact(m1=-1.5, b1=-1.5, m2=0.5, b2=0.5, m3=1.5, b3=1.5)\ndef plot_double_relu(m1, b1, m2, b2, m3, b3):\n    f = mk_triple_relu(m1,b1,m2,b2,m3,b3)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\nThis same approach can be extended to functions with 2, 3, or more parameters. Drawing squiggly lines through some points is literally all that deep learning does. The above steps will, given enough time and enough data, create (for example) an owl recognizer if you feed it enough owls and non-owls.\nWe can could do thousands of computations on a GPU instead of the above CPU computation. We can greatly reduce the amount of computation and data needed by using a convolution instead of a matrix multiplication. We could make things much faster if, instead of starting with random parameters, we start with parameters of someone else’s model that does something similar to what we want (transfer learning).\n\n\n\nFollowing the instructions in the fastai course lesson video, I’ve created a Microsoft Excel deep learning model here for the Titanic Kaggle data.\nAs shown in the course video, I trained three different models—linear regression, neural net (using SUMPRODUCT) and neural net (using MMULT). After running Microsoft Excel’s Solver, I got the final (different than video) mean loss for each model:\n\nlinear: 0.14422715\nnnet: 0.14385956\nmmult: 0.14385956\n\nThe linear model loss in the video was about 0.10 and the neural net loss was about 0.08. So, my models didn’t do as well.\n\n\n\n\nIn this section, I’ll take notes while reading Chapter 4 in the fastai textbook.\n\n\n\nWe’ll use the MNIST dataset for our experiments, which contains handwritten digits.\nMNIST is collected by the National Institute of Standards and Technology and collated into a machine learning dataset by Yann Lecun who used MNIST in 1998 in LeNet-5, the first computer system to demonstrate practically useful recognition of handwritten digits.\nWe’ve seen that the only consisten trait among every fast.ai student who’s gone on to be a world-class practitioner is that they are all very tenacious.\nIn this chapter we’ll create a model that can classify any image as a 3 or a 7.\n\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:00<00:00]\n    \n    \n\n\n\n# ls method added by fastai\n# lists the count of items\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\n\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\n\n# 3 and 7 are the labels\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n\n(#6131) [Path('/root/.fastai/data/mnist_sample/train/3/10.png'),Path('/root/.fastai/data/mnist_sample/train/3/10000.png'),Path('/root/.fastai/data/mnist_sample/train/3/10011.png'),Path('/root/.fastai/data/mnist_sample/train/3/10031.png'),Path('/root/.fastai/data/mnist_sample/train/3/10034.png'),Path('/root/.fastai/data/mnist_sample/train/3/10042.png'),Path('/root/.fastai/data/mnist_sample/train/3/10052.png'),Path('/root/.fastai/data/mnist_sample/train/3/1007.png'),Path('/root/.fastai/data/mnist_sample/train/3/10074.png'),Path('/root/.fastai/data/mnist_sample/train/3/10091.png')...]\n\n\n\n# view one of the images\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\n\n# the image is stored as numbers\narray(im3)[4:10, 4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\n# same thing, but a PyTorch tensor\ntensor(im3)[4:10, 4:10]\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\n\n# use pandas.DataFrame to color code the array\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15, 4:22])\ndf.style.set_properties(**{'font-size': '6pt'}).background_gradient('Greys')\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      29\n      150\n      195\n      254\n      255\n      254\n      176\n      193\n      150\n      96\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      48\n      166\n      224\n      253\n      253\n      234\n      196\n      253\n      253\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      3\n      0\n      93\n      244\n      249\n      253\n      187\n      46\n      10\n      8\n      4\n      10\n      194\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      4\n      0\n      107\n      253\n      253\n      230\n      48\n      0\n      0\n      0\n      0\n      0\n      192\n      253\n      253\n      156\n      0\n      0\n      0\n    \n    \n      5\n      0\n      3\n      20\n      20\n      15\n      0\n      0\n      0\n      0\n      0\n      43\n      224\n      253\n      245\n      74\n      0\n      0\n      0\n    \n    \n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      249\n      253\n      245\n      126\n      0\n      0\n      0\n      0\n    \n    \n      7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14\n      101\n      223\n      253\n      248\n      124\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      11\n      166\n      239\n      253\n      253\n      253\n      187\n      30\n      0\n      0\n      0\n      0\n      0\n    \n    \n      9\n      0\n      0\n      0\n      0\n      0\n      16\n      248\n      250\n      253\n      253\n      253\n      253\n      232\n      213\n      111\n      2\n      0\n      0\n    \n    \n      10\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      43\n      98\n      98\n      208\n      253\n      253\n      253\n      253\n      187\n      22\n      0\n    \n  \n\n\n\nThe background white pixels are stored a the number 0, black is the number 255, and shades of grey between the two. The entire image contains 28 pixels across and 28 pixels down for a total of 768 pixels.\nHow might a computer recognize these two digits?\nIdeas:\n3s and 7s have distinct features. A seven has generally two straight lines at different angles, a three as two sets of curves stacked on each other. The point where the two curves intersect could be a recognizable feature of the the digit three. The point where the two straight-ish lines intersect could be a recognizable feature of the digit seven. One feature of confusion could be handwritten threes with a straight line at the top, similar to a seven. Another feature of confusion could be a handwritten 3 with a straight-ish ending stroke at the bottom, matching a similar stroke of a 7.\n\n\n\nIdea: find the average pixel value for every pixel of the 3s, then do the same for the 7s. To classify an image, see which of the two ideal digits the image is most similar to.\n\nBaseline: A simple model that you are confident should perform reasonably well. It should be simple to implement and easy to test, so that you can then test each of your improved ideas and make sure they are always better than your baseline. Without starting with a sensible baseline, it is difficult to know whether your super-fancy models are any good.\n\n\n# list comprehension of all digit images\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors), len(seven_tensors)\n\n(6131, 6265)\n\n\n\n# use fastai's show_image to display tensor images\nshow_image(three_tensors[1]);\n\n\n\n\nFor every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this, combine all the images in this list into a single three-dimensional tensor.\nWhen images are floats, the pixel values are expected to be between 0 and 1.\n\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\n# the length of a tensor's shape is its rank\n# rank is the number of axes and dimensions in a tensor\n# shape is the size of each axis of a tensor\nlen(stacked_threes.shape)\n\n3\n\n\n\n# rank of a tensor\nstacked_threes.ndim\n\n3\n\n\nWe calculate the mean of all the image tensors by taking the mean along dimension 0 of our stacked, rank-3 tensor. This is the dimension that indexes over all the images.\n\nmean3 = stacked_threes.mean(0)\nmean3.shape\n\ntorch.Size([28, 28])\n\n\n\nshow_image(mean3);\n\n\n\n\nThis is the ideal number 3 based on the dataset. It’s saturated where all the images agree it should be saturated (much of the background, the intersection of the two curves, and top and bottom curve), but it becomes wispy and blurry where the images disagree.\n\n# do the same for sevens\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\nHow would I calculate how similar a particular image is to each of our ideal digits?\nI would take the average of the absolute difference between each pixel’s intensity and the corresponding mean digit pixel intensity. The lower the average difference, the closer the digit is to the ideal digit.\n\n# sample 3\na_3 = stacked_threes[1]\nshow_image(a_3);\n\n\n\n\nL1 norm = Mean of the absolute value of differences.\nRoot mean squared error (RMSE) = square root of mean of the square of differences.\n\n# L1 norm\ndist_3_abs = (a_3 - mean3).abs().mean()\n\n# RMSE\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs, dist_3_sqr\n\n(tensor(0.1114), tensor(0.2021))\n\n\n\n# L1 norm\ndist_7_abs = (a_3 - mean7).abs().mean()\n\n# RMSE\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs, dist_7_sqr\n\n(tensor(0.1586), tensor(0.3021))\n\n\nFor both L1 norm and RMSE, the distance between the 3 and the “ideal” 3 is less than the distance to the ideal 7, so our simple model will give the right prediction in this case.\nBoth distances are provided in PyTorch:\n\nF.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt()\n\n(tensor(0.1586), tensor(0.3021))\n\n\nMSE = mean squared error.\nMSE will penalize bigger mistakes more heavily (and be lenient with small mistakes) than L1 norm.\n\n\n\nA NumPy array is a multidimensional table of data with all items of the same type.\njagged array: nested arrays of different sizes.\nIf the items of the array are all of simple type such as integer or float, NumPy will store them as a compact C data structure in memory.\nPyTorch tensors cannot be jagged. PyTorch tensors can live on the GPU. And can calculate their derivatives.\n\n# creating arrays and tensors\ndata = [[1,2,3], [4,5,6]]\narr = array(data)\ntns = tensor(data)\n\narr\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\ntns\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\n# select a row\ntns[1]\n\ntensor([4, 5, 6])\n\n\n\n# select a column\ntns[:,1]\n\ntensor([2, 5])\n\n\n\n# slice\ntns[1, 1:3]\n\ntensor([5, 6])\n\n\n\n# standard operators\ntns + 1\n\ntensor([[2, 3, 4],\n        [5, 6, 7]])\n\n\n\n# tensor type\ntns.type()\n\n'torch.LongTensor'\n\n\n\n# tensor changes type when needed\n(tns * 1.5).type()\n\n'torch.FloatTensor'\n\n\n\n\n\nmetric = a number that is calculated based on the predictions of our model and the correct labels in our dataset in order to tell us how good our model is.\nCalculate the metric on the validation set.\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\n\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\n\nvalid_3_tens.shape, valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\n# measure distance between image and ideal\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n\nmnist_distance(a_3, mean3)\n\ntensor(0.1114)\n\n\n\n# calculate mnist_distance for digit 3 validation images\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n(tensor([0.1109, 0.1202, 0.1276,  ..., 0.1357, 0.1262, 0.1157]),\n torch.Size([1010]))\n\n\nPyTorch broadcasts mean3 to each of the 1010 valid_3_dist tensors in order to calculate the distance. It doesn’t actually copy mean3 1010 times. It does the whole calculation in C (or CUDA for GPU).\nIn mean((-1, -2)), the tuple (-1, -2) represents a range of axes. This tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor—the horizontal and the vertical dimensions of an image.\nIf the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it’s a 3:\n\ndef is_3(x): return mnist_distance(x, mean3) < mnist_distance(x, mean7)\n\n\nis_3(a_3), is_3(a_3).float()\n\n(tensor(True), tensor(1.))\n\n\n\n# full validation set---thanks to broadcasting\nis_3(valid_3_tens)\n\ntensor([ True,  True,  True,  ..., False,  True,  True])\n\n\n\n# calculate accuracy\naccuracy_3s = is_3(valid_3_tens).float().mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s, accuracy_7s, (accuracy_3s + accuracy_7s) / 2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\nWe are getting more than 90% accuracy on both 3s and 7s. But they are very different looking digits and we’re classifying only 2 out of 10 digits, so we need to make a better model.\n\n\n\nArthur Samuel’s description of machine learning\n\nSuppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.\n\nOur pixel similarity approach doesn’t have any weight assignment, or any way of improving based on testing the effectiveness of a weight assignment. We can’t improve our pixel similarity approach.\nWe could look at each individual pixel and come up with a set of weights for each, such that the highest weights are associated with those pixels most likely to be black for a particular category. For example, pixels toward the bottom right are not very likely to be activate for a 7, so they should have a low weight for a 7, but ther are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category, for instance, the probability of being the number 8:\ndef pr_eight(x,w) = (x*w).sum()\nX is the image, represented as a vector (with all the rows stacked up end to end into a single long line) and the weights are a vector W. We need some way to update the weights to make them a little bit better. We want to find the specific values for the vector W that cause the result of our function to be high for those images that are 8s and low for those images that are not. Searching for the best vector W is a way to search for the best function for recognizing 8s.\nSteps required to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight how changing that weight would change the loss.\nStep (that is, change) all the weights based on that calculation.\nGo back to step 2 and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\nInitialize: Initialize parameters to random values.\nLoss: We need a function that will return a number that is small if the performance of the model is good (by convention).\nStep: Gradients allow us to directly figure out in which direction and by roughly how much to change each weight.\nStop: Keep training until the accuracy of the model started getting worse or we ran out of time, or once the number of epochs we decided are complete.\n\n\n\nCreate an example loss function:\n\ndef f(x): return x**2\n\nPick a tensor value at which we want gradients:\n\nxt = tensor(3.).requires_grad_()\n\n\nyt = f(xt)\nyt\n\ntensor(9., grad_fn=<PowBackward0>)\n\n\nCalculate gradients (backpropagation–during the backward pass of the network, as opposed to forward pass which is where the activations are calculated):\n\nyt.backward()\n\nView the gradients:\n\nxt.grad\n\ntensor(6.)\n\n\nThe derivative of x**2 is 2*x. When x = 3 the derivative is 6, as calculated above.\nCalculating vector gradients:\n\nxt = tensor([3., 4., 10.]).requires_grad_()\nxt\n\ntensor([ 3.,  4., 10.], requires_grad=True)\n\n\nAdd sum to our function so it takes a vector and returns a scalar:\n\ndef f(x): return (x**2).sum()\n\n\nyt = f(xt)\nyt\n\ntensor(125., grad_fn=<SumBackward0>)\n\n\n\nyt.backward()\nxt.grad\n\ntensor([ 6.,  8., 20.])\n\n\nIf the gradients are very large, that may suggest that we have more adjustments to do, whereas if they are very small, that may suggest that we are close to the optimal value.\n\n\n\nDeciding how to change our parameters based on the values of the gradients—multiplying the gradient by some small number called the learning rate (LR):\nw -= w.grad * lr\nThis is knowns as stepping your parameters using an optimization step.\nIf you pick a learning rate too low, that can mean having to do a lot of steps. If you pick a learning rate too high, that’s even worse, because it can result in the loss getting worse. If the learning rate is too high it may also “bounce” around.\n\n\n\nExample: measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, get slower as it went up the hill, and speed up again going downhill.\n\ntime = torch.arange(0,20).float(); time\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n        14., 15., 16., 17., 18., 19.])\n\n\n\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nspeed\n\ntensor([72.1328, 55.1778, 39.8417, 33.9289, 21.9506, 18.0992, 11.3346,  0.3637,\n         7.3242,  4.0297,  3.9236,  4.1486,  1.9496,  6.1447, 12.7890, 23.8966,\n        30.6053, 45.6052, 53.5180, 71.2243])\n\n\n\nplt.scatter(time, speed);\n\n\n\n\nWe added a bit of random noise since measuring things manually isn’t precise.\nWhat was the roller coaster’s speed? Using SGD, we can try to find a function that matches our observations. Guess that it will be a quadratic of the form a*(time**2) + (b*t) + c.\nWe want to distinguish clearly between the function’s input (the time when we are measuring the coaster’s speed) and its parameters (the values that define which quadratic we’re trying).\nCollect parameters in one argument and separate t and params in the function’s signature:\n\ndef f(t, params):\n  a,b,c = params\n  return a*(t**2) + (b*t) + c\n\nDefine a loss function:\n\ndef mse(preds, targets): return ((preds-targets)**2).mean()\n\nStep 1: Initialize the parameters\n\nparams = torch.randn(3).requires_grad_()\n\nStep 2: Calculate the predictions\n\npreds = f(time, params)\n\nCreate a little function to see how close our predictions are to our targets:\n\ndef show_preds(preds, ax=None):\n  if ax is None: ax=plt.subplots()[1]\n  ax.scatter(time, speed)\n  ax.scatter(time, to_np(preds), color='red')\n  ax.set_ylim(-300,100)\n\nshow_preds(preds)\n\n\n\n\nStep 3: Calculate the loss\n\nloss = mse(preds, speed)\nloss\n\ntensor(11895.1143, grad_fn=<MeanBackward0>)\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nparams.grad\n\ntensor([-35554.0117,  -2266.8909,   -171.8540])\n\n\n\nparams\n\ntensor([-0.5364,  0.6043,  0.4822], requires_grad=True)\n\n\nStep 5: Step the weights\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\nLet’s see if the loss has improved (it has) and take a look at the plot:\n\npreds = f(time, params)\nmse(preds, speed)\n\ntensor(2788.1594, grad_fn=<MeanBackward0>)\n\n\n\nshow_preds(preds)\n\n\n\n\nStep 6: Repeat the process\n\ndef apply_step(params, prn=True):\n  preds = f(time, params)\n  loss = mse(preds, speed)\n  loss.backward()\n  params.data -= lr * params.grad.data\n  params.grad = None\n  if prn: print(loss.item())\n  return preds\n\n\nfor i in range(10): apply_step(params)\n\n2788.159423828125\n1064.841552734375\n738.7333984375\n677.02001953125\n665.3380737304688\n663.1239013671875\n662.7010498046875\n662.6172485351562\n662.59765625\n662.5902709960938\n\n\n\n_, axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\nStep 7: Stop\nWe decided to stop after 10 epochs arbitrarily. In practice, we would watch the training and validation losses and our metrics to decide when to stop.\n\n\n\n\nAt the beginning, the weights of our model can be random (training from scratch) or come from a pretrained model (transfer learning).\nIn both cases the model will need to learn better weights.\nUse a loss function to compare model outputs to targets.\nChange the weights to make the loss a bit lower by multiple gradients by the learning rate and subtracting from the parameters.\nIterate until you have reached the lowest loss and then stop.\n\n\n\n\nConcatenate the images into a single tensor. view changes the shape of a tensor without changing its contents. -1 is a special parameter to view that means “make this axis as big as necessary to fit all the data”.\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\nUse the label 1 for 3s and 0 for 7s. Unsqueeze adds a dimension of size one.\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nPyTorch Dataset is required to return a tuple of (x,y) when indexed.\n\ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor([1]))\n\n\nPrepare the validation dataset:\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x, valid_y))\nx,y = valid_dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([1]))\n\n\nStep 1: Initialize the parameters\nWe need an initially random weight for every pixel.\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28,1))\nweights.shape\n\ntorch.Size([784, 1])\n\n\n\\(y = wx + b\\).\nWe created w (weights) now we need to create b (intercept or bias):\n\nbias = init_params(1)\nbias\n\ntensor([-0.0313], requires_grad=True)\n\n\nStep 2: Calculate the predictions\nPrediction for one image\n\n(train_x[0] * weights.T).sum() + bias\n\ntensor([0.5128], grad_fn=<AddBackward0>)\n\n\nIn Python, matrix multiplication is represetend with the @ operator:\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[ 0.5128],\n        [-3.8324],\n        [ 4.9791],\n        ...,\n        [ 3.0790],\n        [ 4.1521],\n        [ 0.3523]], grad_fn=<AddBackward0>)\n\n\nTo decide if an output represents a 3 or a 7, we can just check whether it’s greater than 0:\n\ncorrects = (preds>0.0).float() == train_y\ncorrects\n\ntensor([[ True],\n        [False],\n        [ True],\n        ...,\n        [False],\n        [False],\n        [False]])\n\n\n\ncorrects.float().mean().item()\n\n0.38964182138442993\n\n\nStep 3: Calculate the loss\nA very small change in the value of a weight will often not change the accuracy at all, and thus the gradient is 0 almost everywhere. It’s not useful to use accuracy as a loss function.\nWe need a loss function that when our weights result in slightly better predictions, gives us a slightly better loss.\nIn this case, what does “slightly better prediction mean”: if the correct answer is 3 (1), the score is a little higher, or if the correct answer is a 7 (0), the score is a little lower.\nThe loss function receives not the images themselves, but the predictions from the model.\nThe loss function will measure how distant each prediction is from 1 (if it should be 1) and how distant it is from 0 (if it should be 0) and then it will take the mean of all those distances.\n\ndef mnist_loss(predictions, targets):\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\nTry it out with sample predictions and targets:\n\ntrgts = tensor([1,0,1])\nprds = tensor([0.9, 0.4, 0.2])\ntorch.where(trgts==1, 1-prds, prds)\n\ntensor([0.1000, 0.4000, 0.8000])\n\n\nThis function returns a lower number when predictions are more accurate, when accurate predictions are more confident and when inaccurate predictions are less confident.\nSince we need a scalar for the final loss, mnist_loss takes the mean of the previous tensor:\n\nmnist_loss(prds, trgts)\n\ntensor(0.4333)\n\n\nmnist_loss assumes that predictions are between 0 and 1. We need to ensure that, using sigmoid, which always outputs a number between 0 and 1:\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\nIt’s also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. Update mnist+loss to first apply sigmoid to the inputs:\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\nWe already had a metric, which was overall accuracy. So why did we define a loss?\nTo drive automated learning, the loss must be a function that has a meaningful derivative. It can’t have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level.\nThe loss function is calculated for each item in our dataset, and then at the end of an epoch, the loss values are all averaged and the overall mean is reported for the epoch.\nIt is important that we focus on metrics, rather than the loss, when judging the performance of a model.\n\n\nThe optimization step: change or update the weights based on the gradients.\nTo take an optimization step, we need to calculate the loss over one or more data items. Calculating the loss for the whole dataset would take a long time, calculating it for a single item would not use much information so it would result in an imprecise and unstable gradient.\nCalculate the average loss for a few data items at a time (mini-batch). The number of data items in the mini-batch is called the batch-size.\nA larger batch size means you will get a more accurate and stable estimate of your dataset’s gradients from the loss function, but it will take longer and you will process fewer mini-batches per epoch. Using batches of data works well for GPUs, but give the GPU too many items at once and it will run out of memory.\nWe get better generalization if we can vary things during training (like performing data augmentation). One simple and effective thing we can vary is what data items we put in each mini-batch. Randomly shuffly the dataset before we create mini-batches. The DataLoader will do the shuffling and mini-batch collation for you:\n\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n\n[tensor([10,  3,  8, 11,  0]),\n tensor([6, 1, 7, 9, 4]),\n tensor([12, 13,  5,  2, 14])]\n\n\nFor training, we want a collection containing independent and dependent variables. A Dataset in PyTorch is a collection containing tuples of independent and dependent variables.\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\n\nlist(enumerate(string.ascii_lowercase))[:5]\n\n[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]\n\n\nWhen we pass a Dataset to a Dataloader we will get back many batches that are themselves tuples of tensors representing batches of independent and dependent variables:\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n\n[(tensor([24,  2,  4,  8,  9, 13]), ('y', 'c', 'e', 'i', 'j', 'n')),\n (tensor([23, 17,  6, 14, 25, 18]), ('x', 'r', 'g', 'o', 'z', 's')),\n (tensor([22,  5,  7, 20,  3, 19]), ('w', 'f', 'h', 'u', 'd', 't')),\n (tensor([ 0, 21, 12,  1, 16, 10]), ('a', 'v', 'm', 'b', 'q', 'k')),\n (tensor([11, 15]), ('l', 'p'))]\n\n\n\n\n\n\nIn code, the process will be implemented something like this for each epoch:\nfor x,y in dl:\n  # calculate predictions\n  pred = model(x)\n  # calculate the loss\n  loss = loss_func(pred, y)\n  # calculate the gradients\n  loss.backward()\n  # step the weights\n  parameters -= parameters.grad * lr\nStep 1: Initialize the parameters\n\nweights = init_params((28*28, 1))\nbias = init_params(1)\n\nA DataLoader can be created from a Dataset:\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\nDo the same for the validation set:\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nCreate a mini-batch of size 4 for testing:\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\npreds = linear1(batch)\npreds\n\ntensor([[10.4546],\n        [ 9.4603],\n        [-0.2426],\n        [ 6.7868]], grad_fn=<AddBackward0>)\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(0.1404, grad_fn=<MeanBackward0>)\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nweights.grad.shape, weights.grad.mean(), bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0089), tensor([-0.0619]))\n\n\nCreate a function to calculate gradients:\n\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  loss = mnist_loss(preds, yb)\n  loss.backward()\n\nTest it:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0178), tensor([-0.1238]))\n\n\nLook what happens when we call it again:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0267), tensor([-0.1857]))\n\n\nThe gradients have changed. loss.backward adds the gradients of loss to any gradients that are currently stored. So we have to set the current gradients to 0 first:\n\nweights.grad.zero_()\nbias.grad.zero_();\n\nMethods in PyTorch whose names end in an underscore modify their objects in place.\nStep 5: Step the weights\nWhen we update the weights and biases based on the gradient and learning rate, we have to tell PyTorch not to take the gradient of this step. If we assign to the data attribute of a tensor, PyTorch will not take the gradient of that step. Here’s our basic training loop for an epoch:\n\ndef train_epoch(model, lr, params):\n  for xb,yb in dl:\n    calc_grad(xb, yb, model)\n    for p in params:\n      p.data -= p.grad*lr\n      p.grad.zero_()\n\nWe want to check how we’re doing by looking at the accuracy of the validation set. To decide if an output represents a 3 (1) or a 7 (0) we can just check whether the prediction is greater than 0.\n\npreds, train_y[:4]\n\n(tensor([[10.4546],\n         [ 9.4603],\n         [-0.2426],\n         [ 6.7868]], grad_fn=<AddBackward0>),\n tensor([[1],\n         [1],\n         [1],\n         [1]]))\n\n\n\n(preds>0.0).float() == train_y[:4]\n\ntensor([[ True],\n        [ True],\n        [False],\n        [ True]])\n\n\n\n# if preds is greater than 0 and the label is 1 -> correct 3 prediction\n# if preds is not greater than 0 and the label is 0 -> correct 7 prediction\nTrue == 1, False == 0\n\n(True, True)\n\n\nCreate a function to calculate validation accuracy:\n\ndef batch_accuracy(xb, yb):\n  preds = xb.sigmoid()\n  correct = (preds>0.5) == yb\n  return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.7500)\n\n\nPut the batches back together:\n\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n  return round(torch.stack(accs).mean().item(), 4)\n\nStarting point accuracy:\n\nvalidate_epoch(linear1)\n\n0.5703\n\n\nLet’s train for 1 epoch and see if the accuracy improves:\n\nlr = 1.\nparams = weights, bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.6928\n\n\nStep 6: Repeat the process\nThen do a few more:\n\nfor i in range(20):\n  train_epoch(linear1, lr, params)\n  print(validate_epoch(linear1), end = ' ')\n\n0.852 0.9061 0.931 0.9418 0.9477 0.9569 0.9584 0.9594 0.9599 0.9633 0.9647 0.9652 0.9657 0.9662 0.9672 0.9677 0.9687 0.9696 0.9701 0.9696 \n\n\nWe’re already about at the same accuracy as our “pixel similarity” approach.\n\n\nReplace our linear function with PyTorch’s nn.Lienar module. A module is an object of a class that inherits from the PyTorch nn.Module class, and behaves identically to standard Python functions in that you can call them using parentheses and they will return the activations of a model.\nnn.Linear does the same thing as our init_params and linear together. It contains both weights and biases in a single class:\n\nlinear_model = nn.Linear(28*28, 1)\n\nEvery PyTorch module knows what parameters it has that can be trained; they are available through the parameters method:\n\nw,b = linear_model.parameters()\nw.shape, b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\nWe can use this information to create an optimizer:\n\nclass BasicOptim:\n  def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n  def step(self, *args, **kwargs):\n    for p in self.params: p.data -= p.grad.data * self.lr\n\n  def zero_grad(self, *args, **kwargs):\n    for p in self.params: p.grad = None\n\nWe can create our optimizer by passing in the model’s parameters:\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\nSimplify our training loop:\n\ndef train_epoch(model):\n  for xb,yb in dl:\n    # calculate the gradients\n    calc_grad(xb,yb,model)\n    # step the weights\n    opt.step()\n    opt.zero_grad()\n\nOur validation function doesn’t need to change at all:\n\nvalidate_epoch(linear_model)\n\n0.3985\n\n\nPut our training loop in a function:\n\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')\n\nSimilar results as the previous training:\n\ntrain_model(linear_model, 20)\n\n0.4932 0.7959 0.8506 0.9136 0.9341 0.9492 0.9556 0.9629 0.9658 0.9683 0.9702 0.9717 0.9741 0.9746 0.9761 0.9766 0.9775 0.978 0.9785 0.979 \n\n\nfastai provides the SGD class that by default does the same thing as our BasicOptim:\n\nlinear_model = nn.Linear(28*28, 1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n0.4932 0.8735 0.8174 0.9082 0.9331 0.9468 0.9546 0.9614 0.9653 0.9668 0.9692 0.9727 0.9736 0.9751 0.9756 0.9761 0.9775 0.978 0.978 0.9785 \n\n\nfastai provides Learner.fit which we can use instead of train_model. To create a Learner we first need to create a DataLoaders, by passing our training and validation DataLoaders:\n\ndls = DataLoaders(dl, valid_dl)\n\nTo create a Learner without using an application such as cnn_learner we need to pass in all the elements that we’ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:\n\nlearn = Learner(dls, nn.Linear(28*28, 1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.636474\n      0.503518\n      0.495584\n      00:00\n    \n    \n      1\n      0.550751\n      0.189374\n      0.840530\n      00:00\n    \n    \n      2\n      0.201501\n      0.178350\n      0.839549\n      00:00\n    \n    \n      3\n      0.087588\n      0.105257\n      0.912659\n      00:00\n    \n    \n      4\n      0.045719\n      0.076968\n      0.933759\n      00:00\n    \n    \n      5\n      0.029454\n      0.061683\n      0.947498\n      00:00\n    \n    \n      6\n      0.022817\n      0.052156\n      0.954367\n      00:00\n    \n    \n      7\n      0.019893\n      0.045825\n      0.962709\n      00:00\n    \n    \n      8\n      0.018424\n      0.041383\n      0.965653\n      00:00\n    \n    \n      9\n      0.017549\n      0.038113\n      0.967125\n      00:00\n    \n  \n\n\n\n\n\n\n\nAdding a nonlinearity between two linear classifiers givs us a neural network.\n\ndef simple_net(xb):\n  res = xb@w1 + b1\n  res = res.max(tensor(0.0))\n  res = res@w2 + b2\n  return res\n\n\n# initialize weights\nw1 = init_params((28*28, 30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nw1 has 30 output activations which means w2 must have 30 input activations so that they match. 30 output activations means that the first layer can construct 30 different features, each representing a different mix of pixels. You can change that 30 to anything you like to make the model more or less complex.\nres.max(tensor(0.0)) is called a rectified linear unit or ReLU. It replaces every negative number with a zero.\n\nplot_function(F.relu)\n\n\n\n\nWe need a nonlinearity becauase a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\nThe neural net can solve any computable problem to an arbitrarily high level of accuracy if you can find the right parameters w1 and w2 and if you make the matrices big enough.\nWe can replace our function with PyTorch:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nnn.Sequential create a modeule that will call each of the listed layers or functions in turn. When using nn.Sequential PyTorch requires us to use the module version (nn.ReLU) and not the function version (F.relu). Modules are classes so you have to instantiate them.\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.363529\n      0.409795\n      0.505888\n      00:00\n    \n    \n      1\n      0.165949\n      0.239534\n      0.792934\n      00:00\n    \n    \n      2\n      0.089140\n      0.117148\n      0.913150\n      00:00\n    \n    \n      3\n      0.056798\n      0.078107\n      0.941119\n      00:00\n    \n    \n      4\n      0.042071\n      0.060734\n      0.957311\n      00:00\n    \n    \n      5\n      0.034718\n      0.051121\n      0.962218\n      00:00\n    \n    \n      6\n      0.030605\n      0.045103\n      0.964181\n      00:00\n    \n    \n      7\n      0.027994\n      0.040995\n      0.966143\n      00:00\n    \n    \n      8\n      0.026145\n      0.037990\n      0.969087\n      00:00\n    \n    \n      9\n      0.024728\n      0.035686\n      0.970559\n      00:00\n    \n    \n      10\n      0.023585\n      0.033853\n      0.972522\n      00:00\n    \n    \n      11\n      0.022634\n      0.032346\n      0.973994\n      00:00\n    \n    \n      12\n      0.021826\n      0.031080\n      0.975466\n      00:00\n    \n    \n      13\n      0.021127\n      0.029996\n      0.976448\n      00:00\n    \n    \n      14\n      0.020514\n      0.029053\n      0.975957\n      00:00\n    \n    \n      15\n      0.019972\n      0.028221\n      0.976448\n      00:00\n    \n    \n      16\n      0.019488\n      0.027481\n      0.977920\n      00:00\n    \n    \n      17\n      0.019051\n      0.026818\n      0.978410\n      00:00\n    \n    \n      18\n      0.018654\n      0.026219\n      0.978410\n      00:00\n    \n    \n      19\n      0.018291\n      0.025677\n      0.978901\n      00:00\n    \n    \n      20\n      0.017958\n      0.025181\n      0.978901\n      00:00\n    \n    \n      21\n      0.017650\n      0.024727\n      0.980373\n      00:00\n    \n    \n      22\n      0.017363\n      0.024310\n      0.980864\n      00:00\n    \n    \n      23\n      0.017096\n      0.023925\n      0.980864\n      00:00\n    \n    \n      24\n      0.016846\n      0.023570\n      0.981845\n      00:00\n    \n    \n      25\n      0.016610\n      0.023241\n      0.982336\n      00:00\n    \n    \n      26\n      0.016389\n      0.022935\n      0.982336\n      00:00\n    \n    \n      27\n      0.016179\n      0.022652\n      0.982826\n      00:00\n    \n    \n      28\n      0.015980\n      0.022388\n      0.982826\n      00:00\n    \n    \n      29\n      0.015791\n      0.022142\n      0.982826\n      00:00\n    \n    \n      30\n      0.015611\n      0.021913\n      0.983317\n      00:00\n    \n    \n      31\n      0.015440\n      0.021700\n      0.983317\n      00:00\n    \n    \n      32\n      0.015276\n      0.021500\n      0.983317\n      00:00\n    \n    \n      33\n      0.015120\n      0.021313\n      0.983317\n      00:00\n    \n    \n      34\n      0.014969\n      0.021137\n      0.983317\n      00:00\n    \n    \n      35\n      0.014825\n      0.020972\n      0.983317\n      00:00\n    \n    \n      36\n      0.014686\n      0.020817\n      0.982826\n      00:00\n    \n    \n      37\n      0.014553\n      0.020671\n      0.982826\n      00:00\n    \n    \n      38\n      0.014424\n      0.020532\n      0.982826\n      00:00\n    \n    \n      39\n      0.014300\n      0.020401\n      0.982826\n      00:00\n    \n  \n\n\n\nYou can view the training process in learn.recorder:\n\nplt.plot(L(learn.recorder.values).itemgot(2))\n\n\n\n\nView the final accuracy:\n\nlearn.recorder.values[-1][2]\n\n0.982826292514801\n\n\nAt this point we have:\n\nA function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters.\nA way to find the best set of parameters for any function (stochastic gradient descent).\n\n\n\nWe can add as many layers in our neural network as we want, as long as we add a nonlinearity between each pair of linear layers.\nThe deeper the model gets, the harder it is to optimize the parameters.\nWith a deeper model (one with more layers) we do not need to use as many parameters. We can use smaller matrices with more layers and get better results than we would get with larger matrices and few layers.\nIn the 1990s what held back the field for years was that so few researchers were experimenting with more than one nonlinearity.\nTraining an 18-layer model:\n\ndls = ImageDataLoaders.from_folder(path)\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n/usr/local/lib/python3.10/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.098852\n      0.014919\n      0.996075\n      02:01\n    \n  \n\n\n\n\n\n\n\nActivations: Numbers that are calculated (both by linear and nonlinear layers)\nParameters: Numbers that are randomly initialized and optimized (that is, the numbers that define the model).\nPart of becoming a good deep learning practitioner is getting used to the idea of looking at your activations and parameters, and plotting the and testing whether they are behaving correctly.\nActivations and parameters are all contained in tensors. The number of dimensions of a tensor is its rank.\nA neural network contains a number of layers. Each layer is either linear or nonlinear. We generally alternate between these two kinds of layers in a neural network. Sometimes a nonlinearity is referred to as an activation function.\nKey concepts related to SGD:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nReLU\nFunction that returns 0 for negative numbers and doesn’t change positive numbers.\n\n\nMini-batch\nA small group of inputs and labels gathered together in two arrays. A gradient descent is updated on this batch (rather than a whole epoch).\n\n\nForward pass\nApplying the model to some input and computing the predictions.\n\n\nLoss\nA value that represents how well or badly our model is doing.\n\n\nGradient\nThe derivative of the loss with respect to some parameter of the model.\n\n\nBackward pass\nComputing the gradients of the loss with respect to all model parameters.\n\n\nGradient descent\nTaking a step in the direction opposite to the gradients to make the model parameters a little bit better.\n\n\nLearning rate\nThe size of the step we take when applying SGD to update the parameters of the model.\n\n\n\n\n\n\n1. How is a grayscale image represented on a computer? How about a color image?\nGrayscale image pixels can be 0 (black) to 255 (white). Color image pixels have three values (Red, Green, Blue) where each value can be from 0 to 255.\n2. How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\nMNIST_SAMPLE path has a labels.csv file, a train folder, and a valid folder.\n\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\nThe train folder has a 3 and a 7 folder, each which contains training images.\n\n(path/'valid').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/valid/3'),Path('/root/.fastai/data/mnist_sample/valid/7')]\n\n\nThe valid folder contains a 3 and a 7 folder, each containing validation set images.\n3. Explain how the “pixel similarity” approach to classifying digits works.\nPixel similarity works by calculating the absolute mean difference (L1 norm) between each image and the mean digit 3, and averaging the classification (if the absolute mean difference between the image and the ideal 3 is less than the absolute mean difference between the image and the ideal 7, it’s classified as a 3) across all images of each digit’s validation set as the accuracy of the model.\n4. What is list comprehension? Create one now that selects odd numbers from a list and doubles them.\nList comprehension is syntax for creating a new list based on another sequence or iterable (docs)\n\n# for each element in range(10)\n# if the modulo of the element and 2 is not 0\n# double the element's value and store in this new list\ndoubled_odds = [2*elem for elem in range(10) if elem % 2 != 0]\ndoubled_odds\n\n[2, 6, 10, 14, 18]\n\n\n5. What is a rank-3 tensor?\nA rank-3 tensor is a “cube” (3-dimensional tensor).\n6. What is the difference between tensor rank and shape? How do you get the rank from the shape?\nTensor rank is the number of dimensions of the tensor. Tensor shape is the number of elements in each dimension. The following tensor is a 2-dimensional tensor with rank 2, the shape of which is 3 elements by 2 elements.\n\na_tensor = tensor([[1,3], [4,5], [5,6]])\n# dim == rank\na_tensor.dim(), a_tensor.shape\n\n(2, torch.Size([3, 2]))\n\n\n7. What are RMSE and L1 norm?\nRMSE = Root Mean Squared Error: The square root of the mean of squared differences between two sets of values.\nL1 norm = mean absolute difference: the mean of the absolute value of differences between two sets of values.\n8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\nYou can do so by using tensors on a GPU.\n9. Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom four numbers.\n\na_tensor = tensor([[1,2,3], [4,5,6], [7,8,9]])\na_tensor\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\n\na_tensor = 2 * a_tensor\na_tensor\n\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12],\n        [14, 16, 18]])\n\n\n\na_tensor.view(-1, 9)[0,-4:]\n\ntensor([12, 14, 16, 18])\n\n\n10. What is broadcasting? Broadcasting is when a tensor of smaller rank (or a scalar) is expanded so that you can perform an operation between it and a tensor of larger rank. Broadcasting makes it so that the two operands have the same rank.\n\na_tensor + tensor([1,2,3])\n\ntensor([[ 3,  6,  9],\n        [ 9, 12, 15],\n        [15, 18, 21]])\n\n\n\nAre metrics generally calculated using the training set or the validation set? Why?\n\nMetrics are calculated on the validation set because since that is the data the model does not see during training, the metric tells you how your model performs on data it hasn’t seen before.\n12. What is SGD?\nSGD is Stochastic Gradient Descent, an automated process where a model learns the right parameters needed to solve problems like image classification. The randomly (from scratch) or pretrained (transfer learning) parameters are updated using their gradients with respect to the loss and the learning rate. Metrics like the accuracy measure how well the model is performing.\n13. Why does SGD use mini-batches?\nOne reason is to utilize the ability of a GPU to process a lot of data at once.\nAnother reason is that calculating the loss one image at a time leads to an unstable loss function whereas calculating the loss on the entire dataset takes too long. Mini-batches fall in between these two extremes.\n14. What are the seven steps in SGD for machine learning?\n\nInitialize the weights.\nCalculate the predictions.\nCalculate the loss.\nCalculate gradients.\nStep the weights.\nRepeat the process.\nStop.\n\n15. How do we initialize the weights in a model?\nEither randomly (if training from scratch) or using pretrained weights (if transfer learning from an existing model like resnet18).\n16. What is loss?\nA machine-friendly way to measure how well (or badly) the model is performing. The model is learning to step the weights in order to decrease the loss.\n17. Why can’t we always use a high learning rate?\nBecause we risk overshooting the minimum loss (getting stuck back and forth between the two sides of the parabola) or diverging (resulting in larger losses each step).\n18. What is a gradient?\nThe rate of change or derivative of one variable with respect to another variable. In our case, gradients are the ratio of change in loss to change in parameter at one point.\n19. Do you need to know how to calculate gradients yourself?\nNope! Although you should understand the basic concept of derivatives. PyTorch calculates gradients with the .backward method.\n20. Why can’t we use accuracy as a loss function?\nBecause small changes in predictions do not result in small changes in accuracy. Accuracy drastically jumps (from 0 to 1 in our MNIST_SAMPLE example) at one point, with 0 slope elsewhere. We want a smooth function where you can calculate non-zero and non-infinite derivatives everywhere.\n21. Draw the sigmoid function. What is special about its shape?\nThe sigmoid function outputs between 0 and 1 for input values going from -inf to +inf. It also has a smooth positive slope everywhere so it’s easy to take the derivate.\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n22. What is the difference between a loss function and a metric?\nThe loss function is a machine-friendly way to measure the performance of the model while a metric is a human-friendly way to do the same.\nThe purpose of the loss function is to provide a smooth function to take derivates over so the training system can change the weights little by little towards the optimum.\nThe purpose of the metric is to inform the human how well or badly the model is learning during training.\n23. What is the function to calculate new weights using a learning rate?\nIn code, the function is:\nparameters.data -= parameters.grad * lr\nThe new weights are stepped incrementally in the opposite direction of the gradients. If the gradient is negative, the weights will be increased. If the gradient is positive, the weights will be decreased.\n24. What does the DataLoader class do?\nThe DataLoader class prepares training and validation batches and feeds them to the GPU during training. It also performs any necessary item_tfms or batch_tfms to the data.\n25. Write pseudocode showing the basic steps taken in each epoch for SGD.\ndef train_epoch(model):\n  # calculate predictions\n  preds = model(xb)\n  # calculate the loss\n  loss = loss_func(preds, targets)\n  # calculate gradients\n  loss.backward()\n  # step the weights\n  params.data -= params.grad * lr\n  # reset the gradients\n  params.zero_grad_()\n  # calculate accuracy\n  acc = tensor([accuracy for each batch]).mean()\n\nCreate a function that, if passed two arguments [1, 2, 3, 4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n\n\ndef zipped_tuples(x, y): return list(zip(x,y))\n\n\nzipped_tuples([1,2,3,4], 'abcd')\n\n[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n\n\nThe output data structure is the same structure as the PyTorch Dataset.\n27. What does view do in PyTorch?\nview changes the rank and shape of the tensor.\n\ntensor([1,2,3],[4,5,6]).view(3,2)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntensor([1,2,3],[4,5,6]).view(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n28. What are the bias parameters in a neural network? Why do we need them?\nThe bias parameters are the intercept \\(b\\) in the function \\(y = wx + b\\). We need them for situations where the inputs are 0 (since \\(w*0 = 0\\)). Bias also helps to create a more flexible function (source).\n29. What does the @ operator do in Python?\nMatrix multiplication.\n\nv1 = tensor([1,2,3])\nv2 = tensor([4,5,6])\nv1 @ v2\n\ntensor(32)\n\n\n30. What does the backward method do?\nCalculate the gradients of the loss function with respect to the parameters.\n31. Why do we have to zero the gradients?\nEach time you call .backward PyTorch will add the new gradients to the current gradients, so we need to zero the gradients to prevent them from accumulating.\n32. What information do we have to pass to Learner?\nReference:\nLearner(dls, simple_net, opt_func=SGD,\n            loss_func=mnist_loss, metrics=batch_accuracy)\nWe pass to the Learner:\n\nDataLoaders containing training and validation sets.\nThe model we want to train.\nAn optimizer function.\nA loss function.\nAny metrics we want calculated.\n\n33. Show Python or pseudocode for the basic steps of a training loop.\nSee #25.\n34. What is ReLU? Draw a plot for it for values from -2 to +2.\nReLU is Rectified Linear Unit. It’s a function where if the inputs are negative, they are set to zero, and if the inputs are positive, they are kept as is.\n\nplot_function(F.relu, min=-2, max=2)\n\n\n\n\n35. What is an activation function?\nAn activation function is the function that produces our predictions (in our case, a neural net with linear and nonlinear layers). Sometimes the ReLU is referred to as the activation function.\n36. What’s the difference between F.relu and nn.ReLU?\nF.relu is a function whereas nn.ReLU is a class that needs to be instantiated.\n37. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why wo we normally use more?\nUsing more layers results in more accurate models."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "",
    "text": "An example image from the image dataset used in this lesson. The image has a train going over a bridge with skyscrapers in the background.\nThis chapter introduced two more classification models:\nIn this chapter the authors walk us through in the chapter is the PASCAL dataset.\nHere’s my video walkthrough for this notebook:"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "The Data",
    "text": "The Data\nfastai comes with datasets available for download using the URLs object. We will use the PASCAL_2007 dataset.\n\n# download the dataset\npath = untar_data(URLs.PASCAL_2007)\n\n#read label CSV into a DataFrame\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\n  \n    \n      \n      fname\n      labels\n      is_valid\n    \n  \n  \n    \n      0\n      000005.jpg\n      chair\n      True\n    \n    \n      1\n      000007.jpg\n      car\n      True\n    \n    \n      2\n      000009.jpg\n      horse person\n      True\n    \n    \n      3\n      000012.jpg\n      car\n      False\n    \n    \n      4\n      000016.jpg\n      bicycle\n      True\n    \n  \n\n\n\n\nNext, they have us go through some pandas fundamentals for accessing data in a DataFrame\n\n# accessing all rows and the 0th column\ndf.iloc[:,0]\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# accessing all columns for the 0th row\ndf.iloc[0,:]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# trailing :s are not needed\ndf.iloc[0]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# accessing a column by its name\ndf['fname']\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# creating a new DataFrame and performing operations on it\ndf1 = pd.DataFrame()\n\n# adding a new column\ndf1['a'] = [1,2,3,4]\ndf1\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n    \n      3\n      4\n    \n  \n\n\n\n\n\n# adding a new column\ndf1['b'] = [10, 20, 30, 40]\n\n# adding two columns\ndf1['a'] + df1['b']\n\n0    11\n1    22\n2    33\n3    44\ndtype: int64"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Constructing a DataBlock",
    "text": "Constructing a DataBlock\nA DataBlock can be used to create Datasets from which DataLoaders can be created to use during training. A DataBlock is an object which contains the data and has helper functions which can access and transform the data.\nThey start by creating an empty DataBlock\n\ndblock = DataBlock()\ndblock\n\n<fastai.data.block.DataBlock at 0x7efe5c559d90>\n\n\nThe DataFrame with filenames and labels can be fed to the DataBlock to create a Datasets object, which is > an iterator that contains a training Dataset and validation Dataset\nEach dataset is\n\na collection that returns a tuple of your independent and dependent variable for a single item\n\nA Dataet created from an empty DataBlock (meaning, a DataBlock with no helper functions to tell it how the data is structured and accessed) will contain a tuple for each row of the DataFrame, where both values of the tuple are the same row.\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object, fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object)\n\n\nWhat we want is for the DataBlock to create Datasets of (independent, dependent) values. In this case, the independent variable is the image and the dependent variable is a list of labels.\nIn order to parse the DataFrame rows, we need to provide two helper functions to the DataBlock: get_x and get_y. In ordert to convert them to the appropriate objects that will be used in training, we need to provide two more arguments: ImageBlock and MultiCategoryBlock. In order for the DataBlock to correctly split the data into training and validation datasets, we need to define a splitter function and pass it as an argument as well.\nget_x will access the filename from each row of the DataFrame and convert it to a file path.\nget_y will access the labels from each row and split them into a list.\nImageBlock will take the file path and convert it to a PILImage object.\nMultiCategoryBlock will convert the list of labels to a one-hot encoded tensor using the Dataset’s vocab.\nsplitter will explicitly choose for the validation set the rows where is_valid is True.\nRandomResizedCrop will ensure that each image is the same size, which is a requirement for creating a tensor with all images.\n\ndef get_x(row): return path/'train'/row['fname']\ndef get_y(row): return row['labels'].split(' ')\ndef splitter(df):\n  train = df.index[~df['is_valid']].tolist()\n  valid = df.index[df['is_valid']].tolist()\n  return train, valid\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    splitter=splitter,\n    get_x=get_x,\n    get_y=get_y,\n    item_tfms = RandomResizedCrop(128, min_scale=0.35))\n\ndsets = dblock.datasets(df)\ndls = dblock.dataloaders(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0.]))\n\n\nThe Datasets vocab is a list of alphabetically ordered unique labels:\n\ndsets.train.vocab\n\n['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\n\nLet me breakdown the tuple returned by dsets.train[0]. The first value is a PILImageobject which can be viewed by calling its show() method:\n\ndsets.train[0][0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7efe5c3764d0>\n\n\n\n\n\nThe second value is a one-hot encoded list, where 1s are in the location of the labels in the corresponding vocab list. I’ll use the torch.where method to access the indices where there are 1s:\n\ntorch.where(dsets.train[0][1]==1)\n\n(TensorMultiCategory([6]),)\n\n\n\ndsets.train.vocab[torch.where(dsets.train[0][1]==1)[0]]\n\n(#1) ['car']\n\n\n\ndls.show_batch(nrows=1, ncols=3)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 4: Two-Digit MNIST Classifier",
    "text": "Chapter 4: Two-Digit MNIST Classifier\nI’ll first review the loss function used in the single-label classification models created in Chapters 4 and 5 before reviewing Binary Cross Entropy Loss introduced in this chapter.\nIn this chapter, we built a image classifier which would predict if an input image was an of the digit 3 or the digit 7.\nThe target (or expected outcome) is a list of 0s (for 7) and 1s (for 3). If we gave a batch of images of a 3, a 7 and a 3, the target would be [1, 0, 1].\nSuppose the model predicted the following values: [0.9, 0.4, 0.2] where each value represented the probability or confidence it had that each image was a 3.\nLoss represents the positive difference between the target and the prediction: - 1 - prediction when target == 1 - prediction when target == 0\nFor the first image, the model had 90% confidence it was a 3, and it was indeed a 3. The loss is 1 - 0.9 = 0.1.\nFor the second second image, the model had a 40% confidence it was a three, and the image was of a 7. The loss is 0.4.\nFor the last image, the model had a 20% confidence it was a 3, and the image was a 3. The loss is 1 - 0.2 = 0.8.\nThe average of these three losses is 1.3/3 or 0.433.\nThe following cell illustrates this with code:\n\ndef mnist_loss(predictions, targets):\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\ntargets = tensor([1,0,1])\npredictions = tensor([0.9, 0.4, 0.2])\nmnist_loss(predictions=predictions, targets=targets)\n\ntensor(0.4333)\n\n\nThe assumption that this loss function makes is that the predictions are always between 0 and 1. That may not always be true! In order to make this assumption explicit, we take the sigmoid of the prediction before calculating the loss. The sigmoid function outputs a value between 0 and 1 for any input value.\n\ntensor([0.4,-100,200]).sigmoid()\n\ntensor([0.5987, 0.0000, 1.0000])\n\n\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 5: 37 Breed Pet Classifier",
    "text": "Chapter 5: 37 Breed Pet Classifier\nIn this chapter, we train an image classifier that when given an input image, predicts which of the 37 pet breeds the image shows. The loss function needs to handle 37 activations for each image. In order to ensure the sum of those activations equals 1.0—so that the highest activation represents the model’s highest confidence—the softmax function is used. In order to increase the separation between probabilities, the softmax function’s output is passed through the logarithm function, and the negative value is taken. The combination of softmax and (negative) logarithm is called cross entropy loss.\nSuppose we had 4 images in a batch. The model would output activations something like this:\n\n# create a pseudo-random 4 x 37 tensor \n# with values from -2 to 2\nacts = (-2 - 2) * torch.rand(4, 37) + 2\nacts\n\ntensor([[-1.9994e+00,  7.0629e-01, -1.8230e+00,  8.6118e-02,  8.8579e-01,\n         -9.7763e-01,  9.7619e-01,  5.4613e-01,  9.2020e-01,  8.2653e-01,\n         -1.3831e+00,  1.2236e+00, -4.2582e-01,  1.1371e+00,  1.2409e+00,\n          1.4403e-02, -9.2988e-01, -1.1939e+00, -9.9743e-01, -1.9572e+00,\n         -6.8404e-02,  6.2455e-01,  8.6748e-01, -1.4574e+00, -1.4451e+00,\n          1.1349e-01,  1.7424e+00,  6.5414e-02, -1.2517e+00, -1.9933e+00,\n         -1.5570e+00,  1.3880e+00,  1.5099e+00,  6.2576e-01, -1.4279e-03,\n          1.7448e+00,  1.9862e+00],\n        [ 4.5219e-02,  4.6843e-01, -1.1474e+00, -1.8876e+00, -5.7879e-01,\n          6.9787e-01, -7.2457e-02, -1.7235e+00, -9.9028e-01,  1.2248e+00,\n          6.4889e-01,  5.0363e-01,  1.8472e-01, -1.0468e+00, -1.0113e+00,\n         -1.0628e+00,  1.9783e+00, -1.8394e+00, -8.0410e-02, -5.9383e-01,\n         -1.6868e+00, -2.6366e-01, -8.3354e-01,  6.8552e-01, -8.6600e-02,\n          1.6034e+00,  7.3355e-01,  1.3205e+00,  1.4004e+00, -5.2889e-01,\n          5.6740e-01, -9.6958e-01, -1.4997e+00,  4.6890e-01, -1.7328e+00,\n          1.0302e+00, -5.7672e-01],\n        [-2.0183e-01,  9.5745e-01, -6.7022e-01, -1.4942e+00, -1.7716e+00,\n         -1.5369e+00,  5.3614e-01,  2.1942e-01, -4.8692e-01, -1.0483e+00,\n         -1.3250e+00, -2.7229e-01,  7.0113e-01,  6.7435e-01,  1.3605e+00,\n         -5.5024e-01, -8.2829e-01, -3.0993e-01, -2.9132e-02, -6.5741e-01,\n         -1.8838e+00, -1.5611e+00,  1.3386e+00, -9.3677e-01,  9.4050e-01,\n          1.6461e+00, -1.7923e+00, -1.2952e+00, -1.4606e+00,  1.9617e+00,\n          1.8974e+00, -3.5640e-01, -5.1258e-01,  1.3049e+00,  9.6022e-01,\n          1.8340e+00, -1.6090e+00],\n        [ 3.3658e-01, -1.9117e+00,  1.3840e+00,  1.4359e+00,  3.0289e-01,\n         -1.9664e+00, -1.8941e+00,  4.2836e-02,  1.6804e+00,  1.5752e+00,\n         -4.4672e-01,  1.0409e+00, -2.8504e-01, -1.3567e+00,  3.1620e-01,\n         -1.9444e+00,  1.5615e+00, -5.0563e-01, -1.8748e+00, -1.1123e+00,\n         -1.9222e+00,  1.3545e+00, -2.9159e-01, -4.6669e-01,  1.2639e+00,\n         -1.4171e+00, -2.7517e-01, -1.2380e+00, -1.5908e+00,  1.4929e+00,\n          1.0642e+00, -3.4285e-01, -1.8219e+00,  1.6329e+00, -1.2953e+00,\n          1.7803e+00,  3.6970e-01]])\n\n\nPassing these through softmax will normalize them from 0 to 1:\n\nsm_acts = acts.softmax(dim=1)\nsm_acts[0], sm_acts[0].sum()\n\n(tensor([0.0020, 0.0302, 0.0024, 0.0162, 0.0361, 0.0056, 0.0396, 0.0257, 0.0374,\n         0.0341, 0.0037, 0.0507, 0.0097, 0.0465, 0.0516, 0.0151, 0.0059, 0.0045,\n         0.0055, 0.0021, 0.0139, 0.0278, 0.0355, 0.0035, 0.0035, 0.0167, 0.0851,\n         0.0159, 0.0043, 0.0020, 0.0031, 0.0597, 0.0675, 0.0279, 0.0149, 0.0853,\n         0.1086]), tensor(1.0000))\n\n\nTaking the negative log of this tensor will give us the final loss:\n\nnll_loss = -1. * torch.log(sm_acts)\nnll_loss\n\ntensor([[6.2054, 3.4997, 6.0290, 4.1199, 3.3202, 5.1836, 3.2298, 3.6599, 3.2858,\n         3.3795, 5.5891, 2.9825, 4.6318, 3.0690, 2.9651, 4.1916, 5.1359, 5.3999,\n         5.2035, 6.1632, 4.2744, 3.5815, 3.3385, 5.6635, 5.6511, 4.0925, 2.4636,\n         4.1406, 5.4577, 6.1994, 5.7630, 2.8180, 2.6961, 3.5803, 4.2074, 2.4612,\n         2.2198],\n        [3.9156, 3.4924, 5.1082, 5.8484, 4.5396, 3.2629, 4.0333, 5.6843, 4.9511,\n         2.7360, 3.3119, 3.4572, 3.7761, 5.0076, 4.9721, 5.0235, 1.9825, 5.8002,\n         4.0412, 4.5546, 5.6476, 4.2245, 4.7943, 3.2753, 4.0474, 2.3574, 3.2273,\n         2.6403, 2.5604, 4.4897, 3.3934, 4.9304, 5.4605, 3.4919, 5.6936, 2.9306,\n         4.5375],\n        [4.3197, 3.1604, 4.7881, 5.6121, 5.8895, 5.6548, 3.5817, 3.8985, 4.6048,\n         5.1662, 5.4429, 4.3902, 3.4167, 3.4435, 2.7574, 4.6681, 4.9462, 4.4278,\n         4.1470, 4.7753, 6.0016, 5.6790, 2.7793, 5.0546, 3.1774, 2.4718, 5.9102,\n         5.4131, 5.5785, 2.1562, 2.2205, 4.4743, 4.6305, 2.8130, 3.1577, 2.2839,\n         5.7269],\n        [3.8515, 6.0998, 2.8041, 2.7522, 3.8852, 6.1545, 6.0822, 4.1453, 2.5077,\n         2.6129, 4.6348, 3.1472, 4.4732, 5.5448, 3.8719, 6.1325, 2.6266, 4.6937,\n         6.0629, 5.3004, 6.1103, 2.8336, 4.4797, 4.6548, 2.9243, 5.6052, 4.4633,\n         5.4261, 5.7790, 2.6952, 3.1239, 4.5310, 6.0101, 2.5552, 5.4834, 2.4078,\n         3.8184]])\n\n\nSuppose the target for each image was given by the following tensor, where the target is an integer from 0 to 36 representing one of the pet breeds:\n\ntargs = tensor([3, 0, 34, 10])\nidx = range(4)\nnll_loss[idx, targs]\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\ndef cross_entropy(acts, targs):\n  idx = range(len(targs))\n  sm_acts = acts.softmax(dim=1)\n  nll_loss = -1. * torch.log(sm_acts)\n  return nll_loss[idx, targs].mean()\n\nI compare this with the built-in F.cross_entropy and nn.CrossEntropyLoss functions:\n\nF.cross_entropy(acts, targs,reduction='none')\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targs)\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\nNote that the nn version of the loss function returns an instantiation of that function which then must be called with the activations and targets as its inputs.\n\ntype(nn.CrossEntropyLoss(reduction='none'))\n\ntorch.nn.modules.loss.CrossEntropyLoss"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Binary Cross Entropy Loss",
    "text": "Binary Cross Entropy Loss\nThe authors begin the discussion of explaining the multi-label classification model loss function by observing the activations from the trained model. I’ll do the same—I love that approach since it grounds the concepts involved in the construction of loss function in the actual model outputs.\n\nlearn = cnn_learner(dls, resnet18)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\nx, y = dls.train.one_batch()\nif torch.cuda.is_available():\n    learn.model.cuda()\nactivs = learn.model(x)\nactivs.shape\n\ntorch.Size([64, 20])\n\n\nEach batch has 64 images and each of those images has 20 activations, one for each label in .vocab. Currently, they are not restricted to values between 0 and 1.\nNote: the activations tensor has to first be placed on the cpu and then detached from the graph (which is used to track and calculate gradients of the weights with respect to the loss function) before it can be converted to a numpy array used for the plot.\n\nys = activs[0].cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nPassing them through a sigmoid function achieves that:\n\nys = activs[0].sigmoid().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe negative log of the activations is taken in order to push the differences between loss values. For vocab where the target is 1, -log(inputs) is calculated. For vocab where the target is 0, -log(1-inputs) is calculated. This seems counterintuitive at first, but let’s take a look at the plot of these functions:\n\nys = -activs[0].sigmoid().log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe sigmoid activations that were very close to 0 (Vocab Index = 0, 2, 5, and 16) are now much larger than those that were very close to 1 (Vocab Index = 6, 14, and 18). Since the target is 1, this correctly assigns a larger loss to the inaccurate predictions and the smaller loss to the accurate ones. We can say the same (but opposite) for -log(1-inputs), which is used when the target is 0.:\n\nys = -(1- activs[0].sigmoid()).log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nFinally, the mean of all image loss values is taken for the batch. The Binary Cross Entropy Function look likes this:\n\ndef binary_cross_entropy(inputs, targets):\n  inputs = inputs.sigmoid()\n  return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\nThe inputs (the activations for each vocab value)) is the first value and the targets of each image are the second value of the dls.train.one_batch() tuple.\n\nbinary_cross_entropy(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\nI will compare this with the built-in function F.binary_cross_entropy_with_logits and function class nn.BCEWithLogitsLoss to make sure I receive the same result.\n\nF.binary_cross_entropy_with_logits(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\n\nnn.BCEWithLogitsLoss()(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Mult-Label Classification Accuracy",
    "text": "Mult-Label Classification Accuracy\nFor single-label classification, the accuracy function compared whether the index of the highest activation matched the index of the target vocab. A single index for a single label.\n\ndef accuracy(inputs, targets, axis=-1):\n  predictions = inputs.argmax(dim=axis)\n  return (predictions==targets).float().mean()\n\nFor multi-label classification, each image can have more than one correct corresponding vocab index and the corresponding activations may not be the maximum of the inputs tensor. So instead of using the maximum, a threshold is used to identify predictions. If the activation is above that threshold, it’s considered to be a prediction.\n\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n  if sigmoid: inp = inp.sigmoid()\n  return ((inp > thresh)==targ.bool()).float().mean()\n\ntarg is a one-hot encoded Tensor, so 1s are converted to True and 0s are converted to False using the .bool method."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Training the Model",
    "text": "Training the Model\nAt last! I can now train the model, setting a different accuracy threshold as needed using the built-in partial function.\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.942256\n      0.698276\n      0.239323\n      00:29\n    \n    \n      1\n      0.821279\n      0.566598\n      0.281633\n      00:28\n    \n    \n      2\n      0.602543\n      0.208145\n      0.805498\n      00:28\n    \n    \n      3\n      0.359614\n      0.125162\n      0.939801\n      00:28\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.133149\n      0.112483\n      0.947072\n      00:29\n    \n    \n      1\n      0.115643\n      0.105032\n      0.953028\n      00:29\n    \n    \n      2\n      0.096643\n      0.103564\n      0.952769\n      00:29\n    \n  \n\n\n\nIn about three and a half minutes, this model was able to achieve more than 95% accuracy. I’ll look at its predictions on the validation images:\n\nlearn.show_results(max_n=18)\n\n\n\n\n\n\n\nVarying the threshold will vary the accuracy of the model. The metrics of the learner can be changed after training, and calling the validate method will recalculate the accuracy:\n\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.930816650390625]\n\n\nA threshold of 0.1 decreases the accuracy of the model, as does a threshold of 0.99. A 0.1 threshold includes labels for which the model was not confident, and a 0.99 threshold exclused labels for which the model was not very confident. I can calculate and plot the accuracy for a range of thresholds, as they did in the book:\n\npreds, targs = learn.get_preds()\nxs = torch.linspace(0.05, 0.95, 29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\n\n\n\n\nbest_threshold = xs[np.argmax(accs)]\nbest_threshold\n\ntensor(0.4679)\n\n\n\nlearn.metrics = partial(accuracy_multi, thresh=best_threshold)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.9636053442955017]\n\n\nThe highest accuracy (96.36%) is achieved when the threshold is 0.4679."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Regression",
    "text": "Regression\nThe authors provide some context here which, while I can appreciate, judge I won’t fully understand until I experience the next 5 or 6 chapters.\n\nA model is defined by its independent and dependent variables, along with its loss function. The means that there’s really a far wider array of models than just the simple domain-based split\n\nThe “domain-based split” is a reference to the distinction between computer vision, NLP and other different types of problems.\nTo illustrate their point, they have us work through an image regression problem with much of the same process (and model) as an image classification problem.\n\n# download data\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\n# helper functions to retrieve images\n# and to retrieve text files\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\n\n\n# check that `img2pose` converts file name correctly\nimg_files[0], img2pose(img_files[0])\n\n(Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_rgb.jpg'),\n Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_pose.txt'))\n\n\n\n# check image size\nim = PILImage.create(img_files[0])\nim.shape\n\n(480, 640)\n\n\n\n# view the image\nim.to_thumb(160)\n\n\n\n\n\n# helper function to extract coordinates\n# of the subject's center of head\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n  ctr = np.genfromtxt(img2pose(f), skip_header=3)\n  c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n  c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n  return tensor([c1,c2])\n\n\n# check coordinates of the first file\nget_ctr(img_files[0])\n\ntensor([444.7946, 261.7657])\n\n\n\n# create the DataBlock\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)]\n)\n\n\n# confirm that the data looks OK\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n# view tensors\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\n\nEach batch has 64 images. Each image has 3 channels (rgb) and is 240x320 pixels in size. Each image has 1 pair of coordinates.\n\n# view a single coordinate pair\nyb[0]\n\nTensorPoint([[0.0170, 0.3403]], device='cuda:0')\n\n\n\n# create Learner object\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\nThe y_range argument shifts the final layer’s sigmoid output to a coordinate between -1 and 1. The sigmoid function is transformed using the following function.\n\ndef plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = torch.linspace(min,max)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)\n\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\nplot_function(partial(sigmoid_range, lo=-1, hi=1), min=-4, max=4)\n\n\n\n\n\n# confirm loss function\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nfastai has chosen MSE as the loss function, which is appropriate for a regression problem.\n\n# pick a learning rate\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.033113110810518265)\n\n\n\n\n\n\n# use lr = 2e-2\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.047852\n      0.011552\n      01:55\n    \n    \n      1\n      0.007220\n      0.002150\n      01:56\n    \n    \n      2\n      0.003190\n      0.001313\n      01:56\n    \n    \n      3\n      0.002376\n      0.000295\n      01:56\n    \n    \n      4\n      0.001650\n      0.000106\n      01:54\n    \n  \n\n\n\nA loss of 0.000106 is an accuracy of:\n\nmath.sqrt(0.000106)\n\n0.010295630140987\n\n\nThe conclusion to this (what has felt like a marathon of a) chapter is profound:\n\nIn problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different number of outputs. The loss function is the one thing that changes, which is why it’s important to double-check that you are using the right loss function for your problem…make sure you think hard about your loss function, and remember that you most probably want the following:\n\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "",
    "text": "A chloropleth map of Minnesota Census data\nIn this blog post, I’ll walk through my process of creating an ArcGIS geodatabase and a set of layouts visualizing U.S. Census Data. The data used for this app is from table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months).\nYou can view the final layout PDFs at the following links:"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nGet the Data\n\nTract Boundaries\nACS 5-Year Estimates\nUsing data.census.gov\nUsing the censusapi R package\n\nConnect Data to Geodatabase\n\nTract Boundaries\nACS 5-Year Estimates\n\nVisualize Data\n\nCreate a Map\nCreate a Symbology\nCreate a Layout\n\nNormalize the Data\n\nCreate Additional Layouts"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Get the Data",
    "text": "Get the Data\n\nTract Boundaries\n\nDownload and unzip 2019 TIGER Shapefile for MN (tl_2019_27_tract.zip) (corresponds to the final year, 2019, in the ACS 5-year estimates). These will contain the Census Tract geographies needed to create a map in ArcGIS.\n\n\n\nACS 5-Year Estimates\n\nUsing data.census.gov\n\nOn data.census.gov, search for B20005\n\n\n\nSelect the link to the Table B20005 with “2019 inflation-adjusted dollars”\n\n\n\nClick the dropdown at the top next to the label Product and select 2015: ACS 5-Year Estimates Detailed Tables\n\n\n\nClick Customize Table at the top right of the page\n\n\n\nIn the Geo* section, click Tract > Minnesota > All Census Tracts within Minnesota\n\n\n\nOnce it’s finished loading, click Close and then Download Table\n\n\n\nOnce downloaded, extract the zip folder and open the file ACSDT52015.B20005_data_with_overlays….xslx_ in Excel any tool that can handle tabular data\nSlice the last 11 characters of the GEO_ID (using the RIGHT function in a new column) to replace the existing GEO_ID column values. For example, a GEO_ID of 1400000US27029000100 should be replaced with 27029000100. This will later on be matched with the GEOID field in the tl_2019_27_tract shapefile\nSave/export the file as .XLSX\n\n\n\nUsing the censusapi R package\nPass the following arguments to the censusapi::listCensusMetadata function and assign its return value to B20005_vars:\n\nB20005_vars <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  type=\"variables\",\n  group=\"B20005\"\n)\n\n\nPass the following arguments to censusapi::getCensus and assign its return value to B20005:\n\n\nB20005 <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  region=\"tract:*\",\n  regionin=\"state:27\", # 27 = Minnesota state FIPS code\n  vars=c(\"GEO_ID\", \"NAME\", B20005_vars$name)\n)\n\n\nReplace GEO_ID (or create a new column) with the last 11 characters\n\n\nB20005 <- substr(B20005$GEO_ID, 10, 20)\n\n\nExport to an .XLSX file\n\n\nwrite.xlsx(B20005, “acs5_b20005_minnesota.xlsx”, row.names = FALSE)"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Connect Data to Geodatabase",
    "text": "Connect Data to Geodatabase\nOpen ArcGIS Pro and start a new project.\n\nTract Boundaries\n\nRight click Folders in the Contents pane and click Add folder connection\n\n\n\nSelect the downloaded (and extracted) tl_2019_27_tract folder and click OK\n\n\n\nClick on tl_2019_27_tract folder in the Contents pane\nIn the Catalog pane, right-click tl_2019_27.shp and then click Export > Feature Class to Geodatabase\n\n\n\nConfirm Input Features (tl_2019_27_tract.shp) and Output Geodatabase (Default.gdb or whatever geodatabase you are connected to) and then click the green Run button\nRefresh the Geodatabase and click on it in the Contents pane to view the added shapefile\n\n\n\n\nACS 5-Year Estimates\n\nUnder the View ribbon click on Geoprocessing to open that pane\nIn the Geoprocessing pane, search for Join Field and click on it\n\n\n\nNext to Input Table click on the folder icon to Browse. Select the tl_2019_27_tract table in your geodatabase\n\n\n\nClick the Input Join Field dropdown and select GEOID\nNext to Join Table click on the folder icon to Browse. Select the acs5_b20005_minnesota$ Excel table and click OK (note: the Excel table is inside the XLSX file)\n\n\n\nType GEO_ID under Join Table Field\nClick on the down arrow next to Transfer Fields and select B20005_002E, B20005_003E, B20005_049E, and B20005_050E\n\n\n\nClick on Validate Join\n\n\n\nClick on Run\nA success message should be displayed at the bottom of the Geoprocessing pane"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Visualize the Data",
    "text": "Visualize the Data\nIn this section, I’ll create maps and layouts to visualize the population estimates using Census Tract spatial data.\n\nCreate a Map\n\nIn the Catalog pane, right-click tl_2019_27_tract > Add to New > Map\n\n\n\nTo reference the raw data: from the Feature Layer ribbon, click Attribute Table\n\n\n\n\n\nCreate a Symbology\n\nSelect the tl_2019_27_tract layer in Contents pane\nClick Appearance under the Feature Layer ribbon\nClick the down arrow on Symbology and select Graduated Colors\n\n\n\nSelect B20005_002E in the Field dropdown and Natural Breaks (Jenks) for the Method\n\n\n\nThe class breaks created by this method do not reliably classify the data, which is determined using the City of New York Department of Planning Map Reliability Calculator. There’s a 10.1% chance that a tract is erroneously classified.\n\n\n\nAfter adjusting the class breaks, the following result in a reliable result (less than 10% chance of misclassifying any geography on the map and less than 20% of misclassifying estimates within a class due to sampling error)\n\n\n\nApply these breaks in the Classes tab in the Symbology pane\n\n\n\nThe Map pane displays the updated choropleth\n\n\n\n\nCreate a Layout\nUnder the Insert ribbon click on New Layout and Letter (8.5” x 11”)\n\n\n\nOn the Insert ribbon, click Map Frame and Default Extent under the Map category\n\n\n\nClick and drag the cursor to draw the Map Frame. Under the Layout ribbon select Activate and zoom/pan until the full choropleth is visible. Click Deactivate when you’re finished.\n\n\n\nAdd guides to create 0.5 inch margins by right-clicking on rulers clicking Add Guide\n\n\n\nUnder the Insert ribbon click on Legend and draw a rectangle underneath the map\n\n\n\nRight-click the legend and click Properties to format the font size, text visibility (under Legend Item in the dropdown next to Legend in the Format Legend panel) and more\n\n\n\nOn the Ribbon tab in the Graphics and Text panel, you can choose different text types to add text to your layout. I’ve added titles and explanatory text.\n\n\n\nThe census tracts for the city of Minneapolis are too small to be clearly visible. Under the Insert ribbon click Map Frame, select the map and draw a small rectangle over Wisconsin.\nWith the new Map Frame selected, click Reshape > Circle under the Insert ribbon. Draw a circle over the rectangular map.\n\n\n\nRight-click on the circular map and click Properties to add a border. Add a textbox to label it as the City of Minneapolis.\n\n\n\nFrom the Graphics and Text panel on the Insert ribbon use the straight line and circle tool to add some visual cues indicating that the map frame is a detail view of the city\n\n\n\nUnder the Share ribbon, select Export Layout and export it to a PDF file"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Normalize the Data",
    "text": "Normalize the Data\nWhile the worker population estimates gives us a sense of how workers are distributed across the state, they are a proxy for population density. Census Tracts in Urban areas, like the Minneapolis, will likely have more workers than Rural areas, because they have a higher population. To supplement this layout, I’ll create layouts that show the percentage of the total sex population who are full time workers.\n\nTo duplicate the Male Full TIme Estimates layout, right-click it in the Catalog pane, click Copy and then right click in the gray area underneath it and click Paste\n\n\n\n\nRename the layout to Male Full Time Percentages and open it\nRename the two maps in the Contents pane\n\n\n\nRight-click tl_2019_27_tract under Main Map and click Symbology to open the Symbology pane\n\n\n\nSelect B20005_002E (Total Male Estimate) in the Normalization dropdown. This will be the value that divides a Census Tract’s population estimate\n\n\n\nCalculate the Margin of Error (MOE) for the percentage of total male workers who are full time employed using equation 6 from the “Calculating Measures of Error for Derived Estimates” in the Understanding and Using American Community Survey Data: What All Data Users Need to Know handbook in order to determine the class break reliability. In the equation below, P = X/Y is the percentage of full time workers in the tract (X= B20005_003E and Y = B20005_002E)\n\n\n\nOne reliable set of class breaks, which were few and far between, was the following:\n\n\n\nApply those class breaks in the Symbology pane and update the text to match\n\n\n\nCreate Additional Layouts\n\nRepeat the process to create the following Layouts given the following class breaks\n\nFemale Full Time Estimates\n\n\n\n\nFemale Full Time Percentages\n\n\n\nI hope you enjoyed this tutorial."
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "",
    "text": "Here’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, I work through the first of four “Further Research” problems assigned at the end of Chapter 7 in the textbook “Deep Learning for Coders with fastai and PyTorch”.\nThe prompt for this exercise is:\n\nUse the fastai documentation to build a function that crops an image to a square in each of the four corners; then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai?"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "What is Test Time Augmentation?",
    "text": "What is Test Time Augmentation?\nI’ll quote directly from the text:\n\nDuring inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n\nTTA is data augmentation during validation, in hopes that objects located outside the center of the image (which is the default fastai validation image crop) can be recognized by the model in order to increase the model’s accuracy.\nThe default Learner.tta method averages the predictions on the center crop and four randomly generated crops. The method I’ll create will average the predictions between the center crop and four corner crops.\n\n\n\ntta.png"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "User-defined Test Time Augmentation",
    "text": "User-defined Test Time Augmentation\n\nRead and understand the Learner.tta and RandomCrop source code\ndef tta(self:Learner, ds_idx=1, dl=None, n=4, item_tfms=None, batch_tfms=None, beta=0.25, use_max=False):\n    \"Return predictions on the `ds_idx` dataset or `dl` using Test Time Augmentation\"\n    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n    if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n    try:\n        self(_before_epoch)\n        with dl.dataset.set_split_idx(0), self.no_mbar():\n            if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n            aug_preds = []\n            for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n        aug_preds = torch.cat(aug_preds)\n        aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n        self.epoch = n\n        with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n    finally: self(event.after_fit)\n\n    if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n    preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n    return preds,targs\nclass RandomCrop(RandTransform):\n    \"Randomly crop an image to `size`\"\n    split_idx,order = None,1\n    def __init__(self, size, **kwargs):\n        size = _process_sz(size)\n        store_attr()\n        super().__init__(**kwargs)\n\n    def before_call(self, b, split_idx):\n        self.orig_sz = _get_sz(b)\n        if split_idx: self.tl = (self.orig_sz-self.size)//2\n        else:\n            wd = self.orig_sz[0] - self.size[0]\n            hd = self.orig_sz[1] - self.size[1]\n            w_rand = (wd, -1) if wd < 0 else (0, wd)\n            h_rand = (hd, -1) if hd < 0 else (0, hd)\n            self.tl = fastuple(random.randint(*w_rand), random.randint(*h_rand))\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n        return x.crop_pad(self.size, self.tl, orig_sz=self.orig_sz)\n\n\nPractice cropping images using the .crop method on a PILImage object\nA PIL Image has a method called crop which takes a crop rectangle tuple, (left, upper, right, lower) and crops the image within those pixel bounds.\nHere’s an image with a grizzly bear at the top and a black bear on the bottom. There are four coordinates of interest: left, upper, right and bottom. The leftmost points on the image are assigned a pixel value of 0. The rightmost points are located at the image width pixel pixel value. The uppermost points are at pixel 0, and the bottommost points are at the image height pixel value.\n\nf = \"/content/gdrive/MyDrive/fastai-course-v4/images/test/grizzly_black.png\"\nimg = PILImage.create(f)\nimg.to_thumb(320)\n\n\n\n\n\nTop-Left Corner Crop\nA top-left corner crop the corresponds to a left pixel of 0, upper pixel 0, right pixel of 224, and bottom pixel of 224. The order in the tuple is left, upper, right, bottom, so 0, 0, 224, 224. You can see that this crop is taken from the top left corner of the original image.\n\nimg.crop((0,0,224,224))\n\n\n\n\n\n\nTop Right Corner Crop\nFor the top right corner, I get the image width since the left end of the crop will be 224 pixels from the right end of the image. That translates to w-224. The upper pixel is 0, and the rightmost pixel is at w, and the bottom pixel is 224. You can see that this crop is at the top right corner of the original.\n\nw = img.width\nh = img.height\nimg.crop((w-224, 0, w, 224))\n\n\n\n\n\n\nBottom Right Corner Crop\nFor the bottom right corner the left pixel is 224 from the right end, w-224, the upper pixel is 224 from the bottom, h-224, the right pixel is at w, and the bottom is at h.\n\nimg.crop((w-224, h-224, w, h))\n\n\n\n\n\n\nBottom Left Corner Crop\nThe bottom left corner’s leftmost pixel is 0, uppermost pixel is 224 pixels from the bottom of the whole image, h - 224, the rightmost pixel is 224, and bottommost pixel is the bottom of the whole image, at h.\n\nimg.crop((0, h-224, 224, h))\n\n\n\n\n\n\nCenter Crop\nFinally, for the center crop, the leftmost pixel is 112 left of the image center, w/2 - 112, the upper pixel is 112 above the image center, h/2 - 112, the rightmost pixel is 112 right of the center, w/2 + 112, and the bottom pixel is 112 below the center, h/2 + 112.\n\nimg.crop((w/2-112, h/2-112, w/2+112,h/2+112))\n\n\n\nSummary\nTo better visualize this, here are a couple of images which show the left, upper, right and bottom coordinates for the corner and center crops.\nSummary of corner crop arguments (left, upper, right, bottom)\n\n\n\ncrop_dimensions-01.png\n\n\nSummary of center crop arguments (left, upper, right, bottom)\n\n\n\ncenter_crop_dimensions-01.png\n\n\n\n\n\nDefine a function which takes an image and returns a stacked Tensor with four corner crops and a center crop\nI wrap those five lines of code into a function called corner_crop, which takes a PILImage img, and a square side length size (defaulted to 224) as its arguments. It first grabs the width and height of the image. And then goes on to save the crops of the four corners and center as TensorImages, returning them all in a single stacked Tensor.\n\ndef corner_crop(img, size=224):\n  \"\"\"Returns a Tensor with 5 cropped square images\n  img: PILImage\n  size: int\n  \"\"\"\n  w,h = img.width, img.height\n  top_left = TensorImage(img.crop((0,0,size,size)))\n  top_right = TensorImage(img.crop((w-size, 0, w, size)))\n  bottom_right = TensorImage(img.crop((w-size, h-size, w, h)))\n  bottom_left = TensorImage(img.crop((0, h-size, size, h)))\n  center = TensorImage(img.crop((w/2-size/2, h/2-size/2, w/2+size/2,h/2+size/2)))\n  return torch.stack([top_left, top_right, bottom_right, bottom_left, center])\n\nI’ll test the corner_crop function and make sure that the five images are cropped correctly.\nHere’s the top left corner.\n\nimgs = corner_crop(img)\n\n# Top Left Corner Crop\nimgs[0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1a177d0>\n\n\n\n\n\nTop right corner:\n\n# Top Right Corner Crop\nimgs[1].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e197da50>\n\n\n\n\n\nBottom right:\n\n# Bottom Right Corner Crop\nimgs[2].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e146ed50>\n\n\n\n\n\nBottom left:\n\n# Bottom Left Corner Crop\nimgs[3].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424dd0>\n\n\n\n\n\nAnd center:\n\n# Center Crop\nimgs[4].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424450>\n\n\n\n\n\n\n\nDefine a new CornerCrop transform by extending the Transform class definition\nThe main purpose for all of that was for me to wrap my head around how the crop behavior functions so that I can wrap that into a transform.\nTransforms are any function that you want to apply to your data. I’ll extend the base Transform class and add in the functionality I need for these crops. When an object of the CornerCrop class is constructed, the constructor takes size and corner_type arguments. Since I’ll use this within a for-loop, the corner_type argument is an integer from 0 to 3, corresponding to the loop counter. The transform is applied to the data during the .encodes method. I grab the original image width and height, and create a list of cropped images using the left, upper, right, bottom coordinates we saw above. Finally, based on the corner_type, the corresponding crop is returned.\n\nclass CornerCrop(Transform):\n    \"Create 4 corner and 1 center crop of `size`\"\n    def __init__(self, size, corner_type=0, **kwargs):\n      self.size = size\n      self.corner_type = corner_type\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n      self.w, self.h = x.size\n      self.crops = [\n                    x.crop((0,0,self.size, self.size)),\n                    x.crop((self.w - self.size, 0, self.w, self.size)),\n                    x.crop((self.w-self.size, self.h-self.size, self.w, self.h)),\n                    x.crop((0, self.h-self.size, self.size, self.h))\n                    ]\n      return self.crops[self.corner_type]\n\nTo test this transform, I created an image with top left, top right, bottom right and bottom left identified. I created multiple copies so that I can create batches.\n\n# test image for CornerCrop\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\nImage.open((path/'01.jpg'))\n\n\n\n\nI create a DataBlock and pass my CornerCrop to the item_tfms parameter. I’ll cycle through the different corner types. 0 corresponds to top left, 1 is top right, 2 is bottom right and 3 is bottom left. All images in my batch should be cropped to the same corner.\nI set corner_type to 0, build the DataBlock and DataLoaders and the batch shows top left.\n\n# get the data\n# path = untar_data(URLs.IMAGENETTE)\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,0))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 1, build the DataBlock and DataLoaders and the batch shows top right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,1))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 2, build the DataBlock and DataLoaders and the batch shows bottom right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,2))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 3, build the DataBlock and DataLoaders and the batch shows bottom left.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,3))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nNow, I can implement this transform into a new TTA method.\n\n\nDefine a new Learner.corner_crop_tta method by repurposing the existing Learner.tta definition\nI’ll largely rely on the definition of tta in the built-in Learner class. In this method, predictions are calculated on four sets of augmented data (images) and then averaged along with predictions on a center-crop dataset.\nIn the existing for-loop, four sets of predictions on randomly generated crops are appended into a list.\nfor i in self.progress.mbar if hasattr(self,'progress') else range(n):\n  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\nIn my loop, I create a new DataLoader each time, passing a different corner_type argument to the CornerCrop transform. I also have to pass the ToTensor transform, so that the PIL Image is converted to a Tensor. In the first iteration, it will append predictions on the top left corner crops. In the next one, it will append predictions on the top right, then the bottom right, and finally on the fourth loop, the bottom left.\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nSince I am to average these with the center-crop image predictions, I’ll create a new DataLoader without the CornerCrop transform and calculate the predictions on those images:\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\nFinally, I’ll append the center crop preds to aug_preds list, concatenate them into a single tensor and take the mean of the predictions:\naug_preds.append(preds[None])\npreds = torch.cat(aug_preds).mean(0)\nI decided to create a new Learner2 class which extends the built-in the Learner, and added the corner_crop_tta method by copying over the tta method, commenting out the lines I won’t need and adding the lines and changes I’ve written above.\n\nclass Learner2(Learner):\n  def corner_crop_tta(self:Learner, ds_idx=1, dl=None, n=4, beta=0.25, use_max=False):\n      \"Return predictions on the `ds_idx` dataset or `dl` using Corner Crop Test Time Augmentation\"\n      if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n      # if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n      try:\n          #self(_before_epoch)\n          with dl.dataset.set_split_idx(0), self.no_mbar():\n              if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n              aug_preds = []\n              # Crop image from four corners\n              for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                  dl = dl.new(after_item=[CornerCrop(224,i), ToTensor])\n                  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n         # aug_preds = torch.cat(aug_preds)\n         # aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n          self.epoch = n\n          dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n          # Crop image from center\n          with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n          aug_preds.append(preds[None])\n      finally: self(event.after_fit)\n\n     # if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n     # preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n     # preds = torch.cat([aug_preds, preds]).mean(0)\n      preds = torch.cat(aug_preds).mean(0)\n      return preds,targs\n\n\n\nImplement this new TTA method on the Imagenette classification model\nIn the last section of this notebook, I train a model on the Imagenette dataset, which a subset of the larger ImageNet dataset. Imagenette has 10 distinct classes.\n\n# get the data\npath = untar_data(URLs.IMAGENETTE)\n\n# build the DataBlock and DataLoaders \n# for a single-label classification\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label, # image folder names are the class names\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\ndls = dblock.dataloaders(path, bs=64)\n\n# view a batch\ndls.show_batch()\n\n\n\n\n\n\n\n\n# Try `CornerCrop` on a new DataLoader\n# add `ToTensor` transform to conver PILImage to TensorImage\nnew_dl = dls[1].new(after_item=[CornerCrop(224,3), ToTensor])\nnew_dl.show_batch()\n\n\n\n\n\n# baseline training\nmodel = xresnet50()\nlearn = Learner2(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.628959\n      2.382344\n      0.450336\n      02:39\n    \n    \n      1\n      1.258259\n      3.365233\n      0.386482\n      02:45\n    \n    \n      2\n      0.992097\n      1.129573\n      0.653473\n      02:49\n    \n    \n      3\n      0.709120\n      0.643617\n      0.802091\n      02:47\n    \n    \n      4\n      0.571318\n      0.571139\n      0.824122\n      02:45\n    \n  \n\n\n\nI run the default tta method, pass the predictions and targets to the accuracy function and calculate an accuracy of about 83.5% percent. Which is higher than the default center crop validation accuracy.\n\n# built-in TTA method\npreds_tta, targs_tta = learn.tta()\naccuracy(preds_tta, targs_tta).item()\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\n0.8345780372619629\n\n\nFinally, I run my new corner_crop_tta method, pass the predictions and targets to the accuracy function, and calculate an accuracy of about 70.9% percent. Which is lower than the default center crop validation accuracy.\n\n# user-defined TTA method\npreds, targs = learn.corner_crop_tta()\naccuracy(preds, targs).item()\n\n\n\n\n0.7098581194877625\n\n\nI’ll walk through the corner_crop_tta code to verify the accuracy calculated above.\nI first create an empty list for my augmented image predictions.\nThen I loop through a range of 4, each time creating a new DataLoader which applies the CornerCrop transform for each corner type and append the predictions onto the list.\n\n# get predictions on corner cropped validation images\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nlen(aug_preds), aug_preds[0].shape\n\n\n\n\n\n\n\n\n\n\n\n\n\n(4, torch.Size([1, 2678, 1000]))\n\n\nI then create a new DataLoader without my transform, and get those predictions.\n\n# get predictions on center crop validation images\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\npreds.shape\n\n\n\n\ntorch.Size([2678, 1000])\n\n\nThe shape of these predictions is missing an axis, so I pass None as a Key and it adds on a new axis.\n\n# add an axis to match augmented prediction tensor shape\npreds = preds[None]\npreds.shape\n\ntorch.Size([1, 2678, 1000])\n\n\nI append the center crop predictions onto the augmented predictions and concatenate all five sets of predictions into a Tensor and calculate the mean.\n\n# average all 5 sets of predictions\naug_preds.append(preds)\npreds = torch.cat(aug_preds).mean(0)\n\nI then pass those average predictions and the targets to the accuracy function calculate the accuracy which is slightly higher than above. I ran these five cells multiple times and got the same accuracy value. When I ran the corner_crop_tta method multiple times, I got different accuracy values each time. Something in the corner_crop_tta definition is incorrect. I’ll go with this value since it was consistent.\n\n# calculate validation set accuracy\naccuracy(preds, targs).item()\n\n0.7311426401138306\n\n\nThe following table summarize the results from this training:\n\n\n\nValidation\nAccuracy\n\n\n\n\nCenter Crop\n82.4%\n\n\nCenter Crop + 4 Random Crops: Linearly Interpolated\n83.5%\n\n\nCenter Crop + 4 Random Crops: Averaged\n73.1%\n\n\n\nThere are a few further research items I should pursue in the future:\n\nFix the corner_crop_tta method so that it returns the same accuracy each time it’s run on the same trained model\nTry corner_crop_tta on a multi-label classification dataset such as PASCAL\nTry linear interpolation (between center crop and corner crop maximum) instead of mean"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "title": "Transcribing Sherlock into Spanish",
    "section": "",
    "text": "Sherlock Holmes kneeling next to Toby the bloodhound and pointing, likely towards where he thinks Toby should go next\nTranscription Progress (00:06:07 out of 17:40:32 transcribed)"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Background",
    "text": "Background\nI have watched all four seasons of BBC’s Sherlock probably 5 times. I learn something new about it each time.\nI have tried to learn Spanish using Duolingo, stopping and re-starting every year or so, without much success.\nI don’t really recall how the thought came about but I decided to combine my love of the show with my desire to learn Spanish into one project—this one!"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Setup",
    "text": "Setup\nUsing the embedded Google Translate UI and my partner’s translator-level knowledge of the language, I am transcribing every word of the show into Spanish.\n\n\n\nA screenshot of my translation setup: Google Translate embedded underneath the search bar—the result of googling “Google Translate”. I’ve typed “Sherlock Holmes” in the “English” textbox on the left and it has translated to “Sherlock Holmes” in the Spanish output on the right.\n\n\nIn a second tab, I have the show open (with subtitles on).\n\n\n\nA screenshot of Sherlock playing in the Amazon Prime Video player\n\n\nI transcribe in a .txt file titled transcript.txt, documenting the following fields:\n\nseason number\nepisode number\ntimestamp (hours::minutes:seconds)\nwho is the speaker?\nthe english transcription of what they say\nthe spanish translation of that\nnotes which usually documents specific word translations\n\nAs an example, the first bit of dialogue in the series is John Watson’s therapist Ella asking him “How’s your blog going?” which translates to “Cómo va tu blog?” Where va = goes.\nHow goes your blog? I would say quite well heheheh.\nseason,episode,time,speaker,english,spanish,notes\n1,1,00:01:30,ella,how's your blog going?,cómo va tu blog?, va = goes"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "title": "Transcribing Sherlock into Spanish",
    "section": "What I’m Learning",
    "text": "What I’m Learning\nI’ll write in this blog post some examples of the translations and how I’m thinking through the process, as well as what I’m learning from discussions with my partner.\nThree main themes I’m seeing so far about translating from English to Spanish:\n\nwhich words to use depends a lot on context.\nwords that sound the same but mean different things will sometimes have different emphasis.\na word that is technically correct may not be used frequently in conversation.\n\nI’m not quite sure how to best document what I’m learning so I’ll just start writing.\n\nElla: “You haven’t written a word, have you?”\nSomething I enjoy doing is translating the Spanish back into English without changing word positions. The benefit of this exercise of translating and translating back is that it reveals (or focuses my attention on) nuances I wouldn’t otherwise be aware of.\nEnglish: You haven’t written a word, have you?\nSpanish: No has escrito una palabra verdad?\nBack to English: Not you have written a word true?\nI asked my partner how she would translate it and she said: No has escrito ni una palabra, verdad?\nWhich translates to: You haven’t written not even a word, true?\nIt bothers me that I don’t know why in English the question ends in have you? but in Spanish it ends with true?. Of course this may just be how Spanish works or how conversational Spanish works.\nI asked my partner how you would say just have you? in Spanish and it’s lo has?\nGoogle Translate aligns with this when it translates from Spanish to English:\nSpanish: No has escrito ni una palabra lo has?\nEnglish: You haven’t written a word, have you?\nBut recommends ending with verdad? when I translate from English to Spanish.\n\n\nSpeaker: “You can share mine”\nHere are the Google Translate forward and backward translations:\n\nEnglish: You can share mine.\nSpanish: Puedes compartir el mio.\n\nSpanish: Puedes compartir el mio.\nEnglish: Can you share mine.\n\nHowever, if I start the Spanish translation with tu the English translation matches my original prompt:\n\nSpanish: Tu puedes compartir el mio.\nEnglish: You can share mine.\n\nI think this is a good example of how what is technically correct may or may not be what’s used in conversation—saying tu may not be strictly required for conversation and may be implicitly understood because of the form used—puedes (you can).\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npuedes\nyou can\n\n\npuedo\nI can\n\n\npuedemos\nwe can\n\n\npueden\nthey can\n\n\n\n\n\nLestrade: “Well, they all took the same poison.”\nSomething else I’ve enjoyed and learned from is watching how a translation changes as you type the full sentence in Google Translate.\nFor example when translating from Spanish (pues, todos tomaron el mismo veneno) to English (well, they all took the same poison):\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npues\nwell\n\n\npues, todos\nwell, everyone\n\n\npues, todos tomaron\nwell, they all took\n\n\npues, todos tomaron el\nwell, everyone took\n\n\npues, todos tomaron el mismo\nwell, they all took the same\n\n\npues, todos tomaron el mismo veneno\nwell, they all took the same poison\n\n\n\nWhat I’m observing might have less to do with how Spanish works and more to do with how Google Translate works. Although some words seem interchangeable (todos seems to mean everyone or they all)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "",
    "text": "In this blog post, I have written excerpts from the book Visualization Analysis & Design by Tamara Munzner."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#structure-whats-in-this-book",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#structure-whats-in-this-book",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "Structure: What’s in This Book",
    "text": "Structure: What’s in This Book\n\nChapter 1: high-level introduction to an analysis framework of breaking down vis design to what-why-how questions that have data-task-idiom answers\nChapter 2: addresses the what question with answers about data abstraction\nChapter 3: addresses the why question with task abstractions\nChapter 4: extends the analysis framework to two additional levels: the domain situation level on top and the algorithm level on the bottom\nChapter 5: the principles of marks and channels for encoding information\nChapter 6: eight rules of thumb for design\nChapter 7: how to visually encode data by arranging space for tables\nChapter 8: for spatial data\nChapter 9: for networks\nChapter 10: choices for mapping color and other channels in visual encoding\nChapter 11: ways to manipulate and change a view\nChapter 12: ways to facet data between multiple views\nChapter 13: how to reduce the amount of data shown in each view\nChapter 14: embedding information about a focus set within the context of overview data\nChapter 15: six case studies\n\nAccompanying web page"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-human-in-the-loop",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-human-in-the-loop",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.2 Why Have a Human in the Loop?",
    "text": "1.2 Why Have a Human in the Loop?\nVis allows people to analyze data when they don’t know exactly what questions they need to ask in advance.\nIf a fully automatic solution has been deemed to be acceptable, then there is no need for human judgment, and thus no need for you to design a vis tool.\nThe outcome of designing vis tools targeted at specific real-world domain problems is often a much crisper understanding of the user’s task, in addition to the tool itself."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-computer-in-the-loop",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-computer-in-the-loop",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.3 Why Have a Computer in the Loop?",
    "text": "1.3 Why Have a Computer in the Loop?\nBy enlisting computation, you can build tools that allow people to explore or present large datasets that would be completely unfeasible to draw by hand, thus opening up the possibility of seeing how datasets change over time.\nAs a designer, you can think about what aspects of hand-drawn diagrams are important in order to automatically create drawings that retain the hand-drawn spirit."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-an-external-representation",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-an-external-representation",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.4 Why Use an External Representation?",
    "text": "1.4 Why Use an External Representation?\nVis allows people to offload internal cognition and memory usage to the perceptual system, using carefully designed images as a form of external representations, sometimes called external memory."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-depend-on-vision",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-depend-on-vision",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.5 Why Depend on Vision?",
    "text": "1.5 Why Depend on Vision?\nThe visual system provides a very high-bandwidth channel to our brains. A significant amount of visual information processing occurs in parallel at the preconscious level.\nSound is poorly suited for providing overviews of large information spaces compared with vision. We experience the perceptual channel of sound as a sequential stream, rather than as a simultaneous experience where what we hear over a long period of time is automatically merged together."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-show-the-data-in-detail",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-show-the-data-in-detail",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.6 Why Show the Data in Detail?",
    "text": "1.6 Why Show the Data in Detail?\nStatistical characterization of datasets is a very powerful approach but it has the intrinsic limitation of losing information through summarization.\nAnscombe’s Quartet illustrates how datasets that have identical descriptive statistics can have very different structures that are immediately obvious when the dataset is shown graphically."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-interactivity",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-interactivity",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.7 Why Use Interactivity?",
    "text": "1.7 Why Use Interactivity?\nWhen datasets are large enough, the limitations of both people and display preclude just showing everything at once."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-the-vis-idiom-design-space-huge",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-the-vis-idiom-design-space-huge",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.8 Why is the Vis Idiom Design Space Huge?",
    "text": "1.8 Why is the Vis Idiom Design Space Huge?\nidiom: a distinct approach to creating and manipulating visual representations."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-tasks",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-tasks",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.9 Why Focus on Tasks?",
    "text": "1.9 Why Focus on Tasks?\nA tool that serves well for one task can be poorly suited for another, for exactly the same dataset.\nReframing the users’ task from domain-specific form into abstract form allows you to consider the similarities and differences between what people need across many real-world usage contexts."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-effectiveness",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-effectiveness",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.10 Why Focus on Effectiveness?",
    "text": "1.10 Why Focus on Effectiveness?\nThe goals of the designer are not met if the result is beautiful but not effective.\nAny depiction of data is an abstraction where choices are made about which aspects to emphasize."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-most-designs-ineffective",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-most-designs-ineffective",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.11 Why Are Most Designs Ineffective?",
    "text": "1.11 Why Are Most Designs Ineffective?\nThe vast majority of the possibilities in the design space will be ineffective for any specific usage context.\nIn addressing design problems, it’s not a very useful goal to optimize or find the very best choice. A more appropriate goal when you design is to satisfy or find one of the many possible good solutions rather than one of the even larger number of bad ones.\nProgressively smaller search spaces:\n\nSpace of possible solutions\nSpace of solutions known to the designer\nSpace of solutions you actively consider\nSpace of solutions you investigate in detail\nSelected solution\n\nThe problem of a small consideration space is the higher probability of only considering OK or poor solutions and missing a good one.\nOne way to ensure that more than one possibility is considered is to explicitly generate multiple ideas in parallel."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-validation-difficult",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-validation-difficult",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.12 Why Is Validation Difficult?",
    "text": "1.12 Why Is Validation Difficult?\nHow do you know it works? How do you argue that one design is better or worse than another for the intended users? What does better mean? Do users get something done faster? Do they have more fun doing it? Can they work more effectively? What does effectively mean? How do you measure insight or engagement? What is the design better than? Is it better than another vis system? Is it better than doing the same things manually, without visual support? Is it better than doing the same things completely automatically? And what sort of thing does it do better? How do you decide what sort of task the users should do when testing the system? And who is the user? An expert who has done this task for decades, or a novice who needs the task to be explained before they begin? Are they familiar with how the system works from using it for a long time, or are they seeing it for the first time? Are the users limited by the speed of their own thought process, or their ability to move the mouse, or simply the speed of the computer in drawing each picture?\nHow do you decide what sort of benchmark data you should use when testing the system? Can you characterize what classes of data the system is suitable for? How might you measure the quality of an image generated by a vis tool? How well do any of the automatically computed quantitative metrics of quality match up with human judgments? Does the complexity of the algorithm depend on the number of data items to show or the number of pixels to draw? Is there a trade-off between computer speed and computer memory usage?"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-there-resource-limitations",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-there-resource-limitations",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.13 Why Are There Resource Limitations?",
    "text": "1.13 Why Are There Resource Limitations?\nThree different kinds of limitations:\n\nComputational capacity\nHuman perceptual and cognitive capacity\nDisplay capacity\n\nscalability: design systems to handle large amounts of data gracefully.\nDesigning systems that gracefully handle larger datasets that do not fit into core memory requires significantly more complex algorithms.\nHuman memory for things that are not directly visible is notoriously limited.\nchange blindness: when even very large changes are not noticed if we are attending to something else in our view.\ninformation density: a measure of the amount of information encoded versus the amount of unused space.\nThere is a trade-off between the benefits of showing as much as possible at once (to minimize the need for navigation and exploration) and the costs of showing too much at once (where the user is overwhelmed by visual clutter)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.14 Why Analyze?",
    "text": "1.14 Why Analyze?\nAnalyzing existing systems is a good stepping stone to designing new ones.\nHigh-level framework for analyzing vis use according to three questions:\n\nwhat data the user sees (data)\nwhy the user intends to use a vis tool (task)\nhow the visual encoding and interaction idioms are constructed in terms of design choices (idiom)\n\none of these analysis trios is called an instance.\nComplex vis tool usage often requires analysis in terms of a sequence of instances that are chained together. (sort > finding outliers)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.1 The Big Picture",
    "text": "2.1 The Big Picture\n\nWhat?\n\nDatasets\n\nData Types\n\nItems\nAttributes\nLinks\nPositions\nGrids\n\nData and Dataset Types\n\nTables\nNetworks & Trees\nFields\nGeometry\nClusters, Sets, Lists\n\nDataset Availability\n\nStatic\nDynamic\n\n\nAttributes\n\nAttribute Types\n\nCategorical\nOrdered\n\nOrdinal\nQuantitative\n\n\nOrdering Direction\n\nSequential\nDiverging\nCyclic"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-do-data-semantics-and-types-matter",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-do-data-semantics-and-types-matter",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.2 Why Do Data Semantics and Types Matter?",
    "text": "2.2 Why Do Data Semantics and Types Matter?\nMany aspects of vis design are driven by the kind of data that you have at your disposal.\nsemantics: the real-world meaning of the data.\ntype: the structural or mathematical interpretation of the data."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#data-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#data-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.3 Data Types",
    "text": "2.3 Data Types\nFive basic data types discussed in this book:\n\nItems\n\nIndividual entity that is discrete (such as a row in a simple table or a node in a network)\n\nAttributes\n\nSome specific property that can be measured, observed, or logged\n\nLinks\n\nA relationship between items, typically within a network\n\nPositions\n\nspatial data\n\nGrids"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#dataset-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#dataset-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.4 Dataset Types",
    "text": "2.4 Dataset Types\ndataset: any collection of information that is the target of analysis\nDataset types:\n\ntables\n\nitems\nattributes\n\nnetworks\n\nitems (nodes)\nlinks\nattributes\n\nfields\n\ngrids\npositions\nattributes\n\ngeometry\n\nitems\npositions\n\nclusters, sets and lists\n\nitems\n\n\n\n2.4.1 Tables\nflat table: each row represents and item of data, each column is an attribute of the dataset\ncell: fully specific by the combination of a row and a column (item and attribute) and contains a value for that pair.\nmultidimensional table: more complex structure for indexing into a cell, with multiple keys\n\n\n2.4.2 Networks and Trees\nnetworks: well suited for specifying that there is some kind of relationship (link) between two or more items (nodes)\nA synonym for networks is graphs.\nA synonym for node is vertex.\nA synonym for link is edge.\n\n2.4.2.1 Trees\nTrees: networks with hierarchical structure. Each child node has only one parent node pointing to it.\n\n\n\n2.4.3 Fields\nContains attribute values associated with cells. Each cell in a field contains measurements or calculations from a continuous domain.\nsampling: how frequently to take the measurements (of continuous data).\ninterpolation: how to show values in between the sampled points in a way that does not mislead. Interpolating appropriately between the measurements allows you to reconstruct a new view of the data from an arbitrary viewpoint that’s faithful to what you measured.\ndiscrete: data where a finite number of individual items exist where interpolation between them is not a meaningful concept.\nTechnically all data stored within a computer is discrete rather than continuous; however, the interesting question is whether the underlying semantics of the bits that are stored represents samples of a continuous phenomenon or intrinsically discrete data.\n\n2.4.3.1 Spatial Fields\nCell structure of the field is based on sapling at spatial positions.\nA synonym for nonspatial data is abstract data.\nscientific visualization (scivis): concerned with situations where spatial position is given with the dataset. A central concern in scivis is handling continuous data appropriately within the mathematical framework of signal processing.\ninformation visualization (infovis): concerned with situations where the use of space in a visual encoding is chosen by the designer. A central concern of infovis is determining whether the chosen idiom is suitable for the combination of data and task, leading to the use of methods from human-computer interaction and design.\n\n\n2.4.3.2 Grid Types\nWhen a field contains data created by sampling at completely regular intervals, the calls form a uniform grid.\ngrid geometry: location in space.\ngrid topology: how each cell connects with its neighboring cells.\nrectilinear grid: supports nonuniform sampling, allowing efficient storage of information that has high complexity in some areas and low complexity in others, at the cost of storing some information about the geometric location of each row.\nstructured grid: allows curvilinear shapes, where the geometric location of each cell needs to be specified.\nunstructured grid: provides complete flexibility, but the topological information about how cells connect to each other must be stored explicitly in addition to their spatial positions.\n\n\n\n2.4.4 Geometry\nSpecifies information about the shape of items with explicit spatial positions. Geometry datasets do not necessarily have attributes.\nGeometric data is sometimes shown alone, particularly when shape understanding is the primary task. In other cases, it is the backdrop against which additional information is overlaid.\n\n\n2.4.5 Other Combinations\nset: unordered group of items.\nlist: a group of items with a specified ordering.\ncluster: a grouping based on attribute similarity.\npath: an ordered set of segments formed by links connecting nodes.\ncompound network: a network with an associated tree (all the nodes in the network are the leaves of the tree, and interior nodes in the tree provide a hierarchical structure for the nodes that is different from network links between them).\ndata abstraction: describing the what part of an analysis instance that pertains to data.\n\n\n2.4.6 Dataset Availability\nstatic file (offline): the entire dataset is available all at once.\ndynamic streams (online): the dataset information trickles in over the course of the vis session."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#attribute-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#attribute-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.5 Attribute Types",
    "text": "2.5 Attribute Types\nThe major distinction is between categorical versus ordered.\nOrdered type contains further differentiation between ordinal versus quantitative.\nOrdered data might range sequentially from a minimum to a maximum value, or it might diverge in both directions from a zero point in the middle of a range, or the values may wrap around in a cycle.\nAttributes may have a hierarchical structure.\n\n2.5.1 Categorical\nDoes not have implicit ordering, but if often has hierarchical structure.\nA synonym for categorical is nominal.\nAny arbitrary external ordering can be imposed upon categorical data but these orderings are not implicit in the attribute itself.\n\n\n2.5.2 Ordered: Ordinal and Quantitative\nordered data: does have an implicit ordering.\nordinal data: we cannot do full-fledged arithmetic with, but there is a well defined ordering (shirt sizes, rankings).\nquantitative data: a subset of ordered data. A measurement of magnitude that supports arithmetic comparison (height, weight, temperature, stock price, etc). Both integers and real numbers are quantitative data.\n\n2.5.2.1 Sequential versus Diverging\nsequential: a homogeneous range from a minimum to a maximum value.\ndiverging: two sequences pointing in opposite directions that meet at a common zero point.\n\n\n2.5.2.2 Cyclic\ncyclic: where the values wrap around back to a starting point rather than continuing to increase indefinitely.\n\n\n\n2.5.3 Hierarchical Attributes\nThe attribute of time can be aggregated hierarchically from days up to weeks, months and years.\nThe geographic attribute of a postal code can be aggregated up to the level of cities or states or entire countries."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#semantics",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#semantics",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.6 Semantics",
    "text": "2.6 Semantics\nKnowing the type of an attribute does not tell us about its semantics.\n\n2.6.1 Key versus Value Semantics\nkey attribute: acts as an index that is used to look up value attributes.\nA synonym for key attribute is independent attribute or dimension.\nA synonym for value attribute is dependent attribute or measure.\n\n2.6.1.1 Flat Tables\nflat table: has only one key, where each item corresponds to a row in the table and any number of value attributes. Key may be categorical or ordinal attributes but quantitative attributes are typically unsuitable as keys because there is nothing to prevent them from having the same values for multiple items.\n\n\n2.6.1.2 Multidimensional Tables\nwhere multiple keys are required to look up an item. The combination of all keys must be unique for each item, even though an individual key attribute may contain duplicates.\n\n\n2.6.1.3 Fields\nIn spatial fields, spatial position acts as a quantitative key.\nmultivariate structure of fields depends on the number of value attributes.\nmultidimensional structure of fields depends on the number of keys.\na scalar field has one attribute per cell.\na vector field has two or more attributes per cell.\na tensor field has many attributes per cell.\n\n\n2.6.1.4 Scalar Fields\nare univariate, with a single attribute at each point in space.\n\n\n2.6.1.5 Vector Fields\nare multivariate with a list of multiple attributes at each point. The dimensionality of the field determines the number of components in the direction vector.\n\n\n2.6.1.6 Tensor Fields\nhave an array of attributes at each point, representing a more complex multivariate mathematical structure than the list of numbers in a vector. The full information at each point in a tensor field cannot be represented by just an arrow and would require a more complex shape such as an ellipsoid.\n\n\n2.6.1.7 Field Semantics\nCategorization of spatial fields requires knowledge of the attribute semantics and cannot be determined from type information alone.\n\n\n\n2.6.2 Temporal Semantics\ntemporal attribute: any kind of information that relates to time.\nTemporal analysis tasks often involve finding or verifying periodicity either at a predetermined scale or at some scale not known in advance.\nA temporal key attribute is usually considered to have a quantitative type, although it’s possible to consider it as ordinal data if the duration between events is not interesting.\n\n2.6.2.1 Time-Varying Data\nwhen time is one of the key attributes, as opposed to when the temporal attribute is a value rather than a key.\nThe question of whether time has key or value semantics requires external knowledge about the nature of the dataset and cannot be made purely from type information.\ntime-series dataset: an ordered sequence of time-value pairs. A special case of tables where time is the key.\ndynamic can mean a dataset has time-varying semantics or a dataset has stream type."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-1",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-1",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.1 The Big Picture",
    "text": "3.1 The Big Picture\n\nDiscovery may involve generating or verifying a hypothesis\nSearch can be classified according to whether the identity and location of targets are known or not\n\nboth are known with lookup\nthe target is known but its location is not for locate\nthe location is known but the target is not for browse\nneither the target nor the location are known for explore\n\nQueries can have three scopes:\n\nidentify one target\ncompare some targets\nsummarize all targets\n\nTargets for all kinds of data are finding trends and outliers\nFor one attribute, the target can be:\n\none value,\nthe extremes of minimum and maximum values or\nthe distribution of all values across the entire attribute\n\nFor multiple attributes the target can be:\n\ndependencies\ncorrelations or\nsimilarities between them"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze-tasks-abstractly",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze-tasks-abstractly",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.2 Why Analyze Tasks Abstractly?",
    "text": "3.2 Why Analyze Tasks Abstractly?\nTransforming task descriptions from domain-specific language into abstract form allows you to reason about similarities and differences between them.\nIf you don’t to this kind of translation then everything just appears to be different. The apparent difference is misleading: there are lots of similarities in what people want to do once you strip away the surface language differences.\nThe analysis framework has verbs describing actions and nouns describing targets.\nIt is often useful to consider only one of the user’s goals at a time, in order to more easily consider the question of how a particular idiom supports that goal. To describe complex activities, you can specify a chained sequence of tasks, where the output of one becomes the input to the next.\nTask abstraction can and should guide the data abstraction.\n\n3.3 Who: Designer or User\nOn the specific side, tools are narrow: the designer has built many choices into the design of the tool itself in a way that the user cannot override.\nOn the general side, tools are flexible and users have many choices to make.\nSpecialized vis tools are designed for specific contexts with a narrow range of data configurations, especially those created through a problem-driven process.\n\n\n3.4 Actions\nThree levels of actions that define user goals:\n\nhow the vis is being used to analyze (consume or produce data)\nwhat kind of search is involved (whether target and location are known)\nwhat kind query (identify one target, compare targets, or summarize all targets)\n\n\n3.4.1 Analyze\nTwo possible goals of people who want to analyze data: consume or actively produce new information.\n\nConsume information that has already been generated as data stored in a format amenable to computation\n\nDiscover something new\nPresent something that the user already understands\nEnjoy a vis to indulge their casual interests in a topic\n\n\n\n\n3.4.1.1 Discover\nUsing a vis to find new knowledge that was not previously known, by the serendipitous observation of unexpected phenomena or motivated by existing theories, models, hypotheses or hunches.\ngenerate a new hypothesis: finding completely new things\nverify or disconfirm an existing hypothesis.\nThe discover goal is often discussed as the classic motivation for sophisticated interactive idioms, because the vis designer doesn’t know in advance what the user will need to see.\n(discover = explore, present = explain).\nWhy the vis is being used doesn’t dictate how the vis idiom is designed to achieve those goals.\n\n\n3.4.1.2 Present\nThe use of vis for the succinct communication of information, for telling a story with data, or guiding an audience through a series of cognitive operations.\nThe crucial point about the present goal is that vis is being used by somebody to communicate something specific and already understood to an audience. The knowledge communicated is already known to the presenter in advance. The output of a discover session becomes the input to a present session.\nThe decision about why is separable from how the idiom is designed: presentation can be supported through a wide variety of idiom design choices.\n\n\n3.4.1.3 Enjoy\nCasual encounters with vis.\nA vis tool may have been intended by the designer for the goal of discovery with a particular audience, but it might be used for pure enjoyment by a different group of people.\n\n\n\n3.4.2 Produce\nThe intent of the user is to generate new material.\nThere are three kinds of produce goals:\n\nannotate\nrecord\nderive\n\n\n3.4.2.1 Annotate\nthe addition of graphical or textual annotations associated with one or more preexisting visualization elements, typically as a manual action by the user. Annotation for data items could be thought of as a new attribute for them.\n\n\n3.4.2.2 Record\nSaves or captures visualization elements as persistent artifacts (screenshots, lists of bookmarked elements or locations, parameter settings, interaction logs, or annotations). An annotation made by a user can subsequently be recorded.\n\n\n3.4.2.3 Derive\nProduce new data elements based on existing data elements. There is a strong relationship between the form of the data (the attribute and dataset types) and what kinds of vis idioms are effective at displaying it.\nDon’t just draw what you’re given; decide what the right thing to show is, create it with a series of transformations from the original dataset, and draw that.\nA synonym for derive is transform.\nderived attributes extend the dataset beyond the original set of attributes that it contains.\n\n\n\n3.4.3 Search\nThe classification of search into four alternatives is broken down according to whether the identity and location of the search target is already known. The verb find is often used as a synonym in descriptions of search tasks, implying a successful outcome.\n\n3.4.3.1 Lookup\nUsers already know both what they’re looking for and where it is.\n\n\n3.4.3.2 Locate\nTo find a known target at an unknown location.\n\n\n3.4.3.3 Browse\nWhen users don’t know exactly what they’re looking for but they do have a location in mind of where to look for it.\n\n\n3.4.3.4 Explore\nWhen users don’t know what they’re looking for and are not even sure of the location.\n\n\n\n3.4.4 Query\nOnce a target or set of targets for a search has been found, a low-level user goal is to query these targets at one of three scopes: identify (single target), compare (multiple targets) or summarize (all targets).\n\n3.4.4.1 Identify\nIf a search returns known targets either by lookup or locate then identify returns their characteristics.\nIf a search returns targets matching particular characteristics either by browse or explore, then identify returns specific references.\n\n\n3.4.4.2\nComparison tasks are typically more difficult than identify tasks and require more sophisticated idioms to support the user.\n\n\n3.4.4.3 Summarize\nA synonym for summarize is overview: to provide a comprehensive view of everything (verb) and a summary display of everything (noun)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#targets",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#targets",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.5 Targets",
    "text": "3.5 Targets\nTarget: some aspect of the data that is of interest to the user.\nTargets are nouns whereas actions are verbs.\nThree high-level targets are very broadly relevant for all kinds of data:\n\na trend: high-level characterization of a pattern in the data. (A synonym for trend is pattern)\noutliers: data that don’t fit the trend, synonyms for outliers are anomalies, novelties, deviants and surprises.\nfeatures: definition dependent on the task, any particular structures of interest\n\nThe lowest-level target for an attribute is to find an individual value. Another target is to find the extremes (min/max across a range). Another target is the distribution of all values for an attribute.\nSome targets encompass the scope of multiple attributes:\n\ndependency: the values for the first attribute directly depend on those of the second.\ncorrelation: a tendency for the values of the second attribute to be tied to those of the first.\nsimilarity: a quantitative measurement calculated on all values of two attributes, allowing attributes to be ranked with respect to how similar, or different, they are from each other.\n\nNetwork targets:\n\ntopology: the structure of interconnections in a network.\npath: of one or more links that connects two nodes.\nshape: of spatial data."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#how-a-preview",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#how-a-preview",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.6 How: A Preview",
    "text": "3.6 How: A Preview\n\nEncode\n\nArrange\n\nExpress\nSeparate\nOrder\nAlign\nUse (spatial data)\n\nMap\n\nColor\nSize, Angle, Curvature, …\nShape\nMotion\n\n\nManipulate\n\nChange\nSelect\nNavigate\n\nFacet\n\nJuxtapose\nPartition\nSuperimpose\n\nReduce\n\nFilter\nAggregate\nEmbed\n\n\nThe rest of this book defines, describes and discusses these choices in depth.\nThe Strahler number is a measure of node importance. Very central nodes have large Strahler numbers, whereas peripheral nodes have low values."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-2",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-2",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.1 The Big Picture",
    "text": "4.1 The Big Picture\nFour nested levels of design:\n\nDomain situation\n\nTask and data abstraction\n\nVisual encoding and interaction idiom\n\nAlgorithm"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-validate",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-validate",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.2 Why Validate?",
    "text": "4.2 Why Validate?\nThe vis design space is huge, and most designs are ineffective."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#four-levels-of-design",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#four-levels-of-design",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.3 Four Levels of Design",
    "text": "4.3 Four Levels of Design\n\nDomain situation: where you consider the details of a particular application domain for vis\n\nWhy-why abstraction level (Data-task): where you map those domain-specific problems and data into forms that are independent of the domain\n\nHow level (visual encoding/interaction idiom): specify the approach to visual encoding and interaction\n\nAlgorithm level: instantiate idioms computationally\n\n\n\n\nThe four levels are nested, the output from an upstream level above is input to the downstream level below. A block is the outcome of the design process at that level. Choosing the wrong block at an upstream level inevitable cascades to all downstream levels.\nVis design is usually a highly iterative refinement process, where a better understanding of the blocks at one level will feed back and forward into refining the blocks at the other levels.\n\n4.3.1 Domain Situation\ndomain situation: a group of target users, their domain interest, their questions, and their data.\ndomain: a particular field of interest of the target users of a vis tool.\nSituation blocks are identified.\nThe outcome of the design process is an understanding that the designer reaches about the needs of the user. The outcome of identifying a situation block is a detailed set of questions asked about or actions carried out by the target users, about a possible heterogeneous collection of data that’s also understood in detail.\nMethods include: interviews, observations, or careful research about target users within a specific domain.\nWorking closely with a specific target audience to iteratively refine a design is called user-centered design or human-centered design.\nWhat users say they do when reflecting on their past behavior gives you an incomplete picture compared with what they actually do if you observe them.\n\n\n4.3.2 Task and Data Abstraction\nAbstracting into the domain-independent vocabulary allows you to realize how domain situation blocks that are described using very different language might have similar reasons why the user needs the vis tool and what data it shows.\nTask blocks are identified by the designer as being suitable for a particular domain situation block, just as the situation blocks themselves are identified at the level above.\nAbstract data blocks are designed.\nThe data abstraction level requires you to consider whether and how the same dataset provided by a user should be transformed into another form.\nYour goal is to determine which data type would support a visual representation of it that addresses the user’s problem.\nExplicitly considering the choices made in abstracting from domain-specific to generic tasks and data can be very useful in the vis design process.\n\n\n4.3.3 Visual Encoding and Interaction Idiom\nidiom: a distinct way to create and manipulate the visual representation of the abstract data block that you chose at the previous level, guided by the abstract tasks that you also identified at that level.\nthe visual encoding idiom controls exactly what users see.\nthe interaction idiom controls how users change what they see.\nIdiom blocks are designed.\nThe nested model emphasizes identifying task abstractions and deciding on data abstractions in the previous level exactly so that you can use them to rule out many of the options as being a bad match for the goals of the users. You should make decisions about good and bad matches based on understanding human abilities, especially in terms of visual perception and memory.\n\n\n4.3.4 Algorithm\nalgorithm: a detailed procedure that allows a computer to automatically carry out the desired goal.\nAlgorithm blocks are designed."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#angles-of-attack",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#angles-of-attack",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.4 Angles of Attack",
    "text": "4.4 Angles of Attack\nWith problem-driven work, you start at the top domain situation level and work your way down through abstraction, idiom, and algorithm decisions.\nIn technique-driven work, you work at one of the bottom two levels, idiom or algorithm design, where your goal is to invent new idioms that better support existing abstractions, or new algorithms that better support existing idioms."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#threats-the-validity",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#threats-the-validity",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.5 Threats the Validity",
    "text": "4.5 Threats the Validity\nthreats to validity: different fundamental reasons why you might have made the wrong choices.\n\nWrong problem: You (designer) misunderstood their (target users) needs.\nWrong abstraction: You’re showing them the wrong thing.\nWrong idiom: The way you show it doesn’t work.\nWrong algorithm: Your code is too slow."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#validation-approaches",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#validation-approaches",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.6 Validation Approaches",
    "text": "4.6 Validation Approaches\n\n4.6.1 Domain Validation\nThe primary threat is that the problem is mischaracterized; the target users do not in fact have these problems (that the designer asserts would benefit from vis tool support).\nfield study: where the investigator observes how people act in real-world settings, rather than by bringing them into a laboratory setting. Field studies for domain situation assessment often involve gathering qualitative data through semi-structured interviews.\nOne downstream form of validation is adoption rates of the vis tool.\n\n\n4.6.2 Abstraction Validation\nThe threat at this level is that the identified task abstraction blocks and designed data abstraction blocks do not solve the characterized problems of the target audience. The key aspect of validation against this threat is that the system must be tested by target users doing their own work, rather than doing an abstract task specified by the designers of the vis system.\n\n\n4.6.3 Idiom Validation\nThe threat at this level is that the chosen idioms are not effective at communicating the desired abstraction to the person using the system. One immediate validation approach is to carefully justify the design of the idiom with respect to known perceptual and cognitive principles.\nA downstream approach to validate against this threat is to carry out a lab study: a controlled experiment in a laboratory setting.\n\n\n4.6.4 Algorithm Validation\nThe primary threat at this level is that the algorithm is suboptimal in terms of time or memory performance, either to a theoretical minimum or in comparison with previously proposed algorithms.\nAn immediate form of validation is to analyze the computational complexity of the algorithm, using the standard approaches from the computer science literature.\nThe downstream form of validation is to measures the wall-clock time and memory performance of the implemented algorithm.\n\n\n4.6.5 Mismatches\nA common problem in weak vis projects is a mismatch between the level at which the benefit is claimed (for example, visual encoding idiom) and the validation methodologies chosen (for example, wall-clock timings of the algorithm)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-3",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-3",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.1 The Big Picture",
    "text": "5.1 The Big Picture\nMarks are basic geometric elements that depict items or links, and channels control their appearance. Channels that perceptually convey magnitude information are a good match for ordered data, and those that convey identity information are a good match for categorical data.\n\nMagnitude Channels: Ordered Attributes (Most effective to least):\n\nPosition on common scale\nPosition on unaligned scale\nLength (1D size)\nTilt/angle\nArea (2D size)\nDepth (3D position)\nColor luminance\nColor saturation\nCurvature\nVolume (3D size)\n\nIdentity Channels: Categorical Attributes\n\nSpatial Region\nColor hue\nMotion\nShape"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-marks-and-channels",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-marks-and-channels",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.2 Why Marks and Channels?",
    "text": "5.2 Why Marks and Channels?\nThe core of the design space of visual encodings can be described as an orthogonal combination of two aspects: graphical elements called marks and visual channels to control their appearance."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#defining-marks-and-channels",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#defining-marks-and-channels",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.3 Defining Marks and Channels",
    "text": "5.3 Defining Marks and Channels\nmark: a basic graphical element in an image\n\nPoints (0 dimensional)\nLines (1D)\nArea (2D)\nVolume (3D)\n\nchannel: is a way to control the appearance of marks, independent of the dimensionality of the geometric primative.\n\nPosition\n\nHorizontal\nVertical\nBoth\n\nShape\nSize\n\nLength\nArea\nVolume\n\nColor\nTilt (or Angle)\n\nA single quantitative attribute can be encoded with vertical spatial position. Bar charts show this and the horizontal spatial position channel for the categorical attribute.\nScatterplots encode two quantitative attributes using point marks and both vertial and horizontal spatial position. A third categorical attribute is encoded by adding color to the scatterplot. Adding the visual channel of size encodes a fourth quantitative attribute as well.\nHigher-dimensional mark types usually have built-in constraints (on size and shape) that arise from the way that they are defined. An area or line mark cannot be size or shape coded, but a point can.\n\n5.3.1 Channel Types"
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "",
    "text": "Side view of Jalen Hurts walking on the Eagles sideline with Kansas City Chiefs-colored confetti falling around him"
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "Background and Goals",
    "text": "Background and Goals\nIt’s been 5 years since I last explored NFL’s play-by-play data. It’s also been 5 years since my Eagles won the Super Bowl, which will be played in less than 24 hours from now. Go Birds.\nIt’s been so long since I’ve blogged that fastpages, the blogging library I use, has been deprecated.\nI have thoroughly enjoyed some of the statistical analyses put forth by fans of the NFL this year. My favorite analyst is Deniz Selman, a fellow Eagles fan who makes these beautiful data presentations.\nI also appreciate Deniz’ critique of analysis-without-context that often negates the brilliance of Jalen Hurts:\n\n\nAs I’ve been trying to say all year, EPA/dropback is not nearly as valuable a metric when the offense lets the QB decide whether it’s a “dropback” or not during the play by reading the defense, and that QB is the absolute best at making that decision. #FlyEaglesFly\n\n— Deniz Selman (@denizselman33) February 11, 2023\n\n\nMy second favorite analyst is Ben Baldwin, AKA Computer Cowboy especially his 4th down analysis realtime during games.\nThere has been an onslaught of statistical advances in the NFL since I last explored play-by-play data and I’m excited to learn as much as I can. In particular, I’d like to get a hang of the metrics EPA (Expected Points Added) and DVOA (Defense-adjusted Value Over Average), which may not necessarily intersect with my play-by-play analysis (I believe Football Outsiders is the proprietor of that formula).\nI’d also like to use this project to practice more advanced SQL queries than I’m used to. Given the complexity of the play-by-play dataset (by team, down, field position, etc.) I’m hoping I can get those reps in.\nLastly, I’d like to explore data presentation with these statistics using R, python, Adobe Illustrator and Photoshop. I’ve been inspired by simple, elegant graphics like those made by Peter Gorman in Barely Maps and bold, picturesque statistics posted by PFF on twitter:\n\n\nThe most clutch pass rushers face off in the Super Bowl pic.twitter.com/o50lV9Bkgk\n\n— PFF (@PFF) February 12, 2023\n\n\nI’ll work on this project in this post throughout this year–and maybe beyond if it fuels me with enough material–or it’ll fork off into something entirely new or different.\nI’ll start off by next exploring the schema of the play-by-play dataset."
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#documenting-the-nfl-play-by-play-dataset-fields",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#documenting-the-nfl-play-by-play-dataset-fields",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "Documenting the NFL Play-by-Play Dataset Fields",
    "text": "Documenting the NFL Play-by-Play Dataset Fields\nIn this section, I describe the fields in the 2022 NFL Play-by-Play Dataset. Not all of the fields are intuitive or immediately useful, so not all 372 column descriptions will be listed.\n\nimport pandas as pd\nimport numpy as np\n\n\n# load the data\nfpath = \"../../../nfl_pbp_data/play_by_play_2022.csv\"\npbp_2022 = pd.read_csv(fpath, low_memory=False)\n\npbp_2022.head()\n\n\n\n\n\n  \n    \n      \n      play_id\n      game_id\n      old_game_id\n      home_team\n      away_team\n      season_type\n      week\n      posteam\n      posteam_type\n      defteam\n      ...\n      out_of_bounds\n      home_opening_kickoff\n      qb_epa\n      xyac_epa\n      xyac_mean_yardage\n      xyac_median_yardage\n      xyac_success\n      xyac_fd\n      xpass\n      pass_oe\n    \n  \n  \n    \n      0\n      1\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NaN\n      NaN\n      NaN\n      ...\n      0\n      1\n      0.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      43\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.443521\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      68\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      1.468819\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.440373\n      -44.037291\n    \n    \n      3\n      89\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.492192\n      0.727261\n      6.988125\n      6.0\n      0.60693\n      0.227598\n      0.389904\n      61.009598\n    \n    \n      4\n      115\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.325931\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.443575\n      -44.357494\n    \n  \n\n5 rows × 372 columns\n\n\n\nThe 2022 NFL Play-by-Play dataset has 50147 rows (plays) and 372 columns.\n\npbp_2022.shape\n\n(50147, 372)\n\n\nplay_id is an identifier for each play in each game. It not a unique identifier as there are many duplicates. There are 4597 unique play_id values in this dataset.\n\nlen(pbp_2022.play_id.unique())\n\n4597\n\n\ngame_id is an identifier for each game in the dataset in the format of {year}_{week}_{away_team}_{home_team}. There are 284 unique games in this dataset.\n\nlen(pbp_2022.game_id.unique()), pbp_2022.game_id[1]\n\n(284, '2022_01_BAL_NYJ')\n\n\nThere are 32 unique home_teams and away_teams.\n\nlen(pbp_2022.home_team.unique()), len(pbp_2022.away_team.unique())\n\n(32, 32)\n\n\nThere are two season_type values: 'REG' for regular season and 'POST' for postseason.\n\npbp_2022.season_type.unique()\n\narray(['REG', 'POST'], dtype=object)\n\n\nThere are 22 week values: - 18 regular season weeks (17 games + 1 bye) - 4 postseason weeks - Wild Card Weekend - Divisional Playoffs - Conference Championships - Super Bowl\n\npbp_2022.week.unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22])\n\n\nI believe posteam stands for the team that has possession of the ball. There are 32 unique teams that can have possession of the ball in a game, and in some cases the posteam is nan.\n\nlen(pbp_2022.posteam.unique()), pbp_2022.posteam.unique()\n\n(33,\n array([nan, 'NYJ', 'BAL', 'BUF', 'LA', 'CAR', 'CLE', 'SEA', 'DEN', 'MIN',\n        'GB', 'IND', 'HOU', 'JAX', 'WAS', 'KC', 'ARI', 'LAC', 'LV', 'NE',\n        'MIA', 'ATL', 'NO', 'NYG', 'TEN', 'DET', 'PHI', 'PIT', 'CIN',\n        'CHI', 'SF', 'DAL', 'TB'], dtype=object))\n\n\nposteam_type has values 'home', 'away' and nan.\n\nlen(pbp_2022.posteam_type.unique()), pbp_2022.posteam_type.unique()\n\n(3, array([nan, 'home', 'away'], dtype=object))\n\n\ndefteam lists any of the 32 teams on defense on a given play. It can also have the value nan.\n\nlen(pbp_2022.defteam.unique()), pbp_2022.defteam.unique()\n\n(33,\n array([nan, 'BAL', 'NYJ', 'LA', 'BUF', 'CLE', 'CAR', 'DEN', 'SEA', 'GB',\n        'MIN', 'HOU', 'IND', 'WAS', 'JAX', 'ARI', 'KC', 'LV', 'LAC', 'MIA',\n        'NE', 'NO', 'ATL', 'TEN', 'NYG', 'PHI', 'DET', 'CIN', 'PIT', 'SF',\n        'CHI', 'TB', 'DAL'], dtype=object))\n\n\nside_of_field can be nan, any of the 32 team abbreviations, or 50 (midfield).\n\nlen(pbp_2022.side_of_field.unique()), pbp_2022.side_of_field.unique()\n\n(34,\n array([nan, 'BAL', 'NYJ', 'LA', 'BUF', '50', 'CLE', 'CAR', 'DEN', 'SEA',\n        'GB', 'MIN', 'HOU', 'IND', 'WAS', 'JAX', 'ARI', 'KC', 'LV', 'LAC',\n        'MIA', 'NE', 'NO', 'ATL', 'TEN', 'NYG', 'PHI', 'DET', 'CIN', 'PIT',\n        'SF', 'CHI', 'TB', 'DAL'], dtype=object))\n\n\nyardline_100 can be nan or between 1 and 99.\n\nlen(pbp_2022.yardline_100.unique()), np.nanmin(pbp_2022.yardline_100), np.nanmax(pbp_2022.yardline_100)\n\n(100, 1.0, 99.0)\n\n\nThere are 61 game_date values.\n\nlen(pbp_2022.game_date.unique()), pbp_2022.game_date[0]\n\n(61, '2022-09-11')\n\n\nquarter_seconds_remaining is between 0 and 900 (15 minutes).\n\npbp_2022.quarter_seconds_remaining.min(), pbp_2022.quarter_seconds_remaining.max()\n\n(0, 900)\n\n\nhalf_seconds_remaining is between 0 and 1800 (30 minutes).\n\npbp_2022.half_seconds_remaining.min(), pbp_2022.half_seconds_remaining.max()\n\n(0, 1800)\n\n\ngame_seconds_remaining is between 0 and 3600 (60 minutes).\n\npbp_2022.game_seconds_remaining.min(), pbp_2022.game_seconds_remaining.max()\n\n(0, 3600)\n\n\ngame_half is either Half1 (first half), Half2 (second half), or Overtime.\n\npbp_2022.game_half.unique()\n\narray(['Half1', 'Half2', 'Overtime'], dtype=object)\n\n\nquarter_end is either 1 (True) or 0 (False).\n\npbp_2022.quarter_end.unique(), pbp_2022.query('quarter_end == 1').desc[41]\n\n(array([0, 1]), 'END QUARTER 1')\n\n\ndrive is the current number of drives in the game (including both teams) as well as nan values.\n\npbp_2022.drive.unique()\n\narray([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n       26., 27., 28., 29., 30., 31., 32., 33., 34., 35.])\n\n\nsp teams seems to indicate whether the play involves the Special Teams unit, either 1 (True) or 0 (False).\n\npbp_2022.sp.unique(), pbp_2022.query('sp == 1').desc[32]\n\n(array([0, 1]),\n '(3:19) 9-J.Tucker 24 yard field goal is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\nquarter indicates the current quarter of the play. quarter == 5 represents Overtime.\n\npbp_2022.qtr.unique()\n\narray([1, 2, 3, 4, 5])\n\n\ndown represents the current down of the play (nan, 1st, 2nd, 3rd or 4th).\n\npbp_2022.down.unique()\n\narray([nan,  1.,  2.,  3.,  4.])\n\n\ngoal_to_go indicates whether this play is 1st & Goal, 2nd & Goal, 3rd & Goal or 4th & Goal, either 1 (True) or 0 (False).\n\npbp_2022.goal_to_go.unique()\n\narray([0, 1])\n\n\ntime is the minutes:seconds formatted time left in the current quarter.\n\npbp_2022.head().time.unique()\n\narray(['15:00', '14:56', '14:29', '14:25'], dtype=object)\n\n\nyrdln is a formatted string of team abbreviation and yard number.\n\npbp_2022.yrdln.unique()\n\narray(['BAL 35', 'NYJ 22', 'NYJ 41', ..., 'NYJ 3', 'CIN 6', 'MIN 12'],\n      dtype=object)\n\n\nydstogo is the number of yards before the next first down.\n\npbp_2022.ydstogo.unique()\n\narray([ 0, 10,  5, 15,  6,  2,  1, 12,  9, 19, 11,  3,  8,  4, 16, 17,  7,\n       20, 14, 18, 13, 22, 26, 24, 21, 25, 23, 28, 30, 27, 31, 38, 36, 29,\n       34, 35, 32, 33])\n\n\nydsnet is the net yards (yards gained - yards lost) of the current drive.\n\npbp_2022.ydsnet.unique()\n\narray([ nan,  14.,  21.,   7.,   1.,  15.,   9.,  16.,  44.,  18.,  62.,\n        48.,   3.,  11.,   4.,  88.,  75.,  23.,  43.,  -2.,  38.,   0.,\n        45.,  60.,  13.,   6.,  -1.,  58.,  25.,  89.,  59.,  19.,  66.,\n        29.,  -4.,  24.,   2.,  12.,  42.,  78.,  52.,  57.,  64.,  35.,\n        -3.,  70.,  77.,  72.,  50.,  37.,  31.,  -6.,  32.,  -5.,  20.,\n        79.,  74.,  34.,  65.,   8.,  47.,   5.,  69.,  53.,  33.,  76.,\n        80., -16.,  71.,  68.,  55.,  27.,  90.,  86.,  17.,  30.,  67.,\n        63.,  73.,  61., -13.,  92.,  40.,  22.,  -7.,  39.,  41.,  28.,\n        82.,  49.,  10.,  36.,  46.,  84.,  54., -23., -11.,  83.,  26.,\n        94.,  87., -10.,  85.,  51., -14.,  56.,  -8.,  81.,  -9.,  93.,\n       -12., -15., -17.,  91.,  99.,  98., -19.,  96.,  95.,  97., -20.,\n       -25.])\n\n\ndesc is a narrative description of the current play.\n\npbp_2022.head().desc[1]\n\n'9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).'\n\n\nplay_type is either nan or one of 9 different play types, including no_play.\n\nlen(pbp_2022.play_type.unique()), pbp_2022.play_type.unique()\n\n(10,\n array([nan, 'kickoff', 'run', 'pass', 'punt', 'no_play', 'field_goal',\n        'extra_point', 'qb_kneel', 'qb_spike'], dtype=object))\n\n\nyards_gained is the number of yards gained (positive) or lost (negative) on the current play. It does not capture yards gained or lost due to a penalty.\n\npbp_2022.head().yards_gained, pbp_2022.yards_gained.min()\n\n(0     NaN\n 1     0.0\n 2    19.0\n 3     0.0\n 4     5.0\n Name: yards_gained, dtype: float64,\n -26.0)\n\n\nshotgun indicates whether the quarterback was in shotgun position, either 1 (True) or 0 (False).\n\npbp_2022.shotgun.unique(), pbp_2022.query('shotgun == 1').desc[3]\n\n(array([0, 1]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nno_huddle indicates whether the team huddled before the snap, either 1 (True) or 0 (False).\n\npbp_2022.no_huddle.unique(), pbp_2022.query('no_huddle == 1').desc[3]\n\n(array([0, 1]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nqb_dropback indicates whether the quarterback drops back on the play, either 1 (True), 0 (False) or nan.\n\npbp_2022.qb_dropback.unique(), pbp_2022.query('qb_dropback == 1').desc[3]\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nqb_kneel indicates whether the quarterback kneels on the play, either 1 (True) or 0 (False).\n\npbp_2022.qb_kneel.unique(), pbp_2022.query('qb_kneel == 1').desc[176]\n\n(array([0, 1]), '(:59) 8-L.Jackson kneels to NYJ 43 for -1 yards.')\n\n\nqb_spike indicates whether the quarterback spikes the ball on the play, either 1 (True) or 0 (False).\n\npbp_2022.qb_spike.unique(), pbp_2022.query('qb_spike == 1').desc[520]\n\n(array([0, 1]),\n '(:29) (No Huddle) 7-J.Brissett spiked the ball to stop the clock.')\n\n\nqb_scramble indicates whether the quarterback scrambles on the play, either 1 (True) or 0 (False). It looks like a scramble is not the same as a designed quarterback run, so I’ll dig deeper into this before using this field in analyses.\n\npbp_2022.qb_scramble.unique()\n\narray([0, 1])\n\n\npass_length is either nan, 'short' or 'deep'. I’ll first understand what distance (in yards) corresponds to these designations before I use this field in analyses.\n\npbp_2022.pass_length.unique()\n\narray([nan, 'short', 'deep'], dtype=object)\n\n\npass_location is either nan, 'left', 'right', or 'middle'.\n\npbp_2022.pass_location.unique()\n\narray([nan, 'left', 'right', 'middle'], dtype=object)\n\n\nair_yards is the number of yards a quarterback’s pass traveled in the air. It can be positive, zero or negative.\n\npbp_2022.air_yards.unique()\n\narray([ nan,   0.,  -4.,   3.,   2.,  16.,  11.,   5.,  21.,  14.,  -1.,\n         1.,   7.,   6.,  15.,  -3.,   8.,  10.,  50.,  27.,  25.,  -5.,\n        31.,  -6.,  17.,  51.,  13.,   4.,  12.,  36.,   9.,  32.,  18.,\n        22.,  -2.,  23.,  45.,  40.,  52.,  -7.,  26.,  29.,  20.,  47.,\n        24.,  30.,  28.,  37.,  39.,  -8.,  19.,  41.,  38., -12.,  42.,\n       -10.,  46.,  35.,  33.,  -9.,  34.,  44.,  43.,  53.,  57.,  48.,\n        49.,  54.,  58.,  56.,  59.,  55.,  61., -18., -54., -13.,  62.,\n        65., -20., -16.])\n\n\nyards_after_catch is the number of yards the receiver gains or loses after catching the ball.\n\npbp_2022.yards_after_catch.unique()\n\narray([ nan,   8.,   1.,   6.,   0.,   3.,   5.,   4.,  12.,   9.,  10.,\n        -4.,  18.,   7.,  15.,   2.,  11.,  13.,  -1.,  29.,  30.,  27.,\n        28.,  16.,  26.,  24.,  25.,  -5.,  41.,  14.,  22.,  19.,  17.,\n        21.,  32.,  20.,  -2.,  35.,  -3.,  51.,  66.,  38.,  46.,  23.,\n        31.,  37.,  68.,  -6.,  33.,  52.,  75.,  34.,  71.,  44.,  61.,\n        60.,  58.,  48.,  50.,  53.,  39.,  62.,  47.,  -7.,  42.,  40.,\n        36.,  49.,  70.,  45.,  65.,  43.,  74., -10.,  -9.])\n\n\nrun_location is either nan, 'left', 'right', or 'middle'.\n\npbp_2022.run_location.unique()\n\narray([nan, 'left', 'right', 'middle'], dtype=object)\n\n\nrun_gap represents which offensive line gap the runner ran through. It is either nan, 'end', 'tackle' or 'guard'. I’ll have to dig a bit deeper (look at some video corresponding to the run plays) to understand if 'guard' represents the A (gap between center and guard) or B gap (gap between guard and tackle), if 'tackle' represents the B or C gap (gap between tackle and end), and if 'end' represents the C or D (gap outside the end) gap.\n\npbp_2022.run_gap.unique()\n\narray([nan, 'end', 'tackle', 'guard'], dtype=object)\n\n\nfield_goal_result is either nan, 'made', 'missed', or 'blocked'.\n\npbp_2022.field_goal_result.unique()\n\narray([nan, 'made', 'missed', 'blocked'], dtype=object)\n\n\nkick_distance is the distance of the kick in yards for the following play_type values: 'punt', 'field_goal', 'extra_point', and 'kickoff'. Looking through the data, not all 'kickoff's have a kick_distance value.\n\npbp_2022.kick_distance.unique(), pbp_2022.query('kick_distance.notnull()').play_type.unique()\n\n(array([nan, 45., 40., 48., 24., 50., 56., 41., 33., 20., 49., 43.,  7.,\n        36., 57., 25., 39., 60., 62., 61., 44., 46., 58., 26., 34., 64.,\n        30., 47., 54., 28., 53., 38., 29., 70., 37., 27., 52., 42., 63.,\n        51., 23., 55., 59., 69., 66., 14., 32., 35.,  0., 31., 67., 74.,\n        19., 10., 22., 12.,  8.,  5., -1., 73., 65.,  3., 21.,  9., 16.,\n        15., 13., 18., 17.,  6., 77., 68., 11., 71., 79.]),\n array(['punt', 'field_goal', 'extra_point', 'kickoff'], dtype=object))\n\n\nextra_point_result is either nan, 'good', 'failed' or 'blocked'.\n\npbp_2022.extra_point_result.unique()\n\narray([nan, 'good', 'failed', 'blocked'], dtype=object)\n\n\ntwo_point_conv_result, the result of a two-point conversion is either nan, 'failure' or 'success'.\n\npbp_2022.two_point_conv_result.unique()\n\narray([nan, 'failure', 'success'], dtype=object)\n\n\nhome_timeouts_remaining is the number of timeouts the home team has left. It is either 3, 2, 1, or 0.\n\npbp_2022.home_timeouts_remaining.unique()\n\narray([3, 2, 1, 0])\n\n\naway_timeouts_remaining is the number of timeouts the away team has left. It is either 3, 2, 1, or 0.\n\npbp_2022.away_timeouts_remaining.unique()\n\narray([3, 2, 1, 0])\n\n\ntimeout indicates if a team calls a timeout, either 1 (True) or 0 (False).\n\npbp_2022.timeout.unique(), pbp_2022.query('timeout == 1').desc[13]\n\n(array([nan,  0.,  1.]), 'Timeout #1 by BAL at 09:56.')\n\n\ntimeout_team indicates which team called the timeout, and has 33 unique values—1 nan and 32 team abbreviations.\n\n(pbp_2022.timeout_team.unique(), \npbp_2022.query('timeout == 1').desc[13], \npbp_2022.query('timeout == 1').timeout_team[13])\n\n(array([nan, 'BAL', 'NYJ', 'LA', 'BUF', 'CLE', 'CAR', 'DEN', 'SEA', 'GB',\n        'MIN', 'IND', 'HOU', 'WAS', 'JAX', 'KC', 'ARI', 'LAC', 'LV', 'NE',\n        'MIA', 'ATL', 'NO', 'NYG', 'TEN', 'PHI', 'DET', 'PIT', 'CIN', 'SF',\n        'CHI', 'DAL', 'TB'], dtype=object),\n 'Timeout #1 by BAL at 09:56.',\n 'BAL')\n\n\ntd_team indicates which team scored the touchdown. It is nan or one of 32 team abbreviations.\n\n(pbp_2022.td_team.unique(),\npbp_2022.query('td_team.notnull()').td_team[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, 'BAL', 'NYJ', 'BUF', 'LA', 'CLE', 'CAR', 'SEA', 'DEN', 'MIN',\n        'GB', 'HOU', 'IND', 'WAS', 'JAX', 'KC', 'ARI', 'LAC', 'LV', 'MIA',\n        'NE', 'NO', 'ATL', 'TEN', 'NYG', 'DET', 'PHI', 'PIT', 'CIN', 'SF',\n        'CHI', 'TB', 'DAL'], dtype=object),\n 'BAL',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\ntd_player_name indicates which player scored the touchdown. It is nan or one of 416 players who scored a touchdown in the 2022 season.\n\n(pbp_2022.td_player_name.unique()[:5],\nlen(pbp_2022.td_player_name.unique()),\npbp_2022.query('td_team.notnull()').td_player_name[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, 'D.Duvernay', 'R.Bateman', 'T.Conklin', 'G.Davis'],\n       dtype=object),\n 417,\n 'D.Duvernay',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\ntd_player_id indicates the id of the player who scored the touchdown. There are 422 unique player IDs. Later on, I’ll look into why there are 5 fewer player IDs than player names.\n\n(pbp_2022.td_player_id.unique()[:5],\nlen(pbp_2022.td_player_id.unique()),\npbp_2022.query('td_team.notnull()').td_player_name[68],\n pbp_2022.query('td_team.notnull()').td_player_id[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, '00-0036331', '00-0036550', '00-0034270', '00-0036196'],\n       dtype=object),\n 423,\n 'D.Duvernay',\n '00-0036331',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\nposteam_timeouts_remaining is the number of timeouts remaining for the team with ball possession. It can be nan, 3, 2, 1, or 0.\n\npbp_2022.posteam_timeouts_remaining.unique()\n\narray([nan,  3.,  2.,  0.,  1.])\n\n\ndefteam_timeouts_remaining is the number of timeouts remaining for the team on defense. It can be nan, 3, 2, 1, or 0.\n\npbp_2022.defteam_timeouts_remaining.unique()\n\narray([nan,  3.,  2.,  1.,  0.])\n\n\ntotal_home_score is the total number of points scored by the home team.\n\npbp_2022.total_home_score.unique()[:5]\n\narray([0, 3, 9, 6, 7])\n\n\ntotal_away_score is the total number of points scored by the away team.\n\npbp_2022.total_away_score.unique()[:5]\n\narray([ 0,  3,  9, 10, 16])\n\n\nposteam_score is the total number of points scored by the team with ball possession on the current play.\n\npbp_2022.posteam_score.unique()[:5]\n\narray([nan,  0.,  3.,  9., 10.])\n\n\ndefteam_score is the total number of points scored by the team on defense on the current play.\n\npbp_2022.defteam_score.unique()[:5]\n\narray([nan,  0.,  3., 10., 17.])\n\n\nscore_differential is the difference between posteam_score and defteam_score.\n\npbp_2022.score_differential.unique()[:5]\n\narray([nan,  0., -3.,  3.,  9.])\n\n\npunt_blocked indicates if the punt was blocked. It is either nan, 1 (True) or 0 (False).\n\npbp_2022.punt_blocked.unique(),pbp_2022.query('punt_blocked == 1').desc[3236]\n\n(array([nan,  0.,  1.]),\n '(5:06) 11-R.Dixon punt is BLOCKED by 44-T.Andersen, Center-42-M.Orzech, RECOVERED by ATL-9-L.Carter at LA 26. 9-L.Carter for 26 yards, TOUCHDOWN.')\n\n\nfirst_down_rush indicates whether a first down was achieved by a rushing play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_rush.unique(), \n pbp_2022.query('first_down_rush == 1').desc[2],\n pbp_2022.query('first_down_rush == 1').play_type[2])\n\n(array([nan,  0.,  1.]),\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).',\n 'run',\n nan)\n\n\nfirst_down_pass indicates whether a first down was achieved by a passing play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_pass.unique(), \n pbp_2022.query('first_down_pass == 1').desc[26],\n pbp_2022.query('first_down_pass == 1').play_type[26])\n\n(array([nan,  0.,  1.]),\n '(6:01) 19-J.Flacco pass deep left to 8-E.Moore to NYJ 41 for 24 yards (32-M.Williams).',\n 'pass')\n\n\nfirst_down_penalty indicates whether a first down was achieved by a penalty. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_penalty.unique(), \n pbp_2022.query('first_down_penalty == 1').desc[17],\n pbp_2022.query('first_down_penalty == 1').play_type[17])\n\n(array([nan,  0.,  1.]),\n '(8:31) (Shotgun) 19-J.Flacco pass incomplete deep left to 8-E.Moore. PENALTY on BAL-44-M.Humphrey, Illegal Contact, 5 yards, enforced at NYJ 12 - No Play.',\n 'no_play')\n\n\nthird_down_converted indicates if the team with ball possession on third down got a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.third_down_converted.unique(), \n pbp_2022.query('third_down_converted == 1').down[9],\n pbp_2022.query('third_down_converted == 1').ydstogo[9],\n pbp_2022.query('third_down_converted == 1').desc[9],\npbp_2022.query('third_down_converted == 1').yards_gained[9])\n\n(array([nan,  0.,  1.]),\n 3.0,\n 2,\n '(12:41) (Shotgun) 8-L.Jackson right tackle to BAL 40 for 4 yards (57-C.Mosley, 3-J.Whitehead).',\n 4.0)\n\n\nthird_down_failed indicates if the team with ball possession on third down did not get a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.third_down_failed.unique(), \n pbp_2022.query('third_down_failed == 1').down[5],\n pbp_2022.query('third_down_failed == 1').ydstogo[5],\n pbp_2022.query('third_down_failed == 1').desc[5],\npbp_2022.query('third_down_failed == 1').yards_gained[5])\n\n(array([nan,  0.,  1.]),\n 3.0,\n 5,\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.',\n 0.0)\n\n\nfourth_down_converted indicates if the team with ball possession on fourth down got a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.fourth_down_converted.unique(), \n pbp_2022.query('fourth_down_converted == 1').down[145],\n pbp_2022.query('fourth_down_converted == 1').ydstogo[145],\n pbp_2022.query('fourth_down_converted == 1').desc[145],\npbp_2022.query('fourth_down_converted == 1').yards_gained[145])\n\n(array([nan,  0.,  1.]),\n 4.0,\n 1,\n '(7:32) 19-J.Flacco pass short right to 84-C.Davis to BAL 21 for 7 yards (23-K.Fuller).',\n 7.0)\n\n\nfourth_down_failed indicates if the team with ball possession on fourth down did not get a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.fourth_down_failed.unique(), \n pbp_2022.query('fourth_down_failed == 1').down[154],\n pbp_2022.query('fourth_down_failed == 1').ydstogo[154],\n pbp_2022.query('fourth_down_failed == 1').desc[154],\npbp_2022.query('fourth_down_failed == 1').yards_gained[154])\n\n(array([nan,  0.,  1.]),\n 4.0,\n 6,\n '(4:22) (Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.',\n 0.0)\n\n\nincomplete_pass indicates if the pass was incomplete. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.incomplete_pass.unique(),\n pbp_2022.query('incomplete_pass == 1').desc[3])\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\ntouchback indicates if the kickoff or punt either went past the back of the endzone or was fair-caught in the end zone.\n\n(pbp_2022.touchback.unique(),\n pbp_2022.query('touchback == 1').desc[33])\n\n(array([0, 1]),\n '9-J.Tucker kicks 65 yards from BAL 35 to end zone, Touchback.')\n\n\ninterception indicates if the quarterback’s pass was intercepted by a defender. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.interception.unique(),\n pbp_2022.query('interception == 1').desc[28])\n\n(array([nan,  0.,  1.]),\n '(5:07) (Shotgun) 19-J.Flacco pass short middle intended for 81-L.Cager INTERCEPTED by 32-M.Williams at NYJ 46. 32-M.Williams to NYJ 13 for 33 yards (19-J.Flacco).')\n\n\nfumble_forced indicates if a fumble was forced on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_forced.unique(),\n pbp_2022.query('fumble_forced == 1').desc[80])\n\n(array([nan,  0.,  1.]),\n '(1:16) (Shotgun) 19-J.Flacco pass short right to 83-T.Conklin to BAL 21 for 6 yards (32-M.Williams, 58-M.Pierce). FUMBLES (58-M.Pierce), touched at BAL 25, recovered by NYJ-17-G.Wilson at BAL 27. 17-G.Wilson to BAL 27 for no gain (14-K.Hamilton).')\n\n\nfumble_not_forced indicates if a fumble occurred on the play but was not forced by another player. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_not_forced.unique(),\n pbp_2022.query('fumble_not_forced == 1').desc[264])\n\n(array([nan,  0.,  1.]),\n '(13:46) (Shotgun) 9-M.Stafford to LA 11 for -6 yards. FUMBLES, and recovers at LA 11. 9-M.Stafford sacked at LA 10 for -7 yards (50-G.Rousseau).')\n\n\nfumble_out_of_bounds indicates if a fumbled ball went out of bounds. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_out_of_bounds.unique(),\n pbp_2022.query('fumble_out_of_bounds == 1').desc[1160])\n\n(array([nan,  0.,  1.]),\n '(:32) (Shotgun) 16-T.Lawrence pass short right to 1-T.Etienne to WAS 11 for 3 yards (22-D.Forrest). FUMBLES (22-D.Forrest), ball out of bounds at WAS 19. The Replay Official reviewed the pass completion ruling, and the play was Upheld. The ruling on the field stands.')\n\n\nsolo_tackle indicates if a player made a solo tackle on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.solo_tackle.unique(),\n pbp_2022.query('solo_tackle == 1').desc[1])\n\n(array([nan,  1.,  0.]),\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\nsafety indicates if a defensive player scored a safety on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.safety.unique(),\n pbp_2022.query('safety == 1').desc[3255])\n\n(array([nan,  0.,  1.]),\n '(:13) (Run formation) 19-B.Powell right end ran ob in End Zone for -26 yards, SAFETY (37-D.Alford).')\n\n\npenalty indicates if there was a penalty on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.penalty.unique(),\n pbp_2022.query('penalty == 1').desc[5])\n\n(array([nan,  0.,  1.]),\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\ntackled_for_loss indicates if a player was tackled for a loss of yards. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.tackled_for_loss.unique(),\n pbp_2022.query('tackled_for_loss == 1').desc[15])\n\n(array([nan,  0.,  1.]),\n '(9:49) 20-Br.Hall right end to NYJ 9 for -2 yards (92-J.Madubuike).')\n\n\nfumble_lost indicates if a player lost a fumble to the other team. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_lost.unique(),\n pbp_2022.query('fumble_lost == 1').desc[129])\n\n(array([nan,  0.,  1.]),\n '(14:13) (No Huddle, Shotgun) 19-J.Flacco pass short middle to 20-Br.Hall to BAL 16 for 6 yards (36-C.Clark). FUMBLES (36-C.Clark), RECOVERED by BAL-44-M.Humphrey at BAL 15.')\n\n\nqb_hit indicates if the quarterback was hit on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.qb_hit.unique(),\n pbp_2022.query('qb_hit == 1').desc[5])\n\n(array([nan,  0.,  1.]),\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\nrush_attempt indicates if the play was a rushing play. It is either nan, 1 (True), or 0 (False). A QB scramble is considered a rush attempt.\n\n(pbp_2022.rush_attempt.unique(),\n pbp_2022.query('rush_attempt == 1').desc[9],\n pbp_2022.query('rush_attempt == 1 and qb_scramble == 1').desc[89])\n\n(array([nan,  0.,  1.]),\n '(12:41) (Shotgun) 8-L.Jackson right tackle to BAL 40 for 4 yards (57-C.Mosley, 3-J.Whitehead).',\n '(14:15) (Shotgun) 8-L.Jackson scrambles left end ran ob at BAL 35 for 8 yards (3-J.Whitehead).')\n\n\npass_attempt indicates if the play was a passing play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.pass_attempt.unique(),\n pbp_2022.query('pass_attempt == 1').desc[3])\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nsack indicates if the quarterback was sacked on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.sack.unique(),\n pbp_2022.query('sack == 1').desc[54])\n\n(array([nan,  0.,  1.]),\n '(9:43) (Shotgun) 8-L.Jackson sacked ob at NYJ 49 for 0 yards (56-Qu.Williams).')\n\n\ntouchdown indicates if a player scored a touchdown on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.touchdown.unique(),\n pbp_2022.query('touchdown == 1').desc[68])\n\n(array([nan,  0.,  1.]),\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\npass_touchdown, rush_touchdown, and return_touchdown indicate if the touchdown was a result of a pass, rush or kickoff/punt/fumble/interception return play, respectively. Their value is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.pass_touchdown.unique(),\n pbp_2022.query('pass_touchdown == 1').desc[68])\n\n(array([nan,  0.,  1.]),\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\n\n(pbp_2022.rush_touchdown.unique(),\n pbp_2022.query('rush_touchdown == 1').desc[298])\n\n(array([nan,  0.,  1.]),\n '(13:34) (Shotgun) 17-J.Allen scrambles right end for 4 yards, TOUCHDOWN.')\n\n\n\n(pbp_2022.return_touchdown.unique(),\n pbp_2022.query('return_touchdown == 1').desc[1651],\n pbp_2022.query('return_touchdown == 1').desc[2197],\n pbp_2022.query('return_touchdown == 1').desc[47094])\n\n(array([nan,  0.,  1.]),\n '(7:40) (Shotgun) 10-M.Jones sacked at NE 6 for -9 yards (29-Br.Jones). FUMBLES (29-Br.Jones) [29-Br.Jones], RECOVERED by MIA-6-M.Ingram at NE 2. 6-M.Ingram for 2 yards, TOUCHDOWN.',\n '(6:36) (Shotgun) 16-J.Goff pass short left intended for 88-T.Hockenson INTERCEPTED by 24-J.Bradberry (43-K.White) [95-M.Tuipulotu] at DET 27. 24-J.Bradberry for 27 yards, TOUCHDOWN.',\n '6-N.Folk kicks 66 yards from NE 35 to BUF -1. 20-N.Hines for 101 yards, TOUCHDOWN.')\n\n\nThe following fields indicate if the play involved an attempt at an Extra Point, Two Point Conversion, Field Goal, Kickoff, or Punt, respectively:\n\nextra_point_attempt\ntwo_point_attempt\nfield_goal_attempt\nkickoff_attempt\npunt_attempt\n\nTheir value is either nan, 1 (True), or 0 (False).empt\n\n(pbp_2022.extra_point_attempt.unique(),\n pbp_2022.query('extra_point_attempt == 1').desc[69])\n\n(array([nan,  0.,  1.]),\n '9-J.Tucker extra point is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\n\n(pbp_2022.two_point_attempt.unique(),\n pbp_2022.query('two_point_attempt == 1').desc[1179])\n\n(array([nan,  0.,  1.]),\n 'TWO-POINT CONVERSION ATTEMPT. 16-T.Lawrence pass to 17-E.Engram is incomplete. ATTEMPT FAILS.')\n\n\n\n(pbp_2022.field_goal_attempt.unique(),\n pbp_2022.query('field_goal_attempt == 1').desc[32])\n\n(array([nan,  0.,  1.]),\n '(3:19) 9-J.Tucker 24 yard field goal is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\n\n(pbp_2022.kickoff_attempt.unique(),\n pbp_2022.query('kickoff_attempt == 1').desc[1])\n\n(array([nan,  1.,  0.]),\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.punt_attempt.unique(),\n pbp_2022.query('punt_attempt == 1').desc[6])\n\n(array([nan,  0.,  1.]),\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\nfumble indicates if a player fumbled the ball on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble.unique(),\n pbp_2022.query('fumble == 1').desc[80])\n\n(array([nan,  0.,  1.]),\n '(1:16) (Shotgun) 19-J.Flacco pass short right to 83-T.Conklin to BAL 21 for 6 yards (32-M.Williams, 58-M.Pierce). FUMBLES (58-M.Pierce), touched at BAL 25, recovered by NYJ-17-G.Wilson at BAL 27. 17-G.Wilson to BAL 27 for no gain (14-K.Hamilton).')\n\n\ncomplete_pass indicates if a player completed a pass on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.complete_pass.unique(),\n pbp_2022.query('complete_pass == 1').desc[7])\n\n(array([nan,  0.,  1.]),\n '(13:42) 8-L.Jackson pass short right to 7-R.Bateman pushed ob at BAL 32 for 4 yards (3-J.Whitehead).')\n\n\nassist_tackle indicates if a player assisted on the tackle on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.assist_tackle.unique(),\n pbp_2022.query('assist_tackle == 1').desc[2])\n\n(array([nan,  0.,  1.]),\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThe following fields provide the player_id (string), player_name (string) and yards gained (integer) for the passer, receiver or rusher on the play, respectively.\n\npasser_player_id\npasser_player_name\npassing_yards\nreceiver_player_id\nreceiver_player_name\nreceiving_yards\nrusher_player_id\nrusher_player_name\nrushing_yards\n\n\n(pbp_2022.passer_player_id[3],\n pbp_2022.passer_player_name[3],\n pbp_2022.passing_yards[3],\n pbp_2022.desc[3])\n\n('00-0026158',\n 'J.Flacco',\n nan,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\n(pbp_2022.receiver_player_id[3],\n pbp_2022.receiver_player_name[3],\n pbp_2022.receiving_yards[3],\n pbp_2022.desc[3])\n\n('00-0036924',\n 'Mi.Carter',\n nan,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\n(pbp_2022.rusher_player_id[2],\n pbp_2022.rusher_player_name[2],\n pbp_2022.rushing_yards[2],\n pbp_2022.desc[2])\n\n('00-0036924',\n 'Mi.Carter',\n 19.0,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThe following fields provide the player_id (string) and player_name (string) for players who intercepted the ball, returned a punt, returned a kickoff, punted the ball, kicked off the ball, recovered their own kickoff, or blocked the kick, respectively:\n\ninterception_player_id\ninterception_player_name\npunt_returner_player_id\npunt_returner_player_name\nkickoff_returner_player_name\nkickoff_returner_player_id\npunter_player_id\npunter_player_name\nkicker_player_name\nkicker_player_id\nown_kickoff_recovery_player_id\nown_kickoff_recovery_player_name\nblocked_player_id\nblocked_player_name\n\n\n(pbp_2022.interception_player_id[28],\n pbp_2022.interception_player_name[28],\n pbp_2022.desc[28])\n\n('00-0033894',\n 'M.Williams',\n '(5:07) (Shotgun) 19-J.Flacco pass short middle intended for 81-L.Cager INTERCEPTED by 32-M.Williams at NYJ 46. 32-M.Williams to NYJ 13 for 33 yards (19-J.Flacco).')\n\n\n\n(pbp_2022.punt_returner_player_id[6],\n pbp_2022.punt_returner_player_name[6],\n pbp_2022.desc[6])\n\n('00-0036331',\n 'D.Duvernay',\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\n\n(pbp_2022.kickoff_returner_player_id[1],\n pbp_2022.kickoff_returner_player_name[1],\n pbp_2022.desc[1])\n\n('00-0034419',\n 'B.Berrios',\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.punter_player_id[6],\n pbp_2022.punter_player_name[6],\n pbp_2022.desc[6])\n\n('00-0036313',\n 'B.Mann',\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\n\n(pbp_2022.kicker_player_id[1],\n pbp_2022.kicker_player_name[1],\n pbp_2022.desc[1])\n\n('00-0029597',\n 'J.Tucker',\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.own_kickoff_recovery_player_id[4964],\n pbp_2022.own_kickoff_recovery_player_name[4964],\n pbp_2022.desc[4964])\n\n('00-0033770',\n 'J.Hardee',\n '7-B.Mann kicks onside 12 yards from NYJ 35 to NYJ 47. RECOVERED by NYJ-34-J.Hardee.')\n\n\n\n(pbp_2022.blocked_player_id[1947],\n pbp_2022.blocked_player_name[1947],\n pbp_2022.desc[1947])\n\n('00-0036926',\n 'P.Turner',\n '(:02) 7-Y.Koo 63 yard field goal is BLOCKED (98-P.Turner), Center-48-L.McCullough, Holder-13-B.Pinion, recovered by ATL-13-B.Pinion at ATL 49. 13-B.Pinion to 50 for 1 yard (53-Z.Baun, 48-J.Gray).')\n\n\nThe following fields show player_id (string), player_name (string) or team (string) for a variety of defensive plays such as tackle for loss, quarterback hit, solo tackle, assist tackle and so on.\n\ntackle_for_loss_1_player_id\ntackle_for_loss_1_player_name\ntackle_for_loss_2_player_id\ntackle_for_loss_2_player_name\nqb_hit_1_player_id\nqb_hit_1_player_name\nqb_hit_2_player_id\nqb_hit_2_player_name\nsolo_tackle_1_team\nsolo_tackle_2_team\nsolo_tackle_1_player_id\nsolo_tackle_2_player_id\nsolo_tackle_1_player_name\nsolo_tackle_2_player_name\nassist_tackle_1_player_id\nassist_tackle_1_player_name\nassist_tackle_1_team\nassist_tackle_2_player_id\nassist_tackle_2_player_name\nassist_tackle_2_team\nassist_tackle_3_player_id\nassist_tackle_3_player_name\nassist_tackle_3_team\nassist_tackle_4_player_id\nassist_tackle_4_player_name\nassist_tackle_4_team\ntackle_with_assist\ntackle_with_assist_1_player_id\ntackle_with_assist_1_player_name\ntackle_with_assist_1_team\ntackle_with_assist_2_player_id\ntackle_with_assist_2_player_name\ntackle_with_assist_2_team\npass_defense_1_player_id\npass_defense_1_player_name\npass_defense_2_player_id\npass_defense_2_player_name\nsack_player_id\nsack_player_name\nhalf_sack_1_player_id\nhalf_sack_1_player_name\nhalf_sack_2_player_id\nhalf_sack_2_player_name\n\n\n(pbp_2022.tackled_for_loss[15],\n pbp_2022.tackle_for_loss_1_player_id[15],\n pbp_2022.tackle_for_loss_1_player_name[15],\n pbp_2022.desc[15])\n\n(1.0,\n '00-0036130',\n 'J.Madubuike',\n '(9:49) 20-Br.Hall right end to NYJ 9 for -2 yards (92-J.Madubuike).')\n\n\nThere are no plays where tackle_for_loss_2_player_id has a value.\n\npbp_2022.tackle_for_loss_2_player_id.unique()\n\narray([nan])\n\n\n\n(pbp_2022.qb_hit[5],\n pbp_2022.qb_hit_1_player_id[5],\n pbp_2022.qb_hit_1_player_name[5],\n pbp_2022.desc[5])\n\n(1.0,\n '00-0026190',\n 'C.Campbell',\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\n\n(pbp_2022.qb_hit[55],\n pbp_2022.qb_hit_1_player_id[55],\n pbp_2022.qb_hit_1_player_name[55],\n pbp_2022.qb_hit_2_player_id[55],\n pbp_2022.qb_hit_2_player_name[55],\n pbp_2022.desc[55])\n\n(1.0,\n '00-0034163',\n 'J.Johnson',\n '00-0034163',\n 'J.Martin',\n '(8:59) (Shotgun) 8-L.Jackson sacked at BAL 49 for -2 yards (sack split by 52-J.Johnson and 54-J.Martin).')\n\n\n\n(pbp_2022.solo_tackle[777],\n pbp_2022.solo_tackle_1_team[777],\n pbp_2022.solo_tackle_1_player_id[777],\n pbp_2022.solo_tackle_1_player_name[777],\n pbp_2022.solo_tackle_2_team[777],\n pbp_2022.solo_tackle_2_player_id[777],\n pbp_2022.solo_tackle_2_player_name[777],\n pbp_2022.desc[777])\n\n(1.0,\n 'MIN',\n '00-0032129',\n 'J.Hicks',\n 'GB',\n '00-0036631',\n 'R.Newman',\n '(12:21) 12-A.Rodgers sacked at GB 35 for -9 yards (58-J.Hicks). FUMBLES (58-J.Hicks) [58-J.Hicks], RECOVERED by MIN-94-D.Tomlinson at GB 33. 94-D.Tomlinson to GB 33 for no gain (70-R.Newman).')\n\n\n\n(pbp_2022.assist_tackle[2],\n pbp_2022.assist_tackle_1_team[2],\n pbp_2022.assist_tackle_1_player_id[2],\n pbp_2022.assist_tackle_1_player_name[2],\n pbp_2022.assist_tackle_2_team[2],\n pbp_2022.assist_tackle_2_player_id[2],\n pbp_2022.assist_tackle_2_player_name[2],\n pbp_2022.desc[2])\n\n(1.0,\n 'BAL',\n '00-0033894',\n 'M.Williams',\n 'BAL',\n '00-0033294',\n 'C.Clark',\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThere are no plays where assist_tackle_3_player_id or assist_tackle_4_player_id have a value.\n\npbp_2022.assist_tackle_3_player_id.unique(), pbp_2022.assist_tackle_4_player_id.unique()\n\n(array([nan]), array([nan]))\n\n\ntackle_with_assist is not the same as assist_tackle.\n\n(pbp_2022.tackle_with_assist[2],\n pbp_2022.tackle_with_assist_1_team[2],\n pbp_2022.tackle_with_assist_1_player_id[2],\n pbp_2022.tackle_with_assist_1_player_name[2],\n pbp_2022.tackle_with_assist_2_team[2],\n pbp_2022.tackle_with_assist_2_player_id[2],\n pbp_2022.tackle_with_assist_2_player_name[2],\n pbp_2022.desc[2])\n\n(0.0,\n nan,\n nan,\n nan,\n nan,\n nan,\n nan,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\n(pbp_2022.tackle_with_assist[22659],\n pbp_2022.tackle_with_assist_1_team[22659],\n pbp_2022.tackle_with_assist_1_player_id[22659],\n pbp_2022.tackle_with_assist_1_player_name[22659],\n pbp_2022.tackle_with_assist_2_team[22659],\n pbp_2022.tackle_with_assist_2_player_id[22659],\n pbp_2022.tackle_with_assist_2_player_name[22659],\n pbp_2022.desc[22659])\n\n(1.0,\n 'LAC',\n '00-0031040',\n 'K.Mack',\n 'ATL',\n '00-0035208',\n 'O.Zaccheaus',\n '(9:31) (No Huddle, Shotgun) 1-M.Mariota pass short left to 5-D.London to LAC 6 for 5 yards (52-K.Mack, 43-M.Davis). FUMBLES (52-K.Mack), RECOVERED by LAC-52-K.Mack at LAC 6. 52-K.Mack pushed ob at 50 for 44 yards (17-O.Zaccheaus, 5-D.London).')\n\n\nI’ll explore this more later before using these fields in analyses, but it seems like the assist_tackle fields provide information on players who assisted with the tackle, while tackle_with_assist lists information of the “main” player who was assisted on the tackle.\n\n(pbp_2022.assist_tackle[22659],\n pbp_2022.assist_tackle_1_team[22659],\n pbp_2022.assist_tackle_1_player_id[22659],\n pbp_2022.assist_tackle_1_player_name[22659],\n pbp_2022.assist_tackle_2_team[22659],\n pbp_2022.assist_tackle_2_player_id[22659],\n pbp_2022.assist_tackle_2_player_name[22659],\n pbp_2022.desc[22659])\n\n(1.0,\n 'LAC',\n '00-0033697',\n 'M.Davis',\n 'ATL',\n '00-0037238',\n 'D.London',\n '(9:31) (No Huddle, Shotgun) 1-M.Mariota pass short left to 5-D.London to LAC 6 for 5 yards (52-K.Mack, 43-M.Davis). FUMBLES (52-K.Mack), RECOVERED by LAC-52-K.Mack at LAC 6. 52-K.Mack pushed ob at 50 for 44 yards (17-O.Zaccheaus, 5-D.London).')\n\n\n\n(pbp_2022.pass_defense_1_player_id[1613],\n pbp_2022.pass_defense_1_player_name[1613],\n pbp_2022.pass_defense_2_player_id[1613],\n pbp_2022.pass_defense_2_player_name[1613],\n pbp_2022.desc[1613])\n\n('00-0033050',\n 'X.Howard',\n '00-0036998',\n 'J.Holland',\n '(10:05) (Shotgun) 10-M.Jones pass deep right intended for 1-D.Parker INTERCEPTED by 8-J.Holland (25-X.Howard) at MIA -3. 8-J.Holland to MIA 28 for 31 yards (76-I.Wynn).')\n\n\nThe following fields show player_id (string), player_name (string) or team (string) for a variety of fumble-related plays:\n\nforced_fumble_player_1_team\nforced_fumble_player_1_player_id\nforced_fumble_player_1_player_name\nforced_fumble_player_2_team\nforced_fumble_player_2_player_id\nforced_fumble_player_2_player_name\nfumbled_1_team\nfumbled_1_player_id\nfumbled_1_player_name\nfumbled_2_player_id\nfumbled_2_player_name\nfumbled_2_team\nfumble_recovery_1_team\nfumble_recovery_1_yards\nfumble_recovery_1_player_id\nfumble_recovery_1_player_name\nfumble_recovery_2_team\nfumble_recovery_2_yards\nfumble_recovery_2_player_id\nfumble_recovery_2_player_name\n\n\n(pbp_2022.fumble_forced[9041],\n pbp_2022.forced_fumble_player_1_team[9041],\n pbp_2022.forced_fumble_player_1_player_id[9041],\n pbp_2022.forced_fumble_player_1_player_name[9041],\n pbp_2022.forced_fumble_player_2_team[9041],\n pbp_2022.forced_fumble_player_2_player_id[9041],\n pbp_2022.forced_fumble_player_2_player_name[9041],\n pbp_2022.desc[9041])\n\n(1.0,\n 'NYG',\n '00-0033046',\n 'J.Ward',\n 'NYG',\n '00-0036167',\n 'T.Crowder',\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.fumbled_1_team[9041],\n pbp_2022.fumbled_1_player_id[9041],\n pbp_2022.fumbled_1_player_name[9041],\n pbp_2022.fumbled_2_team[9041],\n pbp_2022.fumbled_2_player_id[9041],\n pbp_2022.fumbled_2_player_name[9041],\n pbp_2022.desc[9041])\n\n('CHI',\n '00-0034279',\n 'E.St. Brown',\n 'CHI',\n '00-0036953',\n 'T.Ebner',\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.fumble_recovery_1_team[9041],\n pbp_2022.fumble_recovery_1_player_id[9041],\n pbp_2022.fumble_recovery_1_player_name[9041],\n pbp_2022.fumble_recovery_1_yards[9041],\n pbp_2022.fumble_recovery_2_team[9041],\n pbp_2022.fumble_recovery_2_player_id[9041],\n pbp_2022.fumble_recovery_2_player_name[9041],\n pbp_2022.fumble_recovery_2_yards[9041],\n pbp_2022.desc[9041])\n\n('CHI',\n '00-0036945',\n 'J.Fields',\n -3.0,\n 'CHI',\n '00-0033082',\n 'L.Patrick',\n -6.0,\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.sack[54],\n pbp_2022.sack_player_name[54],\n pbp_2022.sack_player_id[54],\n pbp_2022.desc[54])\n\n(1.0,\n 'Qu.Williams',\n '00-0035680',\n '(9:43) (Shotgun) 8-L.Jackson sacked ob at NYJ 49 for 0 yards (56-Qu.Williams).')\n\n\nWhen a sack is split, sack == 1 but sack_player_name and id are nan.\n\n(pbp_2022.sack[55],\n pbp_2022.sack_player_name[55],\n pbp_2022.sack_player_id[55],\n pbp_2022.half_sack_1_player_id[55],\n pbp_2022.half_sack_1_player_name[55],\n pbp_2022.half_sack_2_player_id[55],\n pbp_2022.half_sack_2_player_name[55],\n pbp_2022.desc[55])\n\n(1.0,\n nan,\n nan,\n '00-0034163',\n 'J.Johnson',\n '00-0034163',\n 'J.Martin',\n '(8:59) (Shotgun) 8-L.Jackson sacked at BAL 49 for -2 yards (sack split by 52-J.Johnson and 54-J.Martin).')\n\n\nreturn_team (string) and return_yards (integer) are the abbreviation and yardage of the team that returned the kickoff or punt. I’ll look into if fumble returns are included before I use this field for analyses.\n\n(pbp_2022.return_team[1], \n pbp_2022.return_yards[1],\n pbp_2022.desc[1])\n\n('NYJ',\n 25.0,\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\nThe following fields hold information about penalties.\n\npenalty_team (string)\npenalty_player_id (string)\npenalty_player_name (string)\npenalty_yards (integer)\npenalty_type (string)\n\n\n(pbp_2022.penalty[5],\n pbp_2022.penalty_team[5],\n pbp_2022.penalty_player_id[5],\n pbp_2022.penalty_player_name[5],\n pbp_2022.penalty_yards[5],\n pbp_2022.penalty_type[5],\n pbp_2022.desc[5])\n\n(1.0,\n 'NYJ',\n '00-0026158',\n 'J.Flacco',\n 10.0,\n 'Intentional Grounding',\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\n\npbp_2022.penalty_type.unique()\n\narray([nan, 'Intentional Grounding', 'Illegal Contact',\n       'Offensive Holding', 'Defensive Pass Interference',\n       'Defensive Holding', 'Offensive Pass Interference', 'False Start',\n       'Horse Collar Tackle', 'Defensive Too Many Men on Field',\n       'Taunting', 'Delay of Game', 'Roughing the Passer',\n       'Unsportsmanlike Conduct', 'Low Block', 'Illegal Formation',\n       'Ineligible Downfield Pass', 'Unnecessary Roughness',\n       'Neutral Zone Infraction', 'Running Into the Kicker',\n       'Illegal Shift', 'Defensive Offside', 'Illegal Use of Hands',\n       'Illegal Block Above the Waist', 'Offensive Too Many Men on Field',\n       'Encroachment', 'Disqualification', 'Ineligible Downfield Kick',\n       'Face Mask', 'Player Out of Bounds on Kick',\n       'Illegal Forward Pass', 'Chop Block', 'Delay of Kickoff',\n       'Tripping', 'Illegal Substitution', 'Offensive Offside',\n       'Illegal Blindside Block', 'Illegal Touch Pass',\n       'Offside on Free Kick', 'Roughing the Kicker',\n       'Fair Catch Interference', 'Leverage', 'Illegal Motion',\n       'Defensive Delay of Game', 'Illegal Bat', 'Illegal Touch Kick',\n       'Illegal Double-Team Block', 'Invalid Fair Catch Signal',\n       'Illegal Crackback', 'Illegal Kick/Kicking Loose Ball'],\n      dtype=object)\n\n\nreplay_or_challenge (1 for True and 0 for False) and replay_or_challenge_result (nan, 'upheld', or 'reversed') show information about whether a replay or challenge occurred on the play.\n\n(pbp_2022.replay_or_challenge[621],\n pbp_2022.replay_or_challenge_result[621],\n pbp_2022.desc[621])\n\n(1,\n 'upheld',\n '(7:42) (Shotgun) 25-M.Gordon right tackle to SEA 1 for no gain (6-Q.Diggs, 10-U.Nwosu). FUMBLES (6-Q.Diggs), RECOVERED by SEA-30-M.Jackson at SEA 2. 30-M.Jackson to SEA 10 for 8 yards (14-C.Sutton). The Replay Official reviewed the fumble ruling, and the play was Upheld. The ruling on the field stands.')\n\n\nsafety_player_name and safety_player_id have information about the player who caused the safety.\n\n(pbp_2022.safety[3255],\n pbp_2022.safety_player_name[3255],\n pbp_2022.safety_player_id[3255],\n pbp_2022.desc[3255])\n\n(1.0,\n 'D.Alford',\n '00-0037034',\n '(:13) (Run formation) 19-B.Powell right end ran ob in End Zone for -26 yards, SAFETY (37-D.Alford).')\n\n\nseries_result is the result of the offensive series.\n\npbp_2022.series_result.unique()\n\narray(['First down', 'Punt', 'Turnover', 'Field goal',\n       'Missed field goal', 'Touchdown', 'End of half',\n       'Turnover on downs', 'QB kneel', 'Opp touchdown', 'Safety', nan],\n      dtype=object)\n\n\nplay_type_nfl shows slightly different play type categories.\n\npbp_2022.play_type_nfl.unique()\n\narray(['GAME_START', 'KICK_OFF', 'RUSH', 'PASS', 'PUNT', 'TIMEOUT',\n       'PENALTY', 'FIELD_GOAL', 'END_QUARTER', 'SACK', 'XP_KICK',\n       'END_GAME', 'PAT2', nan, 'FREE_KICK'], dtype=object)\n\n\ndrive_play_count shows how many plays the drive had. I’ll look into it more before using it for analyses. It doesn’t always match the number of plays on the drive, or at least seems not to, so I need to understand how they calculate this value.\n\npbp_2022.drive_play_count.unique()\n\narray([nan,  4.,  6.,  5.,  3.,  8.,  1.,  9., 16., 11.,  2., 13.,  7.,\n       14., 10., 15., 12.,  0., 18., 19., 20., 17., 21.])\n\n\ndrive_time_of_possession is a formatted string of minutes:seconds the drive took.\n\npbp_2022.drive_time_of_possession.unique()[:5]\n\narray([nan, '1:18', '3:53', '2:44', '1:04'], dtype=object)\n\n\ndrive_first_downs is the number of first downs achieved on the drive.\n\npbp_2022.drive_first_downs.unique()[:5]\n\narray([nan,  1.,  0.,  3.,  2.])\n\n\ndrive_inside20 is either nan, 1 (True) or 0 (False) and indicates if a drive ended inside of the red zone (20 yards from the end zone).\n\npbp_2022.drive_inside20.unique()\n\narray([nan,  0.,  1.])\n\n\ndrive_ended_with_score indicates if a drive ended with the offensive team scoring. It is either nan, 1 (True) or 0 (False).\n\npbp_2022.drive_ended_with_score.unique()\n\narray([nan,  0.,  1.])\n\n\nI’ll have to look into it more before using it for analyses, but I believe drive_yards_penalized is the total number of offensive penalty yards on the drive.\n\npbp_2022.drive_yards_penalized.unique()[:5]\n\narray([ nan, -10.,   0.,   5.,  32.])\n\n\ndrive_play_id_started and drive_play_id_ended indicate the start and end play_id of the drive. Note that play_id are not consecutive and doesn’t start at 1.\n\n(pbp_2022.drive_play_id_started[1],\npbp_2022.drive_play_id_ended[1])\n\n(43.0, 172.0)\n\n\naway_score and home_score are the final scores of the away team and home team.\n\n(pbp_2022.away_team[1],\n pbp_2022.away_score[1],\n pbp_2022.home_team[1],\n pbp_2022.home_score[1])\n\n('BAL', 24, 'NYJ', 9)\n\n\nresult is the difference between the home and the away team (I think—will look into it more).\n\npbp_2022.result[1]\n\n-15\n\n\ntotal is the total number of points scored by both teams.\n\npbp_2022.total[1]\n\n33\n\n\ndiv_game indicates if the game is between teams in the same division. It is either 1 (True) or 0 (False).\n\npbp_2022.div_game.unique(), pbp_2022.div_game[1]\n\n(array([0, 1]), 0)\n\n\naway_coach and home_coach are the names of the away team and home team coaches, respectively.\n\npbp_2022.away_coach[1], pbp_2022.home_coach[1]\n\n('John Harbaugh', 'Robert Saleh')\n\n\nThe following fields give the name and jersey number of the passer, rusher or receiver on the play:\n\npasser\npasser_id\npasser_jersey_number\nrusher\nrusher_id\nrusher_jersey_number\nreceiver\nreceiver_id\nreceiver_jersey_number\n\n\n(pbp_2022.passer[3], \n pbp_2022.passer_id[3],\n pbp_2022.passer_jersey_number[3])\n\n('J.Flacco', '00-0026158', 19.0)\n\n\n\n(pbp_2022.rusher[2], \n pbp_2022.rusher_id[2],\n pbp_2022.rusher_jersey_number[2])\n\n('Mi.Carter', '00-0036924', 32.0)\n\n\n\n(pbp_2022.receiver[3], \n pbp_2022.receiver_id[3],\n pbp_2022.receiver_jersey_number[3])\n\n('Mi.Carter', '00-0036924', 32.0)\n\n\nThe following fields indicate if the play is a pass, rush, first down, or special teams, respectively. Their value is nan, 1 (True) or 0 (False):\n\npass\nrush\nfirst_down\nspecial\n\n\npbp_2022['pass'][3], pbp_2022.desc[3]\n\n(1,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\npbp_2022.rush[2], pbp_2022.desc[2]\n\n(1,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\npbp_2022.first_down[2], pbp_2022.desc[2]\n\n(1.0,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\npbp_2022.special[1], pbp_2022.desc[1]\n\n(1,\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "vishal bakshi",
    "section": "",
    "text": "welcome to my blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learning for Coders - Part 1\n\n\n\n\n\n\n\n\n\n\n\n\nThursday, July 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaturday, July 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Pandas - Notes and Exercises\n\n\n\n\n\n\n\ndata analysis\n\n\npython\n\n\n\n\nAn update on my progress in the book “Effective Pandas” by Matt Harrison.\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, July 10, 2023\n\n\n\n\n\n\n  \n\n\n\n\nExploring NFL Play-by-Play Data with SQL and Python\n\n\n\n\n\n\n\ndata analysis\n\n\nSQL\n\n\npython\n\n\n\n\nAn update on my analysis and visualization of NFL play-by-play data.\n\n\n\n\n\n\nVishal Bakshi\n\n\nThursday, July 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nVisualize U.S. Census Data with ArcGIS\n\n\n\n\n\n\n\nArcGIS\n\n\ndata analysis\n\n\n\n\nA tutorial to create a geodatabase, maps and layouts to visualize U.S. Census Data in ArcGIS Pro\n\n\n\n\n\n\nVishal Bakshi\n\n\nFriday, June 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRegression and Other Stories - Notes and Excerpts\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nSunday, June 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization Analysis & Design - Excerpts\n\n\n\n\n\n\n\n\n\n\n\n\nThursday, June 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTranscribing Sherlock into Spanish\n\n\n\n\n\n\n\nspanish\n\n\n\n\nAn update on my goal to transcribe BBC’s Sherlock into Spanish.\n\n\n\n\n\n\nVishal Bakshi\n\n\nTuesday, February 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMaking Chicken Wings\n\n\n\n\n\n\n\nfood\n\n\n\n\nAn update on my goal to make 10/10 chicken wings by the 2023 NFL season.\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Classification Models\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Bear Classifier\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 7:Test Time Augmentation\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 8: Collaborative Filter Deep Dive\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR Shiny Census App\n\n\n\n\n\n\n\nR\n\n\ndata analysis\n\n\nSQL\n\n\n\n\nAn explanation of my development process for a census data shiny app\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "my name is vishal bakshi. i’m a data analyst at the City of Portland. i haven’t blogged in a couple of years and recently have had the desire to do so again so here i am."
  },
  {
    "objectID": "about.html#things-i-like-consistently",
    "href": "about.html#things-i-like-consistently",
    "title": "About",
    "section": "things i like consistently",
    "text": "things i like consistently\nhip hop, yoga, lifting free weights, resistance bands, coding, writing, reading, cold weather, philadelphia eagles, la lakers."
  },
  {
    "objectID": "about.html#what-im-currently-reading",
    "href": "about.html#what-im-currently-reading",
    "title": "About",
    "section": "what i’m currently reading",
    "text": "what i’m currently reading\ncheck out my currently reading list on goodreads."
  },
  {
    "objectID": "about.html#what-im-working-on-consistently",
    "href": "about.html#what-im-working-on-consistently",
    "title": "About",
    "section": "what i’m working on consistently",
    "text": "what i’m working on consistently\n\nobserving my thoughts, emotions and behaviors.\nmaking and practicing hip hop music.\nlearning about how and why our society is structured the way it is.\ndata analysis/science/visualization."
  }
]