[
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "",
    "text": "In this notebook I’ll provide a summary of Microsoft Research’s Textbook Are All You Need paper. Here’s the abstract:\n\nWe introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of “textbook quality” data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.\n\nI’ll also review the information published in Textbooks Are All You Need II a technical report in which they introduce phi-1.5 models trained on additional data."
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#main-takeaways",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#main-takeaways",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Main Takeaways",
    "text": "Main Takeaways\n\nModel improvement is obtained with data quality (instead of model size, dataset size and amount of compute).\nHigh quality data means data that is diverse (wide range of concepts, skills, and scenarios; varying difficulty, complexity and style) and non-repetitive.\nFinetuning on 180M tokens led to the largest accuracy increase including for tasks that are not featured in the finetuning dataset.\n1.3B parameter models outperform larger models trained on larger datasets.\n350M parameter model performs decently well.\nTraining on textbook-like data might mean model stores/accesses knowledge more efficiently than if trained on web data."
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#data",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#data",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Data",
    "text": "Data\n\nphi-1\nphi-1 uses two different datasets, one for pretraining and one for finetuning:\n\nCodeTextbook (pretraining)\n\nThe Stack+ (6B tokens): subset of The Stack and StackOverflow, filtered using a LM-based classifier\nGPT-3.5 generated Python textbooks (<1B tokens)\n\nCodeExercises (finetuning)\n\nGPT-3.5 generated Python exercises and solutions (~180M tokens; function completion tasks based on natural language instructions)\n\n\n\n\nphi-1.5\n\nCodeTextbook (7B tokens)\n20B tokens of synthetically generated textbook-like data\n\nA couple of quotes from the paper about data:\n\n…our dataset consists almost exclusively of synthetically generated data\n\n\n…a robust and comprehensive dataset demands more than raw computational power: it requires intricate iterations, strategic topic selection, and a deep understanding of knowledge gaps to ensure quality and diversity of the data.\n\n\n\nphi-1.5-web, phi-1.5-web-only\n\n95B tokens of filtered web data\n\n88B from the Falcon refined web dataset.\n7B from The Stack and Stack Overflow.\n\nphi-1.5-web-only trained only on filtered web data\n\n80% NLP data sources.\n20% code datasets.\n\nphi-1.5-web trained on a mix of filtered data\n\n40%: a subset of filtered web data.\n20%: phi-1’s code data.\n40%: new synthetic NLP data.\n\n\n\nWe speculate that the creation of synthetic datasets will become, in the near future, an important technical skill and a central topic of research in AI."
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#architecture",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#architecture",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Architecture",
    "text": "Architecture\n\n\n\n\n\n\n\n\nDecoder-only Transformer (FlashAttention/MLP in parallel)\nphi-1/phi-1.5\nphi-1-small\n\n\n\n\nParameters\n1.3B\n350M\n\n\nLayers\n24\n20\n\n\nHidden dimension\n2048\n1024\n\n\nMLP inner dimension\n8192\n4096\n\n\nAttention head count\n32\n16\n\n\nAttention head dimension\n64\n64"
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#training",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#training",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Training",
    "text": "Training\n\nphi-1\n\n\n\n\nPre-training (phi-1-base)\nFine-tuning (phi-1)\n\n\n\n\nTime\n<4 days\n7 hours\n\n\nBatch size\n1024\n256\n\n\nMax learning rate\n1e-3\n1e-4\n\n\nWarmup\n750 steps\n50 steps\n\n\nWeight Decay\n0.1\n0.01\n\n\nCheckpoint\n24k steps/8 epochs/50B tokens\nUndisclosed\n\n\nTotal steps\n36000\n6000\n\n\n\n\nBoth phi-1.5 and phi-1.5-web are base models pre-trained on large natural language corpora. In particular we did not perform further instruction-based finetuning to align them with human instructions (emphasis mine).\n\nI was really hoping for more details about their 350M model experiments (I love it when small models perform decently) but they only provided the following:\n\n…phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1\n\n\n\nphi-1.5\n\nPretraining\n\nBatch size: 2048\nConstant learning rate: 2e-4\nWeight decay: 0.1\nTraining tokens: 50B (80% new synthetic data, 20% phi-1 data)"
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#limitations",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#limitations",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Limitations",
    "text": "Limitations\n\nphi-1\n\nThis model is python-specific so it won’t be as performant for other programming languages.\nLacks domain-specific knowledge (APIs, less common packages).\nLess robust to grammar/style variations (small changes in natural language instructions can affect performance).\nUnclear what type of scale in model or dataset size will overcome these limitations.\nGPT-4 should be used to generate synthetic data.\n\n\n\nphi-1.5\n\nNot immune to generating toxic content.\nMakes some intricate mistakes when explaining code."
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#benchmarks",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#benchmarks",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nphi-1\n\nHumanEval\n\nA dataset of 164 hand-written coding problems.\nEach problem includes a function signature, docstring, body and several unit tests (7.7 avg tests per problem)\n\nMBPP\n\n1000 crowd-sources Python programming problems.\nDesigned for entry-level programmers.\nEach problem has a task description, code solution and 3 automated test cases.\n\n\\(pass@k\\) metric\n\n\\(k\\) generated code samples per problem.\nproblem is “solved” if any sample passes the unit tests.\n\n\n\n\n\nFigure 2.1: Pass@1 accuracy on Human Eval for models and training datasets of various sizes.\n\n\nIn the figure above (Figure 2.1 in the paper) note the strong performance of the 350M parameter model trained on 26B tokens for 135 GPU hours. I would love to know more about that checkpoint.\n\n\n\nTable 2: HumanEval and 50 unconventional coding problem scores.\n\n\nIn the table above (Table 2 from the paper) the “Score” column is graded by GPT-4 on a scale of 0 to 10 while the HumanEval column is calculated with pass@1 accuracy.\n\n\n\nTable 1: HumanEval and MBPP Pass@1 accuracy for various model and dataset sizes\n\n\nI found Table 1 in the paper to be the most impressive framing of phi-1’s performance. It beats models that are hundreds of times larger (such as PaLM-Coder) trained on datasets thousands of times as large (such as StarCoder). As a reminder, MBPP consists of 1000 crowdsourced entry-level programming questions."
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#data-decontamination",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#data-decontamination",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Data Decontamination",
    "text": "Data Decontamination\nA standard contamination study will look for n-gram overlaps between the training and test sets to understand how “contaminated” the training set is with information from the test set. They only found four such cases in the paper, including one where the n-gram was the same but for a different context.\nThe authors used a “strong form” of data decontamination: embedding and syntax-based similarity Embedding similarity determines semantic similarity, while AST-based similarity determines how similar the underlying operations of the code are between two dataset items. After removing contaminated dataset items, the authors trained phi-1 on this “pruned dataset” and it performed better than StarCoder-Prompted (15.5B) for all AST-based match rate thresholds and similarity categories (similar, non-similar, total) except for one (see table 3 below).\n\n\n\nTable 3 showing results of phi-1 (better performing overall) and StarCoder-Prompted on different training datasets with varying amounts of similar test data removed\n\n\n\nphi-1.5\n\nThe authors used LM-Eval Harness on 5 common sense benchmarks\n5 standard language understanding tasks\n\nZero-shot accuracy LM-Eval Harness on PIQA, HellaSwag, and OpenbookQA.\n2-shot accuracy on MMLU.\nExact match score on SQUAD.\n\n3 reasoning ability benchmarks\n\nZero-shot pass@1 accuracy on GSM8K for math and HumanEval/MBPP for entry-level Python coding.\n\n1 benchmark for toxicity (ToxiGen)\n\n86 prompts, 34 evaluated as “fail” (bad), 47 as “pass” (good) and 4 as “did not understand”.\n\n\nHere are the results for phi-1.5 on these benchmarks, compared to other (larger) models:\n\n\n\nStandard language understanding tasks\n\n\n\n\n\nStandard language understanding tasks\n\n\n\n\n\nMath and coding tasks\n\n\n\n\n\nToxicity benchmark\n\n\nAs I noted in the screenshot from my slides—I was curious to see phi-1.5-web-only results for toxicity. I would assume it would score worse than the other models."
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#prompt-and-response-examples",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#prompt-and-response-examples",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Prompt and Response Examples",
    "text": "Prompt and Response Examples\nI’ve highlighted a few examples from the paper that show how the phi models behave. First, a comparison between phi-1, phi-1-base and phi-1-small responses to the same prompt asking them to code a problem involving multiple mathematical relationships. phi-1 successfully provides the right answer. phi-1-base (pretrained only) returns relevant code names and values but with absolutely no structure for the operations involved. phi-1-small gets about 80% of the solution right, with a couple of errors within the correct structure of the solution.\n\n\n\n\n\nIn the response below, phi-1.5 is able to take into consideration unconventional information (raining in the middle of July) and incorporate it into its story-telling response.\n\n\n\n\n\nIn the example below, phi-1.5 corretly generates a respone that aligns with the prompt but then continue on to generate unwanted text in the format of Exercise/Answer. I wonder if it’s following its textbook-like training data’s format.\n\n\n\n\n\nLastly, I tried phi-1.5’s code to check latency and found one error in its syntax (the use of decode instead of encode). Otherwise, the syntax matched the documentation example of Python’s subprocess module."
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#further-research",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#further-research",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Further Research",
    "text": "Further Research\n\nThe authors noted that developing high quality datasets is a central direction to improve NLP and related field. High quality data means data that is:\n\nbalanced and representative for model use cases.\ndiverse and non-repetitive (inject randomness and creativity into data generation process to achieve this).\ntaking into consideration ethical/social implications, accountability, transparency, and biases (in both models and data).\n\n\n\nThe open-sourcing of phi-1.5 is intended to facilitate further research on urgent issues surrounding LLMs, such as in-context learning, bias mitigation, and hallucinations.\n\n\nOur work indicates the feasibility of achieving high-level capabilities in smaller LLMs, potentially paving the way for more efficient and environmentally sustainable AI systems.\n\n\nFuture directions include expanding our synthetic dataset to cover a broader array of topics, and to fine-tune phi-1.5 for more specific tasks"
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#phi-2",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#phi-2",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "phi-2",
    "text": "phi-2\nSubsequent to this paper, Microsoft released phi-2 a 2.7B parameter language model.\n\nPhi-2 demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters.\n\n\nPhi-2 is a Transformer-based model with a next-word prediction objective, trained on 1.4T tokens from multiple passes on a mixture of Synthetic and Web datasets for NLP and coding\n\n\nThe training for Phi-2 took 14 days on 96 A100 GPUs. Phi-2 is a base model that has not undergone alignment through reinforcement learning from human feedback (RLHF), nor has it been instruct fine-tuned.\n\nFrom the HuggingFace model page:\n\nPhi-2 was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value).\n\nPhi-2 performs better than Phi-1.5 across 14 different benchmarks:\n\n\n\n\n\nSurprisingly (or perhaps not so surprisingly based on the fact that Phi-2 contains more internet data), Phi-2 performs worse than Phi-1.5 but better than Llama2-7B for toxicity:\n\n\n\n\n\nPhi-2 performs better than Llama-2-7B/13B and Mistral on various benchmarks and is better than Llama-2-70B for coding:"
  },
  {
    "objectID": "posts/2024-03-01-textbooks-are-all-you-need/index.html#final-thoughts",
    "href": "posts/2024-03-01-textbooks-are-all-you-need/index.html#final-thoughts",
    "title": "Paper Summary: Textbooks are All You Need I & II",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’m excited to continue using Phi-2 and look forward to finetuning it later this year. I have used it to classify sentiment for the financial_phrasebank dataset and have gotten comparable results to larger models (I’ll post a blog post on that project once it’s done). In general, I’m always excited to see “smaller” models perform well and I hope that thoughtful dataset curation can push the parameter size down even further, hopefully to the hundreds of millions (like the 350M phi-1-small) and still get decent results.\nI’ll be posting more paper summaries in the coming weeks. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "",
    "text": "In this notebook, I’ll work through the second “Further Research” exercise at the end of Chapter 4 of the Practical Deep Learning for Coders textbook:\n\nComplete all the steps in this chapter using the full MNIST datasets (for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You’ll need to do some of your own research to figure out how to overcome obstacles you’ll meet on the way."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#plan-of-attack",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#plan-of-attack",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nI’ll start by reviewing each step of the training loop covered in this chapter (for 3s and 7s) and identify what elements need to change, why, and an a brief outline of how, in order to accommodate for all 10 digits.\n\nLoad and Prep Data\nIn the chapter, we stacked tensor images of each digit to create n x 28 x 28 tensors (where n is the number of images in the training or validation folder) and then converted them to n x 784 tensors so that each pixel was in a one-dimensional row (corresponding to 784 parameters in the neural net in a one-dimensional row).\nTo handle all 10 digits, I’ll need to expand this logic without too much hard-coding—I don’t want to create 10 tensors individually (stacked_zeros, stacked_ones, …, stacked_tens) for training and validation data.\nInstead, I’ll use list comprehension. First, let’s look at how to access all the subfolders in the train and valid parent folders:\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\n\npath.ls()\n\n(#2) [Path('/root/.fastai/data/mnist_png/training'),Path('/root/.fastai/data/mnist_png/testing')]\n\n\n\nIndependent Variable: Images\nI can iterate through (path/'training').ls() to see the 10 digit subfolders containing training images for the digits:\n\n[path for path in (path/'training').ls()]\n\n[Path('/root/.fastai/data/mnist_png/training/8'),\n Path('/root/.fastai/data/mnist_png/training/6'),\n Path('/root/.fastai/data/mnist_png/training/3'),\n Path('/root/.fastai/data/mnist_png/training/1'),\n Path('/root/.fastai/data/mnist_png/training/9'),\n Path('/root/.fastai/data/mnist_png/training/2'),\n Path('/root/.fastai/data/mnist_png/training/0'),\n Path('/root/.fastai/data/mnist_png/training/4'),\n Path('/root/.fastai/data/mnist_png/training/7'),\n Path('/root/.fastai/data/mnist_png/training/5')]\n\n\nTaking it one layer deeper, I can nest a second list comprehension which collects individual file paths from each of the digit’s folders:\n\ntraining_files = [[file for file in path.ls()] for path in (path/'training').ls().sorted()]\n\nHere are the paths to the first 5 images in the first folder, which corresponds to the digit 0:\n\ntraining_files[0][:5]\n\n[Path('/root/.fastai/data/mnist_png/training/0/35012.png'),\n Path('/root/.fastai/data/mnist_png/training/0/2009.png'),\n Path('/root/.fastai/data/mnist_png/training/0/14472.png'),\n Path('/root/.fastai/data/mnist_png/training/0/7589.png'),\n Path('/root/.fastai/data/mnist_png/training/0/53401.png')]\n\n\nAnd the paths to the first 5 images in the second folder, which corresponds to the digit 1:\n\ntraining_files[1][:5]\n\n[Path('/root/.fastai/data/mnist_png/training/1/47434.png'),\n Path('/root/.fastai/data/mnist_png/training/1/27790.png'),\n Path('/root/.fastai/data/mnist_png/training/1/42000.png'),\n Path('/root/.fastai/data/mnist_png/training/1/15633.png'),\n Path('/root/.fastai/data/mnist_png/training/1/21958.png')]\n\n\nAnd so on for all 10 digits\n\ntraining_files[9][:5]\n\n[Path('/root/.fastai/data/mnist_png/training/9/57008.png'),\n Path('/root/.fastai/data/mnist_png/training/9/28984.png'),\n Path('/root/.fastai/data/mnist_png/training/9/36162.png'),\n Path('/root/.fastai/data/mnist_png/training/9/42013.png'),\n Path('/root/.fastai/data/mnist_png/training/9/18296.png')]\n\n\n\nlen(training_files)\n\n10\n\n\nI’ll illustrate the same for the validation set:\n\nvalidation_files = [[file for file in path.ls()] for path in (path/'testing').ls().sorted()]\n\n\nvalidation_files[0][:5]\n\n[Path('/root/.fastai/data/mnist_png/testing/0/9095.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/5990.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/7505.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/157.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/5838.png')]\n\n\n\nvalidation_files[9][:5]\n\n[Path('/root/.fastai/data/mnist_png/testing/9/2009.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/7298.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/5565.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/9483.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/5705.png')]\n\n\n\nlen(validation_files)\n\n10\n\n\nNext, I’ll flatten the list of training and validation files, convert each to a stacked tensor, convert the pixel values to floating point values, and divide by 255 so the pixel values are between 0 and 1. I referenced this Stack Overflow post for flattening a nested list.\nTo understand how it works, I find it easier to read left-to-right, broken up into two separate parts:\nfile for (sublist in training_files) for (file in sublist)\nIn pseudocode:\npopulate this list with each file in each sublist in `training_files`\n\ntraining_files = [file for sublist in training_files for file in sublist]\nvalidation_files = [file for sublist in validation_files for file in sublist]\n\nThe dataset’s Wikimedia page says that it has 60,000 training images and 10,000 testing images. This matches the counts here:\n\nlen(training_files), len(validation_files)\n\n(60000, 10000)\n\n\nI’ll open a couple of the files and make sure I can view the images as expected:\n\n# this should be an image of a handwritten zero\nshow_image(tensor(Image.open(training_files[0])));\n\n\n\n\n\n# this should be an image of a handwritten nine\nshow_image(tensor(Image.open(training_files[-1])));\n\n\n\n\n\n# this should be an image of a handwritten zero\nshow_image(tensor(Image.open(validation_files[0])));\n\n\n\n\n\n# this should be an image of a handwritten nine\nshow_image(tensor(Image.open(validation_files[-1])));\n\n\n\n\nLooks good! The images are as expected. I can now move on to creating stacked floating point tensors of the training and validation images:\n\ntrain_x = torch.stack([tensor(Image.open(o)) for o in training_files]).float()/255\ntrain_x.shape\n\ntorch.Size([60000, 28, 28])\n\n\n\nvalid_x = torch.stack([tensor(Image.open(o)) for o in validation_files]).float()/255\nvalid_x.shape\n\ntorch.Size([10000, 28, 28])\n\n\nI’ll view my data one more time before changing its shape:\n\n# this should be a zero\nshow_image(train_x[0]);\n\n\n\n\n\n# this should be a nine\nshow_image(train_x[-1]);\n\n\n\n\n\n# this should be a zero\nshow_image(valid_x[0]);\n\n\n\n\n\n# this should be a nine\nshow_image(valid_x[-1]);\n\n\n\n\nGreat! I’ll flatten the images so that they are 784 pixels long, instead of a 28 x 28 matrix.\n\ntrain_x = train_x.view(-1, 28*28)\ntrain_x.shape\n\ntorch.Size([60000, 784])\n\n\n\nvalid_x = valid_x.view(-1, 28*28)\nvalid_x.shape\n\ntorch.Size([10000, 784])\n\n\n\n\nDependent Variable: Labels\nNow that I have the x (independent) variable data prepared, I’ll do the same for the y (dependent) variable data—the labels for the images.\nI’ll reuse my training_files and validation_files lists as they already contain the paths to each image file, from which I’ll extract the label. path.parts splits the path into a tuple of its individual parts (split by “/”). The parent folder (the second-to-last part of the path) of the path is the label of the image.\n\ntraining_labels = [int(path.parts[-2]) for path in training_files]\ntraining_labels[0], training_labels[-1]\n\n(0, 9)\n\n\n\nvalidation_labels = [int(path.parts[-2]) for path in validation_files]\nvalidation_labels[0], validation_labels[-1]\n\n(0, 9)\n\n\n\ntrain_y = tensor(training_labels).unsqueeze(1)\ntrain_y.shape\n\ntorch.Size([60000, 1])\n\n\n\nvalid_y = tensor(validation_labels).unsqueeze(1)\nvalid_y.shape\n\ntorch.Size([10000, 1])\n\n\nExcellent! Now with the data in the right structure, I’ll create a DataLoaders object that will be fed to the learner during training:\n\ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([0]))\n\n\n\nvalid_dset = list(zip(valid_x, valid_y))\nx,y = valid_dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([0]))\n\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\nvalid_xb, valid_yb = first(valid_dl)\nvalid_xb.shape, valid_yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\nI combine the two DataLoaders into a single DataLoaders object:\n\ndls = DataLoaders(dl, valid_dl)\n\nAnd check that they contain the right amount of data:\n\nlen(dls.train.dataset), len(dls.valid.dataset)\n\n(60000, 10000)\n\n\nGreat! With my DataLoaders prepared, I can move on to other aspects of the training loop that will need to be modified to handle 10 digits instead of 2.\n\n\n\nCreate Our Model\nHere is the existing model that we’re using to classify two digits:\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\nIt has 784 inputs and 1 output. For 10 digits, I need to adjust th number of outputs to 10:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\nI assume that since we have more final activations, we would also need to increase the intermediate activations from 30 to a larger number, but I’ll keep it at 30 for now and then make improvements once I’ve actually got a successful training loop.\n\n\nCreate a Loss Function\nThis is the main change that will take place in our training loop: using a loss function that can handle 10 digits instead of 2. In the exercise prompt, they said that we would need to:\n\ndo some of your own research to figure out how to overcome obstacles you’ll meet on the way\n\nAnd I think this is probably the main obstacle to overcome. In the textbook chapter, when they trained the dataset using the built-in Learner, they passed it F.cross_entropy as the loss function:\nlearn = vision_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)\n\nIn the text, they introduce Cross-Entropy Loss in Chapter 5, so I’ll take a detour through that chapter’s relevant sections to inform me on how to create a loss function for a 10-digit classifier.\n\nCross-Entropy Loss\n\nWorks even when our dependent variable has more than two categories.\nResults in faster and more reliable training.\n\nAs is done in the book example, I’ll view one batch of our data:\n\nx,y = dls.one_batch()\ny[:10]\n\ntensor([[0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0]])\n\n\nCurrently, my data is not shuffled, so all the 0s are first, then all the 1s, 2s, …, and 9s. This doesn’t seem like a great way to train the model since it will learn only 1 digit’s features in each batch. I’ll recreate the DataLoaders and pass the parameter value shuffle=True:\n\ndl = DataLoader(dset, batch_size=256, shuffle=True)\nxb,yb = first(dl)\nyb[:10]\n\ntensor([[5],\n        [4],\n        [1],\n        [3],\n        [0],\n        [3],\n        [7],\n        [2],\n        [3],\n        [0]])\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\nvalid_xb, valid_yb = first(valid_dl)\nvalid_yb[:10]\n\ntensor([[4],\n        [7],\n        [9],\n        [6],\n        [1],\n        [8],\n        [9],\n        [5],\n        [3],\n        [0]])\n\n\n\ndls = DataLoaders(dl, valid_dl)\n\n\nlen(dls.train.dataset), len(dls.valid.dataset)\n\n(60000, 10000)\n\n\nNow, when I look at one batch, I can see a variety of labels:\n\nx,y = dls.one_batch()\ny[:10]\n\ntensor([[8],\n        [3],\n        [2],\n        [7],\n        [3],\n        [9],\n        [1],\n        [6],\n        [0],\n        [7]])\n\n\nThe output predictions of the model will contain 10 predictions (one for each digit) that add up to 1. According to Chapter 5, we need to use the softmax function to achieve a result like this.\n\nSoftmax\nAssume we have a scenario with 6 images and 2 possible categories (3 and 7):\n\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\nWe can’t just take the sigmoid of this directly since we want rows to add up to 1.0 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1).\n\nacts.sigmoid()\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\nIn the binary case, a single pair of activations simply indicates the relative confidence of the input being a 3 versus being a 7. The overall values, whether they are both high or both low, don’t matter—all that matters is which is higher and by how much.\nWe can take the difference between the neural net activations because that reflects how much more sure we are of the input being a 3 than a 7, and then take the sigmoid of that:\n\n(acts[:,0] - acts[:,1]).sigmoid()\n\ntensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661])\n\n\nThe second column (the probability of it being a 7) will then just be that value subtracted from 1. The function softmax does this for any number of columns:\n\ndef softmax(x): return torch.exp(x) / torch.exp(x).sum(dim=1, keepdim=True)\n\n\nsm_acts = softmax(acts)\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nsoftmax is the multi-category equivalent of sigmoid. We have to use it any time we have more than two categories and the probabilities of the categories must add up to 1. Taking the exponential ensures all of our numbers are positive, and then dividing by the sum ensures that we are going to have a bunch of numbers that add up to 1.\nsoftmax is the first part of the cross-entropy loss, the second part is log likelihood.\n\n\nLog Likelihood\nWhen we calculated the loss for our MNIST example, we used:\ndef mnist_loss(inputs, targets):\n  inputs = inputs.sigmoid()\n  return torch.where(targets==1, 1-inputs, inputs).mean()\nWe need to extend the loss function to work with more than just binary classification.\nOur activations after softmax are between 0 and 1, and sum to 1 for each row in the batch of predictions, our targets are integers between 0 and 9.\nLet’s say these are our labels:\n\ntarg = tensor([0,1,0,1,1,0])\n\nAnd these are the softmax activations:\n\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nThen for each item of targ, we can use that to select the appropriate column of sm_acts using tensor indexing like this:\n\nidx = range(6)\nsm_acts[idx, targ]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\nAs long as the activation columns sum to 1 (as they will if we use softmax), we’ll have a loss function that shows how well we’re predicting each digit.\nMaking the activation for the correct label as high as possible must mean we’re also decreasing the activations of the remaining columns.\nPyTorch provides nll_loss which does the same thing as sm_acts[range(n), targ] except it takes the negative because when applying log afterwards we want negative numbers:\n\n-sm_acts[idx, targ]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targ, reduction='none')\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\n\nTaking the log\nHere’s a plot of the log function\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\n\nplot_function(torch.log, min=0, max=4)\n\n\n\n\nWe want to transform our probabilities to larger values so we can perform mathematical operations on them. We also want our model to learn the difference between 0.99 and 0.999 (the latter is 10 times more confident). We can use the logarithm function to transform our numbers between 0 and 1 to instead be between negative infinity and positive infinity.\nApplying negative log to the softmax output:\n\ntarg\n\ntensor([0, 1, 0, 1, 1, 0])\n\n\n\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\n\nsm_acts[idx, targ]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\n\n-torch.log(sm_acts[idx, targ])\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\nThe loss is larger for incorrect predictions (2.0160 for the third image with activations of 0.1332 for 3—the target value—and 0.8668 for 7) and for unconfident correct predictions (5.6958 for the fourth image with activations of 0.9966 for 3 and 0.0034 for 7—the target value).\nPyTorch provides the nn.CrossEntropyLoss class and the F.cross_entropy function:\n\nF.cross_entropy(acts, targ, reduction='none')\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targ)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\n\n\n\n\nCreate a Function to Calculate Predictions, Loss and Gradients\nIn this section, the text has the following function to calculate predictions, loss and gradients:\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  loss = mnist_loss(preds, yb)\n  loss.backward()\nI don’t think anything needs to be changed here. In my implementation of a BasicLearner I have generalized that function as:\ndef calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    loss.backward()\nI believe that should work fine with nn.CrossEntropyLoss() as the function assigned to my self.loss_func parameter.\n\n\nCreate an Optimizer\nThe optimizer’s functionality (stepping the parameters and setting the gradients to zero) will not need to be changed to handle 10 digits.\n\n\nCreate a Function to Train One Epoch\nThe steps needed to train an epoch will not need to be changed to handle 10 digits:\ndef train_epoch(model):\n  for xb,yb in dl:\n    calc_grad(xb, yb, model)\n    opt.step()\n    opt.zero_grad()\n\n\nCreate a Function to Calculate a Metric for One Batch\nAlong with the loss function, this is the second big change when dealing with 10 digits instead of 2. Currently, in text, we use a batch_accuracy function as the metric during training:\ndef batch_accuracy(xb, yb):\n  preds = xb.sigmoid()\n  correct = (preds>0.5) == yb\n  return correct.float().mean()\nFor each batch the following steps take place in calculating accuracy:\n\nCalculate the sigmoid value of the predictions.\nDetermine which predictions are greater than 0.5 and if they are, whether they are correctly labeled as 3s.\nReturn the mean value of the previous tensor, which will calculate as number of correct predictions / number of total predictions.\n\nTo understand how to calculate accuracy for a dataset with 10 digits, I’ll create an example calculation similar to what they did in chapter 5 for illustrating the softmax/log likelihood example. Suppose we have 6 images with 10 possible categories (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) and 6 labels:\n\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,10))*2\nacts\n\ntensor([[ 3.8538,  2.9746,  1.8014, -4.2110,  1.3568, -2.4691, -0.0861, -3.2093,\n         -1.5043,  3.2974],\n        [-0.7850, -2.8072, -1.4558, -1.1189, -1.5377,  1.5249,  3.2846, -0.3192,\n         -0.9948,  0.8792],\n        [-1.5163,  2.1566,  1.6016,  3.3612,  2.5582,  2.5928,  1.2209,  2.6695,\n         -0.4632,  0.0835],\n        [-0.5032,  1.7197, -2.7693, -1.7425, -0.4467,  3.4347,  0.6378, -0.8490,\n          0.6114, -1.5492],\n        [-3.1151,  1.9913, -1.7596, -1.2023,  0.7345,  0.3508,  2.7703, -0.8917,\n          2.8903,  1.7128],\n        [ 4.4362,  1.0463,  2.3507,  1.1223, -0.9055, -1.5436, -0.3444,  1.0476,\n          0.1132,  0.8526]])\n\n\n\ntarg = tensor([1,6,4,5,8,0])\n\nWe take the softmax of the activations so that each row sums to 1:\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\ntensor([[4.4919e-01, 1.8645e-01, 5.7688e-02, 1.4122e-04, 3.6982e-02, 8.0615e-04,\n         8.7362e-03, 3.8453e-04, 2.1156e-03, 2.5751e-01],\n        [1.2639e-02, 1.6728e-03, 6.4621e-03, 9.0509e-03, 5.9539e-03, 1.2731e-01,\n         7.3978e-01, 2.0136e-02, 1.0246e-02, 6.6747e-02],\n        [2.4815e-03, 9.7686e-02, 5.6077e-02, 3.2583e-01, 1.4597e-01, 1.5110e-01,\n         3.8323e-02, 1.6314e-01, 7.1126e-03, 1.2288e-02],\n        [1.4239e-02, 1.3148e-01, 1.4766e-03, 4.1232e-03, 1.5065e-02, 7.3058e-01,\n         4.4562e-02, 1.0075e-02, 4.3404e-02, 5.0024e-03],\n        [8.6558e-04, 1.4289e-01, 3.3576e-03, 5.8621e-03, 4.0662e-02, 2.7705e-02,\n         3.1141e-01, 7.9971e-03, 3.5109e-01, 1.0816e-01],\n        [7.7830e-01, 2.6240e-02, 9.6708e-02, 2.8313e-02, 3.7265e-03, 1.9688e-03,\n         6.5311e-03, 2.6273e-02, 1.0321e-02, 2.1619e-02]])\n\n\n\nsm_acts.sum(dim=1)\n\ntensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n\n\nThe model’s prediction is the digit (index) with the largest probability.\n\nsm_acts.max(dim=1).indices\n\ntensor([0, 6, 3, 5, 8, 0])\n\n\nThe accuracy is the average of the following boolean tensor (the number of correct predictions / the number of total predictions):\n\nsm_acts.max(dim=1).indices == targ\n\ntensor([False,  True, False,  True,  True,  True])\n\n\n\n(sm_acts.max(dim=1).indices == targ).float().mean().item()\n\n0.6666666865348816\n\n\nAs a function, this is what batch_accuracy would look like for the 10-digit classifier:\ndef batch_accuracy(xb, yb):\n  preds = torch.softmax(xb, dim=1)\n  preds = preds.max(dim=1).indices\n  return (preds == yb).float().mean().item()\n\n\nCreate a Function to Calculate the Metric for One Epoch\nThis function will be the same for the 10-digit classifier as it was for the 2-digit classifier:\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n  return round(torch.stack(accs).mean().item(), 4)\n\n\nCreate a Function for the Training Loop\nThis function will be the same for the 10-digit classifier as it was for the 2-digit classifier:\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')"
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnist-training-loop",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnist-training-loop",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "MNIST Training Loop",
    "text": "MNIST Training Loop\nWith that overview under my belt, I’ll walk through the training loop with real code and data.\n\nLoad and Prep Data\nI’ve already loaded and prepared a shuffled DataLoaders object, so in this step all I need to do is check it’s size and shape:\n\nlen(dls.train.dataset), len(dls.valid.dataset)\n\n(60000, 10000)\n\n\n\nx,y = first(dls.train)\nx.shape, y.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nx,y = first(dls.valid)\nx.shape, y.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\n# check that the digits are shuffled\ny[:5]\n\ntensor([[6],\n        [8],\n        [2],\n        [0],\n        [6]])\n\n\n\n\nCreate Our Model\nFor the first iteration of the training loop, I’ll use a similarly structured model as before:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n\nCreate a Loss Function\nThe loss function that I will use is nn.CrossEntropyLoss:\n\nmnist_loss = nn.CrossEntropyLoss()\n\n\n\nCreate a function to Calculate Predictions, Loss and Gradients\nThis function is the same as before:\n\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  # yb has to be a 0- or 1-D tensor\n  loss = mnist_loss(preds, yb.squeeze())\n  loss.backward()\n\n\n\nCreate an Optimizer\nI’ll use the BasicOptim optimizer defined in chapter 4:\n\nclass BasicOptim:\n  def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n  def step(self, *args, **kwargs):\n    for p in self.params: p.data -= p.grad.data * self.lr\n\n  def zero_grad(self, *args, **kwargs):\n    for p in self.params: p.grad = None\n\n\nlr = 0.1\n\n\nopt = BasicOptim(simple_net.parameters(), lr)\n\n\n\nCreate a Function to Train One Epoch\nSame as before:\n\ndef train_epoch(model):\n  for xb,yb in dls.train:\n    calc_grad(xb, yb, model)\n    opt.step()\n    opt.zero_grad()\n\n\n\nCreate a Function to Calculate a Metric for One Batch\nThe metric of interest is accuracy, and I’ll define a new batch_accuracy function to handle 10-digits instead of 2:\n\ndef batch_accuracy(xb, yb):\n  preds = torch.softmax(xb, dim=1)\n  preds = preds.max(dim=1).indices\n  # squeeze yb so it's the same shape as preds\n  return (preds == yb.squeeze()).float().mean().item()\n\n\n\nCreate a Function to Calculate the Metric for One Epoch\nSame as before, but iterating through dls.valid instead of a valid_dl DataLoader:\n\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in dls.valid]\n  return round(torch.tensor(accs).mean().item(), 4)\n\n\n\nTrain the Model for One Epoch\nSince I have a new loss function and accuracy function, I’ll train the model for one batch manually and then use a loop for multiple epochs.\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n# create a fresh optimizer\nopt = BasicOptim(simple_net.parameters(), lr=0.01)\n\n\n# get one batch of the training data\nxb, yb = first(dls.train)\n\n\n# calculate predictions\npreds = simple_net(xb)\n\n\npreds.shape\n\ntorch.Size([256, 10])\n\n\nnn.CrossEntropyLoss() expects a 0- or 1-dimensional tensor for the labels (targets) so I have to squeeze the label tensor.\n\n# calculate loss\nloss = mnist_loss(preds, yb.squeeze())\n\n\nloss\n\ntensor(2.3140, grad_fn=<NllLossBackward0>)\n\n\n\n# calculate gradients\nloss.backward()\n\n\n# step the weights\nopt.step()\nopt.zero_grad()\n\n\n# calculate accuracy using validation set\nvalidate_epoch(simple_net)\n\n0.1239\n\n\n\n\nCreate a Function for the Training Loop\nNow that I’ve tested my loss and accuracy function for one epoch, I can loop through the training process for multiple epochs:\n\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')\n\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n# create a fresh optimizer\nopt = BasicOptim(simple_net.parameters(), lr=0.001)\n\n\ntrain_model(simple_net, 40)\n\n0.2094 0.2522 0.3821 0.5208 0.6083 0.6399 0.6674 0.6776 0.6828 0.6894 0.701 0.7071 0.7221 0.7418 0.7549 0.7735 0.7852 0.7896 0.8061 0.8089 0.8212 0.8188 0.8264 0.8309 0.8313 0.8336 0.8378 0.8397 0.8462 0.8487 0.8511 0.8546 0.8545 0.8545 0.8597 0.8604 0.8618 0.8609 0.8612 0.8662 \n\n\nAfter training the model for 40 epochs with a learning rate of 0.001, I achieve an accuracy of about 86%.\nI’ll now test my model and see if it predicts digits correctly:\n\ntest_x = dls.valid.dataset[1][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\npreds = simple_net(test_x)\npreds = torch.softmax(preds, dim=0)\n\n# this should be 0\npreds.argmax().item()\n\n0\n\n\n\ntest_x = dls.valid.dataset[-1][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\npreds = simple_net(test_x)\npreds = torch.softmax(preds, dim=0)\n\n# this should be 9\npreds.argmax().item()\n\n9\n\n\n\ntest_x = dls.valid.dataset[5000][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\npreds = simple_net(test_x)\npreds = torch.softmax(preds, dim=0)\n# this should be 4\npreds.argmax().item()\n\n4\n\n\nLook good! The model is correctly predicting images.\nNext, I’ll modify my BasicLearner class (and call it MNISTLearner) so that it can handle training the full MNIST dataset."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnistlearner-class",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnistlearner-class",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "MNISTLearner Class",
    "text": "MNISTLearner Class\nI’ll build off my BasicLearner class by incorporating some changes (mainly, changing yb to yb.squeeze() when calculating loss).\nI’ll also add a Time column which displays how much time it took to train each epoch.\n\nimport time\n\n\nclass MNISTLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    # calculates loss and gradients\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb.squeeze())\n    # store the loss of each batch\n    # later to be averaged across the epoch\n    self.loss = torch.cat((self.loss, tensor([loss])))\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      # steps the weights\n      self.opt.step()\n      # resets gradient to zero\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    # calculates mean accuracy across validation set\n    return round(torch.tensor(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(f\"{'Epoch':<8}{'Train Loss':<14}{self.metric.__name__:<16}{'Time (s)'}\")\n    for i in range(self.epochs):\n      start_time = time.time()\n      self.loss = tensor([])\n      self.train_epoch()\n      end_time = round(time.time() - start_time,4)\n      mean_loss = round(self.loss.mean().item(), 4)\n      mean_metric = self.validate_epoch()\n      print(f\"{i:<8}{mean_loss:<14}{mean_metric:<16}{end_time}\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    # instantiate optimizer\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    # run training loop\n    self.train_model(self.model, self.epochs)\n\n  def predict(self, x):\n    prediction = self.model(x)\n    # predictions should add up to 1.\n    prediction = torch.softmax(prediction, dim=0)\n    # return probability and label\n    return prediction.max(), prediction.argmax().item()\n\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n# instantiate MNISTLearner\nlearner = MNISTLearner(dls=dls,\n                       model=simple_net,\n                       opt_func=BasicOptim,\n                       loss_func=nn.CrossEntropyLoss(),\n                       metric=batch_accuracy)\n\n\n# run training loop for 40 epochs\nlearner.fit(epochs=40, lr=0.001)\n\nEpoch   Train Loss    batch_accuracy  Time (s)\n0       2.3025        0.1194          0.7052\n1       2.2658        0.1836          0.6548\n2       2.2273        0.2968          0.6137\n3       2.1825        0.3777          0.5981\n4       2.1311        0.4608          0.6194\n5       2.0769        0.5362          0.6074\n6       2.0208        0.5938          0.7253\n7       1.9622        0.6406          0.7677\n8       1.9025        0.6746          0.6411\n9       1.8408        0.6964          0.6073\n10      1.7774        0.7209          0.6361\n11      1.7121        0.7318          0.5707\n12      1.6454        0.7428          0.886\n13      1.5776        0.7583          1.0406\n14      1.5094        0.7699          0.5713\n15      1.4416        0.7759          0.5628\n16      1.3759        0.7796          0.5508\n17      1.3118        0.7882          0.5552\n18      1.2518        0.7876          0.6065\n19      1.1946        0.7981          0.5996\n20      1.1411        0.801           0.5771\n21      1.0918        0.8123          0.5345\n22      1.0467        0.8177          0.5996\n23      1.0042        0.8211          0.7218\n24      0.966         0.8229          0.7094\n25      0.9305        0.8261          0.5933\n26      0.8984        0.8311          0.5768\n27      0.8686        0.8356          0.5928\n28      0.8416        0.8342          0.5969\n29      0.8167        0.8418          0.5945\n30      0.7935        0.8438          0.5933\n31      0.7716        0.8449          0.5819\n32      0.7519        0.8489          0.5428\n33      0.7337        0.8498          0.6047\n34      0.7166        0.8525          0.6062\n35      0.7011        0.8511          0.5831\n36      0.6862        0.8552          0.5418\n37      0.6725        0.86            0.5483\n38      0.6592        0.8539          0.5721\n39      0.6471        0.856           0.5682\n\n\nGreat! Next, I’ll test predictions of this model, one image per digit, to see how it performs. Ideally, I would have a test dataset set aside for this part, but I’ll use the validation set instead:\n\ntest_x = dls.valid.dataset[0][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 0\nlearner.predict(test_x)\n\n(tensor(0.9909, grad_fn=<MaxBackward1>), 0)\n\n\n\ntest_x = dls.valid.dataset[1000][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 1\nlearner.predict(test_x)\n\n(tensor(0.7081, grad_fn=<MaxBackward1>), 1)\n\n\n\ntest_x = dls.valid.dataset[2600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 2\nlearner.predict(test_x)\n\n(tensor(0.5737, grad_fn=<MaxBackward1>), 4)\n\n\n\ntest_x = dls.valid.dataset[3600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 3\nlearner.predict(test_x)\n\n(tensor(0.7091, grad_fn=<MaxBackward1>), 3)\n\n\n\ntest_x = dls.valid.dataset[4600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 4\nlearner.predict(test_x)\n\n(tensor(0.8390, grad_fn=<MaxBackward1>), 4)\n\n\n\ntest_x = dls.valid.dataset[5600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 5\nlearner.predict(test_x)\n\n(tensor(0.4645, grad_fn=<MaxBackward1>), 5)\n\n\n\ntest_x = dls.valid.dataset[6600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 6\nlearner.predict(test_x)\n\n(tensor(0.9363, grad_fn=<MaxBackward1>), 6)\n\n\n\ntest_x = dls.valid.dataset[7100][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 7\nlearner.predict(test_x)\n\n(tensor(0.4163, grad_fn=<MaxBackward1>), 1)\n\n\n\ntest_x = dls.valid.dataset[8500][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 8\nlearner.predict(test_x)\n\n(tensor(0.8225, grad_fn=<MaxBackward1>), 8)\n\n\n\ntest_x = dls.valid.dataset[-1][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 9\nlearner.predict(test_x)\n\n(tensor(0.5962, grad_fn=<MaxBackward1>), 9)\n\n\nManually testing the model, it accurately predicted 80% of the digits, which makes sense given the model accuracy was 85% at the end of training."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#improving-the-model",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#improving-the-model",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "Improving the Model",
    "text": "Improving the Model\nI wonder if adding another layer to my neural net will improve the model’s accuracy. I’ll arbitrarily choose an intermediate number of activations as 784/2:\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 392),\n    nn.ReLU(),\n    nn.Linear(392, 30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\n\n# instantiate MNISTLearner\nlearner = MNISTLearner(dls=dls,\n                       model=simple_net,\n                       opt_func=BasicOptim,\n                       loss_func=nn.CrossEntropyLoss(),\n                       metric=batch_accuracy)\n\n\n# run training loop for 40 epochs\nlearner.fit(epochs=40, lr=0.005)\n\nEpoch   Train Loss    batch_accuracy  Time (s)\n0       2.2885        0.2899          1.771\n1       2.2213        0.3874          2.2249\n2       2.1157        0.5674          1.6725\n3       1.9426        0.6493          1.7144\n4       1.6701        0.708           1.5923\n5       1.3271        0.7588          1.6464\n6       1.0312        0.7899          1.7139\n7       0.8426        0.8186          1.9155\n8       0.726         0.8276          1.8586\n9       0.6467        0.8443          1.6758\n10      0.5893        0.8518          1.6447\n11      0.5452        0.8624          1.6898\n12      0.5098        0.8768          1.653\n13      0.482         0.8792          1.7703\n14      0.4598        0.8817          2.0231\n15      0.4412        0.8829          1.6538\n16      0.4255        0.8902          1.6392\n17      0.4125        0.8919          1.6541\n18      0.4017        0.8894          1.6532\n19      0.3917        0.8951          1.6696\n20      0.3834        0.8965          2.1116\n21      0.3754        0.8963          1.6413\n22      0.3687        0.8975          1.6831\n23      0.3623        0.8994          1.6442\n24      0.3567        0.9065          1.7872\n25      0.3515        0.9039          1.7696\n26      0.3464        0.9052          2.1025\n27      0.3415        0.9075          1.7063\n28      0.3372        0.9098          1.7005\n29      0.3333        0.9116          1.6846\n30      0.3293        0.9107          1.6581\n31      0.3258        0.9117          1.7834\n32      0.3214        0.9113          1.9949\n33      0.3181        0.9121          1.7621\n34      0.3146        0.9122          1.7287\n35      0.3113        0.9147          1.7505\n36      0.3078        0.9134          1.7595\n37      0.3052        0.9176          1.7835\n38      0.3017        0.9174          1.966\n39      0.2993        0.9183          1.9536\n\n\nCool! I ended up with a significant improvement in accuracy, although the training took about 3 times as long to finish.\nI’ll again test all 10 digits to see how well it predicts them:\n\nactuals = list(range(10))\nfor idx, val in enumerate([0, 1000, 2600, 3600, 4600, 5600, 6600, 7100, 8500, -1]):\n  test_x = dls.valid.dataset[val][0]\n  print(f\"{'Actual:':<8}{actuals[idx]:<4} {'Prediction: ':<12}{learner.predict(test_x)}\")\n\nActual: 0    Prediction: (tensor(0.9990, grad_fn=<MaxBackward1>), 0)\nActual: 1    Prediction: (tensor(0.9758, grad_fn=<MaxBackward1>), 1)\nActual: 2    Prediction: (tensor(0.8626, grad_fn=<MaxBackward1>), 4)\nActual: 3    Prediction: (tensor(0.9606, grad_fn=<MaxBackward1>), 3)\nActual: 4    Prediction: (tensor(0.9981, grad_fn=<MaxBackward1>), 4)\nActual: 5    Prediction: (tensor(0.9614, grad_fn=<MaxBackward1>), 5)\nActual: 6    Prediction: (tensor(0.9982, grad_fn=<MaxBackward1>), 6)\nActual: 7    Prediction: (tensor(0.8736, grad_fn=<MaxBackward1>), 7)\nActual: 8    Prediction: (tensor(0.9901, grad_fn=<MaxBackward1>), 8)\nActual: 9    Prediction: (tensor(0.9784, grad_fn=<MaxBackward1>), 9)\n\n\nMy manual testing resulted in the model predicting 90% of the digits correctly, which makes sense given the increase in accuracy of the model."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#further-improvements",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#further-improvements",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "Further Improvements",
    "text": "Further Improvements\nThere are of course unlimited improvements when it comes to trying out different models. I could vary the number of intermediate activations, number of layers, and overall architecture.\nI could also add a validation loss to understand when the model is overfitting.\nOverall, I am thrilled that this exercise was successful, and had a really fun time working through it. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-08-29-tinysentiment-claude-experiments/index.html",
    "href": "posts/2024-08-29-tinysentiment-claude-experiments/index.html",
    "title": "Sentiment Classification with Claude Using claudette",
    "section": "",
    "text": "In this blog post, I demonstrate how I achieved 94.8% accuracy in classifying sentiment in the financial_phrasebank dataset using Claude-3.5-Sonnet, 94.1% accuracy using Claude-3-Opus, and 92.4% accuracy using Haiku, all accessed through the Answer.AI library claudette.\nThis notebook is part of a series of blog posts for a project I’m working called TinySentiment where I’m experimenting with tiny models to improve their ability to classify sentiment in the financial_phrasebank dataset. This notebook establishes a baseline using these larger models.\nHere is a summary of results from this notebook. The best performing approach (3-Shot prompt with Sonnet) cost $2.27, while it cost $3.50 for Zero-Shot Opus:\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompt\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\nZero-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nclaude-3-5-sonnet-20240620\nZero-Shot\n94%\n98% (297/303)\n92% (1279/1391)\n97% (552/570)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nclaude-3-haiku-20240307\nZero-Shot\n89.84%\n96% (292/303)\n85% (1183/1391)\n98% (559/570)\n\n\nclaude-3-haiku-20240307\n6-Shot\n84.99%\n98% (296/303)\n76% (1059/1391)\n99% (564/570)"
  },
  {
    "objectID": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#setup",
    "href": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#setup",
    "title": "Sentiment Classification with Claude Using claudette",
    "section": "Setup",
    "text": "Setup\n\n\nShow imports and setup\n!pip install datasets -Uqq\n!pip install claudette -qq\nfrom datasets import load_dataset, Dataset\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom claudette import *\n\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\",\n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\n\n\n\nShow imports\n!pip install claudette -qq\nfrom claudette import *\n\n\n\n\nShow function to make confusion matrix\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n\n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['responses'], labels=['negative', 'neutral', 'positive'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive'])\n\n    fig, ax = plt.subplots(figsize=(4,4))\n    disp.plot(ax=ax,text_kw={'fontsize': 12}, cmap='Blues', colorbar=False);\n\n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n\n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=14)\n    ax.tick_params(axis='x', labelsize=14)"
  },
  {
    "objectID": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#performing-sentiment-classification-with-claude",
    "href": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#performing-sentiment-classification-with-claude",
    "title": "Sentiment Classification with Claude Using claudette",
    "section": "Performing Sentiment Classification with Claude",
    "text": "Performing Sentiment Classification with Claude\n\n3-Opus\nSince Opus is pricier, I’ll only do sentiment classification once.\nI’ll start by asking Claude (through the UI on claude.ai) for a recommended prompt for this task:\n\nClassify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n[Insert sentence here]\n\n\nprompt = \"\"\"Classify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\n{sentence}\"\"\"\n\n\nformatted_prompt = prompt.format(sentence=dataset['sentence'][0])\n\n\nprint(formatted_prompt)\n\nClassify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\nAccording to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\nmodel = models[0]\nmodel\n\n'claude-3-opus-20240229'\n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat.use\n\nIn: 0; Out: 0; Total: 0\n\n\nTesting it out on a single sentence, it looks like I’m getting the correct result (a single word response).\n\nr = chat(formatted_prompt)\nr\n\nneutral\n\n\nid: msg_01UGxLBjREK1n6rap8vqokJc\ncontent: [{'text': 'neutral', 'type': 'text'}]\nmodel: claude-3-opus-20240229\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'input_tokens': 73, 'output_tokens': 4, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n\n\n\n\nBefore I do the full 2264 rows, I’ll test out the performance on 25 rows which should take about 5 minutes:\n\nresults = []\ntokens = 0\n\nfor row in dataset.select(range(25)):\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = prompt.format(sentence=row['sentence'])\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens\n\n2561\n\n\nLooking at the responses, the model is consistently (correctly) responding with a single-word label.\n\nresults\n\n['neutral',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'neutral',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive']\n\n\n\nresults = []\ntokens = 0\n\nfor row in dataset:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = prompt.format(sentence=row['sentence'])\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens, len(results)\n\n(191874, 2264)\n\n\n\nmodel\n\n'claude-3-opus-20240229'\n\n\n\ndf = dataset.to_pandas()\ndf['label_text'] = df['label'].apply(lambda x: dataset.features['label'].names[x])\ndf['responses'] = results\ndf['match'] = df['label_text'] == df['responses']\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      sentence\n      label\n      label_text\n      responses\n      match\n    \n  \n  \n    \n      0\n      According to Gran , the company has no plans t...\n      1\n      neutral\n      neutral\n      True\n    \n    \n      1\n      For the last quarter of 2010 , Componenta 's n...\n      2\n      positive\n      positive\n      True\n    \n    \n      2\n      In the third quarter of 2010 , net sales incre...\n      2\n      positive\n      positive\n      True\n    \n    \n      3\n      Operating profit rose to EUR 13.1 mn from EUR ...\n      2\n      positive\n      positive\n      True\n    \n    \n      4\n      Operating profit totalled EUR 21.1 mn , up fro...\n      2\n      positive\n      positive\n      True\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nClaude-3 Opus achieves a 94% accuracy, which matches the GPT4 accuracy in the original blog post by Moritz Laurer that motivated my TinySentiment project.\n\ndf['match'].mean()\n\n0.9412544169611308\n\n\nOpus classifies neutral with the highest true positive rate (1333/1391—note that two of the neutral responses were neither positive nor negative), followed by negative (297/303) and positive (501/570) sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\n3.5-Sonnet\n\nmodel = models[1]\nmodel\n\n'claude-3-5-sonnet-20240620'\n\n\n\nprint(prompt)\n\nClassify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\n{sentence}\n\n\n\nresults = []\ntokens = 0\n\nfor row in dataset:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = prompt.format(sentence=row['sentence'])\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens, len(results)\n\n(194516, 2264)\n\n\nWhat I immediately notice about the results is that Sonnet capitalized the first letter of the response in some cases:\n\nresults[0]\n\n'Neutral'\n\n\n\ndf = dataset.to_pandas()\ndf['label_text'] = df['label'].apply(lambda x: dataset.features['label'].names[x])\ndf['responses'] = results\ndf['responses'] = df['responses'].apply(lambda x: x.lower())\ndf['match'] = df['label_text'] == df['responses']\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      sentence\n      label\n      label_text\n      responses\n      match\n    \n  \n  \n    \n      0\n      According to Gran , the company has no plans t...\n      1\n      neutral\n      neutral\n      True\n    \n    \n      1\n      For the last quarter of 2010 , Componenta 's n...\n      2\n      positive\n      positive\n      True\n    \n    \n      2\n      In the third quarter of 2010 , net sales incre...\n      2\n      positive\n      positive\n      True\n    \n    \n      3\n      Operating profit rose to EUR 13.1 mn from EUR ...\n      2\n      positive\n      positive\n      True\n    \n    \n      4\n      Operating profit totalled EUR 21.1 mn , up fro...\n      2\n      positive\n      positive\n      True\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nSonnet gets about the same accuracy, 94%, as Opus and GPT4! Opus cost me $3.50 while Sonnet cost me $0.73.\n\ndf['match'].mean()\n\n0.9399293286219081\n\n\nSonnet outshines Opus in correctly predicting positive sentiment (552 to 501) matches it for negative sentences (297 each) and does a bit worse for neutral sentences (1279 to 1333).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\n3-Haiku\nI expect Haiku to perform worse, but then again I might get surprised!\n\nmodel = models[2]\nmodel\n\n'claude-3-haiku-20240307'\n\n\n\nprint(prompt)\n\nClassify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\n{sentence}\n\n\n\nresults = []\ntokens = 0\n\nfor row in dataset:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = prompt.format(sentence=row['sentence'])\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens, len(results)\n\n(193756, 2264)\n\n\n\ndf = dataset.to_pandas()\ndf['label_text'] = df['label'].apply(lambda x: dataset.features['label'].names[x])\ndf['responses'] = results\ndf['responses'] = df['responses'].apply(lambda x: x.lower())\ndf['match'] = df['label_text'] == df['responses']\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      sentence\n      label\n      label_text\n      responses\n      match\n    \n  \n  \n    \n      0\n      According to Gran , the company has no plans t...\n      1\n      neutral\n      neutral\n      True\n    \n    \n      1\n      For the last quarter of 2010 , Componenta 's n...\n      2\n      positive\n      positive\n      True\n    \n    \n      2\n      In the third quarter of 2010 , net sales incre...\n      2\n      positive\n      positive\n      True\n    \n    \n      3\n      Operating profit rose to EUR 13.1 mn from EUR ...\n      2\n      positive\n      positive\n      True\n    \n    \n      4\n      Operating profit totalled EUR 21.1 mn , up fro...\n      2\n      positive\n      positive\n      True\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nHaiku doesn’t perform as well, but it’s not too shabby at 90% accuracy.\n\ndf['match'].mean()\n\n0.8984098939929329\n\n\nHaiku actually beats both Opus and Sonnet in predicting positive sentences correctly (559 vs. 552 and 501), and is competitive in the negative true positive rate (292 vs 297). Where it lacks is in predicting neutral sentences (1183 vs. 1279 and 1333).\n\nmake_cm(df)"
  },
  {
    "objectID": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#prompt-engineering-with-haiku",
    "href": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#prompt-engineering-with-haiku",
    "title": "Sentiment Classification with Claude Using claudette",
    "section": "Prompt Engineering with Haiku",
    "text": "Prompt Engineering with Haiku\nSince Haiku is the cheapest model, I’ll try different prompts and see if it improves its performance.\n\nPrompt B\nI’ll create a few-shot prompt and exclude the three examples used in the prompt from the dataset.\n\nexclude_idxs = [0, 1, 292]\n\n\ndef ds_subset(dataset, exclude_idxs):\n    idxs = list(range(len(dataset)))\n    idxs = [x for x in idxs if x not in exclude_idxs]\n    ddf = dataset.to_pandas()\n    new_ds = Dataset.from_pandas(ddf.iloc[idxs])\n    return new_ds\n\n\npromptB_ds = ds_subset(dataset, exclude_idxs)\npromptB_ds\n\nDataset({\n    features: ['sentence', 'label', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\nfor example in dataset.select([0, 1, 292]):\n  print(example)\n\n{'sentence': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'label': 1}\n{'sentence': \"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\", 'label': 2}\n{'sentence': 'Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .', 'label': 0}\n\n\n\npromptB = \"\"\"Classify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\nExamples:\n\nsentence: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with ONLY the sentiment label, no other text.\nsentiment: positive\n\nsentence: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with ONLY the sentiment label, no other text.\nsentiment: negative\n\nsentence: {sentence}\nRespond with ONLY the sentiment label, no other text.\nsentiment: \"\"\"\n\n\nformatted_prompt = promptB.format(sentence=promptB_ds['sentence'][0])\nprint(formatted_prompt)\n\nClassify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\nExamples:\n\nsentence: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with ONLY the sentiment label, no other text.\nsentiment: positive\n\nsentence: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with ONLY the sentiment label, no other text.\nsentiment: negative\n\nsentence: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with ONLY the sentiment label, no other text.\nsentiment: \n\n\n\nmodel = models[2]\nmodel\n\n'claude-3-haiku-20240307'\n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat(formatted_prompt)\n\npositive\n\n\nid: msg_0194ehQYJKJ7u5SJVQRXGFSx\ncontent: [{'text': 'positive', 'type': 'text'}]\nmodel: claude-3-haiku-20240307\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'input_tokens': 299, 'output_tokens': 4, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n\n\n\n\n\npromptB_ds['label'][0]\n\n2\n\n\nI’ll do a test run through 25 rows which should take less than a minute:\n\nresults = []\ntokens = 0\n\nfor row in promptB_ds.select(range(25)):\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptB.format(sentence=row['sentence'])\n  print(formatted_prompt)\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\nThe results, at least in terms of formatting, look good.\n\nresults\n\n['positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive']\n\n\nThe results are also correct (all 25 are positive sentences):\n\npromptB_ds.select(range(25))['label']\n\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n\n\n\ntokens\n\n7655\n\n\n\nresults = []\ntokens = 0\n\nfor row in promptB_ds:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptB.format(sentence=row['sentence'])\n\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens, len(results)\n\n(650542, 2261)\n\n\n\ndf = promptB_ds.to_pandas()\ndf['label_text'] = df['label'].apply(lambda x: dataset.features['label'].names[x])\ndf['responses'] = results\ndf['responses'] = df['responses'].apply(lambda x: x.lower())\ndf['match'] = df['label_text'] == df['responses']\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      sentence\n      label\n      __index_level_0__\n      label_text\n      responses\n      match\n    \n  \n  \n    \n      0\n      In the third quarter of 2010 , net sales incre...\n      2\n      2\n      positive\n      positive\n      True\n    \n    \n      1\n      Operating profit rose to EUR 13.1 mn from EUR ...\n      2\n      3\n      positive\n      positive\n      True\n    \n    \n      2\n      Operating profit totalled EUR 21.1 mn , up fro...\n      2\n      4\n      positive\n      positive\n      True\n    \n    \n      3\n      Finnish Talentum reports its operating profit ...\n      2\n      5\n      positive\n      positive\n      True\n    \n    \n      4\n      Clothing retail chain Sepp+ñl+ñ 's sales incre...\n      2\n      6\n      positive\n      positive\n      True\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWith 3-shot prompting, Haiku achieves 92.4% accuracy. Not bad!\n\ndf['match'].mean()\n\n0.9239274657231313\n\n\n\nmake_cm(df)"
  },
  {
    "objectID": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#sonnet-3-shot-prompt",
    "href": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#sonnet-3-shot-prompt",
    "title": "Sentiment Classification with Claude Using claudette",
    "section": "Sonnet: 3-Shot Prompt",
    "text": "Sonnet: 3-Shot Prompt\nGiven the success of 3-shot prompting with Haiku, I’ll spend a couple of dollars and use that prompt for Sonnet:\n\nmodel = models[1]\nmodel\n\n'claude-3-5-sonnet-20240620'\n\n\n\nprint(promptB)\n\nClassify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\nExamples:\n\nsentence: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with ONLY the sentiment label, no other text.\nsentiment: positive\n\nsentence: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with ONLY the sentiment label, no other text.\nsentiment: negative\n\nsentence: {sentence}\nRespond with ONLY the sentiment label, no other text.\nsentiment: \n\n\nTesting out with 10 rows of data:\n\nresults = []\ntokens = 0\n\nfor row in promptB_ds.select(range(10)):\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptB.format(sentence=row['sentence'])\n\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\nThe outputs look good!\n\nresults\n\n['positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive',\n 'positive']\n\n\nRunning inference on the full dataset:\n\nmodel == 'claude-3-5-sonnet-20240620'\n\nTrue\n\n\n\nresults = []\ntokens = 0\n\n\nidxs = [idx for idx in range(len(promptB_ds)) if idx > 2212]\nidxs[0], idxs[-1], len(idxs)\n\n(2213, 2260, 48)\n\n\n\nfor row in promptB_ds.select(idxs):\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptB.format(sentence=row['sentence'])\n\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens, len(results)\n\n(650584, 2261)\n\n\n\ndf = promptB_ds.to_pandas()\ndf['label_text'] = df['label'].apply(lambda x: dataset.features['label'].names[x])\ndf['responses'] = results\ndf['responses'] = df['responses'].apply(lambda x: x.lower())\ndf['match'] = df['label_text'] == df['responses']\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      sentence\n      label\n      __index_level_0__\n      label_text\n      responses\n      match\n    \n  \n  \n    \n      0\n      In the third quarter of 2010 , net sales incre...\n      2\n      2\n      positive\n      positive\n      True\n    \n    \n      1\n      Operating profit rose to EUR 13.1 mn from EUR ...\n      2\n      3\n      positive\n      positive\n      True\n    \n    \n      2\n      Operating profit totalled EUR 21.1 mn , up fro...\n      2\n      4\n      positive\n      positive\n      True\n    \n    \n      3\n      Finnish Talentum reports its operating profit ...\n      2\n      5\n      positive\n      positive\n      True\n    \n    \n      4\n      Clothing retail chain Sepp+ñl+ñ 's sales incre...\n      2\n      6\n      positive\n      positive\n      True\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWith 3-shot prompting, Sonnet-3.5 beats the 0-shot Opus result by 0.65%\n\ndf['match'].mean()\n\n0.9478107032286599\n\n\nCompared to 0-shot prompting, the 3-shot prompt for Sonnet had the same number of true positives for negative sentences (297), 23 more for neutral sentences (1302 vs. 1279) and 8 fewer for positive sentences (544 vs. 552).\n\nmake_cm(df)"
  },
  {
    "objectID": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#shot-haiku",
    "href": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#shot-haiku",
    "title": "Sentiment Classification with Claude Using claudette",
    "section": "6-shot Haiku",
    "text": "6-shot Haiku\nAs a final experiment, I’ll double the number of examples provided in the prompt to 6 and see if that improves Haiku’s performance.\n\npromptC = \"\"\"Classify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\nExamples:\n\nsentence: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with ONLY the sentiment label, no other text.\nsentiment: positive\n\nsentence: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with ONLY the sentiment label, no other text.\nsentiment: negative\n\nsentence: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: {sentence}\nRespond with ONLY the sentiment label, no other text.\nsentiment: \"\"\"\n\n\nformatted_prompt = promptC.format(sentence=dataset[10]['sentence'])\nprint(formatted_prompt)\n\nClassify the sentiment of this financial news sentence as either negative, neutral, or positive. Respond with ONLY the sentiment label, no other text:\n\nExamples:\n\nsentence: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with ONLY the sentiment label, no other text.\nsentiment: positive\n\nsentence: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with ONLY the sentiment label, no other text.\nsentiment: negative\n\nsentence: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with ONLY the sentiment label, no other text.\nsentiment: neutral\n\nsentence: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\nRespond with ONLY the sentiment label, no other text.\nsentiment: \n\n\n\nexclude_idxs = [0, 1, 292, 37, 38, 39]\npromptC_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptC_ds\n\nDataset({\n    features: ['sentence', 'label', '__index_level_0__'],\n    num_rows: 2258\n})\n\n\n\nresults = []\ntokens = 0\n\n\nidxs = [idx for idx in range(len(promptC_ds)) if idx > (len(results) - 1)]\nidxs[0], idxs[-1], len(idxs)\n\n(1974, 2257, 284)\n\n\n\nmodel = models[2]\nmodel == 'claude-3-haiku-20240307'\n\nTrue\n\n\n\nfor row in promptC_ds.select(idxs):\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptC.format(sentence=row['sentence'])\n  r = chat(formatted_prompt)\n  results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens, len(results)\n\n(1144141, 2258)\n\n\n\ndf = promptC_ds.to_pandas()\ndf['label_text'] = df['label'].apply(lambda x: dataset.features['label'].names[x])\ndf['responses'] = results\ndf['responses'] = df['responses'].apply(lambda x: x.lower())\ndf['match'] = df['label_text'] == df['responses']\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      sentence\n      label\n      __index_level_0__\n      label_text\n      responses\n      match\n    \n  \n  \n    \n      0\n      In the third quarter of 2010 , net sales incre...\n      2\n      2\n      positive\n      positive\n      True\n    \n    \n      1\n      Operating profit rose to EUR 13.1 mn from EUR ...\n      2\n      3\n      positive\n      positive\n      True\n    \n    \n      2\n      Operating profit totalled EUR 21.1 mn , up fro...\n      2\n      4\n      positive\n      positive\n      True\n    \n    \n      3\n      Finnish Talentum reports its operating profit ...\n      2\n      5\n      positive\n      positive\n      True\n    \n    \n      4\n      Clothing retail chain Sepp+ñl+ñ 's sales incre...\n      2\n      6\n      positive\n      positive\n      True\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n6-shot prompting actually worsened Haiku’s performance from 92.4% (3-shot) to 85%.\n\ndf['match'].mean()\n\n0.849867139061116\n\n\nInteresting that Haiku performed better on negative sentences (296 vs. 272) and positive sentences (564 vs. 550) but much worse on neutral sentences (1059 vs. 1267) even though there were 4 neutral examples in the prompt.\n\nmake_cm(df)"
  },
  {
    "objectID": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#final-thoughts",
    "href": "posts/2024-08-29-tinysentiment-claude-experiments/index.html#final-thoughts",
    "title": "Sentiment Classification with Claude Using claudette",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere is a summary of results from this notebook. The best performing approach (3-Shot prompt with Sonnet) cost $2.27, cheaper than the $3.50 for Zero-Shot Opus:\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompt\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\nZero-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nclaude-3-5-sonnet-20240620\nZero-Shot\n94%\n98% (297/303)\n92% (1279/1391)\n97% (552/570)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nclaude-3-haiku-20240307\nZero-Shot\n89.84%\n96% (292/303)\n85% (1183/1391)\n98% (559/570)\n\n\nclaude-3-haiku-20240307\n6-Shot\n84.99%\n98% (296/303)\n76% (1059/1391)\n99% (564/570)\n\n\n\nHere are my takeaways:\n\nOpus is pricey: I wasn’t planning on experimenting with all three models. In fact, I accidentally selected Opus for the first experiment instead of Sonnet. What gave it away? How much it cost! Someone on Twitter replied to my post about this saying that inference for a multi-turn conversation was costing them $1 per turn. I believe it.\n1 Million tokens is not that much: I entered this experiment thinking that I’d sit well below the daily 1M token limit. I was wrong! This experiment ended up taking over 3.3M tokens (3,291,956 input / 71,075 output).\nHaiku is competent: For about 10% of the cost, 3-Shot Haiku was only 2.4% less accurate than the best-performing 3-Shot Sonnet. I totally understand now why folks talk about using Haiku for simpler tasks. It’s so much cheaper!\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-6/index.html",
    "href": "posts/2024-02-05-paddy-part-6/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 6",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8\nPart 2: Live Coding 9\nPart 3: Live Coding 10\nPart 4: Live Coding 11\nPart 5: Live Coding 12\nPart 6: Live Coding 13 (You are here)\nPart 7: Improving My Large Ensemble, Part 1\nPart 8: Improving My Large Ensemble, Part 2\n\nLink to Live Coding 13\n\n\n\n!pip install -qq timm==0.6.13\nimport timm\ntimm.__version__\n\n'0.6.13'\n\n\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai')\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\n\npath.ls()\n\n(#4) [Path('../input/paddy-disease-classification/sample_submission.csv'),Path('../input/paddy-disease-classification/train_images'),Path('../input/paddy-disease-classification/train.csv'),Path('../input/paddy-disease-classification/test_images')]\n\n\n\ntrn_path = path/'train_images'\n\n\n\n\nJeremy re-did his approach to the multi-head (multi-task) classification that we started in the last live coding session. Spoiler alert: it didn’t turn out to help our final score, the score was about the same. As soon as Jeremy turned off Zoom and went for a walk, he realized how he should approach this problem. We can make this much much simpler.\nWe are going to try to predict two things: the disease and the variety for each image. The first thing is to create a pair of DataLoaders:\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n      variety\n      age\n    \n  \n  \n    \n      0\n      100330.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      1\n      100365.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      2\n      100382.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      3\n      100632.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      4\n      101918.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n  \n\n\n\n\n\n\n\nRecreate what we had starting from the small models where Jeremy used ImageDataLoaders which is the highest-level, least-flexible function where you can do all of the data processing in a single line of code but only if we want to do something really standard. Trying to predict two things is not standard enough for it.\nWe need to go one layer down. All of the work in ImageDataLoaders is being done by DataBlock, which is still high-level API but very flexible.\nHere’s how to setup a DataBlock for disease classification:\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = parent_label,\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\ndls = dblock.dataloaders(trn_path)\ndls.show_batch()\n\n\n\n\nHere’s the source code of ImageBlock:\ndef ImageBlock(cls:PILBase=PILImage):\n    \"A `TransformBlock` for images of `cls`\"\n    return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)\nWhich calls PILImage.create, which returns an Image from a filename:\n\nPILImage.create?\n\n\nSignature:\nPILImage.create(\n    fn: 'Path | str | Tensor | ndarray | bytes | Image.Image',\n    **kwargs,\n)\nDocstring: Return an Image from `fn`\nFile:      /opt/conda/lib/python3.10/site-packages/fastai/vision/core.py\nType:      method\n\n\n\nget_image_files will return a list of Paths:\n\nget_image_files(trn_path)\n\n(#10407) [Path('../input/paddy-disease-classification/train_images/tungro/109629.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/104765.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/109706.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/100098.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/102734.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/106433.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/108930.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/102019.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/102416.jpg'),Path('../input/paddy-disease-classification/train_images/tungro/101046.jpg')...]\n\n\nThese Paths are passed into PILImage.create to create an image:\n\nPILImage.create(get_image_files(trn_path)[0])\n\n\n\n\nWe have now just replicated what is happening in the code with ImageBlock (which also converts the PILImage object into a tensor with IntToFloatTensor).\nJeremy then made sure that the DataBlock approach to create the DataLoaders resulted in a successful training run, making sure to use a very small model and small image size to make it run quickly (as all we’re testing is that it trains it correctly).\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = parent_label,\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75))\n\ndls = dblock.dataloaders(trn_path)\ndls.show_batch()\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate).to_fp16()\nlearn.fit_one_cycle(1, 0.01)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n100%|██████████| 44.7M/44.7M [00:00<00:00, 52.0MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.510234\n      0.961617\n      0.334455\n      01:08\n    \n  \n\n\n\nYou want something that works this quickly so that you can test that end-to-end things are working.\nNote: fastai will shuffle the training dataset before each epoch to ensure the model is trained on a different order of images each epoch.\nThen, Jeremy ran the training run on the same architecture and number of epochs as before to make sure he got the same error rate:\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = parent_label,\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\ndls = dblock.dataloaders(trn_path)\ndls.show_batch()\n\n\n\n\n\nlearn = vision_learner(dls, 'convnext_small_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(12, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.064408\n      0.718689\n      0.213359\n      01:10\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.498714\n      0.272892\n      0.093224\n      01:27\n    \n    \n      1\n      0.373369\n      0.210002\n      0.066314\n      01:28\n    \n    \n      2\n      0.325415\n      0.251021\n      0.081211\n      01:28\n    \n    \n      3\n      0.272691\n      0.230358\n      0.067756\n      01:28\n    \n    \n      4\n      0.238369\n      0.232206\n      0.068236\n      01:28\n    \n    \n      5\n      0.169128\n      0.104292\n      0.031235\n      01:28\n    \n    \n      6\n      0.132344\n      0.132922\n      0.036521\n      01:28\n    \n    \n      7\n      0.081303\n      0.105022\n      0.027871\n      01:28\n    \n    \n      8\n      0.065025\n      0.095855\n      0.024988\n      01:28\n    \n    \n      9\n      0.050113\n      0.093793\n      0.023066\n      01:28\n    \n    \n      10\n      0.043371\n      0.094051\n      0.023066\n      01:28\n    \n    \n      11\n      0.034400\n      0.093678\n      0.022585\n      01:28\n    \n  \n\n\n\nThe final error rate (~0.02) is similar to the original training done using the DataLoaders made from ImageDataLoaders.\n\n\n\nOne image and two categories (disease and variety). To get it to spit out two categories, you add another CategoryBlock.\nGiven an image id we need a way of getting its variety. Originally Jeremy handled it with “an ugly way” of doing it. Instead create a dict which maps from image_id to variety, and our function will be to just look that up.\n\nimg2variety = { r.image_id: r.variety for _, r in df.iterrows() }\n\n\ndict(list(img2variety.items())[0:10])\n\n{'100330.jpg': 'ADT45',\n '100365.jpg': 'ADT45',\n '100382.jpg': 'ADT45',\n '100632.jpg': 'ADT45',\n '101918.jpg': 'ADT45',\n '102353.jpg': 'ADT45',\n '102848.jpg': 'ADT45',\n '103051.jpg': 'ADT45',\n '103702.jpg': 'ADT45',\n '103920.jpg': 'ADT45'}\n\n\nWhen you access a dict item, like img2variety['100330.jpg'], behind the scenes it’s making a function call img2variety.__getitem__('100330.jpg')\n\nimg2variety.__getitem__('100330.jpg')\n\n'ADT45'\n\n\nHowever, in the DataBlock, get_image_files returns a bunch of Path objects which get passed to the get_y function to get the dependent variables. Since img2variety keys are filename strings, we need to wrap it in a function to handle Path object inputs:\n\ndef get_variety(p): return img2variety[p.name]\n\nWe need to tell the DataBlock how many of the blocks are independent variables with n_inp.\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock, CategoryBlock),\n    n_inp=1,\n    get_items=get_image_files,\n    get_y = [parent_label, get_variety],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\nBefore creating DataLoaders, test with Datasets as they are easier to debug (can access one image at a time instead of a batch).\n\ndss = dblock.datasets(trn_path)\n\n\nimg, y1, y2 = dss.train[0]\n\n\nimg\n\n\n\n\n\ny1\n\nTensorCategory(3)\n\n\n\ny2\n\nTensorCategory(0)\n\n\nLet’s recreate the pipeline of going from filename to these three outputs:\n\nfn = get_image_files(trn_path)[0]\n\n\nfn, fn.name\n\n(Path('../input/paddy-disease-classification/train_images/tungro/109629.jpg'),\n '109629.jpg')\n\n\nRecreating what the ImageBlock does:\n\nPILImage.create(fn)\n\n\n\n\nRecreating what get_y does:\n\n[parent_label(fn), get_variety(fn)]\n\n['tungro', 'ADT45']\n\n\nWould it make sense to have multiple get_items? No, get_items returns a single thing, but it could be whatever you like (tuple, list, dict, etc.). Then get_y and get_x are responsible for pulling out the bit that you need to pass to your blocks. We don’t need get_x in this case because ImageBlocks just take Paths as inputs directly.\nHere is the “hacky” method Jeremy tried originally:\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock, CategoryBlock),\n    n_inp=1,\n    get_x = lambda x: trn_path/x[1]/x[0],\n    get_y = [ColReader(1), ColReader(2)],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\n\nNow we’ll go into the weeds.\n\nTransformBlock??\n\n\nInit signature:\nTransformBlock(\n    type_tfms: 'list' = None,\n    item_tfms: 'list' = None,\n    batch_tfms: 'list' = None,\n    dl_type: 'TfmdDL' = None,\n    dls_kwargs: 'dict' = None,\n)\nSource:        \nclass TransformBlock():\n    \"A basic wrapper that links defaults transforms for the data block API\"\n    def __init__(self, \n        type_tfms:list=None, # One or more `Transform`s\n        item_tfms:list=None, # `ItemTransform`s, applied on an item\n        batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch\n        dl_type:TfmdDL=None, # Task specific `TfmdDL`, defaults to `TfmdDL`\n        dls_kwargs:dict=None, # Additional arguments to be passed to `DataLoaders`\n    ):\n        self.type_tfms  =            L(type_tfms)\n        self.item_tfms  = ToTensor + L(item_tfms)\n        self.batch_tfms =            L(batch_tfms)\n        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)\nFile:           /opt/conda/lib/python3.10/site-packages/fastai/data/block.py\nType:           type\nSubclasses:     \n\n\n\nA TransformBlock stores a bunch of things you pass in, like type transforms, item transforms, batch transforms, it always adds ToTensor since PyTorch works with tensors. Remember that an ImageBlock is a TransformBlock where the type transform is specified as PILImage.create and the batch transform is IntToTensor.\nIf you pass TransformBlock to the DataBlock.blocks, it wont do anything.\n\ndblock = DataBlock(\n    blocks=(TransformBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\n\ndss = dblock.datasets(trn_path)\n\n\ndss.train[0]\n\n(Path('../input/paddy-disease-classification/train_images/blast/105663.jpg'),)\n\n\nAll this does is take the output of get_image_files[0] and turns it into a tuple containing one thing, which is the thing itself. If we have two transform blocks, it returns a tuple with two things in it:\n\ndblock = DataBlock(\n    blocks=(TransformBlock, TransformBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n(Path('../input/paddy-disease-classification/train_images/blast/105663.jpg'),\n Path('../input/paddy-disease-classification/train_images/blast/105663.jpg'))\n\n\nIt’s returning tuples because that’s what we want: we want batches of tuples that contain inputs and outputs (potentially multiple inputs and outputs).\nWe can do stuff to the first thing in the tuple with get_x:\n\ndblock = DataBlock(\n    blocks=(TransformBlock, TransformBlock),\n    get_x=lambda o: o.name,\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n('105663.jpg',\n Path('../input/paddy-disease-classification/train_images/blast/105663.jpg'))\n\n\no.name for each get_image_files output is the filename. We can do stuff to the second thing in the tuple with get_y:\n\ndblock = DataBlock(\n    blocks=(TransformBlock, TransformBlock),\n    get_x=lambda o: o.name,\n    get_y=lambda o: o.parent,\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n('105663.jpg',\n Path('../input/paddy-disease-classification/train_images/blast'))\n\n\nTransformBlocks don’t do anything but the number of them you have is the number of pipelines it’s going to create. Suppose we had three TransformBlocks assigned to DataBlock.blocks—this will require a total of three functions between get_x and get_y:\n\ndblock = DataBlock(\n    blocks=(TransformBlock, TransformBlock, TransformBlock),\n    get_x=[lambda o: o.name, lambda o: o.name],\n    get_y=lambda o: o.parent,\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n('105663.jpg',\n '105663.jpg',\n Path('../input/paddy-disease-classification/train_images/blast'))\n\n\nBy default, the last one in the tuple is the y and the first two are the x, unless we specify n_inp as 1, provide 1 function for get_x and then two functions for get_y:\n\ndblock = DataBlock(\n    blocks=(TransformBlock, TransformBlock, TransformBlock),\n    n_inp=1,\n    get_x=lambda o: o.name,\n    get_y=[lambda o: o.parent, lambda o: o.parent],\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n('105663.jpg',\n Path('../input/paddy-disease-classification/train_images/blast'),\n Path('../input/paddy-disease-classification/train_images/blast'))\n\n\nYou can also pass the functions to TransformBlock.type_tfms instead:\n\ndblock = DataBlock(\n    blocks=(TransformBlock(type_tfms=[lambda o: o.name]), TransformBlock, TransformBlock),\n    n_inp=1,\n    get_y=[lambda o: o.parent, lambda o: o.parent],\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n('105663.jpg',\n Path('../input/paddy-disease-classification/train_images/blast'),\n Path('../input/paddy-disease-classification/train_images/blast'))\n\n\nLet’s create an ImageBlock manually:\n\ndblock = DataBlock(\n    blocks=(TransformBlock(type_tfms=PILImage.create, batch_tfms=IntToFloatTensor), TransformBlock, TransformBlock),\n    n_inp=1,\n    get_y=[lambda o: o.parent, lambda o: o.parent],\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n(PILImage mode=RGB size=480x640,\n Path('../input/paddy-disease-classification/train_images/blast'),\n Path('../input/paddy-disease-classification/train_images/blast'))\n\n\nTransformBlocks don’t do anything at all, they only store things. There’s no __call__, there’s no __forward__, etc. The DataBlock is going to go through and say okay for each thing (from get_image_files) call each of the type_tfms and ToTensor and then each of the item_tfms and for batches call each of the batch_tfms of each TransformBlock.\nget_x and get_y get called first and then type_tfms of TransformBlocks get called on their outputs.\nWhen you call DataBlock.datasets it creates a Datasets object and passes in all of the type, item and batch transforms to it. The item transform gets done by the DataLoaders and not the Datasets.\nThe only reason there’s a lot of code defining the DataBlock is to make sure that if two different things have the same type transforms, we merge them together in a sensible way. Type transforms are happening before DataLoaders time. DataLoaders are the things that are going to take tensors or at least things that can be converted into tensors. Type transforms are going to create your Datasets for you and spit out things which need to be convertible into tensors. And then DataLoaders has item transforms which are things like Resize and batch transforms which are things like data augmentation.\nItem transforms are not going to run on GPU because the items aren’t a batch yet. You need things in a batch before the GPU can be optimized effectively.\nThere is a callback which sticks things on the GPU. It just depends on whether things done are before or after that callback.\nThe fastai implementation of DataLoaders is a superset of PyTorch’s implementation.\nPyTorch’s Dataset is an abstract class and doesn’t do anything at all. A Dataset is something you can index into and it returns a single tuple with independent and dependent variables. That’s how it’s defined by PyTorch and that’s what fastai does as well.\nYou can’t index into a DataLoader, you can only iterate through it, you can grab the next one and it gives you a mini-batch which is a tensor.\nYou need something that converts the output of get_image_files into a Dataset and that’s what type_tfms do.\nThis is not the only way you could do this, but it’s our way that’s really nice because we now have this thing that you can see the 14th image and its label. If we didn’t have type transforms it would be just one more step in item transforms—your Datasets would return two things (outputs of get_x and get_y) and then the DataLoader would have to do more work which would be a perfectly okay way to do things.\nThe rule is that you need something that can be turned into a tensor. That’s the way fastai does it. You need to make sure that your type transform returns something that is a tensor or can be turned into a tensor (which a PILImage can be).\n\n\n\n\ndblock = DataBlock(\n    blocks=(ImageBlock, TransformBlock, TransformBlock),\n    n_inp=1,\n    get_y=[lambda o: o.parent.name, get_variety],\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n(PILImage mode=RGB size=480x640, 'blast', 'ADT45')\n\n\nThis breaks our rule because the last two things can’t be turned into a tensor (they are strings). What do we do with that? W replace strings with integers that are a lookup into the vocabulary. If we change the last two TransformBlocks into CategoryBlocks, that is exactly what CategoryBlock will do.\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock, CategoryBlock),\n    n_inp=1,\n    get_y=[lambda o: o.parent.name, get_variety],\n    get_items=get_image_files,\n    splitter=RandomSplitter(0.2, seed=42),\n)\n\ndss = dblock.datasets(trn_path)\ndss.train[0]\n\n(PILImage mode=RGB size=480x640, TensorCategory(3), TensorCategory(0))\n\n\nCategoryBlock has a type transform called Categorize.\n\nCategoryBlock??\n\n\n\nSignature:\nCategoryBlock(\n    vocab: 'MutableSequence | pd.Series' = None,\n    sort: 'bool' = True,\n    add_na: 'bool' = False,\n)\nSource:   \ndef CategoryBlock(\n    vocab:MutableSequence|pd.Series=None, # List of unique class names\n    sort:bool=True, # Sort the classes alphabetically\n    add_na:bool=False, # Add `#na#` to `vocab`\n):\n    \"`TransformBlock` for single-label categorical targets\"\n    return TransformBlock(type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na))\nFile:      /opt/conda/lib/python3.10/site-packages/fastai/data/block.py\nType:      function\n\n\n\n\ndss.vocab\n\n(#2) [['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro'],['ADT45', 'AndraPonni', 'AtchayaPonni', 'IR20', 'KarnatakaPonni', 'Onthanel', 'Ponni', 'RR', 'Surya', 'Zonal']]\n\n\nTo summarize: get_items gets us the rows or the examples, then we use get_y or get_x to transform it somehow so that we can pass it into the type_tfms of those blocks. The blocks are very general things, so Jeremy didn’t want us to have to write our own every time—ImageBlock will work if you pass it an image path, CategoryBlock will work if you pass it a string. So get_x and get_y then are responsible for ensuring that you pass ImageBlock a Path and CategoryBlock a string. Note that get_image_files is already returning a path so we don’t need a get_x, but it’s not returning strings so we do need a get_y.\nLet’s return to the full DataBlock for multi-classification. Some other time Jeremy will talk about how item_tfms are not applies to the categories but only to the images. The secret is using fastcore’s type dispatch functionality. If we had an image for the y, the item_tfms would apply (see the siamese tutorial on the fastai docs, because that has two images). If you think about it, anytime we do segmentation, that’s what’s happening—data augmentation is happening to x and y. This is really unusual, Jeremy doesn’t know if any other libraries that have this kind of totally transparent ability to do bounding boxes, segmentation, point clouds, whatever as dependent variables and have it all happen in unison very automatically (or at least there didn’t use to be—maybe there is now).\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock, CategoryBlock),\n    n_inp=1,\n    get_items=get_image_files,\n    get_y = [parent_label, get_variety],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\ndls = dblock.dataloaders(trn_path)\n\nfastai does a lot of things automatically because of type dispatch.\n\ndls.show_batch()\n\n\n\n\nAll the stuff we did last time about messing around with multiple different heads and all that is actually totally unecessary. All we need to do when we create our vision_learner is tell it we don’t want 10 outputs but we want 20 outputs.\nThen you need to tell it what loss function to use:\n\norig_lf = CrossEntropyLossFlat()\n\ndef disease_err(inp, disease, variety): return error_rate(inp[:,:10], disease)\ndef variety_err(inp, disease, variety): return error_rate(inp[:,10:], variety)\ndef disease_loss(inp, disease, variety): return orig_lf(inp[:,:10], disease)\ndef variety_loss(inp, disease, variety): return orig_lf(inp[:,10:], variety)\ndef loss(pred, disease, variety): return orig_lf(pred[:,:10], disease)+orig_lf(pred[:,10:],variety)\n\n\nerr_metrics = (disease_err, variety_err)\nall_metrics = err_metrics+(disease_loss, variety_loss)\n\n\narch = 'convnext_small_in22k'\n\n\nlearn = vision_learner(dls, arch, loss_func=loss, metrics=all_metrics, n_out=20).to_fp16()\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small_22k_224.pth\n\n\n\nlearn.fine_tune(12, 0.01) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      2.029203\n      1.043733\n      0.209515\n      0.130226\n      0.610881\n      0.432852\n      01:24\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      0.859452\n      0.475814\n      0.102355\n      0.057665\n      0.314314\n      0.161500\n      01:36\n    \n    \n      1\n      0.581024\n      0.325289\n      0.077847\n      0.029313\n      0.224519\n      0.100770\n      01:37\n    \n    \n      2\n      0.540563\n      0.316766\n      0.068717\n      0.036040\n      0.212697\n      0.104069\n      01:38\n    \n    \n      3\n      0.452651\n      0.236399\n      0.060548\n      0.016338\n      0.188092\n      0.048306\n      01:32\n    \n    \n      4\n      0.336122\n      0.212508\n      0.044210\n      0.014416\n      0.167000\n      0.045508\n      01:32\n    \n    \n      5\n      0.237758\n      0.175937\n      0.040365\n      0.011533\n      0.137471\n      0.038466\n      01:31\n    \n    \n      6\n      0.188718\n      0.135600\n      0.023546\n      0.008169\n      0.099861\n      0.035739\n      01:31\n    \n    \n      7\n      0.144555\n      0.124381\n      0.021144\n      0.008650\n      0.089832\n      0.034549\n      01:31\n    \n    \n      8\n      0.118053\n      0.110137\n      0.021624\n      0.006728\n      0.081897\n      0.028240\n      01:31\n    \n    \n      9\n      0.085133\n      0.105757\n      0.021144\n      0.005766\n      0.080176\n      0.025581\n      01:31\n    \n    \n      10\n      0.063399\n      0.100818\n      0.022105\n      0.004805\n      0.075001\n      0.025817\n      01:32\n    \n    \n      11\n      0.060191\n      0.098222\n      0.020183\n      0.006247\n      0.073829\n      0.024393\n      01:31\n    \n  \n\n\n\nThe final error rate for disease classification (~0.02) is similar to what I got with single-classification.\nThe fastai inference functions have options to decode the predictions, or to use the activation function. But since Jeremy defined a new loss function, you would need to add a softmax to the 20 outputs. You actually don’t need to because for the Kaggle competition, we just needed which disease had the highest prediction, and whether it’s softmax or not, it’s going to be the same because that’s a monotonic function. So it depends whether you actually needs probabilities or not.\nBut you only want to be looking at the first 10 for disease predictions:\navg_pr = t_tta.mean(0)\navg_pr[:,:10].shape\n\nidxs = avg_pr[:,:10].argmax(dim=1)\nAll the resnets and convnexts handle any input image size, it’s only the transformer models that don’t.\nIn my next blog post I work on improving my large ensemble predictions based on how Jeremy created his large ensemble."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html",
    "href": "posts/2024-02-05-paddy-part-1/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8 (You are here)\nPart 2: Live Coding 9\nPart 3: Live Coding 10\nPart 4: Live Coding 11\nPart 5: Live Coding 12\nPart 6: Live Coding 13\nPart 7: Improving My Large Ensemble, Part 1\nPart 8: Improving My Large Ensemble, Part 2\n\nLink to the Live Coding 8 video"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#setup",
    "href": "posts/2024-02-05-paddy-part-1/index.html#setup",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Setup",
    "text": "Setup\n\n!pip install -qq timm==0.6.13\nimport timm\ntimm.__version__\n\n'0.6.13'\n\n\nYou might have to restart kernel (right-click on a cell > scroll down and click “Restart Kernel”) after installing a specific version of `timm`` to make sure Kaggle uses that version (and not the latest version which in January 2024 was 0.9.10).\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai')\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\n\npath.ls()\n\n(#4) [Path('../input/paddy-disease-classification/sample_submission.csv'),Path('../input/paddy-disease-classification/train_images'),Path('../input/paddy-disease-classification/train.csv'),Path('../input/paddy-disease-classification/test_images')]\n\n\n\ntrn_path = path/'train_images'\n\n\nfiles = get_image_files(trn_path)\n\n\nimg = PILImage.create(files[0])\nimg\n\n\n\n\n\nimg.size\n\n(480, 640)"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#get-file-sizes-the-slow-way",
    "href": "posts/2024-02-05-paddy-part-1/index.html#get-file-sizes-the-slow-way",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Get file sizes the slow way",
    "text": "Get file sizes the slow way\n\n%time sizes = [PILImage.create(o).size for o in files]\n\nCPU times: user 30.1 s, sys: 1.76 s, total: 31.9 s\nWall time: 1min 59s\n\n\nIf most of the time is spent reading the image from the disk, doing it in parallel won’t be much faster. If most of the time is spent decoding the JPEG, then doing it in parallel will be faster. Which of these is true depends on whether we are using an SSD.\n\nUsing parallel processing to speed things up\nTo do this in parallel (using the CPU):\n\nfrom fastcore.parallel import *\n\n\ndef f(o): return PILImage.create(o).size\n\nAs you increase the number of workers (n_workers) from 2 to 4, the overall wall time decreases. When you increase from 4 to 8, it actually slightly increases.\n\n%time sizes = parallel(f, files, n_workers=2)\n\nCPU times: user 4.03 s, sys: 917 ms, total: 4.94 s\nWall time: 44.4 s\n\n\n\n%time sizes = parallel(f, files, n_workers=4)\n\nCPU times: user 3.78 s, sys: 689 ms, total: 4.47 s\nWall time: 29.8 s\n\n\n\n%time sizes = parallel(f, files, n_workers=8)\n\nCPU times: user 3.77 s, sys: 720 ms, total: 4.49 s\nWall time: 30.7 s\n\n\nNote: you can’t use lambda functions with parallel."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#create-a-dataloaders-object",
    "href": "posts/2024-02-05-paddy-part-1/index.html#create-a-dataloaders-object",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Create a DataLoaders object",
    "text": "Create a DataLoaders object\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(224)\n)\n\n\ndls.show_batch()\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n  trn_path,\n  valid_pct=0.2,\n  item_tfms=Resize(224))"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#selecting-a-different-image-model-from-timm",
    "href": "posts/2024-02-05-paddy-part-1/index.html#selecting-a-different-image-model-from-timm",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Selecting a different image model from timm",
    "text": "Selecting a different image model from timm\nView which models are available in the timm library:\n\ntimm.list_models('convnext*')\n\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_base_384_in22ft1k',\n 'convnext_base_in22ft1k',\n 'convnext_base_in22k',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_384_in22ft1k',\n 'convnext_large_in22ft1k',\n 'convnext_large_in22k',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_small_384_in22ft1k',\n 'convnext_small_in22ft1k',\n 'convnext_small_in22k',\n 'convnext_tiny',\n 'convnext_tiny_384_in22ft1k',\n 'convnext_tiny_hnf',\n 'convnext_tiny_in22ft1k',\n 'convnext_tiny_in22k',\n 'convnext_xlarge_384_in22ft1k',\n 'convnext_xlarge_in22ft1k',\n 'convnext_xlarge_in22k']\n\n\nIn 'convnext_small_in22k', in refers to ImageNet, 22k refers to the version of ImageNet with 22,000 categories, which is more likely to have seen something like rice paddy images than the 1000 category version of ImageNet."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#start-fine-tuning-model",
    "href": "posts/2024-02-05-paddy-part-1/index.html#start-fine-tuning-model",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Start fine tuning model",
    "text": "Start fine tuning model\n\nlearn = vision_learner(dls, 'convnext_small_in22k', metrics=error_rate)\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small_22k_224.pth\n\n\n\nlearn.fine_tune(2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.170897\n      0.561514\n      0.187410\n      02:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.568585\n      0.306411\n      0.099952\n      03:58\n    \n    \n      1\n      0.363094\n      0.219503\n      0.074964\n      03:57\n    \n  \n\n\n\nWhat does fine_tune do? - Calls Learner.freeze which only allows the optimizer to change the last layer’s weights - Calls Learner.fit_one_cycle which updates the weights in the last layer for freeze_epochs - Calls Learner.unfreeze to allow the optimizer to update all weights - Calls Learner.fit_one_cycle for epochs which updates all of the layers’ weights\nIf you are using a GPU that was released in the last 4 years or so (since 2019ish) it will train faster if you use half precision point. Most of the time on Colab or Kaggle you are not going to get one of those more up to date GPUs. Having said that, there’s really never any harm using half-precision floating point. Even if you are using an older GPU, it’s still going to save memory. To ask fastai to do that for you, add .to_fp16() at the end of the vision_learner call:\nlearn = vision_learner(dls, 'convnext_small_in22k', metrics=error_rate).to_fp16()\n\nlearn = vision_learner(dls, 'convnext_small_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.157001\n      0.536221\n      0.178760\n      00:59\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.580092\n      0.332838\n      0.113407\n      01:16\n    \n    \n      1\n      0.373035\n      0.220751\n      0.065834\n      01:16\n    \n  \n\n\n\nUsing to_fp16 resulted in a training that took only about a third of the time as the full-precision training and actually resulted in a slightly better error rate. To be fair, the first time I ran both trainings, the half-precision training resulted in a slightly worse error rate."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#discussion-of-fit_one_cycle",
    "href": "posts/2024-02-05-paddy-part-1/index.html#discussion-of-fit_one_cycle",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Discussion of fit_one_cycle",
    "text": "Discussion of fit_one_cycle\nfit_one_cycle uses a scheduler which is something that changes the learning rate during training. Remember that the learning rate is the thing we multiply the gradients by before we subtract them from the parameters. When you have a randomly initialized model, or even a pretrained model (we randomly initialize the last layer of weights). At first even a pretrained model that we are fine-tuning can’t do anything. It’s still giving random answers. That means we want to use a really small learning rate because it’s very difficult to get to a point where it’s starting to learn something slightly useful. So with fit_one_cycle when we start training the first few batches use a tiny learning rate, and then as the model gets better and better at doing something useful, you can increase the learning rate because it’s gotten to a point where it knows vaguely what it’s doing. So as it trains the learning rate goes up, and then as you start to get close to the answer, you need to decrease the learning rate because you only need to take really little steps. The schedule used by fit_one_cycle is not interacting with anything (training loss, validation loss, etc) it’s just following the shape of the curve of learning rate vs. batch number.\nSylvain Gugger has a good blog post about the 1cycle policy.\nThe learning rate finder (Learner.lr_find) does something very similar to the 1cycle scheduler, which is that it gradually increases the learning rate while it train up to 100 batches, which is generally far less than an epoch. It doesn’t increase then decrease the learning rate like fit_one_cycle, it just increases it until the whole training falls apart. Once the learning gets too high, the model jumps past the answer and the loss shoots off exponentially.\nIt surprisingly difficult to come up with algorithmically what our eyes do when we look at the loss curve to pick the best learning rate. Zach Mueller did a lot of experimenting with learning rate finder on the suggested learning rate defaults (minimum, steep, valley, slide). Note: “minimum” is the actual minimum divided by 10.\nMost of the time fastai’s default learning rate works fine. Particularly for a tabular dataset the learning rate can be almost anything. It really does depend on the model. If you try something and it doesn’t seem to be training well, run lr_rind just in case the default learning rate is nowhere near the recommended values."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#applying-fine-tuned-model-to-test-set",
    "href": "posts/2024-02-05-paddy-part-1/index.html#applying-fine-tuned-model-to-test-set",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Applying fine tuned model to test set",
    "text": "Applying fine tuned model to test set\nNow that we have a model, we’ll have to apply it to our test set in order to submit it to Kaggle.\nA test DataLoader is used for inference of a bunch of things at once.\n\ntst_files = get_image_files(path/'test_images')\n\n\ntst_files.sort()\n\n\ntst_files\n\n(#3469) [Path('../input/paddy-disease-classification/test_images/200001.jpg'),Path('../input/paddy-disease-classification/test_images/200002.jpg'),Path('../input/paddy-disease-classification/test_images/200003.jpg'),Path('../input/paddy-disease-classification/test_images/200004.jpg'),Path('../input/paddy-disease-classification/test_images/200005.jpg'),Path('../input/paddy-disease-classification/test_images/200006.jpg'),Path('../input/paddy-disease-classification/test_images/200007.jpg'),Path('../input/paddy-disease-classification/test_images/200008.jpg'),Path('../input/paddy-disease-classification/test_images/200009.jpg'),Path('../input/paddy-disease-classification/test_images/200010.jpg')...]\n\n\n\ntst_dl = dls.test_dl(tst_files)\n\n\ntst_dl.show_batch()\n\n\n\n\nThe key difference of a test_dl is that it doesn’t have labels."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-1/index.html#applying-fine-tuned-model-to-test-set-1",
    "href": "posts/2024-02-05-paddy-part-1/index.html#applying-fine-tuned-model-to-test-set-1",
    "title": "Paddy Doctor Kaggle Competition - Part 1",
    "section": "Applying fine tuned model to test set",
    "text": "Applying fine tuned model to test set\nLet’s look at the sample submission provided in the Kaggle competition:\n\nss = pd.read_csv(path/'sample_submission.csv')\n\n\nss\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n    \n  \n  \n    \n      0\n      200001.jpg\n      NaN\n    \n    \n      1\n      200002.jpg\n      NaN\n    \n    \n      2\n      200003.jpg\n      NaN\n    \n    \n      3\n      200004.jpg\n      NaN\n    \n    \n      4\n      200005.jpg\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      3464\n      203465.jpg\n      NaN\n    \n    \n      3465\n      203466.jpg\n      NaN\n    \n    \n      3466\n      203467.jpg\n      NaN\n    \n    \n      3467\n      203468.jpg\n      NaN\n    \n    \n      3468\n      203469.jpg\n      NaN\n    \n  \n\n3469 rows × 2 columns\n\n\n\nUnfortunately this doesn’t help, but we know from the Kaggle competition page that they want the text of the classification in the label column.\n\npreds = learn.get_preds(dl=tst_dl, with_decoded=True)\n\n\n\n\n\n\n\n\n\npreds\n\n(tensor([[1.6281e-02, 1.7275e-04, 3.7474e-04,  ..., 9.6960e-01, 6.2574e-04,\n          8.8058e-05],\n         [1.4577e-05, 1.6399e-04, 1.4309e-05,  ..., 4.8571e-05, 9.9965e-01,\n          1.9105e-05],\n         [5.7042e-04, 2.4972e-03, 1.3404e-04,  ..., 3.4435e-03, 9.8561e-04,\n          1.1331e-04],\n         ...,\n         [2.4891e-03, 9.6513e-05, 3.8919e-05,  ..., 4.2361e-04, 9.9636e-01,\n          2.4615e-04],\n         [1.1240e-03, 5.4271e-01, 2.8814e-03,  ..., 2.3160e-01, 5.7650e-02,\n          7.0772e-02],\n         [1.6173e-11, 5.5747e-10, 1.5082e-08,  ..., 3.0132e-10, 6.3914e-10,\n          3.0830e-07]]),\n None,\n tensor([7, 8, 3,  ..., 8, 1, 5]))\n\n\n\nprobs,_,idxs = preds\n\n\nidxs\n\ntensor([7, 8, 3,  ..., 8, 1, 5])\n\n\nidxs are indexes into the vocab of the DataLoaders.\n\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\nConvert idxs into a Series object so we can map it to the vocab:\n\nidxs = pd.Series(idxs.numpy(), name=\"idxs\")\n\n\nidxs\n\n0       7\n1       8\n2       3\n3       3\n4       3\n       ..\n3464    5\n3465    7\n3466    8\n3467    1\n3468    5\nName: idxs, Length: 3469, dtype: int64\n\n\nWe need to create a dictionary from dls.vocab that we can use as a mapping in idxs.map\n\nmapping = {k: v for k,v in enumerate(dls.vocab)}\n\n\nresults = idxs.map(mapping)\n\nNote: using a function for Series.map is really slow (259 times slower than using a dictionary).\n\n%time idxs.map(mapping)\n\nCPU times: user 784 µs, sys: 0 ns, total: 784 µs\nWall time: 666 µs\n\n\n0                       hispa\n1                      normal\n2                       blast\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nName: idxs, Length: 3469, dtype: object\n\n\n\n%time idxs.map(lambda i: dls.vocab[i])\n\nCPU times: user 241 ms, sys: 2.5 ms, total: 243 ms\nWall time: 243 ms\n\n\n0                       hispa\n1                      normal\n2                       blast\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nName: idxs, Length: 3469, dtype: object\n\n\n\nss['label'] = results\n\n\nss\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n    \n  \n  \n    \n      0\n      200001.jpg\n      hispa\n    \n    \n      1\n      200002.jpg\n      normal\n    \n    \n      2\n      200003.jpg\n      blast\n    \n    \n      3\n      200004.jpg\n      blast\n    \n    \n      4\n      200005.jpg\n      blast\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      3464\n      203465.jpg\n      dead_heart\n    \n    \n      3465\n      203466.jpg\n      hispa\n    \n    \n      3466\n      203467.jpg\n      normal\n    \n    \n      3467\n      203468.jpg\n      bacterial_leaf_streak\n    \n    \n      3468\n      203469.jpg\n      dead_heart\n    \n  \n\n3469 rows × 2 columns\n\n\n\nNormally at this point you should visually check your results:\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\nIn this case, the problem is that I don’t which one of these are right so I don’t know what to look for.\n\nss.to_csv('submission.csv', index=False)\n\n\n!head submission.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nIn my next blog post I walk through the discussion and code from Live Coding 9."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-8/index.html",
    "href": "posts/2024-02-05-paddy-part-8/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 8",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8\nPart 2: Live Coding 9\nPart 3: Live Coding 10\nPart 4: Live Coding 11\nPart 5: Live Coding 12\nPart 6: Live Coding 13\nPart 7: Improving My Large Ensemble, Part 1\nPart 8: Improving My Large Ensemble, Part 2 (You are here)"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-8/index.html#comparing-jeremys-approach-to-mine",
    "href": "posts/2024-02-05-paddy-part-8/index.html#comparing-jeremys-approach-to-mine",
    "title": "Paddy Doctor Kaggle Competition - Part 8",
    "section": "Comparing Jeremy’s Approach to Mine",
    "text": "Comparing Jeremy’s Approach to Mine\nObviously, “my” last approach is largely taken from Jeremy’s own live coding videos, but there are a few differences between our ensemble training:\n\n\n\n\n\n\n\n\nItem\nJeremy\nVishal\n\n\n\n\nLearning Rate\n0.01 for all architectures\n0.005 or 0.015 depending on the architecture\n\n\n# Epochs\n12\n24\n\n\nArchitectures\nconvnext_large_in22kvit_large_patch16_224swinv2_large_window12_192_22kswin_large_patch4_window7_224\nconvnext_large_in22kvit_large_patch16_224swinv2_large_window12_192_22k\n\n\nGradient Accumulation\nYes\nYes\n\n\nBatch Size\n32\n16\n\n\nPrivate score\n0.98732\n0.98617\n\n\nPublic score\n0.98846\n0.98654\n\n\n\nJeremy’s approach resulted in a Private score with an ~10% smaller error rate. In terms of rankings, Jeremy’s Private score ranks #8, while mine is tied from #27 to #58 on the leaderboard."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-8/index.html#replicating-jeremys-approach",
    "href": "posts/2024-02-05-paddy-part-8/index.html#replicating-jeremys-approach",
    "title": "Paddy Doctor Kaggle Competition - Part 8",
    "section": "Replicating Jeremy’s Approach",
    "text": "Replicating Jeremy’s Approach\nI’ll first replicate Jeremy’s approach, including architectures, learning rates, # of epochs, set_seed(42), and train function to see if I get the same score. I would expect to do so. Once that’s confirmed, I will use his approach for the three architectures I chose and re-submit the predictions to see how that scores. If there’s still a difference, I can attribute it to Jeremy including the swin_large_patch4_window7_224 architecture (and multiple transforms for some of the models) in his ensemble.\n\n!pip install -qq timm==0.6.13\n!pip install kaggle -qq\nimport timm\ntimm.__version__\n\n'0.6.13'\n\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\nimport zipfile,kaggle\n\npath = Path('paddy-disease-classification')\nif not path.exists():\n  kaggle.api.competition_download_cli(str(path))\n  zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nfrom fastai.vision.all import *\nset_seed(42)\n\n\nimport gc\n\n\ntst_files = get_image_files(path/'test_images').sorted()\ntrn_path = path/'train_images'\n\n\nres = 640,480\n\n\nmodels = {\n    'convnext_large_in22k': {\n        (Resize(res), (320,224)),\n    }, 'vit_large_patch16_224': {\n        (Resize(480, method='squish'), 224),\n        (Resize(res), 224),\n    }, 'swinv2_large_window12_192_22k': {\n        (Resize(480, method='squish'), 192),\n        (Resize(res), 192),\n    }, 'swin_large_patch4_window7_224': {\n        (Resize(res), 224),\n    }\n}\n\n\ndef train(arch, size, item=Resize(480, method='squish'), accum=1, finetune=True, epochs=12):\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item,\n        batch_tfms=aug_transforms(size=size, min_scale=0.75), bs=64//accum)\n    cbs = GradientAccumulation(64) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    if finetune:\n        learn.fine_tune(epochs, 0.01)\n        return learn.tta(dl=dls.test_dl(tst_files))\n    else:\n        learn.unfreeze()\n        learn.fit_one_cycle(epochs, 0.01)\n\n\ntta_res = []\n\nfor arch,details in models.items():\n    for item,size in details:\n        print('---',arch)\n        print(size)\n        print(item.name)\n        tta_res.append(train(arch, size, item=item, accum=2)) #, epochs=1))\n        gc.collect()\n        torch.cuda.empty_cache()\n\n--- convnext_large_in22k\n(320, 224)\nResize -- {'size': (480, 640), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_large_22k_224.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.887948\n      0.558075\n      0.171072\n      01:50\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.355805\n      0.198265\n      0.062951\n      02:19\n    \n    \n      1\n      0.294646\n      0.232236\n      0.064392\n      02:19\n    \n    \n      2\n      0.279926\n      0.246197\n      0.068236\n      02:18\n    \n    \n      3\n      0.255699\n      0.214052\n      0.054781\n      02:18\n    \n    \n      4\n      0.185353\n      0.169206\n      0.050937\n      02:18\n    \n    \n      5\n      0.186070\n      0.143183\n      0.035560\n      02:18\n    \n    \n      6\n      0.085736\n      0.121303\n      0.030754\n      02:18\n    \n    \n      7\n      0.057243\n      0.090094\n      0.023546\n      02:18\n    \n    \n      8\n      0.055047\n      0.102438\n      0.024027\n      02:17\n    \n    \n      9\n      0.037102\n      0.081514\n      0.017780\n      02:18\n    \n    \n      10\n      0.031336\n      0.076584\n      0.019222\n      02:18\n    \n    \n      11\n      0.026582\n      0.077788\n      0.020183\n      02:17\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n--- vit_large_patch16_224\n224\nResize -- {'size': (480, 640), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.025479\n      0.599246\n      0.190293\n      01:56\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.372387\n      0.254584\n      0.078808\n      02:26\n    \n    \n      1\n      0.363945\n      0.267997\n      0.084575\n      02:26\n    \n    \n      2\n      0.337558\n      0.416980\n      0.118693\n      02:26\n    \n    \n      3\n      0.305778\n      0.237352\n      0.068717\n      02:26\n    \n    \n      4\n      0.205868\n      0.220364\n      0.052859\n      02:26\n    \n    \n      5\n      0.155062\n      0.132949\n      0.037001\n      02:26\n    \n    \n      6\n      0.131659\n      0.115785\n      0.029793\n      02:26\n    \n    \n      7\n      0.084275\n      0.113429\n      0.028352\n      02:26\n    \n    \n      8\n      0.054473\n      0.126284\n      0.028352\n      02:26\n    \n    \n      9\n      0.040826\n      0.095426\n      0.023066\n      02:26\n    \n    \n      10\n      0.030117\n      0.101836\n      0.022105\n      02:26\n    \n    \n      11\n      0.032172\n      0.097483\n      0.021624\n      02:26\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n--- vit_large_patch16_224\n224\nResize -- {'size': (480, 480), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.977947\n      0.495749\n      0.167227\n      01:53\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.445487\n      0.227239\n      0.076886\n      02:24\n    \n    \n      1\n      0.310319\n      0.217010\n      0.065353\n      02:24\n    \n    \n      2\n      0.346164\n      0.222110\n      0.071120\n      02:24\n    \n    \n      3\n      0.316973\n      0.220043\n      0.066314\n      02:24\n    \n    \n      4\n      0.201981\n      0.209637\n      0.057184\n      02:24\n    \n    \n      5\n      0.130500\n      0.139665\n      0.036521\n      02:24\n    \n    \n      6\n      0.142111\n      0.127187\n      0.030754\n      02:24\n    \n    \n      7\n      0.071605\n      0.089311\n      0.022105\n      02:24\n    \n    \n      8\n      0.048045\n      0.083216\n      0.020183\n      02:23\n    \n    \n      9\n      0.046874\n      0.084979\n      0.018260\n      02:23\n    \n    \n      10\n      0.024261\n      0.086005\n      0.019702\n      02:24\n    \n    \n      11\n      0.021223\n      0.083154\n      0.016819\n      02:23\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n--- swinv2_large_window12_192_22k\n192\nResize -- {'size': (480, 480), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\n/usr/local/lib/python3.9/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth\" to /root/.cache/torch/hub/checkpoints/swinv2_large_patch4_window12_192_22k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.929301\n      0.633476\n      0.173955\n      02:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.397221\n      0.207330\n      0.060548\n      02:26\n    \n    \n      1\n      0.349008\n      0.227384\n      0.068236\n      02:27\n    \n    \n      2\n      0.321012\n      0.355698\n      0.104277\n      02:27\n    \n    \n      3\n      0.280713\n      0.199645\n      0.058145\n      02:27\n    \n    \n      4\n      0.228984\n      0.219441\n      0.061028\n      02:26\n    \n    \n      5\n      0.154743\n      0.159890\n      0.039885\n      02:28\n    \n    \n      6\n      0.133811\n      0.156821\n      0.040365\n      02:27\n    \n    \n      7\n      0.082750\n      0.137658\n      0.032196\n      02:27\n    \n    \n      8\n      0.069910\n      0.132426\n      0.029793\n      02:27\n    \n    \n      9\n      0.052760\n      0.111611\n      0.022585\n      02:26\n    \n    \n      10\n      0.039415\n      0.115199\n      0.022585\n      02:27\n    \n    \n      11\n      0.027980\n      0.115816\n      0.023066\n      02:26\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n--- swinv2_large_window12_192_22k\n192\nResize -- {'size': (480, 640), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.937894\n      0.522241\n      0.169630\n      02:04\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.460170\n      0.213483\n      0.073042\n      02:29\n    \n    \n      1\n      0.352201\n      0.178675\n      0.054301\n      02:29\n    \n    \n      2\n      0.386350\n      0.334553\n      0.097549\n      02:28\n    \n    \n      3\n      0.299634\n      0.164248\n      0.046612\n      02:29\n    \n    \n      4\n      0.228867\n      0.126158\n      0.035079\n      02:29\n    \n    \n      5\n      0.178476\n      0.138584\n      0.042768\n      02:29\n    \n    \n      6\n      0.154566\n      0.148524\n      0.039885\n      02:28\n    \n    \n      7\n      0.094369\n      0.078149\n      0.020663\n      02:29\n    \n    \n      8\n      0.066650\n      0.069993\n      0.019222\n      02:29\n    \n    \n      9\n      0.049904\n      0.061477\n      0.017299\n      02:29\n    \n    \n      10\n      0.037704\n      0.060764\n      0.016338\n      02:29\n    \n    \n      11\n      0.033226\n      0.061885\n      0.015858\n      02:29\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n--- swin_large_patch4_window7_224\n224\nResize -- {'size': (480, 640), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth\" to /root/.cache/torch/hub/checkpoints/swin_large_patch4_window7_224_22kto1k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.930659\n      0.492810\n      0.154253\n      01:41\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.422924\n      0.213063\n      0.072561\n      02:02\n    \n    \n      1\n      0.369444\n      0.222754\n      0.066795\n      02:02\n    \n    \n      2\n      0.338899\n      0.212781\n      0.057665\n      02:03\n    \n    \n      3\n      0.308362\n      0.159222\n      0.046612\n      02:03\n    \n    \n      4\n      0.214941\n      0.142208\n      0.037001\n      02:02\n    \n    \n      5\n      0.155058\n      0.139699\n      0.032196\n      02:02\n    \n    \n      6\n      0.161482\n      0.116061\n      0.030754\n      02:02\n    \n    \n      7\n      0.098805\n      0.080427\n      0.022105\n      02:03\n    \n    \n      8\n      0.071636\n      0.073006\n      0.020183\n      02:02\n    \n    \n      9\n      0.056668\n      0.073751\n      0.018260\n      02:02\n    \n    \n      10\n      0.044765\n      0.064573\n      0.015858\n      02:02\n    \n    \n      11\n      0.040009\n      0.063520\n      0.015858\n      02:02\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n#save_pickle('tta_res.pkl', tta_res)\ntta_res = load_pickle(\"tta_res.pkl\")\n\n\nfor i in range(len(tta_res)):\n    print(len(tta_res[i][0]))\n\n3469\n3469\n3469\n3469\n3469\n3469\n\n\n\ntta_prs = first(zip(*tta_res))\n\n\n# double weight the vit predictions\ntta_prs += tta_prs[1:3]\n\n\navg_pr = torch.stack(tta_prs).mean(0)\navg_pr.shape\n\ntorch.Size([3469, 10])\n\n\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\nidxs = avg_pr.argmax(dim=1)\nvocab = np.array(dls.vocab)\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = vocab[idxs]\nss.to_csv('subm.csv', index=False)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis submission resulted in the following Kaggle score:\n\nPrivate score: 0.98617\nPublic score: 0.98923 (new best)\n\nMy Public score error rate decreased by 20%, but my Private score did not budge."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-8/index.html#using-jeremys-approach-for-my-ensemble",
    "href": "posts/2024-02-05-paddy-part-8/index.html#using-jeremys-approach-for-my-ensemble",
    "title": "Paddy Doctor Kaggle Competition - Part 8",
    "section": "Using Jeremy’s Approach for My Ensemble",
    "text": "Using Jeremy’s Approach for My Ensemble\nNow that I have successfully recreated Jeremy’s submission (in the sense that the models ran without error and the submission gave a reasonable score in Kaggle), I’ll now apply the same hyperparameters and functions he used for his architectures and transforms to the ones I chose for my large ensemble. The goal is to see if using his code results in a better score than when I used my code.\n\nmodels = {\n    'convnext_large_in22k': {\n        (Resize(res), (288,224)),\n    }, 'vit_large_patch16_224': {\n        (Resize(480), 224),\n    }, 'swinv2_large_window12_192_22k': {\n        (Resize(480, method='squish'), 192)\n    }\n}\n\n\ntta_res = []\n\nfor arch,details in models.items():\n    for item,size in details:\n        print('---',arch)\n        print(size)\n        print(item.name)\n        tta_res.append(train(arch, size, item=item, accum=2)) #, epochs=1))\n        gc.collect()\n        torch.cuda.empty_cache()\n\n--- convnext_large_in22k\n(288, 224)\nResize -- {'size': (480, 640), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.856573\n      0.475021\n      0.147525\n      01:39\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.383883\n      0.193702\n      0.055262\n      02:07\n    \n    \n      1\n      0.291577\n      0.189317\n      0.055262\n      02:07\n    \n    \n      2\n      0.265584\n      0.190596\n      0.051898\n      02:07\n    \n    \n      3\n      0.260673\n      0.216098\n      0.059106\n      02:07\n    \n    \n      4\n      0.188353\n      0.159554\n      0.047093\n      02:06\n    \n    \n      5\n      0.159173\n      0.157409\n      0.039404\n      02:07\n    \n    \n      6\n      0.100692\n      0.130478\n      0.029793\n      02:06\n    \n    \n      7\n      0.060365\n      0.107081\n      0.025469\n      02:07\n    \n    \n      8\n      0.050812\n      0.080841\n      0.023066\n      02:07\n    \n    \n      9\n      0.035694\n      0.084650\n      0.022105\n      02:07\n    \n    \n      10\n      0.032912\n      0.075940\n      0.016819\n      02:06\n    \n    \n      11\n      0.024196\n      0.081224\n      0.018741\n      02:07\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n--- vit_large_patch16_224\n224\nResize -- {'size': (480, 480), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.040115\n      0.719582\n      0.226814\n      01:53\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.390209\n      0.215410\n      0.070159\n      02:24\n    \n    \n      1\n      0.400067\n      0.283184\n      0.092744\n      02:24\n    \n    \n      2\n      0.341151\n      0.359277\n      0.098030\n      02:25\n    \n    \n      3\n      0.357469\n      0.291627\n      0.096588\n      02:24\n    \n    \n      4\n      0.237050\n      0.233321\n      0.064873\n      02:24\n    \n    \n      5\n      0.162601\n      0.153232\n      0.039885\n      02:24\n    \n    \n      6\n      0.116374\n      0.129873\n      0.034599\n      02:24\n    \n    \n      7\n      0.097705\n      0.106423\n      0.024507\n      02:24\n    \n    \n      8\n      0.062052\n      0.120935\n      0.026430\n      02:24\n    \n    \n      9\n      0.044538\n      0.098947\n      0.023066\n      02:24\n    \n    \n      10\n      0.029300\n      0.100037\n      0.020663\n      02:23\n    \n    \n      11\n      0.026877\n      0.097046\n      0.020663\n      02:24\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n--- swinv2_large_window12_192_22k\n192\nResize -- {'size': (480, 480), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n\n\n/usr/local/lib/python3.9/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth\" to /root/.cache/torch/hub/checkpoints/swinv2_large_patch4_window12_192_22k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.900081\n      0.538801\n      0.184527\n      02:01\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.462490\n      0.211737\n      0.063912\n      02:26\n    \n    \n      1\n      0.331918\n      0.281848\n      0.090822\n      02:26\n    \n    \n      2\n      0.376580\n      0.291321\n      0.093705\n      02:26\n    \n    \n      3\n      0.255427\n      0.163525\n      0.045651\n      02:26\n    \n    \n      4\n      0.237116\n      0.193330\n      0.056223\n      02:26\n    \n    \n      5\n      0.153437\n      0.123250\n      0.040365\n      02:26\n    \n    \n      6\n      0.115951\n      0.133760\n      0.034118\n      02:25\n    \n    \n      7\n      0.080223\n      0.078580\n      0.023066\n      02:25\n    \n    \n      8\n      0.060698\n      0.083489\n      0.020663\n      02:26\n    \n    \n      9\n      0.056002\n      0.078566\n      0.018260\n      02:26\n    \n    \n      10\n      0.035586\n      0.075723\n      0.017299\n      02:26\n    \n    \n      11\n      0.033601\n      0.074444\n      0.016819\n      02:26\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0])\n\n(3, 3469, 3469, 3469)\n\n\n\n# save_pickle('tta_res2.pkl', tta_res)\ntta_res = load_pickle('tta_res2.pkl')\n\nI’ll do three more Kaggle submissions:\n\nAll three model predictions weighted equally.\nconvnext model weighted more (because it had the lowest final training epoch validation error rate)\nvit model weighted more (because the smaller version previously had the best TTA error rate, and it’s also the one Jeremy weighted more)\n\n\ntta_prs = first(zip(*tta_res))\navg_pr = torch.stack(tta_prs).mean(0)\navg_pr.shape\n\ntorch.Size([3469, 10])\n\n\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\nidxs = avg_pr.argmax(dim=1)\nvocab = np.array(dls.vocab)\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = vocab[idxs]\nss.to_csv('subm.csv', index=False)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\n# weigh the convnext preds more\ntta_res += 2 * [tta_res[0]]\n\n\nfor i in range(len(tta_res)):\n    print(len(tta_res[i][0]))\n\n3469\n3469\n3469\n3469\n3469\n\n\n\ntta_prs = first(zip(*tta_res))\navg_pr = torch.stack(tta_prs).mean(0)\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\nidxs = avg_pr.argmax(dim=1)\nvocab = np.array(dls.vocab)\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = vocab[idxs]\nss.to_csv('subm.csv', index=False)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\n# weigh the vit preds more\ntta_res = load_pickle('tta_res2.pkl')\ntta_res += 2 * [tta_res[1]]\n\nfor i in range(len(tta_res)):\n    print(len(tta_res[i][0]))\n\n3469\n3469\n3469\n3469\n3469\n\n\n\ntta_prs = first(zip(*tta_res))\navg_pr = torch.stack(tta_prs).mean(0)\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\nidxs = avg_pr.argmax(dim=1)\nvocab = np.array(dls.vocab)\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = vocab[idxs]\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nHere are the Kaggle scores for those three submissions:\n\n\n\nDescription\nPrivate score\nPublic score\n\n\n\n\nAll three model predictions weighted equally\n0.98617\n0.98769\n\n\nconvnext weighted more\n0.98617\n0.98539\n\n\nvit weighted more\n0.98502\n0.98654\n\n\n\nThe best Private score amongst these three submissions was tied with the previous best of 0.98617.\nThe best Public score still belongs to the submission replicating Jeremy’s approach directly (0.98923).\nHere are the comprehensive Kaggle scoring results for this competition:\n\n\n\n\n\n\n\n\n\nSubmission\nDescription\nPrivate Score\nPublic Score\n\n\n\n\n1\ninitial submission file after creating a quick small model following Jeremy Howard’s walkthrough video.\n0.13709\n0.12418\n\n\n2\ninitial submission using convnext small 2 epochs fine-tuned sorted file list\n0.94124\n0.92541\n\n\n3\nsquish convnext small 12 epoch ft tta\n0.98156\n0.98308\n\n\n4\nensemble small 12 epoch ft tta\n0.98617*\n0.98423\n\n\n5\nswinv2 convnext vit large ensemble 12 epoch ft tta\n0.97811\n0.98039\n\n\n6\nswinv2 convnext vit large ensemble 24 epoch ft tta\n0.98502\n0.98539\n\n\n7\nswinv2 (3x convnext) vit large ensemble 24 epoch ft tta\n0.98387\n0.98423\n\n\n8\n(3x swinv2) convnext vit large ensemble 24 epoch ft tta\n0.98156\n0.985\n\n\n9\nswinv2 convnext (3x vit) large ensemble 24 epoch ft tta\n0.98617*\n0.98462\n\n\n10\nswinv2 large 24 epoch ft tta\n0.98271\n0.98269\n\n\n11\nconvnext large 24 epoch ft tta\n0.98502\n0.98269\n\n\n12\nvit large 24 epoch ft tta\n0.97811\n0.98231\n\n\n13\nswinv2 convnext vit large ensemble 24 epoch ft tta lr_find\n0.98387\n0.98577\n\n\n14\nswinv2 convnext (3x vit) large ensemble 24 epoch ft tta lr_find\n0.98617*\n0.98654\n\n\n15\nFollowing Jeremy Howard’s “Scaling Up: Road to the Top, Part 3” Notebook\n0.98617*\n0.98923**\n\n\n16\nconvnext swinv2 vit large ft 12 epoch tta road to the top\n0.98617*\n0.98769\n\n\n17\n(3 x convnext) swinv2 vit large ft 12 epoch tta road to the top\n0.98617*\n0.98539\n\n\n\n* largest private score (0.98617)\n** largest public score (0.98923)"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-8/index.html#final-thoughts",
    "href": "posts/2024-02-05-paddy-part-8/index.html#final-thoughts",
    "title": "Paddy Doctor Kaggle Competition - Part 8",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI really really enjoyed working through the 6-part live coding series which resulted in this 8-part blog post mini-series. I learned so much across a wide variety of topics. It was also required a lot of patience and tenacity. I ran into endless errors or issues using Kaggle and Google Colab running the trainings for the first 7 blog posts. For some unknown reason, when I was using Kaggle (whether it was in Chrome or Firefox, Incognito/Private Window and otherwise) the tab kept crashing with an “Aw, snap” error (Chrome) or “Gah” error (Firefox). Each time, I lost progress and had to re-run the model training, sometimes losing 4-5 hours of progress because of this. In Google Colab, initially it was smooth sailing until I ran out of compute units (which always show 0 anyways in the free tier). I was debating whether to purchase 100 Google Colab compute units for 10 dollars. I decided instead to upgrade my Paperspace subscription to Pro for 8 dollars/month and thus got access to faster GPUs for “free”. However, that didn’t come without a catch! You can only run free tier GPUs for 6 hours before Paperspace automatically shuts it down. Fortunately, my model training runs in this notebook only took about 4-ish hours, so I escaped unscathed.\nA few takeaways:\n\nI now understand what Jeremy meant when he said that you don’t really need to use lr_find because common problems in vision all require a similar learning rate. It didn’t matter whether I was using large or small versions of convnext, swinv2 or vit architectures, for 12 or 24 epochs. A learning rate of 0.01 for all scenarios performed the best.\nAll three of the architectures I used are pretty stable. There is variance in the final epoch validation error rate, but even after 15 different submissions, with different combinations of architectures, epochs and learning rates, the Kaggle maximum score didn’t break 0.98617.\nKaggle competitions are thrilling even when I submit scores after the competition is closed. I enjoyed trying to beat my previous score (and attempting to beat Jeremy’s score—with his own code and approach). Each time I submitted a CSV, I was excited to see the results. I can imagine the thrill when the competition is live. It must be so stressful as well! I am looking forward to competing in a live competition in 2024.\nIt’s important to both pace myself and be consistent. There were days where I couldn’t get nything accomplished on this project. There were also days where I watched and took notes on an entire live coding video from start to finish, and there were days in between. That’s fine. It happens! What’s important is to not give up just because one particular week (or month) is not producing much output. I also found that my persistence was bolstered by simply logging into Kaggle everyday, and keeping my streak going. Even if all I did was login to Kaggle. I heard someone say on a podcast or Instagram/TikTok video that before they got in shape, all they did was go to the gym and stay there for 5 minutes every day then come back home for 6 weeks. Just that practice solidified their consistency. I’m proud to say that as part of this project, I am on a 70 day Kaggle login streak! Here’s to continuing that streak throughout 2024.\n\n\nAs always, I hope you enjoyed reading this blog post series!"
  },
  {
    "objectID": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html",
    "href": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html",
    "title": "Calculating the Ratio of Horizontal to Vertical Details in a Text Image",
    "section": "",
    "text": "In this notebook, I’ll walk through an algorithm suggested by Claude to calculate the ratio of horizontal to vertical features in a text image.\nI planned to implement a serif detection algorithm but didn’t get good results from the 8-9 algorithms Claude suggested. This horizontal-to-vertical feature ratio algorithm emerged from brainstorming with Claude as an alternative approach.\nAs we’ll see, serifs tend to introduce more horizontal features, especially at the base of letters.\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series."
  },
  {
    "objectID": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#using-pywavelets-to-detect-horizontalvertical-features",
    "href": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#using-pywavelets-to-detect-horizontalvertical-features",
    "title": "Calculating the Ratio of Horizontal to Vertical Details in a Text Image",
    "section": "Using PyWavelets to Detect Horizontal/Vertical Features",
    "text": "Using PyWavelets to Detect Horizontal/Vertical Features\nThis algorithm is quite concise (compared to some of the other ones I’ve done).\nWe start by loading the image in grayscale.\n\n\nShow imports\n!pip install PyWavelets -qq\nimport cv2\nimport numpy as np\nimport pywt\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\npath = 'serif-76px.png'\n\n\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\nimg\n\n\n      ndarray (512, 512) show dataarray([[255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       ...,\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n\n\nWe then pass the image to pywt.dwt2 which according to the docs is a “2D Forward and Inverse Discrete Wavelet Transform” and returns “Approximation, horizontal detail, vertical detail and diagonal detail coefficients”.\nA couple of definitions (from Wikipedia):\n\nwavelet: A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a “brief oscillation”. A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\n\n\nHaar wavelet: In mathematics, the Haar wavelet is a sequence of rescaled “square-shaped” functions which together form a wavelet family or basis.\n\nA bit more context from Claude:\n\nFor images, we use a 2D version of the DWT. It applies the transform first to the rows and then to the columns of the image. This results in four subbands: approximation (LL), horizontal detail (LH), vertical detail (HL), and diagonal detail (HH).\n\n\nThe Haar wavelet is the simplest type of wavelet. It looks like a step function and is good at capturing abrupt transitions in the signal. In the context of images, it’s effective at detecting edges and sudden changes in intensity.\n\nSo it seems like the step-function behavior of the Haar wavelet is useful in finding horizontal and vertical edges and changes in intensity.\n\ncA, (cH, cV, cD) = pywt.dwt2(img, 'haar')\ncA.shape, cH.shape, cV.shape, cD.shape\n\n((256, 256), (256, 256), (256, 256), (256, 256))\n\n\nWe calculate the “energy” of horizontal and vertical details, where “energy” = sum of the absolute values of the details.\n\nh_energy = np.sum(np.abs(cH))\nv_energy = np.sum(np.abs(cV))\nh_energy, v_energy\n\n(143034.5, 169463.5)\n\n\nFinally, we calculate the ratio of the horizontal to vertical energy:\n\nratio = h_energy / (v_energy + 1e-5)\nratio\n\n0.8440431124788499"
  },
  {
    "objectID": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#visualizing-horizontal-and-vertical-details",
    "href": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#visualizing-horizontal-and-vertical-details",
    "title": "Calculating the Ratio of Horizontal to Vertical Details in a Text Image",
    "section": "Visualizing Horizontal and Vertical Details",
    "text": "Visualizing Horizontal and Vertical Details\nI’ll wrap the above functionality, plus visualization code provided by Claude, to visualize the vertical and horizontal features of an image:\n\n\nShow visualize_dwt2 function\ndef visualize_dwt2(path):\n    # Read the image\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\n    # Apply wavelet transform\n    coeffs = pywt.dwt2(img, 'haar')\n    _, (cH, cV, _) = coeffs\n\n    # Calculate energy of horizontal and vertical details\n    h_energy = np.sum(np.abs(cH))\n    v_energy = np.sum(np.abs(cV))\n\n    # Calculate ratio\n    ratio = h_energy / (v_energy + 1e-5)\n\n    # Visualize the results\n    plt.figure(figsize=(15, 10))\n\n    plt.subplot(221)\n    plt.imshow(img, cmap='gray')\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(222)\n    plt.imshow(cH, cmap='gray')\n    plt.title('Horizontal Details')\n    plt.axis('off')\n\n    plt.subplot(223)\n    plt.imshow(cV, cmap='gray')\n    plt.title('Vertical Details')\n    plt.axis('off')\n\n    plt.subplot(224)\n    energy_data = [h_energy, v_energy]\n    plt.bar(['Horizontal', 'Vertical'], energy_data)\n    plt.title('Energy Distribution')\n    plt.ylabel('Energy')\n\n    plt.suptitle(f'Wavelet Transform Analysis\\nH/V Ratio: {ratio:.2f}', fontsize=16)\n\n    plt.tight_layout()\n    plt.show()\n\n\nLooking at the baseline of each row of text in the “Horizontal Details” figure you can see how the serifs have a concentration of darker pixels, resulting in a horizontal/vertical ratio of 0.84.\n\nvisualize_dwt2('serif-76px.png')\n\n\n\n\n\n\n\n\nFor sans serif text, the baselines contain fewer dark regions in the “Horizontal Details” plot. The horizontal/vertical ratio is considerably lower at 0.62.\n\nvisualize_dwt2('display-76px.png')"
  },
  {
    "objectID": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#comparing-horizontalvertical-energy-ratio-for-different-images",
    "href": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#comparing-horizontalvertical-energy-ratio-for-different-images",
    "title": "Calculating the Ratio of Horizontal to Vertical Details in a Text Image",
    "section": "Comparing Horizontal/Vertical Energy Ratio for Different Images",
    "text": "Comparing Horizontal/Vertical Energy Ratio for Different Images\nI’ll wrap the ratio calculation in a function and calculate it for different text images.\n\n\nShow hv_ratio function\ndef hv_ratio(path):\n    # Read the image\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\n    # Apply wavelet transform\n    coeffs = pywt.dwt2(img, 'haar')\n    _, (cH, cV, _) = coeffs\n\n    # Calculate energy of horizontal and vertical details\n    h_energy = np.sum(np.abs(cH))\n    v_energy = np.sum(np.abs(cV))\n\n    # Calculate ratio\n    ratio = h_energy / (v_energy + 1e-5)\n\n    return ratio\n\n\n\nts = ['display', 'serif']\nszs = [18, 24, 36, 76, 240, 330, 420]\nres = []\n\nfor t in ts:\n  for sz in szs:\n    res.append([t, sz, hv_ratio(f\"{t}-{sz}px.png\")])\n\n\nres\n\n[['display', 18, 0.5991887037716151],\n ['display', 24, 0.681398182285936],\n ['display', 36, 0.6607329816912426],\n ['display', 76, 0.621916364155822],\n ['display', 240, 0.8254012445421878],\n ['display', 330, 0.5510498236874258],\n ['display', 420, 0.6070406043729197],\n ['serif', 18, 0.8503324244597174],\n ['serif', 24, 0.8057746917458858],\n ['serif', 36, 0.8090403181165737],\n ['serif', 76, 0.8440431124788499],\n ['serif', 240, 0.7480835327709404],\n ['serif', 330, 0.7622218104302562],\n ['serif', 420, 0.9064592756666484]]\n\n\nOn average the horizontal-to-vertical detail ratio is higher for images with serif texts than for images with sans serif text.\n\nres = pd.DataFrame(res, columns=['typeface', 'font-size', 'hv-ratio'])\nres.groupby('typeface')['hv-ratio'].agg(['mean', 'median'])\n\n\n\n  \n    \n\n\n  \n    \n      \n      mean\n      median\n    \n    \n      typeface\n      \n      \n    \n  \n  \n    \n      display\n      0.649533\n      0.621916\n    \n    \n      serif\n      0.817994\n      0.809040"
  },
  {
    "objectID": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#final-thoughts",
    "href": "posts/2024-08-28-typefaceclassifier-hv-ratio/index.html#final-thoughts",
    "title": "Calculating the Ratio of Horizontal to Vertical Details in a Text Image",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIt was exciting to see such elegant results with a single line of code (pywt.dwt2), exhibiting the power of wavelets, a topic that I’m not very familiar with.\nThe horizontal-to-vertical ratio between serifs and sans serifs is distinguishable, making this algorithm a good candidate for distinguishing between two typefaces.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html",
    "href": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html",
    "title": "Generating Full Text Search Keywords using claudette",
    "section": "",
    "text": "In this notebook, I’ll use claudette to generate keywords from fastbook Questionnaire questions, which will then be used for SQLite full-text keyword search.\nThis notebook is part of a series of blog posts for a project I’m working on called fastbookRAG in which I’m building a hybrid search + LLM pipeline to answer questions from the end-of-chapter Questionnaires in the freely available fastai textbook.\n\n\nShow imports\n!pip install claudette\nfrom claudette import *\nimport pandas as pd\n\n\n\nmodels # available in claudette\n\n('claude-3-opus-20240229',\n 'claude-3-5-sonnet-20240620',\n 'claude-3-haiku-20240307')\n\n\nI’ll be using the Claude-3.5 Sonnet API.\n\nmodel = models[1]\nmodel\n\n'claude-3-5-sonnet-20240620'"
  },
  {
    "objectID": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#testing-out-the-prompt",
    "href": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#testing-out-the-prompt",
    "title": "Generating Full Text Search Keywords using claudette",
    "section": "Testing out the Prompt",
    "text": "Testing out the Prompt\nI have already created keywords for the Chapter 1 Questionnaire questions, so I’ll use a few of them as examples in my prompt.\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat.use\n\nIn: 0; Out: 0; Total: 0\n\n\n\n\nShow prompt\nprompt = \"\"\"I am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\nDo not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\n\nNo yapping.\n\nExamples:\n\nquestion_text: Name five areas where deep learning is now the best in the world\nkeywords: \"deep learning, state of the art, best, world\"\n\nquestion_text: Why is it hard to use a traditional computer program to recognize images in a photo?\nkeywords: \"image, recognize, recognition, traditional, computer, program\"\n\nquestion_text: What were the two theoretical misunderstandings that held back the field of neural networks?\nkeywords: \"theoretical, misunderstandings, held, back, field, neural network\"\n\nquestion_text: {question_text}\nkeywords:\"\"\"\n\n\n\nformatted_prompt = prompt.format(question_text=\"Why is it hard to understand why a deep learning model makes a particular prediction?\")\nprint(formatted_prompt)\n\nI am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\nDo not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\n\nNo yapping. \n\nExamples:\n\nquestion_text: Name five areas where deep learning is now the best in the world\nkeywords: \"deep learning, state of the art, best, world\"\n\nquestion_text: Why is it hard to use a traditional computer program to recognize images in a photo?\nkeywords: \"image, recognize, recognition, traditional, computer, program\"\n\nquestion_text: What were the two theoretical misunderstandings that held back the field of neural networks?\nkeywords: \"theoretical, misunderstandings, held, back, field, neural network\"\n\nquestion_text: Why is it hard to understand why a deep learning model makes a particular prediction?\nkeywords:\n\n\n\nr = chat(formatted_prompt)\nr\n\n“deep learning, prediction, understanding, model, interpretability”\n\n\nid: msg_01GH67UgWfn8yTmufJt83Dyh\ncontent: [{‘text’: ‘“deep learning, prediction, understanding, model, interpretability”’, ‘type’: ‘text’}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {‘input_tokens’: 229, ‘output_tokens’: 16}\n\n\n\n\n\nr.content[0].text\n\n'\"deep learning, prediction, understanding, model, interpretability\"'\n\n\n\nchat.use\n\nIn: 229; Out: 16; Total: 245"
  },
  {
    "objectID": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#generating-keywords-for-one-chapter",
    "href": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#generating-keywords-for-one-chapter",
    "title": "Generating Full Text Search Keywords using claudette",
    "section": "Generating Keywords for One Chapter",
    "text": "Generating Keywords for One Chapter\nI’m always cautious when I use an API, as even with cheap per token costs, things can add up quickly. I’ll first test out for one chapter’s questions. A single question required a total of 245 tokens.\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat.use\n\nIn: 0; Out: 0; Total: 0\n\n\nThe full set of questions (and “gold standard” answers, along with some metadata) is available in this gist that I created. Note that this only includes the chapters covered in Part 1 of the fastai course.\n\n# get all questions\nurl = 'https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/804510c62151142ea940faad9ce132c8c85585de/fastbookRAG_evals.csv'\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nch1_df = df.query(\"chapter == 1\")\nch1_df.shape\n\n(33, 5)\n\n\n\nkeyword_results = []\nfor row in ch1_df['question_text']:\n  formatted_prompt = prompt.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results.append(r.content[0].text)\n\nThese tokens add up fast! This got up to 100k+ tokens used. I think the issue is that each time the previous messages are included in the chat. I would expect the token count to be closer to 8000 (245 tokens used for one question times 33 questions).\n\n245*33\n\n8085\n\n\n\nkeyword_results[-5:]\n\n['\"architecture, neural network, model structure, design\"',\n '\"segmentation, image processing, object detection, pixel-level classification\"',\n '\"y_range, output range, regression, model prediction\"',\n '\"hyperparameters, model configuration, tuning, machine learning\"',\n '\"AI implementation, failure prevention, organizational strategy, best practices\"']\n\n\nI’ll do chapter one again but this time I’ll create a new Chat object for each question so I don’t rack up the tokens so quickly.\n\nkeyword_results2 = []\ntokens = 0\nfor row in ch1_df['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = prompt.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results2.append(r.content[0].text)\n  tokens += chat.use.total\n\nThe token usage is much better!\n\ntokens\n\n8048\n\n\nAlthough it definitely comes up with different keywords when it doesn’t use the accumulated chat history.\n\nkeyword_results2[-5:]\n\n['\"architecture, definition, structure, design\"',\n '\"segmentation, division, partitioning, classification\"',\n '\"y_range, purpose, usage, application\"',\n '\"hyperparameters, machine learning, model configuration, tuning\"',\n '\"AI failures, organization, best practices, risk mitigation, implementation\"']"
  },
  {
    "objectID": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#improving-the-prompt",
    "href": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#improving-the-prompt",
    "title": "Generating Full Text Search Keywords using claudette",
    "section": "Improving the Prompt",
    "text": "Improving the Prompt\nSomething that I want to make sure Claude does is prefer single keywords with commas where possible. Currently, it is grouping words together like \"AI failures\" or \"risk mitigation\" (which I want as \"AI, failures\", and \"risk, mitigation\").\n\n\nShow prompt\nprompt = \"\"\"I am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\nDo not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\nTry to use single-word keywords when possible.\n\nNo yapping.\n\nExamples:\n\nquestion_text: Name five areas where deep learning is now the best in the world\nkeywords: \"deep, learning, best, world\"\n\nquestion_text: Why is it hard to use a traditional computer program to recognize images in a photo?\nkeywords: \"image, recognize, recognition, traditional, computer, program\"\n\nquestion_text: What were the two theoretical misunderstandings that held back the field of neural networks?\nkeywords: \"theoretical, misunderstandings, held, back, field, neural, network\"\n\nquestion_text: {question_text}\nkeywords:\"\"\"\n\n\n\nformatted_prompt = prompt.format(question_text=\"\"\"What's the best way to avoid failures when using AI in an organization?\"\"\")\nprint(formatted_prompt)\n\nI am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\nDo not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\nTry to use single-word keywords when possible.\n\nNo yapping. \n\nExamples:\n\nquestion_text: Name five areas where deep learning is now the best in the world\nkeywords: \"deep, learning, best, world\"\n\nquestion_text: Why is it hard to use a traditional computer program to recognize images in a photo?\nkeywords: \"image, recognize, recognition, traditional, computer, program\"\n\nquestion_text: What were the two theoretical misunderstandings that held back the field of neural networks?\nkeywords: \"theoretical, misunderstandings, held, back, field, neural, network\"\n\nquestion_text: What's the best way to avoid failures when using AI in an organization?\nkeywords:\n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat(formatted_prompt)\n\n“AI, failures, avoid, organization, best practices”\n\n\nid: msg_01FAMozL8CxeSu2ydhx1KdbC\ncontent: [{‘text’: ‘“AI, failures, avoid, organization, best practices”’, ‘type’: ‘text’}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {‘input_tokens’: 236, ‘output_tokens’: 15}\n\n\n\n\n\nkeyword_results3 = []\ntokens = 0\nfor row in ch1_df['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = prompt.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results3.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens\n\n8265\n\n\nThe keywords look promising. Their true effectiveness will be tested when used in full-text search. I’ll adjust the prompt as needed based on those results.\n\nkeyword_results3[-10:]\n\n['\"metric, loss, difference, measurement, evaluation\"',\n '\"pretrained, models, help, benefits\"',\n '\"head, model, neural, network\"',\n '\"CNN, layers, features, early, later\"',\n '\"image, models, photos, usefulness\"',\n '\"architecture, definition, structure, design\"',\n '\"segmentation, division, partition, categorization\"',\n '\"y_range, purpose, usage, necessity\"',\n '\"hyperparameters, machine, learning, parameters, model, configuration\"',\n '\"AI, failures, avoid, organization, best practices\"']\n\n\nSo far I have used 48 cents in API credits."
  },
  {
    "objectID": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#generating-keywords-for-all-chapters",
    "href": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#generating-keywords-for-all-chapters",
    "title": "Generating Full Text Search Keywords using claudette",
    "section": "Generating Keywords for All Chapters",
    "text": "Generating Keywords for All Chapters\nIt took about 43 minutes and 20 cents to generate keywords for 220 questions:\n\nkeywords = []\ntokens = 0\nfor row in df['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = prompt.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keywords.append(r.content[0].text)\n  tokens += chat.use.total\n\n\ntokens\n\n55245\n\n\n\n245*220 # estimated tokens used for 220 questions\n\n53900\n\n\n\nlen(keywords)\n\n220\n\n\nSpot-checking the generated keywords and they look okay!\n\nkeywords[:5]\n\n['\"deep, learning, requirements, math, data, computers, PhD\"',\n '\"deep, learning, areas, best, world\"',\n '\"artificial, neuron, device, first, principle\"',\n '\"parallel, distributed, processing, PDP, requirements, book\"',\n '\"misunderstandings, neural, networks, theoretical, setbacks\"']\n\n\n\nkeywords[-5:]\n\n['\"column, pixels, color, dim, plot, represent\"',\n '\"bad, training, color_dim, why\"',\n '\"batch, normalization, trainable, parameters, layer\"',\n '\"batch, normalization, statistics, training, validation\"',\n '\"batch, normalization, generalization, models\"']"
  },
  {
    "objectID": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#final-thoughts",
    "href": "posts/2024-08-25-fastbookRAG-claude-keywords/index.html#final-thoughts",
    "title": "Generating Full Text Search Keywords using claudette",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nUsing claudette to generate keywords for my fastbookRAG eval questions was really straightforward. I’ll use these keywords for full-text search to answer the questions and plan to revisit and refine the prompt based on the quality of the context retrieved.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html",
    "href": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html",
    "title": "Calculating the Squareness of Letters in an Image",
    "section": "",
    "text": "In this notebook, I’ll walk through an algorithm suggested by Claude to calculate the “squareness” of letters in a text image. This measure is calculated as:\n\n\\[\\frac{\\text{min(width, height)}}{\\text{max(width, height)}}\\]\n\nWhere \\(\\text{width}\\) and \\(\\text{height}\\) are the width and height of the rectangular bounding box around the letter.\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\n\n\nShow imports\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport cv2\n\n\nThis is a very straightforward (code-wise) algorithm compared to some of the other ones I’ve worked through. In fact, this algorithm is just a slight modification to the aspect ratio algorithm. The aspect ratio is the ratio of bounding rectangle width to bounding rectangle length. This “squareness” ratio is the minimum dimension divided by the maximum dimension."
  },
  {
    "objectID": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html#squareness-ratio-algorithm",
    "href": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html#squareness-ratio-algorithm",
    "title": "Calculating the Squareness of Letters in an Image",
    "section": "Squareness Ratio Algorithm",
    "text": "Squareness Ratio Algorithm\nWe start by loading the image an calculating the binary (as always).\n\npath = 'serif-36px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\nWe then find the contours in the image:\n\ncontours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\nFor each contour, we create a bounding rectangle and calculate the squareness ratio:\n\nsquareness_ratio = []\n\nfor contour in contours:\n  x, y, w, h = cv2.boundingRect(contour)\n  squareness = min(w, h) / max(w, h)\n  squareness_ratio.append(squareness)\n\nThen calculate the average ratio across all contours:\n\naverage_squareness = np.median(squareness_ratio) if squareness_ratio else 0\naverage_squareness\n\n0.8333333333333334\n\n\nAnd that’s it!\nSquareness ratios that are close to 1 represent letter forms that are closer to a perfect square. Ratios closer to 0 represent letters that are closer to wide or tall rectangles."
  },
  {
    "objectID": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html#squareness-ratio-for-different-images",
    "href": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html#squareness-ratio-for-different-images",
    "title": "Calculating the Squareness of Letters in an Image",
    "section": "Squareness Ratio for Different Images",
    "text": "Squareness Ratio for Different Images\nI’ll wrap the squareness ratio algorithm in a function and then calculate the median ratio for display versus serif images.\n\ndef squareness_ratio(image_path):\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Threshold the image\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Find contours\n    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    squareness_ratios = []\n\n    for contour in contours:\n        # Get bounding rectangle\n        x, y, w, h = cv2.boundingRect(contour)\n\n        # Calculate squareness (ratio of width to height)\n        # We use min/max to ensure the ratio is always between 0 and 1\n        squareness = min(w, h) / max(w, h)\n\n        squareness_ratios.append(squareness)\n\n    # Calculate average squareness\n    squareness_ratio = np.median(squareness_ratios) if squareness_ratios else 0\n    return squareness_ratio\n\nThe display text is more rectangular than the serif text so I expect it to have a lower squareness ratio.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n\n# Display the first image\nax1.imshow(cv2.imread('display-76px.png', cv2.IMREAD_GRAYSCALE), cmap='gray')\nax1.axis('off')  # Hide axes\nax1.set_title('display (narrower)')\n\n# Display the second image\nax2.imshow(cv2.imread('serif-76px.png', cv2.IMREAD_GRAYSCALE), cmap='gray')\nax2.axis('off')  # Hide axes\nax2.set_title('serif (squarish)')\n\n# Adjust the layout and display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAcross 7 different font sizes, the median squareness ratio for display images is less than serif images.\n\nts = ['display', 'serif']\nszs = [18, 24, 36, 76, 240, 330, 420]\nres = []\n\nfor t in ts:\n    for sz in szs:\n        image_path = f\"{t}-{sz}px.png\"\n        sr = squareness_ratio(image_path)\n        res.append([t, sz, sr])\n\nres = pd.DataFrame(res, columns=['typeface', 'font-size', 'squareness-ratio'])\nres.groupby('typeface')['squareness-ratio'].agg(['mean', 'median'])\n\n\n\n  \n    \n\n\n  \n    \n      \n      mean\n      median\n    \n    \n      typeface\n      \n      \n    \n  \n  \n    \n      display\n      0.763897\n      0.769231\n    \n    \n      serif\n      0.749344\n      0.823529\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nIf we limit the font sizes to 18-76px, the difference is larger:\n\n(\n  res\n    .query(\"`font-size` <= 76 and `font-size` >= 18\")\n    .groupby('typeface')['squareness-ratio']\n    .agg(['mean', 'median'])\n )\n\n\n\n  \n    \n\n\n  \n    \n      \n      mean\n      median\n    \n    \n      typeface\n      \n      \n    \n  \n  \n    \n      display\n      0.762430\n      0.759615\n    \n    \n      serif\n      0.812604\n      0.823927"
  },
  {
    "objectID": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html#final-thoughts",
    "href": "posts/2024-08-31-typefaceclassifier-squareness-ratio/index.html#final-thoughts",
    "title": "Calculating the Squareness of Letters in an Image",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nSimilar to aspect ratio the squareness ratio is a good candidate for distinguishing between typefaces. Even though these algorithms are similar, I’ll keep them both in play for now and see how they perform on a larger and more diverse dataset of text images as they offer different insights:\n\nSquareness ratio:\n\nLow for both short/wide and tall/narrow letters\nConsistent for comparing square vs. non-square shapes\n\nAspect ratio:\n\n< 1 for tall/narrow letters\n> 1 for short/wide letters\nDistinguishes between short/wide letters and tall/narrow letters\n\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "",
    "text": "Here’s the video walkthrough of this notebook\nIn this notebook, I’ll walkthrough the code and concepts introduced in Chapter 8 of the fastai textbook. This chapter explores the various ways fastai can handle a collaborative filtering problem."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "What is Collaborative Filtering?",
    "text": "What is Collaborative Filtering?\nIn a situation where two variables have some numeric relationship, such as users rating movies, collaborative filtering is a solution for predicting ratings that are blank, based on existing data.\nA machine learning model for collaborative filtering implicitly learns the answers to the following questions:\n\nWhat types of movies do users like?\nWhat are characteristics of each movie?\n\n\nLatent Factors\nFor the movie rating example, latent factors are the “types of movies” users like and “characteristics” of each movie. Latent factors are not explicitly categorical, they are numeric values, but they represent the implicit categories of each variable.\nThe reason that they are implicit categories, is that the model learns the ideal latent factors as it trains on the dataset, observing patterns between users and their movie ratings."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "MovieLens 100K Dataset",
    "text": "MovieLens 100K Dataset\nThe dataset used to train the collaborative filtering model is a subset (100,000 rows in length) of the full MovieLens dataset which is 25 million rows.\n\n# a first look at the data\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\n\n\nThe dataset lists users, movies, ratings and a timestamp.\n\n# load the data into a DataFrame\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nBefore we get into the training, I want to familiarize myself with how the data is structured. There are 943 unique users and 1682 unique movies.\n\n# how many unique users and movies are there?\nlen(ratings['user'].unique()), len(ratings['movie'].unique())\n\n(943, 1682)\n\n\nThe movie IDs are a consecutive range from 1 to 1682 and the user IDs are a consecutive range from 1 to 943. The movie ratings range from 1 to 5.\n\n# are movie IDs consecutive?\n(ratings['movie'].sort_values().unique() == np.array(range(1,1683))).sum()\n\n1682\n\n\n\n# are user IDs consecutive?\n(ratings['user'].sort_values().unique() == np.array(range(1,944))).sum()\n\n943\n\n\n\n# what is the range of ratings?\nratings['rating'].min(), ratings['rating'].max()\n\n(1, 5)\n\n\nTo visualize the problem we are trying to solve with collaborative filtering the book recommended that we observe a cross-tabulation of the data because then we can see that what we are trying to predict are the null values between user and movie, and what we are training our model on are the non-null ratings in the dataset.\nThe model will learn something about user 2 and movie 2 in order to predict what rating that user would give that movie. That “something” the model will learn are the latent factors for users and latent factors for movies.\n\n# view crosstab of users and movies with rating values\nct = pd.crosstab(ratings['user'], ratings['movie'], ratings['rating'],aggfunc='mean')\nct\n\n\n\n\n\n  \n    \n      movie\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      ...\n      1643\n      1644\n      1645\n      1646\n      1647\n      1648\n      1649\n      1650\n      1651\n      1652\n      1653\n      1654\n      1655\n      1656\n      1657\n      1658\n      1659\n      1660\n      1661\n      1662\n      1663\n      1664\n      1665\n      1666\n      1667\n      1668\n      1669\n      1670\n      1671\n      1672\n      1673\n      1674\n      1675\n      1676\n      1677\n      1678\n      1679\n      1680\n      1681\n      1682\n    \n    \n      user\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      5.0\n      3.0\n      4.0\n      3.0\n      3.0\n      5.0\n      4.0\n      1.0\n      5.0\n      3.0\n      2.0\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n      3.0\n      4.0\n      5.0\n      4.0\n      1.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      2.0\n      4.0\n      1.0\n      3.0\n      3.0\n      5.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      3.0\n      4.0\n      3.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      939\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      940\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      5.0\n      3.0\n      NaN\n      NaN\n      4.0\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      941\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      942\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      943\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      4.0\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n943 rows × 1682 columns\n\n\n\nInstead of movie IDs, we can get the movie titles into a DataFrame and add that column to our ratings DataFrame by merging the two.\nThe movie titles are in the u.item file, which is pipe-delimited, with latin-1 encoding. The u.item file has 24 columns, but we only want the first two which have the movie id and the title.\n\n# get movie titles\nmovies = pd.read_csv(\n    path/'u.item', \n    delimiter='|', \n    encoding='latin-1', \n    usecols=(0,1), \n    names=('movie', 'title'), \n    header=None)\n\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nThe movies DataFrame and the ratings DataFrame are merged using the movie column as the key to match title in movies to movie ID in ratings. By default, pandas uses as the key whichever column name exists in both DataFrames.\n\n# get the user ratings by title\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n\n\nDoes this change the uniqueness of the data? Yes it actually does! There are 1682 unique movie IDs but there are only 1664 unique movie titles. 18 movies have associated with it duplicate titles.\n\n# how many unique titles and movies are there?\nlen(ratings['title'].unique()), len(ratings['movie'].unique())\n\n(1664, 1682)\n\n\nThe .duplicated DataFrame method takes a list of columns for the subset parameter, finds values in those columns that are duplicated, and returns a boolean Series with a True value at indexes with duplicates. I use that as a mask and pass it to the movies DataFrame to view those duplicate titles.\n\n# 18 movies have duplicate titles\nmovies[movies.duplicated(subset=['title'])]\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      267\n      268\n      Chasing Amy (1997)\n    \n    \n      302\n      303\n      Ulee's Gold (1997)\n    \n    \n      347\n      348\n      Desperate Measures (1998)\n    \n    \n      499\n      500\n      Fly Away Home (1996)\n    \n    \n      669\n      670\n      Body Snatchers (1993)\n    \n    \n      679\n      680\n      Kull the Conqueror (1997)\n    \n    \n      864\n      865\n      Ice Storm, The (1997)\n    \n    \n      880\n      881\n      Money Talks (1997)\n    \n    \n      1002\n      1003\n      That Darn Cat! (1997)\n    \n    \n      1256\n      1257\n      Designated Mourner, The (1997)\n    \n    \n      1605\n      1606\n      Deceiver (1997)\n    \n    \n      1606\n      1607\n      Hurricane Streets (1998)\n    \n    \n      1616\n      1617\n      Hugo Pool (1997)\n    \n    \n      1624\n      1625\n      Nightwatch (1997)\n    \n    \n      1649\n      1650\n      Butcher Boy, The (1998)\n    \n    \n      1653\n      1654\n      Chairman of the Board (1998)\n    \n    \n      1657\n      1658\n      Substance of Fire, The (1996)\n    \n    \n      1679\n      1680\n      Sliding Doors (1998)\n    \n  \n\n\n\n\nfastai has a built-in constructor for DataLoaders specific to collaborative filtering. I pass it the ratings DataFrame, specify that the items are the titles, and that I want 64 rows in each batch.\n\n# create DataLoaders\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      297\n      Indian Summer (1996)\n      4\n    \n    \n      1\n      934\n      Grease (1978)\n      4\n    \n    \n      2\n      846\n      Money Train (1995)\n      2\n    \n    \n      3\n      479\n      Crumb (1994)\n      3\n    \n    \n      4\n      499\n      Local Hero (1983)\n      4\n    \n    \n      5\n      455\n      Adventures of Priscilla, Queen of the Desert, The (1994)\n      3\n    \n    \n      6\n      943\n      Rumble in the Bronx (1995)\n      4\n    \n    \n      7\n      374\n      Dead Poets Society (1989)\n      1\n    \n    \n      8\n      533\n      Deer Hunter, The (1978)\n      3\n    \n    \n      9\n      846\n      Vanya on 42nd Street (1994)\n      2\n    \n  \n\n\n\n\nMatrices for Latent Factors\nWe need the model to find relationships between users and movies. And we need to give the model something concrete and numeric to represent those relationships. We will give it latent factor matrices.\nIn this example, they have chosen to use 5 latent factors for movies and 5 latent factors for users. We represent these latent factors by creating a matrix of random values.\nThe user latent factors will have 944 rows, one for each user including a null user, and 5 columns, one for each latent factor. The movies latent factors will have 1665, one for each movie including a null movie, and 5 columns.\n\n# user and title classes contain '#na#'\nL(dls.classes['title']),L(dls.classes['user'])\n\n((#1665) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...],\n (#944) ['#na#',1,2,3,4,5,6,7,8,9...])\n\n\n\n# define dimensions for users and movies latent factor matrices\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\nn_users, n_movies, n_factors\n\n(944, 1665, 5)\n\n\n\n# build users and movies latent factor matrices\nuser_factors = torch.randn(n_users,n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\nuser_factors.shape, movie_factors.shape\n\n(torch.Size([944, 5]), torch.Size([1665, 5]))\n\n\n\n\nEmbeddings instead of One Hot Encoded Matrix Multiplication\nOkay so how do we use these latent factor matrices?\nFor each row in our batch, we have a user ID and a movie ID. We need to get the latent factors for each of those and calculate the dot product, in order to predict our rating. As it adjusts the latent factors during the training loop, the predictions will get better.\nI’ll grab one batch from the dls DataLoaders object and illustrate an example prediction calculation. Each independent variable, x, is a tensor with [user, movie]. Each dependent variable, y, is a tensor with [rating].\n\nx,y = dls.one_batch()\nx.shape, y.shape\n\n(torch.Size([64, 2]), torch.Size([64, 1]))\n\n\n\nx[0], y[0]\n\n(tensor([466, 614]), tensor([3], dtype=torch.int8))\n\n\n\ntype(x[0]), type(y[0])\n\n(torch.Tensor, torch.Tensor)\n\n\nI determined the order of the x values by looking at the maximum value in each column, specifying axis=0. The movie IDs go up to 1644, so a max value of 1608 means that the movie is the second value in each tensor.\n\nx.max(axis=0)\n\ntorch.return_types.max(values=tensor([ 935, 1642]), indices=tensor([24,  3]))\n\n\nI get the latent factors for the user and movie in the first batch item.\n\nu = user_factors[x[0][0]]\nu\n\ntensor([-0.6595, -0.3355,  1.0491,  1.1764,  0.8750])\n\n\n\nm = movie_factors[x[0][1]]\nm\n\ntensor([-0.1751, -0.5016,  0.6298,  0.2370, -0.7902])\n\n\nI calculate the dot product of the two vectors, which is the sum of the element-wise product.\n\npred = (u * m).sum()\npred\n\ntensor(0.5320)\n\n\nI pass it through sigmoid_range to get a value between 0 and 5. Sigmoid outputs a value between 0 and 1, and sigmoid_range scales and shifts that function to fit the specified range. The output is the prediction for the rating that this user would give this movie.\n\npred = sigmoid_range(pred, 0, 5)\npred\n\ntensor(3.1497)\n\n\nSince the prediction and the target are a single numeric value, we’ll use Mean Squared Error loss. For a single value, the loss is the squared error. For a batch, the mean would be calculated.\n\nloss = (pred - y[0].item()) ** 2\nloss\n\ntensor(0.0224)\n\n\n\n\nBuilding a Collaborative Filtering Model from Scratch\nI’ll create a DotProduct class which builds an Embedding to store latent factor matrices for users and movies, and calculates the prediction in its forward method using the dot product of the user and movie latent factors.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\nI’ll illustrate how this model operates by coding through the calculation of predictions for one batch.\nI create Embeddings for users and movies. Their shape corresponds to the number of users and movies and the number of latent factors.\n\nuser_factors = Embedding(n_users, n_factors)\nmovie_factors = Embedding(n_movies, n_factors)\nuser_factors, movie_factors\n\n(Embedding(944, 5), Embedding(1665, 5))\n\n\nThe weight attribute holds the latent factor values, which are parameters whose gradient can be calculated. The values are from a normal distribution with mean 0 and variance 1.\n\nuser_factors.weight.shape\n\ntorch.Size([944, 5])\n\n\n\nuser_factors.weight\n\nParameter containing:\ntensor([[-0.0066, -0.0111,  0.0091,  0.0056,  0.0075],\n        [-0.0138, -0.0014,  0.0189,  0.0028, -0.0166],\n        [ 0.0149,  0.0053, -0.0153,  0.0078, -0.0119],\n        ...,\n        [-0.0051, -0.0117,  0.0170, -0.0102, -0.0044],\n        [ 0.0037, -0.0084,  0.0042, -0.0049,  0.0186],\n        [ 0.0199, -0.0194,  0.0044,  0.0012,  0.0084]], requires_grad=True)\n\n\nThe user_factors weight has the shape 944 rows by 5 columns. You can see that they are a tensor with requires_grad equals True.\n\nmovie_factors.weight\n\nParameter containing:\ntensor([[-0.0017, -0.0051,  0.0065,  0.0050, -0.0095],\n        [-0.0065, -0.0158,  0.0062, -0.0145, -0.0087],\n        [ 0.0067,  0.0111,  0.0059, -0.0003,  0.0061],\n        ...,\n        [-0.0012,  0.0002, -0.0088, -0.0022, -0.0152],\n        [-0.0053, -0.0058, -0.0074, -0.0033, -0.0171],\n        [ 0.0030,  0.0031, -0.0037, -0.0023,  0.0157]], requires_grad=True)\n\n\nThe movie_factors weight has the shape 1665 rows by 5 columns. And here you can see it is a tensor as well with requires_grad equals True.\n\nmovie_factors.weight.shape\n\ntorch.Size([1665, 5])\n\n\nIn my batch, the 0th column of the dependent variable x holds user indexes. I pass that to user_factors and receive a tensor with those users’ latent factors. Column index 1 holds the movie indexes, I pass that to movie_factors and receive a tensor with those movies’ latent factors.\n\nusers = user_factors(x[:,0])\nusers[:5]\n\ntensor([[ 0.0029,  0.0042, -0.0093,  0.0023, -0.0053],\n        [ 0.0029,  0.0008,  0.0193,  0.0082,  0.0117],\n        [-0.0025,  0.0070, -0.0144, -0.0193,  0.0086],\n        [ 0.0103,  0.0028,  0.0172,  0.0110,  0.0084],\n        [-0.0087, -0.0109,  0.0062, -0.0018, -0.0012]],\n       grad_fn=<SliceBackward>)\n\n\n\nmovies = movie_factors(x[:,1])\nmovies[:5]\n\ntensor([[ 0.0011, -0.0009,  0.0114,  0.0017,  0.0033],\n        [ 0.0049, -0.0019,  0.0175,  0.0027, -0.0014],\n        [-0.0047, -0.0026,  0.0032,  0.0028, -0.0146],\n        [-0.0103, -0.0024,  0.0057, -0.0141, -0.0080],\n        [ 0.0099,  0.0113,  0.0022,  0.0123,  0.0096]],\n       grad_fn=<SliceBackward>)\n\n\nI take the dot product and pass it through a sigmoid_range and the calculate the predictions for the batch.\n\npreds = sigmoid_range((users * movies).sum(dim=1), 0, 5.5)\npreds, preds.shape\n\n(tensor([2.7498, 2.7505, 2.7497, 2.7497, 2.7497, 2.7501, 2.7495, 2.7506, 2.7499,\n         2.7502, 2.7503, 2.7506, 2.7501, 2.7498, 2.7497, 2.7507, 2.7498, 2.7497,\n         2.7499, 2.7500, 2.7499, 2.7500, 2.7502, 2.7501, 2.7502, 2.7499, 2.7500,\n         2.7499, 2.7499, 2.7501, 2.7503, 2.7497, 2.7500, 2.7498, 2.7497, 2.7496,\n         2.7502, 2.7502, 2.7501, 2.7498, 2.7501, 2.7502, 2.7500, 2.7501, 2.7506,\n         2.7500, 2.7498, 2.7499, 2.7501, 2.7502, 2.7502, 2.7501, 2.7498, 2.7501,\n         2.7501, 2.7499, 2.7499, 2.7499, 2.7498, 2.7502, 2.7499, 2.7498, 2.7494,\n         2.7499], grad_fn=<AddBackward0>), torch.Size([64]))\n\n\nThat’s what the DotProduct model will return. I can then take the mean squared error and calculate the loss value, which I can then call backward on, to calculate the gradients for the latent factors. I can then multiply them by the learning rate, and add them to the weights, and repeat the training loop.\n\nloss = ((preds - y) ** 2).mean()\nloss\n\ntensor(2.0156, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\n\n\nuser_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\nmovie_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nWe see here that the gradients are small but not all of them are zero:\n\nuser_factors.weight.grad.sum(), movie_factors.weight.grad.sum()\n\n(tensor(-0.0108), tensor(-0.0026))\n\n\n\nTraining the Model\nThere are five different models that I will build, from simple to complex.\nModel 1 will not use sigmoid_range for the dot product prediction calculation. Instead, I will get a value that is normally distributed with a mean of zero and a variance of 1. Model 2 will pass the predictions through sigmoid_range to get an output between 0 and 5.5. Model 3 will add a bias parameter to the dot product prediction, so that we can establish some baseline rating of each movie that’s independent of a particular latent factor. Model 4 will introduce weight decay in order to better generalize our model, and in Model 5 I’ll implement a custom class instead of using the built-in PyTorch Embedding class.\n\n\n\nModel\nsigmoid_range\nBias\nWeight Decay\nCustom Embedding\n\n\n\n\n1\nN\nN\nN\nN\n\n\n2\nY\nN\nN\nN\n\n\n3\nY\nY\nN\nN\n\n\n4\nY\nY\nY\nN\n\n\n5\nY\nY\nY\nY\n\n\n\n\nModel 1: No sigmoid_range\nThe DotProduct class will initialize an Embedding for users and movies with random values from a normal distribution with mean 0 and variance 1. In the forward method, called during the prediction step of the training loop, the latent factors for the batch’s users and movies are accessed from the corresponding Embedding and the dot product is calculated and returned as the prediction.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return (users * movies).sum(dim=1)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.339355\n      1.274582\n      00:06\n    \n    \n      1\n      1.039260\n      1.061794\n      00:06\n    \n    \n      2\n      0.945793\n      0.954731\n      00:06\n    \n    \n      3\n      0.826578\n      0.871694\n      00:06\n    \n    \n      4\n      0.752750\n      0.858231\n      00:06\n    \n  \n\n\n\nThe average prediction error on the validation ratings is the square root of the final validation loss.\n\nmath.sqrt(learn.recorder.final_record[1])\n\n0.9264077107145671\n\n\nHere are some predictions on the validation set ratings:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      554\n      37\n      4\n      4.181926\n    \n    \n      1\n      561\n      811\n      4\n      3.008064\n    \n    \n      2\n      503\n      298\n      5\n      4.451642\n    \n    \n      3\n      380\n      581\n      4\n      3.474877\n    \n    \n      4\n      666\n      422\n      4\n      4.054492\n    \n    \n      5\n      444\n      933\n      2\n      3.940120\n    \n    \n      6\n      368\n      1612\n      3\n      2.864129\n    \n    \n      7\n      537\n      457\n      3\n      2.955165\n    \n    \n      8\n      224\n      1535\n      1\n      2.940819\n    \n  \n\n\n\n\n\nModel 2 - with sigmoid_range\nIn this model, I’ll force the predictions to fall within the range of actual ratings. The book recommends, based on what they’ve experienced, using a maximum rating for predictions that is slightly larger than the maximum ground truth rating. The range of predictions we’ll use is 0 to 5.5.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.009657\n      0.977510\n      00:07\n    \n    \n      1\n      0.881551\n      0.891973\n      00:06\n    \n    \n      2\n      0.686247\n      0.853506\n      00:07\n    \n    \n      3\n      0.487054\n      0.857519\n      00:06\n    \n    \n      4\n      0.374934\n      0.862651\n      00:06\n    \n  \n\n\n\n\nmath.sqrt(learn.recorder.final_record[1])\n\n\nlearn.show_results()\n\nThe valid loss starts off lower, but by the end of the training, I don’t see an improvement. In fact, the valid loss starts to increase at the end. This is an indication that overfitting is taking place. This will be addressed in Model 4.\n\n\nModel 3 - Add bias\nBut first, let’s look at Model 3, which adds a bias parameter to the predictions, which provides a baseline rating for each movie independent of the weights related to the different latent factors.\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\n\nIn addition to initializing new Embeddings for bias, the dot product is first kept in matrix form by passing keepdim=True to the sum method. The biases are then added on afterwhich the result is passed through sigmoid_range. Here’s a illustrative example for how keepdim=True affects the dot product:\n\na = Tensor([[1,2,3], [4,5,6]])\n(a * a).sum(dim=1).shape, (a * a).sum(dim=1, keepdim=True).shape\n\n(torch.Size([2]), torch.Size([2, 1]))\n\n\nThe first tensor is a 2-vector, whereas the second tensor is a 2 x 1 matrix.\n\nLet’s train Model 3 and view the results!\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.952214\n      0.912603\n      00:07\n    \n    \n      1\n      0.825348\n      0.845193\n      00:07\n    \n    \n      2\n      0.618910\n      0.852381\n      00:07\n    \n    \n      3\n      0.406514\n      0.875637\n      00:07\n    \n    \n      4\n      0.293729\n      0.882840\n      00:07\n    \n  \n\n\n\nThe initial validation loss is lower than the previous trainings, but Model 3 is overfitting even more than Model 2. It’s time to introduce weight decay.\n\n\nModel 4 - Use Weight Decay\nSmaller weights lead to a smoother function which corresponds to fewer inflection points, leading to a better generalization of the model. Larger weights lead to a sharper function, corresponding to more inflection points which overfit the training data.\nThe text uses the basic example of a parabola to illustrate. As the weight a increases, the function becomes narrower, with a sharper trough.\n\nHTML('<iframe src=\"https://www.desmos.com/calculator/uog6rvyubg\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>')\n\n\n\n\nWe will intentionally increase the gradients so that the weights are stepped with larger increments toward a smaller value.\nThis corresponds to an intentionally larger loss value calculated for each batch:\nloss_with_wd = loss + wd * (parameters**2).sum()\nThe parameters are squared to ensure a positive value. Taking the derivative of the loss function means taking the derivative of the following function:\nwd * (parameters**2).sum()\nWhich results in:\n2 * wd * parameters\nInstead of multiplying by 2, we can just use twice the weight decay value as wd. I’ll use a weight decay of 0.1 as they do in the text.\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.939321\n      0.937423\n      00:07\n    \n    \n      1\n      0.851871\n      0.855584\n      00:07\n    \n    \n      2\n      0.720202\n      0.815807\n      00:07\n    \n    \n      3\n      0.630149\n      0.806268\n      00:07\n    \n    \n      4\n      0.491224\n      0.807063\n      00:07\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      877\n      1538\n      3\n      3.708797\n    \n    \n      1\n      601\n      1285\n      1\n      2.832736\n    \n    \n      2\n      292\n      1147\n      2\n      3.675529\n    \n    \n      3\n      132\n      400\n      4\n      3.504542\n    \n    \n      4\n      405\n      1614\n      1\n      1.930779\n    \n    \n      5\n      655\n      458\n      3\n      3.274209\n    \n    \n      6\n      453\n      60\n      4\n      3.864617\n    \n    \n      7\n      629\n      1498\n      4\n      3.598821\n    \n    \n      8\n      724\n      580\n      4\n      3.921505\n    \n  \n\n\n\n\n\nModel 5 - custom Embedding class\nThe final model I’ll train does not have a fundamentally different component than the other four. Instead of using the built-in Embedding PyTorch class, the text has us write our own class.\nOptimizers get a module’s parameters by calling the parameters method. We have to wrap parameters in nn.Parameter for them to be recognized as such. This class also calls requires_grad_ for us.\nI’ll replace each Embedding with a tensor filled with random values from a normal distribution with mean 0 and variance 0.01.\n\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0,0.01))\n\n\ntorch.zeros(3,4)\n\ntensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n\n\ntorch.zeros(3,4).normal_(0,0.01)\n\ntensor([[-0.0134,  0.0098, -0.0124, -0.0032],\n        [ 0.0056,  0.0071,  0.0005,  0.0014],\n        [-0.0236, -0.0024, -0.0060,  0.0017]])\n\n\nI redefine the DotProductBias model using the create_params method instead of Embedding, and train the model.italicized text\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = create_params([n_users, n_factors])\n    self.user_bias = create_params([n_users])\n    self.movie_factors = create_params([n_movies, n_factors])\n    self.movie_bias = create_params([n_movies])\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors[x[:,0]]\n    movies = self.movie_factors[x[:,1]]\n    res = (users * movies).sum(dim=1)\n    res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.934845\n      0.933218\n      00:08\n    \n    \n      1\n      0.841111\n      0.859618\n      00:08\n    \n    \n      2\n      0.730065\n      0.820388\n      00:08\n    \n    \n      3\n      0.599684\n      0.807086\n      00:08\n    \n    \n      4\n      0.484760\n      0.807552\n      00:08\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      487\n      6\n      3\n      2.994869\n    \n    \n      1\n      54\n      349\n      3\n      2.511689\n    \n    \n      2\n      501\n      1252\n      4\n      4.130728\n    \n    \n      3\n      244\n      861\n      4\n      2.314526\n    \n    \n      4\n      322\n      1501\n      5\n      3.823119\n    \n    \n      5\n      537\n      1294\n      2\n      3.124064\n    \n    \n      6\n      193\n      1530\n      3\n      2.546681\n    \n    \n      7\n      581\n      286\n      5\n      3.062707\n    \n    \n      8\n      450\n      154\n      4\n      4.161049\n    \n  \n\n\n\nI get similar results as before!\n\n\n\nInterpreting Embeddings and Biases\nI’ll save this model so that the embedding and bias analyses I perform can be recreated.\n\nlearn = load_learner(\"/content/gdrive/MyDrive/fastai-course-v4/dot_product_bias.pkl\")\nmodel = learn.model\n\nBias represents a baseline rating of a movie regardless of how well the latent factors of the movie match the latent factors of the user. Low bias values correspond to movies that people didn’t enjoy, even if it matched their preferences.\nWhat were the 5 generally least liked movies?\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nmovie_bias = model.movie_bias\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Robocop 3 (1993)',\n 'Showgirls (1995)',\n 'Kansas City (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)']\n\n\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'L.A. Confidential (1997)',\n 'As Good As It Gets (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Good Will Hunting (1997)']\n\n\n\nVisualizing Embeddings\nThe embeddings are a 50-dimensional matrix of latent factors. I’ll use Principal Component Analysis (PCA) to extract the two most descriptive dimensions and plot the latent factor values. I’ll also calculate the distance from 0 of each movie so that I can filter for outliers in order to reduce the number of data points on the plot, and help me understand what these latent factors may be.\n\n!pip install fbpca\nimport fbpca\n\nCollecting fbpca\n  Downloading https://files.pythonhosted.org/packages/a7/a5/2085d0645a4bb4f0b606251b0b7466c61326e4a471d445c1c3761a2d07bc/fbpca-1.0.tar.gz\nBuilding wheels for collected packages: fbpca\n  Building wheel for fbpca (setup.py) ... done\n  Created wheel for fbpca: filename=fbpca-1.0-cp37-none-any.whl size=11376 sha256=719b80446eeb8f157c99e298adb61b0978c0ae279ade82e500dbe37902c447e4\n  Stored in directory: /root/.cache/pip/wheels/53/a2/dd/9b66cf53dbc58cec1e613d216689e5fa946d3e7805c30f60dc\nSuccessfully built fbpca\nInstalling collected packages: fbpca\nSuccessfully installed fbpca-1.0\n\n\nI grab the movie_factors from the trained model, bring it over the the .cpu(), .detach() it from the gradients and convert it to a .numpy() array.\n\nmovie_embeddings = model.movie_factors.cpu().detach().numpy()\n\nI pass those embeddings to the fbpca.pca method and get back the rank-2 approximation.\n\nU, s, Va = fbpca.pca(movie_embeddings, k=2)\n\nI then create a DataFrame from the U matrix which is an m x k (1665 movies x 2 components) matrix. I also create a column with the calculated distance from 0 of each movie, based on the 2-component coordinates, and a column specifying which quadrant the movie is in (First, Second, Third or Fourth).\nMy distance function receives each DataFrame row, and returns the square root of the sum of squares of the two coordinates.\nMy quadrant function received each row and based on the sign of the x or 0 column and the y or 1 column, determines which quadrant that movie lies in.\nI apply both functions to the DataFrame and specify axis=1 so that I can access the column names.\n\n# helper functions\ndef distance(row):\n  return np.sqrt(row[0]**2 + row[1]**2)\n\ndef quadrant(row):\n  if (row[0] > 0 and row[1] > 0):\n    return \"First Quadrant\"\n  elif (row[0] < 0 and row[1] > 0):\n    return \"Second Quadrant\"\n  elif (row[0] < 0 and row[1] < 0):\n    return \"Third Quadrant\"\n  elif (row[0] > 0 and row[1] < 0):\n    return \"Fourth Quadrant\"\n  else:\n    return \"Center\"\n\n\n# create DataFrame from PCA output\ndef pca_to_df(U):\n  df = pd.DataFrame(data=U)\n\n  # calculate the distance of each Embedding from 0\n  df[2] = df.apply(lambda x: np.sqrt(x[0]**2 + x[1]**2), axis=1)\n\n  # identify which quadrant the movie is in\n  df[3] = df.apply(lambda x: quadrant(x), axis=1)\n\n  return df\n\nI’ll import the DataFrame I created from my original PCA output so that I can recreate the corresponding plots.\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movie_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.010150\n      0.004517\n      0.011110\n      First Quadrant\n    \n    \n      1\n      0.025090\n      -0.000186\n      0.025091\n      Fourth Quadrant\n    \n    \n      2\n      -0.005773\n      0.025443\n      0.026090\n      Second Quadrant\n    \n    \n      3\n      0.015933\n      -0.021972\n      0.027141\n      Fourth Quadrant\n    \n    \n      4\n      -0.056279\n      -0.013351\n      0.057841\n      Third Quadrant\n    \n  \n\n\n\n\nWhen I plot the first two columns as x,y coordinates, I can see that the movies are spread out quite evenly across those latent factors.\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06691494d0>\n\n\n\n\n\nI’m going to plot the farthest points from the origin and label them on the plot in order to get a sense of what these two latent factors may represent.\nFor each quadrant, I grab the indexes for the rows with the 5 largest distances and create a DataFrame from that and plot that data.\n\ndef plot_top_5(df):\n  # get the 5 points farthest from 0 from each quadrant\n  idxs = np.array([])\n  for quad in df['3'].unique():\n    idxs = np.append(idxs, df[df['3']==quad]['2'].sort_values(ascending=False).index[:5].values)\n  plot_df = df.loc[idxs]\n\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_top_5(df)\n\n\n\n\nThe first quadrant seems to represent comedies (Ed, Ready to Wear, The Stupids) although Barb Wire is not a comedy.\nThe second quadrant has movies with some more dark and disturbing elements to it.\nThe third quadrant has movies which are drama and I guess share some theme of gaining freedom?\nFinally, the fourth quadrant seems to have drama and action movies which have a distinct American storyline to them. I haven’t seen all of these movies, but Dirty Dancing and The American President seem like anomalies in this group.\nI should also take a look at other movies that do not fall on the extreme ends of each quadrant. For example, which movies fall close to the vertical and horizontal axes?\n\ndef plot_close_to_axes(df):\n  # get 5 points closes to each axis \n  idxs = np.array([])\n  for key in ['0', '1']:\n    idxs = np.append(idxs, df[np.abs(df[key]) < 0.0002].index.values)\n  plot_df = df.loc[idxs]\n  plot_df = plot_df.drop_duplicates()\n\n  # set figure size\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n\n  # annotate with movie titles\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_close_to_axes(df)\n\n\n\n\nThe latent factor corresponding to the vertical axis seems to represent drama (positive values) and romance (negative values) whereas the horizontal axis represents elements mystery (negative values) and comedy (positive values). However, I could just be focusing on genre whereas there are other features of a movie these may represent. Unfortunately, I’m not a movie buff, so the need for a domain expert is evident here!\n\n\nRobust PCA\nAlthough it’s out of scope for this chapter and my understanding, I’d like to at least experiment with using Robust PCA for visualizing embeddings. Robust PCA is an algorithm which decomposes a matrix M into two components: a low rank matrix L and a sparse matrix S such that M = L + S. From what I understand, the sparse matrix S contains anomalies or outliers or “corrupted” data, whereas L contains a more accurate representation of the original data. For example, if an image has some additional noise added to it, the original image matrix M can be decomposed into a noise-less “clean” image L and a sparse noise matrix S. Another example is if you have an image with a background (such as a landscape with lawn, sidewalks, buildings) and a foreground (people walking on the sidewalk) passing that image through the RPCA algorithm would yield a background matrix L with the lawn, sidewalk and buildings and a foreground sparse matrix S with the people. Since my movie_embeddings matrix may contain anomalies which would affect the effectiveness and accuracy of my 2-component PCA approximation, I will pass it through a RPCA algorithm and calculate the 2-component approximation on the low-rank L and sparse S matrices and compare the results with what I calculated above.\nThe following algorithm is from Rachel Thomas’ lesson on RPCA as part of her Computational Linear Algebra course.\n\nfrom scipy import sparse\nfrom sklearn.utils.extmath import randomized_svd\nimport fbpca\n\n\nTOL=1e-9\nMAX_ITERS=3\n\n\ndef converged(Z, d_norm):\n    err = np.linalg.norm(Z, 'fro') / d_norm\n    print('error: ', err)\n    return err < TOL\n\n\ndef shrink(M, tau):\n    S = np.abs(M) - tau\n    return np.sign(M) * np.where(S>0, S, 0)\n\n\ndef _svd(M, rank): return fbpca.pca(M, k=min(rank, np.min(M.shape)), raw=True)\n\n\ndef norm_op(M): return _svd(M, 1)[1][0]\n\n\ndef svd_reconstruct(M, rank, min_sv):\n    u, s, v = _svd(M, rank)\n    s -= min_sv\n    nnz = (s > 0).sum()\n    return u[:,:nnz] @ np.diag(s[:nnz]) @ v[:nnz], nnz\n\n\ndef pcp(X, maxiter=10, k=10): # refactored\n    m, n = X.shape\n    trans = m<n\n    if trans: X = X.T; m, n = X.shape\n        \n    lamda = 1/np.sqrt(m)\n    op_norm = norm_op(X)\n    Y = np.copy(X) / max(op_norm, np.linalg.norm( X, np.inf) / lamda)\n    mu = k*1.25/op_norm; mu_bar = mu * 1e7; rho = k * 1.5\n    \n    d_norm = np.linalg.norm(X, 'fro')\n    L = np.zeros_like(X); sv = 1\n    \n    examples = []\n    \n    for i in range(maxiter):\n        print(\"rank sv:\", sv)\n        X2 = X + Y/mu\n        \n        # update estimate of Sparse Matrix by \"shrinking/truncating\": original - low-rank\n        S = shrink(X2 - L, lamda/mu)\n        \n        # update estimate of Low-rank Matrix by doing truncated SVD of rank sv & reconstructing.\n        # count of singular values > 1/mu is returned as svp\n        L, svp = svd_reconstruct(X2 - S, sv, 1/mu)\n        \n        # If svp < sv, you are already calculating enough singular values.\n        # If not, add 20% (in this case 240) to sv\n        sv = svp + (1 if svp < sv else round(0.05*n))\n        \n        # residual\n        Z = X - L - S\n        Y += mu*Z; mu *= rho\n        \n        examples.extend([S[140,:], L[140,:]])\n        \n        if m > mu_bar: m = mu_bar\n        if converged(Z, d_norm): break\n    \n    if trans: L=L.T; S=S.T\n    return L, S, examples\n\n\nL, S, examples = pcp(movie_embeddings)\n\n\nL.shape, S.shape\n\nI’ll calculate 2-component PCA for the L and S matrices and plot those to see how they compare to the plots above.\n\nU_L, _, _ = fbpca.pca(L, k=2)\nU_S, _, _ = fbpca.pca(S, k=2)\n\nI exported the outputs to CSV for repeatability, so I’ll import them in again:\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_L_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.009254\n      -0.003454\n      0.009877\n      Fourth Quadrant\n    \n    \n      1\n      0.032954\n      0.008637\n      0.034067\n      First Quadrant\n    \n    \n      2\n      -0.014662\n      -0.047128\n      0.049356\n      Third Quadrant\n    \n    \n      3\n      0.012602\n      0.029182\n      0.031787\n      First Quadrant\n    \n    \n      4\n      -0.037841\n      0.008742\n      0.038838\n      Second Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06668a58d0>\n\n\n\n\n\nThe scatter plot for the 2-component PCA result seems much more evenly distributed across the quadrants.\nI take the 5 farthest movies from each quadrant and plot those separately.\n\nplot_top_5(df)\n\n\n\n\nHere are the patterns I observe. Again, someone who has watched these movies and is not just reading online descriptions of them would see themes and patterns that I would not.\n\n\n\nQuadrant\nObservation\n\n\n\n\n1\nRomance/Drama movies. Fausto and Castle Freak seem out of place\n\n\n2\nMore romance movies. Top Gun and Prefontaine seem out of place\n\n\n3\nMore romance movies. The Butcher Boy seems out of place.\n\n\n4\nComedies. The Carmen Miranda documentary seems out of place.\n\n\n\nAfter making these observations, either the low-rank matrix L is a poor choice to use for this type of analysis, or my understanding these movies is too shallow to see the deeper relationships between them. With so many romance movies across the plot, I don’t think these latent factors represent genres.\nI’m not too confident the S matrix will provide more clarity, but let’s see!\n\n# import previously generated CSV\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_S_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      -0.009776\n      0.005041\n      0.010999\n      Second Quadrant\n    \n    \n      1\n      -0.021503\n      0.001765\n      0.021576\n      Second Quadrant\n    \n    \n      2\n      0.005098\n      0.024645\n      0.025166\n      First Quadrant\n    \n    \n      3\n      -0.016745\n      -0.021457\n      0.027218\n      Third Quadrant\n    \n    \n      4\n      0.056964\n      -0.023095\n      0.061467\n      Fourth Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06681c3310>\n\n\n\n\n\n\nplot_top_5(df)\n\n\n\n\nInteresting! This plot looks like the plot of the original M matrix PCA results reflected across the y-axis. Similar movies are grouped together but the latent factors are showing an inverse relationship to the original 2-components.\n\n\n\nUsing fastai.collab\nfastai comes with a built-in method to create a collaborative filtering model similar to the DotProductBias model I created.\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.932735\n      0.930009\n      00:10\n    \n    \n      1\n      0.834800\n      0.862961\n      00:11\n    \n    \n      2\n      0.746893\n      0.822192\n      00:10\n    \n    \n      3\n      0.585107\n      0.811398\n      00:10\n    \n    \n      4\n      0.490022\n      0.812597\n      00:10\n    \n  \n\n\n\nThis yields similar results to what I’ve done above. Here are this model’s results:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      541\n      332\n      1\n      2.542970\n    \n    \n      1\n      899\n      1295\n      4\n      3.555057\n    \n    \n      2\n      346\n      1492\n      3\n      3.456876\n    \n    \n      3\n      933\n      1399\n      4\n      3.380442\n    \n    \n      4\n      310\n      1618\n      5\n      4.623110\n    \n    \n      5\n      276\n      1572\n      4\n      3.636531\n    \n    \n      6\n      463\n      322\n      5\n      3.901797\n    \n    \n      7\n      130\n      408\n      4\n      3.343735\n    \n    \n      8\n      914\n      1617\n      4\n      3.076288\n    \n  \n\n\n\nThe model created is a EmbeddingDotBias model\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nThe top biases can be obtained similar to how we did it before, but with a slightly different API:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Star Wars (1977)',\n \"Schindler's List (1993)\"]\n\n\nSimilar to the distance function I created, PyTorch has a nn.CosineSimilarity function which calculates the cosine of the angle between two vectors. The smaller the angle, the closer the two points are, and the more similar they are. nn.CosineSimilarity returns the similarity (cosine of the angle) between two vectors where 1.000 means the angle is 0.\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\nsimilarity = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = similarity.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Some Folks Call It a Sling Blade (1993)'\n\n\n\n\n\nDeep Learning for Collaborative Filtering\nIn this final section, we create a Deep Learning model which can make predictions on movie ratings after training on the MovieLens dataset. The model uses Embeddings (for users and movies) which are then fed into a small neural net (with one ReLu sandwiched between two Linear layers) which outputs an activation which we normalize using sigmoid_range. The embedding matrices are sized based on a heuristic built-in to fastai with the get_emb_sz method:\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nThe model used is constructed as follows: the user and item latent factors are created using Embeddings, and the neural net is created using the nn.Sequential class. Each time a prediction is needed, the user and item matrices for one batch are concatenated and passed through the neural net. The returned activation is sent to sigmoid_range and a prediction between 0 and 5.5 is calculated.\n\nclass CollabNN(Module):\n  def __init__(self, user_sz, item_sz, y_range=(0, 5.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.item_factors = Embedding(*item_sz)\n    self.layers = nn.Sequential(\n        nn.Linear(user_sz[1]+item_sz[1], n_act),\n        nn.ReLU(),\n        nn.Linear(n_act, 1))\n    self.y_range = y_range\n\n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1))\n    return sigmoid_range(x, *self.y_range)\n\nI want to visualize the forward method, so I’ll create the model and a batch, and walkthrough the code.\n\nmodel = CollabNN(*embs)\nx,y = dls.one_batch()\ndevice = \"cpu\"\nx = x.to(device)\nmodel = model.to(device)\nembs = torch.cat((model.user_factors(x[:,0]), model.item_factors(x[:,1])), dim=1)\nembs.shape\n\ntorch.Size([64, 176])\n\n\n\nx = model.layers(embs)\nsigmoid_range(x, *model.y_range)[:5]\n\ntensor([[2.8637],\n        [2.8647],\n        [2.8624],\n        [2.8696],\n        [2.8601]], grad_fn=<SliceBackward>)\n\n\nThe fastai collab_learner, instead of using the EmbeddingDotBias model, will use a neural network if passed True for its use_nn parameter. The number and size of neural network layers can also be specified.\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.988426\n      0.984418\n      00:15\n    \n    \n      1\n      0.893442\n      0.909180\n      00:16\n    \n    \n      2\n      0.900106\n      0.877499\n      00:16\n    \n    \n      3\n      0.809255\n      0.853736\n      00:16\n    \n    \n      4\n      0.769467\n      0.853571\n      00:16\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      244\n      1305\n      4\n      4.185966\n    \n    \n      1\n      902\n      965\n      3\n      3.474954\n    \n    \n      2\n      87\n      1173\n      5\n      4.206645\n    \n    \n      3\n      759\n      333\n      5\n      4.247213\n    \n    \n      4\n      109\n      1624\n      3\n      3.726794\n    \n    \n      5\n      363\n      743\n      1\n      1.774737\n    \n    \n      6\n      756\n      1216\n      5\n      4.058509\n    \n    \n      7\n      378\n      179\n      4\n      3.192873\n    \n    \n      8\n      18\n      141\n      3\n      3.296141\n    \n  \n\n\n\n\nlearn.model\n\nEmbeddingNN(\n  (embeds): ModuleList(\n    (0): Embedding(944, 74)\n    (1): Embedding(1665, 102)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=176, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=50, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=50, out_features=1, bias=True)\n    )\n    (3): SigmoidRange(low=0, high=5.5)\n  )\n)\n\n\nThe EmbeddingNN architecture extends the TabularModel class which we will explore in Chapter 9.\nThat finishes my review of Chapter 8, I’ll be working through the “Further Research” section in upcoming blog posts and associated videos:\n\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens.\n\n\n\n\nFind three other areas where collaborative filtering is being used, and identify the pros and cons of this approach in those areas.\n\n\n\n\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forums for ideas. Note that there are more columns in the full dataset–see if you can use those too (the next chapter might give you ideas).\n\n\n\n\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "",
    "text": "In this blog post, I work through the book Effective Pandas by Matt Harrison. I’ll take notes, work through examples and end-of-chapter exercises."
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-4-series-introduction",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-4-series-introduction",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 4: Series Introduction",
    "text": "Chapter 4: Series Introduction\nRepresent the following data in pure python:\n\n\n\nArtist\nData\n\n\n\n\n0\n145\n\n\n1\n142\n\n\n2\n38\n\n\n3\n13\n\n\n\n\nseries = {\n    'index': [0, 1, 2, 3],\n    'data': [145, 142, 38, 13],\n    'name': 'songs'\n}\n\nseries\n\n{'index': [0, 1, 2, 3], 'data': [145, 142, 38, 13], 'name': 'songs'}\n\n\nThe get function below can pull items out of this data structure based on the index:\n\ndef get(series, idx):\n    value_idx = series['index'].index(idx)\n    return series['data'][value_idx]\n\n\nget(series, 1)\n\n142\n\n\nThe index method on the list returns the list element at the provided index value.\n\n[0, 1, 2, 3].index(1)\n\n1\n\n\nBelow is an example that has string values for the index:\n\nsongs = {\n    'index': ['Paul', 'John', 'George', 'Ringo'],\n    'data': [145, 142, 38, 13],\n    'name': 'songs'\n}\n\n\nget(songs, 'John')\n\n142\n\n\nCreate a Series object from a list:\n\nimport pandas as pd\n\n\nsongs2 = pd.Series([145, 142, 38, 13], name = 'counts')\nsongs2\n\n0    145\n1    142\n2     38\n3     13\nName: counts, dtype: int64\n\n\nThe series is one-dimensional. The leftmost column is the index, also called the axis. The data (145, 142, 38, 13) is also called the values of the series. A DataFrame has two axes, one for the rows and another for the columns.\n\nsongs2.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nThe default values for an index are monotonically increasing integers. The index can be string-based as well (datatype for the index is object).\n\nsongs3 = pd.Series([145, 142, 38, 13],\n                   name = 'counts',\n                   index = ['Paul', 'John', 'George', 'Ringo'])\nsongs3\n\nPaul      145\nJohn      142\nGeorge     38\nRingo      13\nName: counts, dtype: int64\n\n\n\nsongs3.index\n\nIndex(['Paul', 'John', 'George', 'Ringo'], dtype='object')\n\n\nWe can insert Python objects into a series:\n\nclass Foo:\n    pass\n\nringo = pd.Series(\n    ['Richard', 'Starkey', 13, Foo()],\n    name = 'ringo')\n\nringo\n\n0                                 Richard\n1                                 Starkey\n2                                      13\n3    <__main__.Foo object at 0x135016ce0>\nName: ringo, dtype: object\n\n\nThe object data type is also used for a series with string values and values that have heterogeneous or mixed types.\nHere is a series that has NaN in it:\n\nimport numpy as np\nnan_series = pd.Series([2, np.nan],\n                       index = ['Ono', 'Clapton'])\nnan_series\n\nOno        2.0\nClapton    NaN\ndtype: float64\n\n\nfloat64 supports NaN while int64 does not. As of pandas 0.24, Int64 (nullable integer type) supports NaN.\ncount ignores NaNs, .size does not.\n\nnan_series.count()\n\n1\n\n\n\nnan_series.size\n\n2\n\n\n\nnan_series2 = pd.Series([2, None],\n                        index = ['Ono', 'Clapton'],\n                        dtype = 'Int64')\nnan_series2\n\nOno           2\nClapton    <NA>\ndtype: Int64\n\n\n\nnan_series2.count()\n\n1\n\n\n\n# convert data type\nnan_series.astype('Int64')\n\nOno           2\nClapton    <NA>\ndtype: Int64\n\n\nThe Series object behaves similarly to a NumPy array.\n\nnumpy_ser = np.array([145, 142, 38, 13])\nsongs3[1], numpy_ser[1]\n\n(142, 142)\n\n\nThey both have methods in common\n\nsongs3.mean(), numpy_ser.mean()\n\n(84.5, 84.5)\n\n\nThey both have a notion of a boolean array.\n\nmask = songs3 > songs3.median()\nmask\n\nPaul       True\nJohn       True\nGeorge    False\nRingo     False\nName: counts, dtype: bool\n\n\n\n# use mask as a filter\nsongs3[mask]\n\nPaul    145\nJohn    142\nName: counts, dtype: int64\n\n\n\n# NumPy equivalent\nnumpy_ser[numpy_ser > np.median(numpy_ser)]\n\narray([145, 142])\n\n\nIf can indicate that data is categorical.\nCategorical values:\n\nUse less memory than strings\nImpove performance\nCan have an ordering\nCan perform operations on categories\nEnforce membership on values\n\n\ns = pd.Series(['m', 'l', 'xs', 's', 'xl'], dtype = 'category')\ns\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['l', 'm', 's', 'xl', 'xs']\n\n\nBy default categories don’t have an ordering.\n\ns.cat.ordered\n\nFalse\n\n\nConvert non-categorical series to an ordered category:\n\ns2 = pd.Series(['m', 'l', 'xs', 's', 'xl'])\n\nsize_type = pd.api.types.CategoricalDtype(\n    categories=['s', 'm', 'l'], ordered = True)\n\ns3 = s2.astype(size_type)\ns3\n\n0      m\n1      l\n2    NaN\n3      s\n4    NaN\ndtype: category\nCategories (3, object): ['s' < 'm' < 'l']\n\n\n\n# can perform comparisons on ordered categories\ns3 > 's'\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\n\n# add ordering information to categorical data\ns.cat.reorder_categories(['xs', 's', 'm', 'l', 'xl'], ordered=True)\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['xs' < 's' < 'm' < 'l' < 'xl']\n\n\nFor strings and dates converted to categorical types, we can still use the str or dt attributes on them:\n\ns3.str.upper()\n\n0      M\n1      L\n2    NaN\n3      S\n4    NaN\ndtype: object\n\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\npd.Series(data=None, index=None, dtype=None, name=None, copy=False)\nCreate a series from data (sequence, dictionary or scalar)\n\n\ns.index\nAccess index of series.\n\n\ns.astype(dtype, errors='raise')\nCast a series to dtype. To ignore errors (and return original object) use errors='ignore'\n\n\ns[boolean_array]\nReturn values from s where boolean_array is True\n\n\ns.cat.ordered\nDetermine if a categorical series is ordered\n\n\ns.cat.reorder_categories(new_categories, ordered=False)\nAdd categories (potentially ordered) to the series. new_categories must include all categories.\n\n\n\n\nExercises\n\nUsing Jupyter, create a series with the temperature values for the last seven days. Filter out the vaues below the mean.\nUsing Jupyter, create a series with your favorite colors. Use a categorical type.\n\n\n# temperature series\ntemps = pd.Series([88, 84, 84, 84, 88, 95, 97 ,88])\n\ntemps[temps >= temps.mean()]\n\n5    95\n6    97\ndtype: int64\n\n\n\n# favorite colors\ncolors_series = pd.Series(['orange', 'coral', 'midnight green'], dtype = 'category')\ncolors_series\n\n0            orange\n1             coral\n2    midnight green\ndtype: category\nCategories (3, object): ['coral', 'midnight green', 'orange']"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-5-series-deep-dive",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-5-series-deep-dive",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 5: Series Deep Dive",
    "text": "Chapter 5: Series Deep Dive\n\n# analyze the US Fuel Economy data\nurl = 'https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip'\n\ndf = pd.read_csv(url)\n\ncity_mpg = df.city08\nhighway_mpg = df.highway08\n\n/var/folders/5q/_bn7l90s177_2gq7rnhssjxm0000gn/T/ipykernel_52057/221626492.py:4: DtypeWarning: Columns (68,70,71,72,73,74,76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(url)\n\n\n\ncity_mpg\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\n\nhighway_mpg\n\n0        25\n1        14\n2        33\n3        12\n4        23\n         ..\n41139    26\n41140    28\n41141    24\n41142    24\n41143    21\nName: highway08, Length: 41144, dtype: int64\n\n\nBecause the type is int64 we know that none of the values are missing.\nThe dir function lists the attributes of an object. A series has 400+ attributes:\n\nlen(dir(city_mpg))\n\n412\n\n\n\nlen(dir(highway_mpg))\n\n412\n\n\nFunctionality of series attributes:\n\nDunder methods provide many numeric operations, looping, attribute access, and index access. For the numeric operations, these return Series.\nCorresponding operator methods for many of the numeric operations allow us to tweak the behavior.\nAggregate methods and properties which reduce or aggregate the values in a series down to a single scalar value.\nConversion methods. Some of these start with .to_ and export the data to other formats.\nManipulation methods that return Series objects with the same index.\nIndexing and accessor methods and attributes that return Series or scalars.\nString manipulation methods using .str.\nDate manipulation methods using .dt.\nPlotting methods using .plot.\nCategorical manipulation methods using .cat.\nTransformation methods.\nAttributes such as .index and .dtype.\nA bunch of private attributes (130 of them) that we’ll ignore.\n\n\nExercises\n\nExplore the documentation for five attributes of a series from Jupyter.\nHow many attributes are found on the .str attribute? Look at the documentation for three of them.\nHow many attributes are found on the .dt attribute? Look at the documentation for three of them.\n\n\ncity_mpg.values\n\narray([19,  9, 23, ..., 18, 18, 16])\n\n\n\ncity_mpg.axes\n\n[RangeIndex(start=0, stop=41144, step=1)]\n\n\n\ncity_mpg.empty\n\nFalse\n\n\n\ncity_mpg.at[4]\n\n17\n\n\n\ncity_mpg.loc[1:4]\n\n1     9\n2    23\n3    10\n4    17\nName: city08, dtype: int64\n\n\n\n# 98 string attributes\nlen(dir(s2.str))\n\n98\n\n\n\ns2.str.cat(sep = \".\")\n\n'm.l.xs.s.xl'\n\n\n\ns2.str.capitalize()\n\n0     M\n1     L\n2    Xs\n3     S\n4    Xl\ndtype: object\n\n\n\ns2.str.endswith('l')\n\n0    False\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n\n\n\ndt_series = pd.Series(['2023-01-01', '2023-04-05', '2023-07-06'])\n\ndt_series = pd.to_datetime(dt_series)\ndt_series\n\n0   2023-01-01\n1   2023-04-05\n2   2023-07-06\ndtype: datetime64[ns]\n\n\n\nlen(dir(dt_series.dt))\n\n83\n\n\n\ndt_series.dt.day\n\n0    1\n1    5\n2    6\ndtype: int32\n\n\n\ndt_series.dt.day_of_year\n\n0      1\n1     95\n2    187\ndtype: int32\n\n\n\ndt_series.dt.daysinmonth\n\n0    31\n1    30\n2    31\ndtype: int32"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-6-operators-dunder-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-6-operators-dunder-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 6: Operators (& Dunder Methods)",
    "text": "Chapter 6: Operators (& Dunder Methods)\nThese are the protocols that determine how the Python language reacts to operations.\n\n2 + 4\n\n6\n\n\n\n# under the cover is\n(2).__add__(4)\n\n6\n\n\n\n(city_mpg + highway_mpg) / 2\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\nWhen you operate with two series, pandas will align the index before performing the operation. Because of index alignment, you will want to make sure that the indexes: - are unique - are common to both series\n\n# example of series with repeated and non-common indexes\ns1 = pd.Series([10, 20, 30], index=[1,2,2])\ns2 = pd.Series([35, 44, 53], index=[2,2,4], name = 's2')\n\n\ns1\n\n1    10\n2    20\n2    30\ndtype: int64\n\n\n\ns2\n\n2    35\n2    44\n4    53\nName: s2, dtype: int64\n\n\n\n# index 1 and 4 have NaN\n# index 2 has four results\ns1 + s2\n\n1     NaN\n2    55.0\n2    64.0\n2    65.0\n2    74.0\n4     NaN\ndtype: float64\n\n\nWhen you perform math operations with a scalar, pandas broadcasts the operation to all values. A numeric pandas series is a block of memory, and modern CPUs leverage a technology called Single Instruction/Multiple Data (SIMD) to apply a math operation to the block of memory.\n\n# use `fill_value` parameter to replace missing operands\ns1.add(s2, fill_value = 0)\n\n1    10.0\n2    55.0\n2    64.0\n2    65.0\n2    74.0\n4    53.0\ndtype: float64\n\n\nChaining makes the code easy to read and understand\n\n(city_mpg\n    .add(highway_mpg)\n    .div(2))\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\n\n\n\n\n\n\n\n\nMethod\nOperator\nDescription\n\n\n\n\ns.add(s2)\ns + s2\nAdds series\n\n\ns.radd(s2)\ns2 + s\nAdds series\n\n\ns.sub(s2)\ns - s2\nSubtracts series\n\n\ns.rsub(s2)\ns2 - s\nSubtracts series\n\n\ns.mul(s2)\ns * s2\nMultiplies series\n\n\ns.multiply(s2)\ns * s2\nMultiplies series\n\n\ns.rmul(s2)\ns2 * s\nMultiplies series\n\n\ns.div(s2)\ns / s2\nDivides series\n\n\ns.truediv(s2)\ns / s2\nDivides series\n\n\ns.rdiv(s2)\ns2 / s\nDivides series\n\n\ns.rtruediv(s2)\ns2 / s\nDivides series\n\n\ns.mod(s2)\ns % s2\nModulo of series division\n\n\ns.rmod(s2)\ns2 % s\nModulo of series division\n\n\ns.floordiv(s2)\ns // s2\nFloor divide series\n\n\ns.rfloordiv(s2)\ns2 // s\nFloor divide series\n\n\ns.pow(s2)\ns ** s2\nExponential power of series\n\n\ns.rpow(s2)\ns2 ** s\nExponential power of series\n\n\ns.eq(s2)\ns2 == s\nElementwise equals of series\n\n\ns.ne(s2)\ns2 != s\nElementwise not equals of series\n\n\ns.gt(s2)\ns > s2\nElementwise greater than of series\n\n\ns.ge(s2)\ns >= s2\nElementwise greater than or equals of series\n\n\ns.lt(s2)\ns < s2\nElementwise less than of series\n\n\ns.le(s2)\ns <= s2\nElementwise less than or equals of series\n\n\nnp.invert(s)\n~s\nElementwise inversion of boolean series (no pandas method)\n\n\nnp.logical_and(s, s2)\ns & s2\nElementwise logical and of boolean series (no pandas method)\n\n\nnp.logical_or(s, s2)\ns \\| s2\nElementwise logical or of boolean series (no pandas method)\n\n\n\n\nExercises\nWith a dataset of your choice:\n\nAdd a numeric series to itself.\nAdd 10 to a numeric series.\nAdd a numeric series to itself using the .add method.\nRead the documentation for the .add method.\n\n\ncity_mpg + city_mpg\n\n0        38\n1        18\n2        46\n3        20\n4        34\n         ..\n41139    38\n41140    40\n41141    36\n41142    36\n41143    32\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg + 10\n\n0        29\n1        19\n2        33\n3        20\n4        27\n         ..\n41139    29\n41140    30\n41141    28\n41142    28\n41143    26\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg.add(city_mpg)\n\n0        38\n1        18\n2        46\n3        20\n4        34\n         ..\n41139    38\n41140    40\n41141    36\n41142    36\n41143    32\nName: city08, Length: 41144, dtype: int64\n\n\n\n# experimenting with fill_value parameter\nnan_series3 = pd.Series([2, None])\nnan_series4 = pd.Series([3, None])\n\n\nnan_series3\n\n0    2.0\n1    NaN\ndtype: float64\n\n\n\nnan_series4\n\n0    3.0\n1    NaN\ndtype: float64\n\n\n\n# two corresponding NaN values stay NaN\n# even with fill_value = 0\nnan_series3.add(nan_series4, fill_value=0)\n\n0    5.0\n1    NaN\ndtype: float64"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-7-aggregate-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-7-aggregate-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 7: Aggregate Methods",
    "text": "Chapter 7: Aggregate Methods\nAggregate methods collapse the values of a series down to a scalar.\n\n# calculate the mean\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ncity_mpg.is_unique\n\nFalse\n\n\n\npd.Series([1,2,3]).is_unique\n\nTrue\n\n\n\ncity_mpg.is_monotonic_increasing\n\nFalse\n\n\n\npd.Series([1,2,3]).is_monotonic_increasing\n\nTrue\n\n\n\n# default is median (50% quantile)\ncity_mpg.quantile()\n\n17.0\n\n\n\ncity_mpg.quantile(0.9)\n\n24.0\n\n\n\n# multiple quantiles returns a Series\ncity_mpg.quantile([0.1, 0.5, 0.9])\n\n0.1    13.0\n0.5    17.0\n0.9    24.0\nName: city08, dtype: float64\n\n\nIf you want the count of values that meet some criteria, you can use the .sum method:\n\n# count of cars with mileage greater than 20\n(city_mpg\n     .gt(20)\n     .sum()\n)\n\n10272\n\n\n\n# percentage of cars with mileage greater than 20\n(city_mpg\n     .gt(20)\n     .mul(100)\n     .mean()\n)\n\n24.965973167412017\n\n\nObserve the .mul(100).mean() calculation on a simpler Series:\n\n(pd.Series([1,2,3,4])\n    .gt(2)\n    .mul(100)\n)\n\n0      0\n1      0\n2    100\n3    100\ndtype: int64\n\n\n\n(pd.Series([1,2,3,4])\n     .gt(2)\n     .mul(100)\n     .mean()   \n)\n\n50.0\n\n\nIf you sum up a series of boolean values, the result is the count of True values. If you take the mean of a series of boolean values, the result is the fraction of values that are True.\n.agg can perform multiple operations.\n\ncity_mpg.agg('mean')\n\n18.369045304297103\n\n\n\ndef second_to_last(s):\n    return s.iloc[-2]\n\n\ncity_mpg.agg(['mean', np.var, max, second_to_last])\n\nmean               18.369045\nvar                62.503036\nmax               150.000000\nsecond_to_last     18.000000\nName: city08, dtype: float64\n\n\nAggregation strings and descriptions:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\n'all'\nReturns True if every value is truthy.\n\n\n'any'\nReturns True if any value is truthy.\n\n\n'autocorr'\nReturns Pearson correlation of series with shifted self. Can override lag as keyword argument (default is 1).\n\n\n'corr'\nReturns Pearson correlation of series with other series. Need to specify other\n\n\n'count'\nReturns count of non-missing values.\n\n\n'cov'\nReturns covariance of series with other series. Need to specify other\n\n\n'dtype'\nType of the series.\n\n\n'dtypes'\nType of the series.\n\n\n'empty'\nTrue is no values in series.\n\n\n'hasnans'\nTrue if missing values in series.\n\n\n'idxmax'\nReturns index value of maximum value.\n\n\n'idxmin'\nReturns index value of minimum value.\n\n\n'is_monotonic'\nTrue if values always increase.\n\n\n'is_monotonic_decreasing'\nTrue if values always decrease.\n\n\n'is_monotonic_increasing'\nTrue if values always increase.\n\n\n'kurt'\nReturns “excess” kurtosis (0 is normal distribution). Values greater than 0 have more outliers than normal.\n\n\n'mad'\nReturns the mean absolute deviation.\n\n\n'max'\nReturns the maximum value.\n\n\n'mean'\nReturns the mean value.\n\n\n'median'\nReturns the median value.\n\n\n'min'\nReturns the minimum value.\n\n\n'nbytes'\nReturns the number of bytes of the data.\n\n\n'ndim'\nReturn the number of dimensions (1) of the data.\n\n\n'nunique'\nReturns the count of unique values.\n\n\n'quantile'\nReturns the median value. Can override q to specify other quantile.\n\n\n'sem'\nReturns the unbiarsed standard error.\n\n\n'size'\nReturns the size of the data.\n\n\n'skew'\nReturns the unbiased skew of the data. Negative indicates tail is on the left side.\n\n\n'std'\nReturns the standard deviation of the data.\n\n\n'sum'\nReturns the sum of the series.\n\n\n\nAggregation methods and properties:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ns.agg(func=None, axis=0, *args, **kwargs)\nReturns a scalar if func is a single aggregation function. Returns a series if a list of aggregations are passed to func.\n\n\ns.all(axis=0, bool_only=None, skipna=True, level=None)\nReturns True if every value is truthy. Otherwise False.\n\n\ns.any(axis=0, bool_only=None, skipna=True, level=None)\nReturns True if at least one value is truthy. Otherwise False.\n\n\ns.autocorr(lag=1)\nReturns Pearson correlation between s and shifted s.\n\n\ns.corr(other, method='pearson')\nReturns correlation coefficient for 'pearson', 'spearman', 'kendall', or a callable.\n\n\ns.cov(other, min_periods=None)\nReturns covariance.\n\n\ns.max(axis=None, skipna=None, level=None, numeric_only=None)\nReturns maximum value.\n\n\ns.min(axis=None, skipna=None, level=None, numeric_only=None)\nReturns minimum value.\n\n\ns.mean(axis=None, skipna=None, level=None, numeric_only=None)\nReturns mean value.\n\n\ns.median(axis=None, skipna=None, level=None, numeric_only=None)\nReturns median value.\n\n\ns.prod(axis=None, skipna=None, level=None, numeric_only=None, min_count=0)\nReturns product of s values.\n\n\ns.quantile(q=0.5, interpolation='linear')\nReturns 50% quantile by default. Returns Series if q is a list.\n\n\ns.sem(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns unbiased standard error of mean.\n\n\ns.std(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns sample standard deviation.\n\n\ns.var(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns unbiased variance.\n\n\ns.skew(axis=None, skipna=None, level=None, numeric_only=None)\nReturns unbiased skew.\n\n\ns.kurtosis(axis=None, skipna=None, level=None, numeric_only=None)\nReturns unbiased kurtosis.\n\n\ns.nunique(dropna=True)\nReturns count of unique items.\n\n\ns.count(level=None)\nReturns count of non-missing items.\n\n\ns.size\nNumber of items in series. (Property)\n\n\ns.is_unique\nTrue if all values are unique.\n\n\ns.is_monotonic\nTrue if all values are increasing.\n\n\ns.is_monotonic_increasing\nTrue if all values are increasing.\n\n\ns.is_monotonic_decreasing\nTrue if all values are decreasing.\n\n\n\n\nExercises\nWith a dataset of your choice:\n\nFind the count of non-missing values of a series.\nFind the number of entries of a series.\nFind the number of unique entries of a series.\nFind the mean value of a series.\nFind the maximum value of a series.\nUse the .agg method to find all of the above.\n\n\ncity_mpg.count()\n\n41144\n\n\n\ncity_mpg.size\n\n41144\n\n\n\ncity_mpg.nunique()\n\n105\n\n\n\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ncity_mpg.max()\n\n150\n\n\n\ncity_mpg.agg(['count', 'size', 'nunique', 'mean', 'max'])\n\ncount      41144.000000\nsize       41144.000000\nnunique      105.000000\nmean          18.369045\nmax          150.000000\nName: city08, dtype: float64"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-8-conversion-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-8-conversion-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 8: Conversion Methods",
    "text": "Chapter 8: Conversion Methods\n\n8.1 Automatic Conversion\n.convert_dtypes tries to convert a Series to a type that supports pd.NA. In the case of our city_mpg series it will change the type from int64 to Int64.\n\ncity_mpg.convert_dtypes()\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: Int64\n\n\n.astype works more explicitly. The maximumm 8-bit integer is 127, so we need 16-bit integer for city_mpg since it’s max is 150.\n\ncity_mpg.max()\n\n150\n\n\n\ncity_mpg.astype('Int16')\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: Int16\n\n\n\ncity_mpg.astype('Int8')\n\nTypeError: cannot safely cast non-equivalent int64 to int8\n\n\nIf you can use a narrower type, you can cut back on memory usage, giving you memory to process more data.\nUse NumPy to inspect limits on integer and float types:\n\nnp.iinfo('int64')\n\niinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)\n\n\n\nnp.iinfo('uint8')\n\niinfo(min=0, max=255, dtype=uint8)\n\n\n\nnp.finfo('float16')\n\nfinfo(resolution=0.001, min=-6.55040e+04, max=6.55040e+04, dtype=float16)\n\n\n\nnp.finfo('float64')\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\n\n\n8.2 Memory Usage\nUse the .nbytes property or the .memory_usage method to calculate memory usage of the Series.\nPass deep=True to .memory_usage when dealing with object types in the Series.\n\ncity_mpg.nbytes\n\n329152\n\n\n\ncity_mpg.astype('Int16').nbytes\n\n123432\n\n\nTo get the amount of memory that includes strings in the Series (like the make column), we need to use the .memory_usage method:\n\ndf.make\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            Subaru\n            ...    \n41139        Subaru\n41140        Subaru\n41141        Subaru\n41142        Subaru\n41143        Subaru\nName: make, Length: 41144, dtype: object\n\n\n\ndf.make.nbytes\n\n329152\n\n\n\ndf.make.memory_usage()\n\n329280\n\n\n\ndf.make.memory_usage(deep=True)\n\n2606395\n\n\n.memory_usage includes the index memory and can include the contribution from object types. .nbytes is just the memory that the data is using and not the ancillary parts of the Series.\nConverting to categorical will save a lot of memory for the make data:\n\n(df.make\n .astype('category')\n .memory_usage(deep=True)\n)\n\n95888\n\n\n\n\n8.3 String and Category Types\n\ncity_mpg.astype(str)\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: object\n\n\n\ncity_mpg.astype('category')\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: category\nCategories (105, int64): [6, 7, 8, 9, ..., 137, 138, 140, 150]\n\n\nWhen you convert to categorical data, pandas no longer uses Python strings for each value but optimizes it. Potentially large memory savings if you have many duplicate values.\n\n\n8.4 Ordered Categories\nTo cretae ordered categories you need to define your own CategoricalDtype:\n\nvalues = pd.Series(sorted(set(city_mpg)))\ncity_type = pd.CategoricalDtype(categories=values, ordered=True)\ncity_mpg.astype(city_type)\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: category\nCategories (105, int64): [6 < 7 < 8 < 9 ... 137 < 138 < 140 < 150]\n\n\n\n\n\n\n\n\n\nString or Type\nDescription\n\n\n\n\nstr 'str' 'string'\nConvert to Python string.\n\n\n'string'\nConvert type to pandas string (supports pd.NA).\n\n\nint 'int' 'int64'|Convert type to NumPy int64.| |'int32' 'uint32'|Convert type to 32 signed or unsigned NumPy integer (can also use 16 and 8).| |‘Int64’|Convert type to pandas Int64 (supportspd.NA). Might complain when you convert floats or strings.| |float ‘float’ ‘float64’|Convert type to NumPy float64 (can also support 32 or 16)| |‘category’|Convert type to categorical (supportspd.NA). Can also use instance ofCategoricalDtype| |dates|Don't use this for data conversion, usepd.to_datetime`.\n\n\n\n\n\n\n8.5 Converting to Other Types\nUsing Python lists will slow down your code significantly.\nYou can convert a Series into a DataFrame:\n\ncity_mpg.to_frame()\n\n\n\n\n\n  \n    \n      \n      city08\n    \n  \n  \n    \n      0\n      19\n    \n    \n      1\n      9\n    \n    \n      2\n      23\n    \n    \n      3\n      10\n    \n    \n      4\n      17\n    \n    \n      ...\n      ...\n    \n    \n      41139\n      19\n    \n    \n      41140\n      20\n    \n    \n      41141\n      18\n    \n    \n      41142\n      18\n    \n    \n      41143\n      16\n    \n  \n\n41144 rows × 1 columns\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ns.convert_dtypes(infer_objects=True, convert_string=True, convert_integer=True, convert_boolean=True, convert_floating=True)\nConvert types to appropriate pandas 1 types (that support NA). Doesn’t try to reduce size of integer or float types\n\n\ns.astype(dtype, copy=True, errors='raise')\nCast series into particular type. If errors='ignore' then return original series on error.\n\n\npd.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix', cache=True)\nConvert arg (a series) into datetime. Use format to specify strftime string.\n\n\ns.to_numpy(dtype=None, copy=False, na_value=object, **kwargs)\nConvert the series to a NumPy array.\n\n\ns.values\nConvert the series to a NumPy array.\n\n\ns.to_frame(name=None)\nReturn a dataframe representation of the series.\n\n\npd.CategoricalDtype(categories=None, ordered=False)\nCreate a type for categorical data.\n\n\n\n\n\n8.7 Exercises\nWith a dataset of your choice:\n1. Convert a numeric column to a smaller type.\n\n# currently a float64 type\ndf.barrels08\n\n0        15.695714\n1        29.964545\n2        12.207778\n3        29.964545\n4        17.347895\n           ...    \n41139    14.982273\n41140    14.330870\n41141    15.695714\n41142    15.695714\n41143    18.311667\nName: barrels08, Length: 41144, dtype: float64\n\n\n\n# convert to float16\ndf.barrels08.astype('float16')\n\n0        15.695312\n1        29.968750\n2        12.210938\n3        29.968750\n4        17.343750\n           ...    \n41139    14.984375\n41140    14.328125\n41141    15.695312\n41142    15.695312\n41143    18.312500\nName: barrels08, Length: 41144, dtype: float16\n\n\n2. Calculate the memory savings by converting to smaller numeric types.\n\ndf.barrels08.memory_usage() - df.barrels08.astype('float16').memory_usage()\n\n246864\n\n\n3. Convert a string column into a categorical type.\n\ndf.make\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            Subaru\n            ...    \n41139        Subaru\n41140        Subaru\n41141        Subaru\n41142        Subaru\n41143        Subaru\nName: make, Length: 41144, dtype: object\n\n\n\nvalues = pd.Series(sorted(set(df.make)))\nmake_type = pd.CategoricalDtype(categories=values, ordered=False)\ndf.make.astype(make_type)\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            Subaru\n            ...    \n41139        Subaru\n41140        Subaru\n41141        Subaru\n41142        Subaru\n41143        Subaru\nName: make, Length: 41144, dtype: category\nCategories (136, object): ['AM General', 'ASC Incorporated', 'Acura', 'Alfa Romeo', ..., 'Volvo', 'Wallace Environmental', 'Yugo', 'smart']\n\n\n4. Calculate the memory savings by converting to a categorical type.\n\ndf.make.memory_usage(deep=True) - df.make.astype(make_type).memory_usage(deep=True)\n\n2510507"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-9-manipulation-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-9-manipulation-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 9: Manipulation Methods",
    "text": "Chapter 9: Manipulation Methods\nComparing non-broadcasted .apply method with vectorized code:\n\n# non-vectorized function to check if value is greater than 20\ndef gt20(val): \n    return val > 20\n\n\n%%timeit\ncity_mpg.apply(gt20)\n\n7.08 ms ± 161 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\ncity_mpg.gt(20)\n\n174 µs ± 44 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nThe broadcasted .gt method is 40 times faster than non-broadcasted .apply function.\nShow the top 5 makes and label everything else as Other:\n\n# top 5 makes\ntop5 = df.make.value_counts().index[:5]\n\n# function to use in apply\ndef generalize_top5(val):\n    if val in top5:\n        return val\n    return 'Other'\n\n\n%%timeit\ndf.make.apply(generalize_top5)\n\n48.2 ms ± 4.64 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nA (10 times) faster and more idiomatic manner of doing this uses the .where method, which keeps values from the series it is called on where the boolean array is true. If the boolean array is false, it uses the value of the second parameter, other:\n\n%%timeit\ndf.make.where(df.make.isin(top5), other='Other')\n\n3.68 ms ± 693 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\ndf.make.where(df.make.isin(top5), other='Other')\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\nThe complement of .where is .mask—wherever the condition if False it keeps the original values; if it is True it replaces the value with the other parameter.\n\ndf.make.mask(~df.make.isin(top5), other='Other')\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\n\n%%timeit\ndf.make.mask(~df.make.isin(top5), other='Other')\n\n3.36 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe tilde ~ performs an inversion of the boolean array, switching all true values to false and vice versa.\n\n9.2 If Else with Pandas\nThere is no way to do the following: if I wanted to keep the top five makes and use Top10 for the remainder of the top ten makes, with Other for the rest.\n\nvc = df.make.value_counts()\ntop5 = vc.index[:5]\ntop10 = vc.index[:10]\ndef generalize(val):\n    if val in top5:\n        return val\n    elif val in top10:\n        return 'Top10'\n    else:\n        return 'Other'\n\n\ndf.make.apply(generalize)\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\n\n%%timeit\ndf.make.apply(generalize)\n\n76.4 ms ± 2.37 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nTo replicate in pandas, chain calls to .where:\n\n(df.make\n .where(df.make.isin(top5), 'Top10')\n .where(df.make.isin(top10), 'Other')\n)\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\n\n%%timeit\n(df.make\n .where(df.make.isin(top5), 'Top10')\n .where(df.make.isin(top10), 'Other')\n)\n\n6.05 ms ± 391 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe pandas approach is still about 13 times faster.\nThe select function in NumPy works with pandas series. The interface takes a list of boolean arrays and a list with corresponding replacement values.\n\nnp.select([df.make.isin(top5), df.make.isin(top10)], [df.make, 'Top10'], 'Other')\n\narray(['Other', 'Other', 'Dodge', ..., 'Other', 'Other', 'Other'],\n      dtype=object)\n\n\n\n%%timeit\nnp.select([df.make.isin(top5), df.make.isin(top10)], [df.make, 'Top10'], 'Other')\n\n19.8 ms ± 2.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nYou can wrap it in a Series. I like this syntax for longer if statements than chaining .where calls because I think it is easier to understand.\n\npd.Series(np.select([df.make.isin(top5), df.make.isin(top10)], [df.make, 'Top10'], 'Other'), index=df.make.index)\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nLength: 41144, dtype: object\n\n\n\n\n9.3 Missing Data\nCount the number of missing items with .isna().sum():\n\n(df.cylinders\n .isna()\n .sum()\n)\n\n206\n\n\nLet’s index where the values are missing in the cylinders column and then show what those makes are:\n\nmissing = df.cylinders.isna()\ndf.make.loc[missing]\n\n7138     Nissan\n7139     Toyota\n8143     Toyota\n8144       Ford\n8146       Ford\n          ...  \n34563     Tesla\n34564     Tesla\n34565     Tesla\n34566     Tesla\n34567     Tesla\nName: make, Length: 206, dtype: object\n\n\n\n\n9.4 Filling in Missing Data\nIt seems like cylinders are missing for cars that are electric (they have zero cylinders).\n\ndf.cylinders[df.cylinders.isna()]\n\n7138    NaN\n7139    NaN\n8143    NaN\n8144    NaN\n8146    NaN\n         ..\n34563   NaN\n34564   NaN\n34565   NaN\n34566   NaN\n34567   NaN\nName: cylinders, Length: 206, dtype: float64\n\n\n\ndf.cylinders.fillna(0).loc[7136:7141]\n\n7136    6.0\n7137    6.0\n7138    0.0\n7139    0.0\n7140    6.0\n7141    6.0\nName: cylinders, dtype: float64\n\n\n\n\n9.5 Interpolating Data\n\ntemp = pd.Series([32, 40, None, 42, 39, 32])\ntemp\n\n0    32.0\n1    40.0\n2     NaN\n3    42.0\n4    39.0\n5    32.0\ndtype: float64\n\n\n\ntemp.interpolate()\n\n0    32.0\n1    40.0\n2    41.0\n3    42.0\n4    39.0\n5    32.0\ndtype: float64\n\n\n\n\n9.6 Clipping Data\n\ncity_mpg.loc[:446]\n\n0      19\n1       9\n2      23\n3      10\n4      17\n       ..\n442    15\n443    15\n444    15\n445    15\n446    31\nName: city08, Length: 447, dtype: int64\n\n\n\n(city_mpg\n     .loc[:446]\n     .clip(lower=city_mpg.quantile(0.05),\n           upper=city_mpg.quantile(0.95))\n)\n\n0      19\n1      11\n2      23\n3      11\n4      17\n       ..\n442    15\n443    15\n444    15\n445    15\n446    27\nName: city08, Length: 447, dtype: int64\n\n\n.clip uses .where under the hood.\n\n\n9.7 Sorting Values\nThe .sort_values method will sort the values in ascending order and also rearrange the index accordingly.\n\ncity_mpg.sort_values()\n\n7901       6\n34557      6\n37161      6\n21060      6\n35887      6\n        ... \n34563    138\n34564    140\n32599    150\n31256    150\n33423    150\nName: city08, Length: 41144, dtype: int64\n\n\nBecause of index alignment, you can still do math operations on a sorted series:\n\n(city_mpg.sort_values() + highway_mpg) / 2\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\n\n\n9.8 Sorting the Index\nBelow we unsort the index by sorting the values, then essentially revert that:\n\ncity_mpg.sort_values().sort_index()\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\n\n\n9.9 Dropping Duplicates\nkeep='first' is the default value and keeps the first duplicate value found.\nkeep='last' keeps the last duplicate value found.\nkeep=False will remove any duplicated values (including the initial value).\ndrop_duplicates keep the original index.\n\ncity_mpg.drop_duplicates()\n\n0         19\n1          9\n2         23\n3         10\n4         17\n        ... \n34364    127\n34409    114\n34564    140\n34565    115\n34566    104\nName: city08, Length: 105, dtype: int64\n\n\n\n\n9.10 Ranking Data\nThe .rank method will return a series that keeps the original index but uses the ranks of values from the original series. By default, if two values are the same, their rank will be the average of the positions they take. You can specify method='min' to put equal values in the same rank and method='dense' to not skip any positions:\n\ncity_mpg.rank()\n\n0        27060.5\n1          235.5\n2        35830.0\n3          607.5\n4        19484.0\n          ...   \n41139    27060.5\n41140    29719.5\n41141    23528.0\n41142    23528.0\n41143    15479.0\nName: city08, Length: 41144, dtype: float64\n\n\n\ncity_mpg.rank(method='min')\n\n0        25555.0\n1          136.0\n2        35119.0\n3          336.0\n4        17467.0\n          ...   \n41139    25555.0\n41140    28567.0\n41141    21502.0\n41142    21502.0\n41143    13492.0\nName: city08, Length: 41144, dtype: float64\n\n\n\ncity_mpg.rank(method='dense')\n\n0        14.0\n1         4.0\n2        18.0\n3         5.0\n4        12.0\n         ... \n41139    14.0\n41140    15.0\n41141    13.0\n41142    13.0\n41143    11.0\nName: city08, Length: 41144, dtype: float64\n\n\n\n# a simpler example\npd.Series([1,1,2,3]).rank()\n\n0    1.5\n1    1.5\n2    3.0\n3    4.0\ndtype: float64\n\n\n\npd.Series([1,1,2,3]).rank(method='min')\n\n0    1.0\n1    1.0\n2    3.0\n3    4.0\ndtype: float64\n\n\n\npd.Series([1,1,2,3]).rank(method='dense')\n\n0    1.0\n1    1.0\n2    2.0\n3    3.0\ndtype: float64\n\n\n\n\n9.11 Replacing Data\nThe .replace method allows you to map values to new values.\n\ndf.make.replace('Subaru', 'SUBARU')\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            SUBARU\n            ...    \n41139        SUBARU\n41140        SUBARU\n41141        SUBARU\n41142        SUBARU\n41143        SUBARU\nName: make, Length: 41144, dtype: object\n\n\n\n# you can also use regex\ndf.make.replace(r'(Fer)ra(r.*)', value=r'\\2-other-\\1', regex=True)\n\n0          Alfa Romeo\n1        ri-other-Fer\n2               Dodge\n3               Dodge\n4              Subaru\n             ...     \n41139          Subaru\n41140          Subaru\n41141          Subaru\n41142          Subaru\n41143          Subaru\nName: make, Length: 41144, dtype: object\n\n\n\n\n9.12 Binning Data\nUsing the cut function, you can create bins of equal width:\n\npd.cut(city_mpg, 10)\n\n0        (5.856, 20.4]\n1        (5.856, 20.4]\n2         (20.4, 34.8]\n3        (5.856, 20.4]\n4        (5.856, 20.4]\n             ...      \n41139    (5.856, 20.4]\n41140    (5.856, 20.4]\n41141    (5.856, 20.4]\n41142    (5.856, 20.4]\n41143    (5.856, 20.4]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.856, 20.4] < (20.4, 34.8] < (34.8, 49.2] < (49.2, 63.6] ... (92.4, 106.8] < (106.8, 121.2] < (121.2, 135.6] < (135.6, 150.0]]\n\n\nYou can specify sizes for bin edges. In the following, 5 bins are created (so you need to provide 6 edges):\n\npd.cut(city_mpg, [0, 10, 20, 40, 70, 150])\n\n0        (10, 20]\n1         (0, 10]\n2        (20, 40]\n3         (0, 10]\n4        (10, 20]\n           ...   \n41139    (10, 20]\n41140    (10, 20]\n41141    (10, 20]\n41142    (10, 20]\n41143    (10, 20]\nName: city08, Length: 41144, dtype: category\nCategories (5, interval[int64, right]): [(0, 10] < (10, 20] < (20, 40] < (40, 70] < (70, 150]]\n\n\n\ncity_mpg\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\nNote the bins have a half-open interval. They do not have the start value but do include the end value. If the city_mpg series had values with 0 or values above 150, they would be missing after binning the series.\nIf you wanted 10 bins that had approximately the same number of entries in each bin (rather than each bin width being the same) use the qcut function:\n\npd.qcut(city_mpg, 10)\n\n0         (18.0, 20.0]\n1        (5.999, 13.0]\n2         (21.0, 24.0]\n3        (5.999, 13.0]\n4         (16.0, 17.0]\n             ...      \n41139     (18.0, 20.0]\n41140     (18.0, 20.0]\n41141     (17.0, 18.0]\n41142     (17.0, 18.0]\n41143     (15.0, 16.0]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.999, 13.0] < (13.0, 14.0] < (14.0, 15.0] < (15.0, 16.0] ... (18.0, 20.0] < (20.0, 21.0] < (21.0, 24.0] < (24.0, 150.0]]\n\n\nBoth allow you to set the labels to use instead of the categorical intervals they generate:\n\npd.qcut(city_mpg, 10, labels=list(range(1,11)))\n\n0        7\n1        1\n2        9\n3        1\n4        5\n        ..\n41139    7\n41140    7\n41141    6\n41142    6\n41143    4\nName: city08, Length: 41144, dtype: category\nCategories (10, int64): [1 < 2 < 3 < 4 ... 7 < 8 < 9 < 10]\n\n\nManipulation methods and properties:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ns.apply(func, convert_dtype=True, args=(), **kwds)\nPass in a NumPy function that works on the series, or a Python function that works on a single value. args and kwds are arguments for func. Returns a series, or dataframe if func returns a series.\n\n\ns.where(cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False)\nPass in a boolean series/dataframe, list, or callable as cond. If the value is True, keep it, otherwise use other value. If it is a function, it takes a series and should return a boolean sequence.\n\n\nnp.select(condlist, choicelist, default=0)\nPass in a list of boolean arrays for condlist. If the value is true use the corresponding value from choicelist. If multiple conditions are True, only use the first. Returns a NumPy array.\n\n\ns.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)\nPass in a scalar, dict, series or dataframe for value. If it is a scalar, use that value, otherwise use the index from the old value to the new value.\n\n\ns.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction=None, limit_area=None, downcast=None, **kwargs)\nPerform interpolation with missing values. method may be linear, time among others.\n\n\ns.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)\nReturn a new series with values clipped to lower and upper.\n\n\ns.sort_values(axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)\nReturn a series with values sorted. The kind option may be 'quicksort', 'mergesort' (stable), or 'heapsort'. na_position indicates location of NaNs and may be 'first' or 'last'.\n\n\ns.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)\nReturn a series with index sorted. The kind option may be 'quicksort', 'mergesort' (stable), or 'heapsort'. na_position indicates location of NaNs and may be 'first' or 'last'.\n\n\ns.drop_duplicates(keep='first', inplace=False)\nDrop duplicates. keep may be 'first', 'last', or False. (If False, it removes all values that were duplicated).\n\n\ns.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)\nReturn a series with numerical ranks. method allows you to specify tie handling. 'average', 'min', 'max', 'first' (usses order they appear in series), 'dense' (like 'min', but rank only increases by one after tie). na_option allows you to specify NaN handling. 'keep' (stay at NaN), 'top' (move to smallest), 'bottom' (move to largest).\n\n\ns.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')\nReturn a series with new values. to_replace can be many things. If it is a string, number or regular expression, you can replace it with a scalar value. It can also be a list of those things which requires value to be a list of the same size. Finally, it can be a dictionary mapping old values to new values.\n\n\npd.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise', ordered=True)\nBin values from x (a series). If bins is an integer, use equal-width bins. If bins is a list of numbers (defining minimum and maximum positions) use those for the edges. right defines whether the right edge is open or closed. labels allows you to specify the bin names. Out of bounds values will be missing.\n\n\npd.qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')\nBin values from x (a series) into q equal sized bins. Alternatively, can pass in a list of quantile edges. Out of bounds values will be missing.\n\n\n\n\n\n9.14 Exercises\n1. Create a series from a numeric column that has the value of 'high' if it is equal to or above the mean and 'low' if it is below the mean using .apply.\n\ncity_mpg\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ndef generalize_mean(x, mean_val):\n    if x >= mean_val:\n        return 'high'\n    return 'low'\n\ncity_mpg.apply(generalize_mean, mean_val=city_mpg.mean())\n\n0        high\n1         low\n2        high\n3         low\n4         low\n         ... \n41139    high\n41140    high\n41141     low\n41142     low\n41143     low\nName: city08, Length: 41144, dtype: object\n\n\n2. Create a series from a numeric column that has the value of 'high' if it is equal to or above the mean and 'low' if it is below the mean using np.select.\n\npd.Series(np.select([city_mpg.gt(city_mpg.mean())], ['high'], 'low'))\n\n0        high\n1         low\n2        high\n3         low\n4         low\n         ... \n41139    high\n41140    high\n41141     low\n41142     low\n41143     low\nLength: 41144, dtype: object\n\n\n3. Time the differences between the previous two solutions to see which is faster.\n\n%%timeit\ncity_mpg.apply(generalize_mean, mean_val=city_mpg.mean())\n\n22.3 ms ± 5.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\npd.Series(np.select([city_mpg.gt(city_mpg.mean())], ['high'], 'low'))\n\n4.45 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nnp.select is around 5 times as fast as .apply in this exercise.\n4. Replace the missing values of a numeric series with the median value.\n\ndf.cylinders.isna()[7136:7141]\n\n7136    False\n7137    False\n7138     True\n7139     True\n7140    False\nName: cylinders, dtype: bool\n\n\n\ndf.cylinders.fillna(df.cylinders.median())[7136:7141]\n\n7136    6.0\n7137    6.0\n7138    6.0\n7139    6.0\n7140    6.0\nName: cylinders, dtype: float64\n\n\n5.Clip the values of a numeric series to between the 10th and 90th percentiles.\n\nclip_s = pd.Series([1,2,3,4,5,6,7,8,9,10])\n(clip_s\n     .clip(lower=clip_s.quantile(0.1),\n           upper=clip_s.quantile(0.9))\n)\n\n0    1.9\n1    2.0\n2    3.0\n3    4.0\n4    5.0\n5    6.0\n6    7.0\n7    8.0\n8    9.0\n9    9.1\ndtype: float64\n\n\n6. Using a categorical column, replace any value that is not in the top 5 most frequent values with 'Other'.\n\ndf.fuelType.unique()\n\narray(['Regular', 'Premium', 'Diesel', 'CNG', 'Gasoline or natural gas',\n       'Gasoline or E85', 'Electricity', 'Gasoline or propane',\n       'Premium or E85', 'Midgrade', 'Premium Gas or Electricity',\n       'Regular Gas and Electricity', 'Premium and Electricity',\n       'Regular Gas or Electricity'], dtype=object)\n\n\n\ntop5 = df.fuelType.value_counts().index[:5]\ntop5\n\nIndex(['Regular', 'Premium', 'Gasoline or E85', 'Diesel', 'Electricity'], dtype='object', name='fuelType')\n\n\n\ndf.fuelType.where(df.fuelType.isin(top5), other='Other')\n\n0        Regular\n1        Regular\n2        Regular\n3        Regular\n4        Premium\n          ...   \n41139    Regular\n41140    Regular\n41141    Regular\n41142    Regular\n41143    Premium\nName: fuelType, Length: 41144, dtype: object\n\n\n7. Using a categorical column, replace any value that is not in the top 10 most frequent values with 'Other'.\n\ntop10 = df.fuelType.value_counts().index[:10]\ntop10\n\nIndex(['Regular', 'Premium', 'Gasoline or E85', 'Diesel', 'Electricity',\n       'Premium or E85', 'Midgrade', 'CNG', 'Premium and Electricity',\n       'Regular Gas and Electricity'],\n      dtype='object', name='fuelType')\n\n\n\ndf.fuelType.where(df.fuelType.isin(top10), other='Other')\n\n0        Regular\n1        Regular\n2        Regular\n3        Regular\n4        Premium\n          ...   \n41139    Regular\n41140    Regular\n41141    Regular\n41142    Regular\n41143    Premium\nName: fuelType, Length: 41144, dtype: object\n\n\n8. Make a function that takes a categorical series and a number (n) and returns a replace series that replaces any value that is not in the top n most frequent values with 'Other'.\n\ndef top_n_categorical(s, n):\n    top_n = s.value_counts().index[:n]\n    return s.where(s.isin(top_n), other='Other')\n\n\ns = top_n_categorical(df.fuelType, 10)\ns.unique()\n\narray(['Regular', 'Premium', 'Diesel', 'CNG', 'Other', 'Gasoline or E85',\n       'Electricity', 'Premium or E85', 'Midgrade',\n       'Regular Gas and Electricity', 'Premium and Electricity'],\n      dtype=object)\n\n\n\ns = top_n_categorical(df.fuelType, 5)\ns.unique()\n\narray(['Regular', 'Premium', 'Diesel', 'Other', 'Gasoline or E85',\n       'Electricity'], dtype=object)\n\n\n9. Using a numeric column, bin it into 10 groups that have the same width.\n\npd.cut(city_mpg, 10)\n\n0        (5.856, 20.4]\n1        (5.856, 20.4]\n2         (20.4, 34.8]\n3        (5.856, 20.4]\n4        (5.856, 20.4]\n             ...      \n41139    (5.856, 20.4]\n41140    (5.856, 20.4]\n41141    (5.856, 20.4]\n41142    (5.856, 20.4]\n41143    (5.856, 20.4]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.856, 20.4] < (20.4, 34.8] < (34.8, 49.2] < (49.2, 63.6] ... (92.4, 106.8] < (106.8, 121.2] < (121.2, 135.6] < (135.6, 150.0]]\n\n\n10. Using a numeric column, bin it into 10 groups that have equal sized bins.\n\npd.qcut(city_mpg, 10)\n\n0         (18.0, 20.0]\n1        (5.999, 13.0]\n2         (21.0, 24.0]\n3        (5.999, 13.0]\n4         (16.0, 17.0]\n             ...      \n41139     (18.0, 20.0]\n41140     (18.0, 20.0]\n41141     (17.0, 18.0]\n41142     (17.0, 18.0]\n41143     (15.0, 16.0]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.999, 13.0] < (13.0, 14.0] < (14.0, 15.0] < (15.0, 16.0] ... (18.0, 20.0] < (20.0, 21.0] < (21.0, 24.0] < (24.0, 150.0]]\n\n\nThis Stack Overflow response addresses the below error:\n\npd.qcut(df.cylinders, 10)\n\nValueError: Bin edges must be unique: array([ 2.,  4.,  4.,  4.,  5.,  6.,  6.,  6.,  8.,  8., 16.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg\n\n\nIf I decrease the number of bins to 5, the error is not raised:\n\npd.qcut(df.cylinders, 5)\n\n0        (1.999, 4.0]\n1         (8.0, 16.0]\n2        (1.999, 4.0]\n3          (6.0, 8.0]\n4        (1.999, 4.0]\n             ...     \n41139    (1.999, 4.0]\n41140    (1.999, 4.0]\n41141    (1.999, 4.0]\n41142    (1.999, 4.0]\n41143    (1.999, 4.0]\nName: cylinders, Length: 41144, dtype: category\nCategories (5, interval[float64, right]): [(1.999, 4.0] < (4.0, 5.0] < (5.0, 6.0] < (6.0, 8.0] < (8.0, 16.0]]"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-10-indexing-operations",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-10-indexing-operations",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 10: Indexing Operations",
    "text": "Chapter 10: Indexing Operations\nBoth a series and a dataframe have an index. Both types support the Python indexing operator ([]). Both have attributes .loc and .iloc that you can index against.\n\n10.1 Prepping the Data and Renaming the Index\nUse .rename method to hange the index labels. We can pass in a dictionary to map the previous index label to the new label:\n\nimport itertools\n\ndict(itertools.islice(df.make.to_dict().items(), 10))\n\n{0: 'Alfa Romeo',\n 1: 'Ferrari',\n 2: 'Dodge',\n 3: 'Dodge',\n 4: 'Subaru',\n 5: 'Subaru',\n 6: 'Subaru',\n 7: 'Toyota',\n 8: 'Toyota',\n 9: 'Toyota'}\n\n\n\ncity2 = city_mpg.rename(df.make.to_dict())\n\n\ncity2\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\n              ..\nSubaru        19\nSubaru        20\nSubaru        18\nSubaru        18\nSubaru        16\nName: city08, Length: 41144, dtype: int64\n\n\n\n# view the index\ncity2.index\n\nIndex(['Alfa Romeo', 'Ferrari', 'Dodge', 'Dodge', 'Subaru', 'Subaru', 'Subaru',\n       'Toyota', 'Toyota', 'Toyota',\n       ...\n       'Saab', 'Saturn', 'Saturn', 'Saturn', 'Saturn', 'Subaru', 'Subaru',\n       'Subaru', 'Subaru', 'Subaru'],\n      dtype='object', length=41144)\n\n\nThe .rename method also accepts a series, a scalar, a function that takes an old label and returns a new lable or a sequence. When we pass in a series and the index values are the same, the values from the series that we passed in are used as the index.\n\ncity2 = city_mpg.rename(df.make)\ncity2\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\n              ..\nSubaru        19\nSubaru        20\nSubaru        18\nSubaru        18\nSubaru        16\nName: city08, Length: 41144, dtype: int64\n\n\nIf you pass a scalar value (a single string) into .rename the index will stay the same but the .name attribute of the series will update:\n\ncity2.rename('citympg')\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\n              ..\nSubaru        19\nSubaru        20\nSubaru        18\nSubaru        18\nSubaru        16\nName: citympg, Length: 41144, dtype: int64\n\n\n\n\n10.2 Resetting the Index\n.reset_index by default will return a dataframe, moving the current index into a new column:\n\ncity2.reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      city08\n    \n  \n  \n    \n      0\n      Alfa Romeo\n      19\n    \n    \n      1\n      Ferrari\n      9\n    \n    \n      2\n      Dodge\n      23\n    \n    \n      3\n      Dodge\n      10\n    \n    \n      4\n      Subaru\n      17\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      41139\n      Subaru\n      19\n    \n    \n      41140\n      Subaru\n      20\n    \n    \n      41141\n      Subaru\n      18\n    \n    \n      41142\n      Subaru\n      18\n    \n    \n      41143\n      Subaru\n      16\n    \n  \n\n41144 rows × 2 columns\n\n\n\ndrop=True drops the current index and returns a Series.\n\ncity2.reset_index(drop=True)\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\nNote that .sort_values and .sort_index keep the same index but just rearrange the order so they do not impact operations that align on the index.\n\n\n10.3 The .loc Attribute\nThe .loc attribute deals with index labels. You can pass the following into an index operation on .loc:\n\nA scalar value of one of the index labels.\nA list of index labels.\nA slice of labels (closed interval so it includes the stop value).\nAn index.\nA boolean array (same index labels as the series, but with True and False values).\nA function that accepts a series and returns one of the above.\n\nIf there are duplicate labels in the index, and you pass in a scalar with the label of an index, it will return a series. If there is only one value for that label it will return a scalar.\n\ncity2.loc['Subaru']\n\nSubaru    17\nSubaru    21\nSubaru    22\nSubaru    19\nSubaru    20\n          ..\nSubaru    19\nSubaru    20\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, Length: 885, dtype: int64\n\n\n\ncity2.loc['Fisker']\n\n20\n\n\nIf you want to guarantee that a series is returned, pass in a list rather than passing in a scalar value:\n\ncity2.loc[['Fisker']]\n\nFisker    20\nName: city08, dtype: int64\n\n\n\ncity2.loc[['Ferrari', 'Lamborghini']]\n\nFerrari         9\nFerrari        12\nFerrari        11\nFerrari        10\nFerrari        11\n               ..\nLamborghini     6\nLamborghini     8\nLamborghini     8\nLamborghini     8\nLamborghini     8\nName: city08, Length: 357, dtype: int64\n\n\nSort the index if you are slicing with duplicate index labels:\n\ncity2.loc['Ferrari':'Lamborghini']\n\nKeyError: \"Cannot get left slice bound for non-unique label: 'Ferrari'\"\n\n\n\ncity2.sort_index().loc['Ferrari':'Lamborghini']\n\nFerrari        10\nFerrari        13\nFerrari        13\nFerrari         9\nFerrari        10\n               ..\nLamborghini    12\nLamborghini     9\nLamborghini     8\nLamborghini    13\nLamborghini     8\nName: city08, Length: 11210, dtype: int64\n\n\nSlicing with .loc follows the closed interval, includes both the start index and the final index.\nIf you have a sorted index, you can slice with strings that are not actual labels.\n\ncity2.sort_index().loc[\"F\":\"J\"]\n\nFederal Coach    15\nFederal Coach    13\nFederal Coach    13\nFederal Coach    14\nFederal Coach    13\n                 ..\nIsuzu            15\nIsuzu            15\nIsuzu            15\nIsuzu            27\nIsuzu            18\nName: city08, Length: 9040, dtype: int64\n\n\nYou can also pass an Index to .loc:\n\nidx = pd.Index(['Dodge'])\ncity2.loc[idx]\n\nDodge    23\nDodge    10\nDodge    12\nDodge    11\nDodge    11\n         ..\nDodge    18\nDodge    17\nDodge    14\nDodge    14\nDodge    11\nName: city08, Length: 2583, dtype: int64\n\n\nIf we duplicate 'Dodge' in the Index, the previous operation has twice as many values, a combinatoric explosion:\n\nidx = pd.Index(['Dodge', 'Dodge'])\ncity2.loc[idx]\n\nDodge    23\nDodge    10\nDodge    12\nDodge    11\nDodge    11\n         ..\nDodge    18\nDodge    17\nDodge    14\nDodge    14\nDodge    11\nName: city08, Length: 5166, dtype: int64\n\n\nYou can also pass a boolean array to .loc:\n\nmask = city2 > 50\nmask\n\nAlfa Romeo    False\nFerrari       False\nDodge         False\nDodge         False\nSubaru        False\n              ...  \nSubaru        False\nSubaru        False\nSubaru        False\nSubaru        False\nSubaru        False\nName: city08, Length: 41144, dtype: bool\n\n\n\ncity2.loc[mask]\n\nNissan     81\nToyota     81\nToyota     81\nFord       74\nNissan     84\n         ... \nTesla     140\nTesla     115\nTesla     104\nTesla      98\nToyota     55\nName: city08, Length: 236, dtype: int64\n\n\nYou can use a function with .loc. If I calculate the boolean array before taking into account the inflation, I get the wrong answer:\n\ncost = pd.Series([1.00, 2.25, 3.99, .99, 2.79],\n                 index=['Gum', 'Cookie', 'Melon', 'Roll', 'Carrots'])\n\ninflation = 1.10\n\nmask = cost > 3\n\n\n# wrong answer\n(cost\n     .mul(inflation)\n     .loc[mask]\n)\n\nMelon    4.389\ndtype: float64\n\n\n\n# right answer\n(cost\n     .mul(inflation)\n     .loc[lambda s_: s_ > 3]\n)\n\nMelon      4.389\nCarrots    3.069\ndtype: float64\n\n\nThere is an implicit return statement in the lambda function. You can only put an expression in it, you can have a statement. It is limited to a single line of code.\n\n\n10.4 The .iloc Attribute\nThe .iloc attribute supports indexing with the following:\n\nA scalar index position (an integer).\nA list of index positions.\nA slice of positions (half-open interval so it does not include stop value).\nA NumPy array (or Python list) of boolean values.\nA function that accepts a series and returns one of the above.\n\nBecause index positions are unique, we will always get the scalar value when indexing with .iloc at a position:\n\ncity2.iloc[0]\n\n19\n\n\n\ncity2.iloc[-1]\n\n16\n\n\nIf we want to return a series object, we can index it with a list of positions:\n\ncity2.iloc[[0]]\n\nAlfa Romeo    19\nName: city08, dtype: int64\n\n\n\ncity2.iloc[[0, 1, -1]]\n\nAlfa Romeo    19\nFerrari        9\nSubaru        16\nName: city08, dtype: int64\n\n\nWe can also use slices with .iloc (they follow the half-open interval):\n\ncity2.iloc[0:5]\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\nName: city08, dtype: int64\n\n\n\ncity2.iloc[-8:]\n\nSaturn    21\nSaturn    24\nSaturn    21\nSubaru    19\nSubaru    20\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, dtype: int64\n\n\nYou can use a NumPy array of booleans (or a Python list) but if you use what we call a boolean array (a pandas series with booleans), this will fail:\n\nmask = city2 > 50\ncity2.iloc[mask]\n\nValueError: iLocation based boolean indexing cannot use an indexable as a mask\n\n\n\ncity2.iloc[mask.to_numpy()]\n\nNissan     81\nToyota     81\nToyota     81\nFord       74\nNissan     84\n         ... \nTesla     140\nTesla     115\nTesla     104\nTesla      98\nToyota     55\nName: city08, Length: 236, dtype: int64\n\n\n\n\n10.5 Heads and Tails\n\ncity2.head(3)\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nName: city08, dtype: int64\n\n\n\ncity2.tail(3)\n\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, dtype: int64\n\n\n\n\n10.6 Sampling\nThe code below randomly pulls out six values:\n\ncity2.sample(6, random_state=42)\n\nVolvo         16\nMitsubishi    19\nBuick         27\nJeep          15\nLand Rover    13\nSaab          17\nName: city08, dtype: int64\n\n\n\n\n10.7 Filtering Index Values\nThe filter method will filter index labels by exact match (items), substring (like), or regex (regex).\n\n# exact match fails with duplicate index labels\ncity2.filter(items=['Ford', 'Subaru'])\n\nValueError: cannot reindex on an axis with duplicate labels\n\n\n\ncity2.filter(like='rd')\n\nFord    18\nFord    16\nFord    17\nFord    17\nFord    15\n        ..\nFord    26\nFord    19\nFord    21\nFord    18\nFord    19\nName: city08, Length: 3371, dtype: int64\n\n\n\ncity2.filter(regex='(Ford)|(Subaru)')\n\nSubaru    17\nSubaru    21\nSubaru    22\nFord      18\nFord      16\n          ..\nSubaru    19\nSubaru    20\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, Length: 4256, dtype: int64"
  },
  {
    "objectID": "posts/2024-07-01-dotproductbiasce/index.html",
    "href": "posts/2024-07-01-dotproductbiasce/index.html",
    "title": "Training a Collaborative Filtering Model Using Cross Entropy Loss",
    "section": "",
    "text": "In this notebook, I’ll work through the following prompt given in the “Further Research” of Chapter 8 (Collaborative Filtering):\n\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "posts/2024-07-01-dotproductbiasce/index.html#visual-inspection",
    "href": "posts/2024-07-01-dotproductbiasce/index.html#visual-inspection",
    "title": "Training a Collaborative Filtering Model Using Cross Entropy Loss",
    "section": "Visual Inspection",
    "text": "Visual Inspection\nI’ll start by visually inspecting the DotProductBias model from the chapter that outputs one prediction and a DotProductBiasCE model that I’ve written to output 5 predictions (so that it works with Cross Entropy loss).\n\n\n\nVisual inspection of DotProductBias Modules"
  },
  {
    "objectID": "posts/2024-07-01-dotproductbiasce/index.html#creating-dataloaders",
    "href": "posts/2024-07-01-dotproductbiasce/index.html#creating-dataloaders",
    "title": "Training a Collaborative Filtering Model Using Cross Entropy Loss",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\nSince I want to use Cross Entropy loss, I’ll need to specify that the outputs (or targets) are discrete categories and not continuous numbers. To do this, I’ll use TabularDataLoaders.\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\n\n\n\n\n    \n      \n      100.15% [4931584/4924029 00:00<00:00]\n    \n    \n\n\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndls = TabularDataLoaders.from_df(\n    ratings[['user', 'title', 'rating']],\n    procs=[Categorify],\n    cat_names=['user','title'],\n    y_names=['rating'],\n    y_block=CategoryBlock)\n\nTabularDataLoaders will provide three elements for each training item: the categorical inputs, the continuous inputs and the outputs (or targets, or dependent variable):\n\nb = dls.one_batch()\nlen(b), b[0].shape, b[1].shape, b[2].shape\n\n(3, torch.Size([64, 2]), torch.Size([64, 0]), torch.Size([64, 1]))\n\n\nThis is important to note before I create the model since the forward pass will receive two values: x_cat (categorical inputs) and x_cont (continuous inputs).\nThe TabularDataLoaders also has a vocabulary—the five possible values for the dependent variable (1 through 5).\n\ndls.vocab\n\n[1, 2, 3, 4, 5]"
  },
  {
    "objectID": "posts/2024-07-01-dotproductbiasce/index.html#creating-the-new-model",
    "href": "posts/2024-07-01-dotproductbiasce/index.html#creating-the-new-model",
    "title": "Training a Collaborative Filtering Model Using Cross Entropy Loss",
    "section": "Creating the New Model",
    "text": "Creating the New Model\nI’ll start by creating the original DotProductBias model for reference:\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\n\nThe biggest change in the behavior of the model, to allow the usage of Cross Entropy loss, is to make it output 5 activations instead of 1. I’ll do so by passing the dot product through an nn.Linear layer that projects that value into 5 dimensions (one for each rating 1-5) in the forward pass.\nThe second change in the model behavior is to allow two inputs in the forward pass: x_cat for categorical variables and x_cont for continuous variables, which is how TabularDataLoaders prepares the data. In the case of the MovieLens 100k subset, there are no continuous variables. The only variables of interest are the categoricals users and movies, one column for each in the input x_cat.\n\nclass DotProductBiasCE(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n    self.linear = nn.Linear(1, 5)\n\n  def forward(self, x_cat, x_cont):\n    x = x_cat\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    res = sigmoid_range(res, *self.y_range)\n    return self.linear(res)\n\n\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\n\nn_users, n_movies\n\n(944, 1665)\n\n\n\nmodel = DotProductBiasCE(n_users, n_movies, 50)\n\n\nmodel(x_cat=b[0], x_cont=b[1]).shape\n\ntorch.Size([64, 5])"
  },
  {
    "objectID": "posts/2024-07-01-dotproductbiasce/index.html#training-the-model",
    "href": "posts/2024-07-01-dotproductbiasce/index.html#training-the-model",
    "title": "Training a Collaborative Filtering Model Using Cross Entropy Loss",
    "section": "Training the Model",
    "text": "Training the Model\nI’ll use the same hyperparameters (5 epochs, LR=5e-3 and weight decay of 0.1) as the best training run in the text. Of course, this is a different model so these values may not be optimal.\n\nmodel = DotProductBiasCE(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.426626\n      1.445451\n      0.337150\n      00:15\n    \n    \n      1\n      1.281385\n      1.425528\n      0.339900\n      00:15\n    \n    \n      2\n      1.171326\n      1.431534\n      0.364500\n      00:16\n    \n    \n      3\n      1.105676\n      1.438475\n      0.361650\n      00:16\n    \n    \n      4\n      1.127306\n      1.438119\n      0.363450\n      00:14\n    \n  \n\n\n\nThe model’s not great (although it’s better than guessing ratings randomly which would have an accuracy of 20%) and it’s difficult to compare it with the DotProductBias model since that model only measured RMSE and not accuracy.\nI’ll take a look at the predictions and see how they compare to the actual ratings.\nLooking at the confusion matrix below, here are some observations:\n\nThe model did not predict any 5s.\nThe best predicted rating was a 4 (with 4327/6692, or 65% correct predictions).\nMost of the model’s predictions are 3s or 4s.\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "posts/2024-07-01-dotproductbiasce/index.html#improving-the-model",
    "href": "posts/2024-07-01-dotproductbiasce/index.html#improving-the-model",
    "title": "Training a Collaborative Filtering Model Using Cross Entropy Loss",
    "section": "Improving the Model",
    "text": "Improving the Model\nLooking at these results, I’m starting to think that using sigmoid_range in this model is causing it to predict values in the middle of the range (2-4) and making it harder for it to predict ratings that are at the edges (1 and 5). I’ll remove y_range and sigmoid_range from the model and train it again to see if it makes a difference.\n\nclass DotProductBiasCE(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.linear = nn.Linear(1, 5)\n\n  def forward(self, x_cat, x_cont):\n    x = x_cat\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return self.linear(res)\n\n\nmodel = DotProductBiasCE(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.392305\n      1.434367\n      0.353000\n      00:14\n    \n    \n      1\n      1.238957\n      1.465206\n      0.349450\n      00:15\n    \n    \n      2\n      1.122690\n      1.507049\n      0.354700\n      00:15\n    \n    \n      3\n      1.053334\n      1.523502\n      0.361450\n      00:14\n    \n    \n      4\n      1.038323\n      1.527965\n      0.364200\n      00:15\n    \n  \n\n\n\nThe resulting accuracy is about the same as before. Let’s look at the confusion matrix:\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model is now predicting 5s. Although now it’s not predicting any 1s or 2s! I’ll see if there’s a better learning rate for this architecture:\n\nmodel = DotProductBiasCE(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.005248074419796467)\n\n\n\n\n\n\n\n\n\nI’ll try a learning rate of 0.1, which is two orders of magnitude larger than 5e-3.\n\nmodel = DotProductBiasCE(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.fit_one_cycle(5, 0.1, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.402151\n      1.460189\n      0.337900\n      00:14\n    \n    \n      1\n      1.410952\n      1.438032\n      0.348000\n      00:13\n    \n    \n      2\n      1.353970\n      1.407190\n      0.370100\n      00:13\n    \n    \n      3\n      1.232812\n      1.351564\n      0.401050\n      00:13\n    \n    \n      4\n      1.070482\n      1.324203\n      0.415400\n      00:14\n    \n  \n\n\n\nThe higher learning rate improved the accuracy by about 5%. Looking at the confusion matrix, here are some observations—the model is predicting 1s and 2s better than before and the rating of 4 is still the best predicted rating (62% of all actual 4s are predicted as 4s). However, the model is still predominantly predicting 3s and 4s.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ll also check that fastai is automatically applying softmax so that the final activations add up to 1.00:\n\nprobs, _ = learn.get_preds(dl=dls.valid)\nprobs.sum(dim=1)\n\n\n\n\n\n\n\n\ntensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000])\n\n\n\nprobs.sum(dim=1).sum() # should equal 20k\n\ntensor(20000.)"
  },
  {
    "objectID": "posts/2024-07-01-dotproductbiasce/index.html#final-thoughts",
    "href": "posts/2024-07-01-dotproductbiasce/index.html#final-thoughts",
    "title": "Training a Collaborative Filtering Model Using Cross Entropy Loss",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’ll recap this exercise by displaying the visual comparison between DotProductBias and the final DotProductBiasCE (without y_range and sigmoid_range).\n\n\n\nVisual inspection of DotProductBias Modules\n\n\nMy main takeaway from this exercise is that what may work for one architecture may not necessarily work for another. In this example when passing the dot product through the sigmoid function before passing it through a linear layer, the model did not predict any 5s even though there were many actual 5 ratings in the dataset.\nAnother takeaway is that I wasn’t able to compare two models that used different metrics (RMSE vs. accuracy). So I’m limited in my ability to say which model performed “better”. I asked Claude for ideas on how to compare these two models and it came up with the following:\n\nConvert RMSE to accuracy\n\nRound continuous predictions to nearest integer\nCalculate accuracy using rounded predictions\nCompare this accuracy to the categorical model\n\nConvert accuracy to RMSE-like metric:\n\nCalculate average error for categorical predictions\nCompare this to the continuous model’s RMSE\n\nUse normalized metrics:\n\nNormalize RMSE: RMSE / (max_rating - min_rating)\nNormalize accuracy: (accuracy - random_guess_accuracy) / (1 - random_guess_accuracy)\nCompare normalized values\n\n\nI’ll poke around online (I’ve also asked about this on Twitter and the fastai forums) to see if there are thoughts on or examples of such comparisons, and then follow up with some exploration in a future blog post.\nI hope you enjoyed this exercise! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "",
    "text": "This notebook is a part of series of blog posts for a project I’m calling fastbookRAG where I’m trying to answer questions from the fastbook end-of-chapter Questionnaires using the following pipeline:\n\n\n\nfastbookRAG diagram\n\n\nThis notebook establishes a baseline using semantic search (Cosine Similarity) for retrieval on chunks of the fastbook chapters covered in Part 1 of the fastai course (1, 2, 4, 8, 9, 10, and 13).\nThe evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nThe goal is to retrieve the context necessary to answer all questions. Currently, I manually assess answers, a role that will eventually be performed by LLMs in the final pipeline."
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#summary-of-results",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#summary-of-results",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "Summary of Results",
    "text": "Summary of Results\nHere are the results from my experiments in this notebook—in general, the best performing semantic search method (80.31% Answer Rate overall) was retrieving the top-5 (by Cosine Similarity) 3-paragraph chunks:\n\n\n\n\n\n\n\n\n\n\n\n\nChapter\nCS_A (Top-1 1p)\nCS_B (Top-3 1p)\nCS_C (Top-5 1p)\nCS_D (Top-1 3p)\nCS_E (Top-3 3p)\nCS_F (Top-5 3p)\n\n\n\n\n1\n40% (12/30)\n63.33% (19/30)\n63.33% (19/30)\n46.67% (14/30)\n80% (24/30)\n90% (27/30)\n\n\n2\n26.92% (7/26)\n61.54% (16/26)\n69.23% (18/26)\n53.85% (14/26)\n80.77% (21/26)\n84.62% (22/26)\n\n\n4\n29.03% (9/31)\n54.84% (17/31)\n64.52% (20/31)\n25.81% (8/31)\n67.74% (21/31)\n80.65% (25/31)\n\n\n8\n17.39% (4/23)\n43.48% (10/23)\n47.83% (11/23)\n43.48% (10/23)\n73.91% (17/23)\n91.30% (21/23)\n\n\n9\n28.57% (8/28)\n46.43% (13/28)\n53.57% (15/28)\n42.86% (12/28)\n57.14% (16/28)\n75% (21/28)\n\n\n10\n42.86% (9/21)\n47.62% (10/21)\n47.62% (10/21)\n47.62% (10/21)\n52.38% (11/21)\n57.14% (12/21)\n\n\n13\n41.18% (14/34)\n58.82% (20/34)\n61.76% (21/34)\n47.06% (16/34)\n70.59% (24/34)\n79.41% (27/34)\n\n\nAll\n32.64% (63/193)\n54.40% (105/193)\n59.07% (114/193)\n43.52% (84/193)\n69.43% (134/193)\n80.31% (155/193)"
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#experimental-setup",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#experimental-setup",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "Experimental Setup",
    "text": "Experimental Setup\n\nData Sources\n\nThe freely available Jupyter Notebook-written fastbook.\n\n\n\nData Preprocessing\n\nChunking strategy: Single or multiple paragraphs with corresponding headers.\nRationale: Balances granular content with high-level context.\nGoal: Maintain lean, informative chunks for efficient retrieval.\n\n\n\nDatabase\nI am using a tensor to store the text embeddings for chunks and queries."
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#methodology",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#methodology",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "Methodology",
    "text": "Methodology\n\nWhy Cosine Similarity?\nWhile keyword search approaches resulted in an Answer Rate of up to 76.7% overall (7 chapters) I think there is room for improvement. I expect that for some of the questions where keywords search did not retrieve appropriate context, semantic search will. Why? Because there exists chunks of context that contain the answer to a question without containing the exact keywords explicitly. After performing a question-by-question error analysis (for the 39 questions that none of the keyword search approaches retrieved sufficient context) I expect 23 of those questions (11% of the dataset overall) better suited for a semantic search-based context retrieval.\n\n\nEvaluation Set\nMy evaluation set consists of:\n\n_ Questionnaire questions.\n“Gold standard” solutions to the Questionnaire published by fast.ai Leader Tanishq Abraham who says:\n\n\nmy responses are based on what is supported by the chapter text\n\n(Which is perfect for my retrieval task.)\n\n\nEvaluation Metrics\nMetrics: Score and Answer Rate\nThe evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nWhile this is a straightforward pair of metrics, they do involve some judgment. After reading the retrieved context, I decide if it’s enough to answer the question. A capable LLM should be able to make the same kind of judgment about whether the context is helpful or not."
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#results",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#results",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "Results",
    "text": "Results\nHere are the names and descriptions of each full text search approach explored in this notebook. Top-n means the chunk(s) with the n-highest Cosine Similarity (CS).\n\n\n\nName\nDescription\n\n\n\n\nCS_A\nTop-1 1-Paragraph Chunks\n\n\nCS_B\nTop-3 1-Paragraph Chunks\n\n\nCS_C\nTop-5 1-Paragraph Chunks\n\n\nCS_D\nTop-1 3-Paragraph Chunks\n\n\nCS_E\nTop-3 3-Paragraph Chunks\n\n\nCS_F\nTop-5 3-Paragraph Chunks\n\n\n\n\nBest Approach per Chapter\nThe following table shows name, description and Answer Rate for the best semantic search approach for each Chapter.\nCS_F (Top-5 3-paragraph Chunks) was the best performing approach for all chapters and overall.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nCS_F\nTop-5 3-Paragraph Chunks\n90%\n\n\n2\nCS_F\nTop-5 3-Paragraph Chunks\n84.62%\n\n\n4\nCS_F\nTop-5 3-Paragraph Chunks\n80.65%\n\n\n8\nCS_F\nTop-5 3-Paragraph Chunks\n91.30%\n\n\n9\nCS_F\nTop-5 3-Paragraph Chunks\n75%\n\n\n10\nCS_F\nTop-5 3-Paragraph Chunks\n57.14%\n\n\n13\nCS_F\nTop-5 3-Paragraph Chunks\n79.41%\n\n\nAll\nCS_F\nTop-5 3-Paragraph Chunks\n80.31%\n\n\n\nA couple observations:\n\nChapter 1 had the highest Answer Rate overall (90%), as it did for the BM25 baselines with the same Answer Rate.\nChapter 10 had the lowest Answer Rate overall (57.14%), as it did for the BM25 baselines. However, 57.14% is lower than the best BM25 Answer Rate for the chapter (61.9%).\n\n\n\nAll Approaches for All Chapters\nThe following table shows the Answer Rate for all Cosine Similarity (CS) approaches for each chapter (where in the header, 1p = 1-paragraph chunks and 3p = 3-paragraph chunks).\n\n\n\n\n\n\n\n\n\n\n\n\nChapter\nCS_A (Top-1 1p)\nCS_B (Top-3 1p)\nCS_C (Top-5 1p)\nCS_D (Top-1 3p)\nCS_E (Top-3 3p)\nCS_F (Top-5 3p)\n\n\n\n\n1\n40% (12/30)\n63.33% (19/30)\n63.33% (19/30)\n46.67% (14/30)\n80% (24/30)\n90% (27/30)\n\n\n2\n26.92% (7/26)\n61.54% (16/26)\n69.23% (18/26)\n53.85% (14/26)\n80.77% (21/26)\n84.62% (22/26)\n\n\n4\n29.03% (9/31)\n54.84% (17/31)\n64.52% (20/31)\n25.81% (8/31)\n67.74% (21/31)\n80.65% (25/31)\n\n\n8\n17.39% (4/23)\n43.48% (10/23)\n47.83% (11/23)\n43.48% (10/23)\n73.91% (17/23)\n91.30% (21/23)\n\n\n9\n28.57% (8/28)\n46.43% (13/28)\n53.57% (15/28)\n42.86% (12/28)\n57.14% (16/28)\n75% (21/28)\n\n\n10\n42.86% (9/21)\n47.62% (10/21)\n47.62% (10/21)\n47.62% (10/21)\n52.38% (11/21)\n57.14% (12/21)\n\n\n13\n41.18% (14/34)\n58.82% (20/34)\n61.76% (21/34)\n47.06% (16/34)\n70.59% (24/34)\n79.41% (27/34)\n\n\nAll\n32.64% (63/193)\n54.40% (105/193)\n59.07% (114/193)\n43.52% (84/193)\n69.43% (134/193)\n80.31% (155/193)\n\n\n\nA few observations when looking at the Answer Rate for each approach for each chapter, similar to the BM25 baselines:\n\nIncreasing the number of chunks retrieved generally improves the quality of information retrieved:\n\nFor all chapters: CS_C >= CS_B >= CS_A and CS_F >= CS_E >= CS_D.\n\nIncreasing the chunk size generally improves the quality of information retrieved:\n\nFor 6 out of 7 chapters: CS_D > CS_A, CS_E > CS_B, CS_F > CS_C\n\nFor Chapter 4: CS_D < CS_A.\n\n\nNot all chapters behave the same: For some Chapters, like chapter 10, increasing the number of 1-paragraph chunks retrieved from 3 to 5 did not improve the Answer Rate, while for other chapters it did.\n\n\n\nQuestion-Level Analysis\nLooking at the question-level data offers some additional insights.\n\nimport pandas as pd\nurl = 'https://gist.githubusercontent.com/vishalbakshi/9e0dc5b83c9b02810099f53377ced4ba/raw/3860f7dac972f37cc84cd10e22184c2bfd8813a4/cs_all.csv'\ndf = pd.read_csv(url)\nscore_columns = df.filter(regex='_score$').columns\ndf['total_score'] = df[score_columns].sum(axis=1)\n\n\nDistribution of Scores\nSurprisingly, I have the exact same observations about this distribution as the BM25 baseline results:\n\nBimodal Distribution\n\nApproximately 50 questions (about 25% of the total) were successfully answered by all six full text search methods (a total score of 6).\nOn the other hand, around 40 questions (about 20%) couldn’t be answered by any method, resulting in a total score of 0.\n\nUniform Mid-Range Performance\n\nQuestions answered by 2, 3, 4, or 5 methods each accounted for 20-30 instances, showing a relatively even distribution in this middle range.\n\nLeast Common Outcome\n\nOnly 10 questions were answered by just one method, making this the least frequent result.\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\n\ndf['total_score'].hist(bins=range(0, 8), align='left', rwidth=0.8);\n\nplt.title('Distribution of Total Scores')\nplt.xlabel('Total Score')\nplt.ylabel('Number of Questions');\n\n\n\n\n\n\n\n\n\n\nAverage Score Per Question\nOn average, each question was answered by about 3 semantic search methods.\n\ndf['total_score'].describe()\n\n\n\n\n\n  \n    \n      \n      total_score\n    \n  \n  \n    \n      count\n      193.000000\n    \n    \n      mean\n      3.393782\n    \n    \n      std\n      2.153090\n    \n    \n      min\n      0.000000\n    \n    \n      25%\n      2.000000\n    \n    \n      50%\n      3.000000\n    \n    \n      75%\n      6.000000\n    \n    \n      max\n      6.000000\n    \n  \n\ndtype: float64\n\n\n\n\nUnanswered Questions\nThere were 29 questions for which none of the semantic search approaches retrieved the context needed to answer them.\n\nno_answer = df.query(\"total_score == 0\")[['chapter', 'question_number', 'question_text', 'answer']].drop_duplicates()\nno_answer.shape\n\n(29, 4)\n\n\n\n\nQuestions with 100% Answer Rate\nThere were 51 questions that were answered by all 6 semantic search methods.\n\nall_answer = df.query(\"total_score == 6\")[['chapter', 'question_number', 'question_text']].drop_duplicates()\nall_answer.shape\n\n(51, 3)\n\n\nIt’s worth noting that semantic search successfully retrieved relevant context for the following two questions, where none of the full-text search methods were able to do so.\n\nall_answer.iloc[27]['question_text']\n\n'\"\"What is a categorical variable?\"\"'\n\n\n\nall_answer.iloc[37]['question_text']\n\n'\"\"Why do we have to pass the vocabulary of the language model to the classifier data block?\"\"'\n\n\nOnly 1 of the full text search methods retrieved relevant context for the following three questions where all semantic search methods were able to do so.\n\nall_answer.iloc[6]['question_text']\n\n'\"\"What is an \"\"architecture\"\"?\"\"'\n\n\n\nall_answer.iloc[24]['question_text']\n\n'\"\"Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\"\"'\n\n\n\nall_answer.iloc[41]['question_text']\n\n'\"\"What is a \"\"channel\"\"?\"\"'\n\n\nI’ve done a detailed analysis of questions where semantic search performed unanimously better than full text search (and vice versa) in [this notebook].\n\n\n\nResults CSV\nThe retrieved contexts and my manually assigned scores for each question and semantic search baseline are available in this public gist."
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#limitations",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#limitations",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "Limitations",
    "text": "Limitations\nThere are a number of limitations that I want to highlight in this work, the first and last two are also applicable to my full text search work:\n\nLimited methods: There are inumerable combinations of chunk strategies and top-n retrieval choices. I chose the six (1-paragraph/3-paragraph and Top-1/Top-3/Top-5) that seemed easy to implement, reasonable to accomplish in my desired timeline and reasonably provided me with a diverse set of results.\nLimited scope: I’m only considering the 193 questions in the end-of-chapter Questionnaires whose answer was explicitly in the fastbook text. There are endless questions about the topics covered in the fastbook. I only focused on the 7 chapters covered in Part 1 of the fastai course (as I am still in progress with Part 2). A more general-purpose QA task for deep learning and machine learning would likely require a different set of evals.\nI only used one embedding model: There models other than BAAI/bge-small-en-v1.5, some that create larger embeddings, that may yield better results.\nI used my own judgment: I had to use my judgment to determine whether the retrieved context was sufficient for answering the given question. This is a fuzzy evaluation method.\nI used the official Questionnaire solutions: There is room for interpretation when answering open-ended questions. I chose to strictly follow the “gold standard” answers provided in the course Forums."
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#future-work",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#future-work",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "Future Work",
    "text": "Future Work\nEach of the limitations provides an opportunity for future work:\n\nExperiment with different chunking strategies and observe their impact on retrieval performance.\nExpanding the eval set to include more chapters and question types.\nExperiment with different embedding models.\nIntegrate an LLM to replace my own judgment in the pipeline (something that I’ll be doing as part of the broader fastbookRAG project).\nConducting a deep dive into error analysis to understand why certain questions weren’t answerable (something I’ll do before I conduct any further experiments).\nRemoving questions from my evals that do not have explicit answers in the chapter text (something I’ll do before I conduct any further experiments).\nDeveloping my own set of standardized answers (with the use of an LLM) for each question to ensure consistency."
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#experiments",
    "href": "posts/2024-10-22-fastbookRAG-cs-baselines/index.html#experiments",
    "title": "Establishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project",
    "section": "Experiments",
    "text": "Experiments\n\nHelper Functions\n\n\nShow imports\nimport sqlite3\nimport json\nimport re\nimport os\nimport pandas as pd, numpy as np\nimport requests\nimport torch.nn.functional as F\n\n\n\n\nShow chunking code\ndef get_chunks(notebook_path):\n    with open(notebook_path, 'r', encoding='utf-8') as file:\n        notebook = json.load(file)\n\n    chunks = []\n    current_header = \"\"\n\n    def add_chunk(content):\n        if content.strip():\n            chunks.append(f\"{current_header}\\n\\n{content.strip()}\")\n\n    for cell in notebook['cells']:\n        if cell['cell_type'] == 'markdown':\n            content = ''.join(cell['source'])\n            # see if the cell starts with a markdown header\n            header_match = re.match(r'^(#+\\s+.*?)$', content, re.MULTILINE)\n            if header_match:\n                # grab the header\n                current_header = header_match.group(1)\n                # add any content after the header in the same cell\n                remaining_content = content[len(current_header):].strip()\n                if remaining_content:\n                    # split content into paragraphs\n                    paragraphs = re.split(r'\\n\\s*\\n', remaining_content)\n                    # append the paragraph to the list of chunks\n                    for paragraph in paragraphs:\n                        add_chunk(paragraph)\n            else:\n                # split content into paragraphs\n                paragraphs = re.split(r'\\n\\s*\\n', content)\n                # append the paragraph to the list of chunks\n                for paragraph in paragraphs:\n                    add_chunk(paragraph)\n        elif cell['cell_type'] == 'code':\n          code_content = '```python\\n' + ''.join(cell['source']) + '\\n```'\n\n          # include the output of the code cell\n          output_content = ''\n          if 'outputs' in cell and cell['outputs']:\n              for output in cell['outputs']:\n                  if 'text' in output:\n                      output_content += ''.join(output['text'])\n                  elif 'data' in output and 'text/plain' in output['data']:\n                      output_content += ''.join(output['data']['text/plain'])\n\n          # combine code and output in the same chunk\n          combined_content = code_content + '\\n\\nOutput:\\n' + output_content if output_content else code_content\n          add_chunk(combined_content)\n\n    def filter_chunks(chunks, exclude_headers=[\"Questionnaire\", \"Further Research\"]):\n      filtered_chunks = []\n      for chunk in chunks:\n          lines = chunk.split('\\n')\n          # check if the first line (header) is in the exclude list\n          if not any(header in lines[0] for header in exclude_headers):\n              filtered_chunks.append(chunk)\n      return filtered_chunks\n\n    return filter_chunks(chunks)\n\n\n\n\nData Preprocessing\nYou can download the notebooks from the fastbook repo or run the following cell to download them.\n\nurls = {\n    '01_intro.ipynb': 'https://drive.google.com/uc?export=view&id=1mmBjFH_plndPBC4iRZHChfMazgBxKK4_',\n    '02_production.ipynb': 'https://drive.google.com/uc?export=view&id=1Cf5QHthHy1z13H0iu3qrzAWgquCfqVHk',\n    '04_mnist_basics.ipynb': 'https://drive.google.com/uc?export=view&id=113909_BNulzyLIKUNJHdya0Hhoqie30I',\n    '08_collab.ipynb': 'https://drive.google.com/uc?export=view&id=1BtvStgFjUtvtqbSZNrL7Y2N-ey3seNZU',\n    '09_tabular.ipynb': 'https://drive.google.com/uc?export=view&id=1rHFvwl_l-AJLg_auPjBpNrOgG9HDnfqg',\n    '10_nlp.ipynb': 'https://drive.google.com/uc?export=view&id=1pg1pH7jMMElzrXS0kBBz14aAuDsi2DEP',\n    '13_convolutions.ipynb': 'https://drive.google.com/uc?export=view&id=19P-eEHpAO3WrOvdxgXckyhHhfv_R-hnS'\n}\n\ndef download_file(url, filename):\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Open the file in write-binary mode\n        with open(filename, 'wb') as file:\n            # Write the content of the response to the file\n            file.write(response.content)\n        print(f\"File downloaded successfully: {filename}\")\n    else:\n        print(f\"Failed to download file. Status code: {response.status_code}\")\n\nfor fname, url in urls.items():\n  download_file(url, fname)\n\nI have seven notebooks in total. I’ll start by using get_chunks to split the notebook content into paragraphs (with the corresponding header).\n\n\nShow the dict w/ notebook filenames\nnbs = {\n    '1': '01_intro.ipynb',\n    '2': '02_production.ipynb',\n    '4': '04_mnist_basics.ipynb',\n    '8': '08_collab.ipynb',\n    '9': '09_tabular.ipynb',\n    '10': '10_nlp.ipynb',\n    '13': '13_convolutions.ipynb'\n}\n\n\n\n# chunking each notebook\ndata = {}\n\nfor chapter, nb in nbs.items():\n  data[chapter] = get_chunks(nb)\n\nI’ll print out the length of the total chunks so I get a sense of how many unique chunks there are:\n\ntotal_chunks = 0\nfor chapter, chunks in data.items():\n  print(chapter, len(chunks))\n  total_chunks += len(chunks)\n\nassert total_chunks == 1967 # 1-paragraph chunks\n\n1 307\n2 227\n4 433\n8 157\n9 387\n10 190\n13 266\n\n\n\n\nEmbed the Data\nI’ll create text embeddings for the chunks using a popular embedding model.\n\n!pip install sentence-transformers -Uqq\nfrom sentence_transformers import SentenceTransformer\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n\n\ndata_embs = {}\n\nfor chapter, chunks in data.items():\n  data_embs[chapter] = emb_model.encode(chunks, convert_to_tensor=True)\n\n\nimport pickle\n\nwith open('data_embs.pkl', 'rb') as f:\n    data_embs = pickle.load(f)\n\n\nfor chapter, embs in data_embs.items():\n  print(chapter, embs.shape)\n\n1 torch.Size([307, 384])\n2 torch.Size([227, 384])\n4 torch.Size([433, 384])\n8 torch.Size([157, 384])\n9 torch.Size([387, 384])\n10 torch.Size([190, 384])\n13 torch.Size([266, 384])\n\n\n\n\nLoad and Embed the Question Texts\nI have saved each chapter’s questions and answers in this gist. Note that the total number of questions (193) is different than the total number of questions for the keyword searh evals (202) since after error analysis I deemed that some questions were unanswerable using the chapter text (they were ambiguously worded, were exercises meant to be done by the reader, and/or the chapter text did not contain enough relevant explanatory text to answer the question).\n\nimport pandas as pd\nurl = 'https://gist.githubusercontent.com/vishalbakshi/fa90ec0172924091fa97bb0971b3a713/raw/b5e801c4d887edebc8de4097b44eff49d15d6b49/fastbookRAG_evals_CS.csv'\nquestions = pd.read_csv(url)\nquestions.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nquestions.shape\n\n(193, 5)\n\n\n\nq_embs = {}\n\nfor chapter, _ in data.items():\n  qs = questions[questions['chapter'] == int(chapter)].reset_index()['question_text']\n  q_embs[chapter] = emb_model.encode(qs, convert_to_tensor=True)\n  print(chapter, qs.shape, q_embs[chapter].shape)\n\n1 (30,) torch.Size([30, 384])\n2 (26,) torch.Size([26, 384])\n4 (31,) torch.Size([31, 384])\n8 (23,) torch.Size([23, 384])\n9 (28,) torch.Size([28, 384])\n10 (21,) torch.Size([21, 384])\n13 (34,) torch.Size([34, 384])\n\n\n\nwith open('q_embs.pkl', 'rb') as f:\n    q_embs = pickle.load(f)\n\n\nfor c, e in q_embs.items():\n  print(c,e.shape)\n\n1 torch.Size([30, 384])\n2 torch.Size([26, 384])\n4 torch.Size([31, 384])\n8 torch.Size([23, 384])\n9 torch.Size([28, 384])\n10 torch.Size([21, 384])\n13 torch.Size([34, 384])\n\n\n\n\nCS_A: Top-1 1-Paragraph Chunks\nIn this approach, I’ll select the top-1 retrieved context (1-paragraph chunk) for each question’s keywords and calculate the Answer Rate. As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nI needed to think through this a bit so I’ll walk through my process. I start by adding a unit axis to the data and questions’ embeddings. This prepares it for broadcasting. Here’s an image showing the concept:\n\n\n\nBroadcasting data and questions embeddings\n\n\n\ndata_embs['1'].unsqueeze(1).shape, q_embs['1'].unsqueeze(0).shape\n\n(torch.Size([307, 1, 384]), torch.Size([1, 30, 384]))\n\n\nI can either add the unit axis as the first or second position of the embeddings and get the same result after passing them through F.cosine_similarity:\n\nres1 = F.cosine_similarity(q_embs['1'].unsqueeze(1), data_embs['1'].unsqueeze(0), dim=2)\n\n\nres2 = F.cosine_similarity(q_embs['1'].unsqueeze(0), data_embs['1'].unsqueeze(1), dim=2)\n\n\nres1.shape, res2.T.shape\n\n(torch.Size([30, 307]), torch.Size([30, 307]))\n\n\n\n(res1 == res2.T).float().mean(), (res1.T == res2).float().mean()\n\n(tensor(1.), tensor(1.))\n\n\nI calculate the cosine similarity between the Chapter 1 questions and 1-paragraph chunks, and then sort them in descending order. I take the top-1 highest cosine similarity chunk as the retrieved context for each question.\n\nvals, idxs = F.cosine_similarity(q_embs['1'].unsqueeze(1), data_embs['1'].unsqueeze(0), dim=2).sort(descending=True)\ntop_1_idxs = idxs[:, 0]\n\n\ntop_1_idxs.shape, top_1_idxs[:5]\n\n(torch.Size([30]), tensor([ 4,  7, 15, 20, 16]))\n\n\n\ntop_1_chunks = [data['1'][idx] for idx in top_1_idxs]\n\n\nlen(top_1_chunks)\n\n30\n\n\nThe context retrieved for the first question is correct:\n\ntop_1_chunks[0]\n\n'## Deep Learning Is for Everyone\\n\\n```asciidoc\\n[[myths]]\\n.What you don\\'t need to do deep learning\\n[options=\"header\"]\\n|======\\n| Myth (don\\'t need) | Truth\\n| Lots of math | Just high school math is sufficient\\n| Lots of data | We\\'ve seen record-breaking results with <50 items of data\\n| Lots of expensive computers | You can get what you need for state of the art work for free\\n|======\\n```'\n\n\nI’ll now loop through each chapter, in order from 1 to 13, and retrieve the top-1 1-paragraph chunk based on cosine similarity between the chapter questions and the chapter chunks:\n\nresults = []\nfor chapter in ['1', '2', '4', '8', '9', '10', '13']:\n  _, idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_embs[chapter].unsqueeze(0), dim=2).sort(descending=True)\n  top_1_idxs = idxs[:, 0]\n  top_1_chunks = [data[chapter][idx] for idx in top_1_idxs]\n  results.extend(top_1_chunks)\n\nI should have 193 chunks retrieved (which I do!)\n\nassert len(results) == 193\n\nI’ll add the retrieved contexts to my evals and export it to evaluate it.\n\ncs_a = questions.copy()\ncs_a['cs_a_context'] = results\ncs_a.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      cs_a_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      ## Neural Networks: A Brief History\\n\\nAn MIT ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncs_a.to_csv('cs_a.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall). As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nCS_A\nTop-1 1-paragraph chunks\n40% (12/30)\n\n\n2\nCS_A\nTop-1 1-paragraph chunks\n26.92% (7/26)\n\n\n4\nCS_A\nTop-1 1-paragraph chunks\n29.03% (9/31)\n\n\n8\nCS_A\nTop-1 1-paragraph chunks\n17.39% (4/23)\n\n\n9\nCS_A\nTop-1 1-paragraph chunks\n28.57% (8/28)\n\n\n10\nCS_A\nTop-1 1-paragraph chunks\n42.86% (9/21)\n\n\n13\nCS_A\nTop-1 1-paragraph chunks\n41.18% (14/34)\n\n\nAll\nCS_A\nTop-1 1-paragraph chunks\n32.64% (63/193)\n\n\n\n\n\n\nCS_B: Top-3 1-Paragraph Chunks\nIn this approach, I’ll select the top-3 retrieved context (1-paragraph chunks) for each question and calculate the Answer Rate. As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\nresults = []\nfor chapter in ['1', '2', '4', '8', '9', '10', '13']:\n  _, idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_embs[chapter].unsqueeze(0), dim=2).sort(descending=True)\n  top_3_chunks = ['\\n\\n'.join([data[chapter][i] for i in row[0:3].tolist()]) for row in idxs]\n  results.extend(top_3_chunks)\n\n\nassert len(results) == 193\n\n\ncs_b = questions.copy()\ncs_b['cs_b_context'] = results\ncs_b.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      cs_b_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      ## Neural Networks: A Brief History\\n\\nAn MIT ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncs_b.to_csv('cs_b.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall). As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nCS_B\nTop-3 1-paragraph chunks\n63.33% (19/30)\n\n\n2\nCS_B\nTop-3 1-paragraph chunks\n61.54% (16/26)\n\n\n4\nCS_B\nTop-3 1-paragraph chunks\n54.84% (17/31)\n\n\n8\nCS_B\nTop-3 1-paragraph chunks\n43.48% (10/23)\n\n\n9\nCS_B\nTop-3 1-paragraph chunks\n46.43% (13/28)\n\n\n10\nCS_B\nTop-3 1-paragraph chunks\n47.62% (10/21)\n\n\n13\nCS_B\nTop-3 1-paragraph chunks\n58.82% (20/34)\n\n\nAll\nCS_B\nTop-3 1-paragraph chunks\n54.40% (105/193)\n\n\n\n\n\n\nCS_C: Top-5 1-Paragraph Chunks\nIn this approach, I’ll select the top-5 retrieved context (1-paragraph chunks) for each question and calculate the Answer Rate. As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\nresults = []\nfor chapter in ['1', '2', '4', '8', '9', '10', '13']:\n  _, idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_embs[chapter].unsqueeze(0), dim=2).sort(descending=True)\n  top_5_chunks = ['\\n\\n'.join([data[chapter][i] for i in row[0:5].tolist()]) for row in idxs]\n  results.extend(top_5_chunks)\n\n\nassert len(results) == 193\n\n\ncs_c = questions.copy()\ncs_c['cs_c_context'] = results\ncs_c.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      cs_c_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      ## Neural Networks: A Brief History\\n\\nAn MIT ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncs_c.to_csv('cs_c.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall). As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nCS_C\nTop-5 1-paragraph chunks\n63.33% (19/30)\n\n\n2\nCS_C\nTop-5 1-paragraph chunks\n69.23% (18/26)\n\n\n4\nCS_C\nTop-5 1-paragraph chunks\n64.52% (20/31)\n\n\n8\nCS_C\nTop-5 1-paragraph chunks\n47.83% (11/23)\n\n\n9\nCS_C\nTop-5 1-paragraph chunks\n53.57% (15/28)\n\n\n10\nCS_C\nTop-5 1-paragraph chunks\n47.62% (10/21)\n\n\n13\nCS_C\nTop-5 1-paragraph chunks\n61.76% (21/34)\n\n\nAll\nCS_C\nTop-5 1-paragraph chunks\n59.07% (114/193)\n\n\n\n\n\n\nCS_D: Top-1 3-Paragraph Chunks\nI now want to increase the chunk size (to 3 paragraphs per chunk). I do this by iterating over the 1-paragraph chunks in groups of three, removing the header from the 2nd and 3rd chunk in each triplet and then concatenating the three chunks into new 3-paragraph chunks.\n\ndef combine_chunks(chunks, num_p=3):\n    combined_chunks = []\n    current_header = None\n    current_group = []\n\n    for chunk in chunks:\n        # Extract header from chunk\n        header = chunk.split('\\n\\n')[0]\n\n        if header != current_header:\n            if len(current_group) > 1:  # Only add if group has content besides header\n                # Add current group to combined chunks if header changes\n                combined_chunks.append('\\n\\n'.join(current_group))\n            # Update current header\n            current_header = header\n            # Start new group with header and content of current chunk\n            current_group = [header, chunk.split('\\n\\n', 1)[1] if len(chunk.split('\\n\\n')) > 1 else '']\n        else:\n            if len(current_group) < num_p + 1:  # +1 to account for header\n                # Add chunk content (without header) to current group\n                current_group.append(chunk.split('\\n\\n', 1)[1] if len(chunk.split('\\n\\n')) > 1 else '')\n\n            if len(current_group) == num_p + 1:  # +1 to account for header\n                # Add full group to combined chunks\n                combined_chunks.append('\\n\\n'.join(current_group))\n                # Reset current group, keeping the header\n                current_group = [current_header]\n\n    if len(current_group) > 1:  # Only add if group has content besides header\n        # Add any remaining group to combined chunks\n        combined_chunks.append('\\n\\n'.join(current_group))\n\n    return combined_chunks\n\n\ndata_3p = {}\n\nfor chapter, chunks in data.items():\n  data_3p[chapter] = combine_chunks(chunks, num_p=3)\n\n\ntotal_chunks = 0\n\nfor chapter, chunks in data_3p.items():\n  print(chapter, len(chunks))\n  total_chunks += len(chunks)\n\nassert total_chunks == 713\n\n1 112\n2 84\n4 152\n8 58\n9 141\n10 70\n13 96\n\n\nSince I have new chunks of data (3-paragraphs each) I have to re-calculate the embeddings:\n\ndata_3p_embs = {}\n\nfor chapter, chunks in data_3p.items():\n  data_3p_embs[chapter] = emb_model.encode(chunks, convert_to_tensor=True)\n\n\nfor chapter, embs in data_3p_embs.items():\n  print(chapter, embs.shape)\n\n1 torch.Size([112, 384])\n2 torch.Size([84, 384])\n4 torch.Size([152, 384])\n8 torch.Size([58, 384])\n9 torch.Size([141, 384])\n10 torch.Size([70, 384])\n13 torch.Size([96, 384])\n\n\n\nresults = []\nfor chapter in ['1', '2', '4', '8', '9', '10', '13']:\n  _, idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_3p_embs[chapter].unsqueeze(0), dim=2).sort(descending=True)\n  top_1_idxs = idxs[:, 0]\n  top_1_chunks = [data_3p[chapter][idx] for idx in top_1_idxs]\n  results.extend(top_1_chunks)\n\n\nassert len(results) == 193\n\n\ncs_d = questions.copy()\ncs_d['cs_d_context'] = results\ncs_d.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      cs_d_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncs_d.to_csv('cs_d.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall). As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nCS_D\nTop-1 3-paragraph chunks\n46.67% (14/30)\n\n\n2\nCS_D\nTop-1 3-paragraph chunks\n53.85% (14/26)\n\n\n4\nCS_D\nTop-1 3-paragraph chunks\n25.81% (8/31)\n\n\n8\nCS_D\nTop-1 3-paragraph chunks\n43.48% (10/23)\n\n\n9\nCS_D\nTop-1 3-paragraph chunks\n42.86% (12/28)\n\n\n10\nCS_D\nTop-1 3-paragraph chunks\n47.62% (10/21)\n\n\n13\nCS_D\nTop-1 3-paragraph chunks\n47.06% (16/34)\n\n\nAll\nCS_D\nTop-1 3-paragraph chunks\n43.52% (84/193)\n\n\n\n\n\n\nCS_E: Top-3 3-Paragraph Chunks\n\nresults = []\nfor chapter in ['1', '2', '4', '8', '9', '10', '13']:\n  _, idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_3p_embs[chapter].unsqueeze(0), dim=2).sort(descending=True)\n  top_3_chunks = ['\\n\\n'.join([data_3p[chapter][i] for i in row[0:3].tolist()]) for row in idxs]\n  results.extend(top_3_chunks)\n\n\nassert len(results) == 193\n\n\ncs_e = questions.copy()\ncs_e['cs_e_context'] = results\ncs_e.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      cs_e_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncs_e.to_csv('cs_e.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall). As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nCS_E\nTop-3 3-paragraph chunks\n80% (24/30)\n\n\n2\nCS_E\nTop-3 3-paragraph chunks\n80.77% (21/26)\n\n\n4\nCS_E\nTop-3 3-paragraph chunks\n67.74% (21/31)\n\n\n8\nCS_E\nTop-3 3-paragraph chunks\n73.91% (17/23)\n\n\n9\nCS_E\nTop-3 3-paragraph chunks\n57.14% (16/28)\n\n\n10\nCS_E\nTop-3 3-paragraph chunks\n52.38% (11/21)\n\n\n13\nCS_E\nTop-3 3-paragraph chunks\n70.59% (24/34)\n\n\nAll\nCS_E\nTop-3 3-paragraph chunks\n69.43% (134/193)\n\n\n\n\n\n\nCS_F: Top-5 3-Paragraph Chunks\n\nresults = []\nfor chapter in ['1', '2', '4', '8', '9', '10', '13']:\n  _, idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_3p_embs[chapter].unsqueeze(0), dim=2).sort(descending=True)\n  top_5_chunks = ['\\n\\n'.join([data_3p[chapter][i] for i in row[0:5].tolist()]) for row in idxs]\n  results.extend(top_5_chunks)\n\n\nassert len(results) == 193\n\n\ncs_f = questions.copy()\ncs_f['cs_f_context'] = results\ncs_f.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      cs_f_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncs_f.to_csv('cs_f.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall). As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nCS_F\nTop-5 3-paragraph chunks\n90% (27/30)\n\n\n2\nCS_F\nTop-5 3-paragraph chunks\n84.62% (22/26)\n\n\n4\nCS_F\nTop-5 3-paragraph chunks\n80.65% (25/31)\n\n\n8\nCS_F\nTop-5 3-paragraph chunks\n91.30% (21/23)\n\n\n9\nCS_F\nTop-5 3-paragraph chunks\n75% (21/28)\n\n\n10\nCS_F\nTop-5 3-paragraph chunks\n57.14% (12/21)\n\n\n13\nCS_F\nTop-5 3-paragraph chunks\n79.41% (27/34)\n\n\nAll\nCS_F\nTop-5 3-paragraph chunks\n80.31% (155/193)"
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html",
    "href": "posts/2024-09-26-diffusion-gif/index.html",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "",
    "text": "Show pip installs and imports\n!pip install -q --upgrade transformers==4.25.1 diffusers ftfy accelerate\nfrom base64 import b64encode\n\nimport numpy\nimport torch\nfrom diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\nfrom huggingface_hub import notebook_login\n\n# For video display:\nfrom IPython.display import HTML\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom torch import autocast\nfrom torchvision import transforms as tfms\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer, logging\nimport os\n\ntorch.manual_seed(1)\n\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\n# Set device\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html#background",
    "href": "posts/2024-09-26-diffusion-gif/index.html#background",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "Background",
    "text": "Background\nIn this notebook, I rearrange the code provided in the fastai course Part 2 notebook Stable Diffusion Deep Dive to create a GIF transitioning from a picture of a skunk to a picture of a puppy in a relatively visually smooth and stable manner.\nI combine two concepts introduced in that notebook:\n\nThe diffusion loop starting from a noised version of input (aka image2image)\nReplacing/mixing token embeddings to alter the generated image (for example a mixed skunk/puppy embeddings/image)\n\nI’ll reuse the helper functions provided in that notebook:\n\nset_timesteps: uses the scheduler algorithm to generate the timesteps and noise for a given number of inference steps.\npil_to_latent: uses the VAE to encode a 512x512 pixel image into a 1x4x64x64 latent.\nlatents_to_pil: uses the VAE to decode a 1x4x64x64 latent into a 512x512 PIL Image.\nget_output_embeds: uses (most of the) code from the forward pass of the text_encoder.text_model to encode token + position embeddings.\n\n\n\nShow set_timesteps\n# Prep Scheduler\ndef set_timesteps(scheduler, num_inference_steps):\n    scheduler.set_timesteps(num_inference_steps)\n    scheduler.timesteps = scheduler.timesteps.to(torch.float32) # minor fix to ensure MPS compatibility, fixed in diffusers PR 3925\n\n\n\n\nShow pil_to_latent and latents_to_pil\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latents_to_pil(latents):\n    # bath of latents -> list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\n\n\nShow get_output_embeds\ndef get_output_embeds(input_embeddings):\n    # CLIP's text model uses causal mask, so we prepare it here:\n    bsz, seq_len = input_embeddings.shape[:2]\n    causal_attention_mask = text_encoder.text_model._build_causal_attention_mask(bsz, seq_len, dtype=input_embeddings.dtype)\n\n    # Getting the output embeddings involves calling the model with passing output_hidden_states=True\n    # so that it doesn't just return the pooled final predictions:\n    encoder_outputs = text_encoder.text_model.encoder(\n        inputs_embeds=input_embeddings,\n        attention_mask=None, # We aren't using an attention mask so that can be None\n        causal_attention_mask=causal_attention_mask.to(torch_device),\n        output_attentions=None,\n        output_hidden_states=True, # We want the output embs not the final output\n        return_dict=None,\n    )\n\n    # We're interested in the output hidden state only\n    output = encoder_outputs[0]\n\n    # There is a final layer norm we need to pass these through\n    output = text_encoder.text_model.final_layer_norm(output)\n\n    # And now they're ready!\n    return output\n\n\nI also prepare the token_embedding layer and position_embedding layer as done in the notebook:\n\n# Access the embedding layer\ntoken_emb_layer = text_encoder.text_model.embeddings.token_embedding\npos_emb_layer = text_encoder.text_model.embeddings.position_embedding\n\n\nposition_ids = text_encoder.text_model.embeddings.position_ids[:, :77]\nposition_embeddings = pos_emb_layer(position_ids)\nprint(position_embeddings.shape)\n\ntorch.Size([1, 77, 768])"
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html#mixing-and-replacing-text-embeddings",
    "href": "posts/2024-09-26-diffusion-gif/index.html#mixing-and-replacing-text-embeddings",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "Mixing and Replacing Text Embeddings",
    "text": "Mixing and Replacing Text Embeddings\nI start by taking a prompt, A picture of a puppy, tokenizing it, and creating its embeddings:\n\nprompt = 'A picture of a puppy'\n\n# Tokenize\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n\n# Get token embeddings\ntoken_embeddings = token_emb_layer(input_ids)\n\nNext, I want to replace the token embeddings for puppy with an averaged embedding of skunk and puppy. I start (as the course notebook does) by first generating embeddings for the puppy (6829) and skunk (42194) tokens:\n\n# The new embedding. Which is now a mixture of the token embeddings for 'puppy' and 'skunk'\npuppy_token_embedding = token_emb_layer(torch.tensor(6829, device=torch_device))\nskunk_token_embedding = token_emb_layer(torch.tensor(42194, device=torch_device))\n\nI then choose a weighting factor (I’ll start with 50% puppy and 50% skunk) to mix the two token embeddings:\n\npuppy_factor = 0.5\nreplacement_token_embedding = puppy_factor*puppy_token_embedding + (1-puppy_factor)*skunk_token_embedding\n\nNow for the replacement operation: using where, we swap out the 100% puppy token embeddings with the mixed puppy/skunk embeddings:\n\ntoken_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n\nWe then combine the full set of token embeddings with position embeddings and pass it through the forward pass of the text encoder:\n\n# Combine with pos embs\ninput_embeddings = token_embeddings + position_embeddings\n\n#  Feed through to get final output embs\nmodified_output_embeddings = get_output_embeds(input_embeddings)"
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html#generating-a-mixed-puppyskunk-image",
    "href": "posts/2024-09-26-diffusion-gif/index.html#generating-a-mixed-puppyskunk-image",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "Generating a Mixed Puppy/Skunk Image",
    "text": "Generating a Mixed Puppy/Skunk Image\nWith the text embeddings in hand, we can now generate a puppy/skunk hybrid image using the U-Net: first we set some constants (I’ll use the same ones as the course notebook):\n\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 30            # Number of denoising steps\nguidance_scale = 7.5                # Scale for classifier-free guidance\ngenerator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\nbatch_size = 1\n\nNext we create embeddings for an empty prompt (aka the unconditioned input) and concatenate it to the puppy/skunk hybrid token embeddings:\n\nmax_length = modified_output_embeddings.shape[-2]\n\nuncond_input = tokenizer(\n  [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n    \ntext_embeddings = torch.cat([uncond_embeddings, modified_output_embeddings])\n\nWe initialize the scheduler with the given number of inference steps:\n\n# Prep Scheduler\nset_timesteps(scheduler, num_inference_steps)\n\nAnd initialize a random set of latents as the starting point for the image generation. Note that this one of the main things I’ll change later on when I want to create a smooth GIF transition from 100% skunk to 100% puppy (where I’ll start with a noisy image’s latents instead of fully random noisy latents):\n\n# Prep latents\nlatents = torch.randn(\n(batch_size, unet.in_channels, height // 8, width // 8),\ngenerator=generator,\n)\n\nlatents = latents.to(torch_device)\nlatents = latents * scheduler.init_noise_sigma\n\n/tmp/ipykernel_2631/4273587132.py:3: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  (batch_size, unet.in_channels, height // 8, width // 8),\n\n\nFinally, we can run the diffusion loop, where we generate U-Net predictions for both conditioned and unconditioned (empty) text embeddings, using the guidance_scale to guide the diffusion process towards the conditioned text embeddings.\nThe result is a creature that looks like both a skunk and a puppy—a skunk/puppy hybrid!\n\n\nShow diffusion loop\nfor i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n    sigma = scheduler.sigmas[i]\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n    # compute the previous noisy sample x_t -> x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\nlatents_to_pil(latents)[0]"
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html#transitioning-from-100-skunk-to-100-puppy",
    "href": "posts/2024-09-26-diffusion-gif/index.html#transitioning-from-100-skunk-to-100-puppy",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "Transitioning from 100% Skunk to 100% Puppy",
    "text": "Transitioning from 100% Skunk to 100% Puppy\nI’ll now modify the above process so that I can use the previous image as a starting point for the next image’s generation. Why would I want to do that? Well, to illustrate why, I’ll generate 11 images that each start out with random noisy latents. The first image will be generated with 100% skunk and 0% puppy token embeddings, incrementing by 10% until the last image is 0% skunk and 100% puppy token embeddings. I’ll then combine the images into a GIF and show how it has a choppy transition between frame since each image starts at a random spot in the latent space.\nI’ll use the generate_with_embs function from the course notebook which takes the code above and wraps it into a function:\n\n\nShow generate_with_embs\ndef generate_with_embs(text_embeddings, seed):\n    height = 512                        # default height of Stable Diffusion\n    width = 512                         # default width of Stable Diffusion\n    num_inference_steps = 30            # Number of denoising steps\n    guidance_scale = 7.5                # Scale for classifier-free guidance\n    generator = torch.manual_seed(seed)   # Seed generator to create the inital latent noise\n    batch_size = 1\n\n    max_length = text_input.input_ids.shape[-1]\n    uncond_input = tokenizer(\n      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n    )\n    with torch.no_grad():\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n    # Prep Scheduler\n    set_timesteps(scheduler, num_inference_steps)\n\n    # Prep latents\n    latents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n    )\n    latents = latents.to(torch_device)\n    latents = latents * scheduler.init_noise_sigma\n\n    # Loop\n    for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -> x_t-1\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n    return latents_to_pil(latents)[0]\n\n\nI’ll iterate between a puppy_factor of 0 to 1.0, generating new mixed token embeddings at each step and corresponding image.\n\n\nShow puppy_factor loop\nimgs = []\nseed = 4\nn = 10\nfor i in range(n + 1):\n  puppy_factor = i * 1. / n \n\n  # replace puppy embeddings with weighted average of puppy and skunk embeddings\n  replacement_token_embedding = puppy_factor*puppy_token_embedding + (1-puppy_factor)*skunk_token_embedding\n\n  # Insert this into the token embeddings (\n  token_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n\n  # Combine with pos embs\n  input_embeddings = token_embeddings + position_embeddings\n\n  #  Feed through to get final output embs\n  modified_output_embeddings = get_output_embeds(input_embeddings)\n\n  # Generate an image with these\n  img = generate_with_embs(modified_output_embeddings, seed=seed)\n\n  imgs.append(img)\n\n\nI’ll use this nifty one-liner provided by Claude to generate a GIF:\n\nimgs[0].save(f'/notebooks/skunk-to-puppy_random_{seed}.gif', save_all=True, append_images=imgs[1:], duration=200, loop=0)\n\nAs you can see in the GIF below, the transition from image to the next, especially once it goes from mostly-skunk to mostly-puppy, is choppy. Notice especially how the pose of the creature, as well as the background, significantly change from the start to finish of the animation.\n\n\n\nSkunk to Puppy animation where each subsequent image starts from a random noisy latent"
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html#making-the-skunk-to-puppy-transition-smoother",
    "href": "posts/2024-09-26-diffusion-gif/index.html#making-the-skunk-to-puppy-transition-smoother",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "Making the Skunk-to-Puppy Transition Smoother",
    "text": "Making the Skunk-to-Puppy Transition Smoother\nI’ll now modify the generation loop by doing the following:\n\nOn the first step of the puppy_factor loop, start with a random noisy latent.\nOn all subsequent steps, start with a latent of the previous step’s image with some “delayed” timestep noise added to it.\n\nTo accomplish this (and reuse as much course notebook code as possible), I’ll modify generate_with_embs. First off, I’ll walk through the updated parameters to this function:\n\ntext_embeddings: same as before, these are the mixed text embeddings (with some ratio of skunk-to-puppy)\nencoded: this is the latent of the previous step’s image, created using pil_to_latent as we’ll see later on.\nstart_step=10: this is the “delayed” timestep that we want to start with. The default is 10, which means that it will add noise using the scheduler starting at step 10.\nnum_inference_steps=30: the total number of inference steps. I found that 30 works pretty well (as is used in the course notebook).\nheight=512: the height of the image in pixels.\nwidth=512: the width of the image in pixels.\nbatch_size=1: the batch size.\nguidance_scale: the guidance scale to use when guiding the image away from unconditioned to conditioned generation.\ngenerator: maintaining the random state between images.\n\nIn addition to the function signature, the following two sections of generate_with_embs deviate from the course notebook to achieve a “smoother” transition between skunk and puppy.\nIn the code below, if the start_step is greater than 0 (meaning anything other than the very first image), the latents are generated by adding noise to the encoded image from the previous step using scheduler.noise and the given start_step.\nIf start_step==0 then we’re at the very first image and we just use the random noise latents (passed through as the encoded parameter).\n# Prep latents\n    if start_step > 0 :\n      noise = torch.randn_like(encoded)\n      latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n      latents = latents.to(torch_device).float()\n    else:\n      latents = encoded\nThe following condition is added to the diffusion loop, so that the U-Net inference starts at the given start_step:\nif i >= start_step: \n\n\nShow modified generate_with_embs\ndef generate_with_embs(\n    text_embeddings, \n    encoded, \n    start_step=10, \n    num_inference_steps=30, \n    height=512, \n    width=512, \n    batch_size=1,\n    guidance_scale=5, \n    generator=torch.manual_seed(32)):\n\n    max_length = text_embeddings.shape[-2]\n    assert max_length == 77\n\n    uncond_input = tokenizer(\n      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n    )\n    with torch.no_grad():\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n\n    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n    # Prep Scheduler\n    set_timesteps(scheduler, num_inference_steps)\n\n    # Prep latents\n    if start_step > 0 :\n      noise = torch.randn_like(encoded)\n      latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n      latents = latents.to(torch_device).float()\n    else:\n      latents = encoded\n\n    # Loop\n    for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n      if i >= start_step: \n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -> x_t-1\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n    return latents_to_pil(latents)[0]\n\n\nThe next change necessary to adapt to this new generate_with_embs function is in the puppy_factor-incrementing for-loop.\nFirst, let’s get the unchanged constants out of the way:\n\nheight = 512\nwidth = 512\nnum_inference_steps = 30          \nguidance_scale = 7.5                  \nbatch_size = 1\n\nNow, in the for-loop below, when i==0 (i.e we’re generating the first image), encoded is just random noise and start_step is 0.\nif i == 0: \n    encoded = torch.randn(\n        (batch_size, unet.in_channels, height // 8, width // 8),\n        generator=generator,\n        )\n    encoded = encoded.to(torch_device)\n    encoded = encoded * scheduler.init_noise_sigma\n    start_step=0\nFor all other i’s (i.e. subsequent images after the first one), encoded is created by converting the previous step’s img to latents using pil_to_latents, and start_step is set to 10:\nencoded = pil_to_latent(img)\nstart_step=10\nAfter that, we pass the necessary arguments to generate_with_embs and it generates the 512x512 image that we tack onto the imgs list:\nimg = generate_with_embs(\n      text_embeddings=modified_output_embeddings, \n      encoded=encoded, \n      start_step=start_step, \n      num_inference_steps=num_inference_steps, \n      height=height, \n      width=width, \n      batch_size=batch_size,\n      guidance_scale=guidance_scale, \n      generator=generator)\n\n  imgs.append(img)\nI’ll now generate 20 images using this new approach, going from 100% skunk to 100% puppy.\n\n\nShow mk_imgs\ndef mk_imgs(seed, n):\n    generator = torch.manual_seed(seed)\n    imgs = []\n    for i in range(n + 1):\n        puppy_factor = i * 1. / n\n        \n        # replace puppy embeddings with weighted average of puppy and skunk embeddings\n        replacement_token_embedding = puppy_factor*puppy_token_embedding + (1-puppy_factor)*skunk_token_embedding\n        \n        # Insert this into the token embeddings\n        token_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n        \n        # Combine with pos embs\n        input_embeddings = token_embeddings + position_embeddings\n        \n        #  Feed through to get final output embs\n        modified_output_embeddings = get_output_embeds(input_embeddings)\n        \n        # Generate an image with these\n        if i == 0: \n            encoded = torch.randn(\n                (batch_size, unet.in_channels, height // 8, width // 8),\n                generator=generator,\n            )\n            encoded = encoded.to(torch_device)\n            encoded = encoded * scheduler.init_noise_sigma\n            start_step=0\n        else: \n            encoded = pil_to_latent(img)\n            start_step=10\n            \n        img = generate_with_embs(\n                text_embeddings=modified_output_embeddings, \n                encoded=encoded, \n                start_step=start_step, \n                num_inference_steps=num_inference_steps, \n                height=height, \n                width=width, \n                batch_size=batch_size,\n                guidance_scale=guidance_scale, \n                generator=generator)\n        \n        imgs.append(img)\n        \n    imgs[0].save(f'/notebooks/skunk-to-puppy_smoother_{seed}.gif', save_all=True, append_images=imgs[1:], duration=200, loop=0)\n\n\n\nmk_imgs(seed=4, n=20)\n\nI find the resulting GIF to be smoother: notice how the skunk and the puppy have the same pose/perspective and the background stays more constant throughout the animation.\n\n\n\nSkunk to Puppy animation where each subsequent image starts from a noisy latent of the previous step"
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html#final-thoughts",
    "href": "posts/2024-09-26-diffusion-gif/index.html#final-thoughts",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nRepurposing existing code is a great way to learn! I learned more about each line of code from the original course notebook because I was trying to do something different with it. This process really solidified my understanding of the course notebook.\nGenerating images is finnicky. It took me 100 different seeds to find a handful of animations that I was satisfied with. Some of this variability might be because my prompt is simple and I didn’t iterate on it. For example, I could have added language about the pose and background of the skunk/puppy to yield more stable results. I also experimented a bit with the number of total inference steps and the starting step for each image and found that 30 and 10 (respectively) yielded decent results.\nI feel like I have only scratched the surface. There’s of course much more experimentation, including with different models, that I didn’t do. I’m still building my intuition around image generation, so I’m trying to take my experience with this small project with a large grain of salt.\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-09-26-diffusion-gif/index.html#bonus-additional-gifs",
    "href": "posts/2024-09-26-diffusion-gif/index.html#bonus-additional-gifs",
    "title": "Generating a GIF Animation Using Stable Diffusion",
    "section": "Bonus: Additional GIFs",
    "text": "Bonus: Additional GIFs\nI tried 101 different random seeds to capture the best possible examples to use in this notebook. I’ll drop some of the GIFs I created here (and their random seeds) since they look cute/pretty, and in case you want to try to recreate them.\nseed=10\n\n\n\n\n\n\nseed=19\n\n\n\n\n\n\nseed=42\n\n\n\n\n\n\nseed=50\n\n\n\n\n\n\nseed=59\n\n\n\n\n\n\nseed=65\n\n\n\n\n\n\nseed=73\n\n\n\n\n\n\nseed=77\n\n\n\n\n\n\nseed=79\n\n\n\n\n\n\nseed=88\n\n\n\n\n\n\nseed=98\n\n\n\n\n\n\nseed=99"
  },
  {
    "objectID": "posts/2024-04-16-hms-hbac-recap/index.html",
    "href": "posts/2024-04-16-hms-hbac-recap/index.html",
    "title": "Recap: HMS HBAC Kaggle Competition",
    "section": "",
    "text": "In this notebook, I’ll recap my experience participating in the Harvard Medical School Harmful Brain Activity Classification Kaggle Research Competition. I finished in 2666th place (out of 2767 teams). I fell short of my 2024 goal to place in the top 50%, but I’m happy that I rose 11 spots in the final ranking (compared to the public score rankings) to fall just outside the bottom 100.\nHere’s how I’ll approach my recap:\n\nI’ll summarize overall process, linking to some example public notebooks I published.\nI’ll then analyze my submission results, commenting on any patterns that I see.\nI’ll re-envision my process: what would I do differently if I did this competition again?"
  },
  {
    "objectID": "posts/2024-04-16-hms-hbac-recap/index.html#overall-process",
    "href": "posts/2024-04-16-hms-hbac-recap/index.html#overall-process",
    "title": "Recap: HMS HBAC Kaggle Competition",
    "section": "Overall Process",
    "text": "Overall Process\n\nNotebooks\nFor the initial small model experiments, I created the following sequence of notebooks for each family (convnext, swinv2, vit):\n\nPart 1 [Train] notebook where I train 50+ models, document their hyperparameters and final error rates in this gist.\nPart 1 [Analysis] notebook where I analyze the results of the previous and pick the top models for submission.\nPart 2 [Train] notebook where I re-train those top models and export them for submission.\nPart 2 [Submit] notebook with internet access disabled where I load my models, calculate predictions and export them to submission.csv.\n\n\n\nModels\nI generally followed this approach for each family of models (convnext, swinv2 and vit):\n\nTrain many (50+) small models.\nPick the top 5 best-performing models (with the lowest TTA Validation Error Rate)\nSubmit those 5 models individually, and as 3- or 5-model ensembles where each model is weighted twice.\nPick the top 3 performing models (with the lowest Kaggle Public Score).\n\nAfter I had the top 3 models for each family:\n\nSubmit 9-model ensembles where each model is weighted twice.\nPick the top-5 best performing models.\nSubmit those 5 models as ensembles with each model weighted twice.\nPick the top-3 best performing models.\nSubmit those 3 models as ensembles with each model weighted twice.\n\nIn some cases I submitted top-2 model ensembles. In fact, my best performing submission was such a submission.\n\n\nData\nEach group of models were trained on one of the following datasets that I created, heavily referencing others’ public competition notebooks:\n\nTrain spectrograms (where the 4 brain regions’ spectrograms were vertically stacked) of varying sizes\nTrain spectrograms (vertically stacked, clipped, log-norm values) of varying sizes\nTrain spectrograms (vertically stacked, clipped, log-norm values) with fixed size of 300 px wide x 400 px tall\nTrain spectrograms (vertically stacked) with fixed size of 300 px wide x 400 px tall\nTrain EEG spectrograms (vertically stacked) of fixed size 128 px wide x 512 px tall"
  },
  {
    "objectID": "posts/2024-04-16-hms-hbac-recap/index.html#submission-results",
    "href": "posts/2024-04-16-hms-hbac-recap/index.html#submission-results",
    "title": "Recap: HMS HBAC Kaggle Competition",
    "section": "Submission Results",
    "text": "Submission Results\nI submitted 107 times (more than twice the number of submissions of anyone I got a better score than). My first and worst submission (quick-and-dirty ResNet18) resulted in a Public/Private score of 26.6/26.2. My penultimate and best submission (two swinv2 models trained on different datasets) got a Public/Private score of 1.15/1.17. Data on my full submission results are in this gist. Some key takeaways:\n\nFor 99 out of 107 submissions (including my best submission), the Private score was worse than the Public score, meaning overall, my approach didn’t generalize well to the full test dataset.\nThe best performing ensembles (outside of my best 2-model submission) were those with 4 models, yielding a median 1.39 Private score across 10 submissions.\nThe 400x300 log-norm stacked train spectrograms performed the best, with a median Private score of 1.2 across 10 submissions.\nUsing MixUp improved the median Private score by about 20% points.\nThe swinv2 models, even though they didn’t always have the best final validation error rate during training, ultimately performed the best.\nIn some cases, multiple models from the same family performed better in ensembles with other families.\n\nI’m hesitant to take these patterns too seriously, as my overall Private scores were pretty terrible."
  },
  {
    "objectID": "posts/2024-04-16-hms-hbac-recap/index.html#what-would-i-do-differently",
    "href": "posts/2024-04-16-hms-hbac-recap/index.html#what-would-i-do-differently",
    "title": "Recap: HMS HBAC Kaggle Competition",
    "section": "What Would I Do Differently?",
    "text": "What Would I Do Differently?\nThere are quite a few things I would do differently:\n\nIterate faster. I tried my best to emulate Jeremy Howard’s approach in his Road to the Top series, but still spent a lot of time training early on since I was using the full dataset. For example, instead of taking 30-40 minutes to train each of the 50+ convnext_small_in22k models, if I used a subset of the data I could have trimmed that time down considerably.\nStart earlier. I joined the competition 2 months late, with 1 month to go. I would have spent one of those months trying out more variations (and subsets) of the data and another month experimenting with different architectures and augmentations.\nDon’t waste submissions. There are of course physical constraints (time, energy, resources) but I could have probably squeezed in 20 more submissions if I would have managed my time more efficiently. I lost a few days here and there waiting to see how my models performed in submissions instead of starting to train the next batch of experiments immediately while submitting the last batch. In some cases, I had to wait until all 10+ submissions each round completed to pick the top models to train next. On the other hand, I could have avoided wasting 3-4 days pursuing the wrong approach if I stopped for a day to collect my thoughts and reflect on my strategy more often.\nSubmit more models. My best-performing models didn’t always have best error rate during training, so there’s a chance that models I didn’t submit because they were the 6th or 7th or 12th-best based on training results might have performed well in submissions.\nSet aside a test set. Most of my models performed worse on the Private test set. I also couldn’t submit all the models I wanted because I was running out of days and submissions. If I had set aside a test set, I could have seen how more models performed on data not used during training or validation."
  },
  {
    "objectID": "posts/2024-04-16-hms-hbac-recap/index.html#final-thoughts",
    "href": "posts/2024-04-16-hms-hbac-recap/index.html#final-thoughts",
    "title": "Recap: HMS HBAC Kaggle Competition",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI absolutely enjoyed the thrill of competing in such a challenging Kaggle competition and am absolutely unsatisfied with my final ranking.\nI gained a lot of experience overcoming different first-time experiences (how to upload a model/dataset to Kaggle, how to run inference on a model with internet access disabled, how to manually document hyperparameters and results for 173 training runs, how to train in 6-hour increments without losing progress (Paperspace Free Tier time limit) how to best keep track of notebooks—so many notebooks!). After resting for a few days to collect my thoughts (and energy) I have begun participating in the BirdCLEF 2024 and Automated Essay Scoring 2.0 competitions (both started two weeks ago and have multiple months remaining). I am already improving my approach because of my experience in this competition.\nAs always, I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html",
    "href": "posts/2024-04-26-rewardbench/index.html",
    "title": "Paper Summary: RewardBench",
    "section": "",
    "text": "In this blog post I’ll summarize the research paper RewardBench: Evaluating Reward Models for Language Modeling. Here’s the abstract:\n\nReward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO), and on a spectrum of datasets. We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process."
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html#main-takeaways",
    "href": "posts/2024-04-26-rewardbench/index.html#main-takeaways",
    "title": "Paper Summary: RewardBench",
    "section": "Main Takeaways",
    "text": "Main Takeaways\n\nA good reward function, and therefore a good reward model (RM) stably assigns credit to the classes of good or bad content.\nReward models potentially provide a glimpse into how human values map onto language models.\nReward model output distributions vary across models.\nDPO policies (compared to classifier RMs) fail to generalize to popular preference data test sets and present a higher variance in performance.\nData subsets with low ceilings indicate opportunities to improve preference datasets and modeling methods while subsets with high variability indicate opportunities for improving best practices.\nRewardBench if a framework to which we can add new models."
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html#scoring-method",
    "href": "posts/2024-04-26-rewardbench/index.html#scoring-method",
    "title": "Paper Summary: RewardBench",
    "section": "Scoring Method",
    "text": "Scoring Method\n\n\n\n\n\n\nEach data point consists of a prompt and two completions (chosen and rejected). For each prompt, the score of the reward model is computed. The prompt is then categorized as a win if the score of the prompt with the verified chosen completion is higher than that of the verified rejected completion."
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html#datasets",
    "href": "posts/2024-04-26-rewardbench/index.html#datasets",
    "title": "Paper Summary: RewardBench",
    "section": "Datasets",
    "text": "Datasets\nThe authors define the following subsets on which they evaluate reward model performance:\n\nChat\nChat Hard\nSafety\nReasoning\nPrior Sets\n\nThe first four subsets are curated (some are modified) from existing benchmark evaluation sets. The last subset (Prior Sets) consists of unmodified existing evaluation sets.\n\nChat\nThe Chat subset includes prompts curated from two benchmarks:\n\nAlpacaEval\n\nEasy: 100 prompt-chosen-rejected trios.\nLength: 95 prompt-chosen-rejected trios.\nHard: 95 manually verified prompt-chosen-rejected trios.\n\nMT Bench\n\nEasy: 28 manually verified prompt-chosen-rejected trios.\nMedium: 40 manually verified prompt-chosen-rejected trios.\n\n\nThe AlpacaEval Length subset is designed to differentiate between other Chat subsets by having notably different model capabilities with the same average length.\n\nAlpacaEval\n\nAlpacaEval is an automated tool for evaluation instruction-folliwng language models against the AlpacaFarm dataset.\nAlpacaEval 2.0 with length-controlled win-rates has a spearman correlation of 0.98 with ChatBot Arena.\nChatBot Arena is a crowdsourced open platform for LLM evals with 700,000+ human pairwise comparisons to rank LLMs.\n\n“length-controlled” is a way to account for the bias that LLMs have towards longer responses (i.e. they prefer responses that are longer).\nHere is an example prompt-chosen-rejected tri from the alpacaeval-easy dataset. The chosen-model is the model that generated the chosen response and rejected-model is the model that generated the rejected response.\n\n\n\n\n\nHere a trio from the alpacaeval-length dataset, where the prompt lengths for chosen and rejected responses are similar.\n\n\n\n\n\nHere’s a trio from alpacaeval-length with different prompt lengths for the chosen and rejected response showing that there is variation in prompt lengths even though the average length across the full dataset between chosen and rejected responses is similar.\n\n\n\n\n\n\n\nMT Bench\n\nMT Bench is designed to test multi-turn conversation (AlpacaEval was single-turn only) and instruction-following ability across 8 categories of user prompts: writing, roleplay, extraction, reasining, math, coding, knowledge I (STEM), and knowledge II (humanities/social science).\n\nHere’s a trio from the mt-bench-easy dataset where claude-v1 correctly counts the specific words while raven-14b deviates from the instruction.\n\n\n\n\n\n\n\n\nChat Hard\n\nMT Bench\n\nHard: 37 manually verified prompt-chosen-rejected trios.\n\nLLMBar\n\nNatural: 100 manually verified prompt-chosen-rejected trios.\nAdversarial\n\nNeighbor: 134 trios.\nGPT4Inst: 92 trios.\nGPTOut: 47 trios.\nManual: 46 trios.\n\n\n\n\nLLMBar\nThe LLMBar dataset contains responses to prompts that are preferred and dispreferred.\n\nNatural\n\nInstances from existing subject human-preference datasets, filtered/modified for objective preference.\n\nAdversarial\n\nDispreffered output deviates from the instruction but often has good superficial qualities and may thus distract the evaluator.\n\n\nHere’s a trio where the rejected response deviates from the instruction but is coherent, uses good grammar, and has a conclusive response. When the model is listed as “unknown” it means that both a human and an AI model were involved in the response generation.\n\n\n\n\n\n\n\n\nSafety\n\nRefusals\n\nDangerous: 100 trios.\nOffensive: 100 trios.\n\nDo Not Answer\n\n136 trios.\n\nXSTest\n\nShould Respond\n\n250 manually selected trios.\n\nShould Refuse\n\n154 manually selected trios.\n\n\n\n\nRefusals\nAn in-development refusals dataset at AI2, where the chosen response is a refusal and the rejected is harmful text is of either dangerous or offensive nature.\nThe following refusals-dangerous trio involves a medical scenario where the model should refuse to respond.\n\n\n\n\n\nIn the following trio, the rejected model is tricked by the user prompt that the heist is for a novel and it provides a helpful but dangerous response.\n\n\n\n\n\n\n\nDo Not Answer\n\nA dataset curated and filtered to consist of only instructions responsible language models should now follow.\n\n\n\n\n\n\nThe following trio from the donotanswer dataset shows how the model should refuse to respond to the prompt.\n\n\n\n\n\n\n\nXSTest\nThe prompts from this XSTest subset were designed to test for exaggerated safety behavior by containing certain, seemingly unsafe, trigger words while actually being safe to comply to.\nIn the following trio from the xstest-should-respond dataset, the model tricks the rejected model into thinking the prompt is offensive by using the phrase “not be allowed to lead public companies”.\n\n\n\n\n\n\n\n\nReasoning\n\nHumanEvalPack (164 prompts each)\n\nCPP\nGo\nJavaScript\nJava\nRust\nPython\n\nPRM (Process Reward Model) Math\n\nFiltered/select answers from the PRM800k dataset.\n\n\n\nHumanEvalPack\n\nHumanEvalPack expands the HumanEval benchmark to 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust)\nHandwritten HumanEval measures functional correctness for synthesizing python programs from docstrings.\n\nHere’s a trio from the hep-python dataset:\n\n\n\n\n\n\n\nPRM Math\n\nPRM800k is an 800k step-level labels over 75000 solutions.\n\n\n\n\n\n\nA trio from the math-prm dataset where the chosen response is human-generated.\n\n\n\n\n\n\n\n\nPrior Sets\n\nAnthropic\n\nHelpful\nHarmless*\nHHH\n\nMT Bench\n\nGPT-4*\nHuman*\n\nStanford Human Preferences (SHP)\nOpenAI’s Learning to Summarize\n\n* Not used in the RewardBench leaderboard.\nHere’s a trio from the Anthropic Helpful dataset where the chosen model provides a helpful response to the prompt and rejecte response is not helpful.\n\n\n\n\n\nA trio from Anthropic HHH where the chosen model provides an honest response “I’m not sure…if I had to guess…” while the rejected model provides a confident incorrect response.\n\n\n\n\n\nA trio from the Helpful subset of Anthropic HHH where the chosen model gives pros for each vacation destination whereas the rejected model gives a non-committal response.\n\n\n\n\n\nA trio from SHP where the chosen response is more detailed and thorough than the reject response (in my opinion). My personal opinion is that neither explain the concept at a five year old level.\n\n\n\n\n\nA trio from Learning to Summarize. In my opinion, both responses are similar.\n\n\n\n\n\n\nHere is a summary of the datasets used for the RewardBench leaderboard, including a description of how chosen-rejected responses are generated.\n\n\n\n\n\n\n\nDistribution of Source Completions\nThe chosen and rejected responses are generated by a variety of models (including humans and both humans and models—“unknown”). Humans are the most common source of the response, with “unknown”, GPT-3.5-turbo, GPT-4 and Llama-2-70b-chat rounding out the top five overall and for chosen responses. Two Mistral-7B variants are 4th and 5th most used for rejected responses."
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html#results",
    "href": "posts/2024-04-26-rewardbench/index.html#results",
    "title": "Paper Summary: RewardBench",
    "section": "Results",
    "text": "Results\n\nLeaderboard (from the paper)\nThe following table shows the top-20 models in terms of average performance (accuracy) on the five subsets of Chat, Chat Hard, Safety, Reasoning and Prior Sets. Note that 15 of the top 20 are DPO models, with 5 Sequence Classifiers. A random model would have an accuracty of 50% when chosing the preferred response. The highlighted accuracies are the highest in each column.\n\n\n\n\n\n\n\nLeaderboard as of 4/26/2024\nThe current leaderboard (at the time I made this presentation) had many new models involved. A Starling variant was still in the top 5, while Allen AI’s tulu-2-dpo-70B had dropped to 11th place.\n\n\n\n\n\nWhen sorting by Prior Sets (descending) you can see the paper’s findings in action—none of the top 20 models were trained by DPO.\n\n\n\n\n\n\n\nScaling Trends\nThe authors found that DPO trained models followed scaling laws (accuracy generally increased with model size). Whereas in the Qwen1.5 family (not DPO trained) for example, the accuracy actually regresses on Prior Sets as model size increases.\n\n\n\n\n\n\n\n7B Models\nThe Zephyr-7B variants performed well in Chat Hard and Reasoning.\n\n\n\n\n\n\n\nChat Hard\nOn Chat Hard some models performed worse than random.\n\n\n\n\n\n\n\nSafety\nThe authors highlighted three model behaviors for safety:\n\nTop section: models refuse or respond when they should.\nMiddle: Models always refuse (low accuracy on Should Respond).\nBottom: Models always respond (low accuracy on Should Refuse).\n\n\n\n\n\n\n\n\nDistribution of Model Accuracy by Dataset\nThe highlighted distributions show how for some datasets, none of the models had an accuracy anywhere close to 100%, showing an opportunity to improve these datasets and modeling methods.\n\n\n\n\n\n\n\nPrompt Length Distribution by Dataset\nThe authors showed the variation in (and average of) prompt lengths across the various datasets. Note that the AlpacaEval Length prompt lengths vary a lot although the average prompt length of chosen and rejected responses are close. For most of the other datasets the averages are either close (HumanEvalPack) or the chosen responses had a shorter prompt (LLMBar).\n\n\n\n\n\n\n\nDPO vs Classifier RMs\n\nThis is an understudied field.\nDPO model availability due to low compute requirements.\nDPOs perform well on all subsets except Prior Sets.\nLack of documentation on reference models restricts DPO evaluation because using the “wrong” reference model leads to lower DPO performance.\nDPOs regularize with KL.\nDPOs are trained for multiple epochs (Sequence Classifiers usually trained for 1 epoch)."
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html#future-work",
    "href": "posts/2024-04-26-rewardbench/index.html#future-work",
    "title": "Paper Summary: RewardBench",
    "section": "Future Work",
    "text": "Future Work\n\nExplore reference free DPO model impacts on inference.\n\nThe image below shows going from log probability ratio to probability ratio when the reference model is removed.\n\n\n\n\n\n\nAnalyze hyperparamters’ role in DPO and RM classifier performance.\nIncorporate generative reward modeling scores into leaderboard (already done in the current leaderboard).\nImprove datasets with model accuracy ceilings under 100%.\nImprove RMs to reduce variance (especially for challenging tasks).\nAblate base models and fine-tuning recipes to find the best RMs.\nIdentify a practical RM output distribution for downstream RL training."
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html#rewardbench-result-analysis",
    "href": "posts/2024-04-26-rewardbench/index.html#rewardbench-result-analysis",
    "title": "Paper Summary: RewardBench",
    "section": "RewardBench Result Analysis",
    "text": "RewardBench Result Analysis\nI couldn’t get the rewardbench repo to install locally in time for the presentation so I’ve copy/pasted the critical functions to get the datasets in this notebook in order to create custom visualizations of the model results."
  },
  {
    "objectID": "posts/2024-04-26-rewardbench/index.html#final-thoughts",
    "href": "posts/2024-04-26-rewardbench/index.html#final-thoughts",
    "title": "Paper Summary: RewardBench",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI found this to be another inspiring paper, especially with the rich content in the Future Work section on how much this field needs to be studied. Getting a glimpse into how human values map onto language models is a fascinating frontier to explore.\nI hope you enjoyed this paper summary!"
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "",
    "text": "In this blog post I’ll modify the neural net training loop example in Jeremy Howard’s Lesson 5 notebook Linear model and neural net from scratch to plot training loss, validation loss, and accuracy across a number of training runs. I’ll run 100 trainings for the neural net, record the losses and accuracy, and then plot them to see how they vary by epoch and by training loop.\nI am also inspired by (and learned from) this forum post by a fastai community member (sign-in required) where they plotted losses, gradients, parameters and accuracy for various training runs that included or excluded params.grad_zero() and L2 regularization. They found that for a simple linear model, zeroing the gradients leads to more stable training, smaller coefficients and higher accuracy than letting gradients accumulate each epoch."
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plan-of-attack",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plan-of-attack",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nI want to record values at the end of each epoch, separated for each training run. I’ll create a recorder DataFrame where I store this data. Here’s pseudocode for how the recording will take place, referencing functions defined in Jeremy’s notebook and logic used by in the fastai forum post to collect losses and accuracy:\n# code to clean data\n...\n\n# code to create training and validation xs and ys\n...\n\n\n# new function to run multiple trainings\ndef training_run(runs=100):\n  # initialize recorder object\n  recorder = pd.DataFrame(columns=[\"run\", \"epoch\", \"trn_loss\", \"val_loss\", \"acc\"])\n  for run in range(runs):\n    # get lists of losses and accuracy\n    tl, vl, a = train_model(...)\n    # create list of run and epoch values\n    r = [run] * len(tl)\n    e = [i for i in range(len(tl))]\n    # append new data to recorder DataFrame\n    row = pd.DataFrame(data={\"run\": r, \"epoch\": e, \"trn_loss\": tl, \"val_loss\": vl, \"acc\": a})\n    recorder = pd.concat(recorder, row)\n  return recorder\n    \n\n# modify existing function\ndef train_model(...):\n  tl, vl, a = [], [], []\n  for i in range(epochs):\n    trn_loss, val_loss, acc = one_epoch(...)\n    tl.append(trn_loss)\n    vl.append(val_loss)\n    a.append(acc)\n  return tl, vl, a\n\n# modify existing function\ndef one_epoch(...):\n  trn_loss = calc_loss(...)\n  val_loss = calc_loss(...)\n  trn_loss.backward()\n  with torch.no_grad(): update_coeffs(...)\n  acc = calc_acc(...)\n  return trn_loss, val_loss, acc\n\n# use existing function to calculate predictions\ndef calc_preds(...): ...\n\n# use existing function to calculate loss\ndef calc_loss(...): ...\n\n# use existing function to step the weights\ndef update_coeffs(...): ...\n\n# use existing function to calculate accuracy\ndef calc_acc(...): ...\n\n# use existing function to initiate weights\ndef init_coeffs(...): ...\nWith the pseudocode sketched out, I’ll start building out each function next."
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#building-the-functions",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#building-the-functions",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "Building the Functions",
    "text": "Building the Functions\n\nimport torch, numpy as np, pandas as pd, torch.nn.functional as F\nfrom fastai.data.transforms import RandomSplitter\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)\n\n\nInitialize Coefficients\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\n\nCalculate Predictions\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\n\n\nCalculate Loss\n\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\n\n\nUpdate the Coefficients\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\n\nCalculate Accuracy\n\ndef calc_acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\n\n\n\nTrain One Epoch\n\ndef one_epoch(coeffs, lr):\n  trn_loss = calc_loss(coeffs, trn_indep, trn_dep)\n  trn_loss.backward()\n  with torch.no_grad():\n    val_loss = calc_loss(coeffs, val_indep, val_dep)\n    update_coeffs(coeffs, lr)\n    acc = calc_acc(coeffs)\n  return trn_loss, val_loss, acc\n\n\n\nTrain a Model\n\ndef train_model(epochs, lr, n_coeff, is_seed=True):\n  if is_seed: torch.manual_seed(442)\n  tl, vl, a = [], [], []\n  coeffs = init_coeffs(n_coeff)\n  for i in range(epochs):\n    trn_loss, val_loss, acc = one_epoch(coeffs, lr)\n    tl.append(trn_loss.item())\n    vl.append(val_loss.item())\n    a.append(acc.item())\n  return tl, vl, a\n\n\n\nTrain Multiple Models\n\ndef train_multiple_models(runs=100, epochs=30, lr=4, n_coeff=12, is_seed=False):\n  # initialize recorder object\n  recorder = pd.DataFrame(columns=[\"run\", \"epoch\", \"trn_loss\", \"val_loss\", \"acc\"])\n  for run in range(runs):\n    # get lists of losses and accuracy\n    tl, vl, a = train_model(epochs, lr, n_coeff, is_seed)\n    # create list of run and epoch values\n    r = [run] * epochs\n    e = [i for i in range(epochs)]\n    # append new data to recorder DataFrame\n    row = pd.DataFrame(data={\"run\": r, \"epoch\": e, \"trn_loss\": tl, \"val_loss\": vl, \"acc\": a})\n    recorder = pd.concat([recorder, row])\n  return recorder"
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plotting-training-results",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plotting-training-results",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "Plotting Training Results",
    "text": "Plotting Training Results\nIn this section, I’ll import the data, clean it, create training/validation splits, test out my above functions for a single model training loop, run my experiment for 100 training runs, and plot the results.\n\nLoad the Data\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 1.97MB/s]\n\n\n\n\n\n\n\n\n\n\nClean the Data\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# replace NAs with the mode of the column\nmodes = df.mode().iloc[0]\n\n\ndf.fillna(modes, inplace=True)\n\n\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\n\n# take log(Fare + 1) to make the distribution more reasonable\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\n# convert categoricals to dummy variables\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\n# list out the new dummy variables\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n\n\nfrom torch import tensor\n\n# create tensor of dependent variable data\nt_dep = tensor(df.Survived)\n\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\n# create tensor of independent variable data\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep[:2]\n\ntensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000]])\n\n\n\n# normalize the independent variables\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\nt_indep[:2]\n\ntensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000]])\n\n\n\n# create indexes for training/validation splits\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\n\n# split data into training and validation sets\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\n\n# turn dependent variable into column vector\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\n\n\nTrain a Single Model\nFirst, I’ll train a single model to make sure that I’m getting a similar accuracy as Jeremy’s notebook example:\n\nres = train_model(epochs=30, lr=4, n_coeff=12)\n\n\n# accuracy is the second list in our results\n# the final accuracy should be close to 0.8258\nres[2][-1]\n\n0.8258426785469055\n\n\nGreat! My model’s accuracy matches that of the example notebook. Next, I’ll plot the training loss, validation loss and accuracy of the model across 30 epochs:\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\nxs = [i for i in range(30)]\n\nplt.plot(xs, res[0], c='green');\nplt.plot(xs, res[1], c='red');\nplt.plot(xs, res[2], c='blue');\n\nplt.xlabel(\"Epochs\");\nplt.ylabel(\"Loss\\nAccuracy\");\n\ngreen_patch = mpatches.Patch(color='green', label='Training Loss')\nred_patch = mpatches.Patch(color='red', label='Validation Loss')\nblue_patch = mpatches.Patch(color='blue', label='Accuracy')\n\n\nplt.legend(handles=[green_patch, red_patch, blue_patch]);\n\n\n\n\nExcellent! With that confirmed, I can run my trial of 100 trainings, and then plot the results:\n\n\nTraining Multiple Models\n\nrecorder = train_multiple_models()\n\n\nrecorder.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n    \n  \n  \n    \n      0\n      0\n      0\n      0.552340\n      0.540915\n      0.595506\n    \n    \n      1\n      0\n      1\n      0.488773\n      0.491162\n      0.595506\n    \n    \n      2\n      0\n      2\n      0.474533\n      0.479952\n      0.595506\n    \n    \n      3\n      0\n      3\n      0.461460\n      0.469660\n      0.595506\n    \n    \n      4\n      0\n      4\n      0.450005\n      0.460642\n      0.595506\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nrecorder.tail()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n    \n  \n  \n    \n      25\n      99\n      25\n      0.390775\n      0.414015\n      0.595506\n    \n    \n      26\n      99\n      26\n      0.390258\n      0.413608\n      0.595506\n    \n    \n      27\n      99\n      27\n      0.389781\n      0.413232\n      0.595506\n    \n    \n      28\n      99\n      28\n      0.389341\n      0.412886\n      0.595506\n    \n    \n      29\n      99\n      29\n      0.388933\n      0.412565\n      0.595506\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nrecorder.max()\n\nrun               99\nepoch             29\ntrn_loss    0.623253\nval_loss    0.604715\nacc         0.831461\ndtype: object\n\n\n\nPlot: Training Loss\n\n(recorder\n .pivot_table(values='trn_loss', index='epoch', columns='run')\n .plot(color='green', alpha=0.3, legend=False, title='Training Loss'));\n\n\n\n\n\n\nPlot: Validation Loss\n\n(recorder\n .pivot_table(values='val_loss', index='epoch', columns='run')\n .plot(color='red', alpha=0.3, legend=False, title='Validation Loss'));\n\n\n\n\n\n\nPlot: Accuracy\n\n(recorder\n .pivot_table(values='acc', index='epoch', columns='run')\n .plot(color='blue', alpha=0.3, legend=False, title='Accuracy'));\n\n\n\n\n\n\n\nFinal Thoughts\nThis exercise was fascinating, both in terms of building the code to record losses and accuracy for each epoch, as well as observing the final results of 100 training runs.\nThe main observation that stands out: for all three values (training loss, validation loss and accuracy) there were training runs where the values did not improve at all between the first and last epoch. In the case of training and validation loss, it seems like there were numerous runs where the loss was stuck at around 0.4. There were many trainings where the accuracy was stuck at around 0.6.\nOnly for a handful of training runs did the accuracy cross 0.8.\nIn a significant number of runs (as seen by the darkness of the line color on the plot) the training and validation loss gradually decreased during training.\nAfter running this experiment I am pretty surprised. I knew that training neural networks involved some variability, but it’s almost shocking to see how you can get wildly different results for training the same model. Just by happenchance, I can get a model that seemingly does not work (accuracy stuck throughout) and the same model that achieves a better accuracy than the baseline in Jeremy’s notebook. All in all, I’m grateful that I did this exercise because it gave me some perspective on how volatile neural nets can be."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "title": "R Shiny Census App",
    "section": "",
    "text": "In this blog post, I’ll walk through my development process for a U.S. Census data visualization web app I created using the Shiny package in R.\nYou can access the app at vbakshi.shinyapps.io/census-app."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "title": "R Shiny Census App",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackstory\nCodebase\n\napp.R\n\nWhat’s in my ui?\n\nDropdowns\nTables\nPlot\nDownload buttons\n\nWhat does my server do?\n\nGet Data\nRender Outputs\nPrepare Dynamic Text\nHandle Downloads\n\n\nprep_db.R\n\nDatabase Tables\n\nb20005\nb20005_vars\ncodes\n\nCreate Tables\nWrite to Tables\nLoad the Data\n\nget_b20005_ruca_aggregate_earnings.R\n\nGet Variable Names\nDerive RUCA Level Estimates and MOE\n\ncalculate_median.R\n\nCreate Frequency Distribution\nCalculate Weighted Total\nApproximate Standard Error\nCalculate Median Estimate Bounds\nReshape the Data\n\nformat_query_result.R\n\nExtract data.frame Objects from List\nReshape data.frame Objects\nAdd Descriptive Labels\n\nget_b20005_labels.R\n\nGet Earnings Population Estimate Labels\nGet All Labels\n\nget_b20005_tract_earnings.R\n\nGet Variable Names\nJoin Tables\n\nget_b20005_states.R\nget_design_factor.R\nmake_plot.R"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "title": "R Shiny Census App",
    "section": "Backstory",
    "text": "Backstory\nI started this project by reading the handbook Understanding and Using American Community Survey Data: What State and Local Government Users Need to Know published by the U.S. Census Bureau. I recreated the handbook’s first case study in R, in which they make comparisons across geographic areas, create custom geographic areas from census tracts and calculate margins of error for derived estimates for Minnesota Census Tract 5-year earnings estimates.\nDuring the process of recreating the derived median earnings estimate calculations, I was unable to recreate a key value from the handbook (the Standard Error for the 50% proportion, calculated to be 0.599) because I was unable to deduce the values used in the following formula referenced from page 17 of the PUMS Accuracy of the Data documentation:\n\n\n\nStandard Error equals Design Factor times square root of the product of 95 over 5B and 50 squared\n\n\nThe documentation defines B as the base, which is the calculated weighted total. I chose the value of 1.3 for the design factor DF since it corresponds to STATE = Minnesota, CHARTYP = Population, CHARACTERISTIC = Person Earnings/Income in the Design Factors CSV published by the Census Bureau.\nI called the Census Bureau Customer Help Center for assistance and was transferred to a member of the ACS Data User Support team with whom I discussed my woes. He was unable to confirm the values of the design factor DF or B, and was unable to pull up the contact information for the statistical methodology team, so I emailed him my questions. After a few email exchanges, the statistical methodology team provided the following:\n\nDF = 1.3\nB = the total population estimate for which the median is being calculated, which is 82488 for the case study calculation (Minnesota Rural Male Full Time Workers)\nThe term 95/5 is associated with the finite population correction factor (100 - f) divided by the sample fraction (f), where f = 5% (later on I note in the documentation that this 95/5 term is based on a 68% confidence interval). The data used in the handbook case study is from 5-year estimates. 1-year estimates sample 2.5% of the population, so the 5-year estimates represent a 5 * 2.5 = 12.5% sample. Instead of 95/5, the ratio becomes (100 - 12.5)/12.5 = 87.5/12.5\n\nThe updated formula is then:\n\n\n\nStandard Error equals Design Factor times square root of the product of 87.5 over 12.5B and 50 squared\n\n\nI was able to calculate the median earnings estimate (and associated standard error and margin of error) within a few percent of the values given in the handbook. This provided me with confirmation that I was ready to expand my code to calculate median earnings estimates for other subgroups."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "title": "R Shiny Census App",
    "section": "Codebase",
    "text": "Codebase\nI built this app using the R package Shiny which handles both the UI and the server. I store the data in a sqlite database and access it with queries written using the RSQLite package which uses the DBI API. The following sections break down the R scripts based on functionality. Click on the script name to navigate to that section.\n\napp.R\n\nUI and server functions to handle people inputs and plot/table/text outputs\n\nprep_db.R\n\nImport, clean, combine and then load data into the census_app_db.sqlite database\n\nget_b20005_ruca_aggregate_earnings.R\n\nQueries the database for earnings and associated margins of error for RUCA levels derived from Census Tracts\n\ncalculate_median.R\n\nDerives estimate, standard of error and margin of error of median earnings for RUCA levels\n\nformat_query_result.R\n\nFormats calculate_median query results\n\nget_b20005_labels.R\n\nQueries the database for descriptive labels of B20005 table variables\n\nget_b20005_tract_earnings.R\n\nQueries the database for Census Tract-level earnings and associated margins of error\n\nget_b20005_states.R\n\nQueries the SQLite database for a list of U.S. states\n\nget_design_factor.R\n\nQueries database for the design factor used for the median earnings estimation calculation\n\nmake_plot.R\n\nCreates a bar plot object"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "title": "R Shiny Census App",
    "section": "app.R",
    "text": "app.R\nA shiny app has three fundamental components:\nui <- (...)\nserver <- (...)\nshinyApp(ui, server,...)\nThe ui object holds all UI layout, input and output objects which define the front-end of your app. The server object holds all rendering functions which are assigned to outputs that appear on the UI. The shinyApp function takes a ui and server object (along with other arguments) and creates a shiny app object which can be run in a browser by passing it to the runApp function. Person inputs (such as selections in a dropdown) are assigned to a global input object.\n\nWhat’s in my ui?\nAll of my UI objects are wrapped within a fluidPage call which returns a page layout which “consists of rows which in turn include columns” (from the docs).\nMy app’s UI has four sections:\n\nDropdowns to select state, sex and work status for which the person using the app wants ACS 5-year earnings estimates\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\nA table with population estimates for earnings levels for each RUCA level for the selected state, sex and work status\n\nEach section has a download button so that people can get the CSV files or plot image for their own analysis and reporting. Each section is separated with markdown('---') which renders an HTML horizontal rule (<hr>).\n\nDropdowns\nDropdowns (the HTML <select> element) are a type of UI Input. I define each with an inputId which is a character object for reference on the server-side, a label character object which is rendered above the dropdown, and a list object which defines the dropdown options.\nselectInput(\n  inputId = \"...\",\n  label = \"...\",\n  choices = list(...)\n)\nIn some cases, I want the person to see a character object in the dropdown that is more human-readable (e.g. \"Large Town\") but use a corresponding input value in the server which is more computer-readable (e.g. \"Large_Town). To achieve this, I use a named character vector where the names are displayed in the dropdown, and the assigned values are assigned to the global input:\nselectInput(\n     inputId = \"ruca_level\",\n     label = \"Select RUCA Level\",\n     choices = list(\n       \"RUCA LEVEL\" = c(\n       \"Urban\" = \"Urban\", \n       \"Large Town\" = \"Large_Town\", \n       \"Small Town\" = \"Small_Town\", \n       \"Rural\" = \"Rural\"))\n     )\nIn this case, if the person selects \"Large Town\" the value assigned to input$ruca_level is \"Large_Town\".\n\n\nTables\nTables (the HTML <table> element) are a type of UI Output. I define each with an outputId for reference in the server.\ntableOutput(outputId = \"...\")\n\n\nPlot\nSimilarly, a plot (which is rendered as an HTML <img> element) is a type of UI Output. I define each with an outputId.\nplotOutput(outputId = \"...\")\n\n\nDownload Buttons\nThe download button (an HTML <a> element) is also a type of UI Output. I define each with an outputId and label (which is displayed as the HTML textContent attribute of the <a> element).\ndownloadButton(\n  outputId = \"...\",\n  label = \"...\"\n)\n\n\n\nWhat does my server do?\nThe server function has three parameters: input, output and session. The input object is a ReactiveValues object which stores all UI Input values, which are accessed with input$inputId. The output object similarly holds UI Output values at output$outputId. I do not use the session object in my app (yet).\nMy app’s server has four sections:\n\nGet data from the SQLite database\nRender table and plot outputs\nPrepare dynamic text (for filenames and the plot title)\nHandle data.frame and plot downloads\n\n\nGet Data\nThere are three high-level functions which call query/format/calculation functions to return the data in the format necessary to produce table, text, download and plot outputs:\n\nThe earnings_data function passes the person-selected dropdown options input$sex, input$work_status and input$state to the get_b20005_ruca_aggregate_earnings function to get a query result from the SQLite database. That function call is passed to format_earnings, which in turn is passed to the reactive function to make it a reactive expression. Only reactive expressions (and reactive endpoints in the output object) are allowed to access the input object which is a reactive source. You can read more about Shiny’s “reactive programming model” in this excellent article.\n\nearnings_data <- reactive(\n  format_earnings(\n    get_b20005_ruca_aggregate_earnings(\n      input$sex, \n      input$work_status, \n      input$state)))\n\nThe design_factor function passes the input$state selection to the get_design_factor function which in turn is passed to the reactive function.\n\ndesign_factor <- reactive(get_design_factor(input$state))\n\nThe median_data function passes the return values from earnings_data() and design_factor() to the calculate_median function which in turn is passed to the reactive function.\n\nmedian_data <- reactive(calculate_median(earnings_data(), design_factor()))\n\n\nRender Outputs\nI have two reactive endpoints for table outputs, and one endpoint for a plot. The table outputs use renderTable (with row names displayed) with the data.frame coming from median_data() and earnings_data(). The plot output uses renderPlot, and a helper function make_plot to create a bar plot of earnings_data() for a person-selected input$ruca_level with a title created with the helper function earnings_plot_title().\noutput$median_data <- renderTable(\n  expr = median_data(), \n  rownames = TRUE)\n  \noutput$earnings_data <- renderTable(\n  expr = earnings_data(), \n  rownames = TRUE)\n    \noutput$earnings_histogram <- renderPlot(\n  expr = make_plot(\n    data=earnings_data(), \n    ruca_level=input$ruca_level, \n    plot_title=earnings_plot_title()))\n\n\nPrepare Dynamic Text\nI created four functions that generate filenames for the downloadHandler call when the corresponding downloadButton gets clicked, one function that generates the title used to generate the bar plot, and one function which takes computer-readable character objects (e.g. \"Large_Town\") and maps it to and returns a more human-readable character object (e.g. \"Large Town\"). I chose to keep filenames more computer-readable (to avoid spaces) and the plot title more human-readable.\nget_pretty_text <- function(raw_text){\n  text_map <- c(\"M\" = \"Male\", \n  \"F\" = \"Female\",\n  \"FT\" = \"Full Time\",\n  \"OTHER\" = \"Other\",\n  \"Urban\" = \"Urban\",\n  \"Large_Town\" = \"Large Town\",\n  \"Small_Town\" = \"Small Town\",\n  \"Rural\" = \"Rural\")\n  return(text_map[raw_text])\n  }\n \nearnings_plot_title <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    get_pretty_text(input$work_status),\n    input$ruca_level,\n    \"Workers\",\n    sep=\" \"))\n  }\n\nb20005_filename <- function(){\n    return(paste(\n      input$state,\n      get_pretty_text(input$sex),\n      input$work_status,\n      \"earnings.csv\",\n      sep=\"_\"\n    ))\n  }\n  \nmedian_summary_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex), \n    input$work_status, \n    'estimated_median_earnings_summary.csv',  \n    sep=\"_\")\n  }\n  \nruca_earnings_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex),  \n    input$work_status, \n    'estimated_median_earnings_by_ruca_level.csv',  \n    sep=\"_\")\n  }\n  \nearnings_plot_filename <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    input$work_status,\n    input$ruca_level,\n    \"Workers.png\",\n    sep=\"_\"))\n  }\n\n\nHandle downloads\nI have five download buttons in my app: two which trigger a download of a zip file with two CSVs, two that downloads a single CSV, and one that downloads a single PNG. The downloadHandler function takes a filename and a content function to write data to a file.\nIn order to create a zip file, I use the zip base package function and pass it a vector with two filepaths (to which data is written using the base package’s write.csv function) and a filename. I also specify the contentType as \"application/zip\". In the zip file, one of the CSVs contains a query result from the b20005 SQLite database table with earnings data, and the other file, \"b20005_variables.csv\" contains B20005 table variable names and descriptions. In order to avoid the files being written locally before download, I create a temporary directory with tempdir and prepend it to the filename to create the filepath.\nFor the bar plot image download, I use the ggplot2 package’s ggsave function, which takes a filename, a plot object (returned from the make_plot helper function) and the character object \"png\" (for the device parameter).\noutput$download_selected_b20005_data <- downloadHandler(\n    filename = \"b20005_data.zip\",\n    content = function(fname) {\n      # Create a temporary directory to prevent local storage of new files\n      temp_dir <- tempdir()\n      \n      # Create two filepath character objects and store them in a list\n      # which will later on be passed to the `zip` function\n      path1 <- paste(temp_dir, '/', b20005_filename(), sep=\"\")\n      path2 <- paste(temp_dir, \"/b20005_variables.csv\", sep=\"\")\n      fs <- c(path1, path2)\n      \n      # Create a CSV with person-selection input values and do not add a column\n      # with row names\n      write.csv(\n        get_b20005_earnings(input$state, input$sex, input$work_status), \n        path1,\n        row.names = FALSE)\n      \n      # Create a CSV for table B20005 variable names and labels for reference\n      write.csv(\n        get_b20005_ALL_labels(),\n        path2,\n        row.names = FALSE)\n      \n      # Zip together the files and add flags to maximize compression\n      zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_all_b20005_data <- downloadHandler(\n  filename = \"ALL_B20005_data.zip\",\n  content = function(fname){\n    path1 <- \"ALL_B20005_data.csv\"\n    path2 <- \"b20005_variables.csv\"\n    fs <- c(path1, path2)\n    \n    write.csv(\n      get_b20005_earnings('ALL', 'ALL', 'ALL'),\n      path1,\n      row.names = FALSE)\n    \n    write.csv(\n      get_b20005_ALL_labels(),\n      path2,\n      row.names = FALSE)\n    \n    zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_median_summary <- downloadHandler(\n  filename = median_summary_filename(),\n  content = function(file) {\n    write.csv(median_data(), file)\n    }\n  )\n  \noutput$download_earnings_plot <- downloadHandler(\n  filename = earnings_plot_filename(),\n  content = function(file) {\n    ggsave(\n      file, \n      plot = make_plot(\n        data=earnings_data(), \n        ruca_level=input$ruca_level, \n        plot_title=earnings_plot_title()), \n        device = \"png\")\n      }\n  )\n  \noutput$download_ruca_earnings <- downloadHandler(\n  filename = ruca_earnings_filename(),\n  content = function(file) {\n    write.csv(earnings_data(), file)\n  }\n  )"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "title": "R Shiny Census App",
    "section": "prep_db.R",
    "text": "prep_db.R\nThis script is meant to be run locally, and is not deployed, as doing so would create a long delay to load the app.\n\nDatabase Tables\nThe database diagram is shown below (created using dbdiagram.io):\n\n\n\nDatabase diagram showing the database table schemas and their relationships\n\n\nI have five tables in my database:\n\n\nb20005\nHolds the data from the ACS 2015-2019 5-year detailed table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months). This includes earnings estimates and margins of errors for Male and Female, Full Time and Other workers, for earning ranges (No earnings, $1 - $2499, $2500 - $4999, …, $100000 or more). The following table summarizes the groupings of the (non-zero earnings) variables relevant to this app:\n\n\n\n\nVariable\nDemographic\n\n\n\n\nB20005_003 to B20005_025\nMale Full Time Workers\n\n\nB20005_029 to B20005_048\nMale Other Workers\n\n\nB20005_050 to B20005_072\nFemale Full Time Workers\n\n\nB20005_076 to B20005_095\nFemale Other Workers\n\n\n\n\n\n\nb20005_vars\nHas the name (e.g. B20005_003E) and label (e.g. “Estimate!!Total!!Male!!Worked full-time, year-round in the past 12 months”) for all B20005 variables. Variable names ending with an E are estimates, and those ending with M are margins of error. - ruca contains RUCA (Rural-Urban Commuting Area) codes published by the U.S. Department of Agriculture Economic Research Service which classify U.S. census tracts using measures of population density. The following table shows the code ranges relevant to this app:\n\n\n\n\nRUCA Code\nRUCA Level\n\n\n\n\n1-3\nUrban\n\n\n4-6\nLarge Town\n\n\n7-9\nSmall Town\n\n\n10\nRural\n\n\n99\nZero Population\n\n\n\n\n\n\ncodes\nolds state FIPS (Federal Information Processing Standards) codes and RUCA levels - design_factors contains Design Factors for different characteristics (e.g. Person Earnings/Income) which are used to determine “the standard error of total and percentage sample estimates”, and “reflect the effects of the actual sample design and estimation procedures used for the ACS.” (2015-2019 PUMS 5-Year Accuracy of the Data).\nIn prep_db.R, I use the DBI package, censusapi and base R functions to perform the following protocol for each table:\n\n\nLoad the Data\n\nFor tables b20005 and b20005_vars, I use the censusapi::getCensus and censusapi::listCensusMetadata repsectively to get the data\n\n# TABLE b20005_vars ------------------------------\nb20005_vars <- listCensusMetadata(\n  name = 'acs/acs5',\n  vintage = 2015,\n  type = 'variables',\n  group = 'B20005')\n  \n # TABLE b20005 ----------------------------------\n b20005 <- getCensus(\n  name = 'acs/acs5',\n  region = \"tract:*\",\n  regionin = regionin_value,\n  vintage = 2015,\n  vars = b20005_vars$name,\n  key=\"...\"\n  )\n\nFor tables codes, ruca, and design_factors I load the data from CSVs that I either obtained (in the case of the Design Factors) or created (in the case of the codes and RUCA levels)\n\n # TABLE codes ----------------------------------\nstate_codes <- read.csv(\n  \"data/state_codes.csv\",\n  colClasses = c(\n    \"character\", \n    \"character\", \n    \"character\")\n)\n\nruca_levels <- read.csv(\n  \"data/ruca_levels.csv\",\n  colClasses = c(\n    \"character\",\n    \"character\",\n    \"character\")\n)\n\n\nCreate Tables\nOnce the data is ready, I use DBI::dbExecute to run a SQLite command to create each table. The relationships shown in the image above dictate which fields create the primary key (in some cases, a compound primary key) as listed below:\n\n\n\n\n\n\n\n\nTable\nPrimary Key\nNotes\n\n\n\n\nb20005\n(state, county, tract))\nForeign key for table ruca\n\n\nb20005_vars\nname\ne.g. B20005_001E\n\n\nruca\nTRACTFIPS\nForeign key for table b20005\n\n\ncodes\n(CODE, DESCRIPTION)\ne.g. (1, \"Urban\")\n\n\ndesign_factors\n(ST, CHARACTERISTIC)\ne.g. (\"27\", \"Person Earnings/Income\")\n\n\n\n\n\nWrite to Tables\nOnce the table has been created in the database, I write the data.frame to the corresponding table with the following call:\ndbWriteTable(census_app_db, \"<table name>\", <data.frame>, append = TRUE"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_ruca_aggregate_earnings.R",
    "text": "get_b20005_ruca_aggregate_earnings.R\nThe function inside this script (with the same name), receives inputs from the server, sends queries to the database and returns the results. This process involves two steps:\n\nGet Variable Names\nThe person using the app selects Sex (M or F), Work Status (Full Time or Other) and State (50 states + D.C. + Puerto Rico) for which they want to view and analyze earnings data. As shown above, different variables in table b20005 correspond to different sexes and work statuses, and each tract for which there is all that earnings data resides in a given state.\nI first query b20005_vars to get the relevent variables names which will be used in the query to b20005, as shown below. names that end with “M” (queried with the wilcard '%M') are for margins of error and those that end with “E” (wildcard '%E') are for estimates.\nvars <- dbGetQuery(\n    census_app_db, \n    \"SELECT name FROM b20005_vars \n    WHERE label LIKE $label_wildcard \n    AND name LIKE '%M'\",\n    params=list(label_wildcard=label_wildcard))\nThe b20005_vars.label column holds long string labels (which follow a consistent pattern, which is captured by the $label_wildcard) that describe the variable’s contents. Here are a couple of examples: \n\n\n\n\n\n\n\nb20005_vars.name\nb20005_vars.label\n\n\n\n\nB20005_053E\n\"Estimate!!Total!!Female!!Worked full-time, year-round in the past 12 months!!With earnings\")\n\n\nB20005_076M\n\"Margin of Error!!Total!!Female!!Other!!With earnings!!$1 to $2,499 or loss\"\n\n\n\n\nSince the label string contains the sex and work status, I assign a label_wildcard based on the person inputs from the sex and work status UI dropdowns.\n# Prepare wildcard for query parameter `label_wildcard`\n  if (sex == 'M') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Male!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Male!!Other%\" }\n  }\n  \n  if (sex == 'F') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Female!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Female!!Other%\" }\n  }\n\n\nDerive RUCA Level Estimates and MOE\nOnce the variables are returned, the actual values are queried from b20005, grouped by RUCA level. The ACS handbook Understanding and Using American Community Survey Data: What All Data Users Need to Know shows how to calculate that margin of error for derived estimates. In our case, the margin of error for a RUCA level such as “Urban” for a given state is derived from the margin of error of individual Census Tracts using the formula below:\n\n\n\nThe MOE for a sum of estimates is the square root of the sum of MOEs squared\n\n\nTranslating this to a SQLite query:\n# Construct query string to square root of the sum of margins of error squared grouped by ruca level\nquery_string <- paste0(\n    \"SQRT(SUM(POWER(b20005.\", vars$name, \", 2))) AS \", vars$name, collapse=\",\")\nWhere vars$name is a list of variable names, and the collapse parameter converts a list or vector to a string. The beginning of that query_string looks like:\n\"SQRT(SUM(POWER(b20005.B20005_001M, 2))) AS B20005_001M, SQRT(...\"\nThe query is further built by adding the rest of the SQL statements:\nquery_string <- paste(\n    \"SELECT ruca.DESCRIPTION,\",\n    query_string,\n    \"FROM 'b20005' \n    INNER JOIN ruca \n    ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n    WHERE \n    b20005.state = $state\n    GROUP BY ruca.DESCRIPTION\"\n  )\nThe ruca.DESCRIPTION column, which contains RUCA levels (e.g. \"Urban\") is joined onto b20005 from the ruca table using the foreign keys representing the Census Tract FIPS code (TRACTFIPS for the ruca table and the concatenated field state || county || tract for b20005). The $state parameter is assigned the person-selected state input, and the columns are aggreaggated by RUCA levels (i.e. GROUP BY ruca.DESCRIPTION). Finally, the RUCA level and square root of the sum of MOEs squared are SELECTed from the joined tables.\nThe query for estimates is simpler than MOEs, because estimates only need to be summed over RUCA levels:\n# Construct a query to sum estimates grouped by ruca level\n  query_string <- paste0(\"SUM(b20005.\",vars$name, \") AS \", vars$name, collapse=\",\")\nget_b20005_ruca_aggregate_earnings returns the query result data.frames in a named list:\nreturn(list(\"estimate\" = estimate_rs, \"moe\" = moe_rs))"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "title": "R Shiny Census App",
    "section": "calculate_median.R",
    "text": "calculate_median.R\nThe procedure for calculating a median earnings data estimate is shown starting on page 17 of the Accuracy of PUMS documentation. This script follows it closely:\n\nCreate Frequency Distribution\n\nObtain the weighted frequency distribution for the selected variable. data is a data.frame with earning estimate values. The rows are the earning ranges and the columns are ruca_levels:\n\n\ncum_percent <- 100.0 * cumsum(data[ruca_level]) / sum(data[ruca_level])\n\n\nCalculate Weighted Total\n\nCalculate the weighted total to yield the base, B.\n\n\nB <- colSums(data[ruca_level])\n\n\nApproximate Standard Error\n\nApproximate the standard error of a 50 percent proportion using the formula in Standard Errors for Totals and Percentages. The design_factor is passed to this function by the server who uses the get_design_factor function explained below to query the design_factors table.\n\n\nse_50_percent <- design_factor * sqrt(87.5/(12.5*B) * 50^2)\n\n\nCalculate Median Estimate Bounds\n\nCreate the variable p_lower by subtracting the SE from 50 percent. Create p_upper by adding the SE to 50 percent.\n\n\np_lower <- 50 - se_50_percent\np_upper <- 50 + se_50_percent\n\nDetermine the categories in the distribution that contain p_lower and p_upper…\n\n\n# Determine the indexes of the cumulative percent data.frame corresponding  \n# to the upper and lower bounds of the 50% proportion estimate\ncum_percent_idx_lower <- min(which(cum_percent > p_lower))\ncum_percent_idx_upper <- min(which(cum_percent > p_upper))\n.._If p_lower and p_upper fall in the same category, follow step 6. If p_lower and p_upper fall in different categories, go to step 7…_\n\n# The median estimation calculation is handled differently based on \n# whether the upper and lower bound indexes are equal\n    if (cum_percent_idx_lower == cum_percent_idx_upper) {\n\nIf p_lower and p_upper fall in the same category, do the following:\n\n\nDefine A1 as the smallest value in that category.\n\n\n# A1 is the minimum earnings value (e.g. 30000) of the earning range \n# (e.g. 30000 to 34999) corresponding to the lower bound cumulative percent\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\n\nDefine A2 as the smallest value in the next (higher) category.\n\n\n# A2 is the minimum earnings value of the earning range above the \n# earning range corresponding to the upper bound cumulative percent\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\n\nDefine C1 as the cumulative percent of units strictly less than A1.\n\n\n# C1 is the cumulative percentage of earnings one row below the \n# lower bound cumulative percent\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\n\nDefine C2 as the cumulative percent of units strictly less than A2.\n\n\n# C2 is the cumulative percentage of the earnings below the \n# lower bound cumulative percent\nC2 <- cum_percent[cum_percent_idx_lower, ]\n\nUse the following formulas to approximate the lower and upper bounds for a confidence interval about the median:\n\n\n# the lower bound of the median \nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n      \n# the upper bound of the median\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nIf p_lower and p_upper fall in different categories, do the following:\n\n\nFor the category containing p_lower: Define A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the lower bound.\n\n\n# A1, A2, C1 and C2 are calculated using the lower bound cumulative percent\n# to calculate the lower bound of the median estimate\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\nC2 <- cum_percent[cum_percent_idx_lower, ]\nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n\nFor the category containing p_upper: Define new values for A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the upper bound.\n\n\n# A1, A2, C1 and C2 are calculated using the upper bound cumulative percent\n# to calculate the upper bound of the median estimate\nA1 <- earnings[cum_percent_idx_upper, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_upper + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_upper - 1,]\nC2 <- cum_percent[cum_percent_idx_upper,]\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nUse the lower and upper bounds approximated in steps 6 or 7 to approximate the standard error of the median. SE(median) = 1/2 X (Upper Bound – Lower Bound)\n\n\n# The median earning estimate is the average of the upper and lower bounds\n# of the median estimates calculated above in the if-else block\nmedian_earnings <- 0.5 * (lower_bound + upper_bound)\n    \n# The median SE is half the distance between the upper and lower bounds\n# of the median estimate\nmedian_se <- 0.5 * (upper_bound - lower_bound)\n\n# The 90% confidence interval critical z-score is used to calculate \n# the margin of error\nmedian_90_moe <- 1.645 * median_se\n\n\nReshape the Data\nFinally, a data.frame is returned, which will be displayed in a tableOutput element.\n\n# A data.frame will be displayed in the UI\nmedian_data <- data.frame(\n  \"Estimate\" = median_earnings,\n  \"SE\" = median_se,\n  \"MOE\" = median_90_moe\n)"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "title": "R Shiny Census App",
    "section": "format_query_result.R",
    "text": "format_query_result.R\nThe purpose of this function is to receive two data.frame objects, one for earnings estimate values, and one for the corresponding moe values, and return a single data.frame which is ready to be displayed in a tableOutput.\n\nExtract data.frame Objects from List\nSince get_b20005_ruca_aggregate_earnings returns a named list, I first pull out the estimate and moe data.frame objects:\n\n# Pull out query result data.frames from the list\nestimate <- rs[[\"estimate\"]]\nmoe <- rs[[\"moe\"]]\n\n\nReshape data.frame Objects\nThese data.frame objects have RUCA levels in the column DESCRIPTION and one column for each population estimate. For example, the estimate for Alabama Full Time Female workers looks like this:\n\n\n\n\nDESCRIPTION\n…\nB20005_053E\nB20005_054E\nB20005_055E\n…\n\n\n\n\n1\nLarge Town\n…\n149\n257\n546\n…\n\n\n2\nRural\n…\n75\n66\n351\n…\n\n\n3\nSmall Town\n…\n28\n162\n634\n…\n\n\n4\nUrban\n…\n468\n1061\n4732\n…\n\n\n5\nZero Population\n…\n0\n0\n0\n…\n\n\n\nThe moe data.frame has a similar layout.\nHowever, in the UI, I want the table to look like this:\n\n\n\nPopulation estimates for earnings levels from $1 to $2499 up to $100000 and more for Alabama Full Time Female Workers\n\n\nTo achieve this, I first transpose the estimate and moe data.frames…\n\n# Transpose the query results\ncol_names <- estimate[,\"DESCRIPTION\"]\nestimate <- t(estimate[-1])\ncolnames(estimate) <- col_names\n  \ncol_names <- moe[,\"DESCRIPTION\"]\nmoe <- t(moe[-1])\ncolnames(moe) <- col_names\n…then zip them together, keeping in mind that not all states have tracts designated with all RUCA levels:\n\n# Create a mapping to make column names more computer-readable\nformat_ruca_level <- c(\n  \"Urban\" = \"Urban\", \n  \"Large Town\" = \"Large_Town\", \n  \"Small Town\" = \"Small_Town\", \n  \"Rural\" = \"Rural\",\n  \"Zero Population\" = \"Zero_Population\")\n\n# bind together estimate and corresponding moe columns\n# some states do not have all RUCA levels\n# for example, Connecticut does not have \"Small Town\" tracts\n\n# Create empty objects\noutput_table <- data.frame(temp = matrix(NA, nrow = nrow(estimate), ncol = 0))\ncol_names <- c()\n\nfor (ruca_level in c(\"Urban\", \"Large Town\", \"Small Town\", \"Rural\")) {\n  if (ruca_level %in% colnames(estimate)) {\n    output_table <- cbind(output_table, estimate[,ruca_level], moe[,ruca_level])\n    \n    # paste \"_MOE\" suffix for MOE columns\n    col_names <- c(\n      col_names,\n      format_ruca_level[[ruca_level]],\n      paste0(format_ruca_level[[ruca_level]], \"_MOE\"))\n  }\n}\n\n# Replace old names with more computer-readable names\ncolnames(output_table) <- col_names\n\n\n\nAdd Descriptive Labels\nFinally, merge the output_table data.frame with labels (long form description of the B20005 variables) which are retrieved from the database using the get_b20005_labels function explained later on in this post. Remember that the label is delimited with \"!!\" and the last substring contains earnings ranges (e.g. “$30,000 to $34,999”):\n\n# name rows as long-form labels, by splitting them by '!!' and \n# grabbing the last chunk which has dollar ranges e.g. \n# $30000 to $34999\noutput_table <- merge(output_table, labels, by.x = 0, by.y = \"name\")\nsplit_label <- data.frame(\n  do.call(\n    'rbind', \n    strsplit(as.character(output_table$label),'!!',fixed=TRUE)))\n\nrownames(output_table) <- split_label$X6"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_labels.R",
    "text": "get_b20005_labels.R\nThis script contains two helper functions to retrieve the label column from the b20005_vars table.\n\nGet Earnings Population Estimate Labels\nThe first one, get_b20005_labels retrieves the variable name and label for earning range strings (e.g. “$30,000 to $34,999”):\n\nget_b20005_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    WHERE \n      label LIKE '%$%'\n    ORDER BY name\"\n    )\n  dbDisconnect(census_app_db)\n  return(rs)\n}\n\n\n\nGet All Labels\nThe second function, get_b20005_ALL_labels returns the whole table:\n\nget_b20005_ALL_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    ORDER BY name\"\n  )\n  dbDisconnect(census_app_db)\n  return(rs)\n}"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_tract_earnings.R",
    "text": "get_b20005_tract_earnings.R\nThis function is similar to get_b20005_ruca_aggregate_earnings but does not aggregate by RUCA level, and also includes Census Tracts that are not designated a RUCA level. The label_wildcard is constructed the same way as before.\n\nGet Variable Names\nThe variable names are obtained for both margin of error and estimates in the same query:\n\n # Get b20005 variable names (estimates and moe)\nvars <- dbGetQuery(\n  census_app_db, \n  \"SELECT name FROM b20005_vars \n  WHERE label LIKE $label_wildcard\",\n  params=list(label_wildcard=label_wildcard)\n  )\n\n\n\nJoin Tables\nThe tract-level earnings are queried with the following, using a LEFT JOIN between b20005 and ruca tables to include tracts that do not have a RUCA level.\n\n# Construct query to get tract-level earnings data\nquery_string <- paste(\n  \"SELECT ruca.DESCRIPTION,\n  b20005.state || b20005.county || b20005.tract AS TRACTFIPS,\",\n  paste0(vars$name, collapse=\",\"),\n  \"FROM b20005 \n  LEFT JOIN ruca \n  ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n  WHERE \n  b20005.state LIKE $state\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_states.R",
    "text": "get_b20005_states.R\nThis function retrieves state codes and names from the codes table, and is used to assign choices to selectInput dropdowns. \"United States\" which has a FIPS code of \"00\" is excluded because the b20005 table contains state-level data only. The query result is sorted by the state name so that the dropdown menu choices are in ascending alphabetical order.\nstates <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESCRIPTION, CODE\n  FROM codes \n  WHERE CATEGORY = 'state'\n  AND CODE <> '00'\n  ORDER BY DESCRIPTION\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "title": "R Shiny Census App",
    "section": "get_design_factor.R",
    "text": "get_design_factor.R\nThis function retrieves a single numeric Design Factor for the “Person Earnings/Income” characteristic from the design_factors table for a given state parameter:\n\nrs <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESIGN_FACTOR FROM design_factors\n  WHERE ST = $state\n  AND CHARACTERISTIC = 'Person Earnings/Income'\",\n  params = list(state=state))\n\nrs <- as.numeric(rs[1, \"DESIGN_FACTOR\"])"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "title": "R Shiny Census App",
    "section": "make_plot.R",
    "text": "make_plot.R\nThis is function creates a ggplot.bar_plot object using a given data, RUCA level, and title. The x-axis labels are rotated, both axis labels are resized, and plot title and subtitle are formatted.\n\nmake_plot <- function(data, ruca_level, plot_title){\n  # Prepare x-axis factor for `aes` parameter\n  xs <- rownames(data)\n  xs <- factor(xs, xs)\n\n  bar_plot <- ggplot(\n    data=data,\n    aes(x=xs, y=get(ruca_level))) + \n    geom_bar(stat='identity') + \n\n    theme(\n      # Rotate x-axis labels\n      axis.text.x=element_text(\n        angle = -90, \n        vjust = 0.5, \n        hjust=1, \n        size=12),\n\n      # Resize x-axis labels and move them away from axis\n      axis.title.x=element_text(vjust=-0.75,size=14),\n\n      # Resize y-axis labels\n      axis.text.y=element_text(size=12),\n      axis.title.y=element_text(size=14),\n\n      # Set plot title and subtitle font and placement\n      plot.title = element_text(size = 18, hjust=0.5, face='bold'),\n      plot.subtitle = element_text(size = 12, hjust=0.5)) +\n\n    labs(x=\"Earnings\", y=\"Population Estimate\") + \n    ggtitle(plot_title, subtitle=\"Population Estimate by Earnings Level\")\n\n  return (bar_plot)\n}"
  },
  {
    "objectID": "posts/2024-07-31-bm25-cosine-similarity-demo/index.html",
    "href": "posts/2024-07-31-bm25-cosine-similarity-demo/index.html",
    "title": "BM25 and Cosine Similarity Demo",
    "section": "",
    "text": "In this notebook I’ll work through a simple demo of full text search (using BM25 in SQLite) and cosine similarity (using sentence_transformers) to get my feet wet before I dive into building a pipeline for my fastbookRAG project (where I’ll be trying to answer questions from end-of-chapter questionnaires in the freely available fastai textbook using an LLM, full text search and embedding cosine similarity).\n\n\nIn this demo I intend to show how different keywords can be used to match different texts during full text search.\nSQLite is built-in to the python standard library and has a full text search (using BM25) built-in! I’ll start by creating a demo database.\n\nimport sqlite3\n\nconn = sqlite3.connect('my.db')\nconn.close()\n\nFull text search is available in virtual tables, so I’ll create a table with a single column which will hold the text data that I want to search when responding to a query. The key statement is USING FTS5(text);.\n\nconn = sqlite3.connect('my.db')\ncur = conn.cursor()\nres = cur.execute(\"\"\"\n\nCREATE VIRTUAL TABLE virtual_text_data\nUSING FTS5(text);\n\n\"\"\")\nconn.close()\n\nI’ll now populate my demo “database” with my demo “dataset” that contains three lines of natural language text that I wrote based on ESPN stats (and my opinions). I have intentionally used different parts of the name \"Philadelphia Eagles\" in each record:\n\nconn = sqlite3.connect('my.db')\ncur = conn.cursor()\nres = cur.execute(\"\"\"\n\nINSERT INTO virtual_text_data(text)\nVALUES\n ('The Philadelphia Eagles are set to have a great season in 2024'),\n ('The Eagles signed superstar running back Saquon Barkley in the offseason'),\n ('Philadelphia went 1-7 to finish their season last year');\n\n \"\"\")\nconn.commit()\nconn.close()\n\n\nconn = sqlite3.connect('my.db')\ncur = conn.cursor()\nres = cur.execute(\"SELECT * from virtual_text_data\")\nres.fetchall()\n\n[('The Philadelphia Eagles are set to have a great season in 2024',),\n ('The Eagles signed superstar running back Saquon Barkley in the offseason',),\n ('Philadelphia went 1-7 to finish their season last year',)]\n\n\nFinally, I’ll perform full text search by passing a set of keywords in my SQL query using three ways to refer to the Philadelphia Eagles. The key statement is:\nWHERE virtual_text_data MATCH '\"Philadelphia Eagles\" OR \"Eagles\" OR \"Philadelphia\"'\nThe results are as expected—the record with the full term \"Philadelphia Eagles\" (all three keywords match) has the highest BM25 rank while the other two have equal scores as they both contain just one keyword (\"Philadelphia\" or \"Eagles\"). Note that SQLite multiplies the BM25 rank by -1 since by default it returns ascending order.\n\nres = cur.execute(\"\"\"\n\nSELECT *, rank\n  from virtual_text_data\nWHERE virtual_text_data MATCH '\"Philadelphia Eagles\" OR \"Eagles\" OR \"Philadelphia\"'\nORDER BY rank\n\n\"\"\")\n\nres.fetchall()\n\n[('The Philadelphia Eagles are set to have a great season in 2024',\n  -0.4925110954237839),\n ('Philadelphia went 1-7 to finish their season last year',\n  -1.03862660944206e-06),\n ('The Eagles signed superstar running back Saquon Barkley in the offseason',\n  -1e-06)]\n\n\nChanging the order of keywords does not change the BM25 ranking:\n\nres = cur.execute(\"\"\"\n\nSELECT *, rank\n  from virtual_text_data\nWHERE virtual_text_data MATCH '\"Eagles\" OR \"Philadelphia Eagles\" OR \"Philadelphia\"'\nORDER BY rank\n\n\"\"\")\n\nres.fetchall()\n\n[('The Philadelphia Eagles are set to have a great season in 2024',\n  -0.4925110954237839),\n ('Philadelphia went 1-7 to finish their season last year',\n  -1.03862660944206e-06),\n ('The Eagles signed superstar running back Saquon Barkley in the offseason',\n  -1e-06)]\n\n\n\nres = cur.execute(\"\"\"\n\nSELECT *, rank\n  from virtual_text_data\nWHERE virtual_text_data MATCH '\"Philadelphia\" OR \"Philadelphia Eagles\" OR \"Eagles\"'\nORDER BY rank\n\n\"\"\")\n\nres.fetchall()\n\n[('The Philadelphia Eagles are set to have a great season in 2024',\n  -0.4925110954237839),\n ('Philadelphia went 1-7 to finish their season last year',\n  -1.03862660944206e-06),\n ('The Eagles signed superstar running back Saquon Barkley in the offseason',\n  -1e-06)]\n\n\nIn this way, I can use a set of keywords to perform a full text search of my database. In my fastbookRAG project, I’ll be applying the same approach: querying with a set of keywords a virtual table containing chunks of text from the fastbook chapters."
  },
  {
    "objectID": "posts/2024-07-31-bm25-cosine-similarity-demo/index.html#cosine-similarity",
    "href": "posts/2024-07-31-bm25-cosine-similarity-demo/index.html#cosine-similarity",
    "title": "BM25 and Cosine Similarity Demo",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nFor this demo, I intend to illustrate how cosine similarity can be used to capture differences in semantic meaning between texts.\nI’ll heavily reference Jeremy Howard’s Hacker’s Guide to LLM RAG example.\n\n!pip install sentence-transformers -Uqq\nfrom sentence_transformers import SentenceTransformer\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n\nHere’s my demo “database”—a few strings, with two of them intentionally similar (related to passing) to help illustrate the difference in semantic similarity with the query.\n\nchunks = [\n    'Jalen Hurts threw for 3858 yards in the 2023 season',\n    'Jalen Hurts had a combined 38 TDs (23 passing, 15 rushing) in the 2023 season',\n    \"Hurts' number one target in the 2023 season was A.J. Brown with 158 targets\"\n]\n\n\nq = \"How many passing yards did Jalen Hurts have in the 2023 season?\"\n\nI’ll embed the data and the query:\n\ndata_embs = emb_model.encode(chunks, convert_to_tensor=True)\nq_embs = emb_model.encode(q, convert_to_tensor=True)\ndata_embs.shape, q_embs.shape\n\n(torch.Size([3, 384]), torch.Size([384]))\n\n\nWith the power of broadcasting, I’ll calculate the cosine similarity (with dim=1) between the query and the entire dataset and sort it in descending order:\n\nimport torch\nimport torch.nn.functional as F\n\n\nres = F.cosine_similarity(q_embs, data_embs, dim=1).sort(descending=True)\nres[0]\n\ntensor([0.9298, 0.9263, 0.7729])\n\n\nThe record in my database that best matches the query is the correct answer:\n\nchunks[res[1][0]]\n\n'Jalen Hurts threw for 3858 yards in the 2023 season'\n\n\nAlthough the second-highest incorrect answer is close (0.9263 vs 0.9298) maybe because it contains the word “passing”?\n\nchunks[res[1][1]]\n\n'Jalen Hurts had a combined 38 TDs (23 passing, 15 rushing) in the 2023 season'\n\n\n\nHybrid Search\nI’ll now create an example demonstration of combining both full text search and cosine similarity to answer a query. I have intentionally chosen a set of records where full text search and cosine similarity individually do not provide the correct answer as the top result but both combined do. Reaching this result was considerably harder than I expected.\n\nchunks = [\n    'Jalen Hurts completed 81 passes to DeVonta Smith',\n    'A.J. Brown had 25 more total catches than DeVonta Smith',\n    \"DeVonta Smith caught 25 fewer passes than A.J. Brown\",\n    \"DeVonta Smith caught 7 touchdowns in the 2023 season\",\n    \"DeVonta Smith had 81 catches\",\n]\n\nI created a helper function to expedite my iterations as I tried at least a dozen different text combinations:\n\ndef prep_query(chunks):\n  insert_q = ''\n\n  for i, chunk in enumerate(chunks):\n    if i != len(chunks)-1:\n      insert_q += '(\"' + chunk + '\"),\\n'\n    else:\n      insert_q += '(\"' + chunk + '\");'\n\n  q = f\"\"\"INSERT INTO virtual_text_data(text)\n  VALUES\n  {insert_q}\n  \"\"\"\n\n  return q\n\n\nq = prep_query(chunks)\nprint(q)\n\nINSERT INTO virtual_text_data(text)\n  VALUES \n  (\"Jalen Hurts completed 81 passes to DeVonta Smith\"),\n(\"A.J. Brown had 25 more total catches than DeVonta Smith\"),\n(\"DeVonta Smith caught 25 fewer passes than A.J. Brown\"),\n(\"DeVonta Smith caught 7 touchdowns in the 2023 season\"),\n(\"DeVonta Smith had 81 catches\");\n  \n\n\n\nres = cur.execute('DELETE from virtual_text_data;')\nconn.commit()\n\nres = cur.execute(q)\nconn.commit()\n\nres = cur.execute(\"SELECT * from virtual_text_data\")\nres.fetchall()\n\n[('Jalen Hurts completed 81 passes to DeVonta Smith',),\n ('A.J. Brown had 25 more total catches than DeVonta Smith',),\n ('DeVonta Smith caught 25 fewer passes than A.J. Brown',),\n ('DeVonta Smith caught 7 touchdowns in the 2023 season',),\n ('DeVonta Smith had 81 catches',)]\n\n\nThe keywords I chose:\n'\"DeVonta Smith\" OR \"receptions\" OR \"catches\" OR \"total\"'\nare aiming to get a result that tells us how many total receptions (or catches) that DeVonta Smith had. The correct “document” from the “database” is:\n'DeVonta Smith had 81 catches'\nBM25’s top ranked record is not the correct one:\n'A.J. Brown had 25 more total catches than DeVonta Smith'\nI could be wrong, but BM25 might have chosen this record because it had the most keyword matches (“DeVonta Smith”, “total” and “catches”) while the correct record only has two keyword matches (“DeVonta Smith” and “catches”).\n\nres = cur.execute(\"\"\"\n\nSELECT *, rank\n  from virtual_text_data\nWHERE virtual_text_data MATCH '\"DeVonta Smith\" OR \"receptions\" OR \"catches\" OR \"total\"'\nORDER BY rank\n\n\"\"\")\n\nrts5_res = [el[0] for el in res.fetchall()]\nrts5_res\n\n['A.J. Brown had 25 more total catches than DeVonta Smith',\n 'DeVonta Smith had 81 catches',\n 'Jalen Hurts completed 81 passes to DeVonta Smith',\n 'DeVonta Smith caught 7 touchdowns in the 2023 season',\n 'DeVonta Smith caught 25 fewer passes than A.J. Brown']\n\n\nFor cosine similarity, I intentionally chose a slightly ambiguous query, expecting it to perform worse because of the indirect way of asking the desired question. The query is implicitly asking “How many of Jalen Hurts’ completions were to DeVonta Smith?”.\nI found that this caused the record with the largest cosine similarity to be incorrect:\n'DeVonta Smith caught 7 touchdowns in the 2023 season'\nI don’t have a good sense of why this string is closest to the query in embedding space.\n\nq = \"How many completions to DeVonta Smith?\"\n\n\ndata_embs = emb_model.encode(chunks, convert_to_tensor=True)\nq_embs = emb_model.encode(q, convert_to_tensor=True)\ndata_embs.shape, q_embs.shape\n\n(torch.Size([5, 384]), torch.Size([384]))\n\n\n\nres = F.cosine_similarity(q_embs, data_embs, dim=1).sort(descending=True)[1]\nres\n\ntensor([3, 0, 4, 2, 1])\n\n\n\nchunks[res[0]] # highest cosine similarity\n\n'DeVonta Smith caught 7 touchdowns in the 2023 season'\n\n\nSo far so good—both BM25 and cosine similarity have failed to return the correct answer to my query.\nI’ll combine the two methods in two ways that I’ve heard of before:\n\nPick the top n (in this case 3) BM25 ranked responses, calculate cosine similarity between the query and those responses and pick the record with the highest cosine similarity.\nWeigh the BM25 rank and cosine similarity and pick the record with the highest weighted score.\n\n\n\nCosine Similarity of Top-3 BM25 Records\nObviously, I chose this whole setup because I saw that while BM25 didn’t rank the correct record the highest, it did rank it second-highest, so it would be in my top-3. I was hoping that cosine similarity would take care of the rest, and it did.\n\ntop3 = rts5_res[:3]\ntop3\n\n['A.J. Brown had 25 more total catches than DeVonta Smith',\n 'DeVonta Smith had 81 catches',\n 'Jalen Hurts completed 81 passes to DeVonta Smith']\n\n\nWith this combined approach, cosine similarity correctly yields the correct response:\nJalen Hurts completed 81 passes to DeVonta Smith\nMy guess is that the cosine similarity between this record and the query is largest because of the words \"completed\" and \"completions\" being similar.\n\nq\n\n'How many completions to DeVonta Smith?'\n\n\n\ndata_embs = emb_model.encode(top3, convert_to_tensor=True)\nq_embs = emb_model.encode(q, convert_to_tensor=True)\ndata_embs.shape, q_embs.shape\n\n(torch.Size([3, 384]), torch.Size([384]))\n\n\n\nres = F.cosine_similarity(q_embs, data_embs, dim=1).sort(descending=True)[1]\ntop3[res[0]]\n\n'Jalen Hurts completed 81 passes to DeVonta Smith'\n\n\n\n\nWeighted Cosine Similarity and BM25 Score\nI was unable to achieve the correct response with a weighted average for this particular combination of query, BM25 results and cosine similarity results. The top-most ranked BM25 and cosine similarity scores (for the incorrect responses) were just too high to overcome.\nHere are the BM25 results:\n\nres = cur.execute(\"\"\"\n\nSELECT *, rank\n  from virtual_text_data\nWHERE virtual_text_data MATCH '\"DeVonta Smith\" OR \"receptions\" OR \"catches\" OR \"total\"'\nORDER BY rank\n\n\"\"\")\n\nrts5_res = res.fetchall()\nrts5_res = [(el[0], -1 * el[1]) for el in rts5_res]\nrts5_res\n\n[('A.J. Brown had 25 more total catches than DeVonta Smith',\n  1.2880369135898475),\n ('DeVonta Smith had 81 catches', 0.4059995941883513),\n ('Jalen Hurts completed 81 passes to DeVonta Smith', 1.029379760609358e-06),\n ('DeVonta Smith caught 7 touchdowns in the 2023 season',\n  9.813278008298757e-07),\n ('DeVonta Smith caught 25 fewer passes than A.J. Brown',\n  9.375619425173439e-07)]\n\n\nAnd the cosine similarity results:\n\ndata_embs = emb_model.encode(chunks, convert_to_tensor=True)\nq_embs = emb_model.encode(q, convert_to_tensor=True)\nvalues, indices = F.cosine_similarity(q_embs, data_embs, dim=1).sort(descending=True)\ncs_res = list(zip(values.tolist(), indices.tolist()))\ncs_res = [(chunks[el[1]], el[0]) for el in cs_res]\ncs_res\n\n[('DeVonta Smith caught 7 touchdowns in the 2023 season', 0.7455482482910156),\n ('Jalen Hurts completed 81 passes to DeVonta Smith', 0.7004185914993286),\n ('DeVonta Smith had 81 catches', 0.6995621919631958),\n ('DeVonta Smith caught 25 fewer passes than A.J. Brown', 0.6626170873641968),\n ('A.J. Brown had 25 more total catches than DeVonta Smith',\n  0.6136136651039124)]\n\n\nI’ll combine the two in a DataFrame:\n\nimport pandas as pd\n\ncombined_list = [('BM25',) + item for item in rts5_res] + [('CS',) + item for item in cs_res]\n\ndf = pd.DataFrame(combined_list, columns=['method', 'record', 'score'])\n\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      method\n      record\n      score\n    \n  \n  \n    \n      0\n      BM25\n      A.J. Brown had 25 more total catches than DeVo...\n      1.288037e+00\n    \n    \n      1\n      BM25\n      DeVonta Smith had 81 catches\n      4.059996e-01\n    \n    \n      2\n      BM25\n      Jalen Hurts completed 81 passes to DeVonta Smith\n      1.029380e-06\n    \n    \n      3\n      BM25\n      DeVonta Smith caught 7 touchdowns in the 2023 ...\n      9.813278e-07\n    \n    \n      4\n      BM25\n      DeVonta Smith caught 25 fewer passes than A.J....\n      9.375619e-07\n    \n    \n      5\n      CS\n      DeVonta Smith caught 7 touchdowns in the 2023 ...\n      7.455482e-01\n    \n    \n      6\n      CS\n      Jalen Hurts completed 81 passes to DeVonta Smith\n      7.004186e-01\n    \n    \n      7\n      CS\n      DeVonta Smith had 81 catches\n      6.995622e-01\n    \n    \n      8\n      CS\n      DeVonta Smith caught 25 fewer passes than A.J....\n      6.626171e-01\n    \n    \n      9\n      CS\n      A.J. Brown had 25 more total catches than DeVo...\n      6.136137e-01\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nI’ll normalize the BM25 scores and cosine similarity scores within their respective groups. After calculating the weighted average (with various weights) I couldn’t get the correct response to score the highest.\nThat’s not to say that a weighted average is never the right approach, but it doesn’t work well for this highly-curated toy example.\n\n# normalize scores within each method\ndef normalize(series):\n    return (series - series.min()) / (series.max() - series.min())\n\ndf['norm_score'] = df.groupby('method')['score'].transform(normalize)\n\n# pivot\npivot_df = df.pivot(index='record', columns='method', values='norm_score').reset_index()\npivot_df.columns.name = None\n\n# calculate weighted average (assuming equal weights of 0.5 for simplicity)\npivot_df['weighted_avg'] = pivot_df['BM25'] * 0.4 + pivot_df['CS'] * 0.6\n\n\npivot_df = pivot_df.sort_values('weighted_avg', ascending=False)\npivot_df\n\n\n\n  \n    \n\n\n  \n    \n      \n      record\n      BM25\n      CS\n      weighted_avg\n    \n  \n  \n    \n      2\n      DeVonta Smith caught 7 touchdowns in the 2023 ...\n      3.397875e-08\n      1.000000\n      0.600000\n    \n    \n      3\n      DeVonta Smith had 81 catches\n      3.152075e-01\n      0.651448\n      0.516952\n    \n    \n      0\n      A.J. Brown had 25 more total catches than DeVo...\n      1.000000e+00\n      0.000000\n      0.400000\n    \n    \n      4\n      Jalen Hurts completed 81 passes to DeVonta Smith\n      7.128513e-08\n      0.657939\n      0.394764\n    \n    \n      1\n      DeVonta Smith caught 25 fewer passes than A.J....\n      0.000000e+00\n      0.371422\n      0.222853"
  },
  {
    "objectID": "posts/2024-07-31-bm25-cosine-similarity-demo/index.html#final-thoughts",
    "href": "posts/2024-07-31-bm25-cosine-similarity-demo/index.html#final-thoughts",
    "title": "BM25 and Cosine Similarity Demo",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nEven a toy example had me sweating at times! And I was clearly gaming the setup to get as close to my desired result as possible. As I move forward with experimenting with BM25 and cosine similarity and eventually combining them with an LLM to create a RAG pipeline for a much more complex dataset (multiple chapters from the fastai textbook), I’ll make sure that I respect the nuance involved and plan the project accordingly.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-7/index.html",
    "href": "posts/2024-02-05-paddy-part-7/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 7",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8\nPart 2: Live Coding 9\nPart 3: Live Coding 10\nPart 4: Live Coding 11\nPart 5: Live Coding 12\nPart 6: Live Coding 13\nPart 7: Improving My Large Ensemble, Part 1 (You are here)\nPart 8: Improving My Large Ensemble, Part 2\n\n\nfrom google.colab import userdata\ncreds = userdata.get('kaggle')\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\n\n!pip install -qq timm==0.6.13\nimport timm\ntimm.__version__\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/549.1 kB ? eta -:--:--     ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/549.1 kB 2.8 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 460.8/549.1 kB 7.0 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 549.1/549.1 kB 6.5 MB/s eta 0:00:00\n\n\n'0.6.13'\n\n\n\nimport zipfile,kaggle\n\npath = Path('paddy-disease-classification')\nif not path.exists():\n  kaggle.api.competition_download_cli(str(path))\n  zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading paddy-disease-classification.zip to /content\n\n\n100%|██████████| 1.02G/1.02G [00:35<00:00, 30.9MB/s]\n\n\n\n\n\n\nfrom fastai.vision.all import *\n\n\npath.ls()\n\n(#4) [Path('paddy-disease-classification/train.csv'),Path('paddy-disease-classification/sample_submission.csv'),Path('paddy-disease-classification/test_images'),Path('paddy-disease-classification/train_images')]\n\n\n\ntrn_path = path/'train_images'\n\n\n# run this once and re-use for all trainings\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\n\n\ntst_files[:5]\n\n(#5) [Path('paddy-disease-classification/test_images/200001.jpg'),Path('paddy-disease-classification/test_images/200002.jpg'),Path('paddy-disease-classification/test_images/200003.jpg'),Path('paddy-disease-classification/test_images/200004.jpg'),Path('paddy-disease-classification/test_images/200005.jpg')]"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-7/index.html#using-lr_find-for-large-models",
    "href": "posts/2024-02-05-paddy-part-7/index.html#using-lr_find-for-large-models",
    "title": "Paddy Doctor Kaggle Competition - Part 7",
    "section": "Using lr_find for Large Models",
    "text": "Using lr_find for Large Models\nOne of the students in Live coding 12 faced the same problem as I did: their large model ensemble submission did not improve their Kaggle scores. Jeremy said this is probably because they ran some incorrect code somewhere, and suggested (among other things), to see if using a learning rate from lr_find improved their ensemble. This is what I’ll try next to improve my Kaggle score. If it doesn’t work, I’ll reference Jeremy’s Road to the Top notebook corresponding to large model training, and see what I coded wrong.\n\nkwargs = {'bs': 16}\ncbs = GradientAccumulation(2)\n\n\narch = 'swinv2_large_window12_192_22k'\n\nI wasn’t getting a very promising learning rate result the first few times I rant it. The lr_find plot didn’t have a section that was steep and somewhat linear, so I’ll run lr_find a few times here to show what I was seeing:\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=192, min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=192, min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=192, min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0012022644514217973)\n\n\n\n\n\nThe suggested learning rate (~0.001) is always conservative, so I’ll pick something larger for the swinv2_large_window12_192_22k architecture: 0.005.\n\narch = 'convnext_large_in22k'\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize((640,480)),\n    batch_tfms=aug_transforms(size=(288,224), min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_large_22k_224.pth\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize((640,480)),\n    batch_tfms=aug_transforms(size=(288,224), min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize((640,480)),\n    batch_tfms=aug_transforms(size=(288,224), min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\nFor convnext_large_in22k, I’m tempted to use 0.02, but in the last run of lr_find the loss just starts to inflect upwards at this learning rate. I’ll go with 0.015.\n\narch = 'vit_large_patch16_224'\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(480),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(480),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path,\n    valid_pct=0.2,\n    item_tfms=Resize(480),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75),\n    **kwargs)\n\nlearn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\nlearn.model_dir = '/tmp/model'\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00363078061491251)\n\n\n\n\n\nFor vit_large_patch16_224, I’ll use a learning rate of 0.005. I was tempted to use 0.01 but the loss starts to enter instability in the second lr_find run.\nHere is a summary of learning rates I’ll use for each architecture:\n\n\n\nArchitecture\nLearning Rate\n\n\n\n\nswinv2_large_window12_192_22k\n0.005\n\n\nconvnext_large_in22k\n0.015\n\n\nvit_large_patch16_224\n0.005\n\n\n\n\ntta_res = []\n\nNote that my train function now has the parameters lr and n_epochs to specify learning rate and number of training epochs, respectively.\n\ndef train(arch, item, batch, lr, n_epochs=24, accum=False):\n    kwargs = {'bs': 16} if accum else {}\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item, batch_tfms=batch, **kwargs)\n    cbs = GradientAccumulation(2) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    learn.fine_tune(n_epochs, lr)\n\n    # view losses\n    learn.recorder.plot_loss()\n\n    # TTA predictions using test dataset\n    tst_dl = dls.test_dl(tst_files)\n    tta_res.append(learn.tta(dl=tst_dl))\n\n    # Return error rate using validation dataset\n    print(error_rate(*learn.tta(dl=dls.valid)))\n    return learn, dls\n\n\ndef prep_submission(fn, tta_res):\n    # pull out predictions from tta_res list\n    tta_prs = first(zip(*tta_res))\n\n    # convert tta_res from list to stacked tensor\n    t_tta = torch.stack(tta_prs)\n\n    # take mean of each item's predictions\n    avg_pr = t_tta.mean(0)\n\n    # get the index (class) of the maximum prediction for each item\n    idxs = avg_pr.argmax(dim=1)\n\n    # create DataLoaders to get its vocab\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(224))\n\n    # convert indexes to vocab strings\n    mapping = dict(enumerate(dls.vocab))\n\n    # add vocab strings to sample submission file and export to CSV\n    ss = pd.read_csv(path/'sample_submission.csv')\n    results = pd.Series(idxs.numpy(), name='idxs').map(mapping)\n    ss.label = results\n    ss.to_csv(fn, index=False)\n\n\narch = 'swinv2_large_window12_192_22k'\n\n\ntrain(\n    arch,\n    item=Resize(480, method='squish'),\n    batch=aug_transforms(size=192, min_scale=0.75),\n    lr=0.005,\n    n_epochs=24,\n    accum=True)\n\n/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth\" to /root/.cache/torch/hub/checkpoints/swinv2_large_patch4_window12_192_22k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.070781\n      0.601547\n      0.201346\n      04:03\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      70.83% [17/24 1:33:49<38:38]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.430227\n      0.230742\n      0.080250\n      05:29\n    \n    \n      1\n      0.336965\n      0.175396\n      0.056704\n      05:29\n    \n    \n      2\n      0.298998\n      0.187081\n      0.057665\n      05:30\n    \n    \n      3\n      0.307775\n      0.166967\n      0.055742\n      05:30\n    \n    \n      4\n      0.274865\n      0.170166\n      0.045651\n      05:34\n    \n    \n      5\n      0.314666\n      0.183352\n      0.049976\n      05:31\n    \n    \n      6\n      0.225893\n      0.139452\n      0.038924\n      05:31\n    \n    \n      7\n      0.209061\n      0.147105\n      0.039885\n      05:31\n    \n    \n      8\n      0.166353\n      0.107878\n      0.027871\n      05:31\n    \n    \n      9\n      0.220547\n      0.132710\n      0.033638\n      05:29\n    \n    \n      10\n      0.141531\n      0.133753\n      0.039404\n      05:31\n    \n    \n      11\n      0.076975\n      0.186801\n      0.036040\n      05:30\n    \n    \n      12\n      0.070114\n      0.119698\n      0.026910\n      05:30\n    \n    \n      13\n      0.086218\n      0.103130\n      0.025469\n      05:30\n    \n    \n      14\n      0.059969\n      0.075689\n      0.019702\n      05:31\n    \n    \n      15\n      0.064261\n      0.080532\n      0.018260\n      05:33\n    \n    \n      16\n      0.048361\n      0.083898\n      0.019222\n      05:33\n    \n  \n\n\n    \n      \n      15.58% [81/520 00:47<04:18 0.0468]\n    \n    \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.430227\n      0.230742\n      0.080250\n      05:29\n    \n    \n      1\n      0.336965\n      0.175396\n      0.056704\n      05:29\n    \n    \n      2\n      0.298998\n      0.187081\n      0.057665\n      05:30\n    \n    \n      3\n      0.307775\n      0.166967\n      0.055742\n      05:30\n    \n    \n      4\n      0.274865\n      0.170166\n      0.045651\n      05:34\n    \n    \n      5\n      0.314666\n      0.183352\n      0.049976\n      05:31\n    \n    \n      6\n      0.225893\n      0.139452\n      0.038924\n      05:31\n    \n    \n      7\n      0.209061\n      0.147105\n      0.039885\n      05:31\n    \n    \n      8\n      0.166353\n      0.107878\n      0.027871\n      05:31\n    \n    \n      9\n      0.220547\n      0.132710\n      0.033638\n      05:29\n    \n    \n      10\n      0.141531\n      0.133753\n      0.039404\n      05:31\n    \n    \n      11\n      0.076975\n      0.186801\n      0.036040\n      05:30\n    \n    \n      12\n      0.070114\n      0.119698\n      0.026910\n      05:30\n    \n    \n      13\n      0.086218\n      0.103130\n      0.025469\n      05:30\n    \n    \n      14\n      0.059969\n      0.075689\n      0.019702\n      05:31\n    \n    \n      15\n      0.064261\n      0.080532\n      0.018260\n      05:33\n    \n    \n      16\n      0.048361\n      0.083898\n      0.019222\n      05:33\n    \n    \n      17\n      0.031970\n      0.089230\n      0.019222\n      05:32\n    \n    \n      18\n      0.031545\n      0.079374\n      0.019222\n      05:30\n    \n    \n      19\n      0.026196\n      0.078567\n      0.018260\n      05:32\n    \n    \n      20\n      0.016311\n      0.070167\n      0.018260\n      05:31\n    \n    \n      21\n      0.022081\n      0.068992\n      0.016338\n      05:31\n    \n    \n      22\n      0.024040\n      0.073405\n      0.017780\n      05:32\n    \n    \n      23\n      0.016780\n      0.072001\n      0.017299\n      05:32\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0178)\n\n\n(<fastai.learner.Learner at 0x782e6b00a860>,\n <fastai.data.core.DataLoaders at 0x782e6b00a740>)\n\n\n\n\n\nThe output is unfortunately a bit messy, but here are my observations:\n\nThe TTA error rate is 0.0178 and the final epoch’s validation error rate is a bit lower. This is a good sign as this shows an improvement in error rate compared to the error rate (0.01862) when using a learning rate of 0.01. However, the improvement is only 5%, which seems unremarkable.\nThe training and validation losses look okay. The both decrease over epochs, and the validation loss does not seem to be increasing significantly at the end, so the model is not overfitting.\n\nI’m not convinced that changing the learning rate from 0.01 to 0.005 has significantly improved my model, but I will go ahead and train the other two models in this ensemble and submit the predictions to Kaggle. If the private score does not improve, I’ll reference Jeremy’s “Road to the Top” notebook series to see how he trained his large models.\n\nlen(tta_res), len(tta_res[0][0])\n\n(1, 3469)\n\n\n\narch = 'convnext_large_in22k'\n\n\nlearn, dls = train(\n    arch,\n    item=Resize((640,480)),\n    batch=aug_transforms(size=(288,224), min_scale=0.75),\n    lr=0.015,\n    n_epochs=24,\n    accum=True)\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_large_22k_224.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.350656\n      0.639466\n      0.192696\n      03:17\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.393383\n      0.226754\n      0.069678\n      05:01\n    \n    \n      1\n      0.278609\n      0.194963\n      0.054301\n      05:00\n    \n    \n      2\n      0.258872\n      0.242242\n      0.069678\n      04:59\n    \n    \n      3\n      0.272949\n      0.242155\n      0.070639\n      05:01\n    \n    \n      4\n      0.250942\n      0.288342\n      0.073042\n      04:59\n    \n    \n      5\n      0.256650\n      0.224654\n      0.060067\n      05:00\n    \n    \n      6\n      0.215623\n      0.242569\n      0.054781\n      04:59\n    \n    \n      7\n      0.204849\n      0.183193\n      0.049495\n      04:59\n    \n    \n      8\n      0.175016\n      0.224864\n      0.044210\n      04:58\n    \n    \n      9\n      0.179674\n      0.374226\n      0.078328\n      04:58\n    \n    \n      10\n      0.105390\n      0.209836\n      0.037482\n      04:57\n    \n    \n      11\n      0.088776\n      0.209220\n      0.039404\n      04:58\n    \n    \n      12\n      0.071399\n      0.180012\n      0.034118\n      04:57\n    \n    \n      13\n      0.066381\n      0.153438\n      0.030754\n      04:57\n    \n    \n      14\n      0.062488\n      0.146692\n      0.028832\n      04:58\n    \n    \n      15\n      0.062825\n      0.142316\n      0.026430\n      04:57\n    \n    \n      16\n      0.044528\n      0.153894\n      0.025949\n      04:57\n    \n    \n      17\n      0.043213\n      0.144824\n      0.024027\n      04:56\n    \n    \n      18\n      0.018004\n      0.145353\n      0.022585\n      04:57\n    \n    \n      19\n      0.024245\n      0.138911\n      0.021624\n      04:59\n    \n    \n      20\n      0.009057\n      0.139719\n      0.022105\n      04:57\n    \n    \n      21\n      0.015471\n      0.134287\n      0.020663\n      04:56\n    \n    \n      22\n      0.020319\n      0.135233\n      0.021624\n      04:56\n    \n    \n      23\n      0.011382\n      0.138716\n      0.019702\n      04:57\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n    \n      \n      25.00% [1/4 00:24<01:12]\n    \n    \n\n\n    \n      \n      58.78% [77/131 00:14<00:09 0.0114]\n    \n    \n\n\n\n\n\nTensorBase(0.0197)\n\n\n\n\n\nHere are my observations:\n\nThe larger learning rate of 0.015, although reasonably estimated from the lr_find plot, has decreased the performance of the model. The final validation error rate with lr=0.015 is 0.019702, which is larger than the final validation error rate with lr=0.01 (0.014416). I’ll stick with this for now—perhaps this difference in validation error rate can be attributed to the difference in validation sets. Again, if my large model ensemble does not result in an improved Kaggle score, I’ll reference Jeremy’s solution.\nThe training and validation losses are generally decreasing over the epochs, and are not showing signs of overfitting (i.e., validation loss increasing).\n\nI’ll train the final model next.\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0])\n\n(2, 3469, 3469)\n\n\n\n# save_pickle(\"colab_tta_res.pkl\", tta_res)\n\n\n# tta_res = load_pickle(\"colab_tta_res.pkl\")\n\n\narch = 'vit_large_patch16_224'\n\n\nlearn, dls = train(\n    arch,\n    item=Resize(480),\n    batch=aug_transforms(size=224, min_scale=0.75),\n    lr=0.005,\n    n_epochs=24,\n    accum=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.163018\n      0.609755\n      0.195579\n      04:13\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.495968\n      0.220330\n      0.067756\n      06:21\n    \n    \n      1\n      0.365600\n      0.222172\n      0.073522\n      06:22\n    \n    \n      2\n      0.306510\n      0.231306\n      0.064873\n      06:22\n    \n    \n      3\n      0.312357\n      0.164647\n      0.044690\n      06:21\n    \n    \n      4\n      0.326301\n      0.223919\n      0.060548\n      06:21\n    \n    \n      5\n      0.291703\n      0.213934\n      0.057665\n      06:21\n    \n    \n      6\n      0.221403\n      0.221377\n      0.055262\n      06:18\n    \n    \n      7\n      0.234319\n      0.178601\n      0.044690\n      06:19\n    \n    \n      8\n      0.171401\n      0.246879\n      0.049976\n      06:18\n    \n    \n      9\n      0.194408\n      0.148840\n      0.035079\n      06:18\n    \n    \n      10\n      0.145152\n      0.149591\n      0.041326\n      06:17\n    \n    \n      11\n      0.113229\n      0.159672\n      0.032196\n      06:17\n    \n    \n      12\n      0.085892\n      0.125524\n      0.020663\n      06:17\n    \n    \n      13\n      0.053963\n      0.106966\n      0.023546\n      06:17\n    \n    \n      14\n      0.110243\n      0.099779\n      0.024507\n      06:16\n    \n    \n      15\n      0.048981\n      0.129755\n      0.031235\n      06:18\n    \n    \n      16\n      0.055013\n      0.106256\n      0.017299\n      06:17\n    \n    \n      17\n      0.043242\n      0.111034\n      0.021624\n      06:17\n    \n    \n      18\n      0.034097\n      0.097368\n      0.016819\n      06:16\n    \n    \n      19\n      0.035058\n      0.098730\n      0.017780\n      06:17\n    \n    \n      20\n      0.030310\n      0.098341\n      0.014897\n      06:17\n    \n    \n      21\n      0.019945\n      0.096790\n      0.013936\n      06:17\n    \n    \n      22\n      0.010267\n      0.095050\n      0.014416\n      06:18\n    \n    \n      23\n      0.015022\n      0.094428\n      0.014416\n      06:19\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n    \n      \n      25.00% [1/4 00:45<02:16]\n    \n    \n\n\n    \n      \n      18.43% [40/217 00:08<00:38 0.0150]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0139)\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0])\n\n(3, 3469, 3469, 3469)\n\n\nObservations about this training:\n\nThe final validation error rate for lr=0.005 was 0.014416, which is about 35% less than when lr=0.01 (0.02215). That’s a good sign that this smaller learning rate was a better choice.\nAs with the other models, the training and validation losses generally decrease over epochs.\n\n\n# save_pickle(\"final_tta_res.pkl\", tta_res)\n\n.\n\n# tta_res = load_pickle(\"final_tta_res.pkl\")\n\nI’ll make a new submission with these predictions to Kaggle and see how it scores:\n\nprep_submission(\"subm.csv\", tta_res)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThe submission’s first few values are the same as before, so that’s a good sign.\nThis submission’s scores were:\n\nPrivate: 0.98387 (previous best: 0.98617)\nPublic: 0.98577 (previous best: 0.98577)\n\nThe submissions with the best private score are still my small ensemble of these architectures that were trained for 12 epochs, and a large ensemble with the vit predictions with 3x the weight of the others.\nI’ll triple the weight of the large vit model predictions and submit it to Kaggle to see if it improves the private score:\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0])\n\n(3, 3469, 3469, 3469)\n\n\n\ntta_res += 2 * [tta_res[2]]\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0]), len(tta_res[3][0]), len(tta_res[4][0])\n\n(5, 3469, 3469, 3469, 3469, 3469)\n\n\n\nprep_submission(\"subm.csv\", tta_res)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis submission matched my previous best Private score, and improved upon my previous best Public score:\n\nPrivate score: 0.98617\nPublic score: 0.98654\n\nIt’s interesting to me that I can’t break this ceiling of 0.98617! I will now reference Jeremy’s large model ensemble and see if I can improve my score by following his methodology in my next blog post."
  },
  {
    "objectID": "posts/2024-07-14-practical-deep-learning-for-coders-part-1/index.html",
    "href": "posts/2024-07-14-practical-deep-learning-for-coders-part-1/index.html",
    "title": "Practical Deep Learnings For Coders - Part 1 Notes and Examples",
    "section": "",
    "text": "Vishal Bakshi\nThis notebook contains my notes (of course videos, example notebooks and book chapters) and exercises of Part 1 of the course Practical Deep Learning for Coders.\n\n\n\n\nThe first thing I did was to run through the lesson 1 notebook from start to finish. In this notebook, they download training and validation images of birds and forests then train an image classifier with 100% accuracy in identifying images of birds.\nThe first exercise is for us to create our own image classifier with our own image searches. I’ll create a classifier which accurately predicts an image of an alligator.\nI’ll start by using their example code for getting images using DuckDuckGo image search:\n\n# It's a good idea to ensure you're running the latest version of any libraries you need.\n# `!pip install -Uqq <libraries>` upgrades to the latest version of <libraries>\n# NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities\n!pip install -Uqq fastai fastbook duckduckgo_search timm\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nThe search_images function takes a search term and max_images maximum number of images value. It prints out a line of text that it’s \"Searching for\" the term and returns an L object with the image URL.\nThe ddg_images function returns a list of JSON objects containing the title, image URL, thumbnail URL, height, width and source of the image.\n\nsearch_object = ddg_images('alligator', max_results=1)\nsearch_object\n\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n[{'title': 'The Creature Feature: 10 Fun Facts About the American Alligator | WIRED',\n  'image': 'https://www.wired.com/wp-content/uploads/2015/03/Gator-2.jpg',\n  'thumbnail': 'https://tse4.mm.bing.net/th?id=OIP.FS96VErnOXAGSWU092I_DQHaE8&pid=Api',\n  'url': 'https://www.wired.com/2015/03/creature-feature-10-fun-facts-american-alligator/',\n  'height': 3456,\n  'width': 5184,\n  'source': 'Bing'}]\n\n\nWrapping this list in L object and calling .itemgot('image') on it extracts URL value associated with the image key in the JSON object.\n\nL(search_object).itemgot('image')\n\n(#1) ['https://www.wired.com/wp-content/uploads/2015/03/Gator-2.jpg']\n\n\nNext, they provide some code to download the image to a destination filename and view the image:\n\nurls = search_images('alligator', max_images=1)\n\nfrom fastdownload import download_url\ndest = 'alligator.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\nSearching for 'alligator'\n\n\n\n\n\nFor my not-alligator images, I’ll use images of a swamp.\n\ndownload_url(search_images('swamp photos', max_images=1)[0], 'swamp.jpg', show_progress=False)\nImage.open('swamp.jpg').to_thumb(256,256)\n\nSearching for 'swamp photos'\n\n\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n\n\n\nIn the following code, I’ll search for both terms, alligator and swamp and store the images in alligator_or_not/alligator and alligator_or_not/swamp paths, respectively.\nThe parents=TRUE argument creates any intermediate parent directories that don’t exist (in this case, the alligator_or_not directory). The exist_ok=TRUE argument suppresses the FileExistsError and does nothing.\n\nsearches = 'swamp','alligator'\npath = Path('alligator_or_not')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'swamp photo'\nSearching for 'swamp sun photo'\nSearching for 'swamp shade photo'\nSearching for 'alligator photo'\nSearching for 'alligator sun photo'\nSearching for 'alligator shade photo'\n\n\nNext, I’ll train my model using the code they have provided.\nThe get_image_files function is a fastai function which takes a Path object and returns an L object with paths to the image files.\n\ntype(get_image_files(path))\n\nfastcore.foundation.L\n\n\n\nget_image_files(path)\n\n(#349) [Path('alligator_or_not/swamp/1b3c3a61-0f7f-4dc2-a704-38202d593207.jpg'),Path('alligator_or_not/swamp/9c9141f2-024c-4e26-b343-c1ca1672fde8.jpeg'),Path('alligator_or_not/swamp/1340dd85-5d98-428e-a861-d522c786c3d7.jpg'),Path('alligator_or_not/swamp/2d3f91dc-cc5f-499b-bec6-7fa0e938fb13.jpg'),Path('alligator_or_not/swamp/84afd585-ce46-4016-9a09-bd861a5615db.jpg'),Path('alligator_or_not/swamp/6222f0b6-1f5f-43ec-b561-8e5763a91c61.jpg'),Path('alligator_or_not/swamp/a71c8dcb-7bbb-4dba-8ae6-8a780d5c27c6.jpg'),Path('alligator_or_not/swamp/bbd1a832-a901-4e8f-8724-feac35fa8dcb.jpg'),Path('alligator_or_not/swamp/45b358b3-1a12-41d4-8972-8fa98b2baa52.jpg'),Path('alligator_or_not/swamp/cf664509-8eb6-42c8-9177-c17f48bc026b.jpg')...]\n\n\nThe fastai parent_label function takes a Path object and returns a string of the file’s parent folder name.\n\nparent_label(Path('alligator_or_not/swamp/18b55d4f-3d3b-4013-822b-724489a23f01.jpg'))\n\n'swamp'\n\n\nSome image files that are downloaded may be corrupted, so they have provided a verify_images function to find images that can’t be opened. Those images are then removed (unlinked) from the path.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\n\nfailed\n\n(#1) [Path('alligator_or_not/alligator/1eb55508-274b-4e23-a6ae-dbbf1943a9d1.jpg')]\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\nI’ll train the model using their code which uses the resnet18 image classification model, and fine_tunes it for 3 epochs.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.690250\n      0.171598\n      0.043478\n      00:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.127188\n      0.001747\n      0.000000\n      00:02\n    \n    \n      1\n      0.067970\n      0.006409\n      0.000000\n      00:02\n    \n    \n      2\n      0.056453\n      0.004981\n      0.000000\n      00:02\n    \n  \n\n\n\nThe accuracy is 100%.\nNext, I’ll test the model as they’ve done in the lesson.\n\nPILImage.create('alligator.jpg').to_thumb(256,256)\n\n\n\n\n\nis_alligator,_,probs = learn.predict(PILImage.create('alligator.jpg'))\nprint(f\"This is an: {is_alligator}.\")\nprint(f\"Probability it's an alligator: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is an: alligator.\nProbability it's an alligator: 1.0000\n\n\n\n\n\nIn this section, I’ll take notes while I watch the lesson 1 video.\n\nThis is the fifth version of the course!\nWhat seemed impossible in 2015 (image recognition of a bird) is now free and something we can build in 2 minutes.\nAll models need numbers as their inputs. Images are already stored as numbers in computers. [PixSpy] allows you to (among other things) view the color of each pixel in an image file.\nA DataBlock gives fastai all the information it needs to create a computer vision model.\nCreating really interesting, real, working programs with deep learning is something that doesn’t take a lot of code, math, or more than a laptop computer. It’s pretty accessible.\nDeep Learning models are doing things that very few, if any of us, believed would be possible to do by computers in our lifetime.\nSee the Practical Data Ethics course as well.\nMeta Learning: How To Learn Deep Learning And Thrive In The Digital World.\nBooks on learning/education:\n\nMathematician’s Lament by Paul Lockhart\nMaking Learning Whole by David Perkins\n\nWhy are we able to create a bird-recognizer in a minute or two? And why couldn’t we do it before?\n\n2012: Project looking at 5-year survival of breast cancer patients, pre-deep learning approach\n\nAssembled a team to build ideas for thousands of features that required a lot of expertise, took years.\nThey fed these features into a logistic regression model to predict survival.\nNeural networks don’t require us to build these features, they build them for us.\n\n2015: Matthew D. Zeiler and Rob Fergus looked inside a neural network to see what it had learned.\n\nWe don’t give it features, we ask it to learn features.\nThe neural net is the basic function used in deep learning.\nYou start with a random neural network, feed it examples and you have it learn to recognize things.\nThe deeper you get, the more sophisticated the features it can find are.\nWhat we’re going to learn is how neural networks do this automatically.\nThis is the key difference in why we can now do things that we couldn’t previously conceive of as possible.\n\n\nAn image recognizer can also be used to classify sounds (pictures of waveforms).\nTurning time series into pictures for image classification.\nfastai is built on top of PyTorch.\n!pip install -Uqq fastai to update.\nAlways view your data at every step of building a model.\nFor computer vision algorithms you don’t need particularly big images.\nFor big images, most of the time is taken up opening it, the neural net on the GPU is must faster.\nThe main thing you’re going to try and figure out is how do I get this data into my model?\nDataBlock\n\nblocks=(ImageBlock, CategoryBlock): ImageBlock is the type of input to the model, CategoryBlock is the type of model output\nget_image_files(path) returns a list of all image files in a path.\nIt’s critical that you put aside some data for testing the accuracy of your model (validation set) with something like RandomSplitter for the splitter parameter.\nget_y tells fastai how to get the correct label for the photo.\nMost computer vision architectures need all of your inputs to be the same size, using Resize (either crop out a piece in the middle or squish the image) for the parameter item_tfms.\nDataLoaders contains iterators that PyTorch can run through to grab batches of your data to feed the training algorithm.\nshow_batch shows you a batch of input/label pairs.\nA Learner combines a model (the actual neural network that we are training) and the data we use to train it with.\nPyTorch Image Models (timm).\nresnet has already been trained to recognize over 1 million images of over 1000 different types. fastai downloads this so you can start with a neural network that can do a lot.\nfine_tune takes those pretrained weights downloaded for you and adjusts them in a carefully controlled way to teach the model differences between your dataset and what it was originally trained for.\nYou pass .predict an image, which is how you would deploy your model, returns whether it’s a bird or not as a string, integer and probability of whether it’s a bird (in this example).\n\n\nIn the code blocks below, I’ll train the different types of models presented in the video lesson.\n\n\n\nfrom fastai.vision.all import *\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs = 8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.454409\n      3.015761\n      00:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.928762\n      1.719756\n      00:02\n    \n    \n      1\n      1.649520\n      1.394089\n      00:02\n    \n    \n      2\n      1.533350\n      1.344445\n      00:02\n    \n    \n      3\n      1.414438\n      1.279674\n      00:02\n    \n    \n      4\n      1.291168\n      1.063977\n      00:02\n    \n    \n      5\n      1.174492\n      0.980055\n      00:02\n    \n    \n      6\n      1.073124\n      0.931532\n      00:02\n    \n    \n      7\n      0.992161\n      0.922516\n      00:02\n    \n  \n\n\n\n\nlearn.show_results(max_n=3, figsize=(7,8))\n\n\n\n\n\n\n\n\n\n\n\nIt’s amazing how many it’s getting correct because this model was trained in about 24 seconds using a tiny amount of data.\nI’ll take a look at the codes out of curiousity, which is an array of string elements describing different objects in view.\n\nnp.loadtxt(path/'codes.txt', dtype=str)\n\narray(['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'Tunnel',\n       'VegetationMisc', 'Void', 'Wall'], dtype='<U17')\n\n\n\n\n\n\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ADULT_SAMPLE)\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names='salary',\n                                  cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                                               'relationship', 'race'],\n                                  cont_names = ['age', 'fnlwgt', 'education-num'],\n                                  procs = [Categorify, FillMissing, Normalize])\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      workclass\n      education\n      marital-status\n      occupation\n      relationship\n      race\n      education-num_na\n      age\n      fnlwgt\n      education-num\n      salary\n    \n  \n  \n    \n      0\n      State-gov\n      Some-college\n      Divorced\n      Adm-clerical\n      Own-child\n      White\n      False\n      42.0\n      138162.000499\n      10.0\n      <50k\n    \n    \n      1\n      Private\n      HS-grad\n      Married-civ-spouse\n      Other-service\n      Husband\n      Asian-Pac-Islander\n      False\n      40.0\n      73025.003080\n      9.0\n      <50k\n    \n    \n      2\n      Private\n      Assoc-voc\n      Married-civ-spouse\n      Prof-specialty\n      Wife\n      White\n      False\n      36.0\n      163396.000571\n      11.0\n      >=50k\n    \n    \n      3\n      Private\n      HS-grad\n      Never-married\n      Sales\n      Own-child\n      White\n      False\n      18.0\n      110141.999831\n      9.0\n      <50k\n    \n    \n      4\n      Self-emp-not-inc\n      12th\n      Divorced\n      Other-service\n      Unmarried\n      White\n      False\n      28.0\n      33035.002716\n      8.0\n      <50k\n    \n    \n      5\n      ?\n      7th-8th\n      Separated\n      ?\n      Own-child\n      White\n      False\n      50.0\n      346013.994175\n      4.0\n      <50k\n    \n    \n      6\n      Self-emp-inc\n      HS-grad\n      Never-married\n      Farming-fishing\n      Not-in-family\n      White\n      False\n      36.0\n      37018.999571\n      9.0\n      <50k\n    \n    \n      7\n      State-gov\n      Masters\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      White\n      False\n      37.0\n      239409.001471\n      14.0\n      >=50k\n    \n    \n      8\n      Self-emp-not-inc\n      Doctorate\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      White\n      False\n      50.0\n      167728.000009\n      16.0\n      >=50k\n    \n    \n      9\n      Private\n      HS-grad\n      Married-civ-spouse\n      Tech-support\n      Husband\n      White\n      False\n      38.0\n      247111.001513\n      9.0\n      >=50k\n    \n  \n\n\n\nFor tabular models, there’s not generally going to be a pretrained model that already does something like what you want because every table of data is very different, so generally it doesn’t make too much sense to fine_tune a tabular model.\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.373780\n      0.365976\n      0.832770\n      00:06\n    \n    \n      1\n      0.356514\n      0.358780\n      0.833999\n      00:05\n    \n  \n\n\n\n\n\n\nThe basis of most recommendation systems.\n\nfrom fastai.collab import *\npath = untar_data(URLs.ML_SAMPLE)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n    \n  \n  \n    \n      0\n      457\n      457\n      3.0\n    \n    \n      1\n      407\n      2959\n      5.0\n    \n    \n      2\n      294\n      356\n      4.0\n    \n    \n      3\n      78\n      356\n      5.0\n    \n    \n      4\n      596\n      3578\n      4.5\n    \n    \n      5\n      547\n      541\n      3.5\n    \n    \n      6\n      105\n      1193\n      4.0\n    \n    \n      7\n      176\n      4993\n      4.5\n    \n    \n      8\n      430\n      1214\n      4.0\n    \n    \n      9\n      607\n      858\n      4.5\n    \n  \n\n\n\nThere’s actually no pretrained collaborative filtering model so we could use fit_one_cycle but fine_tune works here as well.\n\nlearn = collab_learner(dls, y_range=(0.5, 5.5))\nlearn.fine_tune(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.498450\n      1.417215\n      00:00\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.375927\n      1.357755\n      00:00\n    \n    \n      1\n      1.274781\n      1.176326\n      00:00\n    \n    \n      2\n      1.033917\n      0.870168\n      00:00\n    \n    \n      3\n      0.810119\n      0.719341\n      00:00\n    \n    \n      4\n      0.704180\n      0.679201\n      00:00\n    \n    \n      5\n      0.640635\n      0.667121\n      00:00\n    \n    \n      6\n      0.623741\n      0.661391\n      00:00\n    \n    \n      7\n      0.620811\n      0.657624\n      00:00\n    \n    \n      8\n      0.606947\n      0.656678\n      00:00\n    \n    \n      9\n      0.605081\n      0.656613\n      00:00\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      15.0\n      35.0\n      4.5\n      3.886339\n    \n    \n      1\n      68.0\n      64.0\n      5.0\n      3.822170\n    \n    \n      2\n      62.0\n      33.0\n      4.0\n      3.088149\n    \n    \n      3\n      39.0\n      91.0\n      4.0\n      3.788227\n    \n    \n      4\n      37.0\n      7.0\n      5.0\n      4.434169\n    \n    \n      5\n      38.0\n      98.0\n      3.5\n      4.380877\n    \n    \n      6\n      3.0\n      25.0\n      3.0\n      3.443295\n    \n    \n      7\n      23.0\n      13.0\n      2.0\n      3.220192\n    \n    \n      8\n      15.0\n      7.0\n      4.0\n      4.306846\n    \n  \n\n\n\nNote: RISE turnes your notebook into a presentation.\nGenerally speaking, if it’s something that a human can do reasonably quickly, even an expert human (like look at a Go board and decide if it’s a good board or not) then that’s probably something that deep learning will probably be good at. If it’s something that takes logical thought process over time, particularly if it’s not based on much data, deep learning probably won’t do that well.\nThe first neural network was built in 1957. The basic ideas have not changed much at all.\nWhat’s going on in these models?\n\nArthur Samuel in late 1950s invented Machine Learning.\nNormal program: input -> program -> results.\nMachine Learning model: input and weights (parameters) -> model -> results.\n\nThe model is a mathematical function that takes the input, multiplies them with one set of weights and adds them up, then does that again for a second set of weights, and so forth.\nIt takes all of the negative numbers and replaces them with 0.\nIt takes all those numbers as inputs to the next layer.\nAnd it repeats a few times.\n\nWeights start out as being random.\nA more useful workflow: input/weights -> model -> results -> loss -> update weights.\nThe loss is a number that says how good the results were.\nWe need a way to come up with a new set of weights that are a bit better than the current weights.\n“bit better” weights means it makes the loss a bit better.\nIf we make it a little bit better a few times, it’ll eventually get good.\nNeural nets proven to solve any computable function (i.e. it’s flexible enough to update weights until the results are good).\n“Generate artwork based on someone’s twitter bio” is a computable function.\nOnce we’ve finished the training procedure we don’t the loss and the weights can be integrated into the model.\nWe end up with inputs -> model -> results which looks like our original idea of a program.\nDeploying a model will have lots of tricky details but there will be one line of code which says learn.predict which takes an input and provides results.\nThe most important thing to do is experiment.\n\n\n\n\n\nChapter 1: Your Deep Learning Journey In this section, I’ll take notes while I read Chapter 1 in the textbook.\n\n\n\nWhat you don’t need for deep learning: lots of math, lots of data, lots of expensive computers.\nDeep learning is a computer technique to extract and transform data by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task.\n\n\n\n\n\nWarren McCulloch and Walter Pitts developed a mathematical model of an artificial neuron in 1943.\nMost of Pitt’s famous work was done while he was homeless.\nPsychologist Frank Rosenblatt further developed the artificial neuron to give it the ability to learn and built the first device that used these principles, the Mark I Perceptron, which was able to recognize simple shapes.\nMarvin Minsky and Seymour Papert wrote a book about the Perceptron and showed that using multiple layers of the devices would allow the limitations of a single layer to be addressed.\nThe 1986 book Parallel Distributed Processing (PDP) by David Rumelhart, James McClelland, and the PDP Research Group defined PDP as requiring the following:\n\nA set of processing units.\nA state of activation.\nAn output function for each unit.\nA pattern of connectivity among units.\nA propogation rule for propagating patterns of activities through the network of connectivities.\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit.\nA learning rule whereby patterns of connectivity are modified by experience.\nAn environment within which the system must operate.\n\n\n\n\n\n\nThe hardest part of deep learning is artisanal: how do you know if you’ve got enough data, whether it is in the right format, if your model is training properly, and, if it’s not, what you should do about it?\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path,\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=is_cat,\n    item_tfms=Resize(224)\n)\n\ndls.show_batch()\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:11<00:00]\n    \n    \n\n\n\n\n\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n/usr/local/lib/python3.10/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 162MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.140327\n      0.019135\n      0.007442\n      01:05\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      4.17% [1/24 00:01<00:34]\n    \n    \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.070464\n      0.024966\n      0.006766\n      01:00\n    \n  \n\n\n\nThe error rate is the proportion of images that were incorrectly identified.\nCheck this model actually works with an image of a dog or cat. I’ll download a picture from google and use it for prediction:\n\nimport ipywidgets as widgets\nuploader = widgets.FileUpload()\nuploader\n\n\n\n\n\nim = PILImage.create(uploader.data[0])\nis_cat, _, probs = learn.predict(im)\nim.to_thumb(256)\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'Is this a cat?: {is_cat}.')\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\n\nIs this a cat?: True.\nProbability it's a cat: 1.000000\n\n\n\n\n\n\nA traditional program: inputs -> program -> results.\nIn 1949, IBM researcher Arthur Samuel started working on machine learning. His basic idea was this: instead of telling the computer the exact steps required to solve a problem, show it examples of the problem to solve, and let it figure out how to solve it itself.\nIn 1961 his checkers-playing program had learned so much that it beat the Connecticut state champion.\nWeights are just variables and a weight assignment is a particular choice of values for those variables.\nThe program’s inputs are values that it processes in order to produce its results (for instance, taking image pixels as inputs, and returning the classification “dog” as a result).\nBecause the weights affect the program, they are in a sense another kind of input.\nA program using weight assignment: inputs and weights -> model -> results.\nA model is a special kind of program, on that can do many different things depending on the weights.\nWeights = parameters, with the term “weights” reserved for a particulat type of model parameter.\nLearning would become entirely automatic when the adjustment of the weights was also automatic.\nTraining a maching learning model: inputs and weights -> model -> results -> performance -> update weights.\nresults are different than the performance of a model.\nUsing a trained model as a program -> inputs -> model -> results.\nmaching learning is the training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.\n\n\n\n\n\nNeural networks is a mathematical function that can solve any problem to any level of accuracy.\nStochastic Gradient Descent (SGD) is a completely general way to update the weights of a neural network, to make it improve at any given task.\nImage classification problem:\n\nOur inputs are the images.\nOur weights are the weights in the neural net.\nOur model is a neural net.\nOur results are the values that are calculated by the neural net, like “dog” or “cat”.\n\n\n\n\n\n\nThe functional form of the model is called its architecture.\nThe weights are called parameters.\nThe predictions are calculated from the independent variable, which is the data not including the labels.\nThe results or the model are called predictions.\nThe measure of performance is called the loss.\nThe loss depends not only on the predictions, but also on the correct labels (also known as targets or the dependent variable).\nDetailed training loop: inputs and parameters -> architecture -> predictions (+ labels) -> loss -> update parameters.\n\n\n\n\n\nA model cannot be created without data.\nA model can learn to operate on only the patterns seen in the input data used to train it.\nThis learning approach creates only predictions, not recommended actions.\nIt’s not enough to just have examples of input data, we need labels for that data too.\nPositive feedback loop: the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.\n\n\n\n\n\nitem_tfms are applied to each item while batch_tfms are applied to a batch of items at a time using the GPU.\nA classification model attempts to predict a class, or category.\nA regression model is one that attempts to predict one or more numeric quantities, such as temperature or location.\nThe parameter seed=42 sets the random seed to the same value every time we run this code, which means we get the same validation set every time we run it. This way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.\nWe care about how well our model works on previously unseen images.\nThe longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is overfitting.\nOverfitting is the single most important and challenging issue when training for all machine learning practitioners, and all algorithms.\nYou should only use methods to avoid overfitting after you have confirmed that overfitting is occurring (i.e., if you have observed the validation accuracy getting worse during training)\nfastai defaults to valid_pct=0.2.\nModels using architectures with more layers take longer to train and are more prone to overfitting, on the other hand, when using more data, they can be quite a bit more accurate.\nA metric is a function that measures the quality of the model’s predictions using the validation set.\nerror_rate tells you what percentage of inputs in the validation set are being classified incorrectly.\naccuracy = 1.0 - error_rate.\nThe entire purpose of loss is to define a “measure of performance” that the training system can use to update weights automatically. A good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand.\nA model that has weights that have already been trained on another dataset is called a pretrained model.\nWhen using a pretrained model, cnn_learner will remove the last layer and replace it with one or more new layers with randomized weights. This last part of the model is known as the head.\nUsing a pretrained model for a task different from what is was originally trained for is known as transfer learning.\nThe architecture only describes a template for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains.\nTo fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of epochs).\nfit will fit a model (i.e., look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels).\nFine-Tuning: a transfer learning technique that updates the parameters of a pretrained model by training for additional epochs using a different task from that used for pretraining.\nfine_tune has a few parameters you can set, but in the default form it does two steps:\n\nUse one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\nUse the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which don’t require many changes from the pretrained weights).\n\nThe head of the model is the part that is newly added to be specific to the new dataset.\nAn epoch is one complete pass through the dataset.\n\n\n\n\n\nWhen we fine tune our pretrained models, we adapt what the last layers focus on to specialize on the problem at hand.\n\n\n\n\n\nA lot of things can be represented as images.\nSound can be converted to a spectogram.\nTimes series data can be created into an image using Gramian Angular Difference Field (GADF).\nIf the human eye can recognize categories from the images, then a deep learning model should be able to do so too.\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nLabel\nThe data that we’re trying to predict\n\n\nArchitecture\nThe template of the model that we’re trying to fit; i.e., the actual mathematical function that we’re passing the input data and parameters to\n\n\nModel\nThe combination of the architecture with a particular set of parameters\n\n\nParameters\nThe values in the model that change what task it can do and that are updated through model training\n\n\nFit\nUpdate the parameters of the model such that the predictions of the model using the input data match the target labels\n\n\nTrain\nA synonym for fit\n\n\nPretrained Model\nA model that has already been trained, generally using a large dataset, and will be fine-tuned\n\n\nFine-tune\nUpdate a pretrained model for a different task\n\n\nEpoch\nOne complete pass through the input data\n\n\nLoss\nA measure of how good the model is, chosen to drive training via SGD\n\n\nMetric\nA measurement of how good the model is using the validation set, chosen for human consumption\n\n\nValidation set\nA set of data held out from training, used only for measuring how good the model is\n\n\nTraining set\nThe data used for fitting the model; does not include any data from the validation set\n\n\nOverfitting\nTraining a model in such a way that it remembers specific features of the input data, rather than generalizing wel to data not seen during training\n\n\nCNN\nConvolutional neural network; a type of neural network that works particularly well for computer vision tasks\n\n\n\n\n\n\n\nSegmentation\nNatural language processing (see below)\nTabular (see Adults income classification above)\nCollaborative filtering (see MovieLens ratings predictor above)\nStart by using one of the cut-down dataset versions and later scale up to the full-size version. This is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and use the full dataset only when they have a good understanding of what they have to do.\n\n\n\n\n\nIf the model makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by actually having seen that particular item.\nHyperparameters: various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors.\nWe, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values and we are in danger of overfitting the validation data through human trial and error and exploration.\nThe test set can be used only to evaluate the model at the very end of our efforts.\nTraining data is fully exposed to training and modeling processes, validation data is less exposed and test data is fully hidden.\nThe test and validation sets should have enough data to ensure that you get a good estimate of your accuracy.\nThe discipline of the test set helps us keep ourselves intellectually honest.\nIt’s a good idea for you to try out a simple baseline model yourself, so you know what a really simply model can achieve.\n\n\n\n\n\nA key property of the validation and test sets is that they must be representative of the new data you will see in the future.\nAs an example, for time series data, use earlier dates for training set and later more recent dates as validation set\nThe data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\n\n\nfrom fastai.text.all import *\n\n# I'm using IMDB_SAMPLE instead of the full IMDB dataset since it either takes too long or\n# I get a CUDA Out of Memory error if the batch size is more than 16 for the full dataset\n# Using a batch size of 16 with the sample dataset works fast\ndls = TextDataLoaders.from_csv(\n    path=untar_data(URLs.IMDB_SAMPLE),\n    csv_fname='texts.csv',\n    text_col=1,\n    label_col=0,\n    bs=16)\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is\n      negative\n    \n    \n      1\n      xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot … which is highly believable to xxunk . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies\n      positive\n    \n    \n      2\n      xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n\\n xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after 20 - odd years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \\n\\n xxmaj none of this excuses him this present ,\n      negative\n    \n    \n      3\n      xxbos i really wanted to love this show . i truly , honestly did . \\n\\n xxmaj for the first time , gay viewers get their own version of the \" the xxmaj bachelor \" . xxmaj with the help of his obligatory \" hag \" xxmaj xxunk , xxmaj james , a good looking , well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ) . xxmaj the only problem is half of them are straight and xxmaj james does n't know this . xxmaj if xxmaj james picks a gay one , they get a trip to xxmaj new xxmaj zealand , and xxmaj if he picks a straight one , straight guy gets $ 25 , xxrep 3 0 . xxmaj how can this not be fun\n      negative\n    \n    \n      4\n      xxbos xxmaj many neglect that this is n't just a classic due to the fact that it 's the first 3d game , or even the first xxunk - up . xxmaj it 's also one of the first xxunk games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - xxunk gaming experience in general . xxmaj with graphics that are terribly dated today , the game xxunk you into the role of xxunk even * think * xxmaj i 'm going to attempt spelling his last name ! ) , an xxmaj american xxup xxunk . caught in an underground bunker . xxmaj you fight and search your way through xxunk in order to achieve different xxunk for the six xxunk , let 's face it , most of them are just an excuse to hand you a weapon\n      positive\n    \n    \n      5\n      xxbos xxmaj i 'm sure things did n't exactly go the same way in the real life of xxmaj homer xxmaj hickam as they did in the film adaptation of his book , xxmaj rocket xxmaj boys , but the movie \" october xxmaj sky \" ( an xxunk of the book 's title ) is good enough to stand alone . i have not read xxmaj hickam 's memoirs , but i am still able to enjoy and understand their film adaptation . xxmaj the film , directed by xxmaj joe xxmaj xxunk and written by xxmaj lewis xxmaj xxunk , xxunk the story of teenager xxmaj homer xxmaj hickam ( jake xxmaj xxunk ) , beginning in xxmaj october of 1957 . xxmaj it opens with the sound of a radio broadcast , bringing news of the xxmaj russian satellite xxmaj xxunk , the first artificial satellite in\n      positive\n    \n    \n      6\n      xxbos xxmaj to review this movie , i without any doubt would have to quote that memorable scene in xxmaj tarantino 's \" pulp xxmaj fiction \" ( xxunk ) when xxmaj jules and xxmaj vincent are talking about xxmaj mia xxmaj wallace and what she does for a living . xxmaj jules tells xxmaj vincent that the \" only thing she did worthwhile was pilot \" . xxmaj vincent asks \" what the hell is a pilot ? \" and xxmaj jules goes into a very well description of what a xxup tv pilot is : \" well , the way they make shows is , they make one show . xxmaj that show 's called a ' pilot ' . xxmaj then they show that show to the people who make shows , and on the strength of that one show they decide if they 're going to\n      negative\n    \n    \n      7\n      xxbos xxmaj how viewers react to this new \" adaption \" of xxmaj shirley xxmaj jackson 's book , which was promoted as xxup not being a remake of the original 1963 movie ( true enough ) , will be based , i suspect , on the following : those who were big fans of either the book or original movie are not going to think much of this one … and those who have never been exposed to either , and who are big fans of xxmaj hollywood 's current trend towards \" special effects \" being the first and last word in how \" good \" a film is , are going to love it . \\n\\n xxmaj things i did not like about this adaption : \\n\\n 1 . xxmaj it was xxup not a true adaption of the book . xxmaj from the xxunk i had\n      negative\n    \n    \n      8\n      xxbos xxmaj the trouble with the book , \" memoirs of a xxmaj geisha \" is that it had xxmaj japanese xxunk but underneath the xxunk it was all an xxmaj american man 's way of thinking . xxmaj reading the book is like watching a magnificent ballet with great music , sets , and costumes yet performed by xxunk animals dressed in those xxunk far from xxmaj japanese ways of thinking were the characters . \\n\\n xxmaj the movie is n't about xxmaj japan or real geisha . xxmaj it is a story about a few xxmaj american men 's mistaken ideas about xxmaj japan and geisha xxunk through their own ignorance and misconceptions . xxmaj so what is this movie if it is n't about xxmaj japan or geisha ? xxmaj is it pure fantasy as so many people have said ? xxmaj yes , but then why\n      negative\n    \n  \n\n\n\n\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.629276\n      0.553454\n      0.740000\n      00:19\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.466581\n      0.548400\n      0.740000\n      00:30\n    \n    \n      1\n      0.410401\n      0.418941\n      0.825000\n      00:30\n    \n    \n      2\n      0.286162\n      0.410872\n      0.830000\n      00:31\n    \n    \n      3\n      0.192047\n      0.405275\n      0.845000\n      00:31\n    \n  \n\n\n\n\n# view actual vs prediction\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n      category_\n    \n  \n  \n    \n      0\n      xxbos xxmaj this film sat on my xxmaj xxunk for weeks before i watched it . i xxunk a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj xxunk . \\n\\n xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" la xxmaj xxunk , \" based on a play by xxmaj arthur xxmaj xxunk , who is given an \" inspired by \" credit . xxmaj it starts from one person , a prostitute , standing on a street corner in xxmaj brooklyn . xxmaj she is picked up by a home contractor , who has sex with her on the hood of a car , but ca n't come . xxmaj he refuses to pay her . xxmaj when he 's off xxunk , she\n      positive\n      positive\n    \n    \n      1\n      xxbos xxmaj bonanza had a great cast of wonderful actors . xxmaj xxunk xxmaj xxunk , xxmaj pernell xxmaj whitaker , xxmaj michael xxmaj xxunk , xxmaj dan xxmaj blocker , and even xxmaj guy xxmaj williams ( as the cousin who was brought in for several episodes during 1964 to replace xxmaj adam when he was leaving the series ) . xxmaj the cast had chemistry , and they seemed to genuinely like each other . xxmaj that made many of their weakest stories work a lot better than they should have . xxmaj it also made many of their best stories into great western drama . \\n\\n xxmaj like any show that was shooting over thirty episodes every season , there are bound to be some weak ones . xxmaj however , most of the time each episode had an interesting story , some kind of conflict ,\n      positive\n      negative\n    \n    \n      2\n      xxbos i watched xxmaj grendel the other night and am compelled to put together a xxmaj public xxmaj service xxmaj announcement . \\n\\n xxmaj grendel is another version of xxmaj beowulf , the thousand - year - old xxunk - saxon epic poem . xxmaj the scifi channel has a growing catalog of xxunk and uninteresting movies , and the previews promised an xxunk low - budget mini - epic , but this one xxunk to let me switch xxunk . xxmaj it was xxunk , xxunk , bad . i watched in xxunk and horror at the train wreck you could n't tear your eyes away from . i reached for a xxunk and managed to capture part of what i was seeing . xxmaj the following may contain spoilers or might just save your xxunk . xxmaj you 've been warned . \\n\\n - xxmaj just to get\n      negative\n      negative\n    \n    \n      3\n      xxbos xxmaj this is the last of four xxunk from xxmaj france xxmaj i 've xxunk for viewing during this xxmaj christmas season : the others ( in order of viewing ) were the uninspired xxup the xxup black xxup tulip ( 1964 ; from the same director as this one but not nearly as good ) , the surprisingly effective xxup lady xxmaj oscar ( 1979 ; which had xxunk as a xxmaj japanese manga ! ) and the splendid xxup cartouche ( xxunk ) . xxmaj actually , i had watched this one not too long ago on late - night xxmaj italian xxup tv and recall not being especially xxunk over by it , so that i was genuinely surprised by how much i enjoyed it this time around ( also bearing in mind the xxunk lack of enthusiasm shown towards the film here and elsewhere when\n      positive\n      positive\n    \n    \n      4\n      xxbos xxmaj this is not really a zombie film , if we 're xxunk zombies as the dead walking around . xxmaj here the protagonist , xxmaj xxunk xxmaj louque ( played by an unbelievably young xxmaj dean xxmaj xxunk ) , xxunk control of a method to create zombies , though in fact , his ' method ' is to mentally project his thoughts and control other living people 's minds turning them into hypnotized slaves . xxmaj this is an interesting concept for a movie , and was done much more effectively by xxmaj xxunk xxmaj lang in his series of ' dr . xxmaj mabuse ' films , including ' dr . xxmaj mabuse the xxmaj xxunk ' ( 1922 ) and ' the xxmaj testament of xxmaj dr . xxmaj mabuse ' ( 1933 ) . xxmaj here it is unfortunately xxunk to his quest to\n      negative\n      positive\n    \n    \n      5\n      xxbos \" once upon a time there was a charming land called xxmaj france … . xxmaj people lived happily then . xxmaj the women were easy and the men xxunk in their favorite xxunk : war , the only xxunk of xxunk which the people could enjoy . \" xxmaj the war in question was the xxmaj seven xxmaj year 's xxmaj war , and when it was noticed that there were more xxunk of soldiers than soldiers , xxunk were sent out to xxunk the ranks . \\n\\n xxmaj and so it was that xxmaj fanfan ( gerard xxmaj philipe ) , caught xxunk a farmer 's daughter in a pile of hay , escapes marriage by xxunk in the xxmaj xxunk xxunk … but only by first believing his future as xxunk by a gypsy , that he will win fame and fortune in xxmaj his xxmaj\n      positive\n      positive\n    \n    \n      6\n      xxbos xxup ok , let me again admit that i have n't seen any other xxmaj xxunk xxmaj ivory ( the xxunk ) films . xxmaj nor have i seen more celebrated works by the director , so my capacity to xxunk xxmaj before the xxmaj rains outside of analysis of the film itself is xxunk . xxmaj with that xxunk , let me begin . \\n\\n xxmaj before the xxmaj rains is a different kind of movie that does n't know which genre it wants to be . xxmaj at first , it pretends to be a romance . xxmaj in most romances , the protagonist falls in love with a supporting character , is separated from the supporting character , and is ( sometimes ) united with his or her partner . xxmaj this movie 's hero has already won the heart of his lover but can not\n      negative\n      negative\n    \n    \n      7\n      xxbos xxmaj first off , anyone looking for meaningful \" outcome xxunk \" cinema that packs some sort of social message with meaningful performances and soul searching dialog spoken by dedicated , xxunk , heartfelt xxunk , please leave now . xxmaj you are wasting your time and life is short , go see the new xxmaj xxunk xxmaj jolie movie , have a good cry , go out & buy a xxunk car or throw away your conflict xxunk if that will make you feel better , and leave us alone . \\n\\n xxmaj do n't let the door hit you on the way out either . xxup the xxup incredible xxup melting xxup man is a grade b minus xxunk horror epic shot in the xxunk of xxmaj oklahoma by a young , xxup tv friendly cast & crew , and concerns itself with an astronaut who is\n      positive\n      negative\n    \n    \n      8\n      xxbos \" national xxmaj treasure \" ( 2004 ) is a thoroughly misguided xxunk - xxunk of plot xxunk that borrow from nearly every xxunk and dagger government conspiracy cliché that has ever been written . xxmaj the film stars xxmaj nicholas xxmaj cage as xxmaj benjamin xxmaj xxunk xxmaj xxunk ( how precious is that , i ask you ? ) ; a seemingly normal fellow who , for no other reason than being of a xxunk of like - minded misguided fortune hunters , decides to steal a ' national treasure ' that has been hidden by the xxmaj united xxmaj states xxunk fathers . xxmaj after a bit of subtext and background that plays laughably ( unintentionally ) like xxmaj indiana xxmaj jones meets xxmaj the xxmaj patriot , the film xxunk into one misguided xxunk after another  attempting to create a ' stanley xxmaj xxunk\n      negative\n      negative\n    \n  \n\n\n\n\nreview_text = \"I really liked the movie!\"\nlearn.predict(review_text)\n\n\n\n\n\n\n\n\n('positive', tensor(1), tensor([0.0174, 0.9826]))\n\n\n\n\n\n\n\nDo you need these for deep learning?\n\nLots of Math (FALSE).\nLots of Data (FALSE).\nLots of expensive computers (FALSE).\nA PhD (FALSE).\n\nName five areas where deep learning is now the best tool in the world\n\nNatural Language Processing (NLP).\nComputer vision.\nMedicine.\nImage generation.\nRecommendation systems.\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nMark I Perceptron.\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nA series of processing units.\nA state of activation.\nAn output function for each unit.\nA pattern of connectivity among units.\nA propagation rule for propagating patterns of activities through the network of connectivities.\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit.\nA learning rule whereby patterns of connectivity are modified by experience.\nAn environment within which the system must operate.\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\nUsing multiple layers of the device would allow limitations of one layer to be addressed—this was ignored.\nMore than two layers are needed to get practical, good perforamnce—only in the last decade has this been more widely appreciated and applied.\n\nWhat is a GPU?\n\nA Graphical Processing Unit, which can perform thousands of tasks at the same time.\n\nOpen a notebook and execute a cell containing: 1+1. What happens?\n\nDepending on the server, it may take some time for the output to generate, but running this cell will output 2.\n\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.\n\n(I did this for the notebook shared for Lesson 1).\n\nComplete the Jupyter Notebook online appendix.\n\nDone. Will reference some of it again.\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\n\nBecause it’s hard to instruct a computer clear instructions to recognize images.\n\nWhat did Samuel mean by “weight assignment”?\n\nA particular choice for weights (variables)\n\nWhat term do we normally use in deep learning for what Samuel called “weights”?\n\nParameters\n\nDraw a picture that summarizes Samuel’s view of a machine learning model\n\ninput and weights -> model -> results -> performance -> update weights/inputs\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\nBecause a deep learning model has many layers and connectivities and activations between neurons that are not intuitive to our understanding.\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\n\nUniversal approximation theorem.\n\nWhat do you need in order to train a model?\n\nLabeled data (Inputs and targets).\nArchitecture.\nInitial weights.\nA measure of performance (loss, accuracy).\nA way to update the model (SGD).\n\nHow could a feedback loop impact the rollout of a predictive policing model?\n\nThe model will end up predicting where arrests are made, not where crime is taking place, so more police officers will go to locations where more arrests are predicted and feed that data back to the model which will reinforce the prediction of arrests in those areas, continuing this feedback loop of predictions -> arrests -> predictions.\n\nDo we always have to use 224x224-pixel images with the cat recognition model?\n\nNo, that’s just the convention for image recognition models.\nYou can use larger images but it will slow down the training process (it takes longer to open up bigger images).\n\nWhat is the difference between classification and regression?\n\nClassification predicts discrete classes or categories.\nRegression predicts continuous values.\n\nWhat is a validation set? What is a test set? Why do we need them?\n\nA validation set is a dataset upon which a model’s accuracy (or metrics in general) is calculated during training, as well as the dataset upon which the performance of different hyperparameters (like batch size and learning rate) are measured.\nA test set is a dataset upon which a model’s final performance is measured, a truly unseen dataset for both the model and the practitioner\n\nWhat will fastai do if you don’t provide a validation set?\n\nSet aside a random 20% of the data as the validation set by default\n\nCan we always use a random sample for a validation set? Why or why not?\n\nNo, in situations where we want to ensure that the model’s accuracy is evaluated on data the model has not seen, we should not use a random validation set. Instead, we should create an intentional validation set. For example:\n\nFor time series data, use the most recent dates as the validation set\nFor human recognition data, use images of different people for training and validation sets\n\n\nWhat is overfitting? Provide an example.\n\nOverfitting is when a model memorizes features of the training dataset instead of learning generalizations of the features in the data. An example of this is when a model memorizes training data facial features but then cannot recognize different faces in the real world. Another example is when a model memorizes the handwritten digits in the training data, so it cannot then recognize digits written in different handwriting. Overfitting can be observed during training when the validation loss starts to increase as the training loss decreases.\n\nWhat is a metric? How does it differ from loss?\n\nA metric a measurement of how good a model is performing, chosen for human consumption. A loss is also a measurement of how good a model is performing, but it’s chosen to drive training using an optimizer.\n\nHow can pretrained models help?\n\nPretrained models are already good at recognizing many generalized features and so they can help by providing a set of weights in an architecture that are capable, reducing the amount of time you need to train a model specific to your task.\n\nWhat is the “head” of the model?\n\nThe last/top few neural network layers which are replaced with randomized weights in order to specialize your model via training on the task at hand (and not the task it was pretrained to perform).\n\nWhat kinds of features do the early layers of a CNN find? How about the later layers?\n\nEarly layers: simple features lie lines, color gradients\nLater layers: compelx features like dog faces, outlines of people\n\nAre image models useful only for photos?\n\nNo! Lots of things can be represented by images so if you can represent something (like a sound) as an image (spectogram) and differences between classes/categories are easily recognizable by the human eye, you can train an image classifier to recognize it.\n\nWhat is an architecture?\n\nA template, mathematical function, to which you pass input data to in order to fit/train a model\n\nWhat is segmentation?\n\nRecognizing different objects in an image based on pixel colors (each object is a different pixel color)\n\nWhat is y_range used for? When do we need it?\n\nIt’s used to specify the output range of a regression model. We need it when the target is a continuous value.\n\nWhat are hyperparameters?\n\nModeling choices such as network architecture, learning rates, data augmentation strategies and other higher level choices that govern the meaning of the weight parameters.\n\nWhat is the best way to avoid failures when using AI in an organization?\n\nMaking sure you have good validation and test sets to evaluate the performance of a model on real world data.\nTrying out a simple baseline model to know what level of performance such a model can achieve.\n\n\n\n\n\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?\n\nCPU vs GPU for Machine Learning\n\nCPUs process tasks in a sequential manner, GPUs process tasks in parallel.\nGPUs can have thousands of cores, processing tasks at the same time.\nGPUs have many cores processing at low speeds, CPUs have few cores processing at high speeds.\nSome algorithms are optimized for CPUs rather than GPUs (time series data, recommendation systems that need lots of memory).\nNeural networks are designed to process tasks in parallel.\n\nCPU vs GPU in Machine Learning Algorithms: Which is Better?\n\nMachine Learning Operations Preferred on CPUs\n\nRecommendation systems that involve huge memory for embedding layers.\nSupport vector machines, time-series data, algorithms that don’t require parallel computing.\nRecurrent neural networks because they use sequential data.\nAlgorithms with intensive branching.\n\nMachine Learning Operations Preferred on GPUs\n\nOperations that involve parallelism.\n\n\nWhy Deep Learning Uses GPUs\n\nNeural networks are specifically made for running in parallel.\n\n\nTry to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice.\n\nHidden Risks of Machine Learning Applied to Healthcare: Unintended Feedback Loops Between Models and Future Data Causing Model Degradation\n\nIf clinicians fully trust the machine learning model (100% adoption of the predicted label) the false positive rate (FPR) grows uncontrollably with the number of updates.\n\nRunaway Feedback Loops in Predictive Policing\n\nOnce police are deployed based on these predictions, data from observations in the neighborhood is then used to further update the model.\nDiscovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated.\nPredictive policing systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.\n\nPitfalls of Predictive Policing: An Ethical Analysis\n\nPredictive policing relies on a large database of previous crime data and forecasts where crime is likely to occur. Since the program relies on old data, those previous arrests need to be unbiased to generate unbiased forecasts.\nPeople of color are arrested far more often than white people for committing the same crime.\nRacially biased arrest data creates biased forecasts in neighborhoods where more people of color are arrested.\nIf the predictive policing algorithm is using biased data to divert more police forces towards less affluent neighborhoods and neighborhoods of color, then those neighborhoods are not receiving the same treatment as others.\n\nBias in Criminal Risk Scores Is Mathematically Inevitable, Researchers Say\n\nThe algorithm COMPAS which predicts whether a person is “high-risk” and deemed more likely to be arrested in the future, leads to being imprisoned (instead of sent to rehab) or longer sentences.\n\nCan bots discriminate? It’s a big question as companies use AI for hiring\n\nIf an older candidate makes it past the resume screening process but gets confused by or interacts poorly with the chatbot, that data could teach the algorithm that candidates with similar profiles should be ranked lower\n\nEcho chambers, rabbit holes, and ideological bias: How YouTube recommends content to real users\n\nWe find that YouTube’s algorithm pushes real users into (very) mild ideological echo chambers.\nWe found that 14 out of 527 (~3%) of our users ended up in rabbit holes.\nFinally, we found that, regardless of the ideology of the study participant, the algorithm pushes all users in a moderately conservative direction.\n\n\n\n\n\n\n\nI’m going to do things a bit differently than how I approached Lesson 1. Jeremy suggested that we first watch the video without pausing in order to understand what we’re going to do and then watch it a second time and follow along. I also want to be mindful of how long I’m running my Paperspace Gradient maching (at $0.51/hour) so that I don’t run the machine when I don’t need its GPU.\nSo, here’s how I’m going to approach Lesson 2: - Read the Chapter 2 Questionnaire so I know what I’ll be “tested” on at the end - Watch the video without taking notes or running code - Rewatch the video and take notes in this notebook - Add the Kaggle code cells to this notebook and run them in Paperspace - Read the Gradio tutorial without running code - Re-read the Gradio tutorial and follow along with my own code - Read Chapter 2 in the textbook and run code in this notebook in Paperspace - Read Chapter 2 in the textbook and take notes in this notebook (including answers to the Questionnaire)\nWith this approach, I’ll have a big picture understanding of each step of the lesson and I’ll minimize the time I’m spending running my Paperspace Gradient machine.\n\n\nLink to this lesson’s video.\n\nIn this lesson we’re doing things that hasn’t been in courses like this before.\nResource: aiquizzes.com—I signed up and answered a couple of questions.\nDon’t forget the FastAI Forums\n\nClick “Summarize this Topic” to get a list of the most upvoted posts\n\nHow do we go about putting a model in production?\n\nFigure out what problem you want to solve\nFigure out how to get data for it\nGather some data\n\nUse DuckDuckGo image function\nDownload data\nGet rid of images that failed to open\n\nData cleaning\n\nBefore you clean your data, train the model\nImageClassifierCleaner can be used to clean (delete or re-label) the wrongly labeled data in the dataset\n\ncleaner orders by loss so you only need to look at the first few\n\nAlways build a model to find out what things are difficult to recognize in your data and to find the things the model can help you find that are problems in the data\n\nTrain your model again\nDeploy to HuggingFace Spaces\n\nInstall Jupyter Notebook Extensions to get features like table of contents and collapsible sections (with which you can also navigate sections using arrow keys)\nType ?? followed by function name to get source code\nType ? followed by function name to get brief info\nIf you have nbdev installed doc(<fn>) will give you link to documentation\nDifferent ways to resize an image\n\nResizeMethod.Squish (to see the whole picture with different aspect ratio)\nResizeMethod.Pad (whole image in correct aspect ratio)\n\nData Augmentation\n\nRandomResizedCrop (different bit of an image everytime)\nbatch_tfms=aug_tranforms() (images get turned, squished, warped, saturated, recolored, etc.)\n\nUse if you are training for more than 5-10 epochs\nIn memory, real-time, the image is being resized/cropped/etc.\n\n\nConfusion matrix (ClassificationInterpretation)\n\nOnly meaningful for category labels\nShows what category errors your model is making (actual vs predicted)\nIn a lot of situations this will let you know what the hard categories to classify are (e.g. breeds of pets hard to identify)\n.plot_top_losses tells us where the loss is the highest (prediction/actual/loss/probability)\n\nA loss will be bad (high) if we are wrong + confident or right + unconfident\n\n\nOn your computer, normal RAM doesn’t get filled up as it saves RAM to hard disk (swapping). GPUs don’t do swapping so do only one thing at a time so you’re not using up all the memory.\nGradio + HuggingFace Spaces\n\nHere is my Hello World HuggingFace Space!\nNext, we’ll put a deep learning model in production. In the code cells below, I will train and export a dog vs cat classifier.\n\n\n\n# import all the stuff we need from fastai\nfrom fastai.vision.all import *\nfrom fastbook import *\n\n\n# download and decompress our dataset\npath = untar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:57<00:00]\n    \n    \n\n\n\n# define a function to label our images\ndef is_cat(x): return x[0].isupper()\n\n\n# create `DataLoaders`\ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path),\n    valid_pct = 0.2,\n    seed = 42,\n    label_func = is_cat,\n    item_tfms = Resize(192))\n\n\n# view batch\ndls.show_batch()\n\n\n\n\n\n# train our model using resnet18 to keep it small and fast\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.199976\n      0.072374\n      0.020298\n      00:19\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.061802\n      0.081512\n      0.020974\n      00:20\n    \n    \n      1\n      0.047748\n      0.030506\n      0.010149\n      00:18\n    \n    \n      2\n      0.021600\n      0.026245\n      0.006766\n      00:18\n    \n  \n\n\n\n\n# export our trained learner\nlearn.export('model.pkl')\n\n\nFollowing the script in the video, as well as the git-lfs and requirements.txt in Tanishq Abraham’s tutorial, I deployed a Dog and Cat Classifier on HuggingFace Spaces.\nIf you run the training for long enough (high number of epochs) the error rate will get worse. We’ll learn why in a future lesson.\nUse fastsetup to setup your local machine with Python and Jupyter.\n\nThey recommend using mamba instead of conda as it is faster.\n\n\n\n\n\nIn the cells below, I’ll run the code provided in the Chapter 2 notebook.\n\n# prepare path and subfolder names\nbear_types = 'grizzly', 'black', 'teddy'\npath = Path('bears')\n\n\n# download images of grizzly, black and teddy bears\nif not path.exists():\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok = True)\n        results = search_images_ddg(f'{o} bear')\n        download_images(dest, urls = results)\n\n\n# view file paths\nfns = get_image_files(path)\nfns\n\n(#570) [Path('bears/grizzly/ca9c20c9-e7f4-4383-b063-d00f5b3995b2.jpg'),Path('bears/grizzly/226bc60a-8e2e-4a18-8680-6b79989a8100.jpg'),Path('bears/grizzly/2e68f914-0924-42ed-9e2e-19963fa03a37.jpg'),Path('bears/grizzly/38e2d057-3eb2-4e8e-8e8c-fa409052aaad.jpg'),Path('bears/grizzly/6abc4bc4-2e88-4e28-8ce4-d2cbdb05d7b5.jpg'),Path('bears/grizzly/3c44bb93-2ac5-40a3-a023-ce85d2286846.jpg'),Path('bears/grizzly/2c7b3f99-4c8e-4feb-9342-dacdccf60509.jpg'),Path('bears/grizzly/a59f16a6-fa06-42d5-9d79-b84e130aa4e3.jpg'),Path('bears/grizzly/d1be6dc8-da42-4bee-ac31-0976b175f1e3.jpg'),Path('bears/grizzly/7bc0d3bd-a8dd-477a-aa16-449124a1afb5.jpg')...]\n\n\n\n# get list of corrupted images\nfailed = verify_images(fns)\nfailed\n\n(#24) [Path('bears/grizzly/2e68f914-0924-42ed-9e2e-19963fa03a37.jpg'),Path('bears/grizzly/f77cfeb5-bfd2-4c39-ba36-621f117a65f6.jpg'),Path('bears/grizzly/37aa7eed-5a83-489d-b8f5-54020ba41390.jpg'),Path('bears/black/90a464ad-b0a7-4cf5-86ff-72d507857007.jpg'),Path('bears/black/f03a0ceb-4983-4b8f-a001-84a0875704e8.jpg'),Path('bears/black/6193c1cf-fda4-43f9-844e-7ba7efd33044.jpg'),Path('bears/teddy/474bdbb3-de2f-49e5-8c5b-62b4f3f50548.JPG'),Path('bears/teddy/58755f3f-227f-4fad-badc-a7d644e54296.JPG'),Path('bears/teddy/eb55dc00-3d01-4385-a7da-d81ac5211696.jpg'),Path('bears/teddy/97eadc96-dc4e-4b3f-8486-88352a3b2270.jpg')...]\n\n\n\n# remove corrupted image files\nfailed.map(Path.unlink)\n\n(#24) [None,None,None,None,None,None,None,None,None,None...]\n\n\n\n# create DataBlockfor training\nbears = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct = 0.2, seed = 42),\n    get_y = parent_label,\n    item_tfms = Resize(128)\n)\n\n\n# create DataLoaders object\ndls = bears.dataloaders(path)\n\n\n# view training batch -- looks good!\ndls.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# view validation batch -- looks good!\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# observe how images react to the \"squish\" ResizeMethod\nbears = bears.new(item_tfms = Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\nNotice how the grizzlies in the third image look abnormally skinny, since the image is squished.\n\n# observe how images react to the \"pad\" ResizeMethod\nbears = bears.new(item_tfms = Resize(128, ResizeMethod.Pad, pad_mode = 'zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\nIn these images, the original aspect ratio is maintained.\n\n# observe how images react to the transform RandomResizedCrop\nbears = bears.new(item_tfms = RandomResizedCrop(128, min_scale = 0.3))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# observe how images react to data augmentation transforms\nbears = bears.new(item_tfms=Resize(128), batch_tfms = aug_transforms(mult = 2))\ndls = bears.dataloaders(path)\n# note that data augmentation occurs on training set\ndls.train.show_batch(max_n = 8, nrows = 2, unique = True)\n\n\n\n\n\n# train the model in order to clean the data\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# train the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 100MB/s] \n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.221027\n      0.206999\n      0.055046\n      00:34\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.225023\n      0.177274\n      0.036697\n      00:32\n    \n    \n      1\n      0.162711\n      0.189059\n      0.036697\n      00:31\n    \n    \n      2\n      0.144491\n      0.191644\n      0.027523\n      00:31\n    \n    \n      3\n      0.122036\n      0.188296\n      0.018349\n      00:31\n    \n  \n\n\n\n\n# view Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model confused a grizzly for a black bear and a black bear for a grizzly bear. It didn’t confuse any of the teddy bears, which makes sense given how different they look to real bears.\n\n# view images with the highest losses\ninterp.plot_top_losses(5, nrows = 1)\n\n\n\n\n\n\n\n\n\n\n\nThe fourth image has two humans in it, which is likely why the model didn’t recognize the bear. The model correctly predicted the the third and fifth images but with low confidence (57% and 69%).\n\n# clean the training and validation sets\nfrom fastai.vision.widgets import *\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI cleaned up the images (deleting an image of a cat, another of a cartoon bear, a dog, and a blank image).\n\n# delete or move images based on the dropdown selections made in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n# create new dataloaders object\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# retrain the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.289331\n      0.243501\n      0.074074\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.225567\n      0.256021\n      0.064815\n      00:32\n    \n    \n      1\n      0.218850\n      0.288018\n      0.055556\n      00:34\n    \n    \n      2\n      0.184954\n      0.315183\n      0.055556\n      00:31\n    \n    \n      3\n      0.141363\n      0.308634\n      0.055556\n      00:31\n    \n  \n\n\n\nWeird!! After cleaning the data, the model got worse (1.8% error rate is now 5.6%). I’ll run the cleaning routine again and retrain the model to see if it makes a difference. Perhaps there are still erroneous images in the mix.\n\n# view Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis time, the model incorrectly predicted 3 grizzlies as black bears, 2 black bears as grizzlies and 1 black bear as a teddy.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# delete or move images based on the dropdown selections made in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n# create new dataloaders object\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\n# The lower right image (cartoon bear) is one that I selected \"Delete\" for\n# in the cleaner so I'm not sure why it's still there\n# I'm wondering if there's something wrong with the cleaner or how I'm using it?\ndls.show_batch()\n\n\n\n\n\n# retrain the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.270627\n      0.130137\n      0.046729\n      00:31\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.183445\n      0.078030\n      0.028037\n      00:32\n    \n    \n      1\n      0.201080\n      0.053461\n      0.018692\n      00:33\n    \n    \n      2\n      0.183515\n      0.019479\n      0.009346\n      00:37\n    \n    \n      3\n      0.144900\n      0.012682\n      0.000000\n      00:31\n    \n  \n\n\n\nI’m still not confident that this is a 100% accurate model given the bad images in the training set (such as the cartoon bear) but I’m going to go with it for now.\n\n\n\n\n\n\nUnderestimating the constraints and overestimating the capabilities of deep learning may lead to frustratingly poor results, at least until you gain some experience and can solve the problems that arise.\nOverstimating the constraints and underestimating the capabilities of deep learning may mean you do not attempt a solvable problem because you talk yourself out of it.\nThe most important thing (as you learn deep learning) is to ensure that you have a project to work on.\nThe goal is not to find the “perfect” dataset or project, but just to get started and iterate from there.\nComplete every step as well as you can in a reasonable amount of time, all the way to the end.\nComputer vision\n\nObject recognition: recognize items in an image\nObject detection: recognition + highlight the location and name of each found object.\nDeep learning algorithms are generally not good at recognizing images that are significantly different in structure or style from those used to train the model.\n\nNLP\n\nDeep learning is not good at generating correct responses.\nText generation models will always be technologically a bit ahead of models for recognizing automatically generated text.\nGoogle’s online translation system is based on deep learning.\n\nCombining text and images\n\nA deep learning model can be trained on input images with output captions written in English, and can learn to generate surprisingly appropriate captions automatically for new images (with no guarantee the captions will be correct).\nDeep learning should be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely.\n\nTabular data\n\nIf you already have a system that is using random forests or gradient boosting machines then switching to or adding deep learning may not result in any dramatic improvement.\nDeep learning greatly increases the variety of columns that you can include.\nDeep learning models generally take longer to train than random forests or gradient boosting machines.\n\nRecommendation systems\n\nA special type of tabular data (a high-cardinality categorical variable representing users and another one representing products or something similar).\nDeep learning models are good at handling high cardinality categorical variables and thus recommendation systems.\nDeep learning models do well when combining these variables with other kinds of data such as natural language, images, or additional metadata represented as tables such as user information, previous transactions, and so forth.\nNearly all machine learning approaches have th downside that they tell you only which products a particular user might like, rather than what recommendations would be helpful for a user.\n\nOther data types\n\nUsing NLP deep learning methods is the current SOTA approach for many types of protein analysis since protein chains look a lot like natural language documents.\n\nThe Drivetrain Approach\n\nDefined objective\nLevers (what inputs can we control)\nData (what inputs we can collect)\nModels (how the levers influence the objective)\n\nGathering data\n\nFor most projects you can find the data online.\nUse duckduckgo_search\n\nFrom Data to DataLoaders\n\nDataLoaders is a thin class that just stores whatever DataLoader objects you pass to it and makes them available as train and valid.\nTo turn data into a DataLoaders object we need to tell fastai four things:\n\nWhat kinds of data we are working with.\nHow to get the list of items.\nHow to label these items.\nHow to create the validation set.\n\nWith the DataBlock API you can customize every stage of the creation of your DataLoaders:\n\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\nexplanation of DataBlock\n\nblocks specifies types for independent (the thing we are using to make predictions from) and dependent (our target) variables.\nComputers don’t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time–called the seed–then you will get the exact same list each time.\nImages need to be all the same size.\nA DataLoader is a class that provides batches of a few items at a time to the GPU.\nfastai default batch size is 64 items.\nResize crops the images to fit a square shape, alternatively you can pad (ResizeMethod.Pad) or squish (ResizeMethod.Squish) the images to fit the square.\nSquishing (model learns that things look differently from how they actually are), cropping (removal of features that would allow us to perform recognition) and padding (lot of empty space which is just wasted computation) are wasteful or problematic approaches. Instead, randomly select part of the image and then crop to just that part. On each epoch, we randomly select a different part of each image (RandomResizedCrop(min_scale)).\nTraining the neural network with examples of images in which objects are in slightly different places and are slightly different sizes helps it to understand the basic concept of what an object is and how it can be represented in an image.\n\nData Augmentation\n\nrefers to creating random variations of our input data, such that they appear different but do not change the meaning of the data (rotation, flipping, perspective warping, brightness changes, and contrast changes).\naug_transforms() provides a standard set of augmentations.\nUse batch_tfms to process a batch at a time on the GPU to save time.\n\nTraining your model and using it to clean your data\n\nView confusion matrix with ClassificationInterpretation.from_learner(learn). The diagonal shows images that are classified correctly. Calculated using validation set.\nSort images by loss using interp.plot_top_losses().\nLoss is high if the model is incorrect (especially if it’s also confident) or if it’s correct but not confident.\nA model can help you find data issues more quickly.\n\nUsing the model for inference\n\nlearn.export() will export a .pkl file.\nGet predictions with learn_inf.predict(<input>). This returns three things: the predicted category in the same format you originally provided, the index of the predicted category and the probabilities for each category.\nYou can access the DataLoaders as an attribute of the Learner: learn_inf.dls.\n\nDeploying your app\n\nYou almost certainly do not need a GPU to serve your model in production.\nTo classify a few users’ images at a time, you need high-volume. If you do have this scenario, use Microsoft’s ONNX Runtime or AWS SageMaker.\nRecommended wherever possible to deploy the model itself to a server and have your mobile/edge application connect to it as a web service.\nIf your application uses sensitive data, your users may be concerned about an approach that sends that data to a remote server.\n\nHow to Avoid Disaster\n\nUnderstanding and testing the behavior of a deep learning model is much more difficult than with most other code you write.\nThe kinds of photos that people are most likely to upload to the internet are the kinds of photos that do a good job of clearly and artistically displaying their subject matter, which isn’t the kind of input this system is going to be getting in real life. We may need to do a lot of our own data collection and labeling to create a useful system.\nout-of-domain data: data that our model sees in production that is very different from what it saw during training.\ndomain shift: data that our model sees changes over time.\nDeployment process\n\nManual Process: run model in parallel, humans check all predictions.\nLimited scope deployment: careful human supervision, time or geography limited.\nGradual expansion: good reporting systems needed, consider what could go wrong.\n\nUnforeseen consequences and feedback loops\n\nYour model may change the behavior of the system it’s a part of.\nfeedback loops can result in negative implications of bias getting worse.\nA helpful exercise prior to rolling out a significant machine learning system is to consider the question “What would happen if it went really, really well?”\n\n\nQuestionnaire\n\nWhere do text models currently have a major deficiency?\n\nProviding correct or accurate information.\n\nWhat are possible negative societal implications of text generation models?\n\nThe viral spread of misinformation, which can lead to real actions and harms.\n\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a god alternative to automating a process?\n\nRun the model in parallel with a human checking its predictions.\n\nWhat kind of tabular data is deep learning particularly good at?\n\nHigh-cardinality categorical data.\n\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\n\nIt will only tell you which products a particular user might like, rather than what recommendations may be helpful for a user.\n\nWhat are the steps of the Drivetrain Approach?\n\nDefine an objective\nDetermine what inputs (levers) you can control\nCollect data\nCreate models (how the levers influence the objective)\n\nHow do the steps of the Drivetrain Approach map to a recommendation system?\n\nObjective: drive additional sales due to recommendations.\nLevel: ranking of the recommendations.\nData: must be collectd to generate recommendations that will cause new sales.\nModels: two for purchasing probabilities conditional on seeing or not seeing a recommendation, the difference between these two probabilities is a utility function for a given recommendation to a customer (low in cases when algorithm recommends a familiar book that the customer has already rejected, or a book they would have bought even without the recommendation).\n\nCreate an image recognition model using data you curate, and deploy it on the web.\n\nHere.\n\nWhat is DataLoaders?\n\nA class that creates validation and training sets/batches that are fed to the GPUS\n\nWhat four things do we need to tell fastai to create DataLoaders?\n\nWhat kinds of data we are working with (independent and dependent variables).\nHow to get the list of items.\nHow to label these items.\nHow to create the validation set.\n\nWhat does the splitter parameter to DataBlock do?\n\nSet aside a percentage of the data as the validation set.\n\nHow do we ensure a random split always gives the same validation set?\n\nSet the seed parameter to the same value.\n\nWhat letters are often used to signify the independent and dependent variables?\n\nIndependent: x\nDependent: y\n\nWhat’s the difference between crop, pad and squish resize approaches? When might you choose one over the others?\n\nCrop: takes a section of the image and resizes it to the desired size. Use when it’s not necessary to have the model traing on the whole image.\nPad: keep the image aspect ratio as is, add white/black padding to make a square. Use when it’s necessary to have the model train on the whole image.\nSquish: distorts the image to fit a square. Use when it’s not necessary to have the model train on the original aspect ratio.\n\nWhat is data augmentation? Why is it needed?\n\nData augmentation is the creation of random variations of input data through techniques like rotation, flipping, brightness changes, contrast changes, perspective warping. It is needed to help the model learn to recognize objects under different lighting/perspective conditions.\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\nWhat is the difference between item_tfms and batch_tfms?\n\nitem_tfms are transforms that are applied to each item in the set.\nbatch_tfms are transforms applied to a batch of items in the set.\n\nWhat is a confusion matrix?\n\nA matrix that shows the counts of predicted (columns) vs. actual (rows) labels, with the diagonal being correctly predicted data.\n\nWhat does export save?\n\nBoth the architecture and the parameters as a .pkl file.\n\nWhat is called when we use a model for making predictions, instead of training?\n\nInference\n\nWhat are IPython widgets?\n\ninteractive browser controls for Jupyter Notebooks.\n\nWhen would you use a CPU for deployment? When might a GPU be better?\n\nCPU: low-volume, single-user inputs for prediction.\nGPU: high-volume, multiple-user inputs for predictions.\n\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n\nRequires internet connectivity (and latency).\nSensitive data transfer may not be okay with your users.\nManaging complexity and scaling the server creates additional overhead.\n\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice?\n\nout-of-domain data: the images captured of real bears may not be represented in the model’s training or validation datasets.\nNumber of bear alerts doubles or halves after rollout of the new system in some location.\nout-of-domain data: the cameras may capture low-resolution images of the bears when the training and validation set had high resolution images.\n\nWhat is out-of-domain data?\n\nData your model sees in production that it hasn’t seen during training.\n\nWhat is domain shift?\n\nChanges in the data that our model sees in production over time.\n\nWhat are the three steps in the deployment process?\n\nManual Process\nLimited scope deployment\nGradual expansion\n\n\nFurther Research\n\nConsider how the Drivetrain Approach maps to a project or problem you’re interested in.\n\nI’ll take the example of a project I will be working on to practice what I’m learning in this book: training a deep learning model which correctly classifies the typeface from a collection of single letter.\n\nThe objective: correctly classify typeface from a collection of single letters.\nLevers: observe key features of key letters that are the “tell” of a typeface.\nData: using an HTML canvas object and Adobe Fonts, generate images of single letters of multiple fonts associated with each category of typeface.\nModels: output the probabilities of each typeface a given collection of single letters is predicted as. This allows for some flexibility in how you categorize letters based on the shared characteristics of more than one typeface that the particular font may possess.\n\n\nWhen might it be best to avoid certain types of data augmentation?\n\nIn my typeface example, it’s best to avoid perspective warping because it will change key features used to recognize a typeface.\n\nFor a project you’re interested in applying deep learning to, consider the thought experiment, “What would happen if it went really, really well?”\n\nIf my typeface classifier works really well, I imagine it would be used by people to take pictures of real-world text and learn what typeface it is. This may inspire a new wave of typeface designers. If a feedback loop was possible, and the classifier went viral, the very definition of typefaces may be affected by popular opinion. Taken a step further, a generative model may be inspired by this classifier, and a new wave of AI typeface would be launched—however this last piece is highly undesirable unless the training of the model involves appropriate licensing and attribution of the typefaces used that are created by humans. Furthermore, from what I understand from reading about typefaces, the process of creating a typeface is an amazing experience and should not be replaced with AI generators. If I created such a generative model (in part 2 of the course) and it went viral (do HuggingFace Spaces go viral? Cuz that’s where I would launch it), I would take it down.\n\nStart a blog (done!)\n\n\n\n\n\n\n\n\n\nLink to this lesson’s video.\n\nHow to do a fast.ai lesson\n\nWatch lecture\nRun notebook & experiment\nReproduce results\nRepeat with different dataset\n\nfastbook repo contains “clean” folder with notebooks without markdown text.\nTwo concepts: training the model and using it for inference.\nOver 500 architectures in timm (PyTorch Image Models).\ntimm.list_models(pattern) will list models matching the pattern.\nPass string name of timm model to the Learner like: vision_learner(dls, 'timm model string', ...).\nin22 = ImageNet with 22k categories, 1k = ImageNet with 1k categories.\nlearn.predict probabilities are in the order of learn.dls.vocab.\nlearn.model contains the trained model which contains lots of nested layers.\nlearn.model.get_submodule takes a dotted string navigating through the hierarchy.\nMachine learning models fit functions to data.\nThings between dollar signs is LaTeX \"$...$\".\nGeneral form of quadratic: def quad(a,b,c,x): return a*x**2 + b*x + c\npartial from functools fixes parameters to a function.\nLoss functions tells us how good our model is.\n@interact from ipywidgets allows sliders tied to the function its above.\nMean Squared Error: def mse(preds, acts): return ((preds - acts)**2).mean()\nFor each parameter we need to know: does the loss get better when we increase or decrease the parameter?\nThe derivative is the function that tells you: if you increase the input does the output increase or decrease, and by how much?\n*params spreads out the list into its elements and passes each to the function.\n1-D (rank 1) tensor (lists of numbers), 2-D tensor (tables of numbers) 3-D tensor (layers of tables of numbers) and so on.\ntensor.requires_grad_() calculates the gradient of the values in the tensor whenever its used in calculation.\nloss.backward() calculates gradients on the inputs to the loss function.\nabc.grad attribute added after gradients are calculated.\nnegative gradient means increasing the parameter will decrease the loss.\nupdate parameters with torch.no_grad() so PyTorch doesn’t calculate the gradient (since it’s being used in a function). We don’t want the derivative of the parameter update, we only want the derivative with respect to the loss.\nAutomate the steps\n\nCalculate Mean Squared Error\nCall .backward.\nSubtract gradient * small number from the parameters\n\nAll optimizers are built on the concept of gradient descent (calculate gradients and decrease the loss).\nWe need a better function than quadratics\nRectified Linear Unit:\n\ndef rectified_linear(m,b,x):\n    y = m*x + b\n    return torch.clip(y, 0.)\n\ntorch.clip turns values less than value specified to the value specified (in this case, it turns negative values to 0.).\nAdding rectified linear functions together gives us an arbitrarily squiggly function that will match as close as we want to the data.\nReLU in 2D gives you surfaces, volumes in 3D, etc.\nWith this incredibly simple foundation you can construct an arbitrarily precise, accurate model.\nWhen you have ReLU’s getting added together, and gradient descent to optimize the parameters, and samples of inputs and outputs that you want, the computer “draws the owl” so to speak.\nDeep learning is using gradient descent to set some parameters to make a wiggly function (the addition of lots of rectified linear units or something very similar to that) that matches your data.\nWhen selecting an architecture, the biggest beginner mistake is that they jump to the highest-accuracy models.\nAt the start of the project, just use resnet18 so you can spend all of your time trying things out (data augmentation, data cleaning, different external data) as fast as possible.\nTrying better architectures is the very last thing to do.\nHow do I know if I have enough data?\n\nVast majority of projects in industry wait far too long until they train their first model.\nTrain your first model on day 1 with whatever CSV files you can hack together.\nSemi-supervised training lets you get dramatically more out of your data.\nOften it’s easy to get lots of inputs but hard to get lots of outputs (labels).\n\nUnits of parameter gradients: for each increase in parameter of 1, the gradient is the amount the loss would change by (if it stayed at that slope—which it doesn’t because it’s a curve).\nOnce you get close enough to the optimal parameter value, all loss functions look like quadratics\n\nThe slope of the loss function decreases as you approach the optimal\n\nLearning rate (a hyperparameter) is multiplied by the gradient, the product of which is subtracted from the parameters\nIf you pick a learning rate that’s too large, you will diverge; if you pick too small, it’ll take too long to train.\nhttp://matrixmultiplication.xyz/\nMatrix multiplication is the critical foundational mathematical operation in deep learning\nGPUs are good at matrix multiplication with tensor cores (multiply together two 4x4 matrices)\nUse a spreadsheet to train a deep learning model on the Kaggle Titanic dataset in which you’re trying to predict if a person survived.\n\nColumns included (convert some of them to binary categorical variables):\n\nSurvivor\nPclass\n\nConvert to Pclass_1 and Pclass_2 (both 1/0).\n\nSex\n\nConvert to Male (0/1) column.\n\nAge\n\nRemove blanks.\nNormalize (Age/Max(Age))\n\nSibSp (how many siblings they have)\nParch (# of parents/children aboard)\nFare\n\nLots of very small and very large fares, log of it has a much more even distribution. (LOG10(Fare + 1).\n\nEmbarked (which city they got on at)\n\nRemove blanks.\nConvert to Embark_S and Embark_C (both 1/0)\n\nOnes\n\nAdd a column of 1s.\n\n\nCreate random numbers for params (including Const) with =RAND() - 0.5.\nRegression\n\nUse SUMPRODUCT to calculate linear function.\nLoss of linear function is (linear function result - Survived) ^ 2.\nAverage loss = AVERAGE(individual losses).\nUser “Solver” with GRG Nonlinear Solving Method. Set Objective to minimize the cell with average loss. Change parameter variables.\n\nNeural Net\n\nTwo sets of params.\nTwo linear columns.\nTwo ReLU columns.\nAdding two linear functions together gives you a linear function, we want all those wiggles (non-linearity) so we use ReLUs.\nReLU: IF(lin1 < 0, 0, lin1)\nPreds = sum of the two ReLUs.\nLoss same as regression.\nSolver process the same as well.\n\nNeural Net (Matrix Multiplication)\n\nTranspose params into two columns.\n=MMULT(...) for Lin1 and Lin2 columns.\nKeep ReLU, Preds and Loss column the same.\nOptimize params using Solver.\nHelpful reminder to build intuition around matrix multiplication: it’s doing the same thing as the SUMPRODUCTs.\n\nDummy variables: Pclass_1, Pclass_2, etc.\n\nNext lesson: NLP\n\nIt’s about making predictions with text data which most of the time is in the form of prose.\nFirst Farsi NLP resource was created by a student of the first fastai course.\nNLP most commonly and practically used for classification.\nDocument = one or two words, a book, a wikipedia page, any length.\nClassification = figure out a category for a document.\nSentiment analysis\nAuthor identification\nLegal discovery (is this document in-scope or out-of-scope)\nOrganizing documents by topic\nTriaging inbound emails\nClassification of text looks similar to images.\nWe’re going to use a different library: HuggingFace Transformers\n\nHelpful to see how things are done in more than one library.\nHuggingFace Transformers doesn’t have the same high-level API. Have to do more stuff manually. Which is good for students at this point of the course.\nIt’s a good library.\n\nBefore the next lesson take a look at the NLP notebook and U.S. Patent to Phrase Matching data.\n\nTrying to figure out in patents whether two concepts are referring to the same thing. The document is text1, text2, and the category is similar (1) or not-similar (0).\n\nWill also talk about the two very important topics of validation sets and metrics.\n\n\n\n\n\n\n\nIn this section, I’ll train a Pets dataset classifier as done by Jeremy in this notebook.\n\nfrom fastai.vision.all import *\nimport timm\n\n\npath = untar_data(URLs.PETS)/'images'\n\n# Create DataLoaders object\ndls = ImageDataLoaders.from_name_func('.',\n                                      get_image_files(path),\n                                      valid_pct=0.2,\n                                      seed=42,\n                                      label_func=RegexLabeller(pat = r'^([^/]+)_\\d+'),\n                                      item_tfms=Resize(224))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:00<00:00]\n    \n    \n\n\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n# train using resnet34 as architecture\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 196MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.496086\n      0.316146\n      0.100135\n      01:12\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      45.65% [42/92 00:25<00:30 0.4159]\n    \n    \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.441153\n      0.315289\n      0.093369\n      01:04\n    \n    \n      1\n      0.289844\n      0.215224\n      0.069012\n      01:05\n    \n    \n      2\n      0.123374\n      0.191152\n      0.060217\n      01:03\n    \n  \n\n\n\nThe pets classifier, using resnet34 and 3 epochs, is about 94% accurate.\n\n# train using a timm architecture\n# from the convnext family of architectures\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name convnext_tiny_in22k to current convnext_tiny.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.130913\n      0.240275\n      0.085927\n      01:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.277886\n      0.193888\n      0.061570\n      01:08\n    \n    \n      1\n      0.196232\n      0.174544\n      0.055480\n      01:09\n    \n    \n      2\n      0.127525\n      0.156720\n      0.048038\n      01:07\n    \n  \n\n\n\nUsing convnext_tiny_in22k, the model is about 95.2% accurate, about a 20% decrease in error rate.\n\n# export to use in gradio app\nlearn.export('pets_model.pkl')\n\nYou can view my pets classifier gradio app here.\n\n\n\nIn this section, I’ll plot the timm model results as shown in Jeremy’s notebook.\n\nimport pandas as pd\n\n\n# load data\ndf_results = pd.read_csv(\"../../../fastai-course/data/results-imagenet.csv\")\ndf_results.head()\n\n\n\n\n\n  \n    \n      \n      model\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count\n      img_size\n      crop_pct\n      interpolation\n    \n  \n  \n    \n      0\n      eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n      90.052\n      9.948\n      99.048\n      0.952\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      1\n      eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n      89.966\n      10.034\n      99.012\n      0.988\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      2\n      eva_giant_patch14_560.m30m_ft_in22k_in1k\n      89.786\n      10.214\n      98.992\n      1.008\n      1,014.45\n      560\n      1.0\n      bicubic\n    \n    \n      3\n      eva02_large_patch14_448.mim_in22k_ft_in1k\n      89.624\n      10.376\n      98.950\n      1.050\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      4\n      eva02_large_patch14_448.mim_m38m_ft_in1k\n      89.570\n      10.430\n      98.922\n      1.078\n      305.08\n      448\n      1.0\n      bicubic\n    \n  \n\n\n\n\ntop1 = what percent of the time the model predicts the correct label with the highest probability.\ntop5 = what percent of the time the model predits the correct label with the top 5 highest probabilities.\nSource\n\n# remove additional text from model name\ndf_results['model_org'] = df_results['model']\ndf_results['model'] = df_results['model'].str.split('.').str[0]\ndf_results.head()\n\n\n\n\n\n  \n    \n      \n      model\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count\n      img_size\n      crop_pct\n      interpolation\n      model_org\n    \n  \n  \n    \n      0\n      eva02_large_patch14_448\n      90.052\n      9.948\n      99.048\n      0.952\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n    \n    \n      1\n      eva02_large_patch14_448\n      89.966\n      10.034\n      99.012\n      0.988\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n    \n    \n      2\n      eva_giant_patch14_560\n      89.786\n      10.214\n      98.992\n      1.008\n      1,014.45\n      560\n      1.0\n      bicubic\n      eva_giant_patch14_560.m30m_ft_in22k_in1k\n    \n    \n      3\n      eva02_large_patch14_448\n      89.624\n      10.376\n      98.950\n      1.050\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_in22k_ft_in1k\n    \n    \n      4\n      eva02_large_patch14_448\n      89.570\n      10.430\n      98.922\n      1.078\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_m38m_ft_in1k\n    \n  \n\n\n\n\n\ndef get_data(part, col):\n    # get benchmark data and merge with model data\n    df = pd.read_csv(f'../../../fastai-course/data/benchmark-{part}-amp-nhwc-pt111-cu113-rtx3090.csv').merge(df_results, on='model')\n    # convert samples/sec to sec/sample\n    df['secs'] = 1. / df[col]\n    # pull out the family name from the model name\n    df['family'] = df.model.str.extract('^([a-z]+?(?:v2)?)(?:\\d|_|$)')\n    # removing `resnetv2_50d_gn` and `resnet50_gn` for some reason\n    df = df[~df.model.str.endswith('gn')]\n    # not sure why the following line is here, \"in22\" was removed in cell above\n    df.loc[df.model.str.contains('in22'),'family'] = df.loc[df.model.str.contains('in22'),'family'] + '_in22'\n    df.loc[df.model.str.contains('resnet.*d'),'family'] = df.loc[df.model.str.contains('resnet.*d'),'family'] + 'd'\n    # only returns subset of families\n    return df[df.family.str.contains('^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin')]\n\n\n# load benchmark inference data\ndf = get_data('infer', 'infer_samples_per_sec')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      model\n      infer_samples_per_sec\n      infer_step_time\n      infer_batch_size\n      infer_img_size\n      param_count_x\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count_y\n      img_size\n      crop_pct\n      interpolation\n      model_org\n      secs\n      family\n    \n  \n  \n    \n      12\n      levit_128s\n      21485.80\n      47.648\n      1024\n      224\n      7.78\n      76.526\n      23.474\n      92.872\n      7.128\n      7.78\n      224\n      0.900\n      bicubic\n      levit_128s.fb_dist_in1k\n      0.000047\n      levit\n    \n    \n      13\n      regnetx_002\n      17821.98\n      57.446\n      1024\n      224\n      2.68\n      68.746\n      31.254\n      88.536\n      11.464\n      2.68\n      224\n      0.875\n      bicubic\n      regnetx_002.pycls_in1k\n      0.000056\n      regnetx\n    \n    \n      15\n      regnety_002\n      16673.08\n      61.405\n      1024\n      224\n      3.16\n      70.278\n      29.722\n      89.528\n      10.472\n      3.16\n      224\n      0.875\n      bicubic\n      regnety_002.pycls_in1k\n      0.000060\n      regnety\n    \n    \n      17\n      levit_128\n      14657.83\n      69.849\n      1024\n      224\n      9.21\n      78.490\n      21.510\n      94.012\n      5.988\n      9.21\n      224\n      0.900\n      bicubic\n      levit_128.fb_dist_in1k\n      0.000068\n      levit\n    \n    \n      18\n      regnetx_004\n      14440.03\n      70.903\n      1024\n      224\n      5.16\n      72.398\n      27.602\n      90.828\n      9.172\n      5.16\n      224\n      0.875\n      bicubic\n      regnetx_004.pycls_in1k\n      0.000069\n      regnetx\n    \n  \n\n\n\n\n\n# plot the data\nimport plotly.express as px\nw,h = 1000, 800\n\ndef show_all(df, title, size):\n    return px.scatter(df,\n                      width=w,\n                      height=h,\n                      size=df[size]**2,\n                      title=title,\n                      x='secs',\n                      y='top1',\n                      log_x=True,\n                      color='family',\n                      hover_name='model_org',\n                      hover_data=[size]\n                     )\n\nshow_all(df, 'Inference', 'infer_img_size')\n\n\n                                                \n\n\n\n# plot a subset of the data\nsubs = 'levit|resnetd?|regnetx|vgg|convnext.*|efficientnetv2|beit|swin'\n\ndef show_subs(df, title, size, subs):\n    df_subs = df[df.family.str.fullmatch(subs)]\n    return px.scatter(df_subs,\n                      width=w,\n                      height=h,\n                      size=df_subs[size]**2,\n                      title=title,\n                      trendline='ols',\n                      trendline_options={'log_x':True},\n                      x='secs',\n                      y='top1',\n                      log_x=True,\n                      color='family',\n                      hover_name='model_org',\n                      hover_data=[size])\n\nshow_subs(df, 'Inference', 'infer_img_size', subs)\n\n\n                                                \n\n\n\n# plot inference speed vs parameter count\npx.scatter(df,\n           width=w,\n           height=h,\n           x='param_count_x',\n           y='secs',\n           log_x=True,\n           log_y=True,\n           color='infer_img_size',\n           hover_name='model_org',\n           hover_data=['infer_samples_per_sec', 'family']\n)\n\n\n                                                \n\n\n\n# repeat plots for training data\ntdf = get_data('train', 'train_samples_per_sec')\nshow_all(tdf, 'Training', 'train_img_size')\n\n\n                                                \n\n\n\n# subset of training data\nshow_subs(tdf, 'Training', 'train_img_size', subs)\n\n\n                                                \n\n\n\n\n\nIn this section, I’ll recreate the content in Jeremy’s notebook here, where he walks through a quadratic example of training a function to match the data.\nA neural network layer:\n\nMultiplies each input by a number of values. These values are known as parameters.\nAdds them up for each group of values.\nReplaces the negative numbers with zeros.\n\n\n# helper functions\nfrom ipywidgets import interact\nfrom fastai.basics import *\n\n\n# helper functions\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\nIn the plot_function definition, I’ll look into why [:,None] is added after torch.linspace(min, max, 100)\n\ntorch.linspace(-1, 1, 10), torch.linspace(-1, 1, 10).shape\n\n(tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n          0.7778,  1.0000]),\n torch.Size([10]))\n\n\n\ntorch.linspace(-1, 1, 10)[:,None], torch.linspace(-1, 1, 10)[:,None].shape\n\n(tensor([[-1.0000],\n         [-0.7778],\n         [-0.5556],\n         [-0.3333],\n         [-0.1111],\n         [ 0.1111],\n         [ 0.3333],\n         [ 0.5556],\n         [ 0.7778],\n         [ 1.0000]]),\n torch.Size([10, 1]))\n\n\n[:, None] adds a dimension to the tensor.\nNext he fits a quadratic function to data:\n\ndef f(x): return 3*x**2 + 2*x + 1\n\nplot_function(f, '$3x^2 + 2x + 1$')\n\n\n\n\nIn order to simulate “finding” or “learning” the right model fit, he creates a general quadratic function:\n\ndef quad(a, b, c, x): return a*x**2 + b*x + c\n\nand uses partial to make new quadratic functions:\n\ndef mk_quad(a, b, c): return partial(quad, a, b, c)\n\n\n# recreating original quadratic with mk_quad\nf2 = mk_quad(3, 2, 1)\nplot_function(f2)\n\n\n\n\n\nf2\n\nfunctools.partial(<function quad at 0x148c6d000>, 3, 2, 1)\n\n\n\nquad\n\n<function __main__.quad(a, b, c, x)>\n\n\nNext he simulates noisy measurements of the quadratic f:\n\n# `scale` parameter is the standard deviation of the distribution\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n\n# noise function matches quadratic x + x^2 (with noise) + constant noise\ndef add_noise(x, mult, add): return x * (1+noise(x, mult)) + noise(x,add)\n\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:, None]\ny = add_noise(f(x), 0.15, 1.5)\n\n\n# values match Jeremy's\nx[:5], y[:5]\n\n(tensor([[-2.0000],\n         [-1.7895],\n         [-1.5789],\n         [-1.3684],\n         [-1.1579]]),\n tensor([[11.8690],\n         [ 6.5433],\n         [ 5.9396],\n         [ 2.6304],\n         [ 1.7947]], dtype=torch.float64))\n\n\n\nplt.scatter(x, y)\n\n<matplotlib.collections.PathCollection at 0x148e16320>\n\n\n\n\n\n\n# overlay data with variable quadratic\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x, y)\n    plot_function(mk_quad(a, b, c), ylim=(-3,13))\n\n\n\n\nImportant note changing sliders: only after changing b and c values do you realize that a also needs to be changed.\nNext, he creates a measure for how well the quadratic fits the data, mean absolute error (distance from each data point to the curve).\n\ndef mae(preds, acts): return (torch.abs(preds-acts)).mean()\n\n\n# update interactive plot\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\nIn a neural network we’ll have tens of millions or more parameters to fit and thousands or millions of data points to fit them to, which we can’t do manually with sliders. We need to automate this process.\nIf we know the gradient of our mae() function with respect to our parameters, a, b and c, then that means we know how adjusting a parameter will change the function. If, say, a has a negative gradient, then we know increasing a will decrease mae(). So we find the gradient of the parameters with respect to the loss function and adjust our parameters a bit in the opposite direction of the gradient sign.\nTo do this we need a function that will take the parameters as a single vector:\n\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n\n\n# testing it out\n# should equal 2.4219\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.4219, dtype=torch.float64)\n\n\n\n# pick an arbitrary starting point for our parameters\nabc = torch.tensor([1.1, 1.1, 1.1])\n\n# tell pytorch to calculate its gradients\nabc.requires_grad_()\n\n# calculate loss\nloss = quad_mae(abc)\nloss\n\ntensor(2.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n\n\n\n# calculate gradients\nloss.backward()\n\n# view gradients\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\n\n# increase parameters to decrease loss based on gradient sign\nwith torch.no_grad():\n    abc -= abc.grad*0.01\n    loss = quad_mae(abc)\n\nprint(f'loss={loss:.2f}')\n\nloss=2.40\n\n\nThe loss has gone down from 2.4219 to 2.40. We’re moving in the right direction.\nThe small number we multiply gradients by is called the learning rate and is the most important hyper-parameter to set when training a neural network.\n\n# use a loop to do a few more iterations\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.40\nstep=1; loss=2.36\nstep=2; loss=2.30\nstep=3; loss=2.21\nstep=4; loss=2.11\nstep=5; loss=1.98\nstep=6; loss=1.85\nstep=7; loss=1.72\nstep=8; loss=1.58\nstep=9; loss=1.46\n\n\nThe loss continues to decrease. Here are our parameters and their gradients at this stage:\n\nabc\n\ntensor([1.9634, 1.1381, 1.4100], requires_grad=True)\n\n\n\nabc.grad\n\ntensor([-13.4260,  -1.0842,  -4.5000])\n\n\nA neural network can approximate any computable function, given enough parameters using two key steps:\n\nMatrix multiplication.\nThe function \\(max(x,0)\\), which simply replaces all negative numbers with zero.\n\nThe combination of a linear function and \\(max\\) is called a rectified linear unit and can be written as:\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y, 0.)\n\n\nplot_function(partial(rectified_linear, 1, 1))\n\n\n\n\n\n# we can do the same thing using PyTorch\nimport torch.nn.functional as F\ndef rectified_linear2(m,b,x): return F.relu(m*x+b)\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\nCreate an interactive ReLU:\n\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m, b), ylim=(-1,4))\n\n\n\n\nObserve what happens when we add two ReLUs together:\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))\n\n\n\n\nCreating a triple ReLU function to fit our data:\n\ndef triple_relu(m1,b1,m2,b2,m3,b3,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x) + rectified_linear(m3,b3,x)\n\ndef mk_triple_relu(m1,b1,m2,b2,m3,b3): return partial(triple_relu, m1,b1,m2,b2,m3,b3)\n\n@interact(m1=-1.5, b1=-1.5, m2=0.5, b2=0.5, m3=1.5, b3=1.5)\ndef plot_double_relu(m1, b1, m2, b2, m3, b3):\n    f = mk_triple_relu(m1,b1,m2,b2,m3,b3)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\nThis same approach can be extended to functions with 2, 3, or more parameters. Drawing squiggly lines through some points is literally all that deep learning does. The above steps will, given enough time and enough data, create (for example) an owl recognizer if you feed it enough owls and non-owls.\nWe can could do thousands of computations on a GPU instead of the above CPU computation. We can greatly reduce the amount of computation and data needed by using a convolution instead of a matrix multiplication. We could make things much faster if, instead of starting with random parameters, we start with parameters of someone else’s model that does something similar to what we want (transfer learning).\n\n\n\nFollowing the instructions in the fastai course lesson video, I’ve created a Microsoft Excel deep learning model here for the Titanic Kaggle data.\nAs shown in the course video, I trained three different models—linear regression, neural net (using SUMPRODUCT) and neural net (using MMULT). After running Microsoft Excel’s Solver, I got the final (different than video) mean loss for each model:\n\nlinear: 0.14422715\nnnet: 0.14385956\nmmult: 0.14385956\n\nThe linear model loss in the video was about 0.10 and the neural net loss was about 0.08. So, my models didn’t do as well.\n\n\n\n\nIn this section, I’ll take notes while reading Chapter 4 in the fastai textbook.\n\n\n\nWe’ll use the MNIST dataset for our experiments, which contains handwritten digits.\nMNIST is collected by the National Institute of Standards and Technology and collated into a machine learning dataset by Yann Lecun who used MNIST in 1998 in LeNet-5, the first computer system to demonstrate practically useful recognition of handwritten digits.\nWe’ve seen that the only consisten trait among every fast.ai student who’s gone on to be a world-class practitioner is that they are all very tenacious.\nIn this chapter we’ll create a model that can classify any image as a 3 or a 7.\n\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:00<00:00]\n    \n    \n\n\n\n# ls method added by fastai\n# lists the count of items\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\n\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\n\n# 3 and 7 are the labels\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n\n(#6131) [Path('/root/.fastai/data/mnist_sample/train/3/10.png'),Path('/root/.fastai/data/mnist_sample/train/3/10000.png'),Path('/root/.fastai/data/mnist_sample/train/3/10011.png'),Path('/root/.fastai/data/mnist_sample/train/3/10031.png'),Path('/root/.fastai/data/mnist_sample/train/3/10034.png'),Path('/root/.fastai/data/mnist_sample/train/3/10042.png'),Path('/root/.fastai/data/mnist_sample/train/3/10052.png'),Path('/root/.fastai/data/mnist_sample/train/3/1007.png'),Path('/root/.fastai/data/mnist_sample/train/3/10074.png'),Path('/root/.fastai/data/mnist_sample/train/3/10091.png')...]\n\n\n\n# view one of the images\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\n\n# the image is stored as numbers\narray(im3)[4:10, 4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\n# same thing, but a PyTorch tensor\ntensor(im3)[4:10, 4:10]\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\n\n# use pandas.DataFrame to color code the array\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15, 4:22])\ndf.style.set_properties(**{'font-size': '6pt'}).background_gradient('Greys')\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      29\n      150\n      195\n      254\n      255\n      254\n      176\n      193\n      150\n      96\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      48\n      166\n      224\n      253\n      253\n      234\n      196\n      253\n      253\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      3\n      0\n      93\n      244\n      249\n      253\n      187\n      46\n      10\n      8\n      4\n      10\n      194\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      4\n      0\n      107\n      253\n      253\n      230\n      48\n      0\n      0\n      0\n      0\n      0\n      192\n      253\n      253\n      156\n      0\n      0\n      0\n    \n    \n      5\n      0\n      3\n      20\n      20\n      15\n      0\n      0\n      0\n      0\n      0\n      43\n      224\n      253\n      245\n      74\n      0\n      0\n      0\n    \n    \n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      249\n      253\n      245\n      126\n      0\n      0\n      0\n      0\n    \n    \n      7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14\n      101\n      223\n      253\n      248\n      124\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      11\n      166\n      239\n      253\n      253\n      253\n      187\n      30\n      0\n      0\n      0\n      0\n      0\n    \n    \n      9\n      0\n      0\n      0\n      0\n      0\n      16\n      248\n      250\n      253\n      253\n      253\n      253\n      232\n      213\n      111\n      2\n      0\n      0\n    \n    \n      10\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      43\n      98\n      98\n      208\n      253\n      253\n      253\n      253\n      187\n      22\n      0\n    \n  \n\n\n\nThe background white pixels are stored a the number 0, black is the number 255, and shades of grey between the two. The entire image contains 28 pixels across and 28 pixels down for a total of 768 pixels.\nHow might a computer recognize these two digits?\nIdeas:\n3s and 7s have distinct features. A seven has generally two straight lines at different angles, a three as two sets of curves stacked on each other. The point where the two curves intersect could be a recognizable feature of the the digit three. The point where the two straight-ish lines intersect could be a recognizable feature of the digit seven. One feature of confusion could be handwritten threes with a straight line at the top, similar to a seven. Another feature of confusion could be a handwritten 3 with a straight-ish ending stroke at the bottom, matching a similar stroke of a 7.\n\n\n\nIdea: find the average pixel value for every pixel of the 3s, then do the same for the 7s. To classify an image, see which of the two ideal digits the image is most similar to.\n\nBaseline: A simple model that you are confident should perform reasonably well. It should be simple to implement and easy to test, so that you can then test each of your improved ideas and make sure they are always better than your baseline. Without starting with a sensible baseline, it is difficult to know whether your super-fancy models are any good.\n\n\n# list comprehension of all digit images\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors), len(seven_tensors)\n\n(6131, 6265)\n\n\n\n# use fastai's show_image to display tensor images\nshow_image(three_tensors[1]);\n\n\n\n\nFor every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this, combine all the images in this list into a single three-dimensional tensor.\nWhen images are floats, the pixel values are expected to be between 0 and 1.\n\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\n# the length of a tensor's shape is its rank\n# rank is the number of axes and dimensions in a tensor\n# shape is the size of each axis of a tensor\nlen(stacked_threes.shape)\n\n3\n\n\n\n# rank of a tensor\nstacked_threes.ndim\n\n3\n\n\nWe calculate the mean of all the image tensors by taking the mean along dimension 0 of our stacked, rank-3 tensor. This is the dimension that indexes over all the images.\n\nmean3 = stacked_threes.mean(0)\nmean3.shape\n\ntorch.Size([28, 28])\n\n\n\nshow_image(mean3);\n\n\n\n\nThis is the ideal number 3 based on the dataset. It’s saturated where all the images agree it should be saturated (much of the background, the intersection of the two curves, and top and bottom curve), but it becomes wispy and blurry where the images disagree.\n\n# do the same for sevens\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\nHow would I calculate how similar a particular image is to each of our ideal digits?\nI would take the average of the absolute difference between each pixel’s intensity and the corresponding mean digit pixel intensity. The lower the average difference, the closer the digit is to the ideal digit.\n\n# sample 3\na_3 = stacked_threes[1]\nshow_image(a_3);\n\n\n\n\nL1 norm = Mean of the absolute value of differences.\nRoot mean squared error (RMSE) = square root of mean of the square of differences.\n\n# L1 norm\ndist_3_abs = (a_3 - mean3).abs().mean()\n\n# RMSE\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs, dist_3_sqr\n\n(tensor(0.1114), tensor(0.2021))\n\n\n\n# L1 norm\ndist_7_abs = (a_3 - mean7).abs().mean()\n\n# RMSE\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs, dist_7_sqr\n\n(tensor(0.1586), tensor(0.3021))\n\n\nFor both L1 norm and RMSE, the distance between the 3 and the “ideal” 3 is less than the distance to the ideal 7, so our simple model will give the right prediction in this case.\nBoth distances are provided in PyTorch:\n\nF.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt()\n\n(tensor(0.1586), tensor(0.3021))\n\n\nMSE = mean squared error.\nMSE will penalize bigger mistakes more heavily (and be lenient with small mistakes) than L1 norm.\n\n\n\nA NumPy array is a multidimensional table of data with all items of the same type.\njagged array: nested arrays of different sizes.\nIf the items of the array are all of simple type such as integer or float, NumPy will store them as a compact C data structure in memory.\nPyTorch tensors cannot be jagged. PyTorch tensors can live on the GPU. And can calculate their derivatives.\n\n# creating arrays and tensors\ndata = [[1,2,3], [4,5,6]]\narr = array(data)\ntns = tensor(data)\n\narr\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\ntns\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\n# select a row\ntns[1]\n\ntensor([4, 5, 6])\n\n\n\n# select a column\ntns[:,1]\n\ntensor([2, 5])\n\n\n\n# slice\ntns[1, 1:3]\n\ntensor([5, 6])\n\n\n\n# standard operators\ntns + 1\n\ntensor([[2, 3, 4],\n        [5, 6, 7]])\n\n\n\n# tensor type\ntns.type()\n\n'torch.LongTensor'\n\n\n\n# tensor changes type when needed\n(tns * 1.5).type()\n\n'torch.FloatTensor'\n\n\n\n\n\nmetric = a number that is calculated based on the predictions of our model and the correct labels in our dataset in order to tell us how good our model is.\nCalculate the metric on the validation set.\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\n\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\n\nvalid_3_tens.shape, valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\n# measure distance between image and ideal\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n\nmnist_distance(a_3, mean3)\n\ntensor(0.1114)\n\n\n\n# calculate mnist_distance for digit 3 validation images\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n(tensor([0.1109, 0.1202, 0.1276,  ..., 0.1357, 0.1262, 0.1157]),\n torch.Size([1010]))\n\n\nPyTorch broadcasts mean3 to each of the 1010 valid_3_dist tensors in order to calculate the distance. It doesn’t actually copy mean3 1010 times. It does the whole calculation in C (or CUDA for GPU).\nIn mean((-1, -2)), the tuple (-1, -2) represents a range of axes. This tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor—the horizontal and the vertical dimensions of an image.\nIf the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it’s a 3:\n\ndef is_3(x): return mnist_distance(x, mean3) < mnist_distance(x, mean7)\n\n\nis_3(a_3), is_3(a_3).float()\n\n(tensor(True), tensor(1.))\n\n\n\n# full validation set---thanks to broadcasting\nis_3(valid_3_tens)\n\ntensor([ True,  True,  True,  ..., False,  True,  True])\n\n\n\n# calculate accuracy\naccuracy_3s = is_3(valid_3_tens).float().mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s, accuracy_7s, (accuracy_3s + accuracy_7s) / 2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\nWe are getting more than 90% accuracy on both 3s and 7s. But they are very different looking digits and we’re classifying only 2 out of 10 digits, so we need to make a better model.\n\n\n\nArthur Samuel’s description of machine learning\n\nSuppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.\n\nOur pixel similarity approach doesn’t have any weight assignment, or any way of improving based on testing the effectiveness of a weight assignment. We can’t improve our pixel similarity approach.\nWe could look at each individual pixel and come up with a set of weights for each, such that the highest weights are associated with those pixels most likely to be black for a particular category. For example, pixels toward the bottom right are not very likely to be activate for a 7, so they should have a low weight for a 7, but ther are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category, for instance, the probability of being the number 8:\ndef pr_eight(x,w) = (x*w).sum()\nX is the image, represented as a vector (with all the rows stacked up end to end into a single long line) and the weights are a vector W. We need some way to update the weights to make them a little bit better. We want to find the specific values for the vector W that cause the result of our function to be high for those images that are 8s and low for those images that are not. Searching for the best vector W is a way to search for the best function for recognizing 8s.\nSteps required to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight how changing that weight would change the loss.\nStep (that is, change) all the weights based on that calculation.\nGo back to step 2 and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\nInitialize: Initialize parameters to random values.\nLoss: We need a function that will return a number that is small if the performance of the model is good (by convention).\nStep: Gradients allow us to directly figure out in which direction and by roughly how much to change each weight.\nStop: Keep training until the accuracy of the model started getting worse or we ran out of time, or once the number of epochs we decided are complete.\n\n\n\nCreate an example loss function:\n\ndef f(x): return x**2\n\nPick a tensor value at which we want gradients:\n\nxt = tensor(3.).requires_grad_()\n\n\nyt = f(xt)\nyt\n\ntensor(9., grad_fn=<PowBackward0>)\n\n\nCalculate gradients (backpropagation–during the backward pass of the network, as opposed to forward pass which is where the activations are calculated):\n\nyt.backward()\n\nView the gradients:\n\nxt.grad\n\ntensor(6.)\n\n\nThe derivative of x**2 is 2*x. When x = 3 the derivative is 6, as calculated above.\nCalculating vector gradients:\n\nxt = tensor([3., 4., 10.]).requires_grad_()\nxt\n\ntensor([ 3.,  4., 10.], requires_grad=True)\n\n\nAdd sum to our function so it takes a vector and returns a scalar:\n\ndef f(x): return (x**2).sum()\n\n\nyt = f(xt)\nyt\n\ntensor(125., grad_fn=<SumBackward0>)\n\n\n\nyt.backward()\nxt.grad\n\ntensor([ 6.,  8., 20.])\n\n\nIf the gradients are very large, that may suggest that we have more adjustments to do, whereas if they are very small, that may suggest that we are close to the optimal value.\n\n\n\nDeciding how to change our parameters based on the values of the gradients—multiplying the gradient by some small number called the learning rate (LR):\nw -= w.grad * lr\nThis is knowns as stepping your parameters using an optimization step.\nIf you pick a learning rate too low, that can mean having to do a lot of steps. If you pick a learning rate too high, that’s even worse, because it can result in the loss getting worse. If the learning rate is too high it may also “bounce” around.\n\n\n\nExample: measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, get slower as it went up the hill, and speed up again going downhill.\n\ntime = torch.arange(0,20).float(); time\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n        14., 15., 16., 17., 18., 19.])\n\n\n\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nspeed\n\ntensor([72.1328, 55.1778, 39.8417, 33.9289, 21.9506, 18.0992, 11.3346,  0.3637,\n         7.3242,  4.0297,  3.9236,  4.1486,  1.9496,  6.1447, 12.7890, 23.8966,\n        30.6053, 45.6052, 53.5180, 71.2243])\n\n\n\nplt.scatter(time, speed);\n\n\n\n\nWe added a bit of random noise since measuring things manually isn’t precise.\nWhat was the roller coaster’s speed? Using SGD, we can try to find a function that matches our observations. Guess that it will be a quadratic of the form a*(time**2) + (b*t) + c.\nWe want to distinguish clearly between the function’s input (the time when we are measuring the coaster’s speed) and its parameters (the values that define which quadratic we’re trying).\nCollect parameters in one argument and separate t and params in the function’s signature:\n\ndef f(t, params):\n  a,b,c = params\n  return a*(t**2) + (b*t) + c\n\nDefine a loss function:\n\ndef mse(preds, targets): return ((preds-targets)**2).mean()\n\nStep 1: Initialize the parameters\n\nparams = torch.randn(3).requires_grad_()\n\nStep 2: Calculate the predictions\n\npreds = f(time, params)\n\nCreate a little function to see how close our predictions are to our targets:\n\ndef show_preds(preds, ax=None):\n  if ax is None: ax=plt.subplots()[1]\n  ax.scatter(time, speed)\n  ax.scatter(time, to_np(preds), color='red')\n  ax.set_ylim(-300,100)\n\nshow_preds(preds)\n\n\n\n\nStep 3: Calculate the loss\n\nloss = mse(preds, speed)\nloss\n\ntensor(11895.1143, grad_fn=<MeanBackward0>)\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nparams.grad\n\ntensor([-35554.0117,  -2266.8909,   -171.8540])\n\n\n\nparams\n\ntensor([-0.5364,  0.6043,  0.4822], requires_grad=True)\n\n\nStep 5: Step the weights\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\nLet’s see if the loss has improved (it has) and take a look at the plot:\n\npreds = f(time, params)\nmse(preds, speed)\n\ntensor(2788.1594, grad_fn=<MeanBackward0>)\n\n\n\nshow_preds(preds)\n\n\n\n\nStep 6: Repeat the process\n\ndef apply_step(params, prn=True):\n  preds = f(time, params)\n  loss = mse(preds, speed)\n  loss.backward()\n  params.data -= lr * params.grad.data\n  params.grad = None\n  if prn: print(loss.item())\n  return preds\n\n\nfor i in range(10): apply_step(params)\n\n2788.159423828125\n1064.841552734375\n738.7333984375\n677.02001953125\n665.3380737304688\n663.1239013671875\n662.7010498046875\n662.6172485351562\n662.59765625\n662.5902709960938\n\n\n\n_, axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\nStep 7: Stop\nWe decided to stop after 10 epochs arbitrarily. In practice, we would watch the training and validation losses and our metrics to decide when to stop.\n\n\n\n\nAt the beginning, the weights of our model can be random (training from scratch) or come from a pretrained model (transfer learning).\nIn both cases the model will need to learn better weights.\nUse a loss function to compare model outputs to targets.\nChange the weights to make the loss a bit lower by multiple gradients by the learning rate and subtracting from the parameters.\nIterate until you have reached the lowest loss and then stop.\n\n\n\n\nConcatenate the images into a single tensor. view changes the shape of a tensor without changing its contents. -1 is a special parameter to view that means “make this axis as big as necessary to fit all the data”.\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\nUse the label 1 for 3s and 0 for 7s. Unsqueeze adds a dimension of size one.\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nPyTorch Dataset is required to return a tuple of (x,y) when indexed.\n\ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor([1]))\n\n\nPrepare the validation dataset:\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x, valid_y))\nx,y = valid_dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([1]))\n\n\nStep 1: Initialize the parameters\nWe need an initially random weight for every pixel.\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28,1))\nweights.shape\n\ntorch.Size([784, 1])\n\n\n\\(y = wx + b\\).\nWe created w (weights) now we need to create b (intercept or bias):\n\nbias = init_params(1)\nbias\n\ntensor([-0.0313], requires_grad=True)\n\n\nStep 2: Calculate the predictions\nPrediction for one image\n\n(train_x[0] * weights.T).sum() + bias\n\ntensor([0.5128], grad_fn=<AddBackward0>)\n\n\nIn Python, matrix multiplication is represetend with the @ operator:\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[ 0.5128],\n        [-3.8324],\n        [ 4.9791],\n        ...,\n        [ 3.0790],\n        [ 4.1521],\n        [ 0.3523]], grad_fn=<AddBackward0>)\n\n\nTo decide if an output represents a 3 or a 7, we can just check whether it’s greater than 0:\n\ncorrects = (preds>0.0).float() == train_y\ncorrects\n\ntensor([[ True],\n        [False],\n        [ True],\n        ...,\n        [False],\n        [False],\n        [False]])\n\n\n\ncorrects.float().mean().item()\n\n0.38964182138442993\n\n\nStep 3: Calculate the loss\nA very small change in the value of a weight will often not change the accuracy at all, and thus the gradient is 0 almost everywhere. It’s not useful to use accuracy as a loss function.\nWe need a loss function that when our weights result in slightly better predictions, gives us a slightly better loss.\nIn this case, what does “slightly better prediction mean”: if the correct answer is 3 (1), the score is a little higher, or if the correct answer is a 7 (0), the score is a little lower.\nThe loss function receives not the images themselves, but the predictions from the model.\nThe loss function will measure how distant each prediction is from 1 (if it should be 1) and how distant it is from 0 (if it should be 0) and then it will take the mean of all those distances.\n\ndef mnist_loss(predictions, targets):\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\nTry it out with sample predictions and targets:\n\ntrgts = tensor([1,0,1])\nprds = tensor([0.9, 0.4, 0.2])\ntorch.where(trgts==1, 1-prds, prds)\n\ntensor([0.1000, 0.4000, 0.8000])\n\n\nThis function returns a lower number when predictions are more accurate, when accurate predictions are more confident and when inaccurate predictions are less confident.\nSince we need a scalar for the final loss, mnist_loss takes the mean of the previous tensor:\n\nmnist_loss(prds, trgts)\n\ntensor(0.4333)\n\n\nmnist_loss assumes that predictions are between 0 and 1. We need to ensure that, using sigmoid, which always outputs a number between 0 and 1:\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\nIt’s also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. Update mnist+loss to first apply sigmoid to the inputs:\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\nWe already had a metric, which was overall accuracy. So why did we define a loss?\nTo drive automated learning, the loss must be a function that has a meaningful derivative. It can’t have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level.\nThe loss function is calculated for each item in our dataset, and then at the end of an epoch, the loss values are all averaged and the overall mean is reported for the epoch.\nIt is important that we focus on metrics, rather than the loss, when judging the performance of a model.\n\n\nThe optimization step: change or update the weights based on the gradients.\nTo take an optimization step, we need to calculate the loss over one or more data items. Calculating the loss for the whole dataset would take a long time, calculating it for a single item would not use much information so it would result in an imprecise and unstable gradient.\nCalculate the average loss for a few data items at a time (mini-batch). The number of data items in the mini-batch is called the batch-size.\nA larger batch size means you will get a more accurate and stable estimate of your dataset’s gradients from the loss function, but it will take longer and you will process fewer mini-batches per epoch. Using batches of data works well for GPUs, but give the GPU too many items at once and it will run out of memory.\nWe get better generalization if we can vary things during training (like performing data augmentation). One simple and effective thing we can vary is what data items we put in each mini-batch. Randomly shuffly the dataset before we create mini-batches. The DataLoader will do the shuffling and mini-batch collation for you:\n\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n\n[tensor([10,  3,  8, 11,  0]),\n tensor([6, 1, 7, 9, 4]),\n tensor([12, 13,  5,  2, 14])]\n\n\nFor training, we want a collection containing independent and dependent variables. A Dataset in PyTorch is a collection containing tuples of independent and dependent variables.\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\n\nlist(enumerate(string.ascii_lowercase))[:5]\n\n[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]\n\n\nWhen we pass a Dataset to a Dataloader we will get back many batches that are themselves tuples of tensors representing batches of independent and dependent variables:\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n\n[(tensor([24,  2,  4,  8,  9, 13]), ('y', 'c', 'e', 'i', 'j', 'n')),\n (tensor([23, 17,  6, 14, 25, 18]), ('x', 'r', 'g', 'o', 'z', 's')),\n (tensor([22,  5,  7, 20,  3, 19]), ('w', 'f', 'h', 'u', 'd', 't')),\n (tensor([ 0, 21, 12,  1, 16, 10]), ('a', 'v', 'm', 'b', 'q', 'k')),\n (tensor([11, 15]), ('l', 'p'))]\n\n\n\n\n\n\nIn code, the process will be implemented something like this for each epoch:\nfor x,y in dl:\n  # calculate predictions\n  pred = model(x)\n  # calculate the loss\n  loss = loss_func(pred, y)\n  # calculate the gradients\n  loss.backward()\n  # step the weights\n  parameters -= parameters.grad * lr\nStep 1: Initialize the parameters\n\nweights = init_params((28*28, 1))\nbias = init_params(1)\n\nA DataLoader can be created from a Dataset:\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\nDo the same for the validation set:\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nCreate a mini-batch of size 4 for testing:\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\npreds = linear1(batch)\npreds\n\ntensor([[10.4546],\n        [ 9.4603],\n        [-0.2426],\n        [ 6.7868]], grad_fn=<AddBackward0>)\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(0.1404, grad_fn=<MeanBackward0>)\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nweights.grad.shape, weights.grad.mean(), bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0089), tensor([-0.0619]))\n\n\nCreate a function to calculate gradients:\n\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  loss = mnist_loss(preds, yb)\n  loss.backward()\n\nTest it:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0178), tensor([-0.1238]))\n\n\nLook what happens when we call it again:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0267), tensor([-0.1857]))\n\n\nThe gradients have changed. loss.backward adds the gradients of loss to any gradients that are currently stored. So we have to set the current gradients to 0 first:\n\nweights.grad.zero_()\nbias.grad.zero_();\n\nMethods in PyTorch whose names end in an underscore modify their objects in place.\nStep 5: Step the weights\nWhen we update the weights and biases based on the gradient and learning rate, we have to tell PyTorch not to take the gradient of this step. If we assign to the data attribute of a tensor, PyTorch will not take the gradient of that step. Here’s our basic training loop for an epoch:\n\ndef train_epoch(model, lr, params):\n  for xb,yb in dl:\n    calc_grad(xb, yb, model)\n    for p in params:\n      p.data -= p.grad*lr\n      p.grad.zero_()\n\nWe want to check how we’re doing by looking at the accuracy of the validation set. To decide if an output represents a 3 (1) or a 7 (0) we can just check whether the prediction is greater than 0.\n\npreds, train_y[:4]\n\n(tensor([[10.4546],\n         [ 9.4603],\n         [-0.2426],\n         [ 6.7868]], grad_fn=<AddBackward0>),\n tensor([[1],\n         [1],\n         [1],\n         [1]]))\n\n\n\n(preds>0.0).float() == train_y[:4]\n\ntensor([[ True],\n        [ True],\n        [False],\n        [ True]])\n\n\n\n# if preds is greater than 0 and the label is 1 -> correct 3 prediction\n# if preds is not greater than 0 and the label is 0 -> correct 7 prediction\nTrue == 1, False == 0\n\n(True, True)\n\n\nCreate a function to calculate validation accuracy:\n\ndef batch_accuracy(xb, yb):\n  preds = xb.sigmoid()\n  correct = (preds>0.5) == yb\n  return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.7500)\n\n\nPut the batches back together:\n\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n  return round(torch.stack(accs).mean().item(), 4)\n\nStarting point accuracy:\n\nvalidate_epoch(linear1)\n\n0.5703\n\n\nLet’s train for 1 epoch and see if the accuracy improves:\n\nlr = 1.\nparams = weights, bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.6928\n\n\nStep 6: Repeat the process\nThen do a few more:\n\nfor i in range(20):\n  train_epoch(linear1, lr, params)\n  print(validate_epoch(linear1), end = ' ')\n\n0.852 0.9061 0.931 0.9418 0.9477 0.9569 0.9584 0.9594 0.9599 0.9633 0.9647 0.9652 0.9657 0.9662 0.9672 0.9677 0.9687 0.9696 0.9701 0.9696 \n\n\nWe’re already about at the same accuracy as our “pixel similarity” approach.\n\n\nReplace our linear function with PyTorch’s nn.Lienar module. A module is an object of a class that inherits from the PyTorch nn.Module class, and behaves identically to standard Python functions in that you can call them using parentheses and they will return the activations of a model.\nnn.Linear does the same thing as our init_params and linear together. It contains both weights and biases in a single class:\n\nlinear_model = nn.Linear(28*28, 1)\n\nEvery PyTorch module knows what parameters it has that can be trained; they are available through the parameters method:\n\nw,b = linear_model.parameters()\nw.shape, b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\nWe can use this information to create an optimizer:\n\nclass BasicOptim:\n  def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n  def step(self, *args, **kwargs):\n    for p in self.params: p.data -= p.grad.data * self.lr\n\n  def zero_grad(self, *args, **kwargs):\n    for p in self.params: p.grad = None\n\nWe can create our optimizer by passing in the model’s parameters:\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\nSimplify our training loop:\n\ndef train_epoch(model):\n  for xb,yb in dl:\n    # calculate the gradients\n    calc_grad(xb,yb,model)\n    # step the weights\n    opt.step()\n    opt.zero_grad()\n\nOur validation function doesn’t need to change at all:\n\nvalidate_epoch(linear_model)\n\n0.3985\n\n\nPut our training loop in a function:\n\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')\n\nSimilar results as the previous training:\n\ntrain_model(linear_model, 20)\n\n0.4932 0.7959 0.8506 0.9136 0.9341 0.9492 0.9556 0.9629 0.9658 0.9683 0.9702 0.9717 0.9741 0.9746 0.9761 0.9766 0.9775 0.978 0.9785 0.979 \n\n\nfastai provides the SGD class that by default does the same thing as our BasicOptim:\n\nlinear_model = nn.Linear(28*28, 1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n0.4932 0.8735 0.8174 0.9082 0.9331 0.9468 0.9546 0.9614 0.9653 0.9668 0.9692 0.9727 0.9736 0.9751 0.9756 0.9761 0.9775 0.978 0.978 0.9785 \n\n\nfastai provides Learner.fit which we can use instead of train_model. To create a Learner we first need to create a DataLoaders, by passing our training and validation DataLoaders:\n\ndls = DataLoaders(dl, valid_dl)\n\nTo create a Learner without using an application such as cnn_learner we need to pass in all the elements that we’ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:\n\nlearn = Learner(dls, nn.Linear(28*28, 1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.636474\n      0.503518\n      0.495584\n      00:00\n    \n    \n      1\n      0.550751\n      0.189374\n      0.840530\n      00:00\n    \n    \n      2\n      0.201501\n      0.178350\n      0.839549\n      00:00\n    \n    \n      3\n      0.087588\n      0.105257\n      0.912659\n      00:00\n    \n    \n      4\n      0.045719\n      0.076968\n      0.933759\n      00:00\n    \n    \n      5\n      0.029454\n      0.061683\n      0.947498\n      00:00\n    \n    \n      6\n      0.022817\n      0.052156\n      0.954367\n      00:00\n    \n    \n      7\n      0.019893\n      0.045825\n      0.962709\n      00:00\n    \n    \n      8\n      0.018424\n      0.041383\n      0.965653\n      00:00\n    \n    \n      9\n      0.017549\n      0.038113\n      0.967125\n      00:00\n    \n  \n\n\n\n\n\n\n\nAdding a nonlinearity between two linear classifiers givs us a neural network.\n\ndef simple_net(xb):\n  res = xb@w1 + b1\n  res = res.max(tensor(0.0))\n  res = res@w2 + b2\n  return res\n\n\n# initialize weights\nw1 = init_params((28*28, 30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nw1 has 30 output activations which means w2 must have 30 input activations so that they match. 30 output activations means that the first layer can construct 30 different features, each representing a different mix of pixels. You can change that 30 to anything you like to make the model more or less complex.\nres.max(tensor(0.0)) is called a rectified linear unit or ReLU. It replaces every negative number with a zero.\n\nplot_function(F.relu)\n\n\n\n\nWe need a nonlinearity becauase a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\nThe neural net can solve any computable problem to an arbitrarily high level of accuracy if you can find the right parameters w1 and w2 and if you make the matrices big enough.\nWe can replace our function with PyTorch:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nnn.Sequential create a modeule that will call each of the listed layers or functions in turn. When using nn.Sequential PyTorch requires us to use the module version (nn.ReLU) and not the function version (F.relu). Modules are classes so you have to instantiate them.\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.363529\n      0.409795\n      0.505888\n      00:00\n    \n    \n      1\n      0.165949\n      0.239534\n      0.792934\n      00:00\n    \n    \n      2\n      0.089140\n      0.117148\n      0.913150\n      00:00\n    \n    \n      3\n      0.056798\n      0.078107\n      0.941119\n      00:00\n    \n    \n      4\n      0.042071\n      0.060734\n      0.957311\n      00:00\n    \n    \n      5\n      0.034718\n      0.051121\n      0.962218\n      00:00\n    \n    \n      6\n      0.030605\n      0.045103\n      0.964181\n      00:00\n    \n    \n      7\n      0.027994\n      0.040995\n      0.966143\n      00:00\n    \n    \n      8\n      0.026145\n      0.037990\n      0.969087\n      00:00\n    \n    \n      9\n      0.024728\n      0.035686\n      0.970559\n      00:00\n    \n    \n      10\n      0.023585\n      0.033853\n      0.972522\n      00:00\n    \n    \n      11\n      0.022634\n      0.032346\n      0.973994\n      00:00\n    \n    \n      12\n      0.021826\n      0.031080\n      0.975466\n      00:00\n    \n    \n      13\n      0.021127\n      0.029996\n      0.976448\n      00:00\n    \n    \n      14\n      0.020514\n      0.029053\n      0.975957\n      00:00\n    \n    \n      15\n      0.019972\n      0.028221\n      0.976448\n      00:00\n    \n    \n      16\n      0.019488\n      0.027481\n      0.977920\n      00:00\n    \n    \n      17\n      0.019051\n      0.026818\n      0.978410\n      00:00\n    \n    \n      18\n      0.018654\n      0.026219\n      0.978410\n      00:00\n    \n    \n      19\n      0.018291\n      0.025677\n      0.978901\n      00:00\n    \n    \n      20\n      0.017958\n      0.025181\n      0.978901\n      00:00\n    \n    \n      21\n      0.017650\n      0.024727\n      0.980373\n      00:00\n    \n    \n      22\n      0.017363\n      0.024310\n      0.980864\n      00:00\n    \n    \n      23\n      0.017096\n      0.023925\n      0.980864\n      00:00\n    \n    \n      24\n      0.016846\n      0.023570\n      0.981845\n      00:00\n    \n    \n      25\n      0.016610\n      0.023241\n      0.982336\n      00:00\n    \n    \n      26\n      0.016389\n      0.022935\n      0.982336\n      00:00\n    \n    \n      27\n      0.016179\n      0.022652\n      0.982826\n      00:00\n    \n    \n      28\n      0.015980\n      0.022388\n      0.982826\n      00:00\n    \n    \n      29\n      0.015791\n      0.022142\n      0.982826\n      00:00\n    \n    \n      30\n      0.015611\n      0.021913\n      0.983317\n      00:00\n    \n    \n      31\n      0.015440\n      0.021700\n      0.983317\n      00:00\n    \n    \n      32\n      0.015276\n      0.021500\n      0.983317\n      00:00\n    \n    \n      33\n      0.015120\n      0.021313\n      0.983317\n      00:00\n    \n    \n      34\n      0.014969\n      0.021137\n      0.983317\n      00:00\n    \n    \n      35\n      0.014825\n      0.020972\n      0.983317\n      00:00\n    \n    \n      36\n      0.014686\n      0.020817\n      0.982826\n      00:00\n    \n    \n      37\n      0.014553\n      0.020671\n      0.982826\n      00:00\n    \n    \n      38\n      0.014424\n      0.020532\n      0.982826\n      00:00\n    \n    \n      39\n      0.014300\n      0.020401\n      0.982826\n      00:00\n    \n  \n\n\n\nYou can view the training process in learn.recorder:\n\nplt.plot(L(learn.recorder.values).itemgot(2))\n\n\n\n\nView the final accuracy:\n\nlearn.recorder.values[-1][2]\n\n0.982826292514801\n\n\nAt this point we have:\n\nA function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters.\nA way to find the best set of parameters for any function (stochastic gradient descent).\n\n\n\nWe can add as many layers in our neural network as we want, as long as we add a nonlinearity between each pair of linear layers.\nThe deeper the model gets, the harder it is to optimize the parameters.\nWith a deeper model (one with more layers) we do not need to use as many parameters. We can use smaller matrices with more layers and get better results than we would get with larger matrices and few layers.\nIn the 1990s what held back the field for years was that so few researchers were experimenting with more than one nonlinearity.\nTraining an 18-layer model:\n\ndls = ImageDataLoaders.from_folder(path)\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n/usr/local/lib/python3.10/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.098852\n      0.014919\n      0.996075\n      02:01\n    \n  \n\n\n\n\n\n\n\nActivations: Numbers that are calculated (both by linear and nonlinear layers)\nParameters: Numbers that are randomly initialized and optimized (that is, the numbers that define the model).\nPart of becoming a good deep learning practitioner is getting used to the idea of looking at your activations and parameters, and plotting the and testing whether they are behaving correctly.\nActivations and parameters are all contained in tensors. The number of dimensions of a tensor is its rank.\nA neural network contains a number of layers. Each layer is either linear or nonlinear. We generally alternate between these two kinds of layers in a neural network. Sometimes a nonlinearity is referred to as an activation function.\nKey concepts related to SGD:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nReLU\nFunction that returns 0 for negative numbers and doesn’t change positive numbers.\n\n\nMini-batch\nA small group of inputs and labels gathered together in two arrays. A gradient descent is updated on this batch (rather than a whole epoch).\n\n\nForward pass\nApplying the model to some input and computing the predictions.\n\n\nLoss\nA value that represents how well or badly our model is doing.\n\n\nGradient\nThe derivative of the loss with respect to some parameter of the model.\n\n\nBackward pass\nComputing the gradients of the loss with respect to all model parameters.\n\n\nGradient descent\nTaking a step in the direction opposite to the gradients to make the model parameters a little bit better.\n\n\nLearning rate\nThe size of the step we take when applying SGD to update the parameters of the model.\n\n\n\n\n\n\n1. How is a grayscale image represented on a computer? How about a color image?\nGrayscale image pixels can be 0 (black) to 255 (white). Color image pixels have three values (Red, Green, Blue) where each value can be from 0 to 255.\n2. How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\nMNIST_SAMPLE path has a labels.csv file, a train folder, and a valid folder.\n\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\nThe train folder has a 3 and a 7 folder, each which contains training images.\n\n(path/'valid').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/valid/3'),Path('/root/.fastai/data/mnist_sample/valid/7')]\n\n\nThe valid folder contains a 3 and a 7 folder, each containing validation set images.\n3. Explain how the “pixel similarity” approach to classifying digits works.\nPixel similarity works by calculating the absolute mean difference (L1 norm) between each image and the mean digit 3, and averaging the classification (if the absolute mean difference between the image and the ideal 3 is less than the absolute mean difference between the image and the ideal 7, it’s classified as a 3) across all images of each digit’s validation set as the accuracy of the model.\n4. What is list comprehension? Create one now that selects odd numbers from a list and doubles them.\nList comprehension is syntax for creating a new list based on another sequence or iterable (docs)\n\n# for each element in range(10)\n# if the modulo of the element and 2 is not 0\n# double the element's value and store in this new list\ndoubled_odds = [2*elem for elem in range(10) if elem % 2 != 0]\ndoubled_odds\n\n[2, 6, 10, 14, 18]\n\n\n5. What is a rank-3 tensor?\nA rank-3 tensor is a “cube” (3-dimensional tensor).\n6. What is the difference between tensor rank and shape? How do you get the rank from the shape?\nTensor rank is the number of dimensions of the tensor. Tensor shape is the number of elements in each dimension. The following tensor is a 2-dimensional tensor with rank 2, the shape of which is 3 elements by 2 elements.\n\na_tensor = tensor([[1,3], [4,5], [5,6]])\n# dim == rank\na_tensor.dim(), a_tensor.shape\n\n(2, torch.Size([3, 2]))\n\n\n7. What are RMSE and L1 norm?\nRMSE = Root Mean Squared Error: The square root of the mean of squared differences between two sets of values.\nL1 norm = mean absolute difference: the mean of the absolute value of differences between two sets of values.\n8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\nYou can do so by using tensors on a GPU.\n9. Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom four numbers.\n\na_tensor = tensor([[1,2,3], [4,5,6], [7,8,9]])\na_tensor\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\n\na_tensor = 2 * a_tensor\na_tensor\n\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12],\n        [14, 16, 18]])\n\n\n\na_tensor.view(-1, 9)[0,-4:]\n\ntensor([12, 14, 16, 18])\n\n\n10. What is broadcasting? Broadcasting is when a tensor of smaller rank (or a scalar) is expanded so that you can perform an operation between it and a tensor of larger rank. Broadcasting makes it so that the two operands have the same rank.\n\na_tensor + tensor([1,2,3])\n\ntensor([[ 3,  6,  9],\n        [ 9, 12, 15],\n        [15, 18, 21]])\n\n\n\nAre metrics generally calculated using the training set or the validation set? Why?\n\nMetrics are calculated on the validation set because since that is the data the model does not see during training, the metric tells you how your model performs on data it hasn’t seen before.\n12. What is SGD?\nSGD is Stochastic Gradient Descent, an automated process where a model learns the right parameters needed to solve problems like image classification. The randomly (from scratch) or pretrained (transfer learning) parameters are updated using their gradients with respect to the loss and the learning rate. Metrics like the accuracy measure how well the model is performing.\n13. Why does SGD use mini-batches?\nOne reason is to utilize the ability of a GPU to process a lot of data at once.\nAnother reason is that calculating the loss one image at a time leads to an unstable loss function whereas calculating the loss on the entire dataset takes too long. Mini-batches fall in between these two extremes.\n14. What are the seven steps in SGD for machine learning?\n\nInitialize the weights.\nCalculate the predictions.\nCalculate the loss.\nCalculate gradients.\nStep the weights.\nRepeat the process.\nStop.\n\n15. How do we initialize the weights in a model?\nEither randomly (if training from scratch) or using pretrained weights (if transfer learning from an existing model like resnet18).\n16. What is loss?\nA machine-friendly way to measure how well (or badly) the model is performing. The model is learning to step the weights in order to decrease the loss.\n17. Why can’t we always use a high learning rate?\nBecause we risk overshooting the minimum loss (getting stuck back and forth between the two sides of the parabola) or diverging (resulting in larger losses each step).\n18. What is a gradient?\nThe rate of change or derivative of one variable with respect to another variable. In our case, gradients are the ratio of change in loss to change in parameter at one point.\n19. Do you need to know how to calculate gradients yourself?\nNope! Although you should understand the basic concept of derivatives. PyTorch calculates gradients with the .backward method.\n20. Why can’t we use accuracy as a loss function?\nBecause small changes in predictions do not result in small changes in accuracy. Accuracy drastically jumps (from 0 to 1 in our MNIST_SAMPLE example) at one point, with 0 slope elsewhere. We want a smooth function where you can calculate non-zero and non-infinite derivatives everywhere.\n21. Draw the sigmoid function. What is special about its shape?\nThe sigmoid function outputs between 0 and 1 for input values going from -inf to +inf. It also has a smooth positive slope everywhere so it’s easy to take the derivate.\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n22. What is the difference between a loss function and a metric?\nThe loss function is a machine-friendly way to measure the performance of the model while a metric is a human-friendly way to do the same.\nThe purpose of the loss function is to provide a smooth function to take derivates over so the training system can change the weights little by little towards the optimum.\nThe purpose of the metric is to inform the human how well or badly the model is learning during training.\n23. What is the function to calculate new weights using a learning rate?\nIn code, the function is:\nparameters.data -= parameters.grad * lr\nThe new weights are stepped incrementally in the opposite direction of the gradients. If the gradient is negative, the weights will be increased. If the gradient is positive, the weights will be decreased.\n24. What does the DataLoader class do?\nThe DataLoader class prepares training and validation batches and feeds them to the GPU during training. It also performs any necessary item_tfms or batch_tfms to the data.\n25. Write pseudocode showing the basic steps taken in each epoch for SGD.\ndef train_epoch(model):\n  # calculate predictions\n  preds = model(xb)\n  # calculate the loss\n  loss = loss_func(preds, targets)\n  # calculate gradients\n  loss.backward()\n  # step the weights\n  params.data -= params.grad * lr\n  # reset the gradients\n  params.zero_grad_()\n  # calculate accuracy\n  acc = tensor([accuracy for each batch]).mean()\n\nCreate a function that, if passed two arguments [1, 2, 3, 4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n\n\ndef zipped_tuples(x, y): return list(zip(x,y))\n\n\nzipped_tuples([1,2,3,4], 'abcd')\n\n[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n\n\nThe output data structure is the same structure as the PyTorch Dataset.\n27. What does view do in PyTorch?\nview changes the rank and shape of the tensor.\n\ntensor([1,2,3],[4,5,6]).view(3,2)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntensor([1,2,3],[4,5,6]).view(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n28. What are the bias parameters in a neural network? Why do we need them?\nThe bias parameters are the intercept \\(b\\) in the function \\(y = wx + b\\). We need them for situations where the inputs are 0 (since \\(w*0 = 0\\)). Bias also helps to create a more flexible function (source).\n29. What does the @ operator do in Python?\nMatrix multiplication.\n\nv1 = tensor([1,2,3])\nv2 = tensor([4,5,6])\nv1 @ v2\n\ntensor(32)\n\n\n30. What does the backward method do?\nCalculate the gradients of the loss function with respect to the parameters.\n31. Why do we have to zero the gradients?\nEach time you call .backward PyTorch will add the new gradients to the current gradients, so we need to zero the gradients to prevent them from accumulating.\n32. What information do we have to pass to Learner?\nReference:\nLearner(dls, simple_net, opt_func=SGD,\n            loss_func=mnist_loss, metrics=batch_accuracy)\nWe pass to the Learner:\n\nDataLoaders containing training and validation sets.\nThe model we want to train.\nAn optimizer function.\nA loss function.\nAny metrics we want calculated.\n\n33. Show Python or pseudocode for the basic steps of a training loop.\nSee #25.\n34. What is ReLU? Draw a plot for it for values from -2 to +2.\nReLU is Rectified Linear Unit. It’s a function where if the inputs are negative, they are set to zero, and if the inputs are positive, they are kept as is.\n\nplot_function(F.relu, min=-2, max=2)\n\n\n\n\n35. What is an activation function?\nAn activation function is the function that produces our predictions (in our case, a neural net with linear and nonlinear layers). Sometimes the ReLU is referred to as the activation function.\n36. What’s the difference between F.relu and nn.ReLU?\nF.relu is a function whereas nn.ReLU is a class that needs to be instantiated.\n37. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why wo we normally use more?\nUsing more layers results in more accurate models.\n\n\n\nSince this lesson’s Further Research was so intensive, I decided to create separate blog posts for each one:\n\nImplementing a fastai Learner from Scratch\nImplementing an MNIST Classifier from Scratch\n\n\n\n\n\n\nAs recommended at the end of the lesson 3 video, I will read + run through the code from Jeremy’s notebook Getting started with NLP for absolute beginners before starting lesson 4.\n\nIn this notebook we’ll see how to solve the Patent Phrase Matching problem by treating it as a classification task, by representing it in a very similar way to that shown above.\n\n\n\n\n\n\n!pip install kaggle\n\n\n! pip install -q datasets\n\n\n! pip install transformers[sentencepiece]\n\n\n!pip install accelerate -U\n\n\n# for working with paths in Python, I recommend using `pathlib.Path`\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\npath = Path('us-patent-phrase-to-phrase-matching')\n\n\nimport zipfile,kaggle\nkaggle.api.competition_download_cli(str(path))\nzipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading us-patent-phrase-to-phrase-matching.zip to /content\n\n\n100%|██████████| 682k/682k [00:00<00:00, 750kB/s]\n\n\n\n\n\n\n\n\n\n!ls {path}\n\nsample_submission.csv  test.csv  train.csv\n\n\n\n\n\n\nimport pandas as pd\n\n\ndf = pd.read_csv(path/'train.csv')\n\n\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n      score\n    \n  \n  \n    \n      0\n      37d61fd2272659b1\n      abatement\n      abatement of pollution\n      A47\n      0.50\n    \n    \n      1\n      7b9652b17b68b7a4\n      abatement\n      act of abating\n      A47\n      0.75\n    \n    \n      2\n      36d72442aefd8232\n      abatement\n      active catalyst\n      A47\n      0.25\n    \n    \n      3\n      5296b0c19e1ce60e\n      abatement\n      eliminating process\n      A47\n      0.50\n    \n    \n      4\n      54c1e3b9184cb5b6\n      abatement\n      forest region\n      A47\n      0.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      36468\n      8e1386cbefd7f245\n      wood article\n      wooden article\n      B44\n      1.00\n    \n    \n      36469\n      42d9e032d1cd3242\n      wood article\n      wooden box\n      B44\n      0.50\n    \n    \n      36470\n      208654ccb9e14fa3\n      wood article\n      wooden handle\n      B44\n      0.50\n    \n    \n      36471\n      756ec035e694722b\n      wood article\n      wooden material\n      B44\n      0.75\n    \n    \n      36472\n      8d135da0b55b8c88\n      wood article\n      wooden substrate\n      B44\n      0.50\n    \n  \n\n36473 rows × 5 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nDataset description\n\ndf.describe(include='object')\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n    \n  \n  \n    \n      count\n      36473\n      36473\n      36473\n      36473\n    \n    \n      unique\n      36473\n      733\n      29340\n      106\n    \n    \n      top\n      37d61fd2272659b1\n      component composite coating\n      composition\n      H01\n    \n    \n      freq\n      1\n      152\n      24\n      2186\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nIn the describe output, freq is the number of rows with the top value in a given column.\n\ndf.query('anchor == \"component composite coating\"').shape\n\n(152, 5)\n\n\nStructure the input data:\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\ndf.input.head()\n\n0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\nName: input, dtype: object\n\n\n\n\n\nTransformers use a Dataset object for storing a dataset. We can create one like so:\n\nfrom datasets import Dataset, DatasetDict\n\nds = Dataset.from_pandas(df)\n\n\nds\n\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n    num_rows: 36473\n})\n\n\nA deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n\nTokenization: Split each text up into words (tokens).\nNumericalization: Convert each word (or token) into a number.\n\nThe details on how this is done depends on the model. So pick a model first:\n\nmodel_nm = 'microsoft/deberta-v3-small'\n\nAutoTokenizer will create a tokenizer appropriate for a given model:\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\nHere’s an example of how the tokenizer splits a text into “tokens” (which are like words, but can be sub-word pieces):\n\ntokz.tokenize(\"G'day folks, I'm Jeremy from fast.ai!\")\n\n['▁G',\n \"'\",\n 'day',\n '▁folks',\n ',',\n '▁I',\n \"'\",\n 'm',\n '▁Jeremy',\n '▁from',\n '▁fast',\n '.',\n 'ai',\n '!']\n\n\nUncommon words will be split into pieces. The start of a new word is represented by _.\n\ntokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")\n\n['▁A',\n '▁platypus',\n '▁is',\n '▁an',\n '▁or',\n 'ni',\n 'tho',\n 'rhynch',\n 'us',\n '▁an',\n 'at',\n 'inus',\n '.']\n\n\nHere’s a simple function which tokenizes our inputs:\n\ndef tok_func(x): return tokz(x[\"input\"])\n\nTo run this quickly in parallel on every row in our dataset, use map:\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\nThis adds a new item to our dataset called input_ids. For instance, here is the input and IDs for the first row of our data:\n\nrow = tok_ds[0]\nrow['input'], row['input_ids']\n\n('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n [1,\n  54453,\n  435,\n  294,\n  336,\n  5753,\n  346,\n  54453,\n  445,\n  294,\n  47284,\n  265,\n  6435,\n  346,\n  23702,\n  435,\n  294,\n  47284,\n  2])\n\n\nThere’s a list called vocab in the tokenizer which contains a unique integer for every possible token string. We can look them up like this, for instance to find the token for the word “of”:\n\ntokz.vocab['▁of']\n\n265\n\n\n265 is present in our input_ids for the first row of data.\n\ntokz.vocab['of']\n\n1580\n\n\nFinally, we need to prepare our labels. Transformers always assumes that your labels has the column name labels, but in our dataset it’s currently score. Therefore, we need to rename it:\n\ntok_ds = tok_ds.rename_columns({'score':'labels'})\n\n\n\n\n\neval_df = pd.read_csv(path/'test.csv')\neval_df.describe()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n    \n  \n  \n    \n      count\n      36\n      36\n      36\n      36\n    \n    \n      unique\n      36\n      34\n      36\n      29\n    \n    \n      top\n      4112d61851461f60\n      el display\n      inorganic photoconductor drum\n      G02\n    \n    \n      freq\n      1\n      2\n      1\n      3\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nThis is the test set. Possibly the most important idea in machine learning is that of having separate training, validation, and test data sets.\n\n\nTo explain the motivation, let’s start simple, and imagine we’re trying to fit a model where the true relationship is this quadratic:\n\ndef f(x): return -3*x**2 + 2*x + 20\n\nUnfortunately matplotlib (the most common library for plotting in Python) doesn’t come with a way to visualize a function, so we’ll write something to do this ourselves:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_function(f, min=-2.1, max=2.1, color='r'):\n    x = np.linspace(min,max, 100)[:,None]\n    plt.plot(x, f(x), color)\n\n\nplot_function(f)\n\n\n\n\nFor instance, perhaps we’ve measured the height above ground of an object before and after some event. The measurements will have some random error. We can use numpy’s random number generator to simulate that. I like to use seed when writing about simulations like this so that I know you’ll see the same thing I do:\n\nfrom numpy.random import normal,seed,uniform\nnp.random.seed(42)\n\n\ndef noise(x, scale): return normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\n\nx = np.linspace(-2, 2, num=20)[:,None]\ny = add_noise(f(x), 0.2, 1.3)\nplt.scatter(x,y);\n\n\n\n\nNow let’s see what happens if we underfit or overfit these predictions. To do that, we’ll create a function that fits a polynomial of some degree (e.g. a line is degree 1, quadratic is degree 2, cubic is degree 3, etc). The details of how this function works don’t matter too much so feel free to skip over it if you like!\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\ndef plot_poly(degree):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(x, y)\n    plt.scatter(x,y)\n    plot_function(model.predict)\n\n\nplot_poly(1)\n\n\n\n\nAs you see, the points on the red line (the line we fitted) aren’t very close at all. This is under-fit – there’s not enough detail in our function to match our data.\nAnd what happens if we fit a degree 10 polynomial to our measurements?\n\nplot_poly(10)\n\n\n\n\nWell now it fits our data better, but it doesn’t look like it’ll do a great job predicting points other than those we measured – especially those in earlier or later time periods. This is over-fit – there’s too much detail such that the model fits our points, but not the underlying process we really care about.\nLet’s try a degree 2 polynomial (a quadratic), and compare it to our “true” function (in blue):\n\nplot_poly(2)\nplot_function(f, color='b')\n\n\n\n\nThat’s not bad at all!\nSo, how do we recognise whether our models are under-fit, over-fit, or “just right”? We use a validation set. This is a set of data that we “hold out” from training – we don’t let our model see it at all. If you use the fastai library, it automatically creates a validation set for you if you don’t have one, and will always report metrics (measurements of the accuracy of a model) using the validation set.\nThe validation set is only ever used to see how we’re doing. It’s never used as inputs to training the model.\nTransformers uses a DatasetDict for holding your training and validation sets. To create one that contains 25% of our data for the validation set, and 75% for the training set, use train_test_split:\n\ndds = tok_ds.train_test_split(0.25, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 27354\n    })\n    test: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9119\n    })\n})\n\n\nAs you see above, the validation set here is called test and not validate, so be careful!\nIn practice, a random split like we’ve used here might not be a good idea – here’s what Dr Rachel Thomas has to say about it:\n\n“One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a train_test_split method, this method takes a random subset of the data, which is a poor choice for many real-world problems.”\n\n\n\n\nSo that’s the validation set explained, and created. What about the “test set” then – what’s that for?\nThe test set is yet another dataset that’s held out from training. But it’s held out from reporting metrics too! The accuracy of your model on the test set is only ever checked after you’ve completed your entire training process, including trying different models, training methods, data processing, etc.\nYou see, as you try all these different things, to see their impact on the metrics on the validation set, you might just accidentally find a few things that entirely coincidentally improve your validation set metrics, but aren’t really better in practice. Given enough time and experiments, you’ll find lots of these coincidental improvements. That means you’re actually over-fitting to your validation set!\nThat’s why we keep a test set held back. Kaggle’s public leaderboard is like a test set that you can check from time to time. But don’t check too often, or you’ll be even over-fitting to the test set!\nKaggle has a second test set, which is yet another held-out dataset that’s only used at the end of the competition to assess your predictions. That’s called the “private leaderboard”.\nWe’ll use eval as our name for the test set, to avoid confusion with the test dataset that was created above.\n\neval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\neval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n\n\n\n\n\n\n\n\nWhen we’re training a model, there will be one or more metrics that we’re interested in maximising or minimising. These are the measurements that should, hopefully, represent how well our model will works for us.\nIn real life, outside of Kaggle, things not easy… As my partner Dr Rachel Thomas notes in The problem with metrics is a big problem for AI:\n\nAt their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so. This is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.\n\nIn Kaggle, however, it’s very straightforward to know what metric to use: Kaggle will tell you! According to this competition’s evaluation page, “submissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores.” This coefficient is usually abbreviated using the single letter r. It is the most widely used measure of the degree of relationship between two variables.\nr can vary between -1, which means perfect inverse correlation, and +1, which means perfect positive correlation. The mathematical formula for it is much less important than getting a good intuition for what the different values look like. To start to get that intuition, let’s look at some examples using the California Housing dataset, which shows “is the median house value for California districts, expressed in hundreds of thousands of dollars”. This dataset is provided by the excellent scikit-learn library, which is the most widely used library for machine learning outside of deep learning.\n\nfrom sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing(as_frame=True)\nhousing = housing['data'].join(housing['target']).sample(1000, random_state=52)\nhousing.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      MedInc\n      HouseAge\n      AveRooms\n      AveBedrms\n      Population\n      AveOccup\n      Latitude\n      Longitude\n      MedHouseVal\n    \n  \n  \n    \n      7506\n      3.0550\n      37.0\n      5.152778\n      1.048611\n      729.0\n      5.062500\n      33.92\n      -118.28\n      1.054\n    \n    \n      4720\n      3.0862\n      35.0\n      4.697897\n      1.055449\n      1159.0\n      2.216061\n      34.05\n      -118.37\n      3.453\n    \n    \n      12888\n      2.5556\n      24.0\n      4.864905\n      1.129222\n      1631.0\n      2.395007\n      38.66\n      -121.35\n      1.057\n    \n    \n      13344\n      3.0057\n      32.0\n      4.212687\n      0.936567\n      1378.0\n      5.141791\n      34.05\n      -117.64\n      0.969\n    \n    \n      7173\n      1.9083\n      42.0\n      3.888554\n      1.039157\n      1535.0\n      4.623494\n      34.05\n      -118.19\n      1.192\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nWe can see all the correlation coefficients for every combination of columns in this dataset by calling np.corrcoef:\n\nnp.set_printoptions(precision=2, suppress=True)\n\nnp.corrcoef(housing, rowvar=False)\n\narray([[ 1.  , -0.12,  0.43, -0.08,  0.01, -0.07, -0.12,  0.04,  0.68],\n       [-0.12,  1.  , -0.17, -0.06, -0.31,  0.  ,  0.03, -0.13,  0.12],\n       [ 0.43, -0.17,  1.  ,  0.76, -0.09, -0.07,  0.12, -0.03,  0.21],\n       [-0.08, -0.06,  0.76,  1.  , -0.08, -0.07,  0.09,  0.  , -0.04],\n       [ 0.01, -0.31, -0.09, -0.08,  1.  ,  0.16, -0.15,  0.13,  0.  ],\n       [-0.07,  0.  , -0.07, -0.07,  0.16,  1.  , -0.16,  0.17, -0.27],\n       [-0.12,  0.03,  0.12,  0.09, -0.15, -0.16,  1.  , -0.93, -0.16],\n       [ 0.04, -0.13, -0.03,  0.  ,  0.13,  0.17, -0.93,  1.  , -0.03],\n       [ 0.68,  0.12,  0.21, -0.04,  0.  , -0.27, -0.16, -0.03,  1.  ]])\n\n\nThis works well when we’re getting a bunch of values at once, but it’s overkill when we want a single coefficient:\n\nnp.corrcoef(housing.MedInc, housing.MedHouseVal)\n\narray([[1.  , 0.68],\n       [0.68, 1.  ]])\n\n\nTherefore, we’ll create this little function to just return the single number we need given a pair of variables:\n\ndef corr(x,y): return np.corrcoef(x,y)[0][1]\n\ncorr(housing.MedInc, housing.MedHouseVal)\n\n0.6760250732906\n\n\nNow we’ll look at a few examples of correlations, using this function (the details of the function don’t matter too much):\n\ndef show_corr(df, a, b):\n    x,y = df[a],df[b]\n    plt.scatter(x,y, alpha=0.5, s=4)\n    plt.title(f'{a} vs {b}; r: {corr(x, y):.2f}')\n\n\nshow_corr(housing, 'MedInc', 'MedHouseVal')\n\n\n\n\nSo that’s what a correlation of 0.68 looks like. It’s quite a close relationship, but there’s still a lot of variation. (Incidentally, this also shows why looking at your data is so important – we can see clearly in this plot that house prices above $500,000 seem to have been truncated to that maximum value).\nLet’s take a look at another pair:\n\nshow_corr(housing, 'MedInc', 'AveRooms')\n\n\n\n\nThe relationship looks like it is similarly close to the previous example, but r is much lower than the income vs valuation case. Why is that? The reason is that there are a lot of outliers – values of AveRooms well outside the mean.\nr is very sensitive to outliers. If there’s outliers in your data, then the relationship between them will dominate the metric. In this case, the houses with a very high number of rooms don’t tend to be that valuable, so it’s decreasing r from where it would otherwise be.\nLet’s remove the outliers and try again:\n\nsubset = housing[housing.AveRooms<15]\nshow_corr(subset, 'MedInc', 'AveRooms')\n\n\n\n\nAs we expected, now the correlation is very similar to our first comparison.\nHere’s another relationship using AveRooms on the subset:\n\nshow_corr(subset, 'MedHouseVal', 'AveRooms')\n\n\n\n\nAt this level, with r of 0.34, the relationship is becoming quite weak.\nLet’s look at one more:\n\nshow_corr(subset, 'HouseAge', 'AveRooms')\n\n\n\n\nAs you see here, a correlation of -0.2 shows a very weak negative trend.\nWe’ve seen now examples of a variety of levels of correlation coefficient, so hopefully you’re getting a good sense of what this metric means.\nTransformers expects metrics to be returned as a dict, since that way the trainer knows what label to use, so let’s create a function to do that:\n\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}\n\n\n\n\nTo train a model in Transformers we’ll need this:\n\nfrom transformers import TrainingArguments,Trainer\n\nWe pick a batch size that fits our GPU, and small number of epochs so we can run experiments quickly:\n\nbs = 128\nepochs = 4\n\nThe most important hyperparameter is the learning rate. fastai provides a learning rate finder to help you figure this out, but Transformers doesn’t, so you’ll just have to use trial and error. The idea is to find the largest value you can, but which doesn’t result in training failing.\n\nlr = 8e-5\n\nTransformers uses the TrainingArguments class to set up arguments. Don’t worry too much about the values we’re using here – they should generally work fine in most cases. It’s just the 3 parameters above that you may need to change for different models.\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n\nWe can now create our model, and Trainer, which is a class which combines the data and model together (just like Learner in fastai):\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=corr_d)\n\n\n\n\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nLet’s train our model!\n\ntrainer.train();\n\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n\n    \n      \n      \n      [856/856 03:28, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.032255\n      0.790911\n    \n    \n      2\n      No log\n      0.023222\n      0.814958\n    \n    \n      3\n      0.040500\n      0.022491\n      0.828246\n    \n    \n      4\n      0.040500\n      0.023501\n      0.828109\n    \n  \n\n\n\nThe key thing to look at is the “Pearson” value in table above. As you see, it’s increasing, and is already above 0.8. That’s great news! We can now submit our predictions to Kaggle if we want them to be scored on the official leaderboard. Let’s get some predictions on the test set:\n\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds\n\n\n\n\narray([[ 0.58],\n       [ 0.69],\n       [ 0.57],\n       [ 0.33],\n       [-0.01],\n       [ 0.5 ],\n       [ 0.55],\n       [-0.01],\n       [ 0.31],\n       [ 1.15],\n       [ 0.29],\n       [ 0.24],\n       [ 0.76],\n       [ 0.91],\n       [ 0.75],\n       [ 0.43],\n       [ 0.33],\n       [-0.01],\n       [ 0.66],\n       [ 0.33],\n       [ 0.46],\n       [ 0.26],\n       [ 0.18],\n       [ 0.22],\n       [ 0.59],\n       [-0.04],\n       [-0.02],\n       [ 0.01],\n       [-0.03],\n       [ 0.59],\n       [ 0.3 ],\n       [-0.  ],\n       [ 0.68],\n       [ 0.52],\n       [ 0.47],\n       [ 0.23]])\n\n\nLook out - some of our predictions are <0, or >1! This once again shows the value of remember to actually look at your data. Let’s fix those out-of-bounds predictions:\n\npreds = np.clip(preds, 0, 1)\n\n\npreds\n\narray([[0.58],\n       [0.69],\n       [0.57],\n       [0.33],\n       [0.  ],\n       [0.5 ],\n       [0.55],\n       [0.  ],\n       [0.31],\n       [1.  ],\n       [0.29],\n       [0.24],\n       [0.76],\n       [0.91],\n       [0.75],\n       [0.43],\n       [0.33],\n       [0.  ],\n       [0.66],\n       [0.33],\n       [0.46],\n       [0.26],\n       [0.18],\n       [0.22],\n       [0.59],\n       [0.  ],\n       [0.  ],\n       [0.01],\n       [0.  ],\n       [0.59],\n       [0.3 ],\n       [0.  ],\n       [0.68],\n       [0.52],\n       [0.47],\n       [0.23]])\n\n\n\n\n\n\nIn this section I’ll run through the explanation and code provided in Jeremy’s notebook here.\nIn this notebook I’ll try to give a taste of how a competitions grandmaster might tackle the U.S. Patent Phrase to Phrase Matching competition. The focus generally should be two things:\n\nCreating an effective validation set\nIterating rapidly to find changes which improve results on the validation set.\n\nIf you can do these two things, then you can try out lots of experiments and find what works, and what doesn’t. Without these two things, it will be nearly impossible to do well in a Kaggle competition (and, indeed, to create highly accurate models in real life!)\nThe more code you have, the more you have to maintain, and the more chances there are to make mistakes. So keep it simple!\n\nfrom pathlib import Path\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle:\n    !pip install -Uqq fastai\nelse:\n    import zipfile,kaggle\n    path = Path('us-patent-phrase-to-phrase-matching')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading us-patent-phrase-to-phrase-matching.zip to /content\n\n\n100%|██████████| 682k/682k [00:00<00:00, 1.49MB/s]\n\n\n\n\n\n\n\n\n\nfrom fastai.imports import *\n\n\nif iskaggle: path = Path('../input/us-patent-phrase-to-phrase-matching')\npath.ls()\n\n(#3) [Path('us-patent-phrase-to-phrase-matching/sample_submission.csv'),Path('us-patent-phrase-to-phrase-matching/test.csv'),Path('us-patent-phrase-to-phrase-matching/train.csv')]\n\n\nLet’s look at the training set:\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n      score\n    \n  \n  \n    \n      0\n      37d61fd2272659b1\n      abatement\n      abatement of pollution\n      A47\n      0.50\n    \n    \n      1\n      7b9652b17b68b7a4\n      abatement\n      act of abating\n      A47\n      0.75\n    \n    \n      2\n      36d72442aefd8232\n      abatement\n      active catalyst\n      A47\n      0.25\n    \n    \n      3\n      5296b0c19e1ce60e\n      abatement\n      eliminating process\n      A47\n      0.50\n    \n    \n      4\n      54c1e3b9184cb5b6\n      abatement\n      forest region\n      A47\n      0.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      36468\n      8e1386cbefd7f245\n      wood article\n      wooden article\n      B44\n      1.00\n    \n    \n      36469\n      42d9e032d1cd3242\n      wood article\n      wooden box\n      B44\n      0.50\n    \n    \n      36470\n      208654ccb9e14fa3\n      wood article\n      wooden handle\n      B44\n      0.50\n    \n    \n      36471\n      756ec035e694722b\n      wood article\n      wooden material\n      B44\n      0.75\n    \n    \n      36472\n      8d135da0b55b8c88\n      wood article\n      wooden substrate\n      B44\n      0.50\n    \n  \n\n36473 rows × 5 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nAnd the test set:\n\neval_df = pd.read_csv(path/'test.csv')\nlen(eval_df)\n\n36\n\n\n\neval_df.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n    \n  \n  \n    \n      0\n      4112d61851461f60\n      opc drum\n      inorganic photoconductor drum\n      G02\n    \n    \n      1\n      09e418c93a776564\n      adjust gas flow\n      altering gas flow\n      F23\n    \n    \n      2\n      36baf228038e314b\n      lower trunnion\n      lower locating\n      B60\n    \n    \n      3\n      1f37ead645e7f0c8\n      cap component\n      upper portion\n      D06\n    \n    \n      4\n      71a5b6ad068d531f\n      neural stimulation\n      artificial neural network\n      H04\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ndf.target.value_counts()\n\ncomposition                    24\ndata                           22\nmetal                          22\nmotor                          22\nassembly                       21\n                               ..\nswitching switch over valve     1\nswitching switch off valve      1\nswitching over valve            1\nswitching off valve             1\nwooden substrate                1\nName: target, Length: 29340, dtype: int64\n\n\nWe see that there’s nearly as many unique targets as items in the training set, so they’re nearly but not quite unique. Most importantly, we can see that these generally contain very few words (1-4 words in the above sample).\n\ndf.anchor.value_counts()\n\ncomponent composite coating              152\nsheet supply roller                      150\nsource voltage                           140\nperfluoroalkyl group                     136\nel display                               135\n                                        ... \nplug nozzle                                2\nshannon                                    2\ndry coating composition1                   2\nperipheral nervous system stimulation      1\nconduct conducting material                1\nName: anchor, Length: 733, dtype: int64\n\n\nWe can see here that there’s far fewer unique values (just 733) and that again they’re very short (2-4 words in this sample).\n\ndf.context.value_counts()\n\nH01    2186\nH04    2177\nG01    1812\nA61    1477\nF16    1091\n       ... \nB03      47\nF17      33\nB31      24\nA62      23\nF26      18\nName: context, Length: 106, dtype: int64\n\n\nThe first character is the section the patent was filed under – let’s create a column for that and look at the distribution:\n\ndf['section'] = df.context.str[0]\ndf.section.value_counts()\n\nB    8019\nH    6195\nG    6013\nC    5288\nA    4094\nF    4054\nE    1531\nD    1279\nName: section, dtype: int64\n\n\nFinally, we’ll take a look at a histogram of the scores:\n\ndf.score.hist();\n\n\n\n\nThere’s a small number that are scored 1.0 - here’s a sample:\n\ndf[df.score==1]\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n      score\n      section\n    \n  \n  \n    \n      28\n      473137168ebf7484\n      abatement\n      abating\n      F24\n      1.0\n      F\n    \n    \n      158\n      621b048d70aa8867\n      absorbent properties\n      absorbent characteristics\n      D01\n      1.0\n      D\n    \n    \n      161\n      bc20a1c961cb073a\n      absorbent properties\n      absorption properties\n      D01\n      1.0\n      D\n    \n    \n      311\n      e955700dffd68624\n      acid absorption\n      absorption of acid\n      B08\n      1.0\n      B\n    \n    \n      315\n      3a09aba546aac675\n      acid absorption\n      acid absorption\n      B08\n      1.0\n      B\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      36398\n      913141526432f1d6\n      wiring trough\n      wiring troughs\n      F16\n      1.0\n      F\n    \n    \n      36435\n      ee0746f2a8ecef97\n      wood article\n      wood articles\n      B05\n      1.0\n      B\n    \n    \n      36440\n      ecaf479135cf0dfd\n      wood article\n      wooden article\n      B05\n      1.0\n      B\n    \n    \n      36464\n      8ceaa2b5c2d56250\n      wood article\n      wood article\n      B44\n      1.0\n      B\n    \n    \n      36468\n      8e1386cbefd7f245\n      wood article\n      wooden article\n      B44\n      1.0\n      B\n    \n  \n\n1154 rows × 6 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nWe can see from this that these are just minor rewordings of the same concept, and isn’t likely to be specific to context. Any pretrained model should be pretty good at finding these already.\n\n\n\n! pip install transformers[sentencepiece] datasets accelerate\n\n\nfrom torch.utils.data import DataLoader\nimport warnings,transformers,logging,torch\nfrom transformers import TrainingArguments,Trainer\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\n\n\nif iskaggle:\n    !pip install -q datasets\nimport datasets\nfrom datasets import load_dataset, Dataset, DatasetDict\n\n\n# quiet huggingface warnings\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\n\n\n# specify which model we are going to be using\nmodel_nm = 'microsoft/deberta-v3-small'\n\nWe can now create a tokenizer for this model. Note that pretrained models assume that text is tokenized in a particular way. In order to ensure that your tokenizer matches your model, use the AutoTokenizer, passing in your model name.\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n\n\n\n\n\n\n\n\n\nWe’ll need to combine the context, anchor, and target together somehow. There’s not much research as to the best way to do this, so we may need to iterate a bit. To start with, we’ll just combine them all into a single string. The model will need to know where each section starts, so we can use the special separator token to tell it:\n\nsep = tokz.sep_token\nsep\n\n'[SEP]'\n\n\n\ndf['inputs'] = df.context + sep + df.anchor + sep + df.target\n\nGenerally we’ll get best performance if we convert pandas DataFrames into HuggingFace Datasets, so we’ll convert them over, and also rename the score column to what Transformers expects for the dependent variable, which is label:\n\nds = Dataset.from_pandas(df).rename_column('score', 'label')\neval_ds = Dataset.from_pandas(eval_df)\n\nTo tokenize the data, we’ll create a function (since that’s what Dataset.map will need):\n\ndef tok_func(x): return tokz(x[\"inputs\"])\n\n\ntok_func(ds[0])\n\n{'input_ids': [1, 336, 5753, 2, 47284, 2, 47284, 265, 6435, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nThe only bit we care about at the moment is input_ids. We can see in the tokens that it starts with a special token 1 (which represents the start of text), and then has our three fields separated by the separator token 2. We can check the indices of the special token IDs like so:\n\ntokz.all_special_tokens\n\n['[CLS]', '[SEP]', '[UNK]', '[PAD]', '[MASK]']\n\n\nWe can now tokenize the input. We’ll use batching to speed it up, and remove the columns we no longer need:\n\ninps = \"anchor\",\"target\",\"context\"\ntok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','id','section'))\n\n\n\n\nLooking at the first item of the dataset we should see the same information as when we checked tok_func above:\n\ntok_ds[0]\n\n{'label': 0.5,\n 'input_ids': [1, 336, 5753, 2, 47284, 2, 47284, 265, 6435, 2],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\n\n\nAccording to this post, the private test anchors do not overlap with the training set. So let’s do the same thing for our validation set.\nFirst, create a randomly shuffled list of anchors:\n\nanchors = df.anchor.unique()\nnp.random.seed(42)\nnp.random.shuffle(anchors)\nanchors[:5]\n\narray(['time digital signal', 'antiatherosclerotic', 'filled interior',\n       'dispersed powder', 'locking formation'], dtype=object)\n\n\nNow we can pick some proportion (e.g 25%) of these anchors to go in the validation set:\n\nval_prop = 0.25\nval_sz = int(len(anchors)*val_prop)\nval_anchors = anchors[:val_sz]\n\nNow we can get a list of which rows match val_anchors, and get their indices:\n\n# is_val is a boolean array\nis_val = np.isin(df.anchor, val_anchors)\nidxs = np.arange(len(df))\nval_idxs = idxs[ is_val]\ntrn_idxs = idxs[~is_val]\nlen(val_idxs),len(trn_idxs)\n\n(9116, 27357)\n\n\nOur training and validation Datasets can now be selected, and put into a DatasetDict ready for training:\n\ndds = DatasetDict({\"train\":tok_ds.select(trn_idxs),\n             \"test\": tok_ds.select(val_idxs)})\n\nBTW, a lot of people do more complex stuff for creating their validation set, but with a dataset this large there’s not much point. As you can see, the mean scores in the two groups are very similar despite just doing a random shuffle:\n\ndf.iloc[trn_idxs].score.mean(),df.iloc[val_idxs].score.mean()\n\n(0.3623021530138539, 0.3613426941641071)\n\n\n\n\n\nLet’s now train our model! We’ll need to specify a metric, which is the correlation coefficient provided by numpy (we need to return a dictionary since that’s how Transformers knows what label to use):\n\ndef corr(eval_pred): return {'pearson': np.corrcoef(*eval_pred)[0][1]}\n\nWe pick a learning rate and batch size that fits our GPU, and pick a reasonable weight decay and small number of epochs:\n\nlr,bs = 8e-5,128\nwd,epochs = 0.01,4\n\nTransformers uses the TrainingArguments class to set up arguments. We’ll use a cosine scheduler with warmup, since at fast.ai we’ve found that’s pretty reliable. We’ll use fp16 since it’s much faster on modern GPUs, and saves some memory. We evaluate using double-sized batches, since no gradients are stored so we can do twice as many rows at a time.\n\ndef get_trainer(dds):\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                   tokenizer=tokz, compute_metrics=corr)\n\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=wd, report_to='none')\n\nWe can now create our model, and Trainer, which is a class which combines the data and model together (just like Learner in fastai):\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n               tokenizer=tokz, compute_metrics=corr)\n\n\n\n\n\ntrainer.train();\n\n\n\n    \n      \n      \n      [856/856 03:02, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.027171\n      0.794542\n    \n    \n      2\n      No log\n      0.026872\n      0.811033\n    \n    \n      3\n      0.035300\n      0.024633\n      0.816882\n    \n    \n      4\n      0.035300\n      0.024581\n      0.817413\n    \n  \n\n\n\n\n\n\nWe now want to start iterating to improve this. To do that, we need to know whether the model gives stable results. I tried training it 3 times from scratch, and got a range of outcomes from 0.808-0.810. This is stable enough to make a start - if we’re not finding improvements that are visible within this range, then they’re not very significant! Later on, if and when we feel confident that we’ve got the basics right, we can use cross validation and more epochs of training.\nIteration speed is critical, so we need to quickly be able to try different data processing and trainer parameters. So let’s create a function to quickly apply tokenization and create our DatasetDict:\n\ndef get_dds(df):\n    ds = Dataset.from_pandas(df).rename_column('score', 'label')\n    tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','id','section'))\n    return DatasetDict({\"train\":tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})\n\n\ndef get_model(): return AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n\n\ndef get_trainer(dds, model=None):\n    if model is None: model = get_model()\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                   tokenizer=tokz, compute_metrics=corr)\n\nPerhaps using the special separator character isn’t a good idea, and we should use something we create instead. Let’s see if that makes things better. First we’ll change the separator and create the DatasetDict:\n\nsep = \" [s] \"\ndf['inputs'] = df.context + sep + df.anchor + sep + df.target\ndds = get_dds(df)\n\n\n\n\n\nget_trainer(dds).train()\n\n\n\n    \n      \n      \n      [856/856 03:27, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.027216\n      0.799765\n    \n    \n      2\n      No log\n      0.025568\n      0.814325\n    \n    \n      3\n      0.031000\n      0.023474\n      0.817759\n    \n    \n      4\n      0.031000\n      0.024206\n      0.817377\n    \n  \n\n\n\nTrainOutput(global_step=856, training_loss=0.023552694610346144, metrics={'train_runtime': 207.9058, 'train_samples_per_second': 526.335, 'train_steps_per_second': 4.117, 'total_flos': 582121520370810.0, 'train_loss': 0.023552694610346144, 'epoch': 4.0})\n\n\nThat’s looking quite a bit better, so we’ll keep that change.\n(Vishal note: I trained it a few times but couldn’t get the pearson coefficient past 0.8174)\nOften changing to lowercase is helpful. Let’s see if that helps too:\n\ndf['inputs'] = df.inputs.str.lower()\ndds = get_dds(df)\nget_trainer(dds).train()\n\n\n\n\n\n\n    \n      \n      \n      [755/856 02:53 < 00:23, 4.33 it/s, Epoch 3.52/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.025207\n      0.798847\n    \n    \n      2\n      No log\n      0.024926\n      0.813183\n    \n    \n      3\n      0.031800\n      0.023556\n      0.815640\n    \n  \n\n\n\n\n\n    \n      \n      \n      [856/856 03:17, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.025207\n      0.798847\n    \n    \n      2\n      No log\n      0.024926\n      0.813183\n    \n    \n      3\n      0.031800\n      0.023556\n      0.815640\n    \n    \n      4\n      0.031800\n      0.024359\n      0.815295\n    \n  \n\n\n\nTrainOutput(global_step=856, training_loss=0.024133934595874536, metrics={'train_runtime': 197.3858, 'train_samples_per_second': 554.386, 'train_steps_per_second': 4.337, 'total_flos': 582121520370810.0, 'train_loss': 0.024133934595874536, 'epoch': 4.0})\n\n\n\n\n\nWhat if we made the patent section a special token? Then potentially the model might learn to recognize that different sections need to be handled in different ways. To do that, we’ll use, e.g. [A] for section A. We’ll then add those as special tokens:\n\ndf['sectok'] = '[' + df.section + ']'\nsectoks = list(df.sectok.unique())\ntokz.add_special_tokens({'additional_special_tokens': sectoks})\n\n8\n\n\n\ndf['inputs'] = df.sectok + sep + df.context + sep + df.anchor.str.lower() + sep + df.target\ndds = get_dds(df)\n\n\n\n\nSince we’ve added more tokens, we need to resize the embedding matrix in the model:\n\nmodel = get_model()\nmodel.resize_token_embeddings(len(tokz))\n\nEmbedding(128009, 768)\n\n\n\ntrainer = get_trainer(dds, model=model)\ntrainer.train()\n\n\n\n    \n      \n      \n      [856/856 03:41, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.025942\n      0.810038\n    \n    \n      2\n      No log\n      0.025694\n      0.814332\n    \n    \n      3\n      0.010500\n      0.023547\n      0.816508\n    \n    \n      4\n      0.010500\n      0.024562\n      0.817200\n    \n  \n\n\n\nTrainOutput(global_step=856, training_loss=0.009868621826171875, metrics={'train_runtime': 221.7169, 'train_samples_per_second': 493.548, 'train_steps_per_second': 3.861, 'total_flos': 695370741753690.0, 'train_loss': 0.009868621826171875, 'epoch': 4.0})\n\n\nBefore submitting a model, retrain it on the full dataset, rather than just the 75% training subset we’ve used here. Create a function like the ones above to make that easy for you!\n\n\n\n\nIn this section, I’ll take notes while watching this lesson’s video.\n\nIntroduction\n\nIn the book, we do NLP using Recurrent Neural Networks (RNNs).\nIn the video, we’re going to be fine-tuning a pretrained NLP model using a library called HuggingFace Transformers.\nIt’s useful to have experience in using more than one library. See the same concepts applied in different ways. Great for understanding the concepts.\nHuggingFace libraries are SOTA in NLP.\nTransformers library in process of being integrated into fastai library.\nHuggingFace Transformers doesn’t have the same layered API as fastai.\n\nFine-Tuning a Pretrained Model\n\nIn the quadratic/sliders example, a pretrained model is like someone telling you that they are confident what parameter a should be, are somewhat confident what b should be, and have no idea what c should be. Then, we would train c until it firts our model, adjust b and keep a as is. That’s what it’s like fine-tuning a pretrained model.\nA pretrained model is a bunch of parameters have already been fit, where for some of them we’re pretty confident of what they should be, and for some of them we really have no idea at all.\nFine-tuning is the process of taking those ones where we have no idea at all what they should be and trying to get them right, and then moving the other ones a little bit.\n\nULMFiT\n\nThe idea of fine-tuning a pretrained NLP model was pioneered by ULMFiT which was first introduced in a fastai course, later turned into an academic paper by Jeremy and Sebastian Ruder which inspired a huge change in NLP capabilities around the world.\nStep 1\n\nBuild a language model using all of Wikipedia that tried to predict the next word of a Wikipedia article. Filling in these kinds of things requires understanding a lot about how language is structured and about the world. Getting good at fitting a language model requires a neural net getting good at a lot of things. It needs to understand language at a reasonably good level, what is true, what is not true, different ways in which things are expressed and so on. Started with random weights. At the end was a model that could predict more than 30% of the time correctly what the next word in a Wikipedia article would be.\n\nStep 2\n\nCreate a second language model, that predicts the next word of a sentence. Took the pretrained model and ran a few more epochs using IMDb movie reviews. So it got very good at predicting the next work of an IMDb movie review.\n\nStep 3\n\nTook those weights and fine-tuned them for the task of predicting whether or not a movie review was positive or negative sentiment.\n\n\nThe first two models don’t require labels. The labels was what’s the next word of the sentence.\nULMFiT built with RNNs.\nTransformers developed at the same time of ULMFiT’s release.\nTransformers can take advantage of modern accelerators like Google’s TPUs.\nTransformers don’t allow you to predict the next word of a sentence, it’s just not how they are structured. Instead they deleted at random a few words and asked the model to predict what words were deleted. The basic concept similar to ULMFiT ,replaced RNN with Transformer. Replaced language model with masked language model.\nHow do you go from a model that’s trained to predict the next word to a model that does classification?\n\nThe first layer of ImageNet classification model finds basic features like diagonal edges, gradients, etc. Layer two combined those (ReLUs added together, activations from sets of ReLUs matrix multipled, etc.)\nLayer 5 had bird and lizard eyeball detectors, dog face detectors, flowers detectors, etc.\nLater layers do things much more specific to the training task.\nPretty unlikely that you need to change the early layers.\nThe layer that says “what is this” is deleted in fine-tuning (the layer that has one output per category). The model is then spitting out a few hundred activations. We stick a new random matrix on top of that and train it, so it can predict what you’re trying to predict. Then we gradually train the rest of the layers.\n\nGetting started with NLP for absolute beginners\n\nUS Patent Phrase to Phrase Matching Competition.\nClassification is probably the most widely use case for NLP.\nDocument = an input to an NLP model that contains text.\nClassifying a document is a rich thing to do: sentiment analysis, author identifiation, legal discovery, organizing documents by topic, triaging inbound emails.\nThe Kaggle competition on US Patents does not immediately look like a classification problem.\nColumns: Anchor, target, context, score\nGoal: come up with a model that automatically determines which anchor and target pairs are talking about the same thing. score = 1.0 means the anchor and target mean the same thing, 0.0 means they are not.\nWhether the anchor and target are determined to be similar or not depends on the context.\nRepresent the problem as <constant string><anchor><seperator><constant string><target> and choose category 0.0, 0.25, 0.50, 0.75 or 1.00.\nKaggle data is already on Kaggle.\nAlways look through the competition’s Data page and read through it before jumping into the data.\nUse DataFrame.describe(include='object') to see stats about the fields (count, unique, top, frequency of top).\nThis dataset contains very small documents (3-4 words) that are not very unique. There’s not a lot of unique data to work with.\nCreate a single string of anchor, target, and context with separators and store as the input column.\nNeural networks work with numbers: We’re going to take the numbers, multiply by matrices, replace negatives with zeros, add them up, and do this a few times.\n\nTokenization: Split each document into tokens (words).\nThe list of unique words is called the vocabulary.\nNumericalization: Each word in the vocabulrary gets a number. The bigger the vocab, the more memory gets used, the more data we need to train. We don’t want a large vocabulary.\nTokenize into sub-words (pieces of words).\n\nWe can turn a pandas DataFrame into a Huggingface dataset’s Dataset using Dataset.from_pandas.\nWhatever pretrained model you used comes with a tokenizer. Before you start tokenizing, you have to decide on which model to use.\nHugginface Model Hub has pretrained models trained on specific corpuses.\nThere are some generally good models, deberta-v3 is one of those.\nNLP has been practically effective for general users for only a year or two, a lot of this stuff we’re figuring out as a community.\nAlways start with a small model, it’s faster to train, we’re going to be able to do more iterations.\nAutoTokenizer.from_pretrained(<model name>) will download the vocab and details about how this particular model tokenized the dataset.\n_ represents the start of a word.\ndef tok_func(x): return tokx(x['input']) takes a document x, and tokenizes it’s input.\nDataset.map will parallelize the process of calling the function on each value. batched=True will do a bunch at a time. Tokenizer library is an optimized Rust library.\ninput_ids will contain numbers in the position of each of the tokens.\nHow do you choose the keywords and the order of the fields when creating input?\n\nIt’s arbitrary, try a few things. We just want something it can learn from that separates one field from another.\n\nIf one of the fields was long (1000 characters) is there any special handling required there?\n\nLong documents in ULMFiT require no special consideration. ULMFiT is the best approach for large documents. It will split large documents into pieces.\nLarge documents are challening for Transformers. It does the whole document at once.\nDocuments over 2000 words: look at ULMFiT.\nUnder 2000 words: Transformers should be fine unless you have a laptop GPU with not much memory.\n\nHuggingFace transformers expect that your target is a column called labels.\ntest.csv doesn’t have a score field.\nPerhaps the most important idea in machine learning is having separate training, validation and test datasets.\nTest and validation sets are all about identifying and controlling for overfitting.\nUnderfit: not enough complexity in the model fit to match the data that’s there. It’s systematically biased.\nCommon misunderstanding is that simpler models are more reliable in some way, but models that are too simple will be systematically incorrect.\nOverfit: it’s done a good job of fitting our data points, but if we sample some more data points from our distribution the model won’t be close to them.\nUnderfitting is easy to recognize (we can look at training data and see that it’s not very close).\nOverfitting is harder to recognize because the training data is very close.\nHow do we tell if we have a good fit that’s not overfitting? We measure how good our model is by looking ONLY at the points we set aside as the validation set.\nfast.ai won’t let you train a model without a validation set and shows metrics only on the validation set.\nCreating a good validation set is not generally as simple as just randomly pulling some of your data out of the data that you train your model on.\nKaggle is a great place to learn how to create a good validation set.\nA test set is another validation set that you don’t use for metrics. Helps you see if you overfit using the validation set.\nKaggle has two test sets: leaderboard feedback during competition and second test set that is private until after competition is finished.\nDon’t accidentally find a model that is good by coincidence. Only if you have a test set that you hold out will you know if you’ve done this.\nIf your model is terrible on the test set—go back to square one.\nYou don’t want functions with gradient of 0 of inf (like accuracy) you want something smooth.\nOne metric is not enough to capture all of the real world dynamics involved in a model’s use.\nGoodhart’s law: when a measure becomes a target, it’s ceases to be a good measure.\nAI is really good at optimizing metrics so you have to be careful what metrics you choose for models that are used in real life (impacting people’s lives).\nPearson correlation coefficient is the most widely used measure of how similar two variables are\n\n-1.0 to +1.0.\nAbbreviated as r.\n\nHow do I plot datasets with far too many points? The answer is: get less points (sample).\nnp.corrcoef gives a diagonally symmetric matrix of r values.\nVisualizing your data is important so you can see things like how data is truncated.\nalpha=0.5 for scatter plots creates darker areas where there’s lots of dots.\nr relies on the square of the difference, big outliers increase that by a lot.\nr is very sensitive to outliers.\nIf you’re trying to win a Kaggle competition that uses r and even a couple of your rows are really wrong, it will be a disaster.\nYou almost can’t see the relationship for \\(r=0.34\\)\nTransformers expects metric to be returned as a dict.\ntok_ds.train_test_split() returns a DatasetDict({train: Dataset, test: Dataset}).\nTransformers calls it validation set test, on which is calculates metrics.\nThe fastai equivalent of Learner is the HuggingFace Transformer’s Trainer.\nThe larger the batch size, the more you can do in parallel and the faster it’ll be, but if it’s too large you’ll get an out-of-memory error on the GPU.\nIf you’re using a framework that doesn’t have a learning rate finder like fastai, you can just start with a really low learning rate and then keep doubling it until it falls apart.\nTrainingArguments is a class that takes all of the configuration (like learning rate, warmup ratio, scheduler type, weight decay, etc.).\nYou always want fp16=True as it will be faster.\nAutoModelForSequenceClassification will create an model for classification, .from_pretrained will use a pretrained model which has a num_labels param which is the number of output columns we have, which in this case is 1 (the score).\nTrainer takes the model, the training and validation data, TrainingArguments(), tokenizer and metrics).\nTrainer.train() will train the model.\nHuggingFace is very verbose, the warnings which you can ignore.\nThe only reason we get a high r value after 4 epochs is because we used a pretrained model.\nThe pretrained model already knows a lot about language and has a good sense of whether two phrases have the same meaning or not.\nHow do you decide when it’s okay to remove outliers?\n\nOutliers should never just be removed for modelling.\nInstead we would observe that clearly from looking at this dataset, these two groups can’t be treated the same way (low income/high # of rooms vs. high income/high # of rooms). Split them into two separate analyses.\nOutlier exists in a statistical sense, it doesn’t exist in a real sense (i.e. things that we should ignore or throw away). Some of the most useful insights in data projects are digging into outliers and understanding what are they? and where did they come from? It’s in those edge cases where you discover really important things like when processes go wrong, labelling problems. Never delete outliers. Investigate them, have a strategy about what you’re going to do with them.\n\nTraining with HuggingFace’s Transformer is similar to the things we’ve seen before with fastai.\ntrainer.predict(eval_ds).predictions.astype(float) to get predictions from Trainer object.\nAlways look at your outputs. So you can see things like having negative predictions or predictions over 1, which are outside the range of the patent phrase matching score. For now, we can at least round these off up to 0 and down to 1, respectively, better ways to do this but this is better than nothing.\nKaggle expects submissions to generally be in a CSV file.\nNLP is probably where the biggest opportunities are for big wins in research and commercialization.\n\nIt’s worth thinking about both use and misuse of modern NLP.\nYou can create bots to generate context appropriate conversation and scale it up to 99% of Twitter and nobody would know. This is worrying because a lot of how people see the world is coming out of social media conversation, which at this point are contrallable. It would not be that hard to create something that’s optimized towards moving a point of view amongst a billion people in a very subtle way, very gradually over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth.\nWhat GPT is used for we may not know for decades, if ever.\n2017: millions of submissions to the FTC about Net Neutrality very heavily biased against it. An analysis showed that something like 99% of them were auto-generated. We don’t know for sure but this seems successful because repealing Net Neutrality went through, the comments were factored into this decision.\nYou can always create a generative model that beats bot classifiers designed to classify its content as auto-generated. Similar problem with spam prevention.\nIf you pass num_labels=1 to AutoModelForSequenceClassification it treats it as a regression problem.\n\n\n\n\nIn this section, I’ll take notes and run code examples from Chapter 10: NLP Deep Dive: RNNs in the textbook.\n\nIn general, in NLP the pretrained model is trained on a different task.\nlanguage model: a model that has been trained to guess the next word in a text (having read the ones before).\nself-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels.\nTo properly guess the next word in a sentence, the model will have to develop an understanding of the natural language.\nSelf-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning.\nSelf-supervised learning and computer vision\nEven if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targeting.\nYou get even better results if you fine-tune the sequence-based language model prior to fine-tuning the classification model.\nThe IMDb dataset contains 100k movie reviews (50k unlabeled, 25k labeled training set reviews, 25k labeled validation set reviews). We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles, this will result in a language model that is particularly good at predicting the next word of a movie review. This is known as Universal Language Model Fine-tuning (ULMFiT).\nThe extra stage of fine-tuning the language model, prior to transfer learning to classification task, resulted in significantly better predictions.\n\n\n\n\nUsing categorical variables as independent variables for a neural network:\n\nMake a list of all possible levels of that categorical variable (the vocab).\nReplace each level with its index in the vocab.\nCreate an embedding matrix for this containing a row for each level (i.e., for each item of the vocab).\nUse this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to, but faster and more efficient than, a matrix that takes as input one-hot-encoded vectors representing the indexes).\n\nWe can do nearly the same thing with text:\n\nFirst we concatenate all of the documents in our dataset into one big long string and split it into words (or tokens), giving us a very long list of words.\nOur independent variable will be the sequence of words starting with the first word in our very long list and ending with the second to last, and our dependent variable will be the sequence of words starting with the second word and ending with the last word.\nOur vocab will consist of a mix of common words that are already in the vocabulary of our pretrained model and new words specific to our corpus.\nOur embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words, we won’t have anything, so we will just initialize the corresponding row with a random vector.\n\nSteps for creating a language model:\n\nTokenization: convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\nNumericalization: List all of the unique words that appear (vocab) and convert each word into a number by looking up its index in the vocab.\nLanguage model data loader creation: fastai’s LMDataLoader automatically handles creating a dependent variable that is offset from the independent variable by one token, and handles important details liks shuffling the training data so that the dependent and independent variables maintain their structure as required.\nLanguage model creation: we need a model that handles input lists that could be arbitrarily big or small. We use a Recurrent Neural Network (RNN).\n\n\n\n\nThere is no one approach to tokenization. There are three main approaches:\n\nWord-based: Split a sentence on spaces and separate parts of meaning even when there are no spaces (“don’t” -> “do n’t”). Punctuation marks are generally split into separate tokens.\nSubword based: Split words into smaller parts, based on the most commonly occurring substrings (“occasion” -> “o c ca sion”).\nCharacter-based: Split a sentence into its individual characters.\n\n\n\n\nRather than providing its own tokenizers, fastai provides a consistent interface to a range of tokenizers in external libraries.\nLet’s try it out with the IMDb dataset:\n\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n\n\n\n\n\n\n    \n      \n      100.00% [144441344/144440600 00:02<00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7) [Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/imdb.vocab'),Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/README')]\n\n\nget_text_files gets all the text files in a path\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\n\nfiles[:10]\n\n(#10) [Path('/root/.fastai/data/imdb/unsup/42765_0.txt'),Path('/root/.fastai/data/imdb/unsup/19120_0.txt'),Path('/root/.fastai/data/imdb/unsup/8649_0.txt'),Path('/root/.fastai/data/imdb/unsup/32022_0.txt'),Path('/root/.fastai/data/imdb/unsup/30143_0.txt'),Path('/root/.fastai/data/imdb/unsup/14876_0.txt'),Path('/root/.fastai/data/imdb/unsup/28162_0.txt'),Path('/root/.fastai/data/imdb/unsup/32133_0.txt'),Path('/root/.fastai/data/imdb/unsup/21844_0.txt'),Path('/root/.fastai/data/imdb/unsup/830_0.txt')]\n\n\nHere’s a review that we will tokenize:\n\ntxt = files[0].open().read(); txt[:75]\n\n\"Despite some humorous banter and a decent supporting cast, I can't really r\"\n\n\nWordTokenizer will always point to fastai’s current default word tokenizer.\nfastai’s coll_repr(collection, n) displays the first n items of collection, along with the full size.\n\ntokz = WordTokenizer()\ntoks = first(tokz([txt]))\nprint(coll_repr(toks, 30))\n\n(#243) ['Despite','some','humorous','banter','and','a','decent','supporting','cast',',','I','ca',\"n't\",'really','recommend','this','movie','.','The','leads','are',\"n't\",'very','likable','and','I','did',\"n't\",'particularly','care'...]\n\n\nTokenization is a surprisingly subtle task. “.” is separated when it terminates a sentence but not in an acronym or number:\n\nfirst(tokz(['The U.S. dollar $1 is $1.00.']))\n\n(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']\n\n\nfastai adds some functionality to the tokenization process with the Tokenizer class:\n\ntkn = Tokenizer(tokz)\nprint(coll_repr(tkn(txt), 31))\n\n(#264) ['xxbos','xxmaj','despite','some','humorous','banter','and','a','decent','supporting','cast',',','i','ca',\"n't\",'really','recommend','this','movie','.','xxmaj','the','leads','are',\"n't\",'very','likable','and','i','did',\"n't\"...]\n\n\nTokens that start with xx are special tokens.\nxxbos is a special token that indicates the start of a new text (“BOS” is a standard NLP acronym that means “beginning of stream”). By recognizing this start token, the model will be able to learn it needs to “forget” what was said previously and focus on upcoming words. These special tokens don’t come from the external tokenizer. fastai adds them by default by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. We are translating the original English language sequence into a simplified tokenized language that is designed to be easy for a model to learn.\nFor example, the rules will replace a sequence of four exclamation points with a single exclamation point follow by a special repeated character token and then the number four.\n\ntkn('!!!!')\n\n(#4) ['xxbos','xxrep','4','!']\n\n\nIn this way, the model’s embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repititions of every punctuation mark. A capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word so the embedding matrix needs only the lowercase version of the words saving compute and memory resources but can still learn the concept of capitalization.\nHere are some of the main special tokens:\nxxbos: Indicates the beginning of a text (in this case, a review).\nxxmaj: Indicates the next word begins with a capital.\nxxunk: Indicates the next word is unknown.\n\ndefaults.text_proc_rules\n\n[<function fastai.text.core.fix_html(x)>,\n <function fastai.text.core.replace_rep(t)>,\n <function fastai.text.core.replace_wrep(t)>,\n <function fastai.text.core.spec_add_spaces(t)>,\n <function fastai.text.core.rm_useless_spaces(t)>,\n <function fastai.text.core.replace_all_caps(t)>,\n <function fastai.text.core.replace_maj(t)>,\n <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]\n\n\nfix_html: replaces special HTML characters with a readable version.\nreplace_rep: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character.\nreplace_wrep: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the character.\nspec_add_spaces: adds spaces around / and #.\nrm_useless_spaces: Removes all repetitions of the space character.\nreplace_all_caps: Lowercases a word written in all caps and adds a special token for all caps (xxcap) in front of it.\nreplace_maj: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it.\nlowercase: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\n\ncoll_repr(tkn(\"&copy;    Fast.ai www.fast.ai/INDEX\"), 31)\n\n\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\"\n\n\n\n\n\nWord tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a sentence. However this assumption is not always appropriate. Languages like Chinese and Japanese don’t use spaces. Turkish and Hungarian can add many subwords together without spaces.\nTwo steps of subword tokenization:\n\nAnalyze a corpus of documents to find the most commonly occuring groups of letters. These becomes the vocab.\nTokenize the corpus string using this vocab of subword units.\n\n\ntxts = L(o.open().read() for o in files[:2000])\n\n\n! pip install sentencepiece\ndef subword(sz):\n  sp = SubwordTokenizer(vocab_sz=sz)\n  sp.setup(txts)\n  return ' '.join(first(sp([txt]))[:40])\n\nsetup reads the documents and finds the common sequences of characters to create the vocab.\n\nsubword(1000)\n\n\n\n\n\n\n\n\n\"▁De s p ite ▁some ▁humor ous ▁b ant er ▁and ▁a ▁de cent ▁support ing ▁cast , ▁I ▁can ' t ▁really ▁recommend ▁this ▁movie . ▁The ▁lead s ▁are n ' t ▁very ▁li k able ▁and ▁I\"\n\n\nWhen using fastai’s subword tokenizer, _ represents a space character in the original text.\nIf we use a smaller vocab, each token will represent fewer characters and it will take more tokens to represent a sentence.\n\nsubword(200)\n\n\n\n\n\n\n\n\n'▁ D es p it e ▁ s o m e ▁h u m or o us ▁b an ter ▁and ▁a ▁ d e c ent ▁ s u p p or t ing ▁ c a s t'\n\n\nIf we use a larger vocab, most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence:\n\nsubword(10000)\n\n\n\n\n\n\n\n\n\"▁Des pite ▁some ▁humorous ▁ban ter ▁and ▁a ▁decent ▁support ing ▁cast , ▁I ▁can ' t ▁really ▁recommend ▁this ▁movie . ▁The ▁leads ▁are n ' t ▁very ▁likable ▁and ▁I ▁didn ' t ▁particular ly ▁care ▁if ▁they\"\n\n\nA larger vocab means fewer tokens per sentence, which means faster training, less memory and less state for the model to remember; but on the downside, it means larger embedding matricces, which require more data to learn.\nSubword tokenization provides a way to easily scale between character tokenization (using a small subword vocab) and word tokenization (using a large subword vocab) and handles every human language. It can even handle genomic sequences or MIDI music notation. It’s likely to become (or has already) the most common tokenization approach.\n\n\n\nNumericalization is the process of mapping tokens to integers.\n\nMake a list of all possible levels of the categorical variable (the vocab).\nReplace each level with its index in the vocab.\n\n\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n\n(#264) ['xxbos','xxmaj','despite','some','humorous','banter','and','a','decent','supporting','cast',',','i','ca',\"n't\",'really','recommend','this','movie','.','xxmaj','the','leads','are',\"n't\",'very','likable','and','i','did',\"n't\"...]\n\n\nJust like with SubwordTokenizer, we need to call setup on Numericalize to create the vocab. That means we’ll need our tokenized corpus first:\n\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n\n(#264) ['xxbos','xxmaj','despite','some','humorous','banter','and','a','decent','supporting'...]\n\n\n\nnum = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab, 20)\n\n\"(#2200) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','of','to','is','in','i','it'...]\"\n\n\nOur special rules tokens appear first, and then every word appears once in frequency order.\nThe defaults to Numericalize are min_freq=3 and max_vocab=60000. max_vocab results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn’t enough data to train useful representations for rare words (better handles by setting min_freq, any word appearing fewer than it is replaced with xxunk).\nfastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter.\nThe Numericalizer object is used like a function:\n\nnums = num(toks)[:20]; nums\n\nTensorText([  2,   8, 418,  68,   0,   0,  12,  13, 618, 419, 190,  11,  18,\n            259,  38,  93, 445,  21,  28,  10])\n\n\nWe can check that the integers map back to the original text:\n\n' '.join(num.vocab[o] for o in nums)\n\n\"xxbos xxmaj despite some xxunk xxunk and a decent supporting cast , i ca n't really recommend this movie .\"\n\n\n\n\n\nWe want our language model to read text in order, so that it can efficiently predict what the next word is, this means each new batch should begin precisely where the previous one left off.\nAt the beginning of each epoch we will shuffle the order of the documents to make a new stream.\nWe then cut this stream into a certain number of batches (which is our batch size). For example, if the stream has 50,000 tokens as we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (1 to 5,000 for the first mini-stream, then from 5,001 to 10,000…) because we want the model to read continuous rows of text. An xxbos token is added at the start of each text during preprocessing, so that the model knowns when it reads the stream when a new entry is beginning.\nFirst apply our Numericalize object to the tokenized texts:\n\nnums200 = toks200.map(num)\n\nThen pass it to the LMDataLoader:\n\ndl = LMDataLoader(nums200)\n\n\nx,y = first(dl)\nx.shape, y.shape\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\n\nx[:1], y[:1]\n\n(LMTensorText([[   2,    8,  418,   68,    0,    0,   12,   13,  618,  419,  190,\n                  11,   18,  259,   38,   93,  445,   21,   28,   10,    8,    9,\n                 693,   42,   38,   72, 1274,   12,   18,   81,   38,  479,  420,\n                  58,   47,  305,  274,   17,    9,  135,   10,   18,  619,   81,\n                  38,   49,    9,  221,  120,  221,   47,  305,  274,   11,   29,\n                   8,    0,    8, 1275,  783,   74,   59,  446,   15,   43,    9,\n                   0,  285,  114,    0,   24,    0]]),\n TensorText([[   8,  418,   68,    0,    0,   12,   13,  618,  419,  190,   11,\n                18,  259,   38,   93,  445,   21,   28,   10,    8,    9,  693,\n                42,   38,   72, 1274,   12,   18,   81,   38,  479,  420,   58,\n                47,  305,  274,   17,    9,  135,   10,   18,  619,   81,   38,\n                49,    9,  221,  120,  221,   47,  305,  274,   11,   29,    8,\n                 0,    8, 1275,  783,   74,   59,  446,   15,   43,    9,    0,\n               285,  114,    0,   24,    0,   30]]))\n\n\nLooking at the first row of the independent variable:\n\n' '.join(num.vocab[o] for o in x[0][:20])\n\n\"xxbos xxmaj despite some xxunk xxunk and a decent supporting cast , i ca n't really recommend this movie .\"\n\n\nWhich is the start of the text.\nThe dependent variable is the same thing offset by one token:\n\n' '.join(num.vocab[o] for o in y[0][:20])\n\n\"xxmaj despite some xxunk xxunk and a decent supporting cast , i ca n't really recommend this movie . xxmaj\"\n\n\nWe are now ready to train our text classifier.\n\n\n\n\nTwo steps to training a state-of-the-art text classifier using transfer learning:\n\nFine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews.\nUse that model to train a classifier.\n\n\n\nfastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock.\n\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb,\n    splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n\n\n\n\n\n\n\n\nfrom_folder tells TextBlock how to access the texts so that it can do initial preprocessing. fastai performs a few optmizations:\n\nIt saves the tokenized documents in a temporary folder, so it doesn’t have to tokenize them more than once.\nIt runs multiple tokenization processes in parallel, to take advantage of your computer’s CPUs.\n\n\ndls_lm.show_batch(max_n=2)\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj caught this at xxmaj cinequest . xxmaj it was well attended , but the crowd seemed disappointed . xxmaj in my humble opinion , \" charlie the xxmaj ox \" was very amateurish and overrated ( it pales in comparison with other cinequest pics i saw ) . xxmaj acting ( with the exception of xxmaj polito ) seemed self - conscious and \" stagey . \" xxmaj photography , despite originating on high - end xxup hd\n      xxmaj caught this at xxmaj cinequest . xxmaj it was well attended , but the crowd seemed disappointed . xxmaj in my humble opinion , \" charlie the xxmaj ox \" was very amateurish and overrated ( it pales in comparison with other cinequest pics i saw ) . xxmaj acting ( with the exception of xxmaj polito ) seemed self - conscious and \" stagey . \" xxmaj photography , despite originating on high - end xxup hd ,\n    \n    \n      1\n      career , seemed to specialize in patriarch roles , such as in \" all the xxmaj president 's xxmaj men \" , \" max xxmaj dugan xxmaj returns \" , and \" you xxmaj ca n't xxmaj take it xxmaj with xxmaj you \" . xxmaj and in this case , those of us who never saw him on the stage get a big treat , because this was a taped xxmaj broadway production . xxmaj he dominates every scene\n      , seemed to specialize in patriarch roles , such as in \" all the xxmaj president 's xxmaj men \" , \" max xxmaj dugan xxmaj returns \" , and \" you xxmaj ca n't xxmaj take it xxmaj with xxmaj you \" . xxmaj and in this case , those of us who never saw him on the stage get a big treat , because this was a taped xxmaj broadway production . xxmaj he dominates every scene ,\n    \n  \n\n\n\nEach item in the training dataset is a document:\n\n' '.join(dls_lm.vocab[o] for o in dls_lm.train.dataset[0][0])\n\n\"xxbos xxmaj it is a delight to watch xxmaj laurence xxmaj harvey as a neurotic chess player , who schemes to murder the opponent he can not defeat at the chessboard . xxmaj this movie has wonderful pacing and several cliffhanger moments , as xxmaj harvey 's plot several times seems on the point of failure or exposure , but he manages to beat the odds yet again . xxmaj columbo wages a skilful war of nerves against this high - strung genius , and the scene where he manages to rattle him enough to cause him to make a mistake while playing chess is one of the highlights of the movie , as xxmaj harvey looks down in disbelief at the board , where he has just allowed himself to be xxunk . xxmaj the climax is almost as strong , and watching xxmaj laurence xxmaj harvey collapse completely as his scheme is exposed brings the movie to a satisfying finish . xxmaj highly recommended .\"\n\n\n\n' '.join(dls_lm.vocab[o] for o in dls_lm.train.dataset[2][0])\n\n\"xxbos xxmaj eyeliner was worn nearly 6 xxrep 3 0 years ago in xxmaj egypt . xxmaj really not that much of a stretch for it to be around in the 12th century . i also did n't realize the series flopped . xxmaj there is a second season airing now is n't there ? xxmaj it is amazing to me when commentaries are made by those who are either ill - informed or do n't watch a show at all . xxmaj it is a waste of space on the boards and of other 's time . xxmaj the first show of the series was maybe a bit painful as the cast began to fall into place , but that is to be expected from any show . xxmaj the remainder of the first season is excellent . i can hardly wait for the second season to begin in the xxmaj united xxmaj states .\"\n\n\nTo confirm my understanding, that the first item in each batch is continuing the mini-stream, I’ll take a look at the first mini-stream of the first two batches:\n\ncounter = 0\nfor xb, yb in dls_lm.train:\n  output = ' '.join(dls_lm.vocab[o] for o in xb[0])\n  print(output)\n  counter += 1\n  if counter == 2: break\n\nxxbos xxmaj just got this in the mail and i was positively surprised . xxmaj as a big fan of 70 's cinema it does n't take much to satisfy me when it comes to these kind of flicks . xxmaj despite the obvious low budget on this movie , the acting is overall good and you can already see why xxmaj pesci was to become on of the greatest actors ever . xxmaj i 'm not sure how authentic\nthis movie is , but it sure is a good contribution to the mob genre … .. xxbos xxmaj why on earth should you explore the mesmerizing nature documentary \" earth \" ? xxmaj how much time do you have on earth so i can explain this to you ? xxup ok , i will not xxunk my review exploration on \" earth \" to infinity , but i must stand my ground on why this is a \" must\n\n\nConfirmed! The second batch’s first mini-stream is a continuation of the first batch’s first mini-stream. In this case, the first mini-stream of the second batch also contains the start of the next movie review (document) as indicated by the xxbos special token.\n\n\n\nTo convert the integer word indices into activations that we can use for our neural network, we will use embeddings. We feed those embeddings into a recurrent neural network (RNN) using an architecture called AWS-LSTM.\nThe embeddings in the pretrained model are merged with random embeddings added for words that weren’t in the pretraining vocabulary.\n\nlearn = language_model_learner(\n    dls_lm,\n    AWD_LSTM,\n    drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]\n).to_fp16()\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:00<00:00]\n    \n    \n\n\nThe loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab).\nPerplexity is a metric often used in NLP for language models. It is the exponential of loss (i.e., torch.exp(cross_entropy)).\nlanguage_model_learner automatically calls freeze when using a pretrained model (which is the default) so this will train only the embeddings (the part of the model that contains randomly initialized weights—embeddings for the words that are in our IMDb vocab, but aren’t in the pretrained model vocab).\nI wasn’t able to train my model on Google Colab (I got a ran out of memory error even for small batches) so I trained the IMDb language model on Paperspace and wrote a separate blog post about it.\n\n\n\n\n\nEven simple algorithms could be used to create fraudulent accounts and try to influence policymakers (99% of the 2017 Net Neutrality public comments were likely faked).\nMany people assume or hope that algorithms will come to our defense here, the problem is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms.\n\n\n\n\n1. What is self-supervised learning?\nSelf-supervised learning is when you train a model on data that does not contain any external labels. Instead, the labels are embedded in the independent variable.\n2. What is a language model?\nA language model is a model that predicts the next word based on the previous words in a text.\n3. Why is a language model considered self-supervised?\nBecause we do not train the model with external labels. The dependent variable is the next token in a sequence of previous tokens (independent variable).\n4. What are self-supervised models usually used for?\nPretraining a model that will be used for transfer learning.\n5. Why do we fine-tune language models?\nIn order for it to learn the style of language used in our specific corpus.\n6. What are the three steps to create a state-of-the-art text classifier?\n\nTrain a language model on a large general corpus like Wikipedia.\nFine-tune a language model using your task-specific corpus.\nFine-tune a classifier using the encoder of the twice-pretrained language model.\n\n7. How do the 50,000 unlabeled movie reviews help create a better text classifier for the IMDb dataset?\nThe 50k unlabeled movie reviews help create a better text classifier for the IMDb dataset because when you fine-tune the pretrained Wikipedia language model using this data, the model learns the particular style and content of IMDb movie reviews, which helps it better understand what the language used in the reviews means when classifying it as positive or negative.\n8. What are the three steps to prepare your data for a language model?\n\nTokenization: convert the text into a list of words (or characters or substrings).\nNumericalization: List all of the words that appear (the vocab) and convert each word into a number by looking up its index in the vocab.\nLanguage model data loader creation: combine the documents into one string and split it into fixed sequence length batches while preserving the order of the tokens, create a dependent variable that is offset from the independent variable by one token, and shuffle the training data (maintaining independent/dependent variable structure).\n\n9. What is tokenization? Why do we need it?\nTokenization is the conversion of text into smaller parts (like words, subwords or characters). In order to convert our documents into numbers (categories) that the language model can learn something about, we first tokenize them (break them into smaller parts) so that we can generate a list of unique tokens (unique levels of a categorical variable) contained in the corpus (categorical variable).\n10. Name three approaches to tokenization.\n\nword-based: split a sentence based on spaces.\nsubword based: split words into commonly occurring substrings.\ncharacter-based: split a sentence into its individual characters.\n\n11. What is xxbos?\nA special token that tells the language model that we are at the start of a new stream (document).\n12. List four rules that fastai applies to text during tokenization.\nI’ll list them all:\n\nfix_html: replace special HTML characters (like &copy—the copyright symbol) with a readable version.\nreplace_rep: replace repeated characters with a special token for repetition (xxrep), the number of times it’s repeated, and then the character.\nreplace_wrep: do the same as replace_rep but for repeated words (using the special token xxwrep).\nspec_add_spaces: add spaces around / and #.\nrm_useless_spaces: remove all repetitions of the space character.\nreplace_all_caps: lowercase all-caps words and place a special token xxcap in front of it.\nreplace_maj: lowercase a capitalized word and place a special token xxmaj in front of it.\nlowercase: lowercase all text and place a special token at the beginning (xxbos) and/or at the end (xxeos).\n\n13. Why are repeated characters replaced with a token showing the number of repetitions and the character that’s repeated?\nSo that the model’s embedding matrix can encode information about general concepts such as repeated punctuation without requiring a unique token for every number of repetitions of a character.\n14. What is numericalization?\nConverting a token to a number by looking up its index in the vocab (unique list of all tokens).\n15. Why might there be words that are replaced with the “unknown word” token?\nIn order to avoid having an overly large embedding matrix, fastai’s numericalization replaces two types of words with with the unknown word token xxunk:\n\nWords that appear less than min_freq times.\nWords that are not in the max_vocab most frequent words.\n\nFor example, if min_freq = 3 then all words that appear once or twice are replaced with xxunk.\nIf max_vocab = 60000 then words the appear less frequently than the 60000th most frequent word are replaced with xxunk.\n16. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain?\nThe second row contains 64 tokens of the (n/b/s+1)th group of tokens where n is the number of tokens, divided by the number of batches b divided by the sequence length s. So, if we have 90 tokens divided into 6 batches (rows) with a sequence length (columns) of 5, then the second row of the first batch contains the 4th (i.e., 3 + 1) group of tokens.\nPutting Tanishq’s answer here as well:\n\nThe dataset is split into 64 mini-streams (batch size).\nEach batch has 64 rows (batch size) and 64 columns (sequence length).\nThe first row of the first batch contains the beginning of the first mini-stream (tokens 1-64).\nThe second row of the first batch contains the beginning of the second mini-stream.\nThe first row of the second batch contains the second chunk of the first mini-stream (tokens 65 - 128).\n\n17. Why do we need padding for text classification? Why don’t we need it for language modeling?\nWhen the data is prepared for language modeling, the documents are concatenated into a single string and broken up into equally-sized batches, so there is no need to pad any batches—they’re already the right size.\nIn the case of text classification, each document is maintained in full length in a batch, and documents will very likely have a varying number of tokens (i.e., everyone is not writing the same length of movie reviews with the same number of special tokens) so in each batch, all of the documents (except the largest) will need to be padded to the batch’s largest document’s size. fastai sorts the data by length each epoch and groups together documents of similar lengths for each batch before applying the padding.\nSomething that I would like to understand however is:\nWhat if the number of tokens in the training dataset is not divisible by the selected batch size and sequence length? Does fastai use padding in that case? Suppose you have 1000 tokens in total, a batch size of 16 and sequence length of 20. 320 goes into 1000 3 times with a remainder. Does fastai create a 4th batch with padding? Or remove the tokens so there’s only 3 batches? I’ll see if I can figure out what it does with some sample code:\n\nbs,sl = 5, 2\nints = L([[0,1,2,3,4,5,6,7,8,9,10,11,12,13]]).map(tensor)\n\n\ndl = LMDataLoader(ints, bs=bs, seq_len=sl)\n\nlist(dl)\n\n[(LMTensorText([[0, 1],\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]]),\n  tensor([[ 1,  2],\n          [ 3,  4],\n          [ 5,  6],\n          [ 7,  8],\n          [ 9, 10]]))]\n\n\n\nlist(LMDataLoader(ints, bs=bs, seq_len=sl, drop_last=False))\n\n[(LMTensorText([[0, 1],\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]]),\n  tensor([[ 1,  2],\n          [ 3,  4],\n          [ 5,  6],\n          [ 7,  8],\n          [ 9, 10]]))]\n\n\nLooks like fastai drops the last batch if it’s not full. I’ve posted this question in the fastai forums to get a confirmation on my understanding.\n18. What does an embedding matrix for NLP contain? What is its shape?\nIt contains the parameters that are trained by the neural net, with each parameter corresponding to each token in the vocab.\nFrom Tanishq’s solutions:\n\nThe embedding matrix has the size (vocab_size x embedding_size) where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens.\n\n19. What is perplexity?\nA metric used in NLP. It is the exponential of the loss.\n20. Why do we have to pass the vocabulary of the language model to the classifier data block?\nThe indexes corresponding to the tokens have to be maintained because we are fine-tuning the language model.\n21. What is gradual unfreezing?\nWhen we train one layer at a time for one epoch before we unfreeze and train the full model (including all layers of the encoder).\n22. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\nBecause text generation models can be trained to beat automatic identification algorithms.\n\n\n\n1. See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\n\nHere is a tweet thread by Arvind Narayan talking about how the danger of ChatGPT is that “you can’t tell when it’s wrong unless you already know the answer”.\nThis New York Times article walks through different examples of ChatGPT responding to prompts with disinformation.\nThis NewsGuard article, which was referenced in the NYT article, discusses how ChatGPT-4 is more prone to perpetuating misinformation than its predecessor GPT-3.5. GPT-3.5 generated 80 of 100 false narratives given as prompts while GPT-4 generated 100 of 100 false narratives. Also, “ChatGPT-4’s responses that contained false and misleading claims were less likely to include disclaimers about the falsity of those claims (23% of the time) [than ChatGPT-3.5 (51% of the time)].\nThis NBC New York article walks through an example of how a ChatGPT written story on Michael Bloomberg was full of made-up quotes and sources. It also talks about how some educators are embracing ChatGPT in the classroom, and while ineffective, there are machine-generated text identification algorithms available. Although it’s important to note, as disussed in the fastai course, that text generation models will always be ahead of automatic identification models (generative models can be trained to beat identification models).\nIn this Harvard Business School Working Knowledge article Scott Van Voorhiss and Tsedal Neeley summarise the story of how Dr. Timnit Gebru went from Ethiopia, to Boston, to a PhD at Stanford, and co-lead of Google AI Ethics, later to be fired when because she co-authored a paper that asked for companies to hold off on building large language models until we figured out how to handle the bias perpetuated by these models.\n\nThe article’s authors use these events as a case study to learn from when handling issues of ethics in AI.\n\n“The biggest message I want to convey is that AI can scale bias in ways that we can barely understand today”.\n“in failing to give Gebru the independence to do her job, might have sacrificed an opportunity to become a global leader in responsible AI development”.\nFinally, in this paper the authors test detection tools for AI-generated text in academic settings. “The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AI-generated text”. Across the 14 tools, the highest average accuracy was less than 80%, with 50% for AI-generated/human-edited text and 26% for machine-paraphrased AI-generated text.\n\n2. Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?\nThe first thing that comes to mind is Glaze by the University of Chicago which “works by understanding the AI models that are training on human art, and using machine learning algorithms, computing a set of minimal changes to artworks, such that it appears unchanged to human eyes, but appears to AI models like a dramatically different art style…So when someone then prompts the model to generate art mimicking the charcoal artist, they will get something quite different from what they expected.”\nI can’t imagine how something analogous to Glaze can be created for language, since plain text is just plain text, but conceptually, if human-written language is altered in a similar way, then it will be prevented from being generated similarly by LLMs like GPT. This would effect not just LLMs but anyone training their model on such altered data, but perhaps that is a cost worth having to prevent the perpetuation of copyrighted or disinformation content.\nAnother idea is that disinformation detection may benefit from a human-in-the-loop. AI-generated content that is not identified automatically may be identified by a human as disinformation. A big enough sample of accounts spreading this misinformation may lead to identifying broader trends in which accounts are fake.\n\n\n\n\n\n\n\nIn this section I’ll run code cells from the “clean” version (no markdown or outputs) of this notebook by Jeremy. I’ll add some thoughts as I run cells and add code to understand what is going on.\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 2.77MB/s]\n\n\n\n\n\n\n\n\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)\n\n\n# load the training data and look at it\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# see how many null values are in each column\ndf.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\nSince each Name is unique, there are 891 modes for the Name column. df.mode() will print these out as a DataFrame, with rows containing NaN for columns with fewer modes (e.g., Age has 1 mode, 24, and that is listed once in the first row in the output DataFrame for df.mode()).\n\n# see the most frequent values in each column\ndf.mode()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0.0\n      3.0\n      Abbing, Mr. Anthony\n      male\n      24.0\n      0.0\n      0.0\n      1601\n      8.05\n      B96 B98\n      S\n    \n    \n      1\n      2\n      NaN\n      NaN\n      Abbott, Mr. Rossmore Edward\n      NaN\n      NaN\n      NaN\n      NaN\n      347082\n      NaN\n      C23 C25 C27\n      NaN\n    \n    \n      2\n      3\n      NaN\n      NaN\n      Abbott, Mrs. Stanton (Rosa Hunt)\n      NaN\n      NaN\n      NaN\n      NaN\n      CA. 2343\n      NaN\n      G6\n      NaN\n    \n    \n      3\n      4\n      NaN\n      NaN\n      Abelson, Mr. Samuel\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      5\n      NaN\n      NaN\n      Abelson, Mrs. Samuel (Hannah Wizosky)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      NaN\n      NaN\n      de Mulder, Mr. Theodore\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      887\n      888\n      NaN\n      NaN\n      de Pelsmaeker, Mr. Alfons\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      888\n      889\n      NaN\n      NaN\n      del Carlo, Mr. Sebastiano\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      889\n      890\n      NaN\n      NaN\n      van Billiard, Mr. Austin Blyler\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      890\n      891\n      NaN\n      NaN\n      van Melkebeke, Mr. Philemon\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n891 rows × 12 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# view the topmost row of the modes DataFrame\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\n# fill missing data with the column's mode\ndf.fillna(modes, inplace=True)\n\n\n# check that we no longer have missing data\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\n\nimport numpy as np\n\n# view a summary of the data\ndf.describe(include=(np.number))\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      446.000000\n      0.383838\n      2.308642\n      28.566970\n      0.523008\n      0.381594\n      32.204208\n    \n    \n      std\n      257.353842\n      0.486592\n      0.836071\n      13.199572\n      1.102743\n      0.806057\n      49.693429\n    \n    \n      min\n      1.000000\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      223.500000\n      0.000000\n      2.000000\n      22.000000\n      0.000000\n      0.000000\n      7.910400\n    \n    \n      50%\n      446.000000\n      0.000000\n      3.000000\n      24.000000\n      0.000000\n      0.000000\n      14.454200\n    \n    \n      75%\n      668.500000\n      1.000000\n      3.000000\n      35.000000\n      1.000000\n      0.000000\n      31.000000\n    \n    \n      max\n      891.000000\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# view the skewed distribution of Fares\ndf['Fare'].hist();\n\n\n\n\nSo that it’s more normally distributed, we take the log of Fare. We add 1 to Fare before taking the logarithm so that we aren’t ever taking log of 0 (which is undefined).\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\ndf['LogFare'].hist();\n\n\n\n\n\n# view the unique values of pclass\npclasses = sorted(df.Pclass.unique())\npclasses\n\n[1, 2, 3]\n\n\n\n# look at string columns\ndf.describe(include=[object])\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Name\n      Sex\n      Ticket\n      Cabin\n      Embarked\n    \n  \n  \n    \n      count\n      891\n      891\n      891\n      891\n      891\n    \n    \n      unique\n      891\n      2\n      681\n      147\n      3\n    \n    \n      top\n      Braund, Mr. Owen Harris\n      male\n      347082\n      B96 B98\n      S\n    \n    \n      freq\n      1\n      577\n      7\n      691\n      646\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# get_dummies returns DataFrame with 0/1 values for categorical variable columns\ndf = pd.get_dummies(df, columns=['Sex', 'Pclass', 'Embarked'])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\n# view the new dummy variables\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Sex_male\n      Sex_female\n      Pclass_1\n      Pclass_2\n      Pclass_3\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nfrom torch import tensor\n\n# convert dependent variable to a tensor\nt_dep = tensor(df.Survived)\nt_dep[:5]\n\ntensor([0, 1, 1, 1, 0])\n\n\n\n# convert independent variables to a tensor\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\nindep_cols\n\n['Age',\n 'SibSp',\n 'Parch',\n 'LogFare',\n 'Sex_male',\n 'Sex_female',\n 'Pclass_1',\n 'Pclass_2',\n 'Pclass_3',\n 'Embarked_C',\n 'Embarked_Q',\n 'Embarked_S']\n\n\n\ndf[indep_cols].values\n\narray([[22.,  1.,  0., ...,  0.,  0.,  1.],\n       [38.,  1.,  0., ...,  1.,  0.,  0.],\n       [26.,  0.,  0., ...,  0.,  0.,  1.],\n       ...,\n       [24.,  1.,  2., ...,  0.,  0.,  1.],\n       [26.,  0.,  0., ...,  1.,  0.,  0.],\n       [32.,  0.,  0., ...,  0.,  1.,  0.]])\n\n\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep\n\ntensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [26.0000,  0.0000,  0.0000,  2.1889,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  1.0000,  0.0000,  3.9908,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  0.0000,  0.0000,  2.2469,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [54.0000,  0.0000,  0.0000,  3.9677,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        ...,\n        [25.0000,  0.0000,  0.0000,  2.0857,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [39.0000,  0.0000,  5.0000,  3.4054,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [27.0000,  0.0000,  0.0000,  2.6391,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [19.0000,  0.0000,  0.0000,  3.4340,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  1.0000,  2.0000,  3.1966,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [26.0000,  0.0000,  0.0000,  3.4340,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [32.0000,  0.0000,  0.0000,  2.1691,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000]])\n\n\n\n# 891 individuals\n# 12 columns\nt_indep.shape\n\ntorch.Size([891, 12])\n\n\n\n# initialize parameters\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625])\n\n\n\ncoeffs.shape\n\ntorch.Size([12])\n\n\n\n# normalize large values\nt_indep.max(dim=0)\n\ntorch.return_types.max(\nvalues=tensor([80.0000,  8.0000,  6.0000,  6.2409,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]),\nindices=tensor([630, 159, 678, 258,   0,   1,   1,   9,   0,   1,   5,   0]))\n\n\n\nvals, indices = t_indep.max(dim=0)\n\n# divide values in each column by the maximum in each column\n# using broadcasting\nt_indep = t_indep / vals\nt_indep\n\ntensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [0.3250, 0.0000, 0.0000, 0.3507, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4375, 0.1250, 0.0000, 0.6395, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        [0.4375, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.3000, 0.0000, 0.0000, 0.3600, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n        [0.6750, 0.0000, 0.0000, 0.6358, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        ...,\n        [0.3125, 0.0000, 0.0000, 0.3342, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4875, 0.0000, 0.8333, 0.5456, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n        [0.3375, 0.0000, 0.0000, 0.4229, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        [0.2375, 0.0000, 0.0000, 0.5502, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        [0.3000, 0.1250, 0.3333, 0.5122, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.3250, 0.0000, 0.0000, 0.5502, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [0.4000, 0.0000, 0.0000, 0.3476, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]])\n\n\nWhen calculating the predictions, each row of t_indep is element-wise multiplied by coeffs and is summed together with .sum(axis=1).\n\n# calculate predictions\n# predictions = matrix multiplication of independent variable values and parameters\n# each row\npreds = (t_indep*coeffs).sum(axis=1)\npreds.shape\n\ntorch.Size([891])\n\n\n\npreds[:10]\n\ntensor([ 0.1927, -0.6239,  0.0979,  0.2056,  0.0968,  0.0066,  0.1306,  0.3476,  0.1613, -0.6285])\n\n\nTo visualize the preds calculation, I’ll do the first prediction (0.1927) manually:\n\n# multiply coeffs by first row of t_indep and take the sum\n(t_indep[0]*coeffs)\n\ntensor([-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625])\n\n\n\n(t_indep[0]*coeffs).sum()\n\ntensor(0.1927)\n\n\n\nloss = torch.abs(preds-t_dep).mean()\nloss\n\ntensor(0.5382)\n\n\n\n# collect calculations into functions\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\n\n# get ready to calculate gradient\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\ntensor(0.5382, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\n\n\ncoeffs.grad\n\ntensor([-0.0106,  0.0129, -0.0041, -0.0484,  0.2099, -0.2132, -0.1212, -0.0247,  0.1425, -0.1886, -0.0191,  0.2043])\n\n\nIf we calculate loss again and calculate the gradients they will be added to the existing gradients:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad # notice how these are 2x the original gradients\n\ntensor([-0.0212,  0.0258, -0.0082, -0.0969,  0.4198, -0.4265, -0.2424, -0.0494,  0.2851, -0.3771, -0.0382,  0.4085])\n\n\nThis is why we set gradients back to zero.\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n  coeffs.sub_(coeffs.grad * 0.1)\n  coeffs.grad.zero_()\n  print(calc_loss(coeffs, t_indep, t_dep))\n\ntensor(0.4945)\n\n\nOur loss decreased after doing gradient descent.\nSplit data into training and validation sets\n\nfrom fastai.data.transforms import RandomSplitter\n\n\n# RandomSplitter gives indexes of the corresponding training/validation split\nRandomSplitter(seed=42)(df)\n\n((#713) [788,525,821,253,374,98,215,313,281,305...],\n (#178) [303,778,531,385,134,476,691,443,386,128...])\n\n\n\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\n\nlen(trn_split), len(val_split)\n\n(713, 178)\n\n\nUsing the training and validation indexes, create training and validation set independent and dependent variables:\n\ntrn_indep,val_indep = t_indep[trn_split], t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split], t_dep[val_split]\n\n\ntrn_indep.shape, trn_dep.shape\n\n(torch.Size([713, 12]), torch.Size([713]))\n\n\n\nval_indep.shape, val_dep.shape\n\n(torch.Size([178, 12]), torch.Size([178]))\n\n\nPut the stepping the parameters code into a function:\n\ndef update_coeffs(coeffs, lr):\n  coeffs.sub_(coeffs.grad * lr)\n  coeffs.grad.zero_()\n\nCreate function to train model for one epoch:\n\ndef one_epoch(coeffs, lr):\n  loss = calc_loss(coeffs, trn_indep, trn_dep)\n  loss.backward()\n  with torch.no_grad(): update_coeffs(coeffs, lr)\n  print(f\"{loss:.3f}\", end=\"; \")\n\nCreate a function to initialize parameters:\n\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\nCreate function to train a model for a given number of epochs:\n\ndef train_model(epochs=30, lr=0.01):\n  torch.manual_seed(442)\n  coeffs = init_coeffs()\n  for i in range(epochs): one_epoch(coeffs, lr=lr)\n  return coeffs\n\nTrain model for 18 epochs:\n\ncoeffs = train_model(18, lr=0.2)\n\n0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; \n\n\nThe loss consistently decreases each epoch.\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\n\nPositive coefficients indicate a positive correlation with survival, negative coefficients indicate negative correlation. For example, Sex_male coefficient is negative meaning that survival variable decreases if Sex_male is 1.\n\nshow_coeffs()\n\n{'Age': tensor(-0.2694),\n 'SibSp': tensor(0.0901),\n 'Parch': tensor(0.2359),\n 'LogFare': tensor(0.0280),\n 'Sex_male': tensor(-0.3990),\n 'Sex_female': tensor(0.2345),\n 'Pclass_1': tensor(0.7232),\n 'Pclass_2': tensor(0.4112),\n 'Pclass_3': tensor(0.3601),\n 'Embarked_C': tensor(0.0955),\n 'Embarked_Q': tensor(0.2395),\n 'Embarked_S': tensor(0.2122)}\n\n\nWith coefficients, we can calculate predictions and therefore accuracy:\n\npreds = calc_preds(coeffs, val_indep)\n\n\npreds[:10]\n\ntensor([ 0.8160,  0.1295, -0.0148,  0.1831,  0.1520,  0.1350,  0.7279,  0.7754,  0.3222,  0.6740])\n\n\n\npreds.shape # recall that we split the data into training and validation sets\n\ntorch.Size([178])\n\n\n\nval_dep.bool()[:10]\n\ntensor([ True, False, False, False, False, False,  True,  True, False,  True])\n\n\n\n(preds>0.5)[:10]\n\ntensor([ True, False, False, False, False, False,  True,  True, False,  True])\n\n\n\nresults = val_dep.bool()==(preds>0.5)\nresults[:10]\n\ntensor([True, True, True, True, True, True, True, True, True, True])\n\n\nCalculate accuracy:\n\nresults.float().mean()\n\ntensor(0.7865)\n\n\nPut accuracy calculation into a function:\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nacc(coeffs)\n\ntensor(0.7865)\n\n\nView sigmoid function:\n\nimport sympy\n\n\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5))\n\n\n\n\n<sympy.plotting.plot.Plot at 0x79c4dd276650>\n\n\nNotice how large positive values of x result in y values closer to 1 and large negative x values result in y closer to 0.\n\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-10,10))\n\n\n\n\n<sympy.plotting.plot.Plot at 0x79c4dc682fe0>\n\n\nWe update our prediction calculation function to incorporate sigmoid:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\n\ncoeffs = train_model(lr=100)\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nSex_male’s coefficient has significantly increased (negatively):\n\nshow_coeffs()\n\n{'Age': tensor(-1.5061),\n 'SibSp': tensor(-1.1575),\n 'Parch': tensor(-0.4267),\n 'LogFare': tensor(0.2543),\n 'Sex_male': tensor(-10.3320),\n 'Sex_female': tensor(8.4185),\n 'Pclass_1': tensor(3.8389),\n 'Pclass_2': tensor(2.1398),\n 'Pclass_3': tensor(-6.2331),\n 'Embarked_C': tensor(1.4771),\n 'Embarked_Q': tensor(2.1168),\n 'Embarked_S': tensor(-4.7958)}\n\n\nPredict test data set values:\n\ntst_df = pd.read_csv(path/'test.csv')\n\n\ntst_df.isna().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\nReplace missing Fare with 0:\n\ntst_df['Fare'] = tst_df.Fare.fillna(0)\n\nReplace other missing values with training set modes:\n\ntst_df.fillna(modes, inplace=True)\n\nApply the same data transformations as training set:\n\ntst_df['LogFare'] = np.log(tst_df['Fare']+1)\ntst_df = pd.get_dummies(tst_df, columns=['Sex', 'Pclass', 'Embarked'])\n\n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals\n\n\ntst_indep[:10]\n\ntensor([[0.4313, 0.0000, 0.0000, 0.3490, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n        [0.5875, 0.1250, 0.0000, 0.3332, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.7750, 0.0000, 0.0000, 0.3796, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n        [0.3375, 0.0000, 0.0000, 0.3634, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.2750, 0.1250, 0.1667, 0.4145, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.1750, 0.0000, 0.0000, 0.3725, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.3750, 0.0000, 0.0000, 0.3453, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n        [0.3250, 0.1250, 0.1667, 0.5450, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        [0.2250, 0.0000, 0.0000, 0.3377, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000],\n        [0.2625, 0.2500, 0.0000, 0.5167, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000]])\n\n\n\ntst_indep.shape\n\ntorch.Size([418, 12])\n\n\nCalculate predictions in the format expected by Kaggle:\n\ntst_df['Survived'] = (calc_preds(tst_indep, coeffs)>0.5).int()\n\n\nsub_df = tst_df[['PassengerId', 'Survived']]\nsub_df.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n    \n  \n  \n    \n      0\n      892\n      0\n    \n    \n      1\n      893\n      0\n    \n    \n      2\n      894\n      0\n    \n    \n      3\n      895\n      0\n    \n    \n      4\n      896\n      0\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nUse @ operator for matrix multiplication:\n\n(val_indep*coeffs).sum(axis=1)\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3512, -13.6469,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])\n\n\n\nval_indep@coeffs\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])\n\n\nUpdate prediction calculation so that it uses matrix multiplication operator:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\nRecreate coefficients and dependent variable so they are in the correct shape for matrix multiplication (when doing matrix-matrix products later on):\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\n\ntrn_dep.shape, val_dep.shape\n\n(torch.Size([713]), torch.Size([178]))\n\n\n\ntrn_dep = trn_dep[:, None]\nval_dep = val_dep[:, None]\n\n\ntrn_dep.shape, val_dep.shape\n\n(torch.Size([713, 1]), torch.Size([178, 1]))\n\n\n\ncoeffs = train_model(lr=100)\n\n0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nOur model hasn’t changed other than the fact that we are now using matrix product explicitly.\nLet’s create a neural net:\n\ntorch.rand(1)[0]\n\ntensor(0.6722)\n\n\n\ndef init_coeffs(n_hidden=20):\n  layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n  layer2 = torch.rand(n_hidden, 1)-0.3\n  const = torch.rand(1)[0]\n  return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n  l1, l2, const = coeffs\n  res = F.relu(indeps@l1)\n  res = res@l2 + const\n  return torch.sigmoid(res)\n\nAs an aside, showing that the order of matrix multiplication operands matters—you get very different results:\n\ntensor([[1,2,3], [4,5,6]]).shape\n\ntorch.Size([2, 3])\n\n\n\ntensor([[1, 2], [3, 4], [5, 6]]).shape\n\ntorch.Size([3, 2])\n\n\n\ntensor([[1,2,3], [4,5,6]]) @ tensor([[1, 2], [3, 4], [5, 6]])\n\ntensor([[22, 28],\n        [49, 64]])\n\n\n\ntensor([[1, 2], [3, 4], [5, 6]]) @ tensor([[1,2,3], [4,5,6]])\n\ntensor([[ 9, 12, 15],\n        [19, 26, 33],\n        [29, 40, 51]])\n\n\nBack to updating our functions to handle neural nets:\n\ndef update_coeffs(coeffs, lr):\n  for layer in coeffs:\n    layer.sub_(layer.grad * lr)\n    layer.grad.zero_()\n\n\ncoeffs = train_model(lr=1.4)\n\n0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; \n\n\n\ncoeffs = train_model(lr=20)\n\n0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nNext we train a deep learning model:\n\ndef init_coeffs():\n  hiddens = [10,10]\n  sizes = [n_coeff] + hiddens + [1]\n  n = len(sizes)\n  layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n  consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n  for l in layers+consts: l.requires_grad_()\n  return layers,consts\n\nI’ll run through this function’s code line by line to make sure I see what’s going on:\n\nhiddens = [10,10]\n\n\nsizes = [n_coeff] + hiddens + [1]\nsizes\n\n[12, 10, 10, 1]\n\n\n\nn = len(sizes)\nn\n\n4\n\n\n\n[(sizes[i], sizes[i+1]) for i in range (n-1)]\n\n[(12, 10), (10, 10), (10, 1)]\n\n\n\n[(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n\n[tensor(-0.0371), tensor(0.0406), tensor(-0.0461)]\n\n\nCool! I can see it now. Next we update the function which calculates predictions to handle a deep neural net:\n\ndef calc_preds(coeffs, indeps):\n  layers,consts = coeffs\n  n = len(layers)\n  res = indeps\n  for i,l in enumerate(layers):\n    res = res@l + consts[i]\n    # pass through ReLU for all layers except the last one\n    if i!=n-1: res = F.relu(res)\n  return torch.sigmoid(res)\n\n\ndef update_coeffs(coeffs, lr):\n  layers,consts = coeffs\n  for layer in layers+consts:\n    layer.sub_(layer.grad * lr)\n    layer.grad.zero_()\n\n\ncoeffs = train_model(lr=4)\n\n0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nThat’s a wrap for that notebook! It all makes clear sense now after running through the code line by line. We trained a linear model, neural net, and deep learning model and got similar results. In this case, as discussed in the video, the deep learning model doesn’t improve our results.\n\n\n\nIn this section I run through the “clean” version of Jeremy’s notebook.\n\nfrom fastai.tabular.all import *\n\n\npd.options.display.float_format = '{:.2f}'.format\nset_seed(42)\n\n\n# read in the data\ndf = pd.read_csv(path/'train.csv')\n\n\n# view the data\ndf.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.00\n      1\n      0\n      A/5 21171\n      7.25\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.00\n      1\n      0\n      PC 17599\n      71.28\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.00\n      0\n      0\n      STON/O2. 3101282\n      7.92\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.00\n      1\n      0\n      113803\n      53.10\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.00\n      0\n      0\n      373450\n      8.05\n      NaN\n      S\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# feature engineering\ndef add_features(df):\n  df['LogFare'] = np.log1p(df['Fare'])\n  df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))\n  df['Family'] = df.SibSp+df.Parch\n  df['Alone'] = df.Family == 0\n  df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')\n  df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n  df['Title'] = df.Title.map(dict(Mr=\"Mr\", Miss=\"Miss\", Mrs=\"Mrs\", Master=\"Master\"))\n\nI’ll look at some of these in more detail to breakdown what is happening:\n\ndf.Cabin.str[0].unique()\n\narray([nan, 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T'], dtype=object)\n\n\n\ndf.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\")).unique()\n\narray([nan, 'ABC', 'DE', 'FG'], dtype=object)\n\n\n\ndf.Ticket\n\n0             A/5 21171\n1              PC 17599\n2      STON/O2. 3101282\n3                113803\n4                373450\n             ...       \n886              211536\n887              112053\n888          W./C. 6607\n889              111369\n890              370376\nName: Ticket, Length: 891, dtype: object\n\n\n\ndf.groupby('Ticket')['Ticket'].transform('count')\n\n0      1\n1      1\n2      1\n3      2\n4      1\n      ..\n886    1\n887    1\n888    2\n889    1\n890    1\nName: Ticket, Length: 891, dtype: int64\n\n\n\n# there should be 2 count of this ticket\ndf.query('Ticket == \"113803\"')\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.00\n      1\n      0\n      113803\n      53.10\n      C123\n      S\n    \n    \n      137\n      138\n      0\n      1\n      Futrelle, Mr. Jacques Heath\n      male\n      37.00\n      1\n      0\n      113803\n      53.10\n      C123\n      S\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# expand = True splits into separate columns\ndf.Name.str.split(', ', expand=True).head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      Braund\n      Mr. Owen Harris\n    \n    \n      1\n      Cumings\n      Mrs. John Bradley (Florence Briggs Thayer)\n    \n    \n      2\n      Heikkinen\n      Miss. Laina\n    \n    \n      3\n      Futrelle\n      Mrs. Jacques Heath (Lily May Peel)\n    \n    \n      4\n      Allen\n      Mr. William Henry\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ndf.Name.str.split(', ', expand=False)\n\n0                                  [Braund, Mr. Owen Harris]\n1      [Cumings, Mrs. John Bradley (Florence Briggs Thayer)]\n2                                   [Heikkinen, Miss. Laina]\n3             [Futrelle, Mrs. Jacques Heath (Lily May Peel)]\n4                                 [Allen, Mr. William Henry]\n                               ...                          \n886                                  [Montvila, Rev. Juozas]\n887                           [Graham, Miss. Margaret Edith]\n888               [Johnston, Miss. Catherine Helen \"Carrie\"]\n889                                  [Behr, Mr. Karl Howell]\n890                                    [Dooley, Mr. Patrick]\nName: Name, Length: 891, dtype: object\n\n\n\ndf.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0].unique()\n\narray(['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess',\n       'Jonkheer'], dtype=object)\n\n\nThe line df.Title.map(dict(Mr=\"Mr\", Miss=\"Miss\", Mrs=\"Mrs\", Master=\"Master\")) reduces the number of titles to 4.\n\ndf.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0].map(dict(Mr=\"Mr\", Miss=\"Miss\", Mrs=\"Mrs\", Master=\"Master\")).unique()\n\narray(['Mr', 'Mrs', 'Miss', 'Master', nan], dtype=object)\n\n\n\n# add the features to our dataframe\nadd_features(df)\n\n\ndf.Title.unique()\n\narray(['Mr', 'Mrs', 'Miss', 'Master', nan], dtype=object)\n\n\n\ndf.Deck.unique()\n\narray([nan, 'ABC', 'DE', 'FG'], dtype=object)\n\n\n\ndf.Family.unique()\n\narray([ 1,  0,  4,  2,  6,  5,  3,  7, 10])\n\n\n\ndf.LogFare.hist();\n\n\n\n\n\ndf.Alone.unique()\n\narray([False,  True])\n\n\n\ndf.TicketFreq.hist();\n\n\n\n\n\n# create training and validation index lists\nsplits = RandomSplitter(seed=42)(df)\n\n\nsplits\n\n((#713) [788,525,821,253,374,98,215,313,281,305...],\n (#178) [303,778,531,385,134,476,691,443,386,128...])\n\n\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'LogFare', 'Deck',\n       'Family', 'Alone', 'TicketFreq', 'Title'],\n      dtype='object')\n\n\n\n# create dataloaders object\ndls = TabularPandas(\n    df,\n    splits=splits,\n    procs=[Categorify, FillMissing, Normalize],\n    cat_names=[\"Sex\", \"Pclass\", \"Embarked\", \"Deck\", \"Title\"],\n    cont_names=[\"Age\", \"SibSp\", \"Parch\", \"LogFare\", \"Alone\", \"TicketFreq\", \"Family\"],\n    y_names=\"Survived\",\n    y_block=CategoryBlock()\n).dataloaders(path=\".\")\n\n\n# view a batch\ndls.show_batch()\n\n\n\n  \n    \n      \n      Sex\n      Pclass\n      Embarked\n      Deck\n      Title\n      Age_na\n      Age\n      SibSp\n      Parch\n      LogFare\n      Alone\n      TicketFreq\n      Family\n      Survived\n    \n  \n  \n    \n      0\n      male\n      3\n      Q\n      #na#\n      Mr\n      True\n      28.00\n      1.00\n      -0.00\n      2.80\n      0.00\n      2.00\n      1.00\n      0\n    \n    \n      1\n      male\n      3\n      C\n      #na#\n      Mr\n      False\n      30.00\n      0.00\n      -0.00\n      2.11\n      1.00\n      1.00\n      -0.00\n      0\n    \n    \n      2\n      male\n      3\n      S\n      #na#\n      Mr\n      False\n      28.00\n      2.00\n      -0.00\n      2.19\n      0.00\n      1.00\n      2.00\n      0\n    \n    \n      3\n      female\n      3\n      S\n      #na#\n      Miss\n      False\n      45.00\n      0.00\n      -0.00\n      2.17\n      1.00\n      1.00\n      -0.00\n      0\n    \n    \n      4\n      male\n      2\n      S\n      #na#\n      Mr\n      True\n      28.00\n      0.00\n      -0.00\n      0.00\n      1.00\n      1.00\n      -0.00\n      0\n    \n    \n      5\n      male\n      3\n      S\n      #na#\n      Mr\n      True\n      28.00\n      0.00\n      -0.00\n      2.78\n      1.00\n      1.00\n      -0.00\n      0\n    \n    \n      6\n      male\n      1\n      S\n      ABC\n      Mr\n      False\n      38.00\n      0.00\n      1.00\n      5.04\n      0.00\n      3.00\n      1.00\n      0\n    \n    \n      7\n      male\n      1\n      C\n      ABC\n      #na#\n      False\n      32.00\n      0.00\n      -0.00\n      3.45\n      1.00\n      1.00\n      -0.00\n      1\n    \n    \n      8\n      male\n      2\n      S\n      #na#\n      Mr\n      False\n      24.00\n      2.00\n      -0.00\n      4.31\n      0.00\n      5.00\n      2.00\n      0\n    \n    \n      9\n      male\n      2\n      S\n      #na#\n      Mr\n      False\n      48.00\n      0.00\n      -0.00\n      2.64\n      1.00\n      1.00\n      -0.00\n      0\n    \n  \n\n\n\n\nlearn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n\n\nlearn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=0.04786301031708717, valley=0.015848932787775993)\n\n\n\n\n\n\nlearn.fit(16, lr=0.03)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.551385\n      0.558225\n      0.595506\n      00:00\n    \n    \n      1\n      0.498181\n      0.578588\n      0.752809\n      00:00\n    \n    \n      2\n      0.472778\n      0.471495\n      0.803371\n      00:00\n    \n    \n      3\n      0.447318\n      0.430369\n      0.825843\n      00:00\n    \n    \n      4\n      0.432644\n      0.454893\n      0.808989\n      00:00\n    \n    \n      5\n      0.421892\n      0.397669\n      0.825843\n      00:00\n    \n    \n      6\n      0.413710\n      0.406790\n      0.814607\n      00:00\n    \n    \n      7\n      0.406777\n      0.430182\n      0.825843\n      00:00\n    \n    \n      8\n      0.402777\n      0.434063\n      0.837079\n      00:00\n    \n    \n      9\n      0.397782\n      0.425264\n      0.814607\n      00:00\n    \n    \n      10\n      0.392991\n      0.413648\n      0.837079\n      00:00\n    \n    \n      11\n      0.390115\n      0.422005\n      0.820225\n      00:00\n    \n    \n      12\n      0.385480\n      0.412861\n      0.837079\n      00:00\n    \n    \n      13\n      0.383542\n      0.403564\n      0.820225\n      00:00\n    \n    \n      14\n      0.380573\n      0.422910\n      0.831461\n      00:00\n    \n    \n      15\n      0.378466\n      0.444065\n      0.820225\n      00:00\n    \n  \n\n\n\n\n# prep test data for submission\ntst_df = pd.read_csv(path/'test.csv')\ntst_df['Fare'] = tst_df.Fare.fillna(0)\ntst_df.columns\n\nIndex(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\nadd_features(tst_df)\ntst_df.columns\n\nIndex(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked', 'LogFare', 'Deck', 'Family',\n       'Alone', 'TicketFreq', 'Title'],\n      dtype='object')\n\n\n\ntst_dl = learn.dls.test_dl(tst_df)\ntst_dl.show_batch()\n\n\n\n  \n    \n      \n      Sex\n      Pclass\n      Embarked\n      Deck\n      Title\n      Age_na\n      Age\n      SibSp\n      Parch\n      LogFare\n      Alone\n      TicketFreq\n      Family\n    \n  \n  \n    \n      0\n      male\n      3\n      Q\n      #na#\n      Mr\n      False\n      34.50\n      0.00\n      -0.00\n      2.18\n      1.00\n      1.00\n      -0.00\n    \n    \n      1\n      female\n      3\n      S\n      #na#\n      Mrs\n      False\n      47.00\n      1.00\n      -0.00\n      2.08\n      0.00\n      1.00\n      1.00\n    \n    \n      2\n      male\n      2\n      Q\n      #na#\n      Mr\n      False\n      62.00\n      0.00\n      -0.00\n      2.37\n      1.00\n      1.00\n      -0.00\n    \n    \n      3\n      male\n      3\n      S\n      #na#\n      Mr\n      False\n      27.00\n      0.00\n      -0.00\n      2.27\n      1.00\n      1.00\n      -0.00\n    \n    \n      4\n      female\n      3\n      S\n      #na#\n      Mrs\n      False\n      22.00\n      1.00\n      1.00\n      2.59\n      0.00\n      1.00\n      2.00\n    \n    \n      5\n      male\n      3\n      S\n      #na#\n      Mr\n      False\n      14.00\n      0.00\n      -0.00\n      2.32\n      1.00\n      1.00\n      -0.00\n    \n    \n      6\n      female\n      3\n      Q\n      #na#\n      Miss\n      False\n      30.00\n      0.00\n      -0.00\n      2.16\n      1.00\n      1.00\n      -0.00\n    \n    \n      7\n      male\n      2\n      S\n      #na#\n      Mr\n      False\n      26.00\n      1.00\n      1.00\n      3.40\n      0.00\n      1.00\n      2.00\n    \n    \n      8\n      female\n      3\n      C\n      #na#\n      Mrs\n      False\n      18.00\n      0.00\n      -0.00\n      2.11\n      1.00\n      1.00\n      -0.00\n    \n    \n      9\n      male\n      3\n      S\n      #na#\n      Mr\n      False\n      21.00\n      2.00\n      -0.00\n      3.22\n      0.00\n      1.00\n      2.00\n    \n  \n\n\n\nget_preds returns predictions for both categories of Survived (0 and 1).\n\nlearn.get_preds(dl=tst_dl)[0][:5]\n\n\n\n\n\n\n\n\ntensor([[0.9141, 0.0859],\n        [0.5954, 0.4046],\n        [0.9711, 0.0289],\n        [0.9268, 0.0732],\n        [0.4136, 0.5864]])\n\n\n\nlearn.get_preds(dl=tst_dl)[0][:5].sum(axis=1)\n\n\n\n\n\n\n\n\ntensor([1., 1., 1., 1., 1.])\n\n\n\n# targets are empty---why?\nlearn.get_preds(dl=tst_dl)[1]\n\n\n\n\n\n\n\n\n\npreds,_ = learn.get_preds(dl=tst_dl)\n\n\n\n\n\n\n\n\n\ntst_df['Survived'] = (preds[:,1]>0.5).int()\n\n\ntst_df.Survived.unique()\n\narray([0, 1], dtype=int32)\n\n\n\nsub_df = tst_df[['PassengerId', 'Survived']]\n\n\nsub_df.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n    \n  \n  \n    \n      0\n      892\n      0\n    \n    \n      1\n      893\n      0\n    \n    \n      2\n      894\n      0\n    \n    \n      3\n      895\n      0\n    \n    \n      4\n      896\n      1\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# ensembling\ndef ensemble():\n  learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n  with learn.no_bar(), learn.no_logging(): learn.fit(16, lr=0.03)\n  return learn.get_preds(dl=tst_dl)[0]\n\n\nlearns = [ensemble() for _ in range(5)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nens_preds = torch.stack(learns).mean(0)\n\n\ntorch.stack(learns).shape\n\ntorch.Size([5, 418, 2])\n\n\n\nens_preds.shape\n\ntorch.Size([418, 2])\n\n\n\ntst_df['Survived'] = (ens_preds[:,1]>0.5).int()\nsub_df = tst_df[['PassengerId', 'Survived']]\n\n\nsub_df.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n    \n  \n  \n    \n      0\n      892\n      0\n    \n    \n      1\n      893\n      0\n    \n    \n      2\n      894\n      0\n    \n    \n      3\n      895\n      0\n    \n    \n      4\n      896\n      1\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\nIn this section I run through the “clean” version of Jeremy’s notebook.\n\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\n\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\n\n\ndf.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.00\n      1\n      0\n      A/5 21171\n      7.25\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.00\n      1\n      0\n      PC 17599\n      71.28\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.00\n      0\n      0\n      STON/O2. 3101282\n      7.92\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.00\n      1\n      0\n      113803\n      53.10\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.00\n      0\n      0\n      373450\n      8.05\n      NaN\n      S\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ntst_df.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.50\n      0\n      0\n      330911\n      7.83\n      NaN\n      Q\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.00\n      1\n      0\n      363272\n      7.00\n      NaN\n      S\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.00\n      0\n      0\n      240276\n      9.69\n      NaN\n      Q\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.00\n      0\n      0\n      315154\n      8.66\n      NaN\n      S\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.00\n      1\n      1\n      3101298\n      12.29\n      NaN\n      S\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                      0.00\nPclass                        3.00\nName           Abbing, Mr. Anthony\nSex                           male\nAge                          24.00\nSibSp                         0.00\nParch                         0.00\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\n# pre-processing\ndef proc_data(df):\n  df['Fare'] = df.Fare.fillna(0)\n  df.fillna(modes, inplace=True)\n  df['LogFare'] = np.log1p(df['Fare'])\n  df['Embarked'] = pd.Categorical(df.Embarked)\n  df['Sex'] = pd.Categorical(df.Sex)\n\n\ndf.Embarked.unique()\n\narray(['S', 'C', 'Q', nan], dtype=object)\n\n\n\npd.Categorical(df.Embarked).unique()\n\n['S', 'C', 'Q', NaN]\nCategories (3, object): ['C', 'Q', 'S']\n\n\n\nproc_data(df)\nproc_data(tst_df)\n\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'LogFare'],\n      dtype='object')\n\n\n\ntst_df.columns\n\nIndex(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked', 'LogFare'],\n      dtype='object')\n\n\n\ndf.Sex\n\n0        male\n1      female\n2      female\n3      female\n4        male\n        ...  \n886      male\n887    female\n888    female\n889      male\n890      male\nName: Sex, Length: 891, dtype: category\nCategories (2, object): ['female', 'male']\n\n\n\ntst_df.Sex\n\n0        male\n1      female\n2        male\n3        male\n4      female\n        ...  \n413      male\n414    female\n415      male\n416      male\n417      male\nName: Sex, Length: 418, dtype: category\nCategories (2, object): ['female', 'male']\n\n\n\ncats=[\"Sex\", \"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare', 'Pclass']\ndep=\"Survived\"\n\nCategoricals are stored as integers but shown as their labels:\n\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\n\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8\n\n\n\nimport seaborn as sns\n\nSex alone is a pretty good indicator of survival:\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\n\ntrn_df[cats].head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Sex\n      Embarked\n    \n  \n  \n    \n      298\n      1\n      2\n    \n    \n      884\n      1\n      2\n    \n    \n      247\n      0\n      2\n    \n    \n      478\n      1\n      2\n    \n    \n      305\n      1\n      2\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nval_df[cats].head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Sex\n      Embarked\n    \n  \n  \n    \n      709\n      1\n      0\n    \n    \n      439\n      1\n      2\n    \n    \n      840\n      1\n      2\n    \n    \n      720\n      0\n      2\n    \n    \n      39\n      0\n      0\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ndef xs_y(df):\n  xs = df[cats+conts].copy()\n  return xs,df[dep] if dep in df else None\n\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\n\ntrn_xs.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Sex\n      Embarked\n      Age\n      SibSp\n      Parch\n      LogFare\n      Pclass\n    \n  \n  \n    \n      298\n      1\n      2\n      24.00\n      0\n      0\n      3.45\n      1\n    \n    \n      884\n      1\n      2\n      25.00\n      0\n      0\n      2.09\n      3\n    \n    \n      247\n      0\n      2\n      24.00\n      0\n      2\n      2.74\n      2\n    \n    \n      478\n      1\n      2\n      22.00\n      0\n      0\n      2.14\n      3\n    \n    \n      305\n      1\n      2\n      0.92\n      1\n      2\n      5.03\n      1\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ntrn_y.head()\n\n298    1\n884    0\n247    1\n478    0\n305    1\nName: Survived, dtype: int64\n\n\n\n# sex as the only predictor\npreds = val_xs.Sex==0\n\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\n\ndf_fare = trn_df[trn_df.LogFare>0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\nIt looks like people survived for LogFare values above 2.7ish (2.5ish is the median LogFare value for deaths).\n\n# LogFare as a sole predictor\npreds = val_xs.LogFare>2.7\n\n\nmean_absolute_error(val_y, preds)\n\n0.336322869955157\n\n\nWe get a larger error than Sex.\n\ndef _side_score(side, y):\n  tot = side.sum()\n  if tot<=1: return 0\n  return y[side].std()*tot\n\n\ndef score(col, y, split):\n  lhs = col<=split\n  return (_side_score(lhs, y) + _side_score(~lhs, y))/len(y)\n\n\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.40787530982063946\n\n\n\nlhs = trn_xs[\"Sex\"] <= 0.5\n\n\nlhs.sum()\n\n229\n\n\n\ntrn_y[lhs].std()*lhs.sum()\n\n100.36927432272375\n\n\n\ntrn_y[~lhs].std()*(~lhs).sum()\n\n172.0914326374634\n\n\n\nlen(trn_y)\n\n668\n\n\n\n(100.36927432272375 + 172.0914326374634)/668\n\n0.40787530982063946\n\n\n\nscore(trn_xs[\"LogFare\"], trn_y, 2.7)\n\n0.47180873952099694\n\n\nA smaller score means less variation on each side.\n\ndef iscore(nm, split):\n  col = trn_xs[nm]\n  return score(col, trn_y, split)\n\nfrom ipywidgets import interact\ninteract(nm=conts, split=15.5)(iscore);\n\n\n\n\n\ninteract(nm=cats, split=15.5)(iscore);\n\n\n\n\n\nnm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nunq[scores.argmin()]\n\n6.0\n\n\n\nscores.min()\n\n0.478316717508991\n\n\n\nscore(trn_xs[\"Age\"], trn_y, 6)\n\n0.478316717508991\n\n\n\ndef min_col(df, nm):\n  col, y = df[nm], df[dep]\n  unq = col.dropna().unique()\n  scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n  idx = scores.argmin()\n  return unq[idx],scores[idx]\n\n\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.478316717508991)\n\n\n\ncols = cats+conts\n{o: min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}\n\n\n\ncols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales, females = trn_df[ismale], trn_df[~ismale]\n\n\n{o: min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}\n\n\n\n{o: min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}\n\n\nThe next split after Sex is Age<=6 for males and Pclass<=2 for females.\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\n\nimport graphviz\n\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n  s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                    special_characters=True, rotate=False, precision=precision, **kwargs)\n  return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\ndef gini(cond):\n  act = df.loc[cond, dep]\n  return 1 - act.mean()**2 - (1-act).mean()**2\n\n\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.2242152466367713\n\n\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=60)\n\n\n\n\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.18385650224215247\n\n\n\ntst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\ntst_xs, _ = xs_y(tst_df)\n\n\ntst_xs.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Sex\n      Embarked\n      Age\n      SibSp\n      Parch\n      LogFare\n      Pclass\n    \n  \n  \n    \n      0\n      1\n      1\n      34.50\n      0\n      0\n      2.18\n      3\n    \n    \n      1\n      0\n      2\n      47.00\n      1\n      0\n      2.08\n      3\n    \n    \n      2\n      1\n      1\n      62.00\n      0\n      0\n      2.37\n      2\n    \n    \n      3\n      1\n      2\n      27.00\n      0\n      0\n      2.27\n      3\n    \n    \n      4\n      0\n      2\n      22.00\n      1\n      1\n      2.59\n      3\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ndef subm(preds, suff):\n  tst_df['Survived'] = preds\n  sub_df = tst_df[['PassengerId', 'Survived']]\n  sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nsubm(m.predict(tst_xs), 'tree')\n\n\ndef get_tree(prop=0.75):\n  n = len(trn_y)\n  idxs = random.choice(n, int(n*prop))\n  return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n\n\ntrees = [get_tree() for t in range(100)]\n\n\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n0.22811659192825115\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\n\n\n\n\nKaggle sets an environment variable that you can check to see if you’re on Kaggle.\ndf.isna() returns a DataFrame with boolean values (True if the value is NaN).\nIf you call sum on a DataFrame it sums up each column.\nEasiest method to impute missing values is to replace them with the mode.\nMode works for both categorical and continuous variables.\nFirst baseline model shouldn’t involve doing complicated things.\nNever throw out columns with missing values. Maybe it turns out that the row missing a value is great predictor.\nSome types of models (like linear) don’t like long-tailed distributions like Fare. Neural nets are better behaved without them as well.\nThings that grow exponentially you want to take the log of (money, population, etc.).\nDummy variables turn categoricals into 1/0 valued columns for each categorical.\nFor n levels if you create n 0/1 columns you don’t have to add a constant term to the model.\nYou can create a 82% accurate model just using names.\nIdea of tensor came from notation in 1950. Ken Iverson.\nThe most important attribute of a tensor is its shape. The length of the shape is its rank.\n\n\n\n\nThe number of coefficients we need is the number of columns in the independent variable.\nComputers can’t create truly random numbers and instead create a sequence of numbers that behave in a random-like way.\nA lot of people are into reproducible results—Jeremy disagrees. An important part of understanding your data is understanding how much it varies from run to run. Run things a few times and get an intuitive sense of how stable it is.\nbroadcasting comes from APL. Happens in optimized C code (CPU) or CUDA (GPU). As long as the last axes match it’ll broadcast. It uses a kind of “virtual copying”.\nLinear model: coefficients times the values, added together.\nAge is bigger than any other columns so it will always have a larger value. Not ideal for optimization.\nNormalize the columns (divide by maximum in the column).\nAnother common way to normalize is subtracting the mean and dividing by the standard deviation.\nMean absolute value is a good loss function to start with.\nIn notebooks, do everything step-by-step manually and then copy it into a function.\nPyTorch functions with an underscore at the end will do an in-place operation.\n.backward() calls the gradient function.\nIf the gradient is negative it means that if we increase that coefficient, the loss will go down. If it’s positive that means if we decrease that coefficient, the loss will go down.\nRandomSplitter(seed=42)(df) returns indexes (training, validation) of the split.\nWe can’t use accuracy as a loss function because it doesn’t have a smooth gradient.\nSigmoid makes it easier to optimize—optimizer doesn’t have to exactly hit 0 or 1, it can predict a really big number and it gets converted to 1 or a really small number that gets converted to 0.\nSigmoid = 1/(1+exp(-x))\nsympy package does symbolic calculations and plots.\nWith sigmoid, we could increae the learning rate from 0.1 to 2, showing that it truly is easier to optimize.\nbinary dependent variable: chuck it through sigmoid.\nfastai always creates an extra category called “other” for categorical columns. At test time if you have a level that wasn’t in training, fastai puts it into the “other” category for you.\nFor categorical variables fastai puts less common ones into “other”.\n\n\n\n\n\n(indeps*coeffs).sum(axis=1) is the same thing as matrix multiplication.\ninit_coeffs changed to create an ncoeff by 1 matrix instead of an ncoeff vector, since for the neural net we will have multiple columns of coefficients.\ntensor[:,None] indexes into second dimension None it creates that dimension.\nDimension of 1 is a “unit axis”.\ntorch.Size([12, 1]) represents a rank-2 tensor with a trailing unit axis.\nIf our coefficients are too big or too small, it’s not going to train at all so you have to fiddle with their magnitude in a from-scratch model.\n\n\n\n\n\nJeremy divides the first layer coefficients by n_hidden since the coeffs will get multiplied by a second layer as well and we want the coeffs to be a similar size as the linear model.\nThe final layer absolutely needs a constant term.\nA deep learning model has multiple hidden layers.\ntorch.sigmoid and F.relu are the activation functions for the layers.\nFor very small datasets with very few columns and columns that are really simple, deep learning is not necessarily going to give you the best result. Nothing is going to be as good as a carefully designed model that uses just the name column.\nFor data types which have a very consitent structure like images or natural language text documents you can chuck a deep learning neural net at it and expect great results. Generally for tabular data that’s not the case. Normally have to think pretty long and hard about feature engineering to get good results.\nYou want to make choices for the non-obvious things and have the obvious things done for you by a package like fastai.\n\n\n\n\n\nCategorify handles dummy variables.\nlearner.lr_find starts at a very small learning rate like 10e-7, trains one batch of data and calculates the loss, increases the learning rate slightly and calculates the loss again. Picking a learning rate between slide and valley generally works well for training.\nlearn.dls.test_dl creates a DataLoader that contains exactly the same processing steps that our learner used.\nYou want to make sure your inference time pre-processing and transformations are exactly the same as training time.\nEnsembling is about creating multiple models and combining their predictions.\n\n\n\n\n\nRandom forets are elegant, and almost impossible to mess up. Jeremy has seen far more examples in industry of people messing up logistic regression than random forests.\nHandy shortcut: from fastai.imports import *.\ndf.col_name.cat.codes shows actual values (numbers corresponding to list of categories) for categorical column.\nA random forest is an ensemble of trees, a tree is an ensemble of binary splits.\nA binary split is something that splits the rows into two groups.\nKernel density plot is like a histogram with infinitesimally narrow bins.\nA good split is one where all of the values of the dependent variable on one side are all pretty much the same and all of dependent variable values on the other side are pretty much the same.\nYou want each of your groups, within the group, to be as similar as possible on the dependent variable.\n“how similar are all the things in the group” = standard deviation.\nSex is the best single binary split model we can find.\n“OneR” model: create a single binary split and stop.\nDon’t assume that you have to go complicated. It’s not a bad idea of always creating a baseline of OneR (a decision tree with a single binary split).\n\n\n\n\n\n\nThe objective of tabular modeling is to predict the value in one column based on the values in the other columns.\n\n\n\n\nContinuous variables can be directly fed to the model (with some optional preprocessing).\nCategorical variables need to be converted to numbers. Addition and multiplication don’t have meaning for them even if they’re stored as numbers.\nRossmann competition example notebook\nThe embedding layer is just another layer in the model.\nThe embedding transforms the categorical variables into inputs that are both continuous and meaningful.\nThe raw categorical data is transformed by an embedding layer before it interacts with the raw continuous input data.\nDeep learning is not always the best starting point fo analyzing tabular data.\n\n\n\n\n\nRecent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n\nEnsembles of decision trees (random forests and gradient boosting machines), mainly for structured data. They train faster, are often easier to interpret, do not require GPU for inference at scale, often require less hyperparameter tuning, and have a more mature ecosystem of tooling and documentation.\nMultilayered neural networks learned with SGD (shallow and/or deep learning) mainly for unstructured data (audio, images, and natural language)\n\nThe critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles.\nThere are tools and methods for answering questions like:\n\nWhich columns in the dataset were the most important for your predictions?\nHow are they related to the dependent variable?\nHow do they interact with each other?\nWhich particular features were most important for some particular observation?\n\nEnsembles of decision trees are our first approach for analyzing a new tabular dataset except when there are some high-cardinality categorical variables that are very important or when there are some columns that contain data that would be understood with a neural network such as plain text data.\n\n\n\n\n\nBlue Book for Bulldozers Kaggle competition: the goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration.\n\n\n!pip install dtreeviz\n\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\n\nimport zipfile,kaggle\n\npath = Path('bluebook-for-bulldozers')\nif not path.exists():\n  kaggle.api.competition_download_cli(str(path))\n  zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading bluebook-for-bulldozers.zip to /content\n\n\n100%|██████████| 48.4M/48.4M [00:01<00:00, 36.3MB/s]\n\n\n\n\n\n\npath.ls(file_type='text')\n\n(#7) [Path('bluebook-for-bulldozers/Valid.csv'),Path('bluebook-for-bulldozers/median_benchmark.csv'),Path('bluebook-for-bulldozers/Test.csv'),Path('bluebook-for-bulldozers/Machine_Appendix.csv'),Path('bluebook-for-bulldozers/TrainAndValid.csv'),Path('bluebook-for-bulldozers/ValidSolution.csv'),Path('bluebook-for-bulldozers/random_forest_benchmark_test.csv')]\n\n\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\n\n\nlen(df.columns)\n\n53\n\n\n\ndf.columns\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\n\ndf.SalePrice.hist();\n\n\n\n\n\ndf.plot(x=\"saledate\", y=\"SalePrice\");\n\n\n\n\n\nlen(df.SalesID.unique())\n\n412698\n\n\n\nlen(df.MachineID.unique())\n\n348808\n\n\n\ndf.MachineHoursCurrentMeter.unique()\n\narray([   68.,  4640.,  2838., ..., 11612., 12097., 14650.])\n\n\n\ndf.Forks.unique()\n\narray(['None or Unspecified', nan, 'Yes'], dtype=object)\n\n\n\ndf.Pad_Type.unique()\n\narray([nan, 'None or Unspecified', 'Reversible', 'Street', 'Grouser'],\n      dtype=object)\n\n\n\ndf.Backhoe_Mounting.unique()\n\narray([nan, 'None or Unspecified', 'Yes'], dtype=object)\n\n\n\ndf.ProductSize.unique()\n\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large',\n       'Compact'], dtype=object)\n\n\n\ndf.SalePrice.unique()[:10]\n\narray([66000., 57000., 10000., 38500., 11000., 26500., 21000., 27000.,\n       21500., 65000.])\n\n\nTell pandas about a suitable ordering of these levels like so:\n\nsizes = 'Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\nFutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n\n\n\n# I believe the ordering should be reverse of this\ndf.ProductSize.unique()\n\n[NaN, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact']\nCategories (6, object): ['Large' < 'Large / Medium' < 'Medium' < 'Small' < 'Mini' < 'Compact']\n\n\nThe metric we will use is RMLSE (root mean squared log error) between the actual and predicted auction prices. Take the log of the prices so that the m_rmse of that value will give us the metric.\n\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\n\ndf.SalePrice.hist();\n\n\n\n\n\n\n\nA decision tree asks a series of binary (yes or no) questions about the data. After each question the data at that part of the tree is split between a Yes and a No branch. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required.\nThe basic steps to train a decision tree:\n\nLoop through each column of the dataset in turn.\nFor each column, loop through each possible level of that column in turn.\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. Treat this as a very simple “model” in which our predictions are simply the average sale price of the item’s group.\nAfter looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two groups of our data, based on the selected split. Treat each group as a separate dataset, and find the best split for each by going back to step 1 for each group.\nContinue this process recursively until you have reached some stopping criterion for each group–for instance, stop splitting a group further when it has only 20 items in it.\n\n\n\nTo help our algorithm handle dates intelligently, we’d like our model to know more than whether a date is more recent or less recent than another. We might want our model to make decisions based on that date’s day of the week, on whether a day is a holiday, on what month it is in, and so forth. To do this, replace every date column with a set of date metadata columns, such as holiday, day of week, and month. These columns provide categorical data that we suspect will be useful.\n\ndf = add_datepart(df, 'saledate')\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\n\nlen(df.columns)\n\n65\n\n\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\n\n\n\nA TabularProc is like a regular Transform except for the following:\n\nIt returns the exact same object that’s passed to it, after modifying the object in place.\nIt runs the transform once, when data is first passed in, rather than lazily as the data is accessed.\n\nCategorify is a TabularProc that replaces a column with a numerical categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing.\n\nprocs = [Categorify, FillMissing]\n\nWe need to be very careful about our validation set. We want to design it so that it is like the test set Kaggle will use to judge the contest.\nThe test set date range is from May 2012 to November 2012.\n\n(df_test.saleYear.astype(str) + \"/\" + df_test.saleMonth.astype(str)).unique()\n\narray(['2012/5', '2012/6', '2012/7', '2012/8', '2012/9', '2012/10',\n       '2012/11'], dtype=object)\n\n\nThe test set dates are later than any data in the training set (which has a latest date of April 2012).\n\nnp.sort((df.saleYear.astype(str) + \"/\" + df.saleMonth.astype(str)).unique())[-10:]\n\narray(['2011/4', '2011/5', '2011/6', '2011/7', '2011/8', '2011/9',\n       '2012/1', '2012/2', '2012/3', '2012/4'], dtype=object)\n\n\nWe’ll define a validation set consisting of data from after November 2011.\n\ncond = (df.saleYear<2011) | (df.saleMonth<10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx), list(valid_idx))\n\nTabularPandas needs to be told which columns are continuous and which are categorical.\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\nlen(to.train), len(to.valid)\n\n(404710, 7988)\n\n\nThe data is still displayed as strings for categories.\n\nto.show(3)\n\n\n\n  \n    \n      \n      UsageBand\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Enclosure\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Hydraulics_Flow\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n      saleIs_month_end\n      saleIs_month_start\n      saleIs_quarter_end\n      saleIs_quarter_start\n      saleIs_year_end\n      saleIs_year_start\n      auctioneerID_na\n      MachineHoursCurrentMeter_na\n      SalesID\n      MachineID\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      saleYear\n      saleMonth\n      saleWeek\n      saleDay\n      saleDayofweek\n      saleDayofyear\n      saleElapsed\n      SalePrice\n    \n  \n  \n    \n      0\n      Low\n      521D\n      521\n      D\n      #na#\n      #na#\n      #na#\n      Wheel Loader - 110.0 to 120.0 Horsepower\n      Alabama\n      WL\n      Wheel Loader\n      #na#\n      EROPS w AC\n      None or Unspecified\n      #na#\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      2 Valve\n      #na#\n      #na#\n      #na#\n      #na#\n      None or Unspecified\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      Standard\n      Conventional\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      1139246\n      999089\n      3157\n      121\n      3.0\n      2004\n      68.0\n      2006\n      11\n      46\n      16\n      3\n      320\n      1.163635e+09\n      11.097410\n    \n    \n      1\n      Low\n      950FII\n      950\n      F\n      II\n      #na#\n      Medium\n      Wheel Loader - 150.0 to 175.0 Horsepower\n      North Carolina\n      WL\n      Wheel Loader\n      #na#\n      EROPS w AC\n      None or Unspecified\n      #na#\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      2 Valve\n      #na#\n      #na#\n      #na#\n      #na#\n      23.5\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      Standard\n      Conventional\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      1139248\n      117657\n      77\n      121\n      3.0\n      1996\n      4640.0\n      2004\n      3\n      13\n      26\n      4\n      86\n      1.080259e+09\n      10.950807\n    \n    \n      2\n      High\n      226\n      226\n      #na#\n      #na#\n      #na#\n      #na#\n      Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\n      New York\n      SSL\n      Skid Steer Loaders\n      #na#\n      OROPS\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      Auxiliary\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      None or Unspecified\n      None or Unspecified\n      None or Unspecified\n      Standard\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      1139249\n      434808\n      7009\n      121\n      3.0\n      2001\n      2838.0\n      2004\n      2\n      9\n      26\n      3\n      57\n      1.077754e+09\n      9.210340\n    \n  \n\n\n\nBu the underlying items are all numeric:\n\nto.items[[\"state\", \"ProductGroup\", \"Drive_System\", \"Enclosure\"]].head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      state\n      ProductGroup\n      Drive_System\n      Enclosure\n    \n  \n  \n    \n      0\n      1\n      6\n      0\n      3\n    \n    \n      1\n      33\n      6\n      0\n      3\n    \n    \n      2\n      32\n      3\n      0\n      6\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThere’s no particular meaning to the numbers in the categorical columns after conversion, they are chosen consecutively as they are seen in a column. The exception is if you first convert a column to a Pandas ordered category.\n\nto.classes['ProductSize']\n\n['#na#', 'Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact']\n\n\n\n\n\n\nxs,y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs,y);\n\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n  s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                    special_characters=True, rotate=False, precision=precision, **kwargs)\n  return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\nThe topmost node is the initial model when all data is in one group. Predicts the average value of the whole dataset. In this case it predicts 10.1 for the logarithm of the sales price, and gives a mean squared error of 0.48. The square root of this is 0.69. There are 404710 records in this group which is the total size of our training set. The best split found was a split based on the coupler_system columns. Asking only about coupler_system predicts an average value of 9.21 versus 10.1.\n\nimport dtreeviz\n\n\nsamp_idx = np.random.permutation(len(y))[:500]\n\nviz_model=dtreeviz.model(m,\n                         X_train=xs.iloc[samp_idx],\n                         y_train=y.iloc[samp_idx],\n                         feature_names=xs.columns,\n                         target_name=dep_var)\n\nviz_model.view(fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n               orientation='LR')\n\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n\n\n\n\n\nThe YearMade data has values of 1000 which we need to change to make it more realistic:\n\nxs.loc[xs['YearMade']<1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']<1900, 'YearMade'] = 1950\n\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs,y);\n\n\nviz_model=dtreeviz.model(m,\n                         X_train=xs.iloc[samp_idx],\n                         y_train=y.iloc[samp_idx],\n                         feature_names=xs.columns,\n                         target_name=dep_var)\n\nviz_model.view(fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n               orientation='LR')\n\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n\n\n\n\n\nThe change in YearMade doesn’t change the model in any significant way—shows how resilient decision trees are to data issues.\nBuild a bigger tree (don’t pass any stopping criteria).\n\nm = DecisionTreeRegressor()\nm.fit(xs,y);\n\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nm_rmse(m, xs, y)\n\n0.0\n\n\nThe model has 0.0 root mean square error but that is on the training set. Let’s check the validation error:\n\nm_rmse(m, valid_xs, valid_y)\n\n0.331731\n\n\nThe model is overfitting pretty badly.\n\nm.get_n_leaves(), len(xs)\n\n(324567, 404710)\n\n\nWe have nearly as many leaf nodes as data points.\nLet’s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 auction records:\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.248564, 0.323369)\n\n\nThat looks better.\n\nm.get_n_leaves()\n\n12397\n\n\n\n\n\n\n\nLeo Breiman in 1994 while retired published a technical report called “Bagging Predictors” which turned out to be one of the most influential ideas in modern machine learning.\nHere is his procedure, known as bagging:\n\nRandomly choose a subset of rows of your data.\nTrain a model using this subset.\nSave that model, and then return to step 1 a few times.\nThis will give you multiple trained models. To make a prediction, predict using all of the models, and then take the average of each of those model’s predictions.\n\nAlthough each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. Different models will make different errors. The average of those errors is zero.\nIf we take the average of all of the models’ predictions, we should end up with a prediction that gets closer and closer to the correct answer, the more models we have.\nWe can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of data, and averaging its predictions.\nRandom Forest: a model that averages the predictions of a large number of decision trees which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters.\nEnsembling: combining the results of multiple models together.\n\n\n\n\nSimilar to creating a decision tree except now we are also specifying parameters that indicate how many trees should be in the forest, how we should subset the data items (the rows) and how we should subset the fields (the columns).\nIn the function rf:\n\nn_estimators: number of trees.\nmax_samples: number of rows to sample for training each tree.\nmax_features: how many columns to sample at each split point (where 0.5 means “take half the total number of columns).\nmin_samples_leaf: when to stop splitting the tree nodes.\nn_jobs=-1: tell sklearn to use all our CPUs to buil the trees in parallel.\n\n\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs):\n  return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs,y)\n\n\nm = rf(xs, y);\n\n\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.170922, 0.233145)\n\n\nRandom forests aren’t very sensitive to the hyperparameter choices such as max_features. You can st n_estimators to as high a number as you have time to train. The more trees you have the more accuracte the model will be. If you have over 200k data points, set max_samples to 200k and it will train faster with little impact on accuracy. The models with the lowest error result from using a subset of features with a larger number of trees.\nGet the predictions from each individual tree in our forest:\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]);\n\n\nr_mse(preds.mean(0), valid_y)\n\n0.233145\n\n\n\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\nThe improvement levels off quite a bit after around 30 trees.\nWe don’t know if the performance on the validation set is worse than on our training set because we’re overfitting or because the validation set covers a different time period.\n\n\n\n\nIn a random forest, each tree is trained on a different subset of the training data. The OOB error is a way of measuring prediction error in the training dataset by including in the calculation of a row’s error trees only where that row was not included in training. This allows us to see whether the model is overfitting without needing a separate validation set. Since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set, which is simply the rows that were not selected for that tree’s training. This is particularly beneficial when you have a small amount of training data.\n\nlen(m.oob_prediction_)\n\n404710\n\n\n\n# use training y\nr_mse(m.oob_prediction_, y)\n\n0.210661\n\n\nOOB error is lower than validation set error, which means that something else is causing that error, in addition to normal generalization error. I’m not sure what that means but the text says it’s looked into later in this chapter.\n\n\n\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors, which can we ignore?\nWhich columns are effectively redundant with each other, for purposes of prediction?\nHow do predictions vary as we vary these columns?\n\n\n\nThe standard deviation of predictions across the trees tells us the relative confidence of predictions. We would want to be more cautious of using the results for rows where trees give very different results (higher standard deviations), compared to cases where they are more consistent (lower standard deviations).\nWe have a prediction for every tree and every auction in the validation set (40 trees and 7,988 auctions):\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]);\n\n\npreds.shape\n\n(40, 7988)\n\n\nGet the standard deviation of the predictions over all the trees for each auction:\n\npreds_std = preds.std(0)\npreds_std[:5]\n\narray([0.21168835, 0.09996709, 0.0911939 , 0.25939701, 0.08520345])\n\n\n\nlen(preds_std)\n\n7988\n\n\nThe confidence in the predictions varies widely. For some auctions there is low std meaning the trees agree. For others it’s higher, meaning the trees don’t agree.\n\n\n\n\ndef rf_feat_importance(m, df):\n  return pd.DataFrame({\n      'cols': df.columns,\n      'imp': m.feature_importances_}\n                      ).sort_values('imp', ascending=False)\n\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      57\n      YearMade\n      0.180981\n    \n    \n      6\n      ProductSize\n      0.116321\n    \n    \n      30\n      Coupler_System\n      0.089648\n    \n    \n      7\n      fiProductClassDesc\n      0.074037\n    \n    \n      32\n      Hydraulics_Flow\n      0.064145\n    \n    \n      54\n      ModelID\n      0.059373\n    \n    \n      31\n      Grouser_Tracks\n      0.053432\n    \n    \n      65\n      saleElapsed\n      0.050231\n    \n    \n      3\n      fiSecondaryDesc\n      0.043258\n    \n    \n      1\n      fiModelDesc\n      0.031560\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndef plot_fi(fi):\n  return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\nThe feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees and finall the scores are normalized such that they add to 1.\n\n\n\nRetrain the model using the subset of columns with importance greater than 0.005:\n\nto_keep = fi[fi.imp>0.005].cols\nlen(to_keep)\n\n20\n\n\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\nm = rf(xs_imp, y)\n\n\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n\n(0.181078, 0.231864)\n\n\nOur accuracy is about the same with fewer columns that we have to study.\n\nlen(xs.columns), len(xs_imp.columns)\n\n(66, 20)\n\n\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\n\n\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n\n\ncluster_columns(xs_imp)\n\n\n\n\nThe pairs of columns that are most similar are the ones that were merged together early, far from the “root” of the tree at the left.\nThe most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (first, second, third, etc within the column) and then the correlation is calculated.\nLet’s try removing some of these closely related features to see if the model can be simplified without impacting accuracy.\nCreate a function that quickly trains a random forest and returns the OOB score by using a lower max_samples and higher min_samples_leaf.\n\ndef get_oob(df):\n  m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n                            max_samples=50_000, max_features=0.5, n_jobs=-1, oob_score=True)\n  m.fit(df, y)\n  return m.oob_score_\n\n\n# baseline\nget_oob(xs_imp)\n\n0.8764807857774278\n\n\nRemove each of our potentially redundant variables and see what score we get:\n\n{c: get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc', 'ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow', 'Grouser_Tracks', 'Coupler_System')}\n\n{'saleYear': 0.875295601149204,\n 'saleElapsed': 0.8717546976024381,\n 'ProductGroupDesc': 0.8767983331719241,\n 'ProductGroup': 0.8764742908741526,\n 'fiModelDesc': 0.8748490763639009,\n 'fiBaseModel': 0.8760895658282863,\n 'Hydraulics_Flow': 0.8770549322909539,\n 'Grouser_Tracks': 0.8775175679664963,\n 'Coupler_System': 0.8764559009225574}\n\n\nNow let’s try dropping multiple variables:\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8751984884391564\n\n\nThis is really not much worse than the model with all the fields so we’ll create DataFrames without these columns:\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\n# check accuracy\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n\n(0.182723, 0.232926)\n\n\n\n\n\nImportant to understand the relationship between the two most important predictors (ProductSize and YearMade) and sale price.\n\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n\n\n\n\nax = valid_xs_final['YearMade'].hist()\n\n\n\n\nPartial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable?\nHow does YearMade impact sale price, all other things being equal? We can’t just take the average sale price for each YearMade, as it would capture the effect of how every other field also changed along with YearMade and how that overall change affected price.\nInstead we replace every single value in the YearMade column with 1950, and then calculate the predicted sale price for every auction, and take the average over all auctions, then do the same for every single year. This isolates the effect of only YearMade.\n\nfrom sklearn.inspection import partial_dependence\n\nfig, ax = plt.subplots(figsize=(6,4))\npdp = partial_dependence(m, valid_xs_final, ['YearMade', 'ProductSize'],\n                        grid_resolution=20)\n\nax.plot(array([0,1,2,3,4,5,6]), pdp['average'].mean(axis=1).squeeze());\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6,4))\nax.plot(pdp['values'][0], pdp['average'].mean(axis=2).squeeze());\n\n\n\n\nAfter 1990, where most of the data is, there is a linear relationship in the plot (y-axis is log(SalePrice) so this is an exponential relationship between YearMade and SalePrice). SalePrice has the lowest price for the last two categories (Large and #na#). This doesn’t make sense because I would expect the price to increase with ProductSize. Missing values can sometimes be useful predictors. Sometimes, they can indicate data leakage.\n\n\n\nData leakage is the use of information in the model training process which would not be expected to be available at prediction time.\nFor example, if you are trying to predict successful grant applications using data that was not available at the time of receiving the application (such as information filled out only when a grant application was accepted such as date of processing).\nIdentifying data leakage involves building a model and then:\n\nCheck whether the accuracy of the model is too good to be true.\nLook for important predictors that don’t make sense in practice.\nLook for partial dependence plot results that don’t make sense in practice.\n\nIt’s often a good idea to build a model first and then do your data cleaning, as the model can help you identify potentially problematic data issues.\n\n\n\nWe still have to answer the following question:\n\nFor predicting with a particular row of data, what were the most important factors, and how did they influence the prediction?\n\n\n!pip install treeinterpreter\n!pip install waterfallcharts\n\nWe computed feature importance across the entire random forest by looking at the contribution of each variable to improving the model, at each branch of every tree and then add up all of these contributions per variable.\nWe can do the same thing but for a single row of data. Let’s say we are looking at a single item at auction. The model might predict that this item will be very expensive, and we want to know why. Take the one row of data, put it through the first decision tree, looking to see what split is used at each point throughout the tree. For each split, we find the decrease or increase in the addition, compared to the parent node of the tree. We do this for every tree and add up the total change in importance by split variable.\n\nrow = valid_xs_final.iloc[:5]\n\n\nfrom treeinterpreter import treeinterpreter\nprediction, bias, contributions = treeinterpreter.predict(m, row.values)\n\n\nprediction[0], bias[0], contributions[0].sum()\n\n(array([10.03216082]), 10.104110088290454, -0.0719492660421904)\n\n\nprediction is the prediction that the random forest makes. bias is the prediction based on taking the mean of the dependent variable (i.e. the model that is at the root of every tree). contributions is the the total change in prediction due to each of the independent variables. The sum of contributions plus bias must equal the prediction for each row.\n\nfrom waterfall_chart import plot as waterfall\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08,\n          rotation_value=45, formatting='{:,.3f}');\n\n\n\n\n\n\n\n\nRandom forests, like all machine learning or deep learning algorithms, don’t always generalize well to new data.\n\n\nConsider the simple task of making predictions from 40 data points showing a slightly noisy linear relationship:\n\nx_lin = torch.linspace(0,20,steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n\n\n\nsklearn expects a matrix of independent variables:\n\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape, xs_lin.shape\n\n(torch.Size([40]), torch.Size([40, 1]))\n\n\n\nx_lin[:, None].shape\n\ntorch.Size([40, 1])\n\n\n\n# use only the first 30 rows\nm_lin = RandomForestRegressor().fit(xs_lin[:30], y_lin[:30])\n\nTest the model on the full dataset:\n\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\n\nWhat we are seeing is that a tree and a random forest can never predict values outside the range of the training data, because a tree simply predicts the average value of the rows in a leaf and a random forest just averages the predictions of a number of trees. Your predictions outside the domain will be systematically too low. Random forests are not able to extrapolate outside the types of data they have seen in a more general sense, that’s why we need to make sure our validation set does not contain out-of-domain data.\n\n\n\nUse a random forest to predict whether a row is in the validation set or the training set.\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      6\n      saleElapsed\n      0.874998\n    \n    \n      9\n      SalesID\n      0.088186\n    \n    \n      12\n      MachineID\n      0.032512\n    \n    \n      0\n      YearMade\n      0.000888\n    \n    \n      5\n      ModelID\n      0.000784\n    \n    \n      11\n      Enclosure\n      0.000594\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThree columns differ significantly between the training and validation sets: saleElapsed, SalesID, and MachineID. It makes sense that saleElapsed is different since it directly encodes date (number of days between the start of the dataset and each row) and the other two likely increment over time.\n\n# baseline\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID', 'saleElapsed', 'MachineID'):\n  m = rf(xs_final.drop(c, axis=1), y)\n  print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.232669\nSalesID 0.230199\nsaleElapsed 0.235264\nMachineID 0.231392\n\n\nWe should be able to remove SalesID and MachineID without losing accuracy:\n\ntime_vars = ['SalesID', 'MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n0.229906\n\n\nRemoving these variables has improved the accuracy and will make the model more resilient over time.\n\nxs['saleYear'].hist();\n\n\n\n\nTry just using the most recent few years of the data:\n\nfilt = xs['saleYear']>2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n\n(0.177093, 0.229919)\n\n\n\n\n\nReplicate the steps to set up the TabularPandas object:\n\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n\nFutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n\n\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\nA great way to handle categorical variables in a neural net is with embeddings. Embedding sizes larger than 10,000 should generally be used only after you’ve tested whether there are better ways to group the variable, so use 9,000 as max_card (lower cardinality means fastai creates embedding for the categorical variable):\n\ncont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\nWe don’t want to treat saleElapsed as categorical since we want to predict auction sale prices in the future and a categorical variable cannot extrapolate outside the range of values that it has seen:\n\n'saleElapsed' in cont_nn, 'saleElapsed' in cat_nn\n\n(True, False)\n\n\n\n# look at cardinality\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nHydraulics_Flow          3\nModelID               5281\nfiSecondaryDesc        177\nfiModelDesc           5059\nHydraulics              12\nEnclosure                6\nfiModelDescriptor      140\nProductGroup             6\nDrive_System             4\ndtype: int64\n\n\nWhen analyzing redundant features relies on similar variables being sorted in the same order (they need to have similarly named levels). Here we see that ModelID and fiModelDesc both have 5000+ levels, meaning they would need 5000+ columns in our embedding matrix. Let’s see the impact of removing one of these model columns on the random forest:\n\nxs_filt2 = xs_filt.drop('fiModelDesc', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDesc', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.183026, 0.233514)\n\n\n\nxs_filt2 = xs_filt.drop('ModelID', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('ModelID', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.18152, 0.232451)\n\n\nDropping ModelID has the smaller effect on accuracy so we’ll drop that variable.\n\ncat_nn.remove('ModelID')\n\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nHydraulics_Flow          3\nfiSecondaryDesc        177\nfiModelDesc           5059\nHydraulics              12\nEnclosure                6\nfiModelDescriptor      140\nProductGroup             6\nDrive_System             4\ndtype: int64\n\n\nA neural net cares about normalization whereas a random forest doesn’t:\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)\n\nTabular models and data don’t generally require much GPU RAM so we can use larger batch sizes:\n\ndls = to_nn.dataloaders(1024)\n\nSet y_range for regression models:\n\ny = to_nn.train.y\ny.min(), y.max()\n\n(8.465899, 11.863583)\n\n\n\nfrom fastai.tabular.all import *\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0002754228771664202)\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.062091\n      0.074148\n      00:08\n    \n    \n      1\n      0.054561\n      0.066272\n      00:04\n    \n    \n      2\n      0.048428\n      0.053494\n      00:06\n    \n    \n      3\n      0.043653\n      0.051082\n      00:04\n    \n    \n      4\n      0.040581\n      0.051459\n      00:05\n    \n  \n\n\n\n\npreds, targs = learn.get_preds()\nr_mse(preds, targs)\n\n\n\n\n\n\n\n\n0.226845\n\n\nThe neural net is more accurate than the random forest.\n\ntabular_learner??\n\n\nTabularModel??\n\n\n\n\n\nIt would be reasonable to expect that the kinds of errors that each model makes (random forest and neural network) would be quite different. We might expect that the average of their predictions would be better than either one’s individual predictions.\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) / 2\n\n\nr_mse(ens_preds, valid_y)\n\n0.223161\n\n\nThis result is better than each individual model.\n\n\nbagging = combining many models (each trained on a different data subset) by averaging them.\nboosting = add models instead of averaging them:\n\nTrain a small model that underfits your dataset.\nCalculate the predictions in the training set for this model.\nSubtract the predictions from the targets; these are called the residuals and represent the error for each point in the training set.\nGo back to step 1, but instead of using the original targets, use the residuals as the targets for the training.\nContinue doing this until you reach a stopping criterion, such as maximum number of trees, or you observe your validation set error getting worse.\n\nEach new tree will be attempting to fit the error of all the previous trees combined. The residuals get smaller and smaller each time. To make predictions, calculate the predictions from each tree and then add them all together. Most common model names: Gradient Boosting Machines(GBMs) and Gradient Boosted Decision Trees (GBDTs). XGBoost is the most popular library for implementing these.\nUsing more trees in a random forest does not lead to overfitting, because each tree is independent of the others. In a boosted ensemble, the more trees you have, the better the training error becomes and eventually you will see overfitting on the validation set. Unlike random forests, gradient boosted trees are extremely sensitive to the choices of hyperparameters.\n\n\n\nThe embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead. Models dramatically improve by using the neural network’s categorical embeddings instead of the raw categories as inputs.\nAt inference time, you can just use an embedding along with a small decision tree ensemble.\nOnce a set of embeddings are learned for a column for a particular task, they could be stored in a central place and reused across multiple models.\n\n\n\n\n\nRandom forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require little preprocessing. They are fast to train, and should not overfit if you have enough trees. But they can be a little less accurate especially if extrapolation is required, such as predicting future time periods.\nGradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of hyperparameters. They can overfit, but they are often a little more accurate than random forests.\nNeural networks take the longest time to train and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.\n\nStart your analysis with a random forest. Then use that model for feature selection and partial dependence analysis, to get a better understanding of your data. Then try neural nets and GBMs and use them if they give significantly better results on your validation set in a reasonable amount of time. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data and see if that helps your decision trees learn better.\n\n\n\n1. What is a continuous variable?\nA variable that can take on any value within a range.\n2. What is a categorical variable?\nA variable that can only take on discrete values or levels within a fixed set.\n3. Provide two of the words that are used for the possible values of a categorical variable.\nLevels or categories.\n4. What is a dense layer?\nA linear layer.\n5. How do entity embeddings reduce memory usage and speed up neural networks?\nThey are dense compared to one-hot-encoded vectors which are sparse.\n6. What kinds of datasets are entity embeddings especially useful for?\nDatasets with categorical variables with high cardinality.\n7. What are the two main families of machine learning algorithms?\n\nEnsembles of decision trees for structured data.\nMultilayered neural networks learned with SGD for unstructured data.\n\n8. Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\nOrdinal columns have a natural order (like size) and can be specified using the Series.cat.set_categories Pandas method.\n9. Summarize what a decision tree algorithm does.\nA decision tree algorithm loops through each column and for each column loops through all possible splits in the data, and calculates the objective (such as average SalePrice) for each group in the split. It then splits the data with the best split, meaning the split that has the highest average objective. Within each split, it continues to split the data and calculate the next best split until some stopping criteria is met.\n10. Why is a date different from a regular categorical or continuous variable, and how can you preprocess is to allow it to be useful in a model?\nDates have many meanings such as day of the week, the month it’s in and whether it’s a holiday. You can preprocess a date variable with fastai’s add_datepart function.\n11. Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\nNo. Since we want to predict auction price for future dates, the validation set should have date values that come after the training set dates.\n12. What is pickle and what is it useful for?\nPickle is a method to save (serialize) Python objects.\n13. How are mse, samples, and values calculated in the decision tree drawn in this chapter?\nThe mse is calculated between the average sale price and the individual sale price in the group. samples are the number of rows in the dataset that correspond to the given split that resulted in the group. values is the average sale price in the group.\n14. How do we deal with outliers before building a decision tree?\nDecision trees are resilient to data issues but if you want to treat outliers you can do so by setting their value to a more reasonable value (as we did by setting any YearMade value less than 1900 to 1950.\n15. How do we handle categorical variables in a decision tree?\nWe don’t have to handle them in anyway other than encoding them as integers. Research shows that one-hot-encoding categorical variables doesn’t improve model performance.\n16. What is bagging?\nAveraging predictions across multiple models that are trained on different subsets of the dataset. Since different models make different errors, the average of the errors is zero.\n17. What is the difference between max_samples and max_features when creating a random forest?\nmax_samples is the maximum number of rows to sample for each decision tree.\nmax_features defines how many columns to sample at each split point.\n18. If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\nNo, because decision trees are trained on a subset of data independent from each other.\n19. In the section “Creating a Random Forest”, after Figure 9-7, why did preds.mean(0) give the same result as our random forest?\nSince the random forest does the same thing: take the average prediction across all trees.\n20. What is out-of-bag error?\nThe error of a tree’s prediction on rows from the dataset that it has not been trained on.\n21. List the reasons that a model’s validation set error might be worse than the OOB error. How could you test your hypotheses?\nIt could be that the model does not generalize well to data other than the training set. It could also mean that the distribution of the validation set is different from the training set (which can be tested by training a random forest on is_valid–1/0 whether the data is validation/train and seeing which features have high importance).\n22. Explain why random forests are well suited to answering each of the following questions:\n\nHow confident are we in our predictions using a particular row of data?\n\nThis is answered by calculating the standard deviation of the trees’ predictions for each row in the validation set.\n\nFor predicting with a particular row of data, what were the mot important factors, and how did they influence that prediction?\n\nUsing treeinterpreter we can see how much each column contributed to the total change in prediction.\n\nWhich columns are the strongest predictors?\n\nThis is answered by calculating the feature importance, which is the (weighted by number of rows in each branch group) improvement to the model made by each feature.\n\nHow do predictions vary as we vary these columns?\n\nWe can look at partial dependence plots to answer this question.\n\n\n23. What’s the purpose of removing unimportant variables?\nTo simplify the model so that we can understand and study how each feature influences the predictions.\n24. What’s a good type of plot for showing tree interpreter results?\nWaterfall charts.\n25. What is the extrapolation problem?\nRandom forests and trees can never predict values outside the range of the training data. Predictions in this case will systematically be too low.\n26. How can you tell if your test or validation set is distributed in a different way than your training set?\nBy training a random forest where the dependent variable is is_valid a field that is 1 for rows in the validation set and 0 for rows in the training set, and then calculating feature importance. Features with the highest importance will differ in value between the training and validation set.\n27. Why do we make saleElapsed a continuous variable, even though it has fewer than 9,000 distinct values?\nsaleElapsed is the number of days since the start of the dataset that the auction took place, so it represents date/time of the auction. Since we want to extrapolate auction prices to future dates, we want to treat it as something that can be extrapolated (continuous variable) as opposed to something that can’t be extrapolated (categorical variable).\n28. What is boosting?\nBoosting is when you train a model to underfit the dataset and train subsequent models on residuals (difference between targets and predictions) and then add together the predictions from the models.\n29. How could we use embeddings with a random forest? Would we expect this to help?\nResearch shows that using neural net trained categorical embeddings as inputs (instead of categorical columns) improves the accuracy of random forests.\n30. Why might we not always use a neural net for tabular modeling?\nNeural nets take the longest time to train (compared to random forests and gradient boosting), require preprocessing and are sensitive to hyperparameters.\n\n\n\n\n\nPlot training loss, validation loss and accuracy for 100 model trainings.\nTrain a model with two outputs (Age and Survived)\nTitanic survival as an NLP classification model\n\n\n\n\n\n\nFurther Research\n\nPick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard.\n\nMy blog post for this exercise.\n\nImplement the decision tree algorithm in this chapter from scratch yourself, and try it on the dataset you used in the first exercise.\n\nMy blog post for this exercise\n\nUse the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw.\n\nMy blog post for this exercise\n\nExplain what each line of the source of TabularModel does (with the exception of BatchNorm1d and Dropout layers).\n\nMy blog post for this exercise\n\n\n\n\n\n\n\nlink to Jeremy’s notebook\n\nWe created binary splits in the Titanic dataset for continuous and categorical variables.\nWe came up with a score of how good a job did that split do of grouping the survival characteristics into two groups where nearly all of one survived and all of one didn’t survive. Small (weighted) standard deviation in each group.\nWhat if we split Males and Females into two other groups each?\nAge <=6 is the biggest predictor of whether males survive.\nPclass <= 2 is the biggest predictor of whether females survive.\nWe hope to get the strongest prediction about survival in the leaf nodes of our decision tree.\nWe use sklearn’s DecisionTreeClassifier.\nscikit-learn focuses on classical machine learning algorithms.\nDecision trees as exploratory data analysis: allows us to get a quick picture of what are the key driving variables in this dataset and how much do they predict what was happening in the data.\ngini is another way of measuring how good a split is: how likely is it that if you go into that sample and grab one item and then go in again and grab another item—how likely is it that you’re going to grab the same item each time? If the entire leaf node is just people who survived or just people who didn’t survive, the probability would be 1.0. If it was an exactly equal mix the probability would be 0.5.\nOneR MAE was 0.215, decision tree with four leaf nodes’ MAE was 0.224. Reflects the fact that we have a small validation set.\nDecision tree with minimum samples of 50 per node has MAE of 0.183.\nOne of the biggest mistakes is not to submit to the leaderboard on Kaggle for a competition. You should try and submit something to the leaderboard everyday.\nWe don’t need to do as much preprocessing for decision trees. All the decision tree cares about is the ordering of the data.\nFor tabular data, always start with a decision tree approach.\nUse dummy variables for <= 4 levels, numeric codes otherwise.\nThere are limitations to how accurate a decision tree can be.\nLeo Breiman came up with the idea of bagging. Decision trees on average will predict the average, they are not biased. Build lots of unbiased, better-than-nothing, uncorrelated models, and average their predictions, ending up with errors on either side of the correct prediction whose average is 0. So it will be better than any individual model.\nWe can get many trees who use some random proportion of rows and columns (called a random forest), make predictions with each of them, and then average the predictions.\nIn each splot of each decision tree in the random forest you can calculate how much the prediction improved (e.g., how much gini value reduced weighted by sample size) on a split by the given column. This gives you the feature importances—how often did the trees pick the feature and how much did it improve the gini when picked as a split?\nCreate a feature importance plot first with a tabular dataset to find the most important columns.\nRule of thumb: use a maximum of 100 trees.\nIf you don’t have much data you can get away with not having a validation set since for each tree in the random forest you can pick the rows not used in that tree as the validation set. The error across all rows not used in training a tree is called out-of-bag (OOB) error.\nFive important insights random forests can provide:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors and which can we ignore?\nWhich columns are effectively redundant with each other, for purposes of prediction?\nHow do predictions vary, as we vary these columns?\n\nYou can do a partial dependence plot for any machine learning model.\n\nTake the dataset and leave it exactly as it is except for the column you want to understand partial dependence on (such as YearMade). Set the column in question to its first value, then predict the dependent variable for every row and average it. Repeat for each value of the column in question.\n\nYou can do feature importance for one row to understand why the random forest made the prediction.\nIf you start deleting trees then you are no longer having an unbiased prediction of the dependent variable. You are biasing it by making a choice. Even the bad trees will be improving the quality of overall average.\nCan you overfit a random forest? Basically no. Adding more trees will make it more accurate,but accuracy asymptotes. If you don’t have enough trees and you let the trees grow very deep, that could overfit, so you have to make sure you have enough trees.\nGiving a random forest lots of randomly generated columns with fake data does not affect its performance.\nGradient boosting machine: fit a very small tree, get the residual (the difference between the prediction and the actual), then create another very small tree which attempts to predict the residual and so forth. Each one is predicting the residual from all the previous ones. Then to calculate the prediction you take the sum of all of the trees’ predictions, because each one has predicted the difference between the actual and all of the previous trees. More accurate than random forests, but you can absolutely overfit, so it’s not the first go-to model.\n\n\n\n\n\nlink to Jeremy’s notebook\n\nWhat does it look like to pick a Kaggle competition and just do like the normal, sensible, mechanical steps you would do for any computer vision model.\nPaddy Disease Classification: recognizing diseases in rice paddies.\nThe library fastkaggle makes it easier to setup Kaggle competition stuff. Use setup_comp to grab data.\nYou can’t hide from the truth in a Kaggle competition.\nFocus on two things:\n\nCreating an effective validation set.\nIterating rapidly to find changes which improve results on the validation set.\n\nWhat can I do that’s going to train in a minute or so and will quickly give me a sense of what I can try and what’s going to work. Try 80 things.\n\n\nDo be successful in Kaggle competitions and machine learning in general you have to do not just one thing well but everything well.\nOnly use random seed when you are sharing a notebook, otherwise you want to see how much things change each time so you can tell if the modifications you are making are improving it, making it worse, or is it just random variation?\nPIL images size is columns x rows. PyTorch size is rows x columns.\nThe amount of time it takes to decode a JPEG is quite significant. Use fastcore.parallel.\nMost common way to do things is to either squish or crop every image to be a square.\nModels are a great way to understand your data. Refer to the notebook The best vision models for fine tuning—trained on PETS (fine-tuning to similar things they are pretrained on) and Planet (fine-tuning to things different than what is pretrained) datasets which are very different datasets. Measured how much memory it used, how accurate was it and how long did it take to fit.\nWhat matters about a model, which is just a function, is its inputs, outputs how accurate it is and how fast it is.\nlr_find will train one batch a time and track the loss at increasing learning rates (starting very small). LR recommendations are conservative.\nWe submit as soon as we can. We want a dataloader that is exactly like what we made for training but pointed at the test set. Use dls.test_dl method. Pass it test dataset files. A test dataloader does not have any labels.\nwith_decoded in learn.get_preds tells you the index of the most probably class. Map them to strings in dls.vocab.\nMake everything fast and easy in the iteration including submitting to Kaggle.\nIf you can create models that predict things well and you can communicate your results in a way that is clear and compelling, you’re a pretty good data scientist.\nBe highly intentional like a scientist; have hypotheses that you test carefully and come out with conclusions that you implement.\nTest out your hypotheses over a couple models from each of the main families (e.g., does squish or crop work better with different models).\nRandom forests will give you good results, GBMs for better results (would run a hyperparameters sweep).\n\n\n\n\nlink to Jeremy’s notebook\n\nInitial training took a minute on home computer, took 4 minutes per epoch on Kaggle. Because they only have two virtual CPUs. You want at least 8 physical CPUs per GPU. It was spending all its time reading data.\nStep 1 was making Kaggle implementation faster—resize_images. It was four times faster with no loss of accuracy.\nKaggle GPU was hardly being used so moved from resnet26d to convnext_small_in22k which was over twice as good.\nresnets are the fastest, use convnext if you’re not sure what to use.\nUse crop instead of squish.\nGet everything for training into a single function that returns a learner.\nPadding is the only way of preprocessing images that doesn’t distort (squish) or lose data (crop) with the downsize of having empty black pixels.\nTest time augmentation (TTA). Get preditions for all augmented images and take the average. Like a mini-bagging approach. learn.tta does this for you. TTA usually gives a better result. TTA uses the same data augmentation that you used during training.\nYour images don’t have to be square. They just have to be the same size.\nidxs has indexes of vocab for each test set image, vocab is np.array(learn.dls.vocab, results is pd.Series[vocab[idxs], name='idxs) will map index to vocab item.\nGenerally speaking in Kaggle competitions, top 25% is a solid, competent, very reasonable level. It’s not easy, you gotta know what you’re doing.\nBatch things that are similar aspect ratios together and use the median rectangles for those and have had good results but honestly 99.99% of people chuck everything into a square.\nfastai uses reflection padding as a default, also provide copy padding, neither really help. Computer wants to know where the image ends.\n\n\n\n\n\n\n\n\nDigging into what’s inside of a neural net in this lesson.\nA neural net has a sandwich of fully connected layers and ReLUs. There’s a lot of tweaks that we can do. Most of the tweaks we care about are tweaking the very first or the very last layer. Over the next couple of weeks we’ll look at the tweaks we can do inside as well.\n\n\n\n\n\nCreated a ConvNeXt model. Did a few types of preprocessing. Added Test Time Augmentation. Scaled that up to larger images and rectangular images.\nLarger models have more parameters which means they can learn more tricky features. And they ought to be more accurate. The also take up more memory on the GPU when calculating gradients. The GPU is not as clever as the CPU at sticking stuff it doesn’t need right now onto virtual memory on the hard drive. When it runs out of memory, it runs out of memory. It also doesn’t shuffle things around to try and find memory, it just allocates blocks of memory that stay allocated until you remove them.\nIf you get a CUDA Out-Of-Memory error, restart your notebook. Tricky to recover from otherwise.\nWill I be able to train on 16GB? One way to quickly do that is train only on one label and see how much memory it used.\nCall python’s garbage collection gc.collect() and PyTorch’s torch.cuda.empty_cache() will get GPU back to a clean state.\nIf you run out of memory—use GradientAccumulation. Using a small batch size (bs = 16 instead of bs = 64) will solve the memory problem but will change the dyanmics of the training since the smaller your batch size the more volatility there is, so now your learning rates need to change. You don’t want to mess around trying to find different hyparameters for every batch size for every architecture. GradientAccumulation(bs) makes the training behave as if the batch size is bs even when it’s not.\nConsider the training loop:\n\nfor x,y in dl:\n  calc_loss(coeffs, x, y).backward()\n  coeffs.data.sub_(coeffs.grad * lr)\n  coeffs.grad.zero_()\n\nNote that you don’t need with torch.no_grad() since you are using coeffs.data.\nHere’s a variation of that loop with GradientAccumulation added:\n\ncount = 0   # track count of items seen since last weight update\nfor x,y in dl:\n  count += len(x) # update count based on this minibatch size\n  calc_loss(coeffs, x, y).backward()\n  if count >= 64: # count is greater than accumulation target so do weight update\n    coeffs.data.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n    count = 0 # reset count\n\nIn PyTorch if you call backward() without zeroing the gradients then it adds new gradients to old gradients.\nDoing two half-size batches without zeroing out between them is adding up the gradients.\nYou don’t need to buy a bigger GPU to train bigger models. Just use GradientAccumulation.\nGradientAccumulation is numerically identical for some architectures. Other architectures use batch normalization (which keeps track of the moving average of standard deviation and averages and does it in a mathematically slightly incorrect way). Using GradientAccumulation with batch normalization can introduce more volatility. Which is not necessarily a bad thing but it’s not numerically identical so you won’t get the same results.\nlr_find uses your DataLoaders batch size.\nPick the largest batch size that you can (you’re getting more parallel processing). Generally a good idea for it to be a multiple of 8 for performance reasons.\nIn fastai use GradientAccumulation by passing it as a cbs (callback)\n\ncbs = GradientAccumulation(<effective batch size>)\nlearn = vision_learner(dls, arch, metrics, cbs=cbs)\n\nFor bigger models you’ll get to a linear scaling with GradientAccumulation. Models have a bit of an overhead.\nNearly all transformer models have a fixed input size.\nUse different training sets (i.e. don’t set seed in the validation splitter) when you are going to ensemble.\nA popular thing is to do k-fold cross validation. 5-fold CV does something similar to what Jeremy did with training on a random 80% split. In theory that could be slightly better because you’re guaranteed that every row appears four times. Also has the benefit that you can average those five validation sets that have no overlap. Jeremy usually doesn’t bother because this way he can add or remove models very easily.\nNVIDIA consumer cards (RTX) are just as good as enterprise cards. NVIDIA will not allow you to use an RTX card in a data center. Which is why cloud computing is more expensive.\nteacher-student models and model distillation—there are ways to make inference faster by training small models that work the same way as large models.\n\n\n\nBuild a model to predict both disease and variety of rice. The first thing you need is a DataLoaders that have two dependent variables:\n\n\ndls = DataBlock(\n  blocks=(ImageBlock, CategoryBlock, CategoryBlock),\n  n_inp=1, # otherwise it doesn't know which of the 3 is ind/dep var\n  get_items=get_image_files,\n  get_y = [parent_label, get_variety],\n  splitter = RandomSplitter(0.2, seed=42),\n  item_tfms = Resize(192, method='squish'),\n  batch_tfms = aug_transforms(size=128, min_scale=0.75),\n).dataloaders(path)\n\nJeremy first create a DataBlock that did exactly the same thing as the single-dependent-variable disease-classifier and then once he got that to work, expanded it to two dependent variables.\nIn pandas you can set one column to be the index:\n\ndf = pd.read_csv(path/'train.csv', index_col='image_id')\nSo that you can then use df.loc['100330.jpg', 'variety'] to get the variety column for a given image_id. You can then wrap this into a function to use for get_y:\ndef get_variety(p): return df.loc[p.name, 'variety']\nWhere p is a Path object.\n\nHow do we get a model that predicts two things? We never had a model that predicted one thing, we had a model that predicts 10 things (probabilities for 10 disease classes). We want a model that now predicts 20 things.\nfastai will pass to metrics and loss function three things: the input and two dependent variables. Can’t just use error_rate as metric since that takes only two inputs. Instead have to create a custom metric that takes three inputs and returns the error rate for disease-only (same thing for loss):\n\ndef disease_err(inp, disease, variety): return error_rate(inp, disease)\ndef disease_loss(inp, disease, variety): return F.cross_entropy(inp, disease)\n\nThe stuff in the middle of the model, you’re going to think about that much but the stuff at the ends you think about a lot.\nCross Entropy Loss example: assume you have a mini imagenet with 5 classes (cat, dog, plane, fish, building):\n\n\n\n\n\noutput\nexp\nsoftmax\nactuals\nindex\n\n\n\n\ncat\n-4.89\n0.01\n0.00\n0\n1\n\n\ndog\n2.60\n13.43\n0.87\n1\n1\n\n\nplane\n0.59\n1.81\n0.12\n0\n2\n\n\nfish\n-2.07\n0.13\n0.01\n0\n3\n\n\nbuilding\n-4.57\n0.01\n0.00\n0\n4\n\n\n\n\noutput is the output from the model (5 values for 5 classes). They’re not probabilities yet, they’re just 5 numbers. We want to convert these into probabilities.\n\nSoftmax: \\[\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\\]\n\nWe’re going to go through each of the categories (1 to K = 5). Take \\(e\\) to the power of the output (\\(z\\)). Sum them all together. That’s the denominator. The numerator is \\(e\\) to the power of the thing that we care about (each row). The sum of these fractions is 1. Now we have things that are probabilities: numbers that are between 0 and 1 and add up to 1.0. Since we did \\(e\\) to the power of the output, the bigger outputs will be pushed up closer to 1.0. We’re making the model really try to pick one thing. There’s no way for it to predict anything other than the categories we are giving it. We are forcing it to pick one. You can have the probabilities add up to more than one (more than one thing being true) or less than one (no things being true).\nThe first part of what nn.CrossEntropy does it to calculate the softmax. It’s actually the log of the softmax.\nNow that we have the 5 probabilities, the next step is the actual cross-entropy calculation:\n\n\n\n\n\nsoftmax\nactuals\nx-entropy\n\n\n\n\ncat\n0.00\n0\n1\n\n\ndog\n0.87\n1\n1\n\n\nplane\n0.12\n0\n2\n\n\nfish\n0.01\n0\n3\n\n\nbuilding\n0.00\n0\n4\n\n\n\n\nThe actuals are one-hot encoded (1 for the thing that is True and 0 everywhere else).\nWe would expect a smaller loss where the softmax is high if the actual is high. Formula for cross-entropy:\n\n\\[-\\sum_{j=1}^M y_j\\log(p(y_j))\\]\n\nWhere \\(y_j\\) is an indicator variable and \\(p(y_j)\\) is the predicted probability (the softmax column). Cross entropy is then -log(softmax). For four classes that value is 0. That equation is finding the probablity for the class that is 1 and taking its log.\nHere’s the equation for binary cross-entropy:\n\n\\[-\\sum_{i=1}^N y_i \\log(p(y_i)) + (1 - y_i)\\log(1 - p(y_i))\\]\n\nWhere \\(y_i\\) is the label, and \\(p(y_i)\\) is the probability of the positive class. \\(y_i=1\\) if it is a cat, \\(y_i=0\\) if it is not a cat.\nPyTorch loss functions have two versions: nn Class which you can instantiate passing in various tweaks and the F function that doesn’t allow these tweaks.\nWhen you have multiple targets you can’t rely on fastai to know what loss function to use so you have to pass your custom loss function to the loss_func parameter in the learner. Same for metrics. Also, fastai no longer knows how many activations to create because there is more than one target so you have to pass a value to n_out which is the number of targets (the size of the last matrix).\nFor two-target situation, we have to set n_out to 20 when creating the learner since 10 of those targets are for disease and 10 are for variety of rice. How does the model know what it’s predicting? The answer is: with the loss function—you’re going to have to tell it. inp is going to have 20 columns (since n_out is 20) so we’re just going to have to decide that the first 10 columns correspond to the disease predictions.\n\ndef disease_loss(inp, disease, variety): return F.cross_entropy(inp[:,:10], disease)\n\nFor variety, we use the second ten columns:\n\ndef variety_loss(inp, disease, variety): return F.cross_entropy(inp[:,10:], variety)\n\nThe overall loss function is the sum of those two things:\n\ndef combine_loss(inp,disease,variety): return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety)\nAs the model trains, this loss function will be minimized when the first ten columns are doing a good job at predicting disease probabilities and the second ten columns are doing a good job at predicting variety probabilities. Therefore the gradients will point in the appropriate direction, the coefficients will get better and better at using those columns for those purposes.\n\nDo the same for error_rate:\n\ndef disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease)\ndef variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety)\n\nerr_metrics = (disease_err,variety_err)\n\nthe Learner looks like:\n\nlearn = vision_learner(dls, arch, loss_func=combine_loss, metrics=err_metrics, n_out=20)\n\nIt was slightly less good at predicting disease but that makes sense because we have trained it for the same number of epochs (5 in this case) but have given it more stuff to do.\nIf we train it for longer, this model might end up getting better at predicting disease than the single-label disease model. It turns out quite often that the kind of features that help you recognize variety of rice also help recognize disease, maybe there are certain textures, or maybe some diseases impact different varieties in different ways.\nBuild a model that predicts two things in the Titanic dataset.\nLook at the inputs and outputs of the multi-target part 4 notebook.\n\n\n\n\n\nThis kind of data is very common:\n\n\n\n\nuser\nmovie\nrating\n\n\n\n\n196\n242\n3\n\n\n186\n302\n3\n\n\n22\n377\n1\n\n\n244\n51\n2\n\n\n166\n346\n1\n\n\n\nAnytime you have a user and product you’ll have this kind of data. What happens when the rating is blank? How do you fill it in? To figure this out, ideally we’d like to know for each movide: what kind of movie is it? What are the features of it? If we had three categories: science-fiction, action and old movies, then Last Skywalker would be represented by the following (where each value ranges from -1 to 1):\nlast_skywalker = np.array([0.98, 0.9, -0.9])\nIt’s very science-fictiony, very action-y and very not old.\nA user who liked modern sci-fi could be represented by:\nuser1 = np.array([0.9, 0.8, -0.6])\nTo calculate the match between last_skywalker and user1 we can multiple the values and sum:\n(user1*last_skywalker).sum() # = 2.142\n\nOn the other hand, the movie Casablanca, not science-fiction, not really very action, and very much an old classic:\n\ncasablanca = np.array([-0.99, -0.3, 0.8])\n\nOn the other hand, the movie Casablanca, not science-fiction, not really very action, and very much an old classic:\n\ncasablanca = np.array([-0.99, -0.3, 0.8])\n\nMatching it with the user:\n\n(user1*casablanca).sum() # -1.611\n\nMultiplying the corresponding elements of two vectors and adding them up is called dot product. The above is a dot product of the users preferences and a type of movie. The problem is we weren’t given this information about users and movies. What we can do is create things called Latent Factors: I don’t know what things about movies matter to people, but there’s probably something, and let’s just try using SGD to find them. We can do it in Microsoft Excel!\nIn Excel we create 5 latent factors (rows) of random numbers for each movieId and userId. We don’t know what these represent but they represent something. Only quirk is that if the actual rating is blank we’re going to set the dot product to 0 by default.\nThe matrix product of a row and a column is the same thing as a dot product. These dot products are everybody’s predicted ratings for movies. They are terrible predictions since the latent factors are just random numbers, but they are predictions nonetheless.\nWhen we have predictions using random numbers, we know how to make them better: stochastic gradient descent. To do that we need a loss function: RMSE = square root of sum of x minus y squared divided by the count (in Excel: =SQRT(SUMXMY2()/COUNT()))\nExcel solver: minimize cell with loss by changing userId and movieId latent factors. In Jeremy’s workbook: starts at 2.81 and ends at 0.42. In my workbook: starts at 2.92 and ends up at 0.43 after Solver is run.\nThe cosine of the angle between vectors is the same as the normalized dot product.\nUsing embeddings: replicating what we’ll have in Python which is a table with rows userid, movieid and rating. We’ll get the embeddings for each userid and the embeddings for each movieid all in one row, and then use Excel function SUMPRODUCT (which is dot product) to get the prediction. This is the same as before but when we put everything next to each other we have to lookup the index of userId and movieId and then lookup the embeddings.\nFor each row calculate the error squared (pred-rating)^2 and take the square root of the average of error squareds to get the rmse, which is 2.71 in my case (Jeremy used the same random initial numbers for the dot product tab and the movielens_emb tab).\nRunning solver: my rmse goes from 2.71 to 0.443 which is about the same as before (with different randomly initiated embeddings).\nWhat is an embedding? It’s just looking something up in an array.\nHow do we do this in PyTorch? We’re going to need DataLoaders.\n\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\nmovies.head() outputs:\n\n\n\n\nmovie\ntitle\n\n\n\n\n0\n1\nToy Story (1995)\n\n\n1\n2\nGoldenEye (1995)\n\n\n2\n3\nFour Rooms (1995)\n\n\n3\n4\nGet Shorty (1995)\n\n\n4\n5\nCopycat (1995)\n\n\n\nMerge this with ratings so we can get the movie titles:\nratings = pd.read_csv(path/'u.data', delimeter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nratings.head() outputs:\n\n\n\n\nuser\nmovie\nrating\ntimestamp\n\n\n\n\n0\n196\n242\n3\n881250949\n\n\n1\n186\n302\n3\n891717742\n\n\n2\n22\n377\n1\n878887116\n\n\n3\n244\n51\n2\n880606923\n\n\n4\n166\n346\n1\n886397596\n\n\n\nMerge with movies to get title:\nratings = ratings.merge(ratings)\nratings.head() output:\n\n\n\n\nuser\nmovie\nrating\ntimestamp\ntitle\n\n\n\n\n1\n63\n242\n3\n875747190\nKolya (1996)\n\n\n2\n226\n242\n5\n883888671\nKolya (1996)\n\n\n3\n154\n242\n3\n879138235\nKolya (1996)\n\n\n4\n306\n242\n5\n876503793\nKolya (1996)\n\n\n\nNext we create the DataLoaders with CollabDataLoaders which expects a user column and item column where item is the service or product that the user is rating. By default the user column should be called user and the item column called item.\ndls = CollabDataLaoders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch() outputs:\n\n\n\n\n\n\n\n\n\n\nuser\ntitle\nrating\n\n\n\n\n0\n518\nRichard III (1995)\n3\n\n\n1\n546\nStar Wars (1977)\n5\n\n\n2\n264\nAdventures of Priscilla, Queen of the Desert, The (1994)\n4\n\n\n3\n201\nKolya (1996)\n4\n\n\n4\n664\nDances with Wolves (1990)\n3\n\n\n5\n391\nJerry Maguire (1996)\n4\n\n\n6\n401\nBeauty and the Beast (1991)\n2\n\n\n7\n771\nStrictly Ballroom (1992)\n5\n\n\n8\n330\n101 Dalmatians (1996)\n4\n\n\n9\n594\nOne Flew Over the Cuckoo’s Nest (1975)\n4\n\n\n\nNow we’re going to create the user factors and movie factors (i.e. the two embedding matrices we created in the Excel file). The number of rows of movie factors is equal to the number of movies and the number of columns will be whatever we want (however many factors we want to create). How many factors to use? Jeremy wrote down how many factors he thought was appropriate for different sized categories in Excel and fitted a function to that and that’s the function fastai uses—a mathematical function that fits Jeremy’s intuition about what works well. It’s pretty fast to train these things so you can try a few.\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\nNow we need to lookup the index of our movie in our movie latent factor matrix (and user index for the user latent factor matrix). When we’ve learned about deep learning we’ve learned about matrix multiplication, not look-something-up-in-a-matrix. In Excel we were using OFFSET which can actually be represented as matrix multiplication. “Find this element in this list” is the same as matrix multiplying a one-hot-encoded vector. Taking the dot product of a one-hot-encoded vector with another vector is the same as looking up that index in the vector.\none_hot_3 = one_hot(3, n_users).float()\nis a vector where the 3rd element is set to 1 and everything else is set to 0s.\nIf we matrix multiply that by our user_factors transposed:\nuser_factors.t() @ one_hot_3\nWe get tensor([-1.2493, -0.3099, 1.4229, 0.0840, 0.4132])\nwhich is the same as the vector at index 3 in the matrix:\nuser_factors[3]\nYou can think of an embedding as a computational shortcut for multiplying something by a one-hot-encoded vector. It’s like dummy variables (without having to create the dummy variables). We never have to create a one-hot-encoded vector, we can just look up an array.\n\nBuilding a collaborative filtering model from scratch\n\nIn PyTorch, a model is a class. Example:\nclass Example:\n  def __init__(self, a): self.a = a\n  def say(self, x): return f'Hello {self.a}, {x}.'\n__init__ is called when you create an object of the given class.\nex = Example('Sylvain')\nex.say('nice to meet you')\noutputs:\n'Hello Sylvain, nice to meet you.'\nYou can put something in parenthesis after your class name, the super class, which will give you some functionality for free. A PyTorch model has to have Module as its super class. fastai also has its own Module class. Here’s a DotProduct class:\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return (users * movies).sum(dim=1)\nPyTorch calls a forward method when you call a model object. This is where you put the calculation of your model. dim=1 because we are summing across the columns for each row in the batch–a prediction for each row. We can now pass the model to the Learner:\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nThen we cant train:\nlearn.fit_one_cycle(5, 5e-3)\nThis runs on CPU and takes about 10 seconds per epoch (100k rows) and gets to 0.86 loss after 5 epochs. A whole lot faster than our few dozen rows in Excel. It’s not a great model. One problem is that some of the predictions are greater than 5.\nWhen we add sigmoid, it squishes things to between 0 and 1 so the model doesn’t have to work so hard to get the predictions into the right zone. If you pass something through sigmoid and multiply it by 5, now you’re going to get something between 0 and 5. Use sigmoid_range to do that:\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\nWhy not use upper limit of 5? That’s because sigmoid can never hit 1. So sigmoid times 5 can never hit 5. In this case, this didn’t improve the loss.\nSome users just loved movies–they give everything 4s and 5s. Some people’s ratings have much more range (1s, 2s, 5s). Some people give nothing a 5. At the moment we don’t have any way in our formulation of this model to say this user tends give low scores and this user tends to give high scores. That would be very easy to add. Let’s add one more number to our 5 factors. Now instead of just matrix multiplying, let’s add this number to it. Matrix multiplication plus user bias plus movie bias. Effectively that’s making it so that we don’t have an intercept of 0 anymore. Implementing this in my Excel dropped the loss from 0.43 to 0.40.\nHere’s the PyTorch version:\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\nIn Jeremy’s case, this made the training worse (the loss increased) and the validation loss started increasing after the second epoch—we might be overfitting. One way to avoid overfitting is to use weight decay (also known as L2 regularization). When we compute the gradients we’ll add to our loss function the sum of the weights squared (times some small number). What would make that loss function go down? If we reduce the magnitude of our weights. For example if we reduce all of our weights to 0, that part of the los function will be 0. The problem is, if our weights are all 0, our model doesn’t do anything. So we want it to increase the weights. But if it increases the weights too much, then it starts overfitting. How is it going to actually get the lowest value of the loss function? By finding the right mix. Weights not too high but high enough to be useful for predicting. If there’s some paramter that’s not useful, it can just set the weight to 0. It won’t be used to predict anything but it also won’t contribute to the weight decay.\nloss_with_wd = loss + wd * (parameters**2).sum()\nIn fact, we don’t even need to do this because the whole purpose of the loss is to take its gradient. The gradient of parameters squared is 2 times parameters.\nparameters.grad += wd * 2 * parameters\nFold the 2 into the wd since it’s just some number we’re going to pick. When you call fit, pass in the wd parameter:\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\nThis finally improves our loss. In fastai applications like vision, fastai sets wd appropriately, but in things like tabular and collaborative filtering fastai doesn’t know enough about your data so you just try a few wd values. Regularization is about making your model no more complex than it has to be. The higher the weights, the more they’re moving the model around, we want to keep the weights down, but not so far down that they don’t make a prediction. If wd is higher, it’ll keep the weights down more, reduce overfitting, but will also reduce the capacity of your model to make good predictions. If it’s lower, it increases the capacity of your model, and increases overfitting.\nCan recommendation systems be built based on average ratings of users’ experience rather than collaborative filtering? Not really, if you’ve got lots of metadata you could (demographic data on users for example) then sure averages would be fine. But if all you’ve got is purchasing history, then you really want the granular data, there’s not enough information there to use averages.\n\n\n\nCollaborative filtering: look at which products the current user has used or liked, find other users who have used or liked similar products, and then recommend other products that those users have used or liked. We don’t necessarily need to know anything about the products except who liked them.\nLatent factors: the key foundational idea in collaborative filtering—the underlying concepts behind users and items that don’t need to be defined explicitly with columns of data.\n\n\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nIf we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors, and actors, and so forth, and we knew the same information about each movie, then a simply way to fill empty ratings would be to multiply this information together for each movie and user combination.\n\nlast_skywalker = np.array([0.98, 0.9, -0.9]) # sci-fi, action, old movie\nlast_skywalker\n\narray([ 0.98,  0.9 , -0.9 ])\n\n\n\nuser1 = np.array([0.9, 0.8, -0.6]) # sci-fi, action, old movie\nuser1\n\narray([ 0.9,  0.8, -0.6])\n\n\n\n# combination with 3 being the max\n(user1 * last_skywalker).sum()\n\n2.1420000000000003\n\n\ndot product: multiplying two vectors together and add up the results.\n\nThe mathematical operation of multiplying the elements of two vectors together, and then summing up the results.\n\n\ncasablanca = np.array([-0.99, -0.3, 0.8]) # sci-fi, action, old movie\ncasablanca\n\narray([-0.99, -0.3 ,  0.8 ])\n\n\n\n# user1 won't like this as much as last skywalker\n(user1 * casablanca).sum()\n\n-1.611\n\n\n\n\n\nThere is surprisingly little difference between specifying the structure of a model and learning one, since we can just use our general gradient descent approach:\n\nStep 1: randomly initialize some parameters.\nStep 2: calculate our predictions.\nStep 3: calculate our loss.\n\nMore details:\n\nStep 1: the parameters we randomly initialize will be a set of latent factors for each user and movie. We’ll use 5 latent factors for now.\nStep 2: calculate predictions by taking the dot product of each movie with each user. If the first latent user factor represents how much the user likes action movies and the first latent movie factor represents whether the movie has a lot of action, the product of those will be particularly high if either the user likes action movies and the movie has a lot of action in it, or the user doesn’t like action movies and the movie doesn’t have any action in it. The product will be low if we have a mismatch.\nStep 3: We’ll pick mean squared error for now.\n\nWith this in place we can optimize our parameters using stochastic gradient descent such as to minimize the loss. At each step, the stochastic gradient descent optimizer will calculate the match between each movie and each user using the dot product and will compare it to the actual rating that each user gave to each movie. It will then calculate the derivative of this value and step the weights by multiplying this by the learning rate. After doing this lots of times the loss will get better and the recommendations will also get better and better.\n\n\n\nTo use the Learner.fit function, we will need to get our data into a DataLoaders. When showing the data we would rather see movie titles than their IDs:\n\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\nmovies.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      210\n      Some Like It Hot (1959)\n      5\n    \n    \n      1\n      651\n      Godfather, The (1972)\n      4\n    \n    \n      2\n      515\n      Starship Troopers (1997)\n      4\n    \n    \n      3\n      49\n      Swimming with Sharks (1995)\n      4\n    \n    \n      4\n      512\n      Nikita (La Femme Nikita) (1990)\n      5\n    \n    \n      5\n      497\n      Rob Roy (1995)\n      4\n    \n    \n      6\n      664\n      Dave (1993)\n      3\n    \n    \n      7\n      880\n      Empire Strikes Back, The (1980)\n      5\n    \n    \n      8\n      185\n      Leaving Las Vegas (1995)\n      4\n    \n    \n      9\n      815\n      Aladdin (1992)\n      3\n    \n  \n\n\n\nTo represent collaborative filtering in PyTorch, we can’t just use the crosstab representation directly, especially if we want to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nn_users, n_movies, n_factors\n\n(944, 1665, 5)\n\n\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nuser_factors.shape, movie_factors.shape\n\n(torch.Size([944, 5]), torch.Size([1665, 5]))\n\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix, and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors.\nWe can represent look up an index as a matrix product by replacing indices with one-hot-encoded vectors.\n\none_hot_3 = one_hot(3, n_users).float()\nuser_factors.t() @ one_hot_3\n\ntensor([-2.5648, -0.4866, -0.9996, -1.8835, -1.0867])\n\n\n\nuser_factors[3]\n\ntensor([-2.5648, -0.4866, -0.9996, -1.8835, -1.0867])\n\n\nIf we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors and that operation will be a matrix multiplication. This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary.\n\none_hot_3\n\ntensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\none_hots = torch.stack([\n    one_hot(3, n_users).float(),\n    one_hot(4, n_users).float(),\n    one_hot(5, n_users).float()])\n\none_hots @ user_factors\n\ntensor([[-2.5648, -0.4866, -0.9996, -1.8835, -1.0867],\n        [-0.0096, -0.0892, -1.4639,  0.6083, -1.0248],\n        [ 0.0330, -0.6358,  0.6536, -0.9384,  0.0973]])\n\n\n\nuser_factors[3:6]\n\ntensor([[-2.5648, -0.4866, -0.9996, -1.8835, -1.0867],\n        [-0.0096, -0.0892, -1.4639,  0.6083, -1.0248],\n        [ 0.0330, -0.6358,  0.6536, -0.9384,  0.0973]])\n\n\nThere is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number 1–we should just be able to index into an array directly with an integer.\nembedding: a special layer that indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector.\nHow do we determine the numbers to characterize these different features of movies and users? We don’t. We let the model learn them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not.\nWe will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\n\n\n\n\n# example class\nclass Example:\n  def __init__(self, a): self.a = a\n  def say(self, x): return f'Hello {self.a}, {x}.'\n\nExample('Vishal').say('how are you?')\n\n'Hello Vishal, how are you?.'\n\n\nCreating a new PyTorch module requires inheriting from Module. When your module is called, PyTorch will call a method in your class called forward and will pass along to that any parameters that are included in the call.\n\nModule??\n\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return (users * movies).sum(dim=1)\n\n\nx, y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\n\n# doing from scratch so use plain Learner\nmodel = DotProduct(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n\nlearn.arch\n\nAttributeError: 'DotProduct' object has no attribute 'arch'\n\n\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.311295\n      1.325218\n      00:11\n    \n    \n      1\n      1.011121\n      1.110533\n      00:12\n    \n    \n      2\n      0.882769\n      1.013594\n      00:11\n    \n    \n      3\n      0.790874\n      0.926204\n      00:12\n    \n    \n      4\n      0.769741\n      0.900602\n      00:11\n    \n  \n\n\n\nApply sigmoid_range to force predictions to be between 0 and 5:\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\nBefore I train, I want to look at why dim=1 is set in sum:\n\nx.shape\n\ntorch.Size([64, 2])\n\n\n\nuser_factors = Embedding(n_users, n_factors)\nmovie_factors = Embedding(n_movies, n_factors)\n\n\nusers = user_factors(x[:,0])\nmovies = movie_factors(x[:,1])\n\n\nusers.shape, movies.shape\n\n(torch.Size([64, 5]), torch.Size([64, 5]))\n\n\n\n(users * movies).sum()\n\ntensor(-0.0019, grad_fn=<SumBackward0>)\n\n\n\n(users * movies).sum(dim=1).shape\n\ntorch.Size([64])\n\n\nSo, dim=1 ensures that the predictions (users * movies) are calculated for each item in the batch.\n\n# doing from scratch so use plain Learner\nmodel = DotProduct(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.923018\n      1.000980\n      00:13\n    \n    \n      1\n      0.663725\n      0.956221\n      00:13\n    \n    \n      2\n      0.439762\n      0.960771\n      00:12\n    \n    \n      3\n      0.361286\n      0.964587\n      00:13\n    \n    \n      4\n      0.333990\n      0.961902\n      00:14\n    \n  \n\n\n\nThis actually worsened the model!\nOne obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. In our current implementation we do not have any way to encode such things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don’t really have any way to say whether most people like it. We can handle this missing piece with biases—a single number for each user and movie that we can add to our score.\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\n\nBefore I train I want to walk through this code to make sure I understand what’s happening at each step and why.\n\nx, y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\n\nuser_factors = Embedding(n_users, n_factors)\nuser_bias = Embedding(n_users, 1)\nmovie_factors = Embedding(n_movies, n_factors)\nmovie_bias = Embedding(n_movies, 1)\nuser_factors, user_bias, movie_factors, movie_bias\n\n(Embedding(944, 5), Embedding(944, 1), Embedding(1665, 5), Embedding(1665, 1))\n\n\n\nusers = user_factors(x[:,0])\nmovies = movie_factors(x[:,1])\nusers.shape, movies.shape\n\n(torch.Size([64, 5]), torch.Size([64, 5]))\n\n\n\nusers[0]\n\ntensor([ 0.0005, -0.0128, -0.0086,  0.0043, -0.0140],\n       grad_fn=<SelectBackward0>)\n\n\n\nx[0]\n\ntensor([ 422, 1407])\n\n\n\nuser_factors(torch.tensor([422]))\n\ntensor([[ 0.0005, -0.0128, -0.0086,  0.0043, -0.0140]],\n       grad_fn=<EmbeddingBackward0>)\n\n\n\n(users * movies).sum(dim=1, keepdim=True).shape, (users * movies).sum(dim=1, keepdim=False).shape\n\n(torch.Size([64, 1]), torch.Size([64]))\n\n\n\nres = (users * movies).sum(dim=1, keepdim=True)\nres += user_bias(x[:,0]) + movie_bias(x[:,1])\nres.shape\n\ntorch.Size([64, 1])\n\n\n\nres = (users * movies).sum(dim=1, keepdim=False)\nres += user_bias(x[:,0]) + movie_bias(x[:,1])\nres.shape\n\nRuntimeError: output with shape [64] doesn't match the broadcast shape [64, 64]\n\n\n\nuser_bias(x[:,0]).shape\n\ntorch.Size([64, 1])\n\n\nkeepdim=True is needed so that we can add the 64 x 1 bias tensors to res.\n\nmodel = DotProductBias(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.838959\n      0.961033\n      00:12\n    \n    \n      1\n      0.584438\n      0.919422\n      00:12\n    \n    \n      2\n      0.406340\n      0.946146\n      00:13\n    \n    \n      3\n      0.323580\n      0.960616\n      00:13\n    \n    \n      4\n      0.303791\n      0.959865\n      00:16\n    \n  \n\n\n\nThe valid_loss was decreasing from the first to second epoch but increased from the second to third and third to fourth epoch, which is a sign of overfitting.\n\n\n\nAdd the sum of all weights squared to the loss so that when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\nThe larger the coefficients are the sharper the canyons we will have in the loss function. Letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\nloss_with_wd = loss + wd * (parameters ** 2).sum()\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better.\nIn practice it would be very inefficient and maybe numerically unstable to compute that big sum and add it to the loss. Adding that sum to the loss function is the same as doing the following to the gradients:\nparameters.grad += wd * 2 * parameters\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.913064\n      0.965725\n      00:12\n    \n    \n      1\n      0.682149\n      0.911183\n      00:12\n    \n    \n      2\n      0.539120\n      0.890211\n      00:12\n    \n    \n      3\n      0.445476\n      0.877169\n      00:12\n    \n    \n      4\n      0.429690\n      0.872437\n      00:12\n    \n  \n\n\n\nFinally! The loss consistently decreases each epoch.\n\n\n\nOptimizers require that they can get all the parameters of a module from the module’s parameters method, but this does not happen automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:\n\nclass T(Module):\n  def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n\n(#0) []\n\n\nTo tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesn’t add any functionality (other than automatically calling requires_grad_ for us). It’s used only as a “marker” to show what ot include in parameters.\n\nclass T(Module):\n  def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\nAll PyTorch modules use nn.Parameter for any trainable paramters:\n\nclass T(Module):\n  def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n\n(#1) [Parameter containing:\ntensor([[ 0.7111],\n        [-0.4145],\n        [ 0.4969]], requires_grad=True)]\n\n\n\ntype(t.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\n\n# create a tensor as a parameter, with random initialization\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\n\n# create DotProductBias without embedding\nclass DotProductBias2(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n    self.user_factors = create_params([n_users, n_factors])\n    self.user_bias = create_params([n_users])\n    self.movie_factors = create_params([n_movies, n_factors])\n    self.movie_bias = create_params([n_movies])\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors[x[:,0]]\n    movies = self.movie_factors[x[:,1]]\n    res = (users * movies).sum(dim=1)\n    res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProductBias2(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.904102\n      0.942020\n      00:11\n    \n    \n      1\n      0.654384\n      0.880052\n      00:10\n    \n    \n      2\n      0.509761\n      0.857724\n      00:09\n    \n    \n      3\n      0.451252\n      0.838781\n      00:10\n    \n    \n      4\n      0.445227\n      0.834458\n      00:10\n    \n  \n\n\n\n\nx[0]\n\ntensor([ 422, 1407])\n\n\n\nlearn.model(x)\n\ntensor([3.0069, 2.4127, 3.1486, 3.0827, 4.2363, 3.0727, 4.6257, 2.8603, 4.0537,\n        2.7821, 1.5680, 3.5075, 1.9389, 4.7167, 3.6408, 2.3103, 3.2855, 3.0170,\n        4.1721, 4.1315, 4.7459, 4.7006, 3.5047, 3.6348, 2.6413, 2.6047, 4.9137,\n        3.3948, 3.4747, 3.9256, 3.8448, 3.3731, 3.9032, 1.3075, 4.4401, 3.6986,\n        3.3045, 3.5697, 4.3211, 3.7295, 1.1871, 3.2220, 3.5667, 3.5309, 4.5825,\n        2.2819, 4.1466, 3.3740, 4.4830, 3.2561, 3.0671, 3.9423, 2.9384, 4.2050,\n        2.3258, 4.0698, 4.0947, 2.9023, 3.8935, 3.0650, 3.3615, 3.3382, 3.2790,\n        4.0849], grad_fn=<AddBackward0>)\n\n\n\nratings.head(2)\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nlearn.get_preds(\n    dl=learn.dls.test_dl(ratings.head(2), with_input=True, with_decoded=True)\n    )\n\n\n\n\n\n\n\n\n(tensor([3.9271, 3.7005]),\n tensor([[3],\n         [3]]))\n\n\n\nlearn.get_preds(dl=learn.dls.test_dl(pd.DataFrame(data={'user': [196, 63], 'title': ['Kolya (1996)', 'Kolya (1996)']})))\n\n\n\n\n\n\n\n\n(tensor([3.9271, 3.7005]), None)\n\n\n\nlearn.model(tensor([[ 196, 242]]))\n\ntensor([3.1319], grad_fn=<AddBackward0>)\n\n\n\n\n\nThe easiest parameters to interpret are biases. For movies with a low bias: even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don’t like it.\n\nlearn.model\n\nDotProductBias2()\n\n\n\nlearn.model.movie_bias.shape\n\ntorch.Size([1665])\n\n\n\nmovie_bias = learn.model.movie_bias.squeeze()\nmovie_bias.shape\n\ntorch.Size([1665])\n\n\n\nmovie_bias[:5]\n\ntensor([ 0.0034, -0.1036,  0.0292, -0.0822,  0.4568], grad_fn=<SliceBackward0>)\n\n\nNote: PyTorch’s squeeze:\n\nReturns a tensor with all specified dimensions of input of size 1 removed.\n\n\n# bottom 5 movies\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Solo (1996)',\n 'Mortal Kombat: Annihilation (1997)',\n 'Crow: City of Angels, The (1996)']\n\n\n\n# top 5 movies\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Good Will Hunting (1997)',\n 'Titanic (1997)',\n \"Schindler's List (1993)\",\n 'Rear Window (1954)']\n\n\n\nlearn.model.movie_factors.shape\n\ntorch.Size([1665, 50])\n\n\n\nmovie_bias.argsort(descending=True)[:5]\n\ntensor([1318,  622, 1501, 1282, 1216])\n\n\n\ndls.classes['title'][1318]\n\n'Shawshank Redemption, The (1994)'\n\n\nPCA code from the fastbook repo:\n\ng = ratings.groupby('title')['rating'].count()\ng\n\ntitle\n'Til There Was You (1997)                  9\n1-900 (1994)                               5\n101 Dalmatians (1996)                    109\n12 Angry Men (1957)                      125\n187 (1997)                                41\n                                        ... \nYoung Guns II (1990)                      44\nYoung Poisoner's Handbook, The (1995)     41\nZeus and Roxanne (1997)                    6\nunknown                                    9\nÁ köldum klaka (Cold Fever) (1994)         1\nName: rating, Length: 1664, dtype: int64\n\n\n\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_movies[:5]\n\narray(['Star Wars (1977)', 'Contact (1997)', 'Fargo (1996)',\n       'Return of the Jedi (1983)', 'Liar Liar (1997)'], dtype=object)\n\n\n\ng.sort_values(ascending=False).index\n\nIndex(['Star Wars (1977)', 'Contact (1997)', 'Fargo (1996)',\n       'Return of the Jedi (1983)', 'Liar Liar (1997)',\n       'English Patient, The (1996)', 'Scream (1996)', 'Toy Story (1995)',\n       'Air Force One (1997)', 'Independence Day (ID4) (1996)',\n       ...\n       'Girl in the Cadillac (1995)', 'He Walked by Night (1948)',\n       'Hana-bi (1997)', 'Object of My Affection, The (1998)',\n       'Office Killer (1997)', 'Great Day in Harlem, A (1994)',\n       'Other Voices, Other Rooms (1997)', 'Good Morning (1971)',\n       'Girls Town (1996)', 'Á köldum klaka (Cold Fever) (1994)'],\n      dtype='object', name='title', length=1664)\n\n\n\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\n\n\nlearn.dls.classes['title'].o2i['Star Wars (1977)']\n\n1399\n\n\n\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\n\n\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.928530\n      0.946365\n      00:12\n    \n    \n      1\n      0.683226\n      0.883067\n      00:10\n    \n    \n      2\n      0.506070\n      0.853040\n      00:10\n    \n    \n      3\n      0.446197\n      0.840192\n      00:10\n    \n    \n      4\n      0.439921\n      0.836284\n      00:10\n    \n  \n\n\n\nView the names of the model layers\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nReplicate previous analyses:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Usual Suspects, The (1995)',\n \"Schindler's List (1993)\",\n 'Silence of the Lambs, The (1991)']\n\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.i_weight.weight[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\nIf there were two movies that were nearly identical, their embedding vectors would also have to be nearly identical, because the users who would like them would be nearly exactly the same. Movie similarity can be defined by the similarity of users who like those movies. The distance between two movies’ embedding vectors can define that similarity.\n\n# find the most similar movie to Silence of the Lambs\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'His Girl Friday (1940)'\n\n\n\ndistances.argsort(descending=True)\n\ntensor([1330,  688,  846,  ..., 1048,  595,  850])\n\n\n\n\n\nboostrapping problem: Having no users and therefore no history to learn from. What products do you recommend to your very first user? What do you do when a new user signs up? What do you do when you add a new product to your portfolio? Use your common sense.\n\nPick a user to represent average taste (instead of averaging all user embeddings as this can incorrectly represent relationships between latent factors).\nUser a tabular model based on user metadata to construct your initial embedding vector. Think about what questions you could ask to help you understand users’ tastes. Create a model with the dependent variable the user’s embedding vector and the independent variables are the rsults of the questions you ask them along with their signup metadata.\nA small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. Such a problem can change the entire makeup of your user base, and the behavior of the system, particularly because of positive feedback loops: a small number of users set the direction of recommendation systems, end up attracting more people like them to your system, amplifyig the original bias, exponentially. Ensure that humans are in the loop of the data pipeline, with careful monitoring of the system and a gradual and thoughtful rollout. Think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data.\n\nThe dot-product approach to collaborative filtering is known as probabilistic matrix factorization (PMF).\n\n\n\nTake the results of the embedding lookup and concatenate those activations together, giving us a matrix that we can then pass through linear layers and nonlinearities.\nSince we’ll be concatenating the embedding matrices, rather than taking their dot product, the two embedding matrices can have different sizes (different number of latent factors). get_emb_sz returns recommended sizes for embedding matrices based on Jeremy’s intuition for what works well.\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\n\nclass CollabNN(Module):\n  def __init__(self, user_sz, item_sz, y_range=(0, 5.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.item_factors = Embedding(*item_sz)\n    self.layers = nn.Sequential(\n        nn.Linear(user_sz[1]+item_sz[1], n_act),\n        nn.ReLU(),\n        nn.Linear(n_act, 1)\n    )\n    self.y_range = y_range\n\n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1))\n    return sigmoid_range(x, *self.y_range)\n\nWorking through this code step-by-step:\n\nuser_factors = Embedding(*embs[0])\nmovie_factors = Embedding(*embs[1])\n\nuser_factors, movie_factors\n\n(Embedding(944, 74), Embedding(1665, 102))\n\n\n\nlayers = nn.Sequential(\n    nn.Linear(embs[0][1]+embs[1][1], 100),\n    nn.ReLU(),\n    nn.Linear(100, 1)\n)\n\nlayers\n\nSequential(\n  (0): Linear(in_features=176, out_features=100, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=100, out_features=1, bias=True)\n)\n\n\n\ny_range = (0, 5.5)\n\n\nx = dls.one_batch()[0]\nx.shape\n\ntorch.Size([64, 2])\n\n\n\nembs = user_factors(x[:,0]), movie_factors(x[:,1])\nembs = torch.cat(embs, dim=1)\nembs.shape\n\ntorch.Size([64, 176])\n\n\n\nx = layers(embs)\nx.shape\n\ntorch.Size([64, 1])\n\n\n\nx = sigmoid_range(x, *y_range)\nx.shape\n\ntorch.Size([64, 1])\n\n\n\n# create a model\nembs = get_emb_sz(dls)\nmodel = CollabNN(*embs)\n\n\nmodel\n\nCollabNN(\n  (user_factors): Embedding(944, 74)\n  (item_factors): Embedding(1665, 102)\n  (layers): Sequential(\n    (0): Linear(in_features=176, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=1, bias=True)\n  )\n)\n\n\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01) # note the smaller wd\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.909496\n      0.933792\n      00:24\n    \n    \n      1\n      0.865688\n      0.909720\n      00:24\n    \n    \n      2\n      0.821369\n      0.876649\n      00:23\n    \n    \n      3\n      0.772402\n      0.854223\n      00:14\n    \n    \n      4\n      0.768974\n      0.850676\n      00:12\n    \n  \n\n\n\n\n# alternative way to train a NN\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.968165\n      0.987966\n      00:18\n    \n    \n      1\n      0.906930\n      0.917807\n      00:13\n    \n    \n      2\n      0.830938\n      0.884382\n      00:13\n    \n    \n      3\n      0.781613\n      0.859800\n      00:13\n    \n    \n      4\n      0.749270\n      0.855626\n      00:13\n    \n  \n\n\n\nfastai uses EmbeddingNN which inherits from TabularModel.\n**kwargs in a parameter list means “put any additional keyword arguments into a dict called kwargs.” And **kwargs in an argument list means “insert all key/value pairs in the kwargs dict as named arguments here.”\nJupyter Notebook doesn’t know what parameter are available with **kwargs so things like tab completion or parameter names and pop-up lists won’t work. fastai resolves this by providing a special @delegates decorator which automatically changes the signature of the class or function to insert all of its keyworkd arguments into the signature.\nWe can incorporate other user and movie information with EmbeddingNN since it uses TabularModel (EmbeddingNN is a TabularModel with n_cont=0 and out_sz=1.)\n\n\n\n1. What problem does colalborative filtering solve?\nIt solves the problem of “filling in the blanks” to predict which items which users (who haven’t bought those items) will buy or rate highly.\n2. How does it solve it?\nIt solves it buy learning different features (latent factors) of users and items and taking the dot product of a user’s latent factors with an item’s latent factors as the prediction.\n3. Why might a collaborative filtering predictive model fail to be a very useful recommendation system?\n4. What does a crosstab representation of collaborative filtering data look like?\nIt looks like a table with rows/columns as users/movies and the cells/values as the rating.\n5. Write the code to create a crosstab representation of the MovieLens data.\n\npd.crosstab(index=ratings['user'], columns=ratings['movie'], values=ratings['rating'], aggfunc='max')\n\n\n\n  \n    \n\n\n  \n    \n      movie\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      1673\n      1674\n      1675\n      1676\n      1677\n      1678\n      1679\n      1680\n      1681\n      1682\n    \n    \n      user\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      5.0\n      3.0\n      4.0\n      3.0\n      3.0\n      5.0\n      4.0\n      1.0\n      5.0\n      3.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      939\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      940\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      5.0\n      3.0\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      941\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      942\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      943\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n943 rows × 1682 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n6. What is a latent factor? Why is it “latent”?\nA latent factor is some characteristic of a variable such as user or item. It is latent because we do not explicitly know or define it. It is learned through training.\n7. What is a dot product? Calculate a dot product manually using pure Python with lists.\nA dot product is the sum of elementwise products between two sequences.\n\na = [0.5, 0.8, 0.6]\nb = [0.3, 0.2, 0.1]\n\ndotproduct = 0\nfor i in range(3):\n  dotproduct += a[i] * b[i]\n\ndotproduct\n\n0.37000000000000005\n\n\n8. What does pandas.DataFrame.merge do?\nIt joins two DataFrames based on a single key column. In our case, we merge ratings with movies to get the title column into ratings.\n9. What is an embedding matrix?\nA matrix where the rows are users or movies, columns are latent factors, and values are decimal values.\n10. What is the relationship between an embedding and a matrix of one-hot-encoded vectors?\nAn embedding is a lookup for which gradients are calculated for an equivalent one-hot-encoded vector matrix multiplication.\n11. Why do we need Embedding if we could use one-hot-encoded vectors for the same thing?\nBecause one-hot-encoded vectors take up a lot of memory.\n12. What does an embedding contain before we start training (assuming we’re not using a pretrained model)?\nRandom numbers.\n13. Create a class (without peeking, if possible!) and use it.\n\nclass Example2():\n  def __init__(self, a): self.a  = a\n  def say(self, text): print(f\"Hello {self.a}, {text}\")\n\n\nExample2('Vishal').say('how are you doing?')\n\nHello Vishal, how are you doing?\n\n\n14. What does x[:,0] mean?\nAll rows of the first column.\n\nx = torch.rand((20, 3))\nx\n\ntensor([[0.9112, 0.0080, 0.1422],\n        [0.5259, 0.5318, 0.4177],\n        [0.7601, 0.4428, 0.7609],\n        [0.1857, 0.6702, 0.5156],\n        [0.7433, 0.9224, 0.0756],\n        [0.1144, 0.9052, 0.4352],\n        [0.2535, 0.6668, 0.5542],\n        [0.1815, 0.1204, 0.7027],\n        [0.6851, 0.2904, 0.1381],\n        [0.4445, 0.2967, 0.3887],\n        [0.0353, 0.8038, 0.7396],\n        [0.3166, 0.4250, 0.4495],\n        [0.8432, 0.9193, 0.0062],\n        [0.9012, 0.0966, 0.8314],\n        [0.7679, 0.5781, 0.4155],\n        [0.4149, 0.3091, 0.3061],\n        [0.3020, 0.6649, 0.8742],\n        [0.6168, 0.4744, 0.0328],\n        [0.9663, 0.3894, 0.5954],\n        [0.0722, 0.1334, 0.2033]])\n\n\n\nx[:,0]\n\ntensor([0.9112, 0.5259, 0.7601, 0.1857, 0.7433, 0.1144, 0.2535, 0.1815, 0.6851,\n        0.4445, 0.0353, 0.3166, 0.8432, 0.9012, 0.7679, 0.4149, 0.3020, 0.6168,\n        0.9663, 0.0722])\n\n\n15. Rewrite the DotProduct class (without peeking, if possible!) and train a model with it.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return (users * movies).sum(dim=1)\n\n\nmodel = DotProduct(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.326553\n      1.332127\n      00:11\n    \n    \n      1\n      1.056348\n      1.104372\n      00:14\n    \n    \n      2\n      0.907263\n      0.994020\n      00:14\n    \n    \n      3\n      0.811368\n      0.898383\n      00:11\n    \n    \n      4\n      0.746635\n      0.878394\n      00:11\n    \n  \n\n\n\n16. What is a good loss function to use for MovieLens? Why?\nMean squared error—because we our predictions are continuous values.\n17. What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\nCurrently the model predicts a single value—a continuous number for the rating.\n\nmodel(torch.tensor([[114, 23]]))\n\ntensor([2.3010], grad_fn=<SumBackward1>)\n\n\nFor Cross-Entropy loss, we need the model to predict 5 probabilities, one for each rating (1, 2, 3, 4, 5). My initial guess for doing this would be to add a linear layer that projects from 1 feature to 5 features.\n\nx,y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\n\nclass DotProduct2(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.linear = nn.Linear(1, 5)\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return self.linear((users * movies).sum(dim=1).unsqueeze(0).permute(1,0))\n\n\nmodel = DotProduct2(n_users, n_movies, n_factors=50)\n\n\nmodel(x).shape\n\ntorch.Size([64, 5])\n\n\nRunning this with CollabDataLoaders won’t work because that sets the ratings as a continuous variable. Instead, I need to create a TabularDataLoaders object where the dependent variable y is a categorical ratings column (with vocab being 0, 1, 2, 3 and 4) and then using my updated linear layer model with CrossEntropyLossFlat.\n\nmodel = DotProduct2(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat())\n\n# this won't work\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n  \n\n\n    \n      \n      0.00% [0/1250 00:00<?]\n    \n    \n\n\nIndexError: Target 5 is out of bounds.\n\n\n\nratings[['user', 'title', 'rating']].head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      196\n      Kolya (1996)\n      3\n    \n    \n      1\n      63\n      Kolya (1996)\n      3\n    \n    \n      2\n      226\n      Kolya (1996)\n      5\n    \n    \n      3\n      154\n      Kolya (1996)\n      3\n    \n    \n      4\n      306\n      Kolya (1996)\n      5\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndls = TabularDataLoaders.from_df(\n    ratings[['user', 'title', 'rating']],\n    procs=[Categorify],\n    cat_names=['user','title'],\n    y_names=['rating'],\n    y_block=CategoryBlock)\n\n\ndls.vocab\n\n[1, 2, 3, 4, 5]\n\n\n\nb = dls.one_batch()\n\n\nb[0].shape, b[1].shape, b[2].shape\n\n(torch.Size([64, 2]), torch.Size([64, 0]), torch.Size([64, 1]))\n\n\nSince the TabularDataLoaders is going to pass categorical and continuous (empty) values to the model, I’ll have to update the model’s forward pass accordingly.\n\nclass DotProduct3(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.linear = nn.Linear(1, 5)\n\n  def forward(self, x_cat, x_cont):\n    x = x_cat\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return self.linear((users * movies).sum(dim=1).unsqueeze(0).permute(1,0))\n\n\nmodel = DotProduct3(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.365067\n      1.474111\n      0.344700\n      00:35\n    \n    \n      1\n      1.181627\n      1.604257\n      0.351150\n      00:15\n    \n    \n      2\n      1.017730\n      1.770345\n      0.358350\n      00:16\n    \n    \n      3\n      0.942282\n      1.899246\n      0.360700\n      00:21\n    \n    \n      4\n      0.902040\n      1.926463\n      0.359450\n      00:25\n    \n  \n\n\n\nIt’s not a great model, but it works! To recap, the three changes I made:\n\nProject the dot product to 5 values using a nn.Linear layer.\nUse TabularDataLoaders instead of CollabDataLoaders.\nUse CrossEntropyLossFlat instead of MSELossFlat.\n\n18. What is the use of bias in the dot product model?\nIf you take away user preferences and movie characteristics, the bias represents how good or bad a movie is. It’s like a baseline rating sans preference. Low bias = bad movie even if it matches your preferences, high bias = good movie regardless of your preference.\n19. What is another name for weight decay?\nL2 regularization.\n20. Write the equation for weight decay (without peeking!)\nloss_with_wd = loss + wd * parameters.sum() ** 2\n21. Write the equation for the gradient of weight decay. Why does it help reduce weights?\nparams.grad += 2 * wd * parameters.sum()\nIncreasing the loss by the weighted sum of parameters causes the parameters to reduce since the model is trying to minimize the loss and therefore minimize the sum of the parameters (weights).\n22. Why does reducing weights lead to better generalization?\nBecause lower weights result in smoother surfaces where the model won’t overfit to data as it does if weights are higher and the surface is full of sharp peaks and valleys.\n23. What does argsort do in PyTorch?\nReturn the indexes of the current values in the tensor in sorted order.\n\nt = torch.tensor([1, 4, 3, 2])\nt.argsort()\n\ntensor([0, 3, 2, 1])\n\n\n24. Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.894813\n      0.955763\n      00:21\n    \n    \n      1\n      0.693337\n      0.897217\n      00:15\n    \n    \n      2\n      0.512773\n      0.869337\n      00:13\n    \n    \n      3\n      0.448445\n      0.854036\n      00:11\n    \n    \n      4\n      0.438284\n      0.849818\n      00:10\n    \n  \n\n\n\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n[\"Schindler's List (1993)\",\n 'Good Will Hunting (1997)',\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Silence of the Lambs, The (1991)']\n\n\n\nratings.groupby('title')['rating'].mean().sort_values(ascending=False)\n\ntitle\nThey Made Me a Criminal (1939)                  5.0\nMarlene Dietrich: Shadow and Light (1996)       5.0\nSaint of Fort Washington, The (1993)            5.0\nSomeone Else's America (1995)                   5.0\nStar Kid (1997)                                 5.0\n                                               ... \nEye of Vichy, The (Oeil de Vichy, L') (1993)    1.0\nKing of New York (1990)                         1.0\nTouki Bouki (Journey of the Hyena) (1973)       1.0\nBloody Child, The (1996)                        1.0\nCrude Oasis, The (1995)                         1.0\nName: rating, Length: 1664, dtype: float64\n\n\nNo, as shown above sorting movie biases doesn’t give the same result as sorting by average movie rating. The reason is that the full rating takes into account different movie and user characteristics, while the bias does not.\n25. How do you print the names and details of the layers in a model?\nBy running a cell with the model list so:\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\n26. What is the “bootstrapping problem” in collaborative filtering?\nDealing with new movies or new users that you don’t have information on.\n27. How could you deal with the bootstrapping problem for new users? For new movies?\nFor new users, you can find the “average user” or use a TabularModel to predict the embeddings for this user. For new movies you can do the same. You can also collect metadata about the movies and users and use that in your model as additional information to train on.\n28. How can feedback loops impact collaborative filtering systems?\nA small number of users who are using and/or rating a lot of products will skew the recommendation system towards their latent factors, recommending products they like to its users and thus attracting more users that like those narrow band of products. The platform will thus become focused on this narrow band of products and users.\n29. When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\nBecause we eventually will concatenate them before passing them through (matrix multiplying by) the first Linear Layer.\n30. Why is there an nn.Sequential in the CollabNN model?\nBecause that is the Neural Net (NN).\n31. What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering system?\nTabular.\n\n\n\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens. Even the type of brackets used in forward has changed?\nFind three other areas where collaborative filtering is being used, and identify the pros and cons of this approach in those areas.\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forums for ideas. Note that there are more columns in the full dataset–see if you can use those too (the next chapter might give you ideas).\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter.\n\n\n\n\n\nI’ll work through the example in Aman Arora’s blog post in which he implements Label Smoothing Cross Entropy Loss.\n\n# logits\nX = torch.tensor([\n    [4.2, -2.4],\n    [1.6, -0.6],\n    [3.6, 1.2],\n    [-0.5, 0.5],\n    [-0.25, 1.7]\n])\n\n# labels\ny = torch.tensor([0,1,1,0,0])\n\nX, y\n\n(tensor([[ 4.2000, -2.4000],\n         [ 1.6000, -0.6000],\n         [ 3.6000,  1.2000],\n         [-0.5000,  0.5000],\n         [-0.2500,  1.7000]]),\n tensor([0, 1, 1, 0, 0]))\n\n\n\nLabelSmoothingCrossEntropy(eps=0.1, reduction='none')(X,y) # matches Excel calculations\n\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\n\n\n\nnoisy_y = tensor([[0.95, 0.05], [0.05, 0.95], [0.05, 0.95], [0.95, 0.05], [0.95, 0.05]])\nnoisy_y\n\ntensor([[0.9500, 0.0500],\n        [0.0500, 0.9500],\n        [0.0500, 0.9500],\n        [0.9500, 0.0500],\n        [0.9500, 0.0500]])\n\n\n\nc = X.size()[-1]\nc # number of classes\n\n2\n\n\n\n# with mean reduction\nlog_preds = F.log_softmax(X, dim=-1)\nloss = reduce_loss(-log_preds.sum(dim=-1), 'mean')\nnll = F.nll_loss(log_preds, y, reduction='mean')\n(1-0.1)*nll + 0.1*(loss/c)\n\ntensor(1.6284)\n\n\n\n# without reduction\nlog_preds = F.log_softmax(X, dim=-1)\nloss = reduce_loss(-log_preds.sum(dim=-1), 'none')\nnll = F.nll_loss(log_preds, y, reduction='none')\n(1-0.1)*nll + 0.1*(loss/c) # matches Excel calculations\n\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\n\n\n\n((1-0.1)*nll + 0.1*(loss/c)).mean() # same as w/ mean reduction\n\ntensor(1.6284)\n\n\n\nloss # this is the negative sum of log_preds for both classes\n\ntensor([6.6027, 2.4102, 2.5737, 1.6265, 2.2160])\n\n\n\n-log_preds.sum(dim=-1) * 0.1 /2 # epsilon weighted negative sum of log_preds for both classes\n\ntensor([0.3301, 0.1205, 0.1287, 0.0813, 0.1108])\n\n\n\nnll * 0.9 # epsilon of negative log loss of target classes\n\ntensor([1.2235e-03, 2.0746e+00, 2.2382e+00, 1.1819e+00, 1.8747e+00])\n\n\n\n0.9 * nll + -log_preds.sum(dim=-1) * 0.1 /2 # matches Excel calculations\n\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\n\n\n\nX\n\ntensor([[ 4.2000, -2.4000],\n        [ 1.6000, -0.6000],\n        [ 3.6000,  1.2000],\n        [-0.5000,  0.5000],\n        [-0.2500,  1.7000]])\n\n\n\n(-torch.log(F.softmax(X, dim=-1)) * noisy_y).sum(dim=-1) # matches Excel calculations\n\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\n\n\n\nnll\n\ntensor([1.3595e-03, 2.3051e+00, 2.4868e+00, 1.3133e+00, 2.0830e+00])\n\n\n\n-torch.log(F.softmax(X, dim=-1)) # notice that nll is the loss values of the target class\n\ntensor([[1.3595e-03, 6.6014e+00],\n        [1.0508e-01, 2.3051e+00],\n        [8.6836e-02, 2.4868e+00],\n        [1.3133e+00, 3.1326e-01],\n        [2.0830e+00, 1.3302e-01]])\n\n\n\n-log_preds # same as -torch.log(F.softmax(X,dim=-1))\n\ntensor([[1.3595e-03, 6.6014e+00],\n        [1.0508e-01, 2.3051e+00],\n        [8.6836e-02, 2.4868e+00],\n        [1.3133e+00, 3.1326e-01],\n        [2.0830e+00, 1.3302e-01]])\n\n\n\n-log_preds.sum(dim=-1) # same as `loss`\n\ntensor([6.6027, 2.4102, 2.5737, 1.6265, 2.2160])\n\n\nSo I think what’s going on here is that nll is just the chosen label’s probabilities whereas loss is the sum of both label’s probs. So multiplying nll by 0.1 and then adding 0.05 * loss results in the ground truth being multiplies by 0.95 (0.9 + 0.05) and not-truth being multiplied by 0.05. I think.\n\n\n\n\nTHE FINAL LESSON OF PART 1!!!!\n\n\nBefore you dig into this, make sure you understand the Linear model and neural net from scratch notebook. I’ll start by walking through that code first:\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\n\npath = Path('titanic')\nif not path.exists():\n    import zipfile,kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 15.2MB/s]\n\n\n\n\n\n\n\n\n\npath = Path('/content/titanic')\n\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)\n\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndf.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n# replacce missing values with mode\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\ndf.fillna(modes, inplace=True)\n\n\ndf.isna().sum().sum()\n\n0\n\n\n\nimport numpy as np\n\ndf.describe(include=(np.number))\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      446.000000\n      0.383838\n      2.308642\n      28.566970\n      0.523008\n      0.381594\n      32.204208\n    \n    \n      std\n      257.353842\n      0.486592\n      0.836071\n      13.199572\n      1.102743\n      0.806057\n      49.693429\n    \n    \n      min\n      1.000000\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      223.500000\n      0.000000\n      2.000000\n      22.000000\n      0.000000\n      0.000000\n      7.910400\n    \n    \n      50%\n      446.000000\n      0.000000\n      3.000000\n      24.000000\n      0.000000\n      0.000000\n      14.454200\n    \n    \n      75%\n      668.500000\n      1.000000\n      3.000000\n      35.000000\n      1.000000\n      0.000000\n      31.000000\n    \n    \n      max\n      891.000000\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf['Fare'].hist();\n\n\n\n\n\n# normalize the values with log\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\ndf['LogFare'].hist();\n\n\n\n\n\n# summary of non-numeric columns\ndf.describe(include=[object])\n\n\n\n  \n    \n\n\n  \n    \n      \n      Name\n      Sex\n      Ticket\n      Cabin\n      Embarked\n    \n  \n  \n    \n      count\n      891\n      891\n      891\n      891\n      891\n    \n    \n      unique\n      891\n      2\n      681\n      147\n      3\n    \n    \n      top\n      Braund, Mr. Owen Harris\n      male\n      347082\n      B96 B98\n      S\n    \n    \n      freq\n      1\n      577\n      7\n      691\n      646\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# create dummy variables for categorical columns with low cardinality\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"], dtype=float)\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      Sex_male\n      Sex_female\n      Pclass_1\n      Pclass_2\n      Pclass_3\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n    \n  \n  \n    \n      0\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n    \n    \n      1\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n    \n    \n      3\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      4\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# create independent and dependent variables as tensors\nfrom torch import tensor\n\nt_dep = tensor(df.Survived)\nt_dep[:10]\n\ntensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1])\n\n\n\ndf[added_cols].dtypes\n\nSex_male      float64\nSex_female    float64\nPclass_1      float64\nPclass_2      float64\nPclass_3      float64\nEmbarked_C    float64\nEmbarked_Q    float64\nEmbarked_S    float64\ndtype: object\n\n\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep.shape\n\ntorch.Size([891, 12])\n\n\n\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\nn_coeff\n\n12\n\n\n\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs.shape\n\ntorch.Size([12])\n\n\n\ncoeffs[:10]\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392])\n\n\n\nOur predictions will be calculated by multiplying each row by the coefficients, and adding them up.\n\n\n# each row has 12 variables, one coefficient per variable\n# the coefficients are broadcasted to each row\n(t_indep*coeffs).shape\n\ntorch.Size([891, 12])\n\n\n\n(t_indep*coeffs)[:2]\n\ntensor([[-10.1838,   0.1386,   0.0000,  -0.4772,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-17.5902,   0.1386,   0.0000,  -0.9681,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000]])\n\n\n\n# normal so age doesn't dominate the values when summing the predictions\nt_indep.max(), t_indep.max(dim=0)\n\n(tensor(80.),\n torch.return_types.max(\n values=tensor([80.0000,  8.0000,  6.0000,  6.2409,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]),\n indices=tensor([630, 159, 678, 258,   0,   1,   1,   9,   0,   1,   5,   0])))\n\n\n\nvals, indices = t_indep.max(dim=0)\n# division by vals broadcasted to each row\nt_indep = t_indep / vals\nt_indep[:2]\n\ntensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000]])\n\n\n\n(t_indep*coeffs)[:2]\n\ntensor([[-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2199,  0.0173,  0.0000, -0.1551, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000]])\n\n\n\n(t_indep*coeffs).sum()\n\ntensor(5.1269)\n\n\n\npreds = (t_indep*coeffs).sum(axis=1)\npreds.shape\n\ntorch.Size([891])\n\n\n\n# look at the predictions\npd.Series(preds).hist();\n\n\n\n\n\n# loss function\ntorch.abs(preds-t_dep).mean()\n\ntensor(0.5382)\n\n\n\n(preds-t_dep)[:5]\n\ntensor([ 0.1927, -1.6239, -0.9021, -0.7944,  0.0968])\n\n\n\n# create helper functions\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\n\n(calc_preds(coeffs, t_indep) == (coeffs*t_indep).sum(axis=1)).sum()\n\ntensor(891)\n\n\n\ncalc_loss(coeffs, t_indep, t_dep) == torch.abs(preds-t_dep).mean()\n\ntensor(True)\n\n\n\n# prepare for gradient descent\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\ntensor(0.5382, grad_fn=<MeanBackward0>)\n\n\n\n# calculate gradients\nloss.backward()\n\n\ncoeffs.grad\n\ntensor([-0.0106,  0.0129, -0.0041, -0.0484,  0.2099, -0.2132, -0.1212, -0.0247,  0.1425, -0.1886, -0.0191,  0.2043])\n\n\n\n# gradients added when calling backward\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad\n\ntensor([-0.0212,  0.0258, -0.0082, -0.0969,  0.4198, -0.4265, -0.2424, -0.0494,  0.2851, -0.3771, -0.0382,  0.4085])\n\n\n\nloss\n\ntensor(0.5382, grad_fn=<MeanBackward0>)\n\n\n\n# do a gradient descent step\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n  coeffs.sub_(coeffs.grad * 0.1) # update the parameters (coefficients)\n  coeffs.grad.zero_() # set gradients of coefficients to 0\n  print(calc_loss(coeffs, t_indep, t_dep)) # yay the loss decreased\n\ntensor(0.4945)\n\n\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\n\nlen(trn_split), len(val_split), trn_split[:5], val_split[:5]\n\n(713, 178, (#5) [788,525,821,253,374], (#5) [303,778,531,385,134])\n\n\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\n\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\n\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\n\ncoeffs = train_model(18, lr=0.2)\n\n0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; \n\n\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()\n\n{'Age': tensor(-0.2694),\n 'SibSp': tensor(0.0901),\n 'Parch': tensor(0.2359),\n 'LogFare': tensor(0.0280),\n 'Sex_male': tensor(-0.3990),\n 'Sex_female': tensor(0.2345),\n 'Pclass_1': tensor(0.7232),\n 'Pclass_2': tensor(0.4112),\n 'Pclass_3': tensor(0.3601),\n 'Embarked_C': tensor(0.0955),\n 'Embarked_Q': tensor(0.2395),\n 'Embarked_S': tensor(0.2122)}\n\n\n\npreds = calc_preds(coeffs, val_indep)\n\n\nresults = val_dep.bool()==(preds>0.5)\nresults[:16]\n\ntensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True,  True, False])\n\n\n\nresults.float().mean()\n\ntensor(0.7865)\n\n\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nacc(coeffs)\n\ntensor(0.7865)\n\n\n\npreds[:28]\n\ntensor([ 0.8160,  0.1295, -0.0148,  0.1831,  0.1520,  0.1350,  0.7279,  0.7754,  0.3222,  0.6740,  0.0753,  0.0389,  0.2216,  0.7631,\n         0.0678,  0.3997,  0.3324,  0.8278,  0.1078,  0.7126,  0.1023,  0.3627,  0.9937,  0.8050,  0.1153,  0.1455,  0.8652,  0.3425])\n\n\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\n\n\n\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\n\ncoeffs = train_model(lr=100)\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\n\nshow_coeffs()\n\n{'Age': tensor(-1.5061),\n 'SibSp': tensor(-1.1575),\n 'Parch': tensor(-0.4267),\n 'LogFare': tensor(0.2543),\n 'Sex_male': tensor(-10.3320),\n 'Sex_female': tensor(8.4185),\n 'Pclass_1': tensor(3.8389),\n 'Pclass_2': tensor(2.1398),\n 'Pclass_3': tensor(-6.2331),\n 'Embarked_C': tensor(1.4771),\n 'Embarked_Q': tensor(2.1168),\n 'Embarked_S': tensor(-4.7958)}\n\n\n\ntst_df = pd.read_csv(path/'test.csv')\ntst_df\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.5\n      0\n      0\n      330911\n      7.8292\n      NaN\n      Q\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.0\n      1\n      0\n      363272\n      7.0000\n      NaN\n      S\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.0\n      0\n      0\n      240276\n      9.6875\n      NaN\n      Q\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.0\n      0\n      0\n      315154\n      8.6625\n      NaN\n      S\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.0\n      1\n      1\n      3101298\n      12.2875\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      413\n      1305\n      3\n      Spector, Mr. Woolf\n      male\n      NaN\n      0\n      0\n      A.5. 3236\n      8.0500\n      NaN\n      S\n    \n    \n      414\n      1306\n      1\n      Oliva y Ocana, Dona. Fermina\n      female\n      39.0\n      0\n      0\n      PC 17758\n      108.9000\n      C105\n      C\n    \n    \n      415\n      1307\n      3\n      Saether, Mr. Simon Sivertsen\n      male\n      38.5\n      0\n      0\n      SOTON/O.Q. 3101262\n      7.2500\n      NaN\n      S\n    \n    \n      416\n      1308\n      3\n      Ware, Mr. Frederick\n      male\n      NaN\n      0\n      0\n      359309\n      8.0500\n      NaN\n      S\n    \n    \n      417\n      1309\n      3\n      Peter, Master. Michael J\n      male\n      NaN\n      1\n      1\n      2668\n      22.3583\n      NaN\n      C\n    \n  \n\n418 rows × 11 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ntst_df['Fare'] = tst_df.Fare.fillna(0)\n\n\ntst_df.fillna(modes, inplace=True)\ntst_df['LogFare'] = np.log(tst_df['Fare']+1)\ntst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"], dtype=float)\n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals\n\n\n# used trained coefficients to predict survival on test set\ntst_df['Survived'] = (calc_preds(tst_indep, coeffs)>0.5).int()\n\n\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n    \n  \n  \n    \n      0\n      892\n      0\n    \n    \n      1\n      893\n      0\n    \n    \n      2\n      894\n      0\n    \n    \n      3\n      895\n      0\n    \n    \n      4\n      896\n      0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      413\n      1305\n      0\n    \n    \n      414\n      1306\n      1\n    \n    \n      415\n      1307\n      0\n    \n    \n      416\n      1308\n      0\n    \n    \n      417\n      1309\n      0\n    \n  \n\n418 rows × 2 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nMultiplying elements together and then adding across rows is identical to doing a matrix-vector product!\n\n\n((val_indep*coeffs).sum(axis=1)).shape\n\ntorch.Size([178])\n\n\n\n(val_indep@coeffs).shape\n\ntorch.Size([178])\n\n\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\n\n# need coeffs to be matrix for matrix products later on\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\n\n# add new dimension\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\n\ntrn_dep.shape\n\ntorch.Size([713, 1])\n\n\n\ncoeffs = train_model(lr=100)\n\n0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\n\nFirst, we’ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like – a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We’ll divide these coefficients by n_hidden so that when we sum them up in the next layer we’ll end up with similar magnitude numbers to what we started with.\n\n\ntorch.rand(1)[0]\n\ntensor(0.6722)\n\n\n\ndef init_coeffs(n_hidden=20):\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()\n\n\nNow we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that’s our non-linearity), and the second is passed to torch.sigmoid as before.\n\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n  l1,l2,const = coeffs # get the two linear layers and bias term\n  res = F.relu(indeps@l1) # matrix product of independent variable values and first linear layer, passed through non-linearity\n  res = res@l2 + const # matrix product of that result and second layer, plus constant\n  return torch.sigmoid(res) # that result passed through sigmoid\n\n\nFinally, now that we have more than one set of coefficients, we need to add a loop to update each one:\n\n\ndef update_coeffs(coeffs, lr):\n  for layer in coeffs:\n    layer.sub_(layer.grad * lr)\n    layer.grad.zero_()\n\n\ncoeffs = train_model(lr=1.4)\n\n0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; \n\n\n\ncoeffs = train_model(lr=20)\n\n0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\n\ntorch.rand(1)\n\ntensor([0.1287])\n\n\n\n# deep learning\ndef init_coeffs():\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1] # inputs, hidden layers, output\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ncoeffs = train_model(lr=4)\n\n0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nContinuing with video notes:\nWe initialized coeffs (coeficients) and a bias term const and updated them by going through each layers and subtracting out the gradient .grad multiplied by the learning rate lr.\nIn PyTorch, we don’t have to keep track of what our coefficients (or parameters, or weights) are, PyTorch does that for us. It does that by looking inside our Module and trying to find anything that looks like a tensor of neural net Parameters and it keeps track of them.\nCreating out own model in PyTorch:\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\n\nclass T(Module):\n  def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n\n(#0) []\n\n\nPyTorch looks inside our Module and keeps track of anything that looks like a tensor of neural network parameters. We can find out what parameters in general PyTorch knows about in our model by instantiating our model and then asking for the parameters.\nThe way you tell PyTorch what your parameter are is by putting them inside a special object called nn.Parameter which hardly does anything—they key thing it does is that when PyTorch checks to see which parameters should it update when it optimizes, it just looks for anything that’s been wrapped in this class.\n\nclass T(Module):\n  def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters()) # by default assumes that we're going to want to require gradient\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\n\nclass T(Module):\n  def __init__(self): self.a = nn.Linear(1, 3, bias=False) # automatically considered a parameter by PyTorch\n\nL(T().parameters())\n\n(#1) [Parameter containing:\ntensor([[-0.5822],\n        [ 0.4630],\n        [-0.4310]], requires_grad=True)]\n\n\n\nt = T()\ntype(t.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\nWe want to create something that works like an Embedding which creates a matrix which will be trained as we train the model, something we can index into (during the forward pass).\nuser_bias will be a vector of parameters, user_factors will be matrix.\nWhen you put a tensor inside nn.Parameter it has all the features a tensor has (for example, we can index into it).\nThe create_params function is all that’s required to recreate PyTorch’s Embedding layer from scratch.\n\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n    self.user_factors = create_params([n_users, n_factors])\n    self.user_bias = create_params([n_users])\n    self.movie_factors = create_params([n_movies, n_factors])\n    self.movie_bias = create_params([n_movies])\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors[x[:, 0]]\n    movies = self.movie_factors[x[:, 1]]\n    res = (users*movies).sum(dim=1)\n    res += self.user_bias[x[:,0]]+self.movie_bias[x[:,1]]\n    return sigmoid_range(res, *self.y_range)\n\nLet’s see if it trains:\n\npath = untar_data(URLs.ML_100k)\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\nratings = ratings.merge(movies)\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\n\n\n\n\n\n\n    \n      \n      100.15% [4931584/4924029 00:00<00:00]\n    \n    \n\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.882364\n      0.953949\n      00:11\n    \n    \n      1\n      0.654987\n      0.886329\n      00:10\n    \n    \n      2\n      0.521378\n      0.869882\n      00:10\n    \n    \n      3\n      0.444553\n      0.858153\n      00:09\n    \n    \n      4\n      0.429629\n      0.853653\n      00:10\n    \n  \n\n\n\n\nmodel.movie_bias # a parameter containing a bunch of numbers that have been trained\n\nParameter containing:\ntensor([-0.0033, -0.1991,  0.0095,  ...,  0.0080,  0.1408,  0.0015],\n       requires_grad=True)\n\n\n\nmodel.movie_bias.shape # 1665 movies\n\ntorch.Size([1665])\n\n\nIn PyTorch, a method that ends in an underscore changes in place the tensor it’s being applied to.\n\ntorch.zeros([4])\n\ntensor([0., 0., 0., 0.])\n\n\n\ntorch.zeros([4]).normal_(0, 0.01)\n\ntensor([-0.0008, -0.0040,  0.0108, -0.0071])\n\n\nWe trained this model—but what did it do? How is it going about predicting who’s going to like what movie?\nWe can find which movies have the highest and lowest movie bias. We can grab the names of those movies from our DataLoaders for each of those 5 lowest or highest numbers.\n\nlearn.model.movie_bias.shape, learn.model.movie_bias.squeeze().shape\n\n(torch.Size([1665]), torch.Size([1665]))\n\n\nThe movies with the lowest movie_bias values are some pretty crappy movies. Why is that? That’s because when it does that matrix product it’s trying to figure out who’s going to like what movie based on previous movies people have enjoyed or not, and then it adds movie bias, which can be positive or negative, that’s a different number for each movie. In order to do a good job at predicting whether you’re going to like a movie or not, it has to know which movies are crap. So that crap movies are going to end up with a very low movie bias parameter. We can find out not only which movies do people really not like, but which movies do people like less than one would expect given the kind of movie that it is?\nSo “Lawnmower Man 2”, not only is it a crappy movie but based on the kind of movie it is (kind of like a high-tech pop kind of sci-fi movie) people who like those kinds of movies still don’t like “Lawnmower Man 2”. In this way we can use a model not just to predict things but to understand things about the data.\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs], movie_bias.sort()[0][:5]\n\n(['Lawnmower Man 2: Beyond Cyberspace (1996)',\n  'Children of the Corn: The Gathering (1996)',\n  'Grease 2 (1982)',\n  'Beverly Hills Ninja (1997)',\n  'Island of Dr. Moreau, The (1996)'],\n tensor([-0.3312, -0.3286, -0.2666, -0.2655, -0.2578], grad_fn=<SliceBackward0>))\n\n\nIf we sort be descending, it’ll give us the exact opposite. Here are movies that people enjoy, even when they don’t enjoy that kind of movie.\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs], movie_bias.sort(descending=True)[0][:5]\n\n(['Shawshank Redemption, The (1994)',\n  'Star Wars (1977)',\n  'L.A. Confidential (1997)',\n  \"Schindler's List (1993)\",\n  'Titanic (1997)'],\n tensor([0.5890, 0.5813, 0.5789, 0.5435, 0.5397], grad_fn=<SliceBackward0>))\n\n\nWe can do the same with users and find out which users just loves movies, even the crappy ones and vice versa.\n\n# users who don't like any movies\nuser_bias = learn.model.user_bias.squeeze()\nidxs = user_bias.argsort()[:5]\n[dls.classes['user'][i] for i in idxs], user_bias.sort()[0][:5]\n\n([181, 405, 724, 774, 445],\n tensor([-0.7536, -0.5753, -0.4381, -0.4175, -0.3963], grad_fn=<SliceBackward0>))\n\n\n\n# users who like all movies\nuser_bias = learn.model.user_bias.squeeze()\nidxs = user_bias.argsort(descending=True)[:5]\n[dls.classes['user'][i] for i in idxs], user_bias.sort(descending=True)[0][:5]\n\n([907, 295, 507, 472, 849],\n tensor([0.7339, 0.6750, 0.6721, 0.6689, 0.6160], grad_fn=<SliceBackward0>))\n\n\nWhat about the latent factors? We can do something called Principal Component Analysis which compresses those 50 columns of latent factors down to (however many you specify).\n\ng = ratings.groupby('title')['rating'].count()\ng\n\ntitle\n'Til There Was You (1997)                  9\n1-900 (1994)                               5\n101 Dalmatians (1996)                    109\n12 Angry Men (1957)                      125\n187 (1997)                                41\n                                        ... \nYoung Guns II (1990)                      44\nYoung Poisoner's Handbook, The (1995)     41\nZeus and Roxanne (1997)                    6\nunknown                                    9\nÁ köldum klaka (Cold Fever) (1994)         1\nName: rating, Length: 1664, dtype: int64\n\n\n\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_movies[:5]\n\narray(['Star Wars (1977)', 'Contact (1997)', 'Fargo (1996)',\n       'Return of the Jedi (1983)', 'Liar Liar (1997)'], dtype=object)\n\n\n\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\ntop_idxs[:5]\n\ntensor([1399,  334,  499, 1235,  861])\n\n\n\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_w.shape\n\ntorch.Size([1000, 50])\n\n\n\nmovie_pca = movie_w.pca(3)\nmovie_pca.shape\n\ntorch.Size([1000, 3])\n\n\n\nfac0, fac1, fac2 = movie_pca.t()\nfac0.shape, fac1.shape, fac2.shape\n\n(torch.Size([1000]), torch.Size([1000]), torch.Size([1000]))\n\n\n\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\n\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\n\nfor i, x, y in zip(top_movies[idxs], X, Y):\n  plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show() # compressed view of the latent factors\n\n\n\n\nfastai provides a collab_learner:\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.907180\n      0.950933\n      00:17\n    \n    \n      1\n      0.657494\n      0.902355\n      00:11\n    \n    \n      2\n      0.501019\n      0.877131\n      00:11\n    \n    \n      3\n      0.444013\n      0.865720\n      00:12\n    \n    \n      4\n      0.409778\n      0.861589\n      00:11\n    \n  \n\n\n\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs], movie_bias.sort(descending=True)[0][:5]\n\n(['Shawshank Redemption, The (1994)',\n  'L.A. Confidential (1997)',\n  'Good Will Hunting (1997)',\n  \"Schindler's List (1993)\",\n  'Star Wars (1977)'],\n tensor([0.6044, 0.5852, 0.5647, 0.5546, 0.5231], grad_fn=<SliceBackward0>))\n\n\nThe fastai model for collaborative filtering (without Neural Network) is pretty much identical to the DotProductBias model we created from scratch. Here’s its forward method:\ndef forward(self, x):\n        users,items = x[:,0],x[:,1]\n        dot = self.u_weight(users)* self.i_weight(items)\n        res = dot.sum(1) + self.u_bias(users).squeeze() + self.i_bias(items).squeeze()\n        if self.y_range is None: return res\n        return torch.sigmoid(res) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\n\n\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) # calculate how far apart each embedding is from the Silence of the Lambs\n# CosineSimilarity is basically the angle between the vectors\ndistances.shape\n\ntorch.Size([1665])\n\n\n\nidx = distances.argsort(descending=True)[1] # the closest movie to Silence of the Lambs\ndls.classes['title'][idx]\n\n'Casablanca (1942)'\n\n\nWe can use Deep Learning instead of dot products.\n\nclass CollabNN(Module):\n  def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.item_factors = Embedding(*item_sz)\n    self.layers = nn.Sequential( # layers of a neural network in order\n        nn.Linear(user_sz[1]+item_sz[1], n_act),\n        nn.ReLU(),\n        nn.Linear(n_act, 1)\n    )\n    self.y_range = y_range\n\n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1)) # concatenate the user and item embeddings with torch.cat\n    return sigmoid_range(x, *self.y_range)\n\nAsk fastai how big our NN embeddings should be (based on a formula that matches Jeremy’s intuition):\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\n\nget_emb_sz??\n\n\nmodel = CollabNN(*embs)\n\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.918632\n      0.965365\n      00:15\n    \n    \n      1\n      0.868667\n      0.928044\n      00:14\n    \n    \n      2\n      0.825990\n      0.911105\n      00:16\n    \n    \n      3\n      0.777693\n      0.879813\n      00:26\n    \n    \n      4\n      0.767727\n      0.872732\n      00:19\n    \n  \n\n\n\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.953472\n      1.010605\n      00:17\n    \n    \n      1\n      0.891541\n      0.936580\n      00:16\n    \n    \n      2\n      0.816217\n      0.904296\n      00:17\n    \n    \n      3\n      0.758014\n      0.884302\n      00:15\n    \n    \n      4\n      0.749216\n      0.878866\n      00:15\n    \n  \n\n\n\nThe dot product version is doing better because it’s taking advantage of our understanding of the problem domain. In practice, companies create a combined model with a dot product component and also has a neural net component. The neural net component is particularly helpful if you have metadata. You can concatenate that in with your embeddings.\nIn collaborative filtering in general there’s an issue where a small number of users and movies overwhelm everybody else. A classic one is anime (small number of viewers who watch it a lot). You have to be careful about these subtlety issues, involves taking various ratios or normalizing things.\nEmbeddings are not just for collaborative filtering. You’ve probably heard about them in the context of Natural Language Processing (NLP). How do we go about using text as inputs to models? You can turn words into integers—taking the unique words from a text and assigning them an id. We then create an embedding matrix for those words. To give this text to a neural net, we list out our words, and for each word we look up the word id (MATCH in Excel) and then find that word’s embeddings using OFFSET. You can then train the embeddings and then interpret them as we’ve done with movie bias factors and the latent factors.\nOur different models, the inputs to them are based on a relatively small number of basic principles. These principles are generally thinks like “look up something in an array.” And then we know inside the model we’re multiplying things, adding them up and replacing the negatives with 0s.\nIn tabular_learner it creates an Embedding for each of the categorical variables (from number of inputs to number of factors based on get_emb_sz). In its forward pass, if there’s embeddings it’ll go through and pass the inputs into them and concatenate the results, and run it through the neural net layers.\nYou can create your neural net, get your trained embeddings and put those embeddings into a random forest or gradient boosted tree and you’re mean average percent error will dramatically improve.\n\n\nWe’ve learned about what goes into the model (categories, embeddings, or continuous numbers). We’ve learned about what comes out the other side (a bunch of activations—a tensor of numbers) which we can use things like softmax to constrain them to add up to 1. We’ve looked at what can go in the middle which is the matrix multiplication sandwiched together with rectified linear units. There are other things that can go in the middle, which is convolutions (another kind of matrix multiplication).\nConvolutional Neural Networks are similar to what we’ve seen so far (inputs, things that are a form of matrix multiplication, sandwiched with activation functions). But there’s a particular thing that makes them very useful for computer vision.\nBack in the mid-90s, Yann LeCun showed really practically useful performance on this dataset which resulted in convnets being used in the american banking system for reading checks.\nIn the Excel file, Jeremy has recreated a 28x28 cell “7” from the MNIST dataset and is multiplying each 3x3 cells with the following filter:\n\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n-1\n-1\n-1\n\n\n\nand taking the max of that dot product and 0. It’s like ReLU but it’s not doing a matrix product, it’s doing a dot product just on those 9 cells (3x3) and just those 9 weights (the 3x3 “filter”). When you move one to the right, it’s using the next 9 cells, and so on.\nA convolution is when you slide a little 3x3 matrix across a bigger matrix and at each location you do a dot product of that 3x3 matrix with the 3x3 matrix of coefficients. Why does that create something that finds something like top edges? It’s because of the way we’ve constructed the coefficient matrix.\nAll of the rows just above are going to get a 1. All of the ones just below are going to get a -1. And all of the ones in the middle are going to get a 0. When the image’s 3x3 is:\n\n\n\n1\n1\n1\n\n\n1\n1\n1\n\n\n1\n1\n1\n\n\n\nMultiplying it by the filter:\n\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n-1\n-1\n-1\n\n\n\nGives us 0. But when the image’s 3x3 is something like:\n\n\n\n1\n1\n1\n\n\n0.8\n0.8\n0.8\n\n\n0\n0\n0\n\n\n\nMultiplying it by the filter gives us 3. We’ll only get such 3s when the image’s 3x3 has the top row as dark as possible (1) and the bottom row blank (0). That’s only going to happen at a horizontal edge.\nA horizontal edge detector is the filter of coefficients:\n\n\n\n1\n0\n-1\n\n\n1\n0\n-1\n\n\n1\n0\n-1\n\n\n\nThe dot product will only be 3 where the 3x3’s leftmost column is 1’s and the rightmost is 0’s.\nYou can think of a convolution as being a sliding window, of little mini dot products of these little 3x3 matrices. They don’t have to be 3x3 we could have just as easily done 5x5 then we’d have a 5x5 matrix of coefficients. Whatever size you like. The size is called the kernel size. A 3x3 kernel for this convolution.\nWe repeat these steps again and again. In the second layer we now have two channels. In the first layer we just had one (the grayscale original image). The two channels are the horizontal edges channel and the vertical edges channel. Our filter is now 3x3x2 or two 3x3 kernels or one 3x3x2 kernel. It combines the horizontal and the vertical edge detectors.\nWe’ll eventually end up with a single set of 10 activations (one for each digit 0-9) or 1 activation (7 or not-7). We’d back propogate through these calculations using SGD. And that is going to end up optimizing the coefficients in the filters. In real life you start with random numbers and then optimize them with SGD (instead of the manual edge detectors Jeremy instantiated).\nA few years what we used was max pooling—which is like a convolution except you don’t take a dot product you take the max of a sliding window (in our case, a 2x2 max pooling). With a 2x2 max pooling, we end up losing half of our activations on each dimension, so we’re going to end up with only 1/4th the activations that we started out with. And that’s a good thing because if we keep doing conv layers and max pools, will have fewer and fewer activations, then we take a dot product of those with a bunch of coefficients (dense layer) for each channel and then add them all up for our final big dot product. MNIST would have 10 such final activations, with a softmax layer after that.\nNowadays we normally don’t have max pool layers. But instead when we do our sliding window, we skip one every time we move to the next 3x3 (after doing column I we skip column J and go straight to K). That’s called a “stride 2” convolution. So every time we do a convolution we reduce our effective feature size by 2 on each axis (reducing by 4x in total), instead of doing max pooling.\nThe other thing is nowadays we don’t have a single dense layer but instead we keep doing stride-2 convolutions until we’ve got about a 7x7 grid, and then we do a single pooling at the end (average instead of max). So we average the activations of each one of the 7x7 features. This is important know to because something like an imagenet style image detector is going to end up with a 7x7 grid for “is this a bear?” and for each of the 7x7 squares it’s seeing if there is a bear in that part of the photo, and it takes the average of those 49 predictions to decide whether there’s a bear in the photo. That works very well if it’s basically a photo of a bear. If the bear is big and takes up most of the frame, then most of the 7x7 bits are bits of the bear. On the other hand, if there’s a teeny tiny bear in the corner, then potentially only one of those 49 squares has a bear in it. Even worse, if it’s a picture with lots of different things only one of which is a bear—it could end up being not a good bear detector. The details of how we construct our model turn out to be important. If you’re trying to find one part of the photo that has a bear in it, you might want to try max pooling (“i think this is a picture of a bear if any one of the 49 bits has a bear in it”). The max/average pool is happening right at the very end. fastai does max pool and average pool and concatenate them together (concat pooling) and that has since been reinvented in at least one paper.\nConvolution is the same thing as a matrix multiplication. Here is convolution as a sliding window—\nthe kernel\n\n\n\n\\(\\alpha\\)\n\\(\\beta\\)\n\n\n\\(\\gamma\\)\n\\(\\delta\\)\n\n\n\nand a 3x3 image:\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\nG\nH\nJ\n\n\n\nand the resulting convolution\n\n\n\nP\nQ\n\n\nR\nS\n\n\n\nI’ll show the first sliding window multiplication (in italics/bold):\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\nG\nH\nJ\n\n\n\nWhich is:\n\\(\\alpha A+\\beta B+ \\gamma D + \\delta E + b = P\\)\nand the rest of the sliding windows:\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\nG\nH\nJ\n\n\n\n\\(\\alpha B+\\beta C+ \\gamma E + \\delta F + b = Q\\)\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\nG\nH\nJ\n\n\n\n\\(\\alpha D+\\beta E+ \\gamma G + \\delta H + b = R\\)\n\n\n\nA\nB\nC\n\n\nD\nE\nF\n\n\nG\nH\nJ\n\n\n\n\\(\\alpha E+\\beta F+ \\gamma H + \\delta J + b = S\\)\nWe can also write it as a matrix multiplication. This matrix of kernel or filter values:\n\n\n\n\\(\\alpha\\)\n\\(\\beta\\)\n0\n\\(\\gamma\\)\n\\(\\delta\\)\n0\n0\n0\n0\n\n\n0\n\\(\\alpha\\)\n\\(\\beta\\)\n0\n\\(\\gamma\\)\n\\(\\delta\\)\n0\n0\n0\n\n\n0\n0\n\\(\\alpha\\)\n\\(\\beta\\)\n0\n\\(\\gamma\\)\n\\(\\delta\\)\n0\n0\n\n\n0\n0\n0\n\\(\\alpha\\)\n\\(\\beta\\)\n0\n\\(\\gamma\\)\n\\(\\delta\\)\n0\n\n\n0\n0\n0\n0\n\\(\\alpha\\)\n\\(\\beta\\)\n0\n\\(\\gamma\\)\n\\(\\delta\\)\n\n\n\nMultiplied by a column of pixels:\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\nD\n\n\n\nE\n\n\n\nF\n\n\n\nG\n\n\n\nH\n\n\n\nJ\n\n\n\n\nplus a column of biases:\n\n\n\nb\n\n\n\nb\n\n\n\nb\n\n\n\nb\n\n\n\n\nyields the convolution:\n\n\n\nP\n\n\n\nQ\n\n\n\nR\n\n\n\nS\n\n\n\n\nIn practice it’s faster to do it as a sliding window but this matrix multiplication is a good way to think about it as a special type of matrix multiplication.\n\n\n\nSame convolutions as before, followed by a bunch of random numbers. We define a dropout factors (from 0.0 to 0.9) which we use to create a dropout mask (if the random number in a given cell/pixel is greater than the dropout factor, use that random number otherwise set it to 0). We start with the image and then corrupt it (random bits of it have been deleted). Higher dropout factor will delete more of the picture. That “corrupted” image is the input to the next layer (which is max pool in our example).\nWhy would we delete some data at random from our processed image/activations after convolutions? The reason is that a human is able to look at the corrupted image and still recognize it’s a 7. A computer should be able to as well. If we randomly delete different bits of the activations each time, then the computer is forced to learn the underlying real representation rather than overfitting. Think of this as data augmentations for the activations. This is called a Dropout layer, which is really helpful for avoiding overfitting. The more dropout you use, the less good it will be on the training data but the better it ought to generalize.\nA different set of activations will be deleted each batch. Dropout was initially rejected by NIPS, disseminated by arxiv. Peer-review is a very fallible thing in both directions.\n\n\n\nWe’ve seen quite a few ways of dealing with inputs to neural networks, things that can happen in the middle of the NN. We’ve talked about Rectified Linear Units (ReLU) (0 if x is less than 0 or x otherwise), there are other activations you can use (except for Identity, with which you end up with a linear model)—these don’t matter very much, any non-linearity works fine–inputs can be one-hot encoded (or embeddings which is a computational shortcut), there are sandwiched layers of matrix multipliers and activation functions, matrix multipliers can be special cases like convolutions or embeddings, the output can go through some tweaking such as Softmax, and you’ve got the loss function such as cross-entropy loss or mean squared error or mean absolute error.\n\n\n\nRead Radek’s book “Meta Learning”. One of the fastai alums went on to create the Mish activation function now used in many SOTA models around the world and is now at Mila, one of the top research labs of the world.\nHow do you stay motivated? You don’t have to know everything—nobody know’s everything and that’s okay. Take an interest in some area and follow that and do the best job of keeping up with some little sub area. If you’re sub area is too much to keep up on, pick a sub-sub area. From time to time, take a dip into other areas you’re not following as closely. Things are not changing that fast at all. Fundamentally the stuff that is in the course now is not that different to what was in the course five years ago. The foundations haven’t changed. It’s not that different to the convolutional neural network that Yann LeCun used on MNIST back in 1996. The basic ideas are forever. Everything else is tweaks. The more you learn about the basic ideas, the more you’ll recognize those tweaks as simple little tricks that you’ll quickly be able to get your head around.\nThe key thing to creating a legitimate business venture is to solve a legitimate problem. A problem that people need solving and will pay you to solve. It’s important not to start with your fun gradio prototype as the basis for your business, but instead start with: here’s a problem that I want to solve. Pick a problem that you understand better than most people. Eric Reis wrote “The Lean Startup” who recommends that what you do next is you fake it. You create a Minimum Viable Product—something that solves the problem that takes as little time to create. It could be very manual, it could be loss making, that’s fine. The bit in the middle where there’s going to be a neural net—you launch without it and do everything by hand. You’re just trying to find out: “are people going to pay for this? is this actually useful?” Once you have confirmed that the need is real and that people will pay for it and you can solve the need you can gradually make it less fake and more and more getting the product to where you want it to be.\nProductivity hacks: not to work too much. Jeremy spends less hours a day working than most people. Jeremy has spent half of every working day since 18 learning or practicing something new. Doing it more slowly than if he used something that he already knew. In the other 50% of the time he’s constantly building up, exponentially, this base of expertise in a wide range of areas so he can do things multiples or orders of magnitudes faster than people around him. Try not overdo things, get good sleep, eat well and exercise well. It’s also a case of tenacity—Jeremy has noticed a lot of people give up much earlier than he does. If you just keep going until something’s actually finished (nicely), then that’s going to put you in a small minority. Most people don’t do that. Jeremy makes things like nbdev that make it easier to finish something nicely. Make the things that you want to do easier so that you’ll do them more.\n\n\n\n\nfeature engineering: creating new transformations of the input data in order to make it easier to model.\nIn the context of an image, a feature is a visually distinctive attribute.\nFinding the edges in an image is a very common task in computer vision and to do it we use something called a convolution which requires nothing more than multiplication and addition. Let’s do this with code:\n\nfrom fastai.vision.all import *\nmatplotlib.rc('image', cmap='Greys')\n\n\ntop_edge = tensor([[-1, -1, -1],\n                   [ 0,  0,  0],\n                   [ 1,  1,  1]])\n\ntop_edge # this is our kernel\n\ntensor([[-1, -1, -1],\n        [ 0,  0,  0],\n        [ 1,  1,  1]])\n\n\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:00<00:00]\n    \n    \n\n\n\nim3 = Image.open(path/'train'/'3'/'12.png')\nshow_image(im3);\n\n\n\n\nMultiply the top 3x3-pixel square of our image and multiply each of those values by each item in our kernel, then add them up.\n\nim3_t = tensor(im3)\nim3_t[0:3, 0:3] * top_edge\n\ntensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n\n\n\n(im3_t[0:3, 0:3] * top_edge).sum()\n\ntensor(0)\n\n\n\n# more interesting results\ndf = pd.DataFrame(im3_t[:10, :20])\ndf.style.set_properties(**{'font-size': '6pt'}).background_gradient('Greys')\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      5\n      0\n      0\n      0\n      12\n      99\n      91\n      142\n      155\n      246\n      182\n      155\n      155\n      155\n      155\n      131\n      52\n      0\n      0\n      0\n      0\n    \n    \n      6\n      0\n      0\n      0\n      138\n      254\n      254\n      254\n      254\n      254\n      254\n      254\n      254\n      254\n      254\n      254\n      252\n      210\n      122\n      33\n      0\n    \n    \n      7\n      0\n      0\n      0\n      220\n      254\n      254\n      254\n      235\n      189\n      189\n      189\n      189\n      150\n      189\n      205\n      254\n      254\n      254\n      75\n      0\n    \n    \n      8\n      0\n      0\n      0\n      35\n      74\n      35\n      35\n      25\n      0\n      0\n      0\n      0\n      0\n      0\n      13\n      224\n      254\n      254\n      153\n      0\n    \n    \n      9\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      90\n      254\n      254\n      247\n      53\n      0\n    \n  \n\n\n\n\nim3_t[4:7, 6:9] # top edge\n\ntensor([[  0,   0,   0],\n        [142, 155, 246],\n        [254, 254, 254]], dtype=torch.uint8)\n\n\n\nim3_t[4:7, 6:9] * top_edge\n\ntensor([[  0,   0,   0],\n        [  0,   0,   0],\n        [254, 254, 254]])\n\n\n\n(im3_t[4:7, 6:9] * top_edge).sum()\n\ntensor(762)\n\n\n\nim3_t[7:10, 17:20] # right edge\n\ntensor([[254,  75,   0],\n        [254, 153,   0],\n        [247,  53,   0]], dtype=torch.uint8)\n\n\n\nim3_t[7:10, 17:20] * top_edge\n\ntensor([[-254,  -75,    0],\n        [   0,    0,    0],\n        [ 247,   53,    0]])\n\n\n\n(im3_t[7:10, 17:20] * top_edge).sum()\n\ntensor(-29)\n\n\nThis calculation is returning a high number where the 3x3-pixel square represents a top edge (where there are low values at the top of the square and high values immediately underneath)—in that case the -1 values in our kernel have little impact.\nLooking at the math, any window of size 3x3 in our image:\n\n\n\na1\na2\na3\n\n\na4\na5\na6\n\n\na7\na8\na9\n\n\n\nMultiplying by a kernel:\n\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n-1\n-1\n-1\n\n\n\nWill return:\na1 + a2 + a3 - a7 - a8 - a9.\nIf a1 = a7, a2 = a8, and a3 = a9, we’ll get 0. If a1 > a7, a2 > a8 and a3 > a9 we’ll get a positive number. This filter detects horizontal edges.\nThe kernel:\n\n\n\n-1\n-1\n-1\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n\ndetects horizontal edges where we go from light to dark.\nThe kernel:\n\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n-1\n-1\n-1\n\n\n\ndetects horizontal edges where we go from dark to light.\nThe kernel:\n\n\n\n1\n0\n-1\n\n\n1\n0\n-1\n\n\n1\n0\n-1\n\n\n\ndetects vertical edges where we go from dark (left) to light (right).\nThe kernel:\n\n\n\n-1\n0\n1\n\n\n-1\n0\n1\n\n\n-1\n0\n1\n\n\n\ndetects vertical edges where we go from light (left) to dark (right).\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom torch import tensor\n\n# Create the tensor\ndata = tensor([[254, 0, 0],\n               [254, 0, 0],\n               [254, 0, 0]])\n\n# Convert the tensor to a Pandas DataFrame\ndf = pd.DataFrame(data.numpy())\n\n# Plot the heatmap\nplt.figure(figsize=(3, 3))\nsns.heatmap(df, annot=True, cmap='Greys', cbar=False, linewidths=.5, fmt='d')\nplt.show()\n\n\n\n\n\n# vertical edge detector (light to dark - left to right)\nk = tensor([[-1, 0, 1],\n            [-1, 0, 1],\n            [-1, 0, 1]])\n\ndata = tensor([[254, 0, 0],\n               [254, 0, 0],\n               [254, 0, 0]])\n\nres = data * k\n# Convert the tensor to a Pandas DataFrame\ndf = pd.DataFrame(res.numpy())\n\n# Plot the heatmap\nplt.figure(figsize=(3, 3))\nsns.heatmap(df, annot=True, cmap='Greys', cbar=False, linewidths=.5, fmt='d')\nplt.show()\n\n\n\n\n\n# vertical edge detector (dark to light - left to right)\nk = tensor([[1, 0, -1],\n            [1, 0, -1],\n            [1, 0, -1]])\n\ndata = tensor([[254, 0, 0],\n               [254, 0, 0],\n               [254, 0, 0]])\n\nres = data * k\n# Convert the tensor to a Pandas DataFrame\ndf = pd.DataFrame(res.numpy())\n\n# Plot the heatmap\nplt.figure(figsize=(3, 3))\nsns.heatmap(df, annot=True, cmap='Greys', cbar=False, linewidths=.5, fmt='d')\nplt.show()\n\n\n\n\nLet’s create a function to do this for one location, and check that it matches our result from before:\n\ndef apply_kernel(row, col, kernel):\n  return (im3_t[row-1:row+2, col-1:col+2] * kernel).sum()\n\n\napply_kernel(5, 7, top_edge)\n\ntensor(762)\n\n\n\nl = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nl[5-1:5+2], l[7-1: 7+2]\n\n([4, 5, 6], [6, 7, 8])\n\n\n\n254*3\n\n762\n\n\nNote that we can’t apply this to a corner since there isn’t a complete 3x3 square there.\n\n\nWe can map apply_kernel across the coordinate grid—taking our 3x3 kernel and applying it to each 3x3 section of our image.\nTo get a grid of coordinates, use a nested list comprehension:\n\n[[(i, j) for j in range(1,5) for i in range(1,5)]]\n\n[[(1, 1),\n  (2, 1),\n  (3, 1),\n  (4, 1),\n  (1, 2),\n  (2, 2),\n  (3, 2),\n  (4, 2),\n  (1, 3),\n  (2, 3),\n  (3, 3),\n  (4, 3),\n  (1, 4),\n  (2, 4),\n  (3, 4),\n  (4, 4)]]\n\n\n\n[[(i, j) for i in range(1,5) for j in range(1,5)]]\n\n[[(1, 1),\n  (1, 2),\n  (1, 3),\n  (1, 4),\n  (2, 1),\n  (2, 2),\n  (2, 3),\n  (2, 4),\n  (3, 1),\n  (3, 2),\n  (3, 3),\n  (3, 4),\n  (4, 1),\n  (4, 2),\n  (4, 3),\n  (4, 4)]]\n\n\n\n[[(i, j) for i in ['inner1', 'inner2', 'inner3'] for j in ['outer1', 'outer2', 'outer3']]]\n\n[[('inner1', 'outer1'),\n  ('inner1', 'outer2'),\n  ('inner1', 'outer3'),\n  ('inner2', 'outer1'),\n  ('inner2', 'outer2'),\n  ('inner2', 'outer3'),\n  ('inner3', 'outer1'),\n  ('inner3', 'outer2'),\n  ('inner3', 'outer3')]]\n\n\n\n# applying kernel over coordinate grid\nrng = range(1,27)\ntop_edge3 = tensor([[apply_kernel(i, j, top_edge) for j in rng] for i in rng])\nshow_image(top_edge3);\n\n\n\n\n\ntensor([[apply_kernel(i, j, top_edge) for j in rng] for i in rng]).shape, \\\ntensor([[apply_kernel(i, j, top_edge) for j in rng for i in rng]]).shape, \\\ntensor([[apply_kernel(i, j, top_edge) for i in rng] for j in rng]).shape, \\\ntensor([[apply_kernel(i, j, top_edge)] for j in rng for i in rng]).shape\n\n(torch.Size([26, 26]),\n torch.Size([1, 676]),\n torch.Size([26, 26]),\n torch.Size([676, 1]))\n\n\n\n# left edge\nleft_edge = tensor([[-1, 1, 0],\n                    [-1, 1, 0],\n                    [-1, 1, 0]]).float()\n\nleft_edge3 = tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\nshow_image(left_edge3);\n\n\n\n\n\n# right edge\nright_edge = tensor([[0, 1, -1],\n                     [0, 1, -1],\n                     [0, 1, -1]]).float()\n\nright_edge3 = tensor([[apply_kernel(i, j, right_edge) for j in rng] for i in rng])\nshow_image(right_edge3);\n\n\n\n\n\ntop_edge\n\ntensor([[-1, -1, -1],\n        [ 0,  0,  0],\n        [ 1,  1,  1]])\n\n\n\n# bottom edge\nbottom_edge = tensor([[1, 1, 1],\n                      [0, 0, 0],\n                      [-1, -1, -1]]).float()\n\nbottom_edge3 = tensor([[apply_kernel(i, j, bottom_edge) for j in rng] for i in rng])\n\nshow_image(bottom_edge3);\n\n\n\n\n\n# top right diagonal\ntop_right_diagonal = tensor([[1, -1, -1],\n                            [0,  1, -1],\n                            [0,  0,  1]]).float()\n\ntop_right_diagonal3 = tensor([[apply_kernel(i, j, top_right_diagonal) for j in rng] for i in rng])\n\nshow_image(top_right_diagonal3);\n\n\n\n\n\n# top left diagonal\ntop_left_diagonal = tensor([[-1, -1,  0],\n                             [-1,  0,  1],\n                             [ 0,  1,  1]]).float()\n\ntop_left_diagonal3 = tensor([[apply_kernel(i, j, top_left_diagonal) for j in rng] for i in rng])\n\nshow_image(top_left_diagonal3);\n\n\n\n\n\n# bottom right diagonal\nbottom_right_diagonal = tensor([[1,  1,  0],\n                                [1,  0, -1],\n                                [0, -1, -1]]).float()\n\nbottom_right_diagonal3 = tensor([[apply_kernel(i, j, bottom_right_diagonal) for j in rng] for i in rng])\n\nshow_image(bottom_right_diagonal3);\n\n\n\n\n\n# bottom left diagonal\nbottom_left_diagonal = tensor([[ 0,  1, 1],\n                               [-1,  0, 1],\n                               [-1, -1, 0]]).float()\n\nbottom_left_diagonal3 = tensor([[apply_kernel(i, j, bottom_left_diagonal) for j in rng] for i in rng])\n\nshow_image(bottom_left_diagonal3);\n\n\n\n\nA image with height h and width w will have h-2 by w-2 3x3 windows. In our case we have 28x28 image, and 26x26 resulting convolutions.\n\n\n\nPyTorch wants a rank-4 tensor as input (minibatch, in_channels, iH, iW) and weight (out_channels, in_channels, kH, kW) so that it can apply a convolution to multiple images at the same time (every item in a batch at once) and apply multiple kernels at the same time.\n\ndiag1_edge = tensor([[ 0, -1, 1],\n                     [-1,  1, 0],\n                     [ 1,  0, 0]]).float()\n\ndiag1_edge3 = tensor([[apply_kernel(i, j, diag1_edge) for j in rng] for i in rng])\n\nshow_image(diag1_edge3);\n\n\n\n\n\ndiag2_edge = tensor([[1, -1, 0],\n                     [0, 1, -1],\n                     [0, 0, 1]])\n\ndiag2_edge3 = tensor([[apply_kernel(i, j, diag2_edge) for j in rng] for i in rng])\n\nshow_image(diag2_edge3);\n\n\n\n\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])\nedge_kernels.shape\n\ntorch.Size([4, 3, 3])\n\n\n\nmnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock),\n                  get_items=get_image_files,\n                  splitter=GrandparentSplitter(),\n                  get_y=parent_label)\n\n\ndls = mnist.dataloaders(path)\nxb, yb = first(dls.valid)\nxb.shape\n\ntorch.Size([64, 1, 28, 28])\n\n\n\n# by default fastai puts batches onto GPU when using DataBlocks\nxb, yb = to_cpu(xb), to_cpu(yb)\n\nA channel is a single basic color in an image. PyTorch represents an image as a rank-3 tensor with these dimensions:\n[channels, rows, columns]\nKernels passed to F.conv2d need to be rank-4 tensors:\n[channels_in, features_out, rows, columns]\nedge_kernels is missing a dimension currently. We need to tell PyTorch that the number of input channels in the kernel is 1 which we can do by inserting an axis of size 1 (called a unit axis) in the first location.\n\nedge_kernels.shape, edge_kernels.unsqueeze(1).shape\n\n(torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3]))\n\n\n\nedge_kernels = edge_kernels.unsqueeze(1)\n\n\nbatch_features = F.conv2d(xb, edge_kernels)\n\n\nbatch_features.shape\n\ntorch.Size([64, 4, 26, 26])\n\n\n\nshow_image(batch_features[0,0]); # left edge\n\n\n\n\n\nshow_image(batch_features[0,1]); # top edge\n\n\n\n\n\nshow_image(batch_features[0,2]); # diag1\n\n\n\n\n\nshow_image(batch_features[0,3]); # diag2\n\n\n\n\nTo become a strong deep learning practitioner, one skill to practice is giving your GPU plenty of work to do at a time. Our manual convolution loop would be millions of times slower.\nTo avoid losing 2 pixels on each axis, we add padding (commonly zeroes).\n\n\n\nIf we add a kernel of size ks by ks (where ks is an odd number) the necessary padding on each side to keep the same shape is ks//2. An even number of ks would require a different amount of padding on the top/bottom and left/right but in practice we almost never use an even filter size.\nstride-2: move over two pixels after each kernel application, useful for decreasing the size of our outputs.\nstride-1 convolutions are useful for adding layers without changing the output size.\nThe most common kernel size in practice is 3x3 and the most common padding is 1.\nThe general formula for output size given input image dimension n, padding pad, stride and kernel size ks:\n(n + 2*pad - ks) // stride + 1\nSo for a 5x5 image with a 3x3 kernel, stride-2 and 1 pixel of padding:\n(5 + 2 * 1 - 3) // 2 + 1 = 4/2 + 1 = 3\nWhen looking at convolution as a matrix multiplication, it has two properties:\n\nThe zeros in the matrix are untrainable. They stay 0 throughout the optimization process.\nSome of the weights are equal and while they are trainable (i.e. changeable), they must remain equal. These are called shared weights.\n\n\n\n\nThere is no reason to believe that some particular edge filters are the most useful kernels for image recognition. We don’t have a good idea for how to manually construct lower layer filters (of which later layer convolution kernels become complex transformations). Have the model learn the values of the kernels. When we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network (CNN).\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n\nsimple_net\n\nSequential(\n  (0): Linear(in_features=784, out_features=30, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=30, out_features=1, bias=True)\n)\n\n\nUse convolutional layers instead of linear.\n\nbroken_cnn = sequential(\n    nn.Conv2d(1, 30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30, 1, kernel_size=3, padding=1)\n)\n\nWe don’t need to specify 28*28 as the input size because the convolution is applied over each pixel automatically. The weights depend only on the number of input and output channels and the kernel size.\n\nxb.shape\n\ntorch.Size([64, 1, 28, 28])\n\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([64, 1, 28, 28])\n\n\n\nshow_image(broken_cnn(xb)[0,0]);\n\n\n\n\n\nnn.Conv2d(1, 30, kernel_size=3, padding=1)(xb).shape\n\ntorch.Size([64, 30, 28, 28])\n\n\n\nshow_image(nn.Conv2d(1, 30, kernel_size=3, padding=1)(xb)[0,29]);\n\n\n\n\nWe can perform enough stride-2 convolutions to get this down to a single value for classification. 28x28 -> 14x14 -> 7x7 -> 4x4 -> 2x2 -> 1x1.\n\ndef conv(ni, nf, ks=3, act=True):\n  res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n  if act: res = nn.Sequential(res, nn.ReLU())\n  return res\n\nWhen using stride-2, increase the number of features at the same time because we are decreasing the number of activations by 4 (we don’t want to decrease the capacity of a layer by too much at a time).\n\nsimple_cnn = sequential(\n    conv(1, 4),            # 14 x 14\n    conv(4, 8),            # 7x7\n    conv(8, 16),           # 4x4\n    conv(16,32),           # 2x2\n    conv(32,2,act=False),  #1x1\n    Flatten()\n)\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([64, 2])\n\n\n\nconv(1,4)(xb).shape\n\ntorch.Size([64, 4, 14, 14])\n\n\n\nconv(4, 8)(\n    conv(1,4)(xb)\n).shape\n\ntorch.Size([64, 8, 7, 7])\n\n\n\nconv(8, 16)(\n    conv(4, 8)(\n        conv(1,4)(xb))).shape\n\ntorch.Size([64, 16, 4, 4])\n\n\n\nconv(16,32)(\n    conv(8, 16)(\n        conv(4, 8)(\n            conv(1,4)(xb)))).shape\n\ntorch.Size([64, 32, 2, 2])\n\n\n\nconv(32, 2, act=False)(\n    conv(16,32)(\n        conv(8, 16)(\n            conv(4, 8)(\n                conv(1,4)(xb))))).shape\n\ntorch.Size([64, 2, 1, 1])\n\n\n\nFlatten()(\n    conv(32, 2, act=False)(\n        conv(16,32)(\n          conv(8, 16)(\n            conv(4, 8)(\n                conv(1,4)(xb)))))\n).shape\n\ntorch.Size([64, 2])\n\n\n\n# create our Learner\nlearn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)\n\n\nlearn.summary()\n\n\n\n\n\n\n\n\nSequential (Input shape: 64 x 1 x 28 x 28)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 4 x 14 x 14    \nConv2d                                    40         True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 8 x 7 x 7      \nConv2d                                    296        True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 16 x 4 x 4     \nConv2d                                    1168       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 32 x 2 x 2     \nConv2d                                    4640       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 2 x 1 x 1      \nConv2d                                    578        True      \n____________________________________________________________________________\n                     64 x 2              \nFlatten                                                        \n____________________________________________________________________________\n\nTotal params: 6,722\nTotal trainable params: 6,722\nTotal non-trainable params: 0\n\nOptimizer used: <function Adam at 0x790bb9f92830>\nLoss function: <function cross_entropy at 0x790c7f74e950>\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n\n\nFlatten is like PyTorch’s squeeze but as a Module.\nLet’s train! Since this is a deeper network than we’ve built from scratch before we’ll use a lower learning rate and more epochs:\n\nlearn.fit_one_cycle(2, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.058796\n      0.036284\n      0.988714\n      00:28\n    \n    \n      1\n      0.022334\n      0.025466\n      0.991168\n      00:22\n    \n  \n\n\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return F.conv2d(input, weight, bias, self.stride,\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n\n\nInput size is 64x1x28x28 which is batch, channel, height, width.\nFirst layer of the model:\n\nm = learn.model[0]\n\n\nm # 1 input channel, 4 output channels, 3x3 kernel\n\nSequential(\n  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n\n\n\n# layer 1 weights\nm[0].weight.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\n4 x 1 x 3 x 3 = 36 weights, but learn.summary says this layer has 40 params. What are the other 4? Bias!\n\nm[0].bias.shape # one bias for each channel\n\ntorch.Size([4])\n\n\n\nlearn.model[1]\n\nSequential(\n  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n\n\n\nlearn.model[1][0].weight.shape\n\ntorch.Size([8, 4, 3, 3])\n\n\n\n8*4*3*3\n\n288\n\n\n288 params + 8 bias values = 296 params.\nIgnoring bias, this layer has 14 x 14 = 196 locations multiplied by 288 parameters resulting in 56_448 multiplications.\nThe next layer:\n\nlearn.model[2]\n\nSequential(\n  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n\n\n\nlearn.model[2][0].weight.shape\n\ntorch.Size([16, 8, 3, 3])\n\n\n\n16*8*3*3\n\n1152\n\n\nWill have 7 x 7 x 1152 = 56_448 multiplications. We halved the grid size from 14x14 to 7x7 (using stride-2) and doubled the number of filters from 8 to 16.\n\n7*7*1152\n\n56448\n\n\nIf we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper, but we know that deeper layers have to compute semantically rich features (such as eyes or furs) so we wouldn’t expect that doing less computation would make sense.\n\n\n\nThe receptive field is the area of an image that is involved in the calculation of a layer. The deeper we are in the network (the more stride-2 convs we have before a layer) the larger the receptive field for an activation in that layer is. A larger receptive field means that a large amount of the input image is used to calculate each activation in that layer. We’d expect that we’d need more weights for each of the deeper layer’s richer features to handle this increased complexity—which is why with stride-2 we increase the number of features in each deeper layer (since the input size decreases).\n\n\n\nA color picture is a rank-3 tensor.\n\nim = image2tensor(Image.open('/content/grizzly.jpg'))\nim.shape\n\ntorch.Size([3, 1000, 846])\n\n\n\nshow_image(im);\n\n\n\n\nThe first axis contains red, green and blue channels\n\n_,axs = subplots(1,3)\nfor bear,ax,color in zip(im,axs,('Reds', 'Greens', 'Blues')):\n  show_image(255-bear, ax=ax, cmap=color)\n\n\n\n\n\nshow_image(255-im[0],cmap='Reds');\n\n\n\n\n\nshow_image(255-im[1],cmap='Greens');\n\n\n\n\n\nshow_image(255-im[2],cmap='Blues');\n\n\n\n\n\n_,axs = subplots(1,3)\nfor bear,ax in zip(im,axs):\n  show_image(255-bear, ax=ax)\n\n\n\n\nIn one sliding window we have a certain number of channels and we need as many filters (we don’t use the same kernel for all the channels). kernel size: ch_in x 3 x 3. We sum the results for all three channel window x filter multiplications to produce a single number for each grid location for each ch_out output feature. The result of our convolutional layer: ch_out x ch_in x ks x ks.\nThere are as many biases as we have kernels. The bias is a vector of size ch_out.\nChanging the encoding of colors won’t make any difference to your model results, as long as you don’t lose information in the transformation (transforming to B/W is a bad idea as it loses color information while converting to HSV generally won’t make a difference).\n\n\n\nCreate a 10-digit classifier.\n\ndef conv(ni, nf, ks=3, act=True):\n  res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n  if act: res = nn.Sequential(res, nn.ReLU())\n  return res\n\n\npath = untar_data(URLs.MNIST)\npath.ls()\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\n(#2) [Path('/root/.fastai/data/mnist_png/testing'),Path('/root/.fastai/data/mnist_png/training')]\n\n\n\n# create a function to change dls params\ndef get_dls(bs=64):\n  return DataBlock(\n      blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n      get_items=get_image_files,\n      splitter=GrandparentSplitter('training', 'testing'),\n      get_y=parent_label,\n      batch_tfms=Normalize()\n  ).dataloaders(path, bs=bs)\n\n\ndls = get_dls()\n\n\ndls.show_batch(max_n=9, figsize=(4,4))\n\n\n\n\n\npd.Series([el.parent.name for el in dls.valid.items]).sort_values().unique()\n\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)\n\n\n\n\n\nUse a similar CNN as before but with more activations (more numbers to differentiate = we’ll likely need more filters).\nWe generally want to double the number of filters each time we have a stride-2 layer. One way to increase the number of filters throughout our network is to double the number of activations in the first layer—then every layer after that will end up twice as big as in the previous version.\nNeural networks will create useful features only if they’re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs. If we have a 3x3 kernel, the number of inputs is 9. If we have 8 filters, we’ll be using 9 numbers (3x3) to calculate 8 numbers. It isn’t learning much at all (input and output size is the same). To fix this, use a larger kernel for the first layer, 5x5, so that 25 values are used to learn 8 values (one for each of the 8 filters) at each location.\n\ndef simple_cnn():\n  return sequential(\n      conv(1, 8, ks=5),         # 14x14\n      conv(8, 16),              # 7x7\n      conv(16, 32),             # 4x4\n      conv(32, 64),             # 2x2\n      conv(64, 10, act=False),  # 1x1\n      Flatten()\n  )\n\n\nxb, yb = first(dls.valid)\nxb, yb = to_cpu(xb), to_cpu(yb)\n\n\nconv(1, 8, ks=5)(xb).shape\n\ntorch.Size([64, 8, 14, 14])\n\n\nWe can look inside of our models while they’re training with the Activation Stats callback which records the mean, standard deviation and histogram of activations of every trainable layer.\n\nfrom fastai.callback.hook import *\n\n\ndef fit(epochs=1):\n  learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))\n  learn.fit(epochs, 0.06)\n  return learn\n\n\nlearn = fit()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.307114\n      2.306540\n      0.101000\n      01:07\n    \n  \n\n\n\nThat didn’t train well, let’s find out why.\n\nlearn.summary()\n\n\n\n\n\n\n\n\nSequential (Input shape: 64 x 1 x 28 x 28)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 8 x 14 x 14    \nConv2d                                    208        True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 16 x 7 x 7     \nConv2d                                    1168       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 32 x 4 x 4     \nConv2d                                    4640       True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 64 x 2 x 2     \nConv2d                                    18496      True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 10 x 1 x 1     \nConv2d                                    5770       True      \n____________________________________________________________________________\n                     64 x 10             \nFlatten                                                        \n____________________________________________________________________________\n\nTotal params: 30,282\nTotal trainable params: 30,282\nTotal non-trainable params: 0\n\nOptimizer used: <function Adam at 0x7d6f11cb0700>\nLoss function: <function cross_entropy at 0x7d6fd5b369e0>\n\nModel unfrozen\n\nCallbacks:\n  - ActivationStats\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n\n\n\nlearn.activation_stats.plot_layer_stats(0) # first layer\n\n\n\n\nGenerally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training. Activations near zero are problematic because that means the model is doing nothing and that carries over to the next layer.\n\n# penultimate layer\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\nThe problem, as expected, gets worse by the end of the network with nearly 100% of the activations close to 0.\n\nl_stats = learn.activation_stats.layer_stats(0)\n\n\nlen(l_stats[0])\n\n937\n\n\n\nlen(dls.train.items) / 64\n\n937.5\n\n\nNote: 937 = number of batches in training set, so the mean activation is across the batch (as is standard deviation, and % near zero).\n\n\n\nOne way to make training more stable is to increase the batch size. Large batches have gradients that are more accurate since they’re calculated from more data with the downside of fewer opportunities to update weights (fewer batches per epoch).\n\ndls = get_dls(512)\n\n\nlearn = fit()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.398756\n      0.203404\n      0.935500\n      01:01\n    \n  \n\n\n\n\n# penultimate layer\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\nEven though the accuracy is higher, most of the activations are near zero.\n\n\n\nOur initial weights are not well suited to the task we’re trying to solve. Starting with a large learning rate may diverge the training from the start. We don’t want to end with a high learning rate either because we don’t want to skip over the minimum. We should change learning rate from low, to high, and then back to low again. Leslie Smith developed this idea—a schedule where in the first phase the learning rate grows from the minimum value to the maximum value (warmup), and then decreases back to minimum (annealing)—1cycle training which allows for higher learning rates (trains faster, “super-convergence” and overfits less by skipping over the sharp local minima).\nA model that generalizes well is one whose loss would not change very much if you changed the input by a small amount (I think one way to think about that is a smooth loss surface—no quick or sudden sharp changes). If a model trains with a large learning rate and finds a good loss when doing so (i.e. a loss that doesn’t change very much) it will generalize well.\nOnce we have found a nice smooth area for our parameters, we want to find the very best part of that area so we bring learning rates down again.\nmomentum: the optimizer takes a step not only in the direction of the gradients, but also that continues in the direction of previous steps. Momentum varies in the opposite direction of the learning rate: high learning rates -> less momentum (Leslie Smith, AGAIN!).\n\ndef fit(epochs=1, lr=0.06):\n  learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))\n  learn.fit_one_cycle(epochs, lr)\n  return learn\n\n\nlearn = fit()\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.196365\n      0.070784\n      0.977500\n      01:11\n    \n  \n\n\n\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n% near zero is lower for some batches but still overall high.\n\nlearn.recorder.plot_sched()\n\n\n\n\nfastai implements cosine annealing in the learning rate scheduler.\nfit_one_cycle parameters:\nlr_max: the highest learning rate to be used (can also be a list for each layer group or a Python slice object containing the first and last layer group learning rates)\ndiv: How much to divide lr_max by to get the starting learning rate.\ndiv_final: how much to divide lr_max by to get the ending learning rate.\npct_start: what percentages of the batches to use for the warmup\nmoms: a tuple (mom1,mom2,mom3) where mom1 is the initial momentum, mom2 is the minimum momentum, and mom3 is the final momentum.\nThe axes on the graph is the number of batches (60_000 training images / 512 images per batch = 117 batches).\n\n# colorful dimension\nlearn.activation_stats.color_dim(-2)\n\n\n\n\nThis is a classic picture of “bad training”. White = zero activations. Black at the bottom left is near-zero activations. The near-zero activations exponentially increase and then collapse almost like the training is starting over. We see this increase and collapse again a few times before the distribution is spread throughout the range. Starting the training smooth from the start can be achieved with batch normalization.\n\n\n\nWe need to fix the initial large percentage of near-zero activations and then try to maintain a good distribution of activations throughout training.\nFrom the Batch Normalization paper (2015, Ioffe and Szegedy):\ninternal covariate shift: distribution of each layer’s inputs changes during training as the parameters of the previous layers change which slows down the training (lower learning rates required) and requires careful parameter initialization.\n\nMaking normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.\n\nbatchnorm: taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the activations. The network will want to make some activations really high to make accurate predictions so there are two learnable parameters gamma and beta. After normalizing the activations to get some new activation vector y a batchnorm layer returns gamma*y + beta.\nOur activations can have any mean and variance independent from the mean and standard deviation of the results of the previous layer. During training we use mean and std of the batch to normalize the data, during validation we use a running mean aof the stats calculated during training.\n\n# add batchnorm\ndef conv(ni, nf, ks=3, act=True):\n  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n  layers.append(nn.BatchNorm2d(nf))\n  if act: layers.append(nn.ReLU())\n  return nn.Sequential(*layers)\n\n\nlearn = fit()\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.126032\n      0.055595\n      0.987400\n      01:12\n    \n  \n\n\n\n\nlearn.activation_stats.color_dim(-4)\n\n\n\n\nThat’s what we hope to see—a smooth development of activations with no collapses. We see batchnorm in nearly all modern neural networks.\nWe haven’t as yet seen rigorous analysis of what’s going on here, but most researchers believe that the reason models containing batch norm layers generalize better is that the normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and std than other mini-batches. The activations will be normalized by different values each time. The model will learn to become robust to these variations to make accurate predictions. Adding additional randomization to the training process often helps.\n\nlearn = fit(5, lr=0.1)\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.187965\n      0.117173\n      0.963600\n      01:09\n    \n    \n      1\n      0.079750\n      0.052391\n      0.983100\n      01:08\n    \n    \n      2\n      0.052439\n      0.048151\n      0.985200\n      01:03\n    \n    \n      3\n      0.032153\n      0.032204\n      0.989200\n      01:10\n    \n    \n      4\n      0.017457\n      0.024827\n      0.992300\n      01:04\n    \n  \n\n\n\n\nlearn.activation_stats.color_dim(-4)\n\n\n\n\n\n\n\nConvolutions are matrix multiplications with two constraints—some elements are always zero and som elements are tied (forced to be equal). These constraints enforce a certain pattern of connectivity, and allow us to use fewer parameters without sacrificing ability to represent complex visual features. We can train deeper models faster with less overfitting. Regular linear layers are called fully connected. Batch norm helps regularize training and makes it smoother.\n\n\n\n1. What is a feature?\nA visually distinctive attribute of an image.\n2. Write out the convolutional kernel matrix for a top edge detector.\n\n\n\n-1\n-1\n-1\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n\n3. Write out the mathematical operation applied by a 3x3 kernel to a single pixel in an image.\nAssuming that this question pertains to a 3x3 grid in the image, suppose we apply the kernel in #2 to the following 3x3 grid:\n\n\n\na1\na2\na3\n\n\na4\na5\na6\n\n\na7\na8\na9\n\n\n\nThe result is the equation:\n-a1 - a2 - a3 + a7 + a8 + a9\n4. What is the value of a convolutional kernel applied to a 3x3 matrix of zeros?\n0\n5. What is padding?\nAdding additional pixels around the border of the image to avoid skipping two pixels on each axis. With padding, instead of the kernel fitting fully inside the image at the edges, a portion of the kernel is on the padding pixels.\n6. What is stride?\nThe number of pixels by which the kernel moves or slides over the image. Stride-2 means the kernel moves over by two pixels (skipping one pixel).\n7. Create a nested list comprehension to complete any task that you choose.\n\n[(i, j, k) for i in range(3) for j in range(4) for k in range(2)]\n\n[(0, 0, 0),\n (0, 0, 1),\n (0, 1, 0),\n (0, 1, 1),\n (0, 2, 0),\n (0, 2, 1),\n (0, 3, 0),\n (0, 3, 1),\n (1, 0, 0),\n (1, 0, 1),\n (1, 1, 0),\n (1, 1, 1),\n (1, 2, 0),\n (1, 2, 1),\n (1, 3, 0),\n (1, 3, 1),\n (2, 0, 0),\n (2, 0, 1),\n (2, 1, 0),\n (2, 1, 1),\n (2, 2, 0),\n (2, 2, 1),\n (2, 3, 0),\n (2, 3, 1)]\n\n\n\n[[[i, j, k] for i in ['i1', 'i2', 'i3']] for j in ['j1', 'j2'] for k in ['k1', 'k2']]\n\n[[['i1', 'j1', 'k1'], ['i2', 'j1', 'k1'], ['i3', 'j1', 'k1']],\n [['i1', 'j1', 'k2'], ['i2', 'j1', 'k2'], ['i3', 'j1', 'k2']],\n [['i1', 'j2', 'k1'], ['i2', 'j2', 'k1'], ['i3', 'j2', 'k1']],\n [['i1', 'j2', 'k2'], ['i2', 'j2', 'k2'], ['i3', 'j2', 'k2']]]\n\n\n8. What are the shapes of the input and weight parameters to PyTorch’s 2D convolution?\ninput: (minibatch, in_channels, lH, lW) weight: filters of shape (out_channels, in_channels, kH, kW)\n9. What is a channel?\nA single basic color in an image.\n10. What is the relationship between a convolution and a matrix multiplication?\nA convolution is a matrix multiplication with two constraints:\n\nsome elements in the kernel matrix always stay 0.\nsome elements in that matrix are always equal to each other.\n\n11. What is a convolutional neural network?\nA neural network with non-linearity function sandwiched between convolutions. In other words, replacing the fully-connected (linear) layers in a neural network with convolutions.\n12. What is the benefit of refactoring parts of your neural network definition?\nLess likely to get errors due to inconsistencies in architecutre. More obvious to reader which parts of your layers are actually changing.\n13. What is Flatten? Where does it need to be included in the MNIS CNN? Why?\nA fastai/PyTorch layer that flattens the input to a single dimension, used at the end of the model. In our case, it removes the final 1x1 convolution dimensions.\n\nx = torch.ones(2, 1, 1)\nx.shape\n\ntorch.Size([2, 1, 1])\n\n\n\nx\n\ntensor([[[1.]],\n\n        [[1.]]])\n\n\n\nx[1], x[1][0], x[1][0][0]\n\n(tensor([[1.]]), tensor([1.]), tensor(1.))\n\n\n\nx[1].shape, x[1][0].shape, x[1][0][0].shape\n\n(torch.Size([1, 1]), torch.Size([1]), torch.Size([]))\n\n\n\nFlatten()(x).shape\n\ntorch.Size([2, 1])\n\n\n\nFlatten()(x), Flatten()(x)[1], Flatten()(x)[1][0]\n\n(tensor([[1.],\n         [1.]]),\n tensor([1.]),\n tensor(1.))\n\n\n\nFlatten()(x)[1].shape, Flatten()(x)[1][0].shape\n\n(torch.Size([1]), torch.Size([]))\n\n\n14. What does NCHW mean?\nN = batch size\nC = channels\nH = height\nW = width\n15. Why does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications?\n7x7 is the dimension of the resulting filters coming out of the previous convolution (second layer). 1152 (which is 1168 - 16) is the number of non-bias parameters in the third layer, which comes from the fact that the weight in that layers convolution has dimensions 8 x 16 x 3 x 3 (8 inputs, 16 output features and a 3x3 kernel).\n\n8*16*3*3\n\n1152\n\n\n16. What is a receptive field?\nThe area of an image that is involved in the calculation of a layer.\n17. What is the size of the receptive field of an activation after two stride-2 convolutions? Why?\n7x7. The activation in layer 2 is made from a 3x3 receptive field in layer 1. That 3x3 layer 1 area is made up of 7x7 receptive field in layer 0 (the original image).\nLet’s focus on the top-most and left-most activation in layer 2. That comes from the top-left-most 3x3 pixels in layer 1. The top-left-most pixel in layer 1 comes from the top-left-most pixel in layer 0. The next pixel to the right in layer 1 comes from a 3x3 grid starting at the the first-row third pixel in layer 0. The next pixel to the right in layer 1 comes from a 3x3 grid starting at the first-row fifth pixel in layer, which covers pixel 5, 6, and 7 in the first row of layer 0 (the original image). In this way, vertically, pixels 1 through 7 in the first column are involved, and the whole 7x7 grid is involved in layer 0 when looking at the whole 3x3 grid in layer 1.\n18. Run conv-example.xlsx yourself and experiment with trace precedents\nI recreated the whole notebook while following the Lesson 8 video. I also used trace precedents when answering question #17.\n19. Have a look at Jeremy’s or Sylvain’s recent Twitter “likes”, and see if you find any interesting resources or ideas there.\nLikes are no longer public on Twitter but I have follow a bunch of folks that Jeremy follows.\n20. How is a color image represented as a tensor?\nA rank-3 tensor (3 channels, height, width).\n21. How does a convolution work with a color input?\nIt applies a different filter to each channel and then sums the results for each pixel.\n22. What method can we use to see the data in DataLoaders?\nshow_batch\n23. Why do we double the number of filters after each stride-2 conv?\nSince the image reduces in size by half, we double the number of filters so that as deeper layers learn more complex features there’s enough data to learn from.\n24. Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\nIf we use a 3x3 kernel to produce 8 filters, that’s 9 inputs producing 8 outputs—the model isn’t learning much. For the model to learn things, the number of inputs should be larger than the number of outputs. So if we use a 5x5 kernel, that’s 25 pixels producing 8 outputs, so the model has to learn useful features.\n25. What information does ActivationStats save for each layer?\nMean and standard deviation of activations.\n26. How can we access a learner’s callback after training?\nLearner.<name of callback> for example the ActivationStats callback is accessed after training with Learner.activation_stats.\n27. What are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\nThree statistics plotted: mean activations, standard deviation of activations and % of activations near zero. The x-axis represents the batches.\n28. Why are activations near zero problematic?\nBecause they result in the model doing nothing after the computation (multiplying by 0 = 0). Also, the resulting 0-activations result in more 0 activations in the next layer. In this way, the deeper you go, the more the activations are near zero if the early layers have many near-zero activations.\n29. What are the upsides and downsides of training with a larger batch size?\nUpside: smoother training. Downside: fewer gradient updates (opportunities to “learn”).\n30. Why should we avoid using a high learning rate at the start of training?\nWe will “overshoot” the minimum and the training will diverge (exploding gradeints).\n31. What is 1cycle training?\nA learning rate schedule where the learning starts starts off small, warms up to a larger value then anneals back down to a smaller value.\n32. What are the benefits of training with a high learning rate?\nYou can train quicker and overfit less (since we skip over sharp local minima).\n33. Why do we want to use a low learning rate at the end of training?\nAssuming that we have found the minimum, we don’t want to overshoot it so a smaller learning rate takes smaller steps towards the minimum.\n34. What is cyclical momentum?\nMomentum is “a technique whereby the optimizer takes a stepo not only in the direction of the gradients, but also that continues in the direction of previous steps.” Cyclical momentum is when the momentum is on a schedule going from large, to small to large again (in the case of 1cycle training).\n35. What callback tracks hyperparameter values during training (along with other information)?\nRecorder\n36. What does one column of pixels in the color_dim plot represent?\nThe activations for one batch.\n37. What does “bad training” look like in color_dim? Why?\nCyclical increase and collapse of nonzero activations. This results in near zero activations at the end of training which can lead to poor results.\n38. What trainable parameters does a batch normalization layer contain?\ngamma and beta where given a vector y of normalized activations, gamma * y + beta is the “learned” normalization returned by batchnorm.\n39. What statistics are used to normalize in batch normalization during training? How about during validation?\ntraining: mean and standard deviation of the activations of the batch.\nvalidation: running mean and standard deviations from previous\n40. Why do models with batch normalization layers generalize better?\nWe don’t fully know (at least at the time of writing) but it’s likely because the normalization (including the learned parameters) adds randomness and additional randomness generally helps the model generalize better."
  },
  {
    "objectID": "posts/2024-03-30-attention-is-all-you-need/index.html",
    "href": "posts/2024-03-30-attention-is-all-you-need/index.html",
    "title": "Paper Summary: Attention is All You Need",
    "section": "",
    "text": "In this notebook I’ll provide a summary of the Attention is All You Need paper. I’ll also heavily reference the fantastic code walkthroughs by CodeEmporium on YouTube for the Encoder and Decoder.\nOther resources that were critical to my understanding of this paper:\n\nBenjamin Warner’s two-part blog post on creating a transformer from scratch (Attention mechanism and the rest of the transformer).\nIntroduction to RNNs by Max Weichart.\nThe Illustrated Transformer and The Illustrated GPT-2 by Jay Alammar.\nUnderstanding LSTM Networks by Christopher Olah.\nUnderstanding Encoder and Decoder LLMs by Sebastian Raschka.\nWhat are Word and Sentence Embeddings? by Cohere.\nIllustrated Guide to Transformers Neural Network: A step by step explanation by the AI Hacker."
  },
  {
    "objectID": "posts/2024-03-30-attention-is-all-you-need/index.html#sequence-modeling-review",
    "href": "posts/2024-03-30-attention-is-all-you-need/index.html#sequence-modeling-review",
    "title": "Paper Summary: Attention is All You Need",
    "section": "Sequence Modeling Review",
    "text": "Sequence Modeling Review\nBefore getting into the details of the Transformer architecture introduced in this paper, I’ll do a short overview of the main type of architecture (RNN) that the Transformer is improving upon. Most importantly, the Transformer is improves upon the dependencies between tokens in long sequences.\n\nRecurrent Neural Nets (RNNs)\nIn Max’s post he provides the following illustration of RNNs, where the inputs are recursively passed through the hidden laye and at each iteration, the hidden layer state from the previous step is incorporated in the current state’s calculation. In this way, RNNs store information about the previous step in the next step.\nThe forward pass in Max’s post is given as:\ndef forward(self, X, i=0, h=0):\n  l1 = lin(X[i], self.w1)\n  h = relu(l1 + h*self.w2)\n  if (i+1 != len(X)):\n    return self.foward(X, i+1, h)\nThe hidden state from the previous iteration h is multiplied by a trainable weight w2, added to the output of the linear function lin(X[i], self.w1) and passed through a non-linearity (in this case a ReLU) to get the current state h. Until the end of the input sequence is reached, the forward pass continues to recursively incorporate previous state information into the current input’s calculation.\n\n\nThe study of RNNs highlights how, in the basic RNN architecture, as the time instants considered increase, the product chain determined by backpropagation through time tends to zero or tends to extremely large values. In the first case, we have a vanishing gradient, in the second case an exploding gradient. (source).\n\n\n\nLong Short-Term Memory (LSTM)\nTo combat this training instability for long sequences, the LSTM network is used. This is an RNN architecutre capable of learning long-term dependencies with long-term memory (cell state C in the diagram) and short-term memory (hidden state H in the diagram).\n\n(source).\nThe LSTM uses past information (H) and new information (X) to update long-term memory (C). It then uses C to update H, and the cycle continues for the next input sequence.\nIn the diagram below, at the bottom left, the Hidden state from the previous step, \\(\\textbf{H}_{t-1}\\) is combined with the new Input of the current step \\(\\textbf{X}_t\\) and goes into the different gates for different purposes (Forget gate, Input gate, Candidate memory, and Output gate).\nThe \\(+\\) operator is the combining of long-term memory from the previous step \\(\\textbf{C}_{t-1}\\) with the output of the Candidate memory \\(\\tilde{C_t}\\).\nFinally, the Output gate \\(\\textbf{O}_t\\) combines the long-term memory \\(\\textbf{C}_t\\) with the sigmoid output of \\(\\textbf{H}_{t-1}\\) and \\(\\textbf{X}_t\\) to create the hidden state for the current step \\(\\textbf{H}_t\\).\nThe hidden state and long-term memory are then used in the next input step."
  },
  {
    "objectID": "posts/2024-03-30-attention-is-all-you-need/index.html#transformer-architecture-overview",
    "href": "posts/2024-03-30-attention-is-all-you-need/index.html#transformer-architecture-overview",
    "title": "Paper Summary: Attention is All You Need",
    "section": "Transformer Architecture Overview",
    "text": "Transformer Architecture Overview\n\nDoes not use recurrence.\nRelies entirely on the attention mechanism for global dependencies.\nAllows for parallelization (as opposed to sequential processing).\n\n\nThe Transformer achieves better BLUE scores than previous state-of-the-art (SOTA) models on the English-to-German and English-to-French machine translation tests at a fraction of the training cost.\n\n\nHere are the current English-to-German SOTA results:\n\nsource\nAnd the current English-to-French SOTA results:\n\nsource\nHere is the paper’s beautiful diagram (with my annotations) of an Encoder-Decoder Transformer architecture:\n\nThe inputs (numericalized tokens) pass through the Input Embedding which projects these numbers into a much larger number of dimensions, dimensions in which different information about the tokens will be learned through training. In this paper they use a dimension of 512 (referred to as the “hidden dimension”). This value is a hyperparameter and different architectures use different numbers of hidden dimensions.\nThe output of this Embedding is passed through a Positional Encoding step which quantitatively stores information about the position of each token. Since Transformers don’t explicitly express position as sequence modeling does, we have to implicitly express position in this way.\nThe inputs, after going through the Embedding and Positional Encoding, now enter the Encoder which is a type of Transformer Block containing Mult-Head Attention, Add & Norm layers and a Feed Forward Network.\nThe outputs follow a similar path, first through an Output Embedding, then Positional Encoding, and then a Decoder which is another type of Transform Block. A Transformer can be Encoder-only, Decoder-only or Encoder-Decoder. In this paper they focus on Encoder-Decoder Transformers, where information learned in the Encoder is used in the Decoder in a process call cross-attention that we’ll look into shortly. In this paper, they have 6 Encoder blocks and 6 Decoder blocks. The number of blocks can be varied (i.e. it’s a hyperparameter).\nThe outputs of the Decoder pass through a final linear layer and then a softmax layer (transformed into 0 to 1.0 probabilities).\n\nThe encoder receives the input text that is to be translated, and the decoder generates the translated text. (source)\n\n\nFundamentally, both encoder- and decoder-style architectures use the same self-attention layers to enocde word tokens. However, the main difference is that encoders are designed to learn embeddings that can be used for various predictive modeling tasks such as classification. In contrast, decoders are designed to generate new texts, for example, answering user queries. (source)\n\n\nThe encoder part in the original transformer…is responsible for understanding and extracting the relevant information from the input text. It then outputs a continuous representation (embedding) of the input text that is passed to the decoder. Finally, the decoder generates the translated text (target language) based on the continuous representation received from the encoder. (source)\n\nFrom Benjamin Warner’s post:\n\nIf we use the first sentence in this post and assume each word is a token:\nTransformers are everywhere.\nthen the “Transformers” token would predict “are”, and “are” would predict “everywhere.”\nTo create our inputs (line 1) we’ll drop the last token and to create the labels (line 2) we’ll remove the first token:\n\nTransformers are\nare everywhere"
  },
  {
    "objectID": "posts/2024-03-30-attention-is-all-you-need/index.html#code-overview-encoder",
    "href": "posts/2024-03-30-attention-is-all-you-need/index.html#code-overview-encoder",
    "title": "Paper Summary: Attention is All You Need",
    "section": "Code Overview: Encoder",
    "text": "Code Overview: Encoder\nIn this section I’ll walk through some of the code presented in the YouTube video by CodeEmporium.\nI’ll start by defining some constants that I’ll use throughout. d_model is the hidden dimension hyperparameter.\n\nimport torch\nimport math\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n\nbatch_size = 64\nmax_sequence_length = 200\nd_model = 512\nvocab_size = 10_000\ncontext_size = 200\n\nIn a real scenarios, the inputs (tokens) would be numericalized tokens corresponding to a real natural language dataset. In this example, I’ll use random integers.\n\ntokens = torch.randint(0, vocab_size, (batch_size, max_sequence_length))\ntokens.shape, tokens.min(), tokens.max()\n\n(torch.Size([64, 200]), tensor(0), tensor(9999))\n\n\nI have 64 batches of 200 tokens each, where each token is an integer from 0 to 10_000.\n\nInput Embedding\nThe input Embedding is a PyTorch object which takes an integer and returns a tensor of a given dimension (in this case 512).\n\nvocab_embed = nn.Embedding(vocab_size, d_model)\nvocab_embed\n\nEmbedding(10000, 512)\n\n\nWhen I pass a tensor integer, I get in return a 512 dimension tensor filled with float values.\n\nvocab_embed(torch.tensor([4])).shape\n\ntorch.Size([1, 512])\n\n\n\nvocab_embed(torch.tensor([4]))[0][:5]\n\ntensor([-0.0996,  1.2077, -0.8627, -0.4755,  0.5210], grad_fn=<SliceBackward0>)\n\n\nWhen I pass my batched tokens to the Embedding, I get back a batched set of 512 float values:\n\ntoken_embs = vocab_embed(tokens)\ntoken_embs.shape\n\ntorch.Size([64, 200, 512])\n\n\nIn other words, my tokens, which are integers that represent natural language, are now projected into 512 dimensions, dimensions in which the Embedding will learn something about the tokens and therefore about language.\n\n\nPositional Encodings\nThe formula used in the paper for positional encodings are as follows (sine for even i values and cosine for odd):\n\\[PE_{(pos, 2i)} = \\sin(\\text{pos} / 10000^{2i/d_{model}})\\] \\[PE_{(pos, 2i+1)} = \\cos(\\text{pos} / 10000^{2i/d_{model}})\\]\nI’ll reuse the code provided in Benjamin’s blog post:\n\n# create the positional encoding tensor of shape\n# maximum sequence length (MS) by embedding dimension (C)\npe = torch.zeros(context_size, d_model, dtype=torch.float)\n\n# pre-populate the position and the div_terms\nposition = torch.arange(context_size).unsqueeze(1)\ndiv_term = torch.exp(\n    torch.arange(0, d_model, 2) * (-math.log(10000) / d_model)\n)\n\n# even positional encodings use sine, odd cosine\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\n\nI want to make sure I understand the div_term since I didn’t understand it at first glance:\ntorch.exp(torch.arange(0, d_model, 2) * (-math.log(10000) / d_model))\nTranslating that to math gives us:\n\\[\\exp\\big(-2i * \\ln(10000) / d_{model}\\big)\\]\nUsing the negative exponent rule: \\(\\exp(-a) = \\frac{1}{\\exp(a)}\\):\n\\[\\exp\\big({\\frac{-2i * ln(10000)}{d_{model}}}\\big) = \\frac{1}{\\exp \\big( \\frac{2i * ln(10000)}{d_{model}}\\big)}\\]\nUsing the power of a power rule: \\(\\exp(ab) = \\exp(a)^b\\):\n\\[\\frac{1}{\\exp \\big( \\frac{2i * ln(10000)}{d_{model}}\\big)} = \\frac{1}{\\exp\\big(\\ln(10000)\\big)^{2i/d_{model}}}\\]\nThe term \\(\\exp(\\ln(10000))\\) equals just \\(10000\\):\n\\[\\frac{1}{\\exp\\big(\\ln(10000)\\big)^{2i/d_{model}}} = \\frac{1}{10000^{2i/d_{model}}}\\]\nWhich is the same as the divison term in the paper’s math formula.\n\npe[0][:5], pe[1][:5]\n\n(tensor([0., 1., 0., 1., 0.]),\n tensor([0.8415, 0.5403, 0.8219, 0.5697, 0.8020]))\n\n\nI’ll add the positional encoding to the embedded tokens—note that here PyTorch uses broadcasting to “copy” pe over each of the 64 batches.\n\npe.shape, token_embs.shape\n\n(torch.Size([200, 512]), torch.Size([64, 200, 512]))\n\n\n\ntoken_embs = token_embs + pe\ntoken_embs.shape\n\ntorch.Size([64, 200, 512])\n\n\nIn CodeEmporium’s implementation, at this point token_embs is passed through a Dropout layer, so I’ll do the same:\n\nembed_drop = nn.Dropout(0.1)\nx = embed_drop(token_embs)\nx.shape\n\ntorch.Size([64, 200, 512])\n\n\n\n\nAttention Mechanism\nAt this point, the inputs are now ready to enter the attention mechanism. Before we do that, I’ll save the current state of the inputs in a variable so that later on I can add it to the output of the attention mechanism.\n\nresidual_x = x\n\nThe particular flavor of attention used at this point is Scaled Dot-Product Attention across multiple heads. Here’s the steps taken in Scaled Dot-Product Attention:\n\nWhere \\(Q\\) (query), \\(K\\) (key) and \\(V\\) (value) are matrices (initially of random numbers) that consist of learned weights during training.\nThe first step is the matrix multiplication of \\(Q\\) and \\(K^T\\), followed by scaling that result by the square root of the dimension \\(d_k\\). The encoder doesn’t have a mask (the decoder does). Finally, the softmax is taken of that scaled dot product and its output matrix multiplied with \\(V\\).\nHere’s a conceptual understanding of attention from the Illustrated GPT-2:\n\nAnd here’s a visualization of attention values between tokens:\n\nBefore we get into the code for attention, here is a visualization of Mult-Head Attention, where the Scaled Dot-Product Attention occurs simultaneously across multiple heads, displaying the parallelization capability of Transformers:\n\nWe’ll go bottom-up in the diagram:\n\nCreate Q, K, V matrices. Split them across \\(h\\) heads.\nPerform Scaled Dot-Product Attention.\nConcatenate them from \\(h\\) heads.\nPass them through a final Linear layer.\n\nBoth Benjamin and CodeEmporium created a single Linear layer and then split them into \\(Q\\), \\(K\\) and \\(V\\), so I’ll do the same. A reminder (to myself and the reader) that these are weight matrices that will be used eventually to multiply by the inputs.\n\nqkv_layer = nn.Linear(d_model, 3 * d_model)\nqkv_layer\n\nLinear(in_features=512, out_features=1536, bias=True)\n\n\nPassing the inputs through this linear layer gives us the matrices:\n\nqkv = qkv_layer(x)\nqkv.shape\n\ntorch.Size([64, 200, 1536])\n\n\nNext, we project the \\(Q\\), \\(K\\) and \\(V\\) combined matrix across 8 heads\n\nnum_heads = 8\nhead_dim = d_model // num_heads\n\nqkv = qkv.reshape(batch_size, max_sequence_length, num_heads, 3 * head_dim)\nqkv.shape\n\ntorch.Size([64, 200, 8, 192])\n\n\nThis splits the 1536 values into 8 sets of 192.\nIn CodeEmporium’s code, they swap the middle two dimensions so it’s broadcastable with tensors later on\n\nqkv = qkv.permute(0, 2, 1, 3)\nqkv.shape\n\ntorch.Size([64, 8, 200, 192])\n\n\nWe then split qkv into three separate matrices, each with 200 x 64 values on each of the 8 heads:\n\nq, k, v = qkv.chunk(3, dim=-1)\nq.shape, k.shape, v.shape\n\n(torch.Size([64, 8, 200, 64]),\n torch.Size([64, 8, 200, 64]),\n torch.Size([64, 8, 200, 64]))\n\n\nFinally, we can create the attention matrix. First we perform the scaled dot-product between \\(Q\\) and \\(K\\)\n\nd_k = torch.tensor(q.shape[-1]) # 64\n\nscaled_dot_product = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(d_k)\nscaled_dot_product.shape\n\ntorch.Size([64, 8, 200, 200])\n\n\nNote that when \\(K\\) is transposed, the last two dimensions are swapped to allow for correct matrix multiplication dimension order.\n\nq.shape, k.shape, k.transpose(-1, -2).shape\n\n(torch.Size([64, 8, 200, 64]),\n torch.Size([64, 8, 200, 64]),\n torch.Size([64, 8, 64, 200]))\n\n\nThe dimension of 64 matches between \\(Q\\) and \\(K^T\\) after .transpose(-1 ,-2) swaps the last two dimensions of \\(K\\).\nOne thing I noticed in both Benjamin and CodeEmporium’s code is that they define attention as the output of passing the scaled dot-product through softmax. This is the “attention matrix” I’ve seen referred to in places. The paper defines attention as the product of the matrix multiplication between that softmax output and the \\(V\\) (values) matrix.\n\n\nattention = F.softmax(scaled_dot_product, dim=-1)\n\n\nattention.shape\n\ntorch.Size([64, 8, 200, 200])\n\n\ndim is set to -1 so that the values in the last dimension are between 0 and 1.\n\nattention[0,0,0].shape, attention[0,0,0].sum()\n\n(torch.Size([200]), tensor(1., grad_fn=<SumBackward0>))\n\n\nattention’s final dimensions are of size 200 x 200, representing weights corresponding to the relationship between each of the 200 tokens.\n\nv = torch.matmul(attention, v)\nv.shape\n\ntorch.Size([64, 8, 200, 64])\n\n\nFrom Benjamin’s post:\n\nNext we matrix multiply the Attention weights with our value matrix \\(V\\) which applies the Attention weights to our propagating token embeddings\n\n\nx.shape\n\ntorch.Size([64, 200, 512])\n\n\n\nx = attention @ v\nx.shape\n\ntorch.Size([64, 8, 200, 64])\n\n\nNext, we have to concatenate across the 8 heads:\n\nx = x.reshape(batch_size, max_sequence_length, num_heads * head_dim)\nx.shape\n\ntorch.Size([64, 200, 512])\n\n\nNow the 64 dimensions across 8 heads are concatenated to get back to the embedding size of 512. We still maintain the 64 batches and 200 sequence length.\nThe last step before the attention mechanism is fully complete is to pass these values through a linear layer:\n\nlinear_layer = nn.Linear(d_model, d_model)\nx = linear_layer(x)\nx.shape\n\ntorch.Size([64, 200, 512])\n\n\nThe linear layer maintains the dimension (512 in, 512 out).\nx then passes through a Dropout layer and a Layer Normalization layer. Note that residual_x is added to x before the sum is passed through the Layer Normalization.\nI won’t walk through the details of Layer Normalization, but CodeEmporium provides the following code that I’ll highlight the following few lines from:\nmean = x.mean(dim=[-1], keepdim=True)\nvar = ((x - mean) ** 2).mean(dim=[-1], keepdim=True)\nstd = (var + 1e-5).sqrt()\n\ny = (x - mean) / std\nx = gamma * y + beta\nWhere gamma and beta are learnable nn.Parameter weights. Note that the values are normalized (resulting in y) and then normalization occurs across all samples (gamma * y + beta).\nx is stored as residual_x to add on later, and then x goes through a Feed Forward Network (a non-linearity, in this case a GELU, and a Dropout layer sandwiched between two linear layers), and then through another Dropout layer and Layer Normalization (where residual_x is added to x)."
  },
  {
    "objectID": "posts/2024-03-30-attention-is-all-you-need/index.html#code-overview-decoder",
    "href": "posts/2024-03-30-attention-is-all-you-need/index.html#code-overview-decoder",
    "title": "Paper Summary: Attention is All You Need",
    "section": "Code Overview: Decoder",
    "text": "Code Overview: Decoder\nThere are some similarities and some differences between the Encoder and the Decoder. Note that in CodeEmporium’s implementation, the Decoder contains Self Attention and Encoder-Decoder Attention (also called Cross Attention in Benjamin’s post).\n\nThe first main difference is that what goes into the Decoder are the outputs (the inputs shifted by one token).\nIn the Decoder, attention is masked. Only the current token and previous output tokens are “visible” to the model. Future tokens are masked. How does this masking take place? Here’s CodeEmporium’s code:\nStart by creating a 200 x 200 tensor full of negative infinity (negative infinity is used so that when you take the softmax of it, it goes to 0)\n\nmask = torch.full([max_sequence_length, max_sequence_length], float('-inf'))\nmask\n\ntensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        ...,\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])\n\n\nKeep the upper triangle as -inf and make everything else 0 with torch.triu:\n\nmask = torch.triu(mask, diagonal=1)\nmask\n\ntensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n        [0., 0., -inf,  ..., -inf, -inf, -inf],\n        [0., 0., 0.,  ..., -inf, -inf, -inf],\n        ...,\n        [0., 0., 0.,  ..., 0., -inf, -inf],\n        [0., 0., 0.,  ..., 0., 0., -inf],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\nNow, when the mask is added to the scaled dot-product, the upper triangle will go to -inf (since anything plus -inf is -inf). Taking the softmax of that to get the attention matrix will result in a matrix with an upper triangle of zeros:\n\nscaled_dot_product = scaled_dot_product + mask\nscaled_dot_product[0][0]\n\ntensor([[-9.5668e-01,        -inf,        -inf,  ...,        -inf,\n                -inf,        -inf],\n        [-3.3439e-01, -1.0772e+00,        -inf,  ...,        -inf,\n                -inf,        -inf],\n        [-2.8391e-01,  2.7374e-02,  7.6844e-01,  ...,        -inf,\n                -inf,        -inf],\n        ...,\n        [ 3.8771e-04, -2.7279e-01,  3.2622e-01,  ..., -1.5672e-01,\n                -inf,        -inf],\n        [ 9.1237e-01,  9.8978e-01,  8.4105e-02,  ...,  7.8569e-01,\n         -1.8654e-03,        -inf],\n        [-8.3207e-01, -1.6773e-01, -8.6295e-01,  ..., -3.1891e-01,\n         -7.7460e-01, -8.4962e-01]], grad_fn=<SelectBackward0>)\n\n\n\nF.softmax(scaled_dot_product, dim=-1)[0][0]\n\ntensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.6776, 0.3224, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.1912, 0.2610, 0.5477,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0049, 0.0037, 0.0068,  ..., 0.0042, 0.0000, 0.0000],\n        [0.0073, 0.0079, 0.0032,  ..., 0.0065, 0.0029, 0.0000],\n        [0.0031, 0.0060, 0.0030,  ..., 0.0052, 0.0033, 0.0030]],\n       grad_fn=<SelectBackward0>)\n\n\nCross Attention works differently—the “Cross” in Cross Attention is talking about the relationship between the Encoder and Decoder. Specifically, the \\(K\\) and \\(V\\) weights are applied to the Encoder outputs and the \\(Q\\) weights are applied to the Decoder outputs. The rest of the process (scaled dot product, softmax, concatenation, linear layer) are the same as before (with the addition of adding the mask to the scaled dot product).\nAfter passing the through Cross Attention, the outputs go through Dropout and Layer Normalization, then a Feed Forward Network, and then through another Dropout and Layer Normalization step. The inputs to the Layer Normalization call are the residual_x plus x, which is said to stabilize the training process.\nFinally, the outputs go through a final linear layer which projects the outputs to the vocabulary size and then a final softmax call which converts those logits to probabilities per vocabulary token (in other words, answering the question: what are the probabilities that the next token will be each token in the vocabulary?)\nThere are a lot of details that I have left out of this post for brevity so to get the full Transformers code experience, see Benjamin’s commented-transformers repository, and CodeEmporium’s Encoder/Decoder notebooks."
  },
  {
    "objectID": "posts/2024-03-30-attention-is-all-you-need/index.html#final-thoughts",
    "href": "posts/2024-03-30-attention-is-all-you-need/index.html#final-thoughts",
    "title": "Paper Summary: Attention is All You Need",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI was pleasantly surprised at how understandable the code is for the Transformer architecture. The paper does a great job of helping the reader visualizing the concepts in play, especially the process involved in calculating scaled dot-product attention across multiple heads. The number excellent resources available that I’ve referenced throughout this blog post are also essential to understanding the code and concepts involved.\nOn a personal note, I recall going to a presentation on this paper a few years ago and leaving feeling so incredibly lost, and that maybe I wouldn’t understand how this critical architecture actually works—like I had hit a wall of complexity that I wouldn’t be able to overcome. Reading this paper, understanding it, presenting on it and writing this blog post felt like redemption for me. I obviously couldn’t have done it without the excellent resources I’ve linked above.\nAs always, I hope you enjoyed this paper summary!"
  },
  {
    "objectID": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html",
    "href": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html",
    "title": "Determining the Ratio of Lowercase to Uppercase Letter Heights Using Signal Analysis",
    "section": "",
    "text": "In this notebook, I’ll walk through an algorithm I modified (originally created by Claude) to calculate the ratio of lowercase to uppercase letter height from an image of text.\nThe lowercase height (x-height) is the distance between the top and bottom of lowercase (and uppercase) letters. The uppercase height (cap-height) is the distance from the top of uppercase letters to the bottom of uppercase (and lowercase) letters.\nThe ratio in question is defined as:\n\\[\\frac{\\text{x-height}}{\\text{cap-height}}\\]\n\nCalculating this x-height to cap-height ratio is just one of several non-ML baselines I plan to establish before building a neural network to classify typeface categories (e.g., humanist sans, grotesque sans, display, script) from text images. I’m calling this project TypefaceClassifier and will continue sharing updates in this blog series over the coming weeks.\n\n\n\nAn example of cap-height, x-height and baseline for a line of text\n\n\n\n\nShow imports\nimport cv2\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\nimport os\nfrom os import listdir\nfrom scipy.signal import find_peaks"
  },
  {
    "objectID": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#horizontal-projection-pixel-sum-per-row",
    "href": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#horizontal-projection-pixel-sum-per-row",
    "title": "Determining the Ratio of Lowercase to Uppercase Letter Heights Using Signal Analysis",
    "section": "Horizontal Projection: Pixel Sum Per Row",
    "text": "Horizontal Projection: Pixel Sum Per Row\nThe basis of this algorithm is a horizontal projection of the image—in other words, taking the sum of each row’s pixels, resulting in an 1-D array of values.\n\npath = 'serif-24px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\n\nh_proj = np.sum(binary, axis=1)\nplt.plot(h_proj);\n\n\n\n\n\n\n\n\nSuperimposing the h_proj plot onto the text image reveals that spikes in h_proj align with denser white pixels, while troughs correspond to rows with more black pixels.\n\n\nShow code for plotting function\ndef img_with_peaks(path):\n  # Read the image\n  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n  _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n  # Calculate horizontal projection\n  h_proj = np.sum(binary, axis=1)\n\n  # Create figure and axis\n  fig, ax = plt.subplots(figsize=(6, 6))\n\n  # Display the image\n  ax.imshow(binary, cmap='gray', aspect='equal')\n\n  # Plot the horizontal projection\n  max_proj = np.max(h_proj)\n  scaled_proj = h_proj / max_proj * binary.shape[1]  # Scale projection to image width\n  ax.plot(scaled_proj, np.arange(len(h_proj)), color='red', linewidth=2, alpha=0.7)\n\n  # Customize the plot\n  ax.set_xlim(0, binary.shape[1])\n  ax.set_ylim(binary.shape[0], 0)  # Invert y-axis to match image coordinates\n  ax.axis('off')  # Remove axes\n\n  # Adjust the plot size and position\n  plt.tight_layout()\n  plt.show()\n\n\n\nimg_with_peaks(path)"
  },
  {
    "objectID": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#identifying-letter-heights-signal-analysis",
    "href": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#identifying-letter-heights-signal-analysis",
    "title": "Determining the Ratio of Lowercase to Uppercase Letter Heights Using Signal Analysis",
    "section": "Identifying Letter Heights: Signal Analysis",
    "text": "Identifying Letter Heights: Signal Analysis\nTo determine the pixel heights of lowercase and uppercase letters I need to determine three locations:\n\nthe bottom of the lowercase and uppercase letters (approximately what is called the baseline on which the letters “sit”)\nthe top of the lower case letters\nthe top of the uppercase letters\n\nThe image above shows empty rows of black pixels between lines of text, with these rows having a sum of 0. By capturing groups of non-zero elements in h_proj (between the 0s), I can isolate the row pixel sums for the lines of text. I can then extract the bottom and top of the lowercase and uppercase letters.\n\nGrouping Non-Zero Elements of an Array\nTo capture these groups of non-zero elements between 0s (each group corresponds to a line of text), we can do the following:\nCreate a boolean mask that indicates if each element is non-zero.\n\nmask = h_proj != 0\nmask[:5]\n\narray([False, False, False, False, False])\n\n\nTo find the “boundary” of each group (indices where the mask changes from False to True) we start by adding a False to the beginning and end of mask—this will capture any non-zero groups that start at the first or last element of h_proj:\n\nnp.concatenate(([False], mask, [False]))[0], np.concatenate(([False], mask, [False]))[-1]\n\n(False, False)\n\n\nNext we calculate the difference between consecutive elements in the array:\n\nnp.diff(np.concatenate(([False], mask, [False])))[:5]\n\narray([False, False, False, False, False])\n\n\nFinally, we get the indices of the elements where h_proj changes from non-zero to 0 (or 0 to non-zero):\n\nchange_indices = np.where(np.diff(np.concatenate(([False], mask, [False]))))[0]\nchange_indices\n\narray([ 29,  51,  63,  85,  96, 118, 130, 152, 164, 186, 197, 219, 231,\n       251, 264, 286, 298, 320, 331, 353, 365, 387, 399, 419, 432, 454,\n       466, 488])\n\n\n\n# h_proj goes from 0 to non-zero on index 29\n# h_proj goes from non-zero to 0 on index 51\nh_proj[28], h_proj[29], h_proj[50], h_proj[51]\n\n(0, 1530, 1275, 0)\n\n\nNow that we have the boundaries of these non-zero groups, we can iterate through them and capture the non-zero elements:\n\npairs = change_indices.reshape(-1, 2)\npairs\n\narray([[ 29,  51],\n       [ 63,  85],\n       [ 96, 118],\n       [130, 152],\n       [164, 186],\n       [197, 219],\n       [231, 251],\n       [264, 286],\n       [298, 320],\n       [331, 353],\n       [365, 387],\n       [399, 419],\n       [432, 454],\n       [466, 488]])\n\n\n\ngroups = [(h_proj[start:end], np.arange(start, end)) for start, end in pairs]\ngroups[0] # the h_proj values for the first line of text\n\n(array([ 1530,  5355,  3570,  3570,  4080,  5355, 54825, 51255, 39015,\n        41310, 40290, 34425, 34680, 37740, 41055, 52785, 62220,  1275,\n         1275,  1275,   765,  1275], dtype=uint64),\n array([29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n        46, 47, 48, 49, 50]))\n\n\nWe’ll wrap this up in a function group_nonzero for later use:\n\ndef group_nonzero(arr):\n    # Create a boolean mask for non-zero elements\n    mask = arr != 0\n\n    # Find the indices where the mask changes (start and end of groups)\n    change_indices = np.where(np.diff(np.concatenate(([False], mask, [False]))))[0]\n\n    # Pair up start and end indices\n    pairs = change_indices.reshape(-1, 2)\n\n    # Create groups\n    groups = [(arr[start:end], np.arange(start, end)) for start, end in pairs]\n\n    return groups\n\n\n\nIdentifying Peaks of Non-Zero Arrays\nNow that we have the horizontal projection (row sum of pixels) of each line of text, we can use scipy.find_peaks to identify the indices (pixel values) with relatively high concentrations of pixels.\nLet’s use the first group (line of text) as an example. Running it through scipy.find_peaks returns the indices where scipy identifies peaks.\n\npeaks, _ = find_peaks(groups[0][0]) # groups[0][0] are the rowsum values\npeaks\n\narray([ 1,  6,  9, 16])\n\n\nThere exists a peak on the 1st, 6th, 9th and 16th index. These are relative indices, the absolute indices (across the 512 pixels of the image height) can be captured as so:\n\nindices = groups[0][1][peaks]\nindices\n\narray([30, 35, 38, 45])\n\n\nTo illustrate, I’ll plot horizontal lines at each of these pixel heights.\nThe topmost (red) line represents the top of the capital letters. The next (green) line is the top of the lower case letters. The next (blue) line is located at the height of the “bar” in the letter “e”. The final (black) line is at the bottom of the lowercase letters.\n\n\nShow code to plot lines on image\nimg = cv2.imread(path)\n\ncolors = [\n    (225, 0, 0), # red\n    (0, 225, 0), # green\n    (0, 0, 225), # blue\n    (0, 0,  0 )  # black\n]\n\nfor i, x in enumerate(indices):\n  color_idx = i % 4\n  cv2.line(img, [0, x], [512, x], colors[color_idx], 1)\n\nimg\n\n\n\n      ndarray (512, 512, 3) show dataarray([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       ...,\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8)\n\n\nThe top of the uppercase letters is easy to identify—it’s the first peak. To better visualize how we can distinguish between the top and the bottom of the lowercase letters, I’ll plot the image, the peaks, and the four horizontal lines all on the same image:\n\n\nShow code for plotting function\ndef img_with_peaks(path):\n  # Read the image\n  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n  _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n  # Calculate horizontal projection\n  h_proj = np.sum(binary, axis=1)\n\n  # get first line's peaks\n  groups = group_nonzero(h_proj)\n  peaks, _ = find_peaks(groups[0][0])\n  indices = groups[0][1][peaks]\n\n  # Create figure and axis\n  fig, ax = plt.subplots(figsize=(6, 6))\n\n  # Display the image\n  ax.imshow(binary, cmap='gray', aspect='equal')\n\n  # Draw horizontal lines at specified indices\n  for idx in indices:\n      ax.axhline(y=idx, color='grey', linewidth=1, alpha=1)\n\n  # Plot the horizontal projection\n  max_proj = np.max(h_proj)\n  scaled_proj = h_proj / max_proj * binary.shape[1]  # Scale projection to image width\n  ax.plot(scaled_proj, np.arange(len(h_proj)), color='red', linewidth=1.25, alpha=1)\n\n  # Customize the plot\n  ax.set_xlim(0, binary.shape[1])\n  ax.set_ylim(binary.shape[0], 0)  # Invert y-axis to match image coordinates\n  ax.axis('off')  # Remove axes\n\n  # Adjust the plot size and position\n  plt.tight_layout()\n  plt.show()\n\n\nIt’s tough to see it clearly on this image, but the top and bottom of the lowercase letters have the two highest peaks of pixel sums.\n\nimg_with_peaks('serif-24px.png')\n\n\n\n\n\n\n\n\nYou can see it a bit more clearly with larger font—the top of the “u” at the end of the first line corresponds to the second highest blue peak. The bottom of the “u” on the first line corresponds to the highest blue peak.\n\nimg_with_peaks('serif-52px.png')\n\n\n\n\n\n\n\n\nTo summarize:\n\nthe top of the uppercase letters is the first peak in each line.\nthe top and bottom of the lowercase letters are the two-highest peaks in each line."
  },
  {
    "objectID": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#calculating-x-height-to-cap-height-ratio",
    "href": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#calculating-x-height-to-cap-height-ratio",
    "title": "Determining the Ratio of Lowercase to Uppercase Letter Heights Using Signal Analysis",
    "section": "Calculating x-height to cap-height Ratio",
    "text": "Calculating x-height to cap-height Ratio\nGiven this heuristic, I can calculate the x-height and cap-height (and the ratio between the two) for each line of text:\n\nx_to_cap = []\n\nfor group in groups:\n  peaks, _ = find_peaks(group[0])\n\n  # top of uppercase letters is the first peak\n  cap_top_idx = group[1][peaks[0]]\n  cap_top_peak = group[0][peaks[0]]\n\n  # top and bottom of lowercase letters are the two highest remaining peaks\n  x_peaks = sorted(zip(group[0][peaks][1:], group[1][peaks][1:]), reverse=True)[:2]\n  x_peaks = [el[1] for el in x_peaks]\n\n  # top of lowercase letters is the smaller index from the top\n  # bottom of lowercase letters is the larger index from the top\n  x_top = np.min(x_peaks)\n  x_bottom = np.max(x_peaks)\n\n  # calculate lowercase height (x_height) and uppercase height (cap_height)\n  cap_height = x_bottom - cap_top_idx\n  x_height = x_bottom - x_top\n\n  # calculate aspect ratio\n  ratio = x_height / cap_height\n  x_to_cap.append(ratio)\n\nThe fourteen lines of text (when the font size is 24px) have a nearly consistent ratio between x-height and cap-height\n\nx_to_cap\n\n[0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6666666666666666,\n 0.6,\n 0.6666666666666666]\n\n\nI can wrap the fully functional code (creating h_proj from the image, getting non-zero groups, calculating the x-height-to-cap-height ratio for each line) to get the median x-height to cap-height ratio for an image of text:\n\n\nShow code for median_x_to_cap\ndef median_x_to_cap(path):\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    final_img = cv2.imread(path)\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    h_proj = np.sum(binary, axis=1)\n\n    groups = group_nonzero(h_proj)\n    ratios = []\n\n    for group in groups:\n        # group[0] are the array values\n        # group[1] are the indices\n        peaks, _ = find_peaks(group[0]) # `peaks` is the indices of peaks\n\n        if len(peaks) < 3:\n            continue  # Skip lines with insufficient peaks\n\n        # top of uppercase letters\n        cap_top_idx = group[1][peaks[0]]\n\n        # top and bottom of lowercase letters\n        x_peaks = sorted(zip(group[0][peaks][1:], group[1][peaks][1:]), reverse=True)[:2]\n        x_peaks = [el[1] for el in x_peaks]\n\n        x_top = np.min(x_peaks)\n        x_bottom = np.max(x_peaks)\n\n        # calculate lowercase height (x_height) and uppercase height (cap_height)\n        cap_height = x_bottom - cap_top_idx\n        x_height = x_bottom - x_top\n\n        # if find_peaks didn't capture top of uppercase letters\n        if cap_height == x_height:\n          continue\n\n        # calculate aspect ratio\n        ratio = x_height / cap_height\n        ratios.append(ratio)\n\n    median_ratio = np.median(ratios)\n\n    return median_ratio\n\n\nI can then try it on different images of the same font (serif). These images have different font sizes and different text of the same font, so they should result in about the same value (0.67) which they do:\n\nmedian_x_to_cap('serif-24px.png'), \\\nmedian_x_to_cap('serif-36px.png'), \\\nmedian_x_to_cap('serif-76px.png'),\n\n(0.6666666666666666, 0.6521739130434783, 0.6382978723404256)\n\n\nThree different images of the same font (display) with different font sizes and texts result in a (somewhat less) consistent median x-height to cap-height ratio:\n\nmedian_x_to_cap('display-24px.png'), \\\nmedian_x_to_cap('display-36px.png'), \\\nmedian_x_to_cap('display-76px.png'),\n\n(0.6428571428571429, 0.6190476190476191, 0.6038961038961039)\n\n\nOne limitation of this algorithm is that it results in significantly different ratios for very small or very large font sizes. These ratios should be close to 0.67:\n\nmedian_x_to_cap('serif-8px.png'), \\\nmedian_x_to_cap('serif-330px.png')\n\n(nan, 0.24040133294697189)\n\n\nThese should be close to 0.62:\n\nmedian_x_to_cap('display-8px.png'), \\\nmedian_x_to_cap('display-330px.png')\n\n(nan, 0.875)\n\n\nFor now, I’ll move forward with this algorithm knowing that I’ll have to constraint the font sizes I use to within 24px and 76px."
  },
  {
    "objectID": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#final-thoughts",
    "href": "posts/2024-08-12-typefaceclassifier-x-to-cap-height/index.html#final-thoughts",
    "title": "Determining the Ratio of Lowercase to Uppercase Letter Heights Using Signal Analysis",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nExploring non-ML baselines has significantly expanded my understanding of coding and algorithms. A seemingly simple task, like identifying the tops and bottoms of letters, introduced me to concepts such as horizontal image projection and signal peak detection—ideas I hadn’t encountered before. Calculating the x-height to cap-height ratio is just one of several non-ML baselines I plan to establish before building a neural network to classify typeface categories (e.g., humanist sans, grotesque sans, display, script) from text images. I’m calling this project TypefaceClassifier and will continue sharing updates in this blog series over the coming weeks.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html",
    "href": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html",
    "title": "Calculating the Ratio of Corners in an Image",
    "section": "",
    "text": "In this notebook I’ll walk through an algorithm suggested by Claude to distinguish one typeface (like display) from another (like serif) in which we calculate the ratio of the number of corners to number of pixels in a text image.\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom google.colab.patches import cv2_imshow"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#load-image-and-binarize-it",
    "href": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#load-image-and-binarize-it",
    "title": "Calculating the Ratio of Corners in an Image",
    "section": "Load Image and Binarize It",
    "text": "Load Image and Binarize It\nAs usual, we load the image and binarize it so it’s easier to distinguish between background (black pixels) and text (white pixels).\n\npath = 'serif-76px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#detect-corners",
    "href": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#detect-corners",
    "title": "Calculating the Ratio of Corners in an Image",
    "section": "Detect Corners",
    "text": "Detect Corners\nI’ll use the Harris Corner Detection algorithm to identify the pixels at which there exists a corner in the image.\n\ncorners = cv2.cornerHarris(binary, blockSize=2, ksize=3, k=0.04)\n\nDilating the corners makes them brighter\n\ncorners = cv2.dilate(corners, None)\n\n\ncorners.shape\n\n(512, 512)\n\n\n\ncorners[:5]\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\nthresholding the pixels so that pixels with a corners value above 1% of the maximum are set to white (255).\n\n_, corners = cv2.threshold(corners, 0.01 * corners.max(), 255, 0)\n\nThe end result is a set of pixels that are the locations of corners in the image. Notice how the straight segments of the letters are (correctly) not identified as corners, while curves are identified as corners.\n\ncv2_imshow(corners)"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#calculating-the-ratio-of-corner-pixels-to-total-pixels",
    "href": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#calculating-the-ratio-of-corner-pixels-to-total-pixels",
    "title": "Calculating the Ratio of Corners in an Image",
    "section": "Calculating the Ratio of Corner Pixels to Total Pixels",
    "text": "Calculating the Ratio of Corner Pixels to Total Pixels\nThe number of corners are the number of corners pixels that have a value greater than 0.\n\nnp.sum(corners > 0)\n\n15127\n\n\nThe ratio of number of corners to pixels in an image is given as the ratio of the number of corners pixels greater than 0 divided by the number of binary pixels greater than 0. In this case, 67% of the pixels represent corners.\n\nnp.sum(corners > 0), np.sum(binary > 0), np.sum(corners > 0) / np.sum(binary > 0)\n\n(15127, 22475, 0.6730589543937708)\n\n\nFor a different image, notably display (sans serif) text of the same font-size (76px), this ratio is considerably smaller (39% < 67%).\n\npath = 'display-76px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\n\ncorners = cv2.cornerHarris(binary, blockSize=2, ksize=3, k=0.04)\ncorners = cv2.dilate(corners, None)\n_, corners = cv2.threshold(corners, 0.01 * corners.max(), 255, 0)\nnp.sum(corners > 0), np.sum(binary > 0), np.sum(corners > 0) / np.sum(binary > 0)\n\n(13650, 34992, 0.3900891632373114)\n\n\nVisually, you can see how much fewer corners are detected in this image than in the image with serif text—this makes sense! Serifs are intricate flourishes added to the ends of letters and contain more changes in stroke (corners).\n\ncv2_imshow(corners)"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#calculating-contour-ratio-for-different-images",
    "href": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#calculating-contour-ratio-for-different-images",
    "title": "Calculating the Ratio of Corners in an Image",
    "section": "Calculating Contour Ratio for Different Images",
    "text": "Calculating Contour Ratio for Different Images\nI’ll now wrap the code above into a function and apply it to a wide variety of images (of two typefaces, display and serif and 8 different font sizes). I expect, on average, the serif images to have a higher corner ratio than display images, since serifs introduce more corners.\n\ndef corner_ratio(path):\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    corners = cv2.cornerHarris(binary, blockSize=2, ksize=3, k=0.04)\n\n    corners = cv2.dilate(corners, None)\n    _, corners = cv2.threshold(corners, 0.01 * corners.max(), 255, 0)\n    corners = np.uint8(corners)\n\n    num_corners = np.sum(corners > 0)\n    total_pixels = np.sum(binary > 0)\n\n    corner_ratio = num_corners / total_pixels if total_pixels > 0 else 0\n\n    return corner_ratio\n\nOn average, images with serif fonts have more corners present.\n\nszs = [8, 18, 24, 36, 76, 240, 330, 420]\nts = ['display', 'serif']\nres = []\n\nfor t in ts:\n    for sz in szs:\n        image_path = f\"{t}-{sz}px.png\"\n        sr = corner_ratio(image_path)\n        res.append([t, sz, sr])\n\nres = pd.DataFrame(res, columns=['typeface', 'font-size', 'corner-ratio'])\nres.groupby('typeface')['corner-ratio'].agg(['mean', 'median'])\n\n\n\n  \n    \n\n\n  \n    \n      \n      mean\n      median\n    \n    \n      typeface\n      \n      \n    \n  \n  \n    \n      display\n      0.989564\n      0.734144\n    \n    \n      serif\n      1.287083\n      1.134465\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThis trend holds up for each font size:\n\nres.sort_values(by='font-size')\n\n\n\n  \n    \n\n\n  \n    \n      \n      typeface\n      font-size\n      corner-ratio\n    \n  \n  \n    \n      0\n      display\n      8\n      2.827288\n    \n    \n      8\n      serif\n      8\n      2.874287\n    \n    \n      1\n      display\n      18\n      1.784839\n    \n    \n      9\n      serif\n      18\n      2.573949\n    \n    \n      2\n      display\n      24\n      1.609162\n    \n    \n      10\n      serif\n      24\n      2.192324\n    \n    \n      3\n      display\n      36\n      1.078198\n    \n    \n      11\n      serif\n      36\n      1.595871\n    \n    \n      4\n      display\n      76\n      0.390089\n    \n    \n      12\n      serif\n      76\n      0.673059\n    \n    \n      5\n      display\n      240\n      0.113082\n    \n    \n      13\n      serif\n      240\n      0.173800\n    \n    \n      6\n      display\n      330\n      0.061339\n    \n    \n      14\n      serif\n      330\n      0.123992\n    \n    \n      7\n      display\n      420\n      0.052513\n    \n    \n      15\n      serif\n      420\n      0.089383"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#final-thoughts",
    "href": "posts/2024-09-09-typefaceclassifier-corner-ratio/index.html#final-thoughts",
    "title": "Calculating the Ratio of Corners in an Image",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nSimilar to the contour ratio algorithm there is a clear and consistent difference in value between serif and sans serif fonts for this corner ratio algorithm, making this a good candidate for distinguishing between typefaces.\nThis is also another relatively simple algorithm, and each step can be easily visualized.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html",
    "href": "posts/2024-05-23-y-range/index.html",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "",
    "text": "In Chapter 8 of the fastai text we train a collaborative filtering model that predicts movie ratings for users (who have not watched those movies yet). It’s one way of asking the question: would this user like this movie given their interests and the movie’s characteristics? The users’ “interests” and the movies’ “characteristics” are the latent factors that we train. The ratings (predictions) are the dot product between the user and movie latent factors. This dot product passes through the sigmoid_range function which squeezes the input values into output values within a given range. In the textbook, the range we use is 0 to 5.5. We use 5.5 because the sigmoid function never reaches 1 so 5 * sigmoid would never reach 5 (the maximum movie rating). Overshooting by 0.5 solves this issue. 5.5 times sigmoid will be able to reach an output of 5 comfortably.\nIn this blog post I’ll explore the question: how does model performance vary as y_range varies?\nHere is a summary of my results of different statistics (rows) for different y_range values (columns) for the validation set (20k samples):\n\n\n\nStatistic\nNone\n(0, 5.5)\n(0.5, 5.5)\n(0.75, 5.25)\n(1,5)\n(-2,8)\n\n\n\n\nMedian Prediction\n3.49\n3.53\n3.54\n3.54\n3.53\n3.55\n\n\nMean Prediction\n3.43\n3.48\n3.5\n3.5\n3.49\n3.51\n\n\nKurtosis\n0.78\n-0.06\n-0.1\n-0.11\n-0.14\n0.12\n\n\nSkew\n-0.59\n-0.39\n-0.34\n-0.36\n-0.38\n-0.32\n\n\nAnderson-Darling\n71.7\n42.4\n33.4\n37\n42\n21.8\n\n\n% preds outside 1-5\n0.93%\n0.23%\n0.21%\n0.07%\n0.00%\n1.19%\n\n\n\nAnd for the training set (80k samples):\n\n\n\nStatistic\nNone\n(0, 5.5)\n(0.5, 5.5)\n(0.75, 5.25)\n(1,5)\n(-2,8)\n\n\n\n\nMedian Prediction\n3.50\n3.60\n3.60\n3.60\n3.59\n3.63\n\n\nMean Prediction\n3.44\n3.49\n3.50\n3.50\n3.50\n3.52\n\n\nKurtosis\n0.42\n0.15\n0.003\n0.003\n-0.06\n0.2\n\n\nSkew\n-0.49\n-0.61\n-0.53\n-0.56\n-0.56\n-0.5\n\n\nAnderson-Darling\n228.9\n490.4\n388.1\n444.8\n467.6\n350.4\n\n\n% preds outside 1-5\n0.68%\n0.31%\n0.23%\n0.05%\n0.00%\n1.85%"
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html#training-without-y_range",
    "href": "posts/2024-05-23-y-range/index.html#training-without-y_range",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "Training without y_range",
    "text": "Training without y_range\nI think it makes sense to first explore the loss and output distribution when I don’t set y_range when training a collaborative filtering model on the 100k subset of MovieLens. I’ll reuse the code from the text to prepare the data and DataLoaders and use a weight decay of 0.1:\n\nfrom scipy.stats import anderson\n\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n\n    \n      \n      100.15% [4931584/4924029 00:00<00:00]\n    \n    \n\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      815\n      Groundhog Day (1993)\n      4\n    \n    \n      1\n      357\n      Phantom, The (1996)\n      3\n    \n    \n      2\n      246\n      Blown Away (1994)\n      3\n    \n    \n      3\n      311\n      Casablanca (1942)\n      4\n    \n    \n      4\n      457\n      Immortal Beloved (1994)\n      4\n    \n    \n      5\n      241\n      Titanic (1997)\n      4\n    \n    \n      6\n      525\n      Independence Day (ID4) (1996)\n      4\n    \n    \n      7\n      394\n      Cape Fear (1991)\n      4\n    \n    \n      8\n      109\n      Dante's Peak (1997)\n      3\n    \n    \n      9\n      334\n      Wolf (1994)\n      2\n    \n  \n\n\n\n\nlearn = collab_learner(dls, n_factors=50, y_range=None)\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.257861\n      1.301678\n      00:13\n    \n    \n      1\n      1.071218\n      1.113060\n      00:12\n    \n    \n      2\n      0.989054\n      1.017373\n      00:11\n    \n    \n      3\n      0.856945\n      0.928325\n      00:12\n    \n    \n      4\n      0.848923\n      0.905493\n      00:12\n    \n  \n\n\n\nI want to see the distribution of predictions for the training and validation set and understand how they vary. I’ll create a helper function for that.\n\ndef plot_preds(preds, title):\n  preds = pd.Series(preds)\n  preds.hist();\n  plt.title(f'{title} preds distribution')\n  print('median:', preds.median())\n  print('mean:', preds.mean())\n  print('kurtosis: ', preds.kurtosis())\n  print('skew: ', preds.skew())\n\n  result = anderson(preds, dist='norm')\n  print(f'Statistic: {result.statistic}')\n  print(f'Critical values: {result.critical_values}')\n  print(f'Significance levels: {result.significance_level}')\n\n  cond = (preds < 1) | (preds > 5)\n  print(f'% of preds outside of 1-5 range: {100*cond.sum()/cond.count():.2f}%')\n\n\npreds, targ = learn.get_preds(dl=dls.valid)\n\n\n\n\n\n\n\n\n\n# check loss---should be close to 0.905493\nMSELossFlat()(preds, targ)\n\nTensorBase(0.9055)\n\n\n\nplot_preds(learn.get_preds(dl=dls.valid)[0], 'valid')\n\n\n\n\n\n\n\n\nmedian: 3.4890234\nmean: 3.4260304\nkurtosis:  0.7783028\nskew:  -0.58709365\nStatistic: 71.65442338831053\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.93%\n\n\n\n\n\nThe validation set predictions are slightly skewed left with a median rating of about 3.5. Based on the Anderson-Darling statistic (which is significantly larger than the most stringent critical value of 1.092), these 20k samples don’t come from a normal distribution. Less than 1% of the values fall outside of the expected rating range of 1 to 5.\n\nplot_preds(learn.get_preds(dl=dls.train)[0], 'train')\n\n\n\n\n\n\n\n\nmedian: 3.4968839\nmean: 3.435657\nkurtosis:  0.41849822\nskew:  -0.49159753\nStatistic: 228.91494857503858\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.68%\n\n\n\n\n\nThe training set predictions are similarly distributed, with a slightly larger peak resulting in a slightly larger median rating, still around 3.5.\nIn general there are more values outside of the realistic range (1 to 5) of ratings in the validation predictions than the training predicitons. Although, the model is doing pretty well at predicting values within the desired range with less than 1% falling outside this range."
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html#training-with-y_range0-5.5",
    "href": "posts/2024-05-23-y-range/index.html#training-with-y_range0-5.5",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "Training with y_range=(0, 5.5)",
    "text": "Training with y_range=(0, 5.5)\n\nlearn2 = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn2.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.882406\n      0.942118\n      00:13\n    \n    \n      1\n      0.650510\n      0.887792\n      00:12\n    \n    \n      2\n      0.542655\n      0.862130\n      00:12\n    \n    \n      3\n      0.440741\n      0.848899\n      00:12\n    \n    \n      4\n      0.442999\n      0.842771\n      00:12\n    \n  \n\n\n\nUsing a y_range of 0 to 5.5 resulted in a ~7% lower loss.\n\nplot_preds(learn2.get_preds(dl=dls.valid)[0], 'valid')\n\n\n\n\n\n\n\n\nmedian: 3.5321503\nmean: 3.4844122\nkurtosis:  -0.055667587\nskew:  -0.3875332\nStatistic: 42.351315721156425\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.23%\n\n\n\n\n\nThis distribution is still not normal but has half the Anderson-Darling statistic as when y_range was None. The kurtosis is closer to 0 as well. The key point is that only about 1/4th of the values as before are outside of the 1-5 rating range.\n\nplot_preds(learn2.get_preds(dl=dls.train)[0], 'train')\n\n\n\n\n\n\n\n\nmedian: 3.5977917\nmean: 3.4933543\nkurtosis:  0.14653848\nskew:  -0.6128638\nStatistic: 490.3643317096139\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.31%\n\n\n\n\n\nThe training predictions are more skewed than the validation predictions."
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html#training-with-y_range0.5-5.5",
    "href": "posts/2024-05-23-y-range/index.html#training-with-y_range0.5-5.5",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "Training with y_range=(0.5, 5.5)",
    "text": "Training with y_range=(0.5, 5.5)\n\nratings['rating'].min(), ratings['rating'].max()\n\n(1, 5)\n\n\nI can’t find it anymore, but there was a fastai forums post where someone was questioning why the lower range in y_range wasn’t 0.5 (0.5 less than the minimum rating of 1 matching the upper range 5.5 is 0.5 more than the maximum rating of 5). I’ll see if training with y_range=(0.5, 5.5) improves the loss or changes the distribution of predictions.\n\nlearn3 = collab_learner(dls, n_factors=50, y_range=(0.5, 5.5))\nlearn3.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.841459\n      0.931699\n      00:14\n    \n    \n      1\n      0.652540\n      0.878996\n      00:12\n    \n    \n      2\n      0.530454\n      0.865976\n      00:12\n    \n    \n      3\n      0.448474\n      0.856127\n      00:13\n    \n    \n      4\n      0.423248\n      0.852660\n      00:12\n    \n  \n\n\n\nThat actually worsened the loss, increasing it by about 1%. I’ll look at the training and validation prediction distributions:\n\nplot_preds(learn3.get_preds(dl=dls.valid)[0], 'valid')\n\n\n\n\n\n\n\n\nmedian: 3.5413134\nmean: 3.5004866\nkurtosis:  -0.102446005\nskew:  -0.3400191\nStatistic: 33.359148298073706\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.21%\n\n\n\n\n\nThe median and mean ratings are a bit higher and about the same amount of ratings are outside the acceptable range. The distribution is similarly not normal but has the lowest Anderson-Darling statistic so far.\n\nplot_preds(learn3.get_preds(dl=dls.train)[0], 'train')\n\n\n\n\n\n\n\n\nmedian: 3.6018043\nmean: 3.5078757\nkurtosis:  0.0025408994\nskew:  -0.5326863\nStatistic: 388.08379248825077\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.23%\n\n\n\n\n\nThe median and mean for the training predictions are also a tiny bit larger but mostly the distribution is the same as y_range=(0, 5.5) (although the kurtosis is much smaller)."
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html#training-with-y_range0.75-5.25",
    "href": "posts/2024-05-23-y-range/index.html#training-with-y_range0.75-5.25",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "Training with y_range=(0.75, 5.25)",
    "text": "Training with y_range=(0.75, 5.25)\nI’m curious if a “tighter” range changes the results.\n\nlearn4 = collab_learner(dls, n_factors=50, y_range=(0.75, 5.25))\nlearn4.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.891943\n      0.931708\n      00:12\n    \n    \n      1\n      0.676929\n      0.879818\n      00:13\n    \n    \n      2\n      0.531733\n      0.866186\n      00:12\n    \n    \n      3\n      0.459268\n      0.852890\n      00:13\n    \n    \n      4\n      0.454604\n      0.848512\n      00:12\n    \n  \n\n\n\nThis results in the second-best loss value thus far.\n\nplot_preds(learn4.get_preds(dl=dls.valid)[0], 'valid')\n\n\n\n\n\n\n\n\nmedian: 3.5425978\nmean: 3.5018144\nkurtosis:  -0.113488525\nskew:  -0.36151984\nStatistic: 36.919869251567434\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.07%\n\n\n\n\n\n\nplot_preds(learn4.get_preds(dl=dls.train)[0], 'train')\n\n\n\n\n\n\n\n\nmedian: 3.5998974\nmean: 3.503809\nkurtosis:  0.0028086598\nskew:  -0.5607914\nStatistic: 444.8073482159525\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.05%\n\n\n\n\n\nThe training and validation predictions have the lowest amount of predictions falling outside the acceptable range—this makes sense because sigmoid is not going to get as close to 1 and 5 as a y_range of (0.5, 5.5) or (0, 5.5)."
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html#training-with-y_range1-5",
    "href": "posts/2024-05-23-y-range/index.html#training-with-y_range1-5",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "Training with y_range=(1, 5)",
    "text": "Training with y_range=(1, 5)\nJust to cover my bases, I’ll train with a y_range not recommended: from 1 to 5. With this range, sigmoid will never output ratings of exactly 1 or 5.\n\nlearn5 = collab_learner(dls, n_factors=50, y_range=(1, 5))\nlearn5.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.890540\n      0.942143\n      00:13\n    \n    \n      1\n      0.675952\n      0.874900\n      00:12\n    \n    \n      2\n      0.560956\n      0.855053\n      00:14\n    \n    \n      3\n      0.500103\n      0.847492\n      00:17\n    \n    \n      4\n      0.492499\n      0.844006\n      00:14\n    \n  \n\n\n\nSurprisingly, this has supplanted y_range=(0.75, 5.25) with the second-best loss after 5 epochs. I wonder if that is because the overall range is lower?\n\nplot_preds(learn5.get_preds(dl=dls.valid)[0], 'valid')\n\n\n\n\n\n\n\n\nmedian: 3.5273356\nmean: 3.489109\nkurtosis:  -0.14329968\nskew:  -0.37828833\nStatistic: 42.07929809941925\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.00%\n\n\n\n\n\nAs expected, 0.00% of the ratings fall outside of the minimum of 1 and maximum of 5.\n\nplot_preds(learn5.get_preds(dl=dls.train)[0], 'train')\n\n\n\n\n\n\n\n\nmedian: 3.5868726\nmean: 3.4960902\nkurtosis:  -0.0628498\nskew:  -0.55758834\nStatistic: 467.5922112545086\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 0.00%"
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html#training-with-y_range-2-8",
    "href": "posts/2024-05-23-y-range/index.html#training-with-y_range-2-8",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "Training with y_range=(-2, 8)",
    "text": "Training with y_range=(-2, 8)\nAs a last fun experiment, I’ll use a much-wider-than-needed y_range and see how that affects the loss as well as the prediction distributions.\n\nlearn6 = collab_learner(dls, n_factors=50, y_range=(-2, 8))\nlearn6.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.806267\n      0.923924\n      00:13\n    \n    \n      1\n      0.556011\n      0.928603\n      00:14\n    \n    \n      2\n      0.437159\n      0.907485\n      00:13\n    \n    \n      3\n      0.346756\n      0.900347\n      00:12\n    \n    \n      4\n      0.331412\n      0.895803\n      00:13\n    \n  \n\n\n\nInterestingly, the training loss is significantly lower than any of the other training runs. The validation loss is about 5% higher than the lowest validation loss achieved prior. I’m curious to see how the distributions compare.\n\nplot_preds(learn6.get_preds(dl=dls.valid)[0], 'valid')\n\n\n\n\n\n\n\n\nmedian: 3.5484176\nmean: 3.5100946\nkurtosis:  0.11679816\nskew:  -0.32186633\nStatistic: 21.7676292314718\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 1.19%\n\n\n\n\n\nAbout 6 times as many predictions fall outside of the acceptable range (~1.2% to ~0.2%) which makes sense since the y_range is wider. The overall distributions is similar to the other validation predictions although this distribution (still very not normal) has the lowest Anderson-Darling statistic.\n\nplot_preds(learn6.get_preds(dl=dls.train)[0], 'train')\n\n\n\n\n\n\n\n\nmedian: 3.632931\nmean: 3.5240762\nkurtosis:  0.015062247\nskew:  -0.50895566\nStatistic: 350.41688774364593\nCritical values: [0.576 0.656 0.787 0.918 1.092]\nSignificance levels: [15.  10.   5.   2.5  1. ]\n% of preds outside of 1-5 range: 1.85%\n\n\n\n\n\nThe training loss distribution looks funkier than before (more than 10x the Anderson-Darling statistic), and it has a slightly larger median, and almost 9 times the values falling outside of the acceptable range."
  },
  {
    "objectID": "posts/2024-05-23-y-range/index.html#final-thoughts",
    "href": "posts/2024-05-23-y-range/index.html#final-thoughts",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different y_range Values",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI ended up enjoying this experiment more than I expected to. It was helpful to see intuitive results being validated through observing the actual prediction distributions (for example, y_range=(1,5) had 0 prediction outside of that range while y_range=(-2,8) had the most.\nThere were some surprises along the way: a y_range of (-2,8) had the lowest training loss—not sure what to make of that—a y_range of (1,5) resulted in the second-best loss (perhaps because there is a smaller range to predict within?) and although the none of distributions were normal, there were varying degrees of non-normality.\nAs part of the fastai Part 1 Lesson 7 homework, I’ll be training models on the full MovieLens dataset (~25M rows) so it’ll be fun to experiment with y_range values and see if I get different results.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-error-analysis/index.html",
    "href": "posts/2024-10-22-fastbookRAG-cs-error-analysis/index.html",
    "title": "Conducting a Question-by-Question Error Analysis on Semantic Search Results",
    "section": "",
    "text": "In this notebook I’ll do a deep dive error analysis of my semantic search results, where I implemented 6 different keyword-based full text searches to retrieve context sufficient to answer questions from the end-of-chapter Questionnaires in fastbook. Here is the summary of results from those experiments:\n\n\n\n\n\n\n\n\n\n\n\n\nChapter\nCS_A (Top-1 1p)\nCS_B (Top-3 1p)\nCS_C (Top-5 1p)\nCS_D (Top-1 3p)\nCS_E (Top-3 3p)\nCS_F (Top-5 3p)\n\n\n\n\n1\n40% (12/30)\n63.33% (19/30)\n63.33% (19/30)\n46.67% (14/30)\n80% (24/30)\n90% (27/30)\n\n\n2\n26.92% (7/26)\n61.54% (16/26)\n69.23% (18/26)\n53.85% (14/26)\n80.77% (21/26)\n84.62% (22/26)\n\n\n4\n29.03% (9/31)\n54.84% (17/31)\n64.52% (20/31)\n25.81% (8/31)\n67.74% (21/31)\n80.65% (25/31)\n\n\n8\n17.39% (4/23)\n43.48% (10/23)\n47.83% (11/23)\n43.48% (10/23)\n73.91% (17/23)\n91.30% (21/23)\n\n\n9\n28.57% (8/28)\n46.43% (13/28)\n53.57% (15/28)\n42.86% (12/28)\n57.14% (16/28)\n75% (21/28)\n\n\n10\n42.86% (9/21)\n47.62% (10/21)\n47.62% (10/21)\n47.62% (10/21)\n52.38% (11/21)\n57.14% (12/21)\n\n\n13\n41.18% (14/34)\n58.82% (20/34)\n61.76% (21/34)\n47.06% (16/34)\n70.59% (24/34)\n79.41% (27/34)\n\n\nAll\n32.64% (63/193)\n54.40% (105/193)\n59.07% (114/193)\n43.52% (84/193)\n69.43% (134/193)\n80.31% (155/193)\n\n\n\nThe granular question-level results are available in this public gist.\nAs a reminder, the two metrics I use for evaluation are Score and Answer Rate\nThe evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nWhile this is a straightforward pair of metrics, they do involve some judgment. After reading the retrieved context, I decide if it’s enough to answer the question.\nHere is a summary of tags across the 29 questions where the Answer Rate for all of my semantic search methods was 0% (i.e. none of the methods retrieved the context needed to answer the question):\n\n\n\nTag\nCount\n\n\n\n\nunknown failure\n12\n\n\nchunking strategy\n9\n\n\nkeyword-based question\n4\n\n\ndifficult question\n2\n\n\nunanswerable\n2\n\n\nrequires image\n1\n\n\n\nHere is a summary of how many 0% Answer Rate questions there are for each chapter:\n\n\n\nChapter\n# of Questions with 0% Answer Rate\n% of Chapter Questions\n\n\n\n\n9\n7\n25%\n\n\n10\n7\n33%\n\n\n13\n5\n15%\n\n\n2\n4\n15%\n\n\n1\n2\n7%\n\n\n4\n2\n6%\n\n\n8\n2\n9%\n\n\n\nThis notebook is a part of series of blog posts for a project I’m calling fastbookRAG where I’m trying to answer questions from the fastbook end-of-chapter Questionnaires using the following pipeline:\n\n\n\nfastbookRAG diagram"
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-error-analysis/index.html#error-analysis",
    "href": "posts/2024-10-22-fastbookRAG-cs-error-analysis/index.html#error-analysis",
    "title": "Conducting a Question-by-Question Error Analysis on Semantic Search Results",
    "section": "Error Analysis",
    "text": "Error Analysis\nFor 29 questions, none of the 6 semantic search methods retrieved sufficient context to provide an answer. I’ll be looking at each of those 29 questions, their “gold standard” answer (obtained from the fastai Forums Questionnaire wikis), and the relevant context from the fastbook chapter.\nI have three objectives for this error analysis:\n\nUnderstand what kinds of questions are difficult to answer using semantic search.\nIdentify ambigious questions that need to be removed from the evaluation set.\nIdentify unanswerable questions that need to be removed.\n\nThe overarching goal of this analysis: look at your data!\nFor each of the 29 questions, I will write four sections:\n\nRelevant Context: the paragraph(s) from the fastbook text that are sufficient to answer the question.\nAnalysis: my interpretation/explanation for why a semantic search did not retrieve the context.\nConclusion: what I think is needed to retrieve the sufficient context (or if I think this question should be removed)\nTags: keywords that describe the type of error.\n\n\nimport pandas as pd\nurl = 'https://gist.githubusercontent.com/vishalbakshi/9e0dc5b83c9b02810099f53377ced4ba/raw/3860f7dac972f37cc84cd10e22184c2bfd8813a4/cs_all.csv'\ndf = pd.read_csv(url)\nscore_columns = df.filter(regex='_score$').columns\ndf['total_score'] = df[score_columns].sum(axis=1)\nno_answer = df.query(\"total_score == 0\")\nno_answer.shape\n\n(29, 18)\n\n\n\ndef print_data(idx):\n  row = no_answer.iloc[idx]\n  print('Chapter, Question Number:',row['chapter'], row['question_number'])\n  print('Question Text:', row['question_text'])\n  print('Answer:', row['answer'])\n\n\nChapter 1\n\nQuestion 16\n\nprint_data(0)\n\nChapter, Question Number: 1 16\nQuestion Text: \"\"What do you need in order to train a model?\"\"\nAnswer: \"\"You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer).\"\"\n\n\nRelevant Context:\n\nFrom this picture we can now see some fundamental things about training a deep learning model:\n\nA model cannot be created without data.\nA model can only learn to operate on the patterns seen in the input data used to train it.\nThis learning approach only creates predictions, not recommended actions.\nIt’s not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren’t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n\n\n\nMachine learning is a discipline where we define a program not by writing it entirely ourselves, but by learning from data. Deep learning is a specialty within machine learning that uses neural networks with multiple layers. Image classification is a representative example (also known as image recognition). We start with labeled data; that is, a set of images where we have assigned a label to each image indicating what it represents. Our goal is to produce a program, called a model, which, given a new image, will make an accurate prediction regarding what that new image represents.\nEvery model starts with a choice of architecture, a general template for how that kind of model works internally. The process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data. In order to define how well a model does on a single prediction, we need to define a loss function, which determines how we score a prediction as good or bad.\n\nAnalysis:\nSome of the methods did retrieve the first paragraph and bulleted list, however the key paragraph “Every model starts with a choice of architecture…” was not retrieved by any semantic search method.\nConclusion:\nI would expect that increasing the chunk size would result in semantic search methods retrieving the relevant context.\nTags: chunking strategy\n\n\nQuestion 28\n\nprint_data(1)\n\nChapter, Question Number: 1 28\nQuestion Text: \"\"Are image models only useful for photos?\"\"\nAnswer: \"\"Nope! Image models can be useful for other types of images like sketches, medical data, etc.\nHowever, a lot of information can be represented as images . For example, a sound can be converted into a spectrogram, which is a visual interpretation of the audio. Time series (ex: financial data) can be converted to image by plotting on a graph. Even better, there are various transformations that generate images from time series, and have achieved good results for time series classification. There are many other examples, and by being creative, it may be possible to formulate your problem as an image classification problem, and use pretrained image models to obtain state-of-the-art results!\"\"\n\n\nRelevant Context:\n\nAn image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks.\n\nAnalysis:\nI was surprised that semantic search did not find the relevant context for this question. Some methods, like CS_F (Top-5 3-Paragraph Chunks) did at least retrieve a chunk from the correct section in the chapter (“Image Recognizers Can Tackle Non-Image Tasks”) but those chunks didn’t contain relevant context.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\n\nChapter 2\n\nQuestion 1\n\nprint_data(2)\n\nChapter, Question Number: 2 1\nQuestion Text: \"\"Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\"\"\nAnswer: \"\"Working with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\"\"\n\n\nRelevant Context:\n\nThis can result in disaster! For instance, let’s say we really were rolling out a bear detection system that will be attached to video cameras around campsites in national parks, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded there would be all kinds of problems in practice, such as:\n\nWorking with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\n\n\nAnalysis:\nI was expecting semantic search to find similarity between “work poorly in production” and phrases in the relevant context such as “rolling out” and “problems in practice”. The retrieved context focused on other sections in the chapter talking about other ways your ML system could go awry and ways to avoid those situations.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 4\n\nprint_data(3)\n\nChapter, Question Number: 2 4\nQuestion Text: \"\"In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\"\"\nAnswer: \"\"The predictions of the model could be reviewed by human experts for them to evaluate the results and determine what is the best next step. This is especially true for applying machine learning for medical diagnoses. For example, a machine learning model for identifying strokes in CT scans can alert high priority cases for expedited review, while other cases are still sent to radiologists for review. Or other models can also augment the medical professional’s abilities, reducing risk but still improving efficiency of the workflow. For example, deep learning models can provide useful measurements for radiologists or pathologists.\"\"\n\n\nRelevant Context:\n\nThe ability of deep learning to combine text and images into a single model is, generally, far better than most people intuitively expect. For example, a deep learning model can be trained on input images with output captions written in English, and can learn to generate surprisingly appropriate captions automatically for new images! But again, we have the same warning that we discussed in the previous section: there is no guarantee that these captions will actually be correct.\n\n\nBecause of this serious issue, we generally recommend that deep learning be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely. This can potentially make humans orders of magnitude more productive than they would be with entirely manual methods, and actually result in more accurate processes than using a human alone. For instance, an automatic system can be used to identify potential stroke victims directly from CT scans, and send a high-priority alert to have those scans looked at quickly. There is only a three-hour window to treat strokes, so this fast feedback loop could save lives. At the same time, however, all scans could continue to be sent to radiologists in the usual way, so there would be no reduction in human input. Other deep learning models could automatically measure items seen on the scans, and insert those measurements into reports, warning the radiologists about findings that they may have missed, and telling them about other cases that might be relevant.\n\nAnalysis:\nThe semantic search methods did retrieve context related to the topics of rollout and having a human-in-the-loop, but were not able to retrieve the relevant context needed to answer this question. Some of the retrieved context deviated from the relevant topic, focusing instead on errors in the dataset.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 17\n\nprint_data(4)\n\nChapter, Question Number: 2 17\nQuestion Text: \"\"What is the difference between `item_tfms` and `batch_tfms`?\"\"\nAnswer: \"\"item_tfms are transformations applied to a single data sample x on the CPU. Resize() is a common transform because the mini-batch of input images to a cnn must have the same dimensions. Assuming the images are RGB with 3 channels, then Resize() as item_tfms will make sure the images have the same width and height.\nbatch_tfms are applied to batched data samples (aka individual samples that have been collated into a mini-batch) on the GPU. They are faster and more efficient than item_tfms. A good example of these are the ones provided by aug_transforms(). Inside are several batch-level augmentations that help many models.\"\"\n\n\nRelevant Context:\n\nOur images are all different sizes, and this is a problem for deep learning: we don’t feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size. So, we need to add a transform which will resize these images to the same size. Item transforms are pieces of code that run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we use the Resize transform here:\n\n\nData augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes. For natural photo images such as the ones we are using here, a standard set of augmentations that we have found work pretty well are provided with the aug_transforms function. Because our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the batch_tfms parameter (note that we’re not using RandomResizedCrop in this example, so you can see the differences more clearly; we’re also using double the amount of augmentation compared to the default, for the same reason):\n\nAnalysis:\nMy guess here is that the embedding model doesn’t know how to properly embed the terms item_tfms and batch_tfms because they are parameters to a fastai function. As a result, semantic search doesn’t find the appropriate match and instead all of my semantic search methods just returned random blocks of code from the chapter that contained either item_tfms or batch_tfms.\nConclusion:\nThis type of jargon-specific question might not be suitable for semantic search.\nTags: keyword-based question\n\n\nQuestion 27\n\nprint_data(5)\n\nChapter, Question Number: 2 27\nQuestion Text: \"\"What are the three steps in the deployment process?\"\"\nAnswer: \"\"Manual process – the model is run in parallel and not directly driving any actions, with humans still checking the model outputs.\nLimited scope deployment – The model’s scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised.\nGradual expansion – The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better).\"\"\n\n\nRelevant Context:\n\nWhere possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense. For instance, with our bear classifier a park ranger could have a screen displaying video feeds from all the cameras, with any possible bear sightings simply highlighted in red. The park ranger would still be expected to be just as alert as before the model was deployed; the model is simply helping to check for problems at this point.\n\n\nThe second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time-constrained trial of the model-driven approach. Rather than rolling our bear classifier out in every national park throughout the country, we could pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out.\n\n\nThen, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process. For instance, if the number of bear alerts doubles or halves after rollout of the new system in some location, we should be very concerned. Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and ensure that your regular reporting includes that information.\n\nAnalysis:\nAll of the semantic search methods retrieved context that was related to deployment, and some even retrieved the HTML image tag for the relevant “Deployment process”, but none of them retrieved the relevant paragraphs. It’s interesting to note that 1-Paragraph chunk methods retrieved the “Deployment process” image tag but non of the 3-Paragraph chunk methods retrieved any chunks from the correct section “## How to avoid disaster”\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\n\nChapter 4\n\nQuestion 10\n\nprint_data(6)\n\nChapter, Question Number: 4 10\nQuestion Text: \"\"What is broadcasting?\"\"\nAnswer: \"\"Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.\"\"\n\n\nRelevant Context:\n\nTake another look at our function mnist_distance, and you’ll see we have there the subtraction (a-b). The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use broadcasting. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.\n\n\nAfter broadcasting so the two argument tensors have the same rank, PyTorch applies its usual logic for two tensors of the same rank: it performs the operation on each corresponding element of the two tensors, and returns the tensor result. For instance:\n\nAnalysis:\nAll of my semantic search approaches retrieved chunks from the correct chapter section (“Computing Metrics Using Broadcasting”) but not the two relevant chunks needed to answer the question. This might be a scenario where including the markdown heading in each chunk’s text distracts the search from finding relevant context.\nConclusion:\nI’m going to attribute the lack of retrieval to chunking strategy.\nTags: chunking strategy\n\n\nQuestion 12\n\nprint_data(7)\n\nChapter, Question Number: 4 12\nQuestion Text: \"\"What is SGD?\"\"\nAnswer: \"\"SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\"\"\n\n\nRelevant Context:\n\nTo be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\nStep (that is, change) all the weights based on that calculation.\nGo back to the step 2, and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\n\n\nWe need to define first what we mean by “best.” We define this precisely by choosing a loss function, which will return a value based on a prediction and a target, where lower values of the function correspond to “better” predictions. It is important for loss functions to return lower values when predictions are more accurate, as the SGD procedure we defined earlier will try to minimize this loss. For continuous data, it’s common to use mean squared error:\n\n\nAs we’ve seen, we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some loss function that represents how good our model is. That is because the gradients are a measure of how that loss function changes with small tweaks to the weights.\n\n\nLooking good! We’re already about at the same accuracy as our “pixel similarity” approach, and we’ve created a general-purpose foundation we can build on. Our next step will be to create an object that will handle the SGD step for us. In PyTorch, it’s called an optimizer.\n\nAnalysis:\nThere are 4 chunks in the chapter needed to answer the question in a way that matches the gold standard. Each of the 4 chunks are located in a different section of the chapter (“Stochastic Gradient Descent (SGD)”, “An End-to-End SGD Example”, “The MNIST Loss Function”, “Putting It All Together”). Three of the semantic search methods I used retrieved only one of these four chunks.\nConclusion:\nI consider this a very difficult question to answer because there are at least 4 chunks needed to answer it and these chunks are spread out across the chapter. I’m attributing this error to chunking strategy.\nTags: chunking strategy\n\n\n\nChapter 8\n\nQuestion 14\n\nprint_data(8)\n\nChapter, Question Number: 8 14\nQuestion Text: \"\"What does x[:,0] return?\"\"\nAnswer: \"\"The user ids\"\"\n\n\nRelevant Context:\n\nNote that the input of the model is a tensor of shape batch_size x 2, where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors:\n\nAnalysis:\nSimilar to Chapter 2 Question 17, this question is about a specific code syntax, and maybe the embedding model didn’t know how to correctly embed those tokens if it wasn’t trained on PyTorch code?\nConclusion:\nThis type of jargon-specific question might not be suitable for semantic search.\nTags: keyword-based question\n\n\nQuestion 18\n\nprint_data(9)\n\nChapter, Question Number: 8 18\nQuestion Text: \"\"What is the use of bias in a dot product model?\"\"\nAnswer: \"\"A bias will compensate for the fact that some movies are just amazing or pretty bad. It will also compensate for users who often have more positive or negative recommendations in general.\"\"\n\n\nRelevant Context:\n\nThis is a reasonable start, but we can do better. One obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don’t really have any way to say whether most people like it.\n\n\nThat’s because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. So first of all, let’s adjust our model architecture:\n\nAnalysis:\nI can’t say for sure, but I feel like there’s something about how the relevant context is worded that caused a lack of similarity between the question and context embeddings. Neither paragraph chunks mention the word “bias” (the second paragraph uses the word “biases”) and the answer is not explicitly stated in a single sentence (i.e. “in a dot product model the bias encodes how some users are more positive or negative in their recommendations or how some movies are just plain better or worse than others”). I don’t have a strong intuition about embeddings yet and perhaps what I am suggesting is more applicable to a keyword-based search.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\n\nChapter 9\n\nQuestion 5\n\nprint_data(10)\n\nChapter, Question Number: 9 5\nQuestion Text: \"\"How do entity embeddings reduce memory usage and speed up neural networks?\"\"\nAnswer: \"\"Especially for large datasets, representing the data as one-hot encoded vectors can be very inefficient (and also sparse). On the other hand, using entity embeddings allows the data to have a much more memory-efficient (dense) representation of the data. This will also lead to speed-ups for the model.\"\"\n\n\nRelevant Context:\n\nEntity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables… [It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit… As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.\n\nAnalysis:\nAfter re-reading the question, relevant context and the gold standard answer dozens of items, I have come to conclude that this question is unanswerable given the chapter text. The questions asks how entity embeddings reduce memory usage and speed-up neural networks, but the only relevant context I found in the chapter states that “Entity embedding not only reduces memory usage and speeds up neural networks…” and doesn’t explain how, as stated in the gold standard answer.\nYou could argue that the memory efficiency/inefficiency of embeddings and one-hot encoded vectors has already been addressed in Chapter 8 in the following paragraphs:\n\nIf we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number one—we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an embedding.\njargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix.\n\nHowever, I am not searching the entire textbook for each question, I am isolating the search for Chapter 9 questions to the Chapter 9 text.\nI’ll also point out that for the keyword-based results, I accepted the retrieved context as sufficient to answer the question, but upon reflection I am changing my opinion on that.\nConclusion:\nThis question is unanswerable given just the Chapter 9 text.\nTags: unanswerable\n\n\nQuestion 9\n\nprint_data(11)\n\nChapter, Question Number: 9 9\nQuestion Text: \"\"Summarize what a decision tree algorithm does.\"\"\nAnswer: \"\"The basic idea of what a decision tree algorithm does is to determine how to group the data based on “questions” that we ask about the data. That is, we keep splitting the data based on the levels or values of the features and generate predictions based on the average target value of the data points in that group. Here is the algorithm:\nLoop through each column of the dataset in turn\nFor each column, loop through each possible level of that column in turn\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable)\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group\nAfter looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group\nContinue this process recursively, and until you have reached some stopping criterion for each group — for instance, stop splitting a group further when it has only 20 items in it.\"\"\n\n\nRelevant Context:\n\nLet’s consider how we find the right questions to ask. Of course, we wouldn’t want to have to create all these questions ourselves—that’s what computers are for! The basic steps to train a decision tree can be written down very easily:\n\nLoop through each column of the dataset in turn.\nFor each column, loop through each possible level of that column in turn.\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group.\nAfter looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.\nContinue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it.\n\n\nAnalysis:\nOnly one of the semantic search methods retrieved context that was close to the desired context in the chapter:\n\nDecision tree ensembles, as the name suggests, rely on decision trees. So let’s start there! A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a “yes” and a “no” branch, as shown in <>. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required.\n\nHowever, that paragraph doesn’t address everything that’s included in the gold standard answer so I didn’t consider that as a successful retrieval.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 13\n\nprint_data(12)\n\nChapter, Question Number: 9 13\nQuestion Text: \"\"How are mse, samples, and values calculated in the decision tree drawn in this chapter?\"\"\nAnswer: \"\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"\"\n\n\nRelevant Context:\n\nThe top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remember that unless you see m_rmse, or a root mean squared error, then the value you are looking at is before taking the square root, so it is just the average of the square of the differences.) We can also see that there are 404,710 auction records in this group—that is the total size of our training set. The final piece of information shown here is the decision criterion for the best split that was found, which is to split based on the coupler_system column.\n\n\nMoving down and to the left, this node shows us that there were 360,847 auction records for equipment where coupler_system was less than 0.5. The average value of our dependent variable in this group is 10.21. Moving down and to the right from the initial model takes us to the records where coupler_system was greater than 0.5.\n\n\nThe bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node containing records where coupler_system was greater than 0.5. The average value here is 9.21, so we can see the decision tree algorithm did find a single binary decision that separated high-value from low-value auction results. Asking only about coupler_system predicts an average value of 9.21 versus 10.1.\n\n\nReturning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether YearMade is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, based on coupler_system and YearMade) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly.\n\nAnalysis:\nThe gold standard answer is not explicitly stated (or even paraphrased) in the relevant context but serves as a summary of it. Especially without the image, I find that this is a difficult question to answer just based on the chapter’s text—you have to synthesize the information presented in both the text and the image and then summarize the overall process of how MSE, samples and values are calculated.\nOne of the semantic search methods did retrieve the first paragraph of the relevant context (“The top node represents the initial model…”) but I didn’t consider that sufficient to answer the question. Based just on that question (without the image and without the subsequent paragraphs) I would arrive at the conclusion that MSE, samples and values are calculated in the top node, and would not include the relevant context about traversing the tree, or how there are nodes other than the top one in the tree which contain this information.\nI’m considering removing this question from the evals because of these reasons, but will keep it for now in case the LLM can deduce the correct answer given the first paragraph of context.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. I attribute this error to this question requiring the image to answer.\nTags: requires image\n\n\nQuestion 14\n\nprint_data(13)\n\nChapter, Question Number: 9 14\nQuestion Text: \"\"How do we deal with outliers, before building a decision tree?\"\"\nAnswer: \"\"Finding out of domain data (Outliers)\nSometimes it is hard to even know whether your test set is distributed in the same way as your training data or, if it is different, then what columns reflect that difference. There’s actually a nice easy way to figure this out, which is to use a random forest!\nBut in this case we don’t use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set.\"\"\n\n\nRelevant Context:\n\nSometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There’s actually an easy way to figure this out, which is to use a random forest!\n\n\nBut in this case we don’t use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set. To see this in action, let’s combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance:\n\nAnalysis:\nNone of the semantic search methods retrieved context from the correct section in the chapter (“Finding Out-of-Domain Data”). I don’t have much intuition or insight into why, but I would think that it has to do with the embeddings of “outliers” being dissimilar to the embeddings of “Out-of-Domain” and “distributed.”\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 15\n\nprint_data(14)\n\nChapter, Question Number: 9 15\nQuestion Text: \"\"How do we handle categorical variables in a decision tree?\"\"\nAnswer: \"\"We convert the categorical variables to integers, where the integers correspond to the discrete levels of the categorical variable. Apart from that, there is nothing special that needs to be done to get it to work with decision trees (unlike neural networks, where we use embedding layers).\"\"\n\n\nRelevant Context:\n\nCategorify is a TabularProc that replaces a column with a numeric categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing. These two transforms are needed for nearly every tabular dataset you will use, so this is a good starting point for your data processing:\n\n\nWe can see that the data is still displayed as strings for categories (we only show a few columns here because the full table is too big to fit on a page):\n\n\nHowever, the underlying items are all numeric:\n\n\nThe conversion of categorical columns to numbers is done by simply replacing each unique level with a number. The numbers associated with the levels are chosen consecutively as they are seen in a column, so there’s no particular meaning to the numbers in categorical columns after conversion. The exception is if you first convert a column to a Pandas ordered category (as we did for ProductSize earlier), in which case the ordering you chose is used. We can see the mapping by looking at the classes attribute:\n\nAnalysis:\nThe relevant context needed to answer this question contains paragraphs that do not appear consecutively in the chapter and are interweaved with chunks of code. No single sentence (or paragraph) contains the information presented in the gold standard answer tha:\n\nwe convert the categorical variables to integers, where the integers correspond to the discrete levels of the categorical variable.\n\nFor these reasons, I think semantic search failed to retrieve any paragraph from the correct section (“Using TabularPandas and TabularProc”).\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. I attribute this to chunking strategy.\nTags: chunking strategy\n\n\nQuestion 18\n\nprint_data(15)\n\nChapter, Question Number: 9 18\nQuestion Text: \"\"If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\"\"\nAnswer: \"\"A higher n_estimators mean more decision trees are being used. However, since the trees are independent of each other, using higher n_estimators does not lead to overfitting.\"\"\n\n\nRelevant Context:\n\nNote that, unlike with random forests, with this approach there is nothing to stop us from overfitting. Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others. But in a boosted ensemble, the more trees you have, the better the training error becomes, and eventually you will see overfitting on the validation set.\n\n\nOne of the most important properties of random forests is that they aren’t very sensitive to the hyperparameter choices, such as max_features. You can set n_estimators to as high a number as you have time to train—the more trees you have, the more accurate the model will be. max_samples can often be left at its default, unless you have over 200,000 data points, in which case setting it to 200,000 will make it train faster with little impact on accuracy. max_features=0.5 and min_samples_leaf=4 both tend to work well, although sklearn’s defaults work well too.\n\nAnalysis:\nBoth paragraphs are needed to answer the question and they are not located next to each other in the chapter. The first paragraph is from the “Boosting” section, and the second paragraph is from the “Creating a Random Forest” section. Four of my semantic search methods retrieved the first paragraph (corresponding to the “does not lead to overfitting” part of the gold standard answer) but none of the methods retrieved the second paragraph which corresponds to the “a higher n_estimators mean more decision trees are being used” part of the answer.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. I’m attributing this error to chunking strategy.\nTags: chunking strategy\n\n\nQuestion 23\n\nprint_data(16)\n\nChapter, Question Number: 9 23\nQuestion Text: \"\"What's the purpose of removing unimportant variables?\"\"\nAnswer: \"\"Sometimes, it is better to have a more interpretable model with less features, so removing unimportant variables helps in that regard.\"\"\n\n\nRelevant Context:\n\nWe’ve found that generally the first step to improving a model is simplifying it—78 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain.\n\n\nIt seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let’s try just keeping those with a feature importance greater than 0.005:\n\nAnalysis:\nIn order to match the content of the gold standard answer, both of these paragraphs are needed.\nNone of my semantic search methods retrieved these paragraphs. Interesting to note, three of the methods did retrieve the following paragraph from a different section (“Removing Redundant Features”) but the question is about unimportant variables, not redundant ones (though you could argue that redundant variables are unimportant). Regardless, just this one paragraph is not sufficient to match the content in the gold standard answer (it doesn’t mention anything about importance):\n\nOne thing that makes this harder to interpret is that there seem to be some variables with very similar meanings: for example, ProductGroup and ProductGroupDesc. Let’s try to remove any redundent features.\n\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\n\nChapter 10\n\nQuestion 7\n\nprint_data(17)\n\nChapter, Question Number: 10 7\nQuestion Text: \"\"How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\"\"\nAnswer: \"\"By learning how to predict the next word of a movie review, the model better understands the language style and structure of the text classification dataset and can, therefore, perform better when fine-tuned as a classifier.\"\"\n\n\nRelevant Context:\n\nOne reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model. For instance, for the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached. Since there are 25,000 labeled reviews in the training set and 25,000 in the validation set, that makes 100,000 movie reviews altogether. We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles; this will result in a language model that is particularly good at predicting the next word of a movie review.\n\nAnalysis:\nThis question is asking about a specific paragraph in the chapter which explicitly states the “50,000 additional movie reviews.” None of my semantic search methods retrieved this paragraph.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. My guess for why is that this question requires the keyword “50,000” to be in the answer.\nTags: keyword-based question\n\n\nQuestion 8\n\nprint_data(18)\n\nChapter, Question Number: 10 8\nQuestion Text: \"\"What are the three steps to prepare your data for a language model?\"\"\nAnswer: \"\"Tokenization\nNumericalization\nLanguage model DataLoader\"\"\n\n\nRelevant Context:\n\nEach of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are:\n\n\n\nTokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model) Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab\nLanguage model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required\nLanguage model creation:: We need a special kind of model that does something we haven’t seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the <>, but for now, you can think of it as just another deep neural network.\n\n\nAnalysis:\nAll of my semantic search methods retrieved the first paragraph introducing the three steps (“Each of the steps necessary…The steps are:” but did not retrieve the very next paragraph which contained information about the three steps.\nConclusion:\nI attribute this error to chunking strategy.\nTags: chunking strategy\n\n\nQuestion 9\n\nprint_data(19)\n\nChapter, Question Number: 10 9\nQuestion Text: \"\"What is \"\"tokenization\"\"? Why do we need it?\"\"\nAnswer: \"\"Tokenization is the process of converting text into a list of words. It is not as simple as splitting on the spaces. Therefore, we need a tokenizer that deals with complicated cases like punctuation, hypenated words, etc.\"\"\n\n\nRelevant Context:\n\nTokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\n\nAnalysis:\nWhile all of my sementic search methods retrieved context related to tokenization, none of them retrieved the relevant context that defined the term.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 12\n\nprint_data(20)\n\nChapter, Question Number: 10 12\nQuestion Text: \"\"List four rules that fastai applies to text during tokenization.\"\"\nAnswer: \"\"Here are all the rules:\nfix_html :: replace special HTML characters by a readable version (IMDb reviews have quite a few of them for instance) ;\nreplace_rep :: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it’s repeated, then the character ;\nreplace_wrep :: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it’s repeated, then the word ;\nspec_add_spaces :: add spaces around / and # ;\nrm_useless_spaces :: remove all repetitions of the space character ;\nreplace_all_caps :: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\nreplace_maj :: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\nlowercase :: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\"\"\n\n\nRelevant Context:\n\nHere is a brief summary of what each does:\n\n\n\nfix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\nreplace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character\nreplace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the word\nspec_add_spaces:: Adds spaces around / and #\nrm_useless_spaces:: Removes all repetitions of the space character\nreplace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\nreplace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\nlowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)\n\n\nAnalysis:\nNone of my semantic search methods retrieved the relevant context, although three of them retrieved context related to this topic.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 13\n\nprint_data(21)\n\nChapter, Question Number: 10 13\nQuestion Text: \"\"Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\"\"\nAnswer: \"\"We can expect that repeated characters could have special or different meaning than just a single character. By replacing them with a special token showing the number of repetitions, the model’s embedding matrix can encode information about general concepts such as repeated characters rather than requiring a separate token for every number of repetitions of every character.\"\"\n\n\nRelevant Context:\n\nFor instance, the rules will replace a sequence of four exclamation points with a special repeated character token, followed by the number four, and then a single exclamation point. In this way, the model’s embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization.\n\nAnalysis:\nNone of my semantic search methods retrieved the relevant context. Three approaches did get close, as they retrieved the following chunk from the same section in the chapter:\n\nHere is a brief summary of what each does:\n\nfix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\nreplace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character\nreplace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the word\nspec_add_spaces:: Adds spaces around / and #\nrm_useless_spaces:: Removes all repetitions of the space character\nreplace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\nreplace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\nlowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)\n\n\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 15\n\nprint_data(22)\n\nChapter, Question Number: 10 15\nQuestion Text: \"\"Why might there be words that are replaced with the \"\"unknown word\"\" token?\"\"\nAnswer: \"\"If all the words in the dataset have a token associated with them, then the embedding matrix will be very large, increase memory usage, and slow down training. Therefore, only words with more than min_freq occurrence are assigned a token and finally a number, while others are replaced with the “unknown word” token.\"\"\n\n\nRelevant Context:\n\nOur special rules tokens appear first, and then every word appears once, in frequency order. The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn’t enough data to train useful representations for rare words. However, this last issue is better handled by setting min_freq; the default min_freq=3 means that any word appearing less than three times is replaced with xxunk.\n\nAnalysis:\nThe closest any of the methods got to retrieving the relevant context was by retrieving the following chunk that comes from the same section in the chapter:\n\nHere are some of the main special tokens you’ll see:\n\nxxbos:: Indicates the beginning of a text (here, a review)\nxxmaj:: Indicates the next word begins with a capital (since we lowercased everything)\nxxunk:: Indicates the word is unknown\n\n\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. However, I’m not sure why this is the case.\nTags: unknown failure\n\n\nQuestion 18\n\nprint_data(23)\n\nChapter, Question Number: 10 18\nQuestion Text: \"\"What does an embedding matrix for NLP contain? What is its shape?\"\"\nAnswer: \"\"It contains vector representations of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens.\"\"\n\n\nRelevant Context:\n\n\nMake a list of all possible levels of that categorical variable (we’ll call this list the vocab).\nReplace each level with its index in the vocab.\nCreate an embedding matrix for this containing a row for each level (i.e., for each item of the vocab).\nUse this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)\n\n\n\nOur vocab will consist of a mix of common words that are already in the vocabulary of our pretrained model and new words specific to our corpus (cinematographic terms or actors names, for instance). Our embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words we won’t have anything, so we will just initialize the corresponding row with a random vector.\n\n\n\n\nAnalysis:\nThe relevant context from the chapter is sufficient to answer the question in a way that matches the gold standard. Previous knowledge of embeddings (outside of this chapter) is required to know that there is such a thing as embedding_size.\nConclusion:\nTo be consistent, I’ll remove this question from my evals since it cannot be fully answered by the text in the relevant chapter. That being said, I will be evaluating an LLM on this and other “unanswerable” questions as a capable LLM will likely be able to answer some of them.\nTags: unanswerable\n\n\n\nChapter 13\n\nQuestion 4\n\nprint_data(24)\n\nChapter, Question Number: 13 4\nQuestion Text: \"\"What is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\"\"\nAnswer: \"\"A zero matrix.\"\"\n\n\nRelevant Context:\n\nNow we’re going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we’ll add them up, like so:\n\n\nim3_t = tensor(im3)\nim3_t[0:3,0:3] * top_edge\n     \ntensor([[-0., -0., -0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n(im3_t[0:3,0:3] * top_edge).sum()\ntensor(0.)\nNot very interesting so far—all the pixels in the top-left corner are white. But let’s pick a couple of more interesting spots:\n\nAnalysis:\nThis is a challenging question to answer as it requires context that’s a mix of plain text and code. This relevant context is equal to 4 separate paragraphs which is larger than the chunk size I’m using.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. Perhaps a different chunking strategy would help.\nTags: difficult question, chunking strategy\n\n\nQuestion 5\n\nprint_data(25)\n\nChapter, Question Number: 13 5\nQuestion Text: \"\"What is \"\"padding\"\"?\"\"\nAnswer: \"\"Padding is the additional pixels that are added around the outside of the image, allows the kernel to be applied to the edge of the image for a convolution.\"\"\n\n\nRelevant Context:\n\nIt would be nice to not lose those two pixels on each axis. The way we do that is to add padding, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added.\n\nAnalysis:\nAll of my semantic search methods retrieved context from the section titled “Strides and Padding” instead of the section where the definition was found, “Convolutions in PyTorch”.\nConclusion:\nIncluding the header in the chunk text might have distracted semantic search from the retrieving relevant context.\nTags: chunking strategy\n\n\nQuestion 20\n\nprint_data(26)\n\nChapter, Question Number: 13 20\nQuestion Text: \"\"How is a color image represented as a tensor?\"\"\nAnswer: \"\"It is a rank-3 tensor of shape (3, height, width)\"\"\n\n\nRelevant Context:\n\nOne batch contains 64 images, each of 1 channel, with 28×28 pixels. F.conv2d can handle multichannel (i.e., color) images too. A channel is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions [channels, rows, columns].\n\nAnalysis:\nAll of my semantic search methods retrieved irrelevant context from the section titled “Color Image” but not from the section containing the relevant context, “Convolutions in PyTorch”.\nConclusion:\nIncluding the header in the chunk text might have distracted semantic search from the retrieving relevant context.\nTags: chunking strategy\n\n\nQuestion 22\n\nprint_data(27)\n\nChapter, Question Number: 13 22\nQuestion Text: \"\"What method can we use to see that data in DataLoaders?\"\"\nAnswer: \"\"show_batch\"\"\n\n\nRelevant Context:\n\nRemember, it’s always a good idea to look at your data before you use it:\ndls.show_batch(max_n=9, figsize=(4,4))\n\nAnalysis:\nThe relevant context shown was the only occurence of show_batch in the chapter and all of my semantic search methods failed to retrieve it.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer. I attribute this error to this question requiring the keyword “show_batch” in the answer.\nTags: keyword-based question\n\n\nQuestion 24\n\nprint_data(28)\n\nChapter, Question Number: 13 24\nQuestion Text: \"\"Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\"\"\nAnswer: \"\"With the first layer, if the kernel size is 3x3, with four output filters, then nine pixels are being used to produce 8 output numbers so there is not much learning since input and output size are almost the same. Neural networks will only create useful features if they’re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer.\"\"\n\n\nRelevant Context:\n\nAs we discussed, we generally want to double the number of filters each time we have a stride-2 layer. One way to increase the number of filters throughout our network is to double the number of activations in the first layer–then every layer after that will end up twice as big as in the previous version as well.\n\n\nBut there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3×3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn’t really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they’re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs.\n\n\nTo fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features:\n\nAnalysis:\nThis is a complex topic and the relevant context needed to answer this question is relatively large. To match the gold standard answer, the three paragraphs must be summarized as the answer is not explicitly or simply stated.\nConclusion:\nWhile this question may not be unanswerable by semantic search in general, the methods I used certainly found it impossible to answer.\nTags: difficult question"
  },
  {
    "objectID": "posts/2024-10-22-fastbookRAG-cs-error-analysis/index.html#final-thoughts",
    "href": "posts/2024-10-22-fastbookRAG-cs-error-analysis/index.html#final-thoughts",
    "title": "Conducting a Question-by-Question Error Analysis on Semantic Search Results",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere is a summary of tags across the 29 questions where the Answer Rate for all of my semantic search methods was 0% (i.e. none of the methods retrieved the context needed to answer the question):\n\n\n\nTag\nCount\n\n\n\n\nunknown failure\n12\n\n\nchunking strategy\n9\n\n\nkeyword-based question\n4\n\n\ndifficult question\n2\n\n\nunanswerable\n2\n\n\nrequires image\n1\n\n\n\nThe most common error was due to unknown failure (12) where I didn’t have a guess for why semantic search failed. I attribute this to my lack of experience using embeddings.\nThe next most common error type was chunking strategy (9). These were situations where the relevant context needed to match the gold standard answer was longer than 3 paragraphs or spread out across multiple sections in the chapter.\nThere were 4 keyword-based questions where the question and relevant context contained a specific keyword (like show_batch or “50,000”). I felt these were better suited for a keyword-based search.\nThe relevant context for the 2 difficult questions did not explicitly state/paraphrase the gold standard answer and required inferring/summarization. With the right prompting, a capable LLM could handle this correctly.\nThe 2 unanswerable questions were those where the corresponding chapter text did not contain the answer, and required external knowledge (including other chapters of the book).\nFinally, 1 question required an image to achieve the gold standard answer.\nI’m curious to see for which of these questions keyword-based search yielded better results (and vice versa). I’ll be analyzing that next.\nHere is a summary of how many 0% Answer Rate questions there are for each chapter. Semantic search struggled considerably in retrieving relevant context for Chapter 9 and 10 questions.\n\n\n\nChapter\n# of Questions with 0% Answer Rate\n% of Chapter Questions\n\n\n\n\n9\n7\n25%\n\n\n10\n7\n33%\n\n\n13\n5\n15%\n\n\n2\n4\n15%\n\n\n1\n2\n7%\n\n\n4\n2\n6%\n\n\n8\n2\n9%"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "",
    "text": "In this blog post I will train a deep learning model to classify two dependent variables in a tabular dataset. I’m not sure how this works under the hood and will use this as an opportunity to learn.\nI will reference the code and logic from Jeremy Howard’s notebook Linear model and neural net from scratch where he builds a deep learning neural net from scratch."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#plan-of-attack",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#plan-of-attack",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nThere are a few places in the existing code that I’ll need to modify to fit a two-output use case:\n\nPrepping the Data\nCurrently, there are 12 different independent variables and 1 dependent variable. In my use case, I will have 11 independent variables and 2 dependent variables (Age and Survived).\nThe current dependent variables are turned into column vectors as follows:\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\nI’m not sure if I’ll have to do this for two dependent variables as well. It’s something I’ll look into and make sure matches the expected shape before I proceed.\n\n\nInitializing Coefficients\nCurrently, the sizes of each of the layers of the deep neural net are as follows:\nhiddens = [10,10]\nsizes = [n_coeff] + hiddens + [1]\nFor a model with two outputs, the last layer will have size 2:\nsizes = [n_coeff] + hiddens + [2]\n\n\nCalculating Predictions\nWhen calculating predictions for one output, the final line returns torch.sigmoid of the result. In my case, that will work for predicting Survived since it’s between 0 and 1, but Age is a continuous variable that can be greater than 0 so instead of sigmoid I’ll use F.relu and see if that works.\n\n\nCalculating Loss\nHere’s the current loss function:\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\nGiven broadcasting, I think it should work as is, but will test it out and see.\n\n\nCalculating Metrics\nAccuracy is calculated as follows:\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nThis will work for my first output (Survived) but I will need something else like Root Mean Squared Error for the second output Age which is continuous."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#load-and-clean-the-data",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#load-and-clean-the-data",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Load and Clean the Data",
    "text": "Load and Clean the Data\nI’ll reuse the code in Jeremy’s notebook which replaces NA values with the mode of each column, adds a LogFare column, and normalizes all columns.\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 15.4MB/s]\n\n\n\n\n\n\n\n\n\nfrom fastai.tabular.all import *\n\npd.options.display.float_format = '{:.2f}'.format\nset_seed(42)\n\n\n# load the training data\ndf = pd.read_csv(path/'train.csv')\n\n\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.00\n      1\n      0\n      A/5 21171\n      7.25\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.00\n      1\n      0\n      PC 17599\n      71.28\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.00\n      0\n      0\n      STON/O2. 3101282\n      7.92\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.00\n      1\n      0\n      113803\n      53.10\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.00\n      0\n      0\n      373450\n      8.05\n      NaN\n      S\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                      0.00\nPclass                        3.00\nName           Abbing, Mr. Anthony\nSex                           male\nAge                          24.00\nSibSp                         0.00\nParch                         0.00\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\ndf.fillna(modes, inplace=True)\n\n\ndf.isna().sum() # should be all zeros\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-independent-and-dependent-variables",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-independent-and-dependent-variables",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Prepare Independent and Dependent Variables",
    "text": "Prepare Independent and Dependent Variables\n\n# transposed so it has the shape 891, 2\nt_dep = tensor(df.Survived, df.Age).T\n\n\nt_dep.shape\n\ntorch.Size([891, 2])\n\n\n\nt_dep[:5]\n\ntensor([[ 0., 22.],\n        [ 1., 38.],\n        [ 1., 26.],\n        [ 1., 35.],\n        [ 0., 35.]])\n\n\n\nindep_cols = ['SibSp', 'Parch', 'LogFare'] + added_cols\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep.shape\n\ntorch.Size([891, 11])\n\n\n\n# normalize independent variables\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\n\nt_indep[:2]\n\ntensor([[0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n         0.0000, 1.0000],\n        [0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n         0.0000, 0.0000]])"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-training-and-validation-splits",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-training-and-validation-splits",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Prepare Training and Validation Splits",
    "text": "Prepare Training and Validation Splits\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#define-training-functions",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#define-training-functions",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Define Training Functions",
    "text": "Define Training Functions\nNext, I’ll redefine the training functions in Jeremy’s notebook to handle two outputs:\n\nInitializing Coefficients\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\nTo make sure I understand what’s going on in that function with this change, I’ll run each line one at a time:\n\nn_coeff = 11\n\n\nhiddens = [10, 10]\n\n\nsizes = [n_coeff] + hiddens + [2]\n\n\nsizes\n\n[11, 10, 10, 2]\n\n\n\nn = len(sizes)\nn\n\n4\n\n\n\nlayers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n\n\nlayers[0].shape\n\ntorch.Size([11, 10])\n\n\n\nlayers[1].shape\n\ntorch.Size([10, 10])\n\n\n\nlayers[2].shape\n\ntorch.Size([10, 2])\n\n\n\nlayers[2]\n\ntensor([[-0.2763,  0.9819],\n        [-0.2831,  1.3895],\n        [-0.0237,  1.0026],\n        [ 0.6002,  0.6650],\n        [ 0.2466,  0.8107],\n        [-0.0168, -0.5426],\n        [ 0.0158,  1.1835],\n        [ 0.1367,  0.7143],\n        [ 0.0303,  1.1501],\n        [ 0.9985,  0.7530]])\n\n\n\nconsts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\nconsts\n\n[tensor(-0.0256), tensor(-0.0409), tensor(0.0019)]\n\n\n\n\nCalculating Predictions\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.stack([torch.sigmoid(res[:,0]), F.relu(res[:,1])]).T\n\nI’ll test out this function by initializing some random coefficients and using the independent variables to see what kind of outputs I get:\n\ncoeffs = init_coeffs(11)\n\n\n# three layers and three constants\ncoeffs[0][0].shape, coeffs[0][1].shape, coeffs[0][2].shape, len(coeffs[1])\n\n(torch.Size([11, 10]), torch.Size([10, 10]), torch.Size([10, 2]), 3)\n\n\n\nlayers,consts = coeffs\n\n\nn = len(layers)\nn\n\n3\n\n\n\nres = trn_indep\n\n\nres.shape\n\ntorch.Size([713, 11])\n\n\n\n# first layer\nres = res@layers[0] + consts[0]\nres = F.relu(res)\n\n\nres.shape\n\ntorch.Size([713, 10])\n\n\n\n# second layer\nres = res@layers[1] + consts[1]\nres = F.relu(res)\n\n\nres.shape\n\ntorch.Size([713, 10])\n\n\n\n# last layer\nres = res@layers[2] + consts[2]\nres.shape\n\ntorch.Size([713, 2])\n\n\n\n# final treatment of predictions\ntorch.stack([torch.sigmoid(res[:,0]), F.relu(res[:,1])]).T.shape\n\ntorch.Size([713, 2])\n\n\n\n\nCalculating Loss\n\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\n\ncalc_preds(coeffs, trn_indep)[:2]\n\ntensor([[0.7009, 1.4735],\n        [0.7032, 1.3960]], grad_fn=<SliceBackward0>)\n\n\n\ntrn_dep[:2]\n\ntensor([[ 1.0000,  1.0000],\n        [ 0.0000, 40.5000]])\n\n\n\n(calc_preds(coeffs, trn_indep)-trn_dep)[:5]\n\ntensor([[ -0.2991,   0.4735],\n        [  0.7032, -39.1040],\n        [ -0.3204, -25.7099],\n        [  0.6899, -28.6214],\n        [  0.6613,  -1.9130]], grad_fn=<SliceBackward0>)\n\n\n\ncalc_loss(coeffs,trn_indep, trn_dep)\n\ntensor(13.8010, grad_fn=<MeanBackward0>)\n\n\nThe loss function seems to work without modification so I’ll keep it as is.\n\n\nCalculating Metrics\nI am mixing a classification output (Survived) and a regression output (Age) so I’ll have to spit back two different metrics.\n\ndef calc_metrics(coeffs):\n  preds = calc_preds(coeffs, val_indep)\n  acc = (val_dep[:,0].bool()==(preds[:,0]>0.5)).float().mean()\n  rmse = ((preds[:,1]-val_dep[:,1])**2).mean().sqrt()\n  return acc, rmse\n\nI’ll walk through the steps of calculating each metric manually. First, calculating accuracy for the Survived dependent variable:\n\npreds = calc_preds(coeffs, val_indep)\n\n\n(val_dep[:,0])[:5]\n\ntensor([1., 0., 0., 0., 0.])\n\n\n\n(preds[:,0]>0.5)[:5]\n\ntensor([True, True, True, True, True])\n\n\n\n(val_dep[:,0].bool()==(preds[:,0]>0.5))[:5]\n\ntensor([ True, False, False, False, False])\n\n\n\n(val_dep[:,0].bool()==(preds[:,0]>0.5)).float().mean()\n\ntensor(0.4045)\n\n\nThen, calculating RMSE for the Age dependent variable:\n\npreds[:,1][:5]\n\ntensor([1.4279, 1.3960, 1.3844, 1.7690, 1.7181], grad_fn=<SliceBackward0>)\n\n\n\nval_dep[:,1][:5]\n\ntensor([24., 24., 24., 18., 25.])\n\n\n\n((preds[:,1]-val_dep[:,1])**2).mean().sqrt()\n\ntensor(30.2869, grad_fn=<SqrtBackward0>)\n\n\nFinally, testing the written function:\n\ncalc_metrics(coeffs)\n\n(tensor(0.4045), tensor(30.2869, grad_fn=<SqrtBackward0>))\n\n\n\n\nUpdate Coefficients\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\n\nTraining One Epoch\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad():\n      update_coeffs(coeffs, lr)\n      acc, rmse = calc_metrics(coeffs)\n    print((acc.item(), rmse.item()), end=\"\\n\")\n\n\n\nTraining the Model\n\ndef train_model(epochs=30, lr=0.01, n_coeff=11):\n    coeffs = init_coeffs(n_coeff)\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nWith everything (hopefully) in place, I’ll try and train the model and see what happens:\n\ncoeffs = train_model()\n\n(0.38764044642448425, 30.570125579833984)\n(0.38764044642448425, 30.408233642578125)\n(0.38764044642448425, 30.238630294799805)\n(0.40449437499046326, 30.06170654296875)\n(0.40449437499046326, 29.877859115600586)\n(0.40449437499046326, 29.687469482421875)\n(0.40449437499046326, 29.487010955810547)\n(0.40449437499046326, 29.277685165405273)\n(0.40449437499046326, 29.056915283203125)\n(0.40449437499046326, 28.823810577392578)\n(0.40449437499046326, 28.57526397705078)\n(0.40449437499046326, 28.313222885131836)\n(0.40449437499046326, 28.037466049194336)\n(0.40449437499046326, 27.746822357177734)\n(0.40449437499046326, 27.441844940185547)\n(0.40449437499046326, 27.11766242980957)\n(0.40449437499046326, 26.774660110473633)\n(0.40449437499046326, 26.409164428710938)\n(0.40449437499046326, 26.020383834838867)\n(0.40449437499046326, 25.609088897705078)\n(0.40449437499046326, 25.171112060546875)\n(0.40449437499046326, 24.705785751342773)\n(0.40449437499046326, 24.209741592407227)\n(0.40449437499046326, 23.682376861572266)\n(0.40449437499046326, 23.12194061279297)\n(0.40449437499046326, 22.530494689941406)\n(0.40449437499046326, 21.905399322509766)\n(0.40449437499046326, 21.243457794189453)\n(0.40449437499046326, 20.541717529296875)\n(0.42696627974510193, 19.800212860107422)\n\n\nThe model is successfully training, so I take it that the code “works” in the sense that matrix multiplications and loss calculations are being performed. However, the accuracy for predicting Survived and the error for predicting Age are not great. Looking at some predictions:\n\nval_dep[:10]\n\ntensor([[ 1., 24.],\n        [ 0., 24.],\n        [ 0., 24.],\n        [ 0., 18.],\n        [ 0., 25.],\n        [ 0., 34.],\n        [ 1.,  4.],\n        [ 1., 28.],\n        [ 0.,  1.],\n        [ 1., 24.]])\n\n\n\ncalc_preds(coeffs, val_indep)[:10]\n\ntensor([[ 0.5220, 12.6086],\n        [ 0.5365, 13.6072],\n        [ 0.5167, 14.2770],\n        [ 0.5119, 14.4791],\n        [ 0.5162, 14.1781],\n        [ 0.5163, 14.3243],\n        [ 0.5276, 13.5216],\n        [ 0.5296, 13.1540],\n        [ 0.5437, 15.1101],\n        [ 0.5275, 13.6733]], grad_fn=<SliceBackward0>)"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#running-and-recording-trainings",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#running-and-recording-trainings",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Running and Recording Trainings",
    "text": "Running and Recording Trainings\nI’ll reuse code that I implemented in my previous blog post on plotting losses and accuracy, modifying it to capture both accuracy and rmse from the training:\n\ndef one_epoch(coeffs, lr):\n  trn_loss = calc_loss(coeffs, trn_indep, trn_dep)\n  trn_loss.backward()\n  with torch.no_grad():\n    val_loss = calc_loss(coeffs, val_indep, val_dep)\n    update_coeffs(coeffs, lr)\n    acc, rmse = calc_metrics(coeffs)\n  return trn_loss, val_loss, acc, rmse\n\n\ndef train_model(epochs, lr, n_coeff, is_seed=True):\n  if is_seed: torch.manual_seed(442)\n  tl, vl, a, r = [], [], [], []\n  coeffs = init_coeffs(n_coeff)\n  for i in range(epochs):\n    trn_loss, val_loss, acc, rmse = one_epoch(coeffs, lr)\n    tl.append(trn_loss.item())\n    vl.append(val_loss.item())\n    a.append(acc.item())\n    r.append(rmse.item())\n  return tl, vl, a, r\n\n\ndef train_multiple_models(runs=100, epochs=30, lr=4, n_coeff=11, is_seed=False):\n  # initialize recorder object\n  recorder = pd.DataFrame(columns=[\"run\", \"epoch\", \"trn_loss\", \"val_loss\", \"acc\", \"rmse\"])\n  for run in range(runs):\n    # get lists of losses and accuracy\n    tl, vl, a, rmse = train_model(epochs, lr, n_coeff, is_seed)\n    # create list of run and epoch values\n    r = [run] * epochs\n    e = [i for i in range(epochs)]\n    # append new data to recorder DataFrame\n    row = pd.DataFrame(data={\"run\": r, \"epoch\": e, \"trn_loss\": tl, \"val_loss\": vl, \"acc\": a, \"rmse\": rmse})\n    recorder = pd.concat([recorder, row])\n  return recorder\n\n\nrecorder = train_multiple_models()\n\n\nPlotting Training Results\n\nrecorder.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n      rmse\n    \n  \n  \n    \n      0\n      0\n      0\n      13.62\n      13.71\n      0.40\n      1187.97\n    \n    \n      1\n      0\n      1\n      594.61\n      594.02\n      0.60\n      31.69\n    \n    \n      2\n      0\n      2\n      14.51\n      14.62\n      0.60\n      31.69\n    \n    \n      3\n      0\n      3\n      14.50\n      14.61\n      0.60\n      31.69\n    \n    \n      4\n      0\n      4\n      14.50\n      14.61\n      0.60\n      31.69\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nrecorder.tail()\n\n\n\n  \n    \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n      rmse\n    \n  \n  \n    \n      25\n      99\n      25\n      14.47\n      14.58\n      0.60\n      31.69\n    \n    \n      26\n      99\n      26\n      14.46\n      14.58\n      0.60\n      31.69\n    \n    \n      27\n      99\n      27\n      14.46\n      14.58\n      0.60\n      31.69\n    \n    \n      28\n      99\n      28\n      14.46\n      14.58\n      0.60\n      31.69\n    \n    \n      29\n      99\n      29\n      14.46\n      14.58\n      0.60\n      31.69\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nrecorder['acc'].max()\n\n0.5955055952072144\n\n\n\nrecorder['rmse'].min()\n\n13.89392375946045\n\n\n\ndef plot_recorder(recorder):\n  fig, axs = plt.subplots(2, 2)\n\n  (recorder\n   .pivot_table(values='trn_loss', index='epoch', columns='run')\n   .plot(color='green', alpha=0.3, legend=False, title='Training Loss', ax=axs[0, 0]));\n\n  (recorder\n   .pivot_table(values='val_loss', index='epoch', columns='run')\n   .plot(color='red', alpha=0.3, legend=False, title='Validation Loss', ax=axs[0,1]));\n\n  (recorder\n   .pivot_table(values='acc', index='epoch', columns='run')\n   .plot(color='blue', alpha=0.3, legend=False, title='Accuracy', ax=axs[1,0]));\n\n  (recorder\n   .pivot_table(values='rmse', index='epoch', columns='run')\n   .plot(color='orange', alpha=0.3, legend=False, title='RMSE', ax=axs[1,1]));\n\n  for ax in axs.flat:\n    ax.label_outer()\n\n\nplot_recorder(recorder)"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#modifying-the-model",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#modifying-the-model",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Modifying the Model",
    "text": "Modifying the Model\nI think the main takeaway from this exercise so far is that the model is unstable. The training runs don’t look promising—there are sharp changes in loss and RMSE, and the accuracy plateaus quickly. I would hope to see relatively smooth curves in both losses and metrics across epochs.\nJeremy had mentioned in the lesson video that he had to fiddle with some of the constants inside the neural network as well as the learning rate to get a stable training. So with that in mind, I’ll give it a try and adjust those values and see if it makes training more stable.\nThe two things I’ll fiddle with: the arbitrary constants in init_coeffs and the learning rate.\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.5)/sizes[i+1]*6 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.5 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models()\n\n\nplot_recorder(recorder)\n\n\n\n\nIncreasing some of the constants somewhat improved the loss and RMSE and significantly improved the accuracy:\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.8370786309242249, 13.419356346130371)\n\n\n\nrecorder = train_multiple_models(lr=0.03)\n\n\nplot_recorder(recorder)\n\n\n\n\nIncreasing the learning rate significantly changed the way losses and RMSE change over time and seems to have made accuracy more chaotic. The minimum RMSE still remains at around 13:\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.7921348214149475, 13.494928359985352)\n\n\nI’ll continue to increase the constants in the model and the learning rate:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.75)/sizes[i+1]*8 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.75 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.05)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.6404494643211365, 30.712871551513672)\n\n\nI seem to have broken the model, I’ll keep increasing the constants and learning rate one more time and see it if continues to break the training:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-1.0)/sizes[i+1]*10 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*1.00 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.07)\n\n\nplot_recorder(recorder)\n\n\n\n\nOkay continuing to increase them is not helping. I’ll go in the opposite direction—-decreasing the constants and learning rate:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.1)/sizes[i+1]*2 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.05 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.005)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.40449437499046326, 23.53632354736328)\n\n\nThe trainings look MUCH smoother in terms of loss and RMSE, however the accuracy does not improve at all in any of the 100 trainings and the minimum RMSE achieved is not as low as some of the previous configurations. I’ll increase the learning rate and keep the constants as they are:\n\nrecorder = train_multiple_models(lr=0.01)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.40449437499046326, 15.42644214630127)\n\n\nMoving in the right direction for RMSE but still getting a bad accuracy. I’ll increase the learning rate a bit more:\n\nrecorder = train_multiple_models(lr=0.03)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.40449437499046326, 13.523417472839355)\n\n\nSomething about this new model favors Age and penalizes Survived. It’s hard to know what is really going on. I’ll continue to fiddle with the parameters:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.1)/sizes[i+1]*1 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.01)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.5955055952072144, 26.605520248413086)\n\n\nI’ll return to a previous configuration which yielded a pretty high accuracy (79%) and the lowest RMSE so far (13.5).\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.5)/sizes[i+1]*6 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.5 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.1)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.7977527976036072, 12.552786827087402)\n\n\nI could go on and on with changing arbitrary constants and the learning rate, but I’ll stop here as I’m getting a relatively stable training with a high accuracy and low RMSE. I’ll take a look at Age predictions more closely here:\n\npreds = calc_preds(coeffs, val_indep)\n\n\nage_df = pd.DataFrame({'age_actual': val_dep[:,1].numpy(), 'age_pred': preds[:,1].detach().numpy()})\n\n\nage_df.plot();\n\n\n\n\nThe model didn’t really learn anything useful on how to calculate age. It essentially just found some average value and stuck with it for each passenger."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#training-a-regression-model-for-age",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#training-a-regression-model-for-age",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Training a Regression Model for Age",
    "text": "Training a Regression Model for Age\nBefore I close out this blog post with my final thoughts, I want to see how well our model can predict Age as a single output. I’ll rewrite the training functions to handle a single regression output value:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return 100 * torch.sigmoid(res) # assuming age is between 0 and 100\n\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\ndef calc_rmse(coeffs):\n  preds = calc_preds(coeffs, val_indep)\n  rmse = ((preds-val_dep)**2).mean().sqrt()\n  return rmse\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad():\n      update_coeffs(coeffs, lr)\n      rmse = calc_rmse(coeffs)\n    print(rmse.item(), end=\"\\n\")\n\ndef train_model(epochs=30, lr=0.01, n_coeff=11):\n    coeffs = init_coeffs(n_coeff)\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nI’ll re-prepare the data so that the appropriate variables are categorized as independent and dependent:\n\nt_dep = tensor(df.Age)\n\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\n\nval_indep.shape\n\ntorch.Size([178, 11])\n\n\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\n\nval_dep.shape\n\ntorch.Size([178, 1])\n\n\n\ncoeffs = train_model()\n\n40.933284759521484\n20.56239128112793\n17.421009063720703\n15.360074996948242\n14.128280639648438\n13.583489418029785\n13.374798774719238\n13.349427223205566\n13.402131080627441\n13.475241661071777\n13.540617942810059\n13.617292404174805\n13.704315185546875\n13.765442848205566\n13.83024787902832\n13.8984956741333\n13.92748737335205\n13.957006454467773\n13.987039566040039\n14.017572402954102\n14.045609474182129\n14.074031829833984\n14.102828979492188\n14.131990432739258\n14.161505699157715\n14.191366195678711\n13.757979393005371\n13.822358131408691\n13.89020824432373\n13.91904354095459\n\n\n\npreds = calc_preds(coeffs, val_indep)\n\n\nage_df = pd.DataFrame({'age_actual': val_dep.squeeze(1).numpy(), 'age_pred': preds.squeeze(1).detach().numpy()})\n\n\nage_df.plot();\n\n\n\n\nAgain, the model does not seem to learn anything useful about age, it just finds the mean and uses it as the value for each prediction.\n\nval_dep.mean()\n\ntensor(28.7374)\n\n\n\npreds.mode().values[0]\n\ntensor(24.7849, grad_fn=<SelectBackward0>)\n\n\nTo see if I am getting this result just by chance, or if there is something consistent about this model’s Age predictions, I’ll train this model 100 times and plot the result age distribution:\n\ndef train_multiple_models(runs=100, epochs=30, lr=4, n_coeff=11, is_seed=False):\n  recorder = pd.DataFrame()\n  for run in range(runs):\n    coeffs = train_model(epochs, lr, n_coeff)\n    preds = calc_preds(coeffs, val_indep)\n    recorder[run] = preds.squeeze(1).detach().numpy()\n  return recorder\n\n\nrecorder = train_multiple_models(lr=0.01)\n\n\nfig, ax = plt.subplots(1)\n\nrecorder.plot(legend=False, color='black', alpha=0.6, ax=ax);\n\nax.plot(val_dep.squeeze(1).numpy());\n\n\n\n\nIn many trainings (dark black lines between age values of 24 and 25) the model simply finds the mean. There are some training runs where the predicted age has slightly more variability, but it pales in comparison to the variability in actual age. I would imagine that changing the randomly instantiated coefficients’ constants and learning rate would affect this result."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#final-thoughts",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#final-thoughts",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAlthough my model was not successful in correctly prediction both the age of the passenger and whether they survived, I did find this exercise to be helpful. A few takeaways:\n\nCreating a model that predicts more than one variable will require modifying the model as well as the training and validation data structure.\nChanging the arbitrary coefficients inside the deep neural net and the learning rate significantly affects the training stability and classification accuracy.\nA simple deep learning model predicts the mean for continuous variables (based on what I’ve seen here).\n\nI really enjoyed working through this example and feel more comfortable with building and modifying neural net architecture then when I started writing this blog post. I hope you enjoyed it too!"
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "",
    "text": "In this blog post I will use fastai to train a model that predicts more than one target for the Kaggle Titanic dataset.\nI’ve referenced the notebook Multi-target: Road to the Top, Part 4 by Jeremy Howard as well as a derivative notebook Small models + Multi-targets by Kaggle user Archie Tram (in which he creates a test DataLoader to get predictions from the model)."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#plan-of-attack",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#plan-of-attack",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Plan of Attack",
    "text": "Plan of Attack\n\nCreating DataLoaders\nIn Jeremy’s notebook, he is classifying images of plants with two targets: disease and variety of plant.\nHe creates his DataLoaders object as follows:\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock,CategoryBlock),\n    n_inp=1,\n    get_items=get_image_files,\n    get_y = [parent_label,get_variety],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(192, method='squish'),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n).dataloaders(trn_path)\nThere are three blocks: 1 input ImageBlock and 2 output CategoryBlocks. The model gets the outputs with parent_label (for the disease) and a custom function get_variety (which grabs the variety column value of the given image from a DataFrame).\nIn my use case, I will have to follow a similar approach, albeit catered to tabular data.\n\n\nCalculating Losses\nJeremy calculates loss as the sum of the following:\n\nCross-Entropy loss of the disease inputs\nCross-Entropy loss of the variety inputs\n\nI’ll follow a similar approach, except if I use continuous variables as targets I’ll use MSE instead of Cross-Entropy.\n\n\nCalculating Metrics\nSimilar to the loss calculation, I’ll combine the calculation of the metric for each of the two targets. For continuous variables, I’ll use RMSE."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#training-a-multi-target-model",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#training-a-multi-target-model",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Training a Multi-Target Model",
    "text": "Training a Multi-Target Model\nWith a rough plan outlined, I’ll start the training process with loading and cleaning the Titanic dataset.\n\nLoad and Clean Data\n\nfrom fastai.tabular.all import *\n\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 6.18MB/s]\n\n\n\n\n\n\n\n\n\n# load the training data and look at it\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# feature engineering\ndef add_features(df):\n  df['LogFare'] = np.log1p(df['Fare'])\n  df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))\n  df['Family'] = df.SibSp+df.Parch\n  df['Alone'] = df.Family == 0\n  df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')\n  df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n  df['Title'] = df.Title.map(dict(Mr=\"Mr\", Miss=\"Miss\", Mrs=\"Mrs\", Master=\"Master\"))\n\n\n# add the features to our dataframe\nadd_features(df)\n\n\n# view the topmost row of the modes DataFrame\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nLogFare                   2.202765\nDeck                           ABC\nFamily                         0.0\nAlone                         True\nTicketFreq                     1.0\nTitle                           Mr\nName: 0, dtype: object\n\n\n\n# fill missing data with the column's mode\ndf.fillna(modes, inplace=True)\n\n\n# check that we no longer have missing data\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\nLogFare        0\nDeck           0\nFamily         0\nAlone          0\nTicketFreq     0\nTitle          0\ndtype: int64\n\n\n\n# create training and validation index lists\nsplits = RandomSplitter(seed=42)(df)\n\n\n\nCreate DataLoaders\nI’ll take most of the code from the Why you should use a framework notebook by Jeremy, with the following changes:\n\nRemove \"Age\" from cont_names and move it to y_names along with \"Survived\" which will be our two targets.\nSet n_out=2 for the RegressionBlock.\n\nI’ll treat both targets as a regression, as I wasn’t able to provide two DataBlocks for y_block.\nSince I’ve filled in missing values manually, I have removed the FillMissing item from procs.\n\n# create dataloaders object\ndls = TabularPandas(\n    df,\n    splits=splits,\n    procs=[Categorify, Normalize],\n    cat_names=[\"Sex\", \"Pclass\", \"Embarked\", \"Deck\", \"Title\"],\n    cont_names=[\"SibSp\", \"Parch\", \"LogFare\", \"Alone\", \"TicketFreq\", \"Family\"],\n    y_names=[\"Age\", \"Survived\"],\n    y_block=RegressionBlock(n_out=2)\n).dataloaders(path=\".\")\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      Sex\n      Pclass\n      Embarked\n      Deck\n      Title\n      SibSp\n      Parch\n      LogFare\n      Alone\n      TicketFreq\n      Family\n      Age\n      Survived\n    \n  \n  \n    \n      0\n      male\n      1\n      S\n      ABC\n      Mr\n      1.000000e+00\n      -9.897945e-09\n      3.970292\n      2.458140e-08\n      2.0\n      1.000000e+00\n      42.0\n      0.0\n    \n    \n      1\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.230014\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      18.0\n      0.0\n    \n    \n      2\n      male\n      2\n      S\n      ABC\n      Mr\n      1.000000e+00\n      2.000000e+00\n      3.358638\n      2.458140e-08\n      3.0\n      3.000000e+00\n      36.0\n      0.0\n    \n    \n      3\n      male\n      3\n      C\n      ABC\n      Mr\n      1.000000e+00\n      1.000000e+00\n      2.107689\n      2.458140e-08\n      1.0\n      2.000000e+00\n      17.0\n      0.0\n    \n    \n      4\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.351375\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      28.0\n      0.0\n    \n    \n      5\n      female\n      3\n      S\n      ABC\n      Mrs\n      1.000000e+00\n      4.000000e+00\n      3.363842\n      2.458140e-08\n      6.0\n      5.000000e+00\n      45.0\n      0.0\n    \n    \n      6\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.324836\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      23.0\n      0.0\n    \n    \n      7\n      female\n      3\n      C\n      ABC\n      Mrs\n      1.689237e-09\n      -9.897945e-09\n      2.107178\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      24.0\n      1.0\n    \n    \n      8\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.188856\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      39.0\n      1.0\n    \n    \n      9\n      female\n      2\n      C\n      ABC\n      Mrs\n      1.000000e+00\n      -9.897945e-09\n      3.436269\n      2.458140e-08\n      2.0\n      1.000000e+00\n      14.0\n      1.0\n    \n  \n\n\n\n\n\nCreate Loss Function\nIf I understand correctly, we will get 2 columns of predictions, and two variables of targets to compute the loss with:\n\ndef age_loss(pred, yb): return F.mse_loss(pred[:,0], yb[:,0])\ndef survived_loss(pred, yb): return F.mse_loss(pred[:,1], yb[:,1])\n\ndef combine_loss(pred, yb): return age_loss(pred, yb) + survived_loss(pred, yb)\n\n\n\nCreate Metric Function\nI’ll create an RMSE function for each target variable:\n\ndef age_rmse(pred, yb): return torch.sqrt(F.mse_loss(pred[:,0], yb[:,0]))\ndef survived_rmse(pred, yb): return torch.sqrt(F.mse_loss(pred[:,1], yb[:,1]))\n\nrmse_metrics = (age_rmse, survived_rmse)\n\n\nlearn = tabular_learner(dls, loss_func=combine_loss, metrics=rmse_metrics, layers=[10,10], n_out=2)\n\nMost times that I ran the learning rate finder, the loss was steadily increasing from the get-go. I randomly came across the following learning rate regime which looks more stable, so I’ll use the given value.\n\nlearn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=6.309573450380412e-07, valley=0.14454397559165955)\n\n\n\n\n\n\nlearn.fit(20, lr=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      age_rmse\n      survived_rmse\n      time\n    \n  \n  \n    \n      0\n      766.320923\n      554.657837\n      23.431234\n      0.721841\n      00:00\n    \n    \n      1\n      460.486603\n      170.207932\n      13.030014\n      0.590790\n      00:00\n    \n    \n      2\n      335.931213\n      132.264999\n      11.456180\n      0.649899\n      00:00\n    \n    \n      3\n      265.317535\n      116.719322\n      10.778342\n      0.477045\n      00:00\n    \n    \n      4\n      221.392242\n      121.840828\n      11.004195\n      0.441827\n      00:00\n    \n    \n      5\n      192.420349\n      132.113815\n      11.457218\n      0.472019\n      00:00\n    \n    \n      6\n      173.592255\n      120.654694\n      10.943729\n      0.462033\n      00:00\n    \n    \n      7\n      159.223709\n      113.375626\n      10.612040\n      0.519316\n      00:00\n    \n    \n      8\n      148.853653\n      114.346222\n      10.654099\n      0.484549\n      00:00\n    \n    \n      9\n      140.409439\n      109.572639\n      10.437387\n      0.467927\n      00:00\n    \n    \n      10\n      133.942352\n      114.497719\n      10.642965\n      0.590436\n      00:00\n    \n    \n      11\n      129.807709\n      110.892578\n      10.500125\n      0.455730\n      00:00\n    \n    \n      12\n      125.972458\n      112.508110\n      10.570338\n      0.451019\n      00:00\n    \n    \n      13\n      122.350586\n      126.790512\n      11.167099\n      0.512433\n      00:00\n    \n    \n      14\n      119.345764\n      112.307846\n      10.571351\n      0.579465\n      00:00\n    \n    \n      15\n      117.329689\n      113.805359\n      10.628425\n      0.484336\n      00:00\n    \n    \n      16\n      116.328194\n      115.227859\n      10.696632\n      0.475317\n      00:00\n    \n    \n      17\n      115.390640\n      115.162354\n      10.710686\n      0.500142\n      00:00\n    \n    \n      18\n      116.044281\n      125.941689\n      11.149549\n      0.558260\n      00:00\n    \n    \n      19\n      115.501900\n      116.436340\n      10.739085\n      0.500779\n      00:00\n    \n  \n\n\n\nAfter a few epochs, the RMSE values stop improving. The validation loss also fluctuates throughout the training after decreasing for the first three epochs.\n\n\nComparing Predictions to Actuals\nBased on how the training went, I’m not expecting this model to be able to predict Age and Survived very well. I’ll use the validation set to get predictions and then calculate accuracy for Survived and correlation between actuals vs. predictions for Age.\n\npreds, targ = learn.get_preds(dl=dls.valid)\n\n\n\n\n\n\n\n\n\n# Survived accuracy\n(targ[:,1] == (preds[:,1]>0.5)).float().mean()\n\ntensor(0.6348)\n\n\n\ndef corr(x,y): return np.corrcoef(x,y)[0][1]\n\n\n# Age plot\nfig, ax = plt.subplots(1)\n\nax.axis('equal')\nplt.title(f'Predicted Age vs Actual; r: {corr(preds[:,0], targ[:,0]):.2f}')\nax.scatter(preds[:,0], targ[:,0]);\n\n\n\n\nThe model achieved shoddy accuracy (63%) and an uninspiring correlation between predicted and actual age. The model did particularly poorly in predicting ages above 40."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#comparing-to-single-target-models",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#comparing-to-single-target-models",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Comparing to Single-Target Models",
    "text": "Comparing to Single-Target Models\nI’m curious to see how the model performs when I train it for single targets. I’ll train one regression model for Age, another separate regression model for Survived, and see how their results compare to the combined two-target model.\n\nSingle Target: Age\n\n# create dataloaders object\nage_dls = TabularPandas(\n    df,\n    splits=splits,\n    procs=[Categorify, Normalize],\n    cat_names=[\"Sex\", \"Pclass\", \"Embarked\", \"Deck\", \"Title\"],\n    cont_names=[\"SibSp\", \"Parch\", \"LogFare\", \"Alone\", \"TicketFreq\", \"Family\"],\n    y_names=\"Age\",\n    y_block=RegressionBlock()\n).dataloaders(path=\".\")\n\n\nage_learn = tabular_learner(age_dls, metrics=rmse, layers=[10,10])\n\nI ran the learning rate finder 10 times and got similar charts each time, which tells me that something about this model is more stable than my two-target model.\n\nage_learn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=6.309573450380412e-07, valley=0.0831763744354248)\n\n\n\n\n\n\nage_learn.fit(16, lr=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      781.124268\n      233.326263\n      15.275021\n      00:00\n    \n    \n      1\n      454.851532\n      408.981842\n      20.223301\n      00:00\n    \n    \n      2\n      328.806274\n      116.149773\n      10.777281\n      00:00\n    \n    \n      3\n      263.302643\n      119.088097\n      10.912749\n      00:00\n    \n    \n      4\n      219.239166\n      127.125175\n      11.274981\n      00:00\n    \n    \n      5\n      190.565811\n      111.707756\n      10.569189\n      00:00\n    \n    \n      6\n      171.005737\n      113.618858\n      10.659215\n      00:00\n    \n    \n      7\n      157.105713\n      109.284859\n      10.453939\n      00:00\n    \n    \n      8\n      146.396072\n      118.541183\n      10.887661\n      00:00\n    \n    \n      9\n      138.696716\n      107.435219\n      10.365096\n      00:00\n    \n    \n      10\n      132.795654\n      109.071220\n      10.443716\n      00:00\n    \n    \n      11\n      128.642639\n      112.930344\n      10.626869\n      00:00\n    \n    \n      12\n      124.508675\n      107.584816\n      10.372310\n      00:00\n    \n    \n      13\n      121.428909\n      113.099953\n      10.634846\n      00:00\n    \n    \n      14\n      119.856216\n      114.224464\n      10.687585\n      00:00\n    \n    \n      15\n      118.349365\n      109.042511\n      10.442342\n      00:00\n    \n  \n\n\n\nThe validation loss also fluctuates in this model’s training. The RMSE metric also does not really improve after the first couple of epochs. Similar to last time, I’ll plot the predicted age vs actual and calculate the correlation between the two:\n\nage_preds, age_targ = age_learn.get_preds(dl=age_dls.valid)\n\n\n\n\n\n\n\n\n\n# Age plot\nfig, ax = plt.subplots(1)\n\nax.axis('equal')\nplt.title(f'Predicted Age vs Actual; r: {corr(age_preds[:,0], age_targ[:,0]):.2f}')\nax.scatter(age_preds[:,0], age_targ[:,0]);\n\n\n\n\nSurprisingly, the single target Age model does not perform much better than my two-target model. I get a similar correlation, and this model also fails to predict ages above around 40.\n\n\nSingle Target: Survived\nIn Jeremy’s “Why you should use a framework” notebook, he achieves about an 83% accuracy. I’ll use this as a benchmark to compare my model with.\n\n# create dataloaders object\nsurvived_dls = TabularPandas(\n    df,\n    splits=splits,\n    procs=[Categorify, Normalize],\n    cat_names=[\"Sex\", \"Pclass\", \"Embarked\", \"Deck\", \"Title\"],\n    cont_names=[\"SibSp\", \"Parch\", \"LogFare\", \"Alone\", \"TicketFreq\", \"Family\"],\n    y_names=\"Survived\",\n    y_block=RegressionBlock()\n).dataloaders(path=\".\")\n\n\nsurvived_learn = tabular_learner(survived_dls, metrics=rmse, layers=[10,10])\n\n\nsurvived_learn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=0.05754399299621582, valley=0.0063095735386013985)\n\n\n\n\n\n\nsurvived_learn.fit(16, lr=0.02)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      0.250865\n      0.240620\n      0.490530\n      00:00\n    \n    \n      1\n      0.200100\n      0.214276\n      0.462899\n      00:00\n    \n    \n      2\n      0.177398\n      0.150440\n      0.387866\n      00:00\n    \n    \n      3\n      0.163052\n      0.135140\n      0.367615\n      00:00\n    \n    \n      4\n      0.153141\n      0.131269\n      0.362311\n      00:00\n    \n    \n      5\n      0.147007\n      0.133025\n      0.364726\n      00:00\n    \n    \n      6\n      0.143294\n      0.132439\n      0.363922\n      00:00\n    \n    \n      7\n      0.138928\n      0.131754\n      0.362979\n      00:00\n    \n    \n      8\n      0.135169\n      0.128147\n      0.357976\n      00:00\n    \n    \n      9\n      0.133087\n      0.125253\n      0.353910\n      00:00\n    \n    \n      10\n      0.130366\n      0.126195\n      0.355240\n      00:00\n    \n    \n      11\n      0.128971\n      0.130248\n      0.360899\n      00:00\n    \n    \n      12\n      0.127474\n      0.128108\n      0.357922\n      00:00\n    \n    \n      13\n      0.126128\n      0.124583\n      0.352963\n      00:00\n    \n    \n      14\n      0.125103\n      0.125416\n      0.354142\n      00:00\n    \n    \n      15\n      0.123530\n      0.129710\n      0.360152\n      00:00\n    \n  \n\n\n\n\nsurvived_preds, survived_targ = survived_learn.get_preds(dl=survived_dls.valid)\n\n\n\n\n\n\n\n\n\n(survived_targ == (survived_preds>0.5)).float().mean()\n\ntensor(0.8258)\n\n\nI get an accuracy of around 83% as well."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#final-thoughts",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#final-thoughts",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are my takeaways from this experiment:\n\nA single-target regression model predicts Survived better than Age.\nA two-target regression model (Survived and Age) predicts Survived significantly worse than a single-target model (Survived only). Something about introducing an output for Age decreases the model’s performance when predicting survival rate.\nA two-target regression model (Survived and Age) predicts Age with about the same correlation as a single-target model (Age only).\n\nSomething about this dataset (and how the model learns from it) makes Age a poor target for prediction. Perhaps it’s the distribution of ages in the dataset, or the relationship with other columns, that makes it harder for the model to predict it accurately.\nI’m happy and proud that I was able to run this experiment after failing to overcome some errors the first couple of times I tried to train a two-target model earlier this week.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-06-25-dotproductbias/index.html",
    "href": "posts/2024-06-25-dotproductbias/index.html",
    "title": "Comparing PyTorch Embeddings with Custom Embeddings",
    "section": "",
    "text": "In this notebook I’ll work through the following “Further Research” prompt at the end of Chapter 8 of the fastai textbook:\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens (even the type of brackets used in forward has changed!)"
  },
  {
    "objectID": "posts/2024-06-25-dotproductbias/index.html#visual-inspection-of-code",
    "href": "posts/2024-06-25-dotproductbias/index.html#visual-inspection-of-code",
    "title": "Comparing PyTorch Embeddings with Custom Embeddings",
    "section": "Visual Inspection of Code",
    "text": "Visual Inspection of Code\nI’ll start by visually inspecting and annotating the differences between the two functions (I made this visual in Google Slides using the beautiful Ubuntu Mono font):\n\n\n\nVisual inspection of two DotProductBias Modules"
  },
  {
    "objectID": "posts/2024-06-25-dotproductbias/index.html#stepping-through-the-code",
    "href": "posts/2024-06-25-dotproductbias/index.html#stepping-through-the-code",
    "title": "Comparing PyTorch Embeddings with Custom Embeddings",
    "section": "Stepping Through the Code",
    "text": "Stepping Through the Code\nNext, I’ll step through each DotProductBias implementation’s code using real data. I was getting an SSL error when using untar_data(URLs.ML_100k) so I manually downloaded the data.\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\n\n!unzip /content/ml-100k.zip\n\n\npath = Path('/content/ml-100k')\n\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\nratings = ratings.merge(movies)\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      713\n      Wings of the Dove, The (1997)\n      3\n    \n    \n      1\n      788\n      In & Out (1997)\n      2\n    \n    \n      2\n      270\n      Benny & Joon (1993)\n      5\n    \n    \n      3\n      682\n      Searching for Bobby Fischer (1993)\n      5\n    \n    \n      4\n      543\n      Fantasia (1940)\n      4\n    \n    \n      5\n      535\n      Contact (1997)\n      5\n    \n    \n      6\n      463\n      Waiting for Guffman (1996)\n      3\n    \n    \n      7\n      326\n      Man Who Would Be King, The (1975)\n      5\n    \n    \n      8\n      712\n      Around the World in 80 Days (1956)\n      4\n    \n    \n      9\n      804\n      North by Northwest (1959)\n      5\n    \n  \n\n\n\n\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nn_users, n_movies, n_factors\n\n(944, 1665, 5)\n\n\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 2]), torch.Size([64, 1]))\n\n\n\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\n\nCreating User Latent Factors\nThe first line in each implementation creates the (untrained) latent factors for users. For all variable names, I’ll append _emb for the Embedding model implementation and _cp for the create_params implementation.\nThe first difference between the two implementations (other than the obvious that one uses Embedding and the other uses create_params) is that Embedding is given two arguments to determine its size (ni and nf), whereas create_params is given one list with both sizes. I’ll illustrate this by showing the error thrown by each one when called without passing arguments:\n\nuser_factors_emb = Embedding(n_users, n_factors)\nuser_factors_cp = create_params([n_users, n_factors])\n\nuser_factors_emb, user_factors_cp.shape\n\n(Embedding(944, 5), torch.Size([944, 5]))\n\n\n\nEmbedding()\n\nTypeError: Embedding.__init__() missing 2 required positional arguments: 'ni' and 'nf'\n\n\n\ncreate_params()\n\nTypeError: create_params() missing 1 required positional argument: 'size'\n\n\n\n\nCreating User Bias\nThe next line in each model creates the bias object for users:\nThe main difference here is that the output dimension of 1 is specified for the Embedding but only a single size is given to create_params, the reason being that create_params is just a tensor which will return a tensor when indexed, whereas Embedding needs an explicit output feature size. The consequence of this will be seen later on in the forward pass.\n\nuser_bias_emb = Embedding(n_users, 1)\nuser_bias_cp = create_params([n_users])\n\nuser_bias_emb, user_bias_cp.shape\n\n(Embedding(944, 1), torch.Size([944]))\n\n\n\nuser_bias_emb(xb[:,0]).shape, user_bias_cp[xb[:,0]].shape\n\n(torch.Size([64, 1]), torch.Size([64]))\n\n\n\n\nCreating Movie Latent Factors and Bias\nThe next two lines in each model do the same thing but for the movies (items):\n\nmovie_factors_emb = Embedding(n_movies, n_factors)\nmovie_factors_cp = create_params([n_movies, n_factors])\n\nmovie_bias_emb = Embedding(n_movies, 1)\nmovie_bias_cp = create_params([n_movies])\n\nmovie_factors_emb, movie_factors_cp.shape, movie_bias_emb, movie_bias_cp.shape,\n\n(Embedding(1665, 5),\n torch.Size([1665, 5]),\n Embedding(1665, 1),\n torch.Size([1665]))\n\n\n\n\nForward Pass\nThere are two differences in how the forward method is defined when using Embedding versus create_params:\n\nYou have to call an Embedding but index the tensor created by create_params.\nYou have to specify keepdim=True for the dot product in the Embedding model when using sum but don’t need to do so in the create_params model.\n\nThe first bullet point can be illustrated easily:\n\n(\n    movie_factors_emb(xb[:,1]).shape, # call Embeddings to get output (a tensor)\n    movie_factors_cp[xb[:,1]].shape # index `create_params` output (a tensor)\n)\n\n(torch.Size([64, 5]), torch.Size([64, 5]))\n\n\nThe second bullet point is illustrated by not passing keepdim=True to the sum call for the Embedding output product and seeing what happens:\nThe output product here has a single dimension with 64 items in that dimension. In the next line of the forward pass, we will try to add the bias vectors to this product:\n\nres_emb = (user_factors_emb(xb[:,0]) * movie_factors_emb(xb[:,1])).sum(dim=1)\nres_emb.shape\n\ntorch.Size([64])\n\n\n\nres_emb += user_bias_emb(xb[:,0]) + movie_bias_emb(xb[:,1])\n\nRuntimeError: output with shape [64] doesn't match the broadcast shape [64, 64]\n\n\nBut that doesn’t work because the bias Embedding outputs a tensor with dimensions 64 x 1. We need that unit axis at the end in order to allow for tensor addition to take place. keepdim=True preserves that unit axis:\n\nres_emb = (user_factors_emb(xb[:,0]) * movie_factors_emb(xb[:,1])).sum(dim=1, keepdim=True) # unit axis preserved\nres_emb += user_bias_emb(xb[:,0]) + movie_bias_emb(xb[:,1])\nres_emb.shape\n\ntorch.Size([64, 1])\n\n\nWe don’t get this issue when using create_params because we didn’t specify a unit axis to begin with.\nRecall that when creating the user and movie bias in the model’s __init__ method, only a single size is given to create_params:\nself.user_bias = create_params([n_users])\n...\nself.movie_bias = create_params([n_movies])\nWhen we perform the dot product between users and movies in the create_params model, we don’t need to preserve that second dimension:\n\nres_cp = (user_factors_cp[xb[:,0]] * movie_factors_cp[xb[:,1]]).sum(dim=1) # keepdim=False\nres_cp += user_bias_cp[xb[:,0]] + movie_bias_cp[xb[:,1]]\nres_cp.shape\n\ntorch.Size([64])\n\n\nThe output of the create_params model has a single dimension, while the output of the Embedding model has an additional unit axis.\nI’m displaying the visual inspection again to recap the differences we saw in the implementation of the two models:\n\n\n\nVisual inspection of two DotProductBias Modules"
  },
  {
    "objectID": "posts/2024-06-25-dotproductbias/index.html#training-each-model",
    "href": "posts/2024-06-25-dotproductbias/index.html#training-each-model",
    "title": "Comparing PyTorch Embeddings with Custom Embeddings",
    "section": "Training Each Model",
    "text": "Training Each Model\nTo cap off this experiment, I’ll show that both models train similarly:\n\nclass DotProductBiasEmb(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel_emb = DotProductBiasEmb(n_users, n_movies, n_factors=50)\nlearn_emb = Learner(dls, model_emb, loss_func=MSELossFlat())\nlearn_emb.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.889749\n      0.947634\n      00:14\n    \n    \n      1\n      0.663791\n      0.881707\n      00:16\n    \n    \n      2\n      0.526889\n      0.861133\n      00:22\n    \n    \n      3\n      0.465532\n      0.845524\n      00:14\n    \n    \n      4\n      0.452247\n      0.841644\n      00:13\n    \n  \n\n\n\n\nclass DotProductBiasCP(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n    self.user_factors = create_params([n_users, n_factors])\n    self.user_bias = create_params([n_users])\n    self.movie_factors = create_params([n_movies, n_factors])\n    self.movie_bias = create_params([n_movies])\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors[x[:,0]]\n    movies = self.movie_factors[x[:,1]]\n    res = (users * movies).sum(dim=1)\n    res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel_cp = DotProductBiasCP(n_users, n_movies, n_factors=50)\nlearn_cp = Learner(dls, model_cp, loss_func=MSELossFlat())\nlearn_cp.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.892142\n      0.941568\n      00:15\n    \n    \n      1\n      0.681598\n      0.885387\n      00:14\n    \n    \n      2\n      0.526930\n      0.868586\n      00:15\n    \n    \n      3\n      0.448445\n      0.854881\n      00:17\n    \n    \n      4\n      0.445660\n      0.849560\n      00:14\n    \n  \n\n\n\nThe top 5 movies (based on learned bias) for each model are the same. Note that since DotProductBiasEmb uses Embeddings for the bias, I have to access the weight of the Embedding so I can sort the values.\n\n# Embedding model\nmovie_bias_emb = learn_emb.model.movie_bias.weight.squeeze()\nidxs = movie_bias_emb.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n[\"Schindler's List (1993)\",\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Good Will Hunting (1997)',\n 'Star Wars (1977)']\n\n\n\n# create_params model\nmovie_bias_cp = learn_cp.model.movie_bias.squeeze()\nidxs = movie_bias_emb.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n[\"Schindler's List (1993)\",\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Good Will Hunting (1997)',\n 'Star Wars (1977)']"
  },
  {
    "objectID": "posts/2024-06-25-dotproductbias/index.html#final-thoughts",
    "href": "posts/2024-06-25-dotproductbias/index.html#final-thoughts",
    "title": "Comparing PyTorch Embeddings with Custom Embeddings",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAs always, I am reminded of the value that Jupyter Notebooks can bring to the coding (and learning) experience. Being able to run each line of code in the two variants of the DotProductBias model while writing out formatted explanatory text solidifies my understanding of how a batch of data passes through each model. Telling a story while writing code is a satisfying experience.\nThis exercise also illustrates how much the behavior of a model changes with a seemingly small difference—-both Embedding and create_params output tensors (and allow for the same type of training) but they are of different shapes; the construction of Embeddings and the inputs to create_params are different as well. These differences trickle through the entire model as well as the post-training analysis.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-09-02-matrix-class/index.html",
    "href": "posts/2024-09-02-matrix-class/index.html",
    "title": "Implementing a Matrix Class in Python",
    "section": "",
    "text": "In Lesson 10 of the fastai course (Part 2) Jeremy introduces the following Matrix class to allow for PyTorch- and NumPy-like indexing into a Python list:\nclass Matrix:\n  def __init__(self, xs): self.xs = xs\n  def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\nIn this notebook, I’ll expand this class to add the following methods to further mimic NumPy arrays:\n\nA __repr__ method which returns a displayable/printable representation of the Matrix\nA shape method which displays the shape of the Matrix.\nA min method\nA max method\n\n\nimport numpy as np"
  },
  {
    "objectID": "posts/2024-09-02-matrix-class/index.html#expanding-the-matrix-class",
    "href": "posts/2024-09-02-matrix-class/index.html#expanding-the-matrix-class",
    "title": "Implementing a Matrix Class in Python",
    "section": "Expanding the Matrix Class",
    "text": "Expanding the Matrix Class\nI’ll start by defining the Matrix class as done in Lesson 10 and instantiating a Matrix object:\n\nclass Matrix:\n  def __init__(self, xs): self.xs = xs\n  def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nlst1 = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\n\n\nm = Matrix(lst1)\n\nI’ll illustrate the __getitem__ method:\n\nm[1,2]\n\n6\n\n\n\nAdding a __repr__ Method\nCurrently when I print out the Matrix object, it just shows a default object representation. From the Python docs:\n\nCalled by the repr() built-in function to compute the “official” string representation of an object. If at all possible, this should look like a valid Python expression that could be used to recreate an object with the same value (given an appropriate environment). If this is not possible, a string of the form <…some useful description…> should be returned. The return value must be a string object.\n\n\nm\n\n<__main__.Matrix at 0x7ce8aeae24d0>\n\n\nNumPy has a prettier object representation:\n\narr = np.array(lst1)\narr\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nI’ll add a __repr__ method that does that (thanks Claude and ChatGPT!):\n\nclass Matrix:\n  def __init__(self, xs): self.xs = xs\n  def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n  def __repr__(self):\n        # Convert each element to string\n        str_matrix = [[str(elem) for elem in row] for row in self.xs]\n\n        # Compute max column widths\n        col_widths = [max(map(len, col)) for col in zip(*str_matrix)]\n\n        # Format each row with proper padding\n        formatted_rows = (\n            '[' + ', '.join(elem.rjust(width) for elem, width in zip(row, col_widths)) + ']'\n            for row in str_matrix\n        )\n\n        # Join rows with newline and proper indentation\n        matrix_str = ',\\n        '.join(formatted_rows)\n\n        # Add the class name and wrapping brackets\n        return f\"Matrix([{matrix_str}])\"\n\nThat looks much prettier! Albeit a lot more code in the class definition.\n\nm = Matrix(lst1)\nm\n\nMatrix([[1, 2, 3],\n        [4, 5, 6]])\n\n\nI’ll walk through each line in this __repr__ method. We start with a nested list comprehension with maintains the nested list structure of self.xs but replaces the values with strings:\n\nstr_matrix = [[str(elem) for elem in row] for row in m.xs]\nstr_matrix\n\n[['1', '2', '3'], ['4', '5', '6']]\n\n\nTo determine the “width” of each column, the following list comprehension does the following:\n\nTranspose str_matrix with zip(*str_matrix)\nCalculate the number of characters in each column with map(len, col)\nReturn the maximum length of each value in each column with max()\n\n\ncol_widths = [max(map(len, col)) for col in zip(*str_matrix)]\ncol_widths\n\n[1, 1, 1]\n\n\n\nlist(zip(*str_matrix)) # transposed matrix\n\n[('1', '4'), ('2', '5'), ('3', '6')]\n\n\n\n[list(map(len, col)) for col in zip(*str_matrix)] # width of each value in each column\n\n[[1, 1], [1, 1], [1, 1]]\n\n\n\n[max(map(len, col)) for col in zip(*str_matrix)] # maximum width of each column value\n\n[1, 1, 1]\n\n\nThe next line is another nested list comprehension. We iterate over each row in str_matrix inside the .join call and then do the following:\n\nzip together the row and the col_widths list and iterate over it\nfor each elem, width in zip(row, col_widths) we call elem.rjust(width) which right-adjusts elem inside the given string width.\n\nformatted_rows is a list of strings where each row is a string representation of each row in the Matrix:\n\nformatted_rows = (\n            '[' + ', '.join(elem.rjust(width) for elem, width in zip(row, col_widths)) + ']'\n            for row in str_matrix\n        )\nlist(formatted_rows)\n\n['[1, 2, 3]', '[4, 5, 6]']\n\n\nBuilding up the formatted_rows logic line by line:\n\n[row for row in str_matrix]\n\n[['1', '2', '3'], ['4', '5', '6']]\n\n\n\n[list(((elem, width) for elem, width in zip(row, col_widths))) for row in str_matrix]\n\n[[('1', 1), ('2', 1), ('3', 1)], [('4', 1), ('5', 1), ('6', 1)]]\n\n\n\n[list((elem.rjust(width) for elem, width in zip(row, col_widths))) for row in str_matrix]\n\n[['1', '2', '3'], ['4', '5', '6']]\n\n\n\n[', '.join(elem.rjust(width) for elem, width in zip(row, col_widths)) for row in str_matrix]\n\n['1, 2, 3', '4, 5, 6']\n\n\n\n['[' + ', '.join(elem.rjust(width) for elem, width in zip(row, col_widths)) + ']' for row in str_matrix]\n\n['[1, 2, 3]', '[4, 5, 6]']\n\n\nThe final line add a new line for each row and indents it to make it print prettily. Note there are 8 spaces after the newline character \\n for the 8 characters in the string Matrix([:\n\nformatted_rows = (\n            '[' + ', '.join(elem.rjust(width) for elem, width in zip(row, col_widths)) + ']'\n            for row in str_matrix\n        )\n\nmatrix_str = ',\\n        '.join(formatted_rows)\nprint(f\"Matrix([{matrix_str}])\")\n\nMatrix([[1, 2, 3],\n        [4, 5, 6]])\n\n\nTesting out the __repr__ method on a Matrix with a different size and different values (note the right-adjustment of the values):\n\nlst2 = [[1, 2], [2, 30], [4, 50]]\n\n\nm = Matrix(lst2)\nm\n\nMatrix([[1,  2],\n        [2, 30],\n        [4, 50]])\n\n\nPretty! And pretty complicated.\n\n\nAdding a shape Property\nOne of the most useful properties in PyTorch and NumPy is the shape of an array or tensor. I constantly use it throughout my coding to make sure I’m dealing with the appropriately shaped tensors and arrays.\n\nnp.array(lst2).shape\n\n(3, 2)\n\n\nI’ll implement a shape property for my Matrix:\n\nclass Matrix:\n  def __init__(self, xs): self.xs = xs\n  def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n  @property\n  def shape(self): return len(self.xs), len(self.xs[0])\n\n  def __repr__(self):\n        # Convert each element to string\n        str_matrix = [[str(elem) for elem in row] for row in self.xs]\n\n        # Compute max column widths\n        col_widths = [max(map(len, col)) for col in zip(*str_matrix)]\n\n        # Format each row with proper padding\n        formatted_rows = (\n            '[' + ', '.join(elem.rjust(width) for elem, width in zip(row, col_widths)) + ']'\n            for row in str_matrix\n        )\n\n        # Join rows with newline and proper indentation\n        matrix_str = ',\\n        '.join(formatted_rows)\n\n        # Add the class name and wrapping brackets\n        return f\"Matrix([{matrix_str}])\"\n\n\nm = Matrix(lst2)\nm\n\nMatrix([[1,  2],\n        [2, 30],\n        [4, 50]])\n\n\n\nm.shape\n\n(3, 2)\n\n\nExplaining my one-liner: since the Matrix is always going to be rectangular, the number of rows is the len of the list xs and the number of columns is the len of either of the rows.\n\nlen(m.xs), len(m.xs[0])\n\n(3, 2)\n\n\nThat’s it for the shape method! Simple.\n\n\nAdding a min and max Method\nThe last pair of methods I’ll implement are a min and a max method similar to NumPy.\n\narr = np.array(lst2)\narr\n\narray([[ 1,  2],\n       [ 2, 30],\n       [ 4, 50]])\n\n\n\narr.min()\n\n1\n\n\n\narr.min(axis=0)\n\narray([1, 2])\n\n\n\narr.min(axis=1)\n\narray([1, 2, 4])\n\n\n\narr.max()\n\n50\n\n\n\narr.max(axis=0)\n\narray([ 4, 50])\n\n\n\narr.max(axis=1)\n\narray([ 2, 30, 50])\n\n\n\nfrom itertools import chain\n\nclass Matrix:\n  def __init__(self, xs): self.xs = xs\n  def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n\n  @property\n  def shape(self): return len(self.xs), len(self.xs[0])\n\n  def min(self, axis=None):\n    if axis is None: return min(chain(*self.xs))\n    elif axis == 0: return min(self.xs)\n    elif axis == 1: return min(zip(*self.xs))\n    else: raise IndexError(\"Matrix only has two axes\")\n\n  def max(self, axis=None):\n    if axis is None: return max(chain(*self.xs))\n    elif axis == 0: return max(self.xs)\n    elif axis == 1: return max(zip(*self.xs))\n    else: raise IndexError(\"Matrix only has two axes\")\n\n  def __repr__(self):\n        # Convert each element to string\n        str_matrix = [[str(elem) for elem in row] for row in self.xs]\n\n        # Compute max column widths\n        col_widths = [max(map(len, col)) for col in zip(*str_matrix)]\n\n        # Format each row with proper padding\n        formatted_rows = (\n            '[' + ', '.join(elem.rjust(width) for elem, width in zip(row, col_widths)) + ']'\n            for row in str_matrix\n        )\n\n        # Join rows with newline and proper indentation\n        matrix_str = ',\\n        '.join(formatted_rows)\n\n        # Add the class name and wrapping brackets\n        return f\"Matrix([{matrix_str}])\"\n\n\nm = Matrix(lst2)\nm\n\nMatrix([[1,  2],\n        [2, 30],\n        [4, 50]])\n\n\nWhen no axis is specified, min returns the minimum value of the flattened Matrix (flattened using chain(*self.xs)).\n\nm.min()\n\n1\n\n\nWhen axis=0, min returns the minimum value in each row, which is simply the built-in return value when min is applied to a list (min(self.xs)).\n\nm.min(axis=0)\n\n[1, 2]\n\n\nWhen axis=1, min returns the minimum value in each column. I first transpose the list and then pass it through min to get the desired result:\n\nm.min(axis=1)\n\n(1, 2, 4)\n\n\nFinally, if the axis is some other value, I throw an IndexError:\n\nm.min(axis=3)\n\nIndexError: Matrix only has two axes\n\n\nmax operates similarly:\n\nm.max(), m.max(axis=0), m.max(axis=1)\n\n(50, [4, 50], (2, 30, 50))\n\n\n\nm.max(axis=3)\n\nIndexError: Matrix only has two axes\n\n\nThat’s it! Another relatively simple implementation."
  },
  {
    "objectID": "posts/2024-09-02-matrix-class/index.html#final-thoughts",
    "href": "posts/2024-09-02-matrix-class/index.html#final-thoughts",
    "title": "Implementing a Matrix Class in Python",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nCreating my own Matrix class was fun and educational. I didn’t expect the object representation as a string to be so involved—and I haven’t even added functionality like NumPy or PyTorch where they “summarize” very long arrays and tensors instead of listing out all of the values.\nI also learned a few things along the way:\n\nUsing zip to transpose a list\nUsing the @property decorator\nJustifying string with rjust\nImplementing the built-in IndexError for situations when the user passes an incompatible axis argument\n\nI’m assuming that we’ll be building on this Matrix class further down the road in Part 2, so it was helpful to get a jump on that and start thinking about how this class would function right now.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "",
    "text": "In this notebook I want to compare the differences between fine-tuning a pretrained model with and without using LoRA. This exercise is a fastai community study group homework assignment.\nHere is a comparison of the full-fine-tuning (Full FT) vs. LoRA fine-tuning (LoRA FT) process on the EleutherAI/pythia-70m model using the roneneldan/TinyStoriesInstruct dataset (which comes from the TinyStories paper):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nParameters\nTraining Set\nValidation Set\nPerplexity\nBatch Size\nEpochs\nTrain Steps\nTrain Time (Minutes)\n\n\n\n\nFull FT\n70.4M\n240k\n60k\n8.51\n16\n3\n22500\n100\n\n\nLoRA FT\n98k\n256k\n64k\n12.68\n16\n4\n32000\n120"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#resources",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#resources",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Resources",
    "text": "Resources\n\nI’ll use a small subset of the roneneldan/TinyStoriesInstruct dataset from HuggingFace for both trainings since when I use the full dataset I’m getting CUDA out-of-memory errors.\nI’m referencing the following to patch together the code in this notebook:\n\nJeremy Howard’s Getting started with NLP for absolute beginners for fundamental setup of data, model, and tokenizer.\nHuggingFace’s Causal language modeling tutorial for updating the tokenizer with a pad token, data_collator and training arguments.\nThis forum response that shows how to select a subset of a dataset with a given set of indexes.\nThe TinyStories author’s hyperparameters as listed in their 33M parameter model page\nHuggingFace’s LoRA Conceptual Guide for steps on how to implement LoRA using peft.\nThis blog post which walks through an example LoRA training.\nThis forum response by Sylvain Gugger which says to set save_strategy to \"no\" to avoid the Trainer creating checkpoints as I was running into errors around this."
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#plan-of-attack",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#plan-of-attack",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nIn my first iteration of this exercise (see below) I manually ran multiple different trainings with different models, dataset sizes and training arguments. The code was flexible and easy to update but I through that process I re-ran a lot of cells with different values and lost track a bit exactly the order of things I was running. In this second iteration, I’ll create a helper function get_trainer which takes various arguments (model, bs,tokz, train_ds, etc.) and returns a HuggingFace Trainer. This will help clear up some of the redundancy in my code and make it a bit cleaner to read.\n\n# all the imports\n!pip install peft accelerate evaluate -Uqq\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoModelForCausalLM, pipeline\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom evaluate import load\nimport math"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#get_trainer-helper-function",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#get_trainer-helper-function",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "get_trainer Helper Function",
    "text": "get_trainer Helper Function\nThis function prepares and returns Trainer object for a given model, tokenizer (and tokenize function), training/validation dataset, learning rate, batch size and number of epochs:\n\ndef get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr, bs, epochs):\n    # get tokenized datasets\n    train_tok_ds = train_ds.map(tok_func, batched=True)\n    eval_tok_ds = eval_ds.map(tok_func, batched=True)\n    \n    # sometimes for whatever reason the datasets are not the right size so checking it here\n    print(train_tok_ds)\n    \n    # not sure what this does but I get an error that the model didn't return a loss value without it\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokz, mlm=False)\n    \n    # define training arguments\n    training_args = TrainingArguments(\n        output_dir=\"outputs\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=lr,\n        lr_scheduler_type = \"cosine\",\n        weight_decay=0.1,\n        per_device_train_batch_size=bs, \n        per_device_eval_batch_size=bs,\n        num_train_epochs=epochs,\n        report_to='none',\n        fp16=True,\n        logging_steps=10,\n        save_strategy=\"no\"\n    )\n    \n    # define Trainer\n    trainer = Trainer(model, training_args, train_dataset=train_tok_ds, eval_dataset=eval_tok_ds,\n                  tokenizer=tokz, data_collator=data_collator)\n    \n    return trainer"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#load-the-dataset",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#load-the-dataset",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Load the Dataset",
    "text": "Load the Dataset\nAs recommended in the study group, I’ll use the TinyStoriesInstruct dataset which comes from the paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English?.\n\nds = load_dataset(\"roneneldan/TinyStoriesInstruct\")\n\n\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 21755681\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 218380\n    })\n})"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#full-fine-tuning-with-eleutheraipythia-70m",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#full-fine-tuning-with-eleutheraipythia-70m",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Full Fine-Tuning with EleutherAi/pythia-70m",
    "text": "Full Fine-Tuning with EleutherAi/pythia-70m\nFirst, I’ll fully fine-tune an existing pretrained model on a subset of the TinyStoriesInstruct dataset using the EleutherAI/pythia-70m model. I chose this model because larger models were giving me CUDA-out-of-memory errors even for small dataset and batch sizes.\n\nmodel_nm = 'EleutherAI/pythia-70m'\ntokz = AutoTokenizer.from_pretrained(model_nm)\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ndef tok_func(x): return tokz(x[\"text\"])\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_nm)\n\n\nmodel\n\nGPTNeoXForCausalLM(\n  (gpt_neox): GPTNeoXModel(\n    (embed_in): Embedding(50304, 512)\n    (layers): ModuleList(\n      (0-5): 6 x GPTNeoXLayer(\n        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (attention): GPTNeoXAttention(\n          (rotary_emb): RotaryEmbedding()\n          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n          (dense): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (mlp): GPTNeoXMLP(\n          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n          (act): GELUActivation()\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n)\n\n\n\nmodel.num_parameters()\n\n70426624\n\n\nI first trained the model on a very small subset (1000 rows) for both full-finetuning and LoRA to make sure it worked, then slowly increased the training and validation size until I got the CUDA out-of-memory error.\nFor small datasets, I noticed that the validation loss started increasing after 3 epochs so I’ve kept the number of epochs at 3. With larger datasets I could try to increase the number of epochs and see if it still overfits.\nI couldn’t figure out how to implement perplexity during training. I was getting a Sizes of tensors must match except in dimension 0. error when passing any function to compute_metrics so I calculate perplexity at the end of training instead.\nWhen I tried to train the model with 240k, 220k or 200k training samples, I got the following error after 1.60, 1.75 and 1.92 epochs respectively:\nRuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/9: file write failed\nI set the save_strategy argument in the training arguments dictionary to \"no\" and this resolved this error. However, in the future, if I wanted checkpoints during my training I would have to figure out how to resolve this error differently.\n\ntrain_ds = ds['train'].select(range(240000))\neval_ds = ds['validation'].select(range(60000))\n\n\ntrainer = get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr=5e-4, bs=16, epochs=3)\n\n\ntrainer.train()\n\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [22500/22500 1:40:16, Epoch 3/3]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n    \n  \n  \n    \n      1\n      2.385700\n      2.407521\n    \n    \n      2\n      2.098300\n      2.192903\n    \n    \n      3\n      1.841100\n      2.141196\n    \n  \n\n\n\nTrainOutput(global_step=22500, training_loss=2.1849648211161297, metrics={'train_runtime': 6016.472, 'train_samples_per_second': 119.671, 'train_steps_per_second': 3.74, 'total_flos': 1.64194783592448e+16, 'train_loss': 2.1849648211161297, 'epoch': 3.0})\n\n\n\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n\n\n    \n      \n      \n      [1875/1875 03:25]\n    \n    \n\n\nPerplexity: 8.51\n\n\nI’ll generate some text from the pretrained model and fully fine-tuned model to see how they compare:\n\nprompt = \"Once upon a time,\"\ngenerator = pipeline('text-generation', model=model_nm, tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n‘Once upon a time, thefirst two are not in agreement. The second is to be expected; and it would have been an easy task for them if they had known that he was going on their way from home as soon after leaving his house at night or when there were no other guests than himself who wanted him back with all of her belongings before returning into town again by midnight (and then later). But this one has never seen such things since I've lived here.”’\n\n\ngenerator = pipeline('text-generation', model=trainer.model.to('cpu'), tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n‘Once upon a time, there was an old man. He had a big mustache and he loved to wear it every day. One morning when the sun came out, his eyes lit up with joy! 0He wanted to go outside but couldn't find anything else. So he decided to take off his hat and coat so that no one could see him. The old man smiled at Jimmy's face and said “I'm glad you like it”. Jimmy was happy again and thanked the old man’\n\nThe pre-trained model as is does not generate text that resembles a story whatsoever. The fully fine-tuned model’s generated text is somewhat coherent and it resembles a story although elements of it still don’t make sense."
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#fine-tuning-eleutheraipythia-70m-with-lora",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#fine-tuning-eleutheraipythia-70m-with-lora",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Fine-Tuning EleutherAI/pythia-70m with LoRA",
    "text": "Fine-Tuning EleutherAI/pythia-70m with LoRA\nSince a LoRA model has less trainable parameters, I can increase the dataset size for the training. I’ll also see if I can train for more epochs without overfitting since I’m using more data.\n\ntrain_ds = ds['train'].select(range(256000))\neval_ds = ds['validation'].select(range(64000))\n\n\nlora_config = LoraConfig(task_type=TaskType.CAUSAL_LM)\nmodel = AutoModelForCausalLM.from_pretrained(model_nm)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ntrainable params: 98,304 || all params: 70,524,928 || trainable%: 0.13938901149959346\n\nmodel\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): GPTNeoXForCausalLM(\n      (gpt_neox): GPTNeoXModel(\n        (embed_in): Embedding(50304, 512)\n        (layers): ModuleList(\n          (0-5): 6 x GPTNeoXLayer(\n            (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (attention): GPTNeoXAttention(\n              (rotary_emb): RotaryEmbedding()\n              (query_key_value): Linear(\n                in_features=512, out_features=1536, bias=True\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=512, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (mlp): GPTNeoXMLP(\n              (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n              (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n              (act): GELUActivation()\n            )\n          )\n        )\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n    )\n  )\n)\n\n\n\ntrainer = get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr=5e-4, bs=16, epochs=4)\n\n\ntrainer.train()\n\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n    \n      \n      \n      [32000/32000 2:00:46, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n    \n  \n  \n    \n      1\n      2.616000\n      2.614058\n    \n    \n      2\n      2.575500\n      2.570585\n    \n    \n      3\n      2.605000\n      2.547680\n    \n    \n      4\n      2.493900\n      2.540338\n    \n  \n\n\n\nTrainOutput(global_step=32000, training_loss=2.621225409567356, metrics={'train_runtime': 7252.3347, 'train_samples_per_second': 141.196, 'train_steps_per_second': 4.412, 'total_flos': 2.33350953959424e+16, 'train_loss': 2.621225409567356, 'epoch': 4.0})\n\n\n\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n\n\n    \n      \n      \n      [2000/2000 03:32]\n    \n    \n\n\nPerplexity: 12.68\n\n\n\nprompt = \"Once upon a time,\"\ngenerator = pipeline('text-generation', model=trainer.model.to('cpu'), tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n“Once upon a time, there was an old man who lived in the park. He had many friends and loved to play with him every day at his house all night long! One morning he decided that it would be best for everyone else because they were so happy together as each other on their own one by another’s side of town hall or doorstep…so when something unexpected happened she started playing outside - her mommy said no but could help herself out here until someone came up close enough.. She”\n\nThe generated text resembles a story and is a bit coherent for the first couple of sentences before it stops making sense in the second half.\nHere is a comparison of the full-fine-tuning (Full FT) vs. LoRA fine-tuning (LoRA FT) process on the EleutherAI/pythia-70m:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nParameters\nTraining Set\nValidation Set\nPerplexity\nBatch Size\nEpochs\nTrain Steps\nTrain Time (Minutes)\n\n\n\n\nFull FT\n70.4M\n240k\n60k\n8.51\n16\n3\n22500\n100\n\n\nLoRA FT\n98k\n256k\n64k\n12.68\n16\n4\n32000\n120"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#generating-text-from-the-pre-trained-tinystories-model",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#generating-text-from-the-pre-trained-tinystories-model",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Generating Text from the Pre-Trained TinyStories Model",
    "text": "Generating Text from the Pre-Trained TinyStories Model\nThe authors of the paper that this dataset comes released their fine-tuned model on HuggingFace, so I’ll use it to generate text to see how a state-of-the-art TinyStories model performs:\n\nmodel_nm = \"EleutherAI/gpt-neo-125M\"\ntokz = AutoTokenizer.from_pretrained(model_nm)\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ndef tok_func(x): return tokz(x[\"text\"])\n\n\ngenerator = pipeline('text-generation', model='roneneldan/TinyStories-33M', tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n‘Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine and pick flowers. One day, she found an ancient book on her porch. It had lots of pictures inside that looked very old.opened the book and saw many words written around it. But then, she heard a loud noise coming from the house next door. She went to investigate and found out that someone had broken into their home. ran back to’\n\nThe model is so good! It can hold a consistent, coherent theme in story format for multiple sentences."
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#final-thoughts",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#final-thoughts",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’m happy to have got this all to work, as that alone was a big step in my learning process. This is the first time I have trained a causal language model using HuggingFace. One thought to close out this exercise: Would restructuring the data help? Currently the dataset has text values like “Summary:” and “Features:”, which are the prompts used by the TinyStories paper authors to generate stories using GPT-3.5 and 4. Perhaps removing these prompts from the dataset and keeping only the story text would help improve the model. I’ll explore this in a future exercise.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "",
    "text": "In this notebook I’ll establish a baseline performance of embedding cosine similarity for retrieving relevant context needed to answer the fastbook Chapter 1 Questionnaire.\nThis is part of a larger project that I’m calling fastbookRAG, where I’ll be building a keyword+semantic search-to-LLM RAG pipeline. The goal of this pipeline will be to accurately answer questions from the fastbook end-of-chapter Questionnaires. fastbook is the freely available fastai textbook.\nHere are the results from this notebook. “Retrieved context relevancy” means the percentage of the 33 questions in the Questionnaire I was able to answer given the retrieved notebook chunks. Eventually in my pipeline, I’ll replace me with an LLM that will be using this retrieved context to answer the questions. “Top-n” means the selected chunks used for context had the top n cosine similiarity scores. The best performing approach was using the top-5 (cosine similarity value) small-sized chunks.\n\n\n\nTop-n\nChunk Size\nRetrieved Context Relevancy\n\n\n\n\nTop-5\nSmall\n76%\n\n\nTop-3\nSmall\n70%\n\n\nTop-3\nLarge\n67%\n\n\nTop-1\nSmall\n51%\n\n\nTop-1\nLarge\n48%\n\n\n\n\n\nShow imports\nimport sqlite3\nimport json\nimport re\nimport pandas as pd, numpy as np\nimport textwrap\nimport torch\nfrom torch import tensor\nimport torch.nn.functional as F\n\n!pip install sentence-transformers -Uqq\nfrom sentence_transformers import SentenceTransformer\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n\ndef wrapper(line, width):\n  textwrap.TextWrapper(\n    width=width,\n    replace_whitespace=False,\n    drop_whitespace=False)(line)\n\ndef print_wrap_text(text, width):\n  print(\"\\n\".join(wrapper.fill(line) for line in text.splitlines()))"
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#chunking-fastbook-chapter-1-into-paragraphs",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#chunking-fastbook-chapter-1-into-paragraphs",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Chunking fastbook Chapter 1 into Paragraphs",
    "text": "Chunking fastbook Chapter 1 into Paragraphs\nAs I did with my full text search demo, I’ll start by chunking the fastbook Chapter 1 Jupyter Notebook into paragraphs (that include the header from the section it’s in).\n\n\nShow the chunking code\ndef get_chunks(notebook_path):\n    with open(notebook_path, 'r', encoding='utf-8') as file:\n        notebook = json.load(file)\n\n    chunks = []\n    current_header = \"\"\n\n    def add_chunk(content):\n        if content.strip():\n            chunks.append(f\"{current_header}\\n\\n{content.strip()}\")\n\n    for cell in notebook['cells']:\n        if cell['cell_type'] == 'markdown':\n            content = ''.join(cell['source'])\n            header_match = re.match(r'^(#+\\s+.*?)$', content, re.MULTILINE)\n            if header_match:  # Check if the cell starts with a header\n                current_header = header_match.group(1)\n                # Add any content after the header in the same cell\n                remaining_content = content[len(current_header):].strip()\n                if remaining_content:\n                    paragraphs = re.split(r'\\n\\s*\\n', remaining_content)\n                    for paragraph in paragraphs:\n                        add_chunk(paragraph)\n            else:\n                paragraphs = re.split(r'\\n\\s*\\n', content)\n                for paragraph in paragraphs:\n                    add_chunk(paragraph)\n        elif cell['cell_type'] == 'code':\n            code_content = '```python\\n' + ''.join(cell['source']) + '\\n```'\n            add_chunk(code_content)\n\n    return chunks\n\ndef filter_chunks(chunks, exclude_headers):\n  filtered_chunks = []\n  for chunk in chunks:\n      lines = chunk.split('\\n')\n      # Check if the first line (header) is in the exclude list\n      if not any(header in lines[0] for header in exclude_headers):\n          filtered_chunks.append(chunk)\n  return filtered_chunks\n\nexclude_headers = [\"Questionnaire\", \"Further Research\"]\n\n\n\nnotebook_path = '01_intro.ipynb'\nchunks = get_chunks(notebook_path)\nassert len(chunks) == 315\nfiltered_chunks = filter_chunks(chunks, exclude_headers)\nassert len(filtered_chunks) == 307\n\n\nprint(filtered_chunks[-3])\n\n### Use Judgment in Defining Test Sets\n\nNow that you have gotten a taste of how to build a model, you can decide what you want to dig into next."
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#embedding-the-notebook-chunks-and-questionnaire-questions",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#embedding-the-notebook-chunks-and-questionnaire-questions",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Embedding the Notebook Chunks and Questionnaire Questions",
    "text": "Embedding the Notebook Chunks and Questionnaire Questions\nI’ll embed the notebook chunks using the bge-small-en-v1.5 model.\n\ndata_embs = emb_model.encode(filtered_chunks, convert_to_tensor=True)\ndata_embs.shape\n\ntorch.Size([307, 384])\n\n\nI’ll grab the questions from a gist I created:\n\ndf = pd.read_csv(\"https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/bc6cd2ab15b64a92ec23796c61702f413fdd2b40/fastbookRAG_evals.csv\")\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n    \n    \n      3\n      1\n      4\n      Based on the book of the same name, what are t...\n      A set of processing units\\\\nA state of activat...\n      parallel, distributed, processing, requirement...\n    \n    \n      4\n      1\n      5\n      What were the two theoretical misunderstanding...\n      In 1969, Marvin Minsky and Seymour Papert demo...\n      theoretical, misunderstandings, held, back, fi...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAnd embed them:\n\nq_embs = emb_model.encode(df['question_text'], convert_to_tensor=True)\nq_embs.shape\n\ntorch.Size([33, 384])"
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#retrieving-context-small-chunks-using-cosine-similarity",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#retrieving-context-small-chunks-using-cosine-similarity",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Retrieving Context (Small Chunks) Using Cosine Similarity",
    "text": "Retrieving Context (Small Chunks) Using Cosine Similarity\nI’ll use cosine similarity between the first question and the full set of notebook chunks to determine the best match between query and context:\n\nres = F.cosine_similarity(q_embs[0], data_embs, dim=1).sort(descending=True)\nres[0][0], res[1][0]\n\n(tensor(0.8431), tensor(4))\n\n\n\n# get the chunk with the highest cosine similarity value\nfiltered_chunks[res[1][0]]\n\n'## Deep Learning Is for Everyone\\n\\n```asciidoc\\n[[myths]]\\n.What you don\\'t need to do deep learning\\n[options=\"header\"]\\n|======\\n| Myth (don\\'t need) | Truth\\n| Lots of math | Just high school math is sufficient\\n| Lots of data | We\\'ve seen record-breaking results with <50 items of data\\n| Lots of expensive computers | You can get what you need for state of the art work for free\\n|======\\n```'\n\n\nThat’s the correct context needed to answer this question! I’ll now loop through each question and store the response. Then, I’ll download the CSV of question/context pairs and score them manually in Excel to calculate the retrieved context relevancy (i.e. the percentage of retrieved contexts that are relevant and sufficient to answer the question).\n\nresults = []\n\nfor q in q_embs:\n  res = F.cosine_similarity(q, data_embs, dim=1).sort(descending=True)\n  results.append(filtered_chunks[res[1][0]])\n\n\nassert len(results) == 33\n\n\ndf['cos_sim_res'] = pd.Series(results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      cos_sim_res\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('cos_sim_results.csv', index=False)\n\nSelecting the notebook chunk with the highest cosine similarity score resulted in a 51% retrieved context relevancy. The best I achieved with full text search (using sqlite) was 72% (with the retrieval of the top-3 BM25 ranked larger notebook chunks)."
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#selecting-the-top-3-cosine-similarity-small-chunks",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#selecting-the-top-3-cosine-similarity-small-chunks",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Selecting the Top-3 Cosine Similarity Small Chunks",
    "text": "Selecting the Top-3 Cosine Similarity Small Chunks\nInstead of selecting the chunk with the highest cosine similarity with the query, I’ll choose the top-3 and see if that allows me to answer more questions.\n\nresults = []\n\nfor q in q_embs:\n  res = F.cosine_similarity(q, data_embs, dim=1).sort(descending=True)\n  ctx = ''\n  for idx in res[1][:3]:\n    ctx += filtered_chunks[idx] + '\\n'\n  results.append(ctx)\n\n\ndf['cos_sim_res'] = pd.Series(results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      cos_sim_res\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('top-3_cos_sim_results.csv', index=False)\n\nIncreased the number of context retrieved to 3 improve resulted in a retrieved context relevancy of 70%. I noted that cosine similarity was able to retrieve the right context for some questions where full text search did not, and vice versa, which leads me to believe that a hybrid approach will be optimal."
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#increasing-chunk-size",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#increasing-chunk-size",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Increasing Chunk Size",
    "text": "Increasing Chunk Size\nAs I did when experimenting with full text search, I’ll now increase the chunk size (to three paragraphs instead of 1) and see if retrieving the top-1 cosine similarity chunk answers more questions (than retrieving the top-1 single paragraph chunk).\n\nlarger_chunks = [\"\\n\".join(filtered_chunks[i:i+3]) for i in range(0, len(filtered_chunks), 3)]\n\n\nlen(larger_chunks)\n\n103\n\n\nNote that since my chunks’ content has changed, I’ll have to create new embeddings for them:\n\ndata_embs = emb_model.encode(larger_chunks, convert_to_tensor=True)\ndata_embs.shape\n\ntorch.Size([103, 384])\n\n\n\n\nShow cosine similarity for-loop\nresults = []\n\nfor q in q_embs:\n  res = F.cosine_similarity(q, data_embs, dim=1).sort(descending=True)\n  results.append(larger_chunks[res[1][0]])\n\n\n\nassert len(results) == 33\n\n\ndf['cos_sim_res'] = pd.Series(results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      cos_sim_res\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## How to Learn Deep Learning\\n\\n> : A PhD is ...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('larger_cos_sim_results.csv', index=False)\n\nInterestingly enough, increasing the chunk size actually decreased the performance of retrievel using cosine similarity. I was able to answer only 48% of the questions with the retrieved context."
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#selecting-the-top-3-larger-chunks",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#selecting-the-top-3-larger-chunks",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Selecting the Top-3 Larger Chunks",
    "text": "Selecting the Top-3 Larger Chunks\nI’ll see if retrieving the top-3 larger chunks yields a better result.\n\n\nShow cosine similarity for-loop\nresults = []\n\nfor q in q_embs:\n  res = F.cosine_similarity(q, data_embs, dim=1).sort(descending=True)\n  ctx = ''\n  for idx in res[1][:3]:\n    ctx += larger_chunks[idx] + '\\n'\n  results.append(ctx)\n\n\n\ndf['cos_sim_res'] = pd.Series(results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      cos_sim_res\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## How to Learn Deep Learning\\n\\n> : A PhD is ...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('top-3-large_cos_sim_results.csv', index=False)\n\nRetrieving the top-3 large chunks helped me answer 67% of the questions, performing worse (1 question less) than using top-3 small chunks."
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#selecting-the-top-5-cosine-similarity-small-chunks",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#selecting-the-top-5-cosine-similarity-small-chunks",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Selecting the Top-5 Cosine Similarity Small Chunks",
    "text": "Selecting the Top-5 Cosine Similarity Small Chunks\nI am getting better results using smaller chunks so I’ll increase the number of small chunks retrieved to 5. I’ll have to re-embed the smaller chunks:\n\ndata_embs = emb_model.encode(filtered_chunks, convert_to_tensor=True)\ndata_embs.shape\n\ntorch.Size([307, 384])\n\n\n\n\nShow cosine similarity for-loop\nresults = []\n\nfor q in q_embs:\n  res = F.cosine_similarity(q, data_embs, dim=1).sort(descending=True)\n  ctx = ''\n  for idx in res[1][:5]:\n    ctx += filtered_chunks[idx] + '\\n'\n  results.append(ctx)\n\n\n\ndf['cos_sim_res'] = pd.Series(results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      cos_sim_res\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('top-5_cos_sim_results.csv', index=False)\n\nThe chunks retrieved with this approach allowed me to answer 76% of the questions! That’s the best performance I have reached so far (with cosine similarity or full text search). I did not try retrieving top-5 small chunks for full text search, so that’s something I’ll try in the future."
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#using-numpy-to-calculate-cosine-similarity",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#using-numpy-to-calculate-cosine-similarity",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Using NumPy to Calculate Cosine Similarity",
    "text": "Using NumPy to Calculate Cosine Similarity\nI’m curious if I can implement cosine similarity using NumPy (as eventually I want to host this project and not have to install PyTorch and dependencies on the server).\nFirst, I’ll recreate my embeddings for questions and context, returning NumPy arrays and not PyTorch tensors:\n\ndata_embs = emb_model.encode(filtered_chunks)\ndata_embs.shape\n\n(307, 384)\n\n\n\nq_embs = emb_model.encode(df['question_text'])\nq_embs.shape\n\n(33, 384)\n\n\n\ntype(data_embs), type(q_embs)\n\n(numpy.ndarray, numpy.ndarray)\n\n\n\n# thanks Claude\ndef cosine_similarity_multiple(a, b):\n    # Ensure a is a 2D array\n    a = np.atleast_2d(a)\n\n    # Compute dot product for each row of b with a\n    dot_product = np.dot(a, b.T)\n\n    # Compute magnitudes\n    a_norm = np.linalg.norm(a, axis=1)\n    b_norm = np.linalg.norm(b, axis=1)\n\n    # Compute cosine similarity\n    similarity = dot_product / (a_norm[:, np.newaxis] * b_norm)\n    similarity = similarity.flatten()\n\n    return np.sort(similarity)[::-1], np.argsort(similarity)[::-1]\n\n\nres = cosine_similarity_multiple(q_embs[0], data_embs)\nassert res[1][0] == 4\nassert res[0][0] - 0.8431 < 1e-4\n\n\n\nShow pytorch cosine similarity for-loop\npt_results = []\n\nfor q in q_embs:\n  res = F.cosine_similarity(tensor(q), tensor(data_embs), dim=1).sort(descending=True)\n  ctx = ''\n  for idx in res[1][:5]:\n    ctx += filtered_chunks[idx] + '\\n'\n  pt_results.append(ctx)\n\n\n\n\nShow numpy cosine similarity loop\nnp_results = []\n\nfor q in q_embs:\n  res = cosine_similarity_multiple(q, data_embs)\n  ctx = ''\n  for idx in res[1][:5]:\n    ctx += filtered_chunks[idx] + '\\n'\n  np_results.append(ctx)\n\n\nApplying cosine similarity with NumPy yields the exact same results (retrieved contexts) as PyTorch.\n\npt_results == np_results\n\nTrue"
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#summary-of-results",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#summary-of-results",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Summary of Results",
    "text": "Summary of Results\nHere are the results from this notebook. “Retrieved context relevancy” means the percentage of the 33 questions in the Questionnaire I was able to answer given the retrieved notebook chunks. Eventually in my pipeline, I’ll replace me with an LLM that will be using this retrieved context to answer the questions. “Top-n” means the selected chunks used for context had the top n cosine similiarity scores. The best performing approach was using the top-5 (cosine similarity value) small-sized chunks.\n\n\n\nTop-n\nChunk Size\nRetrieved Context Relevancy\n\n\n\n\nTop-5\nSmall\n76%\n\n\nTop-3\nSmall\n70%\n\n\nTop-3\nLarge\n67%\n\n\nTop-1\nSmall\n51%\n\n\nTop-1\nLarge\n48%"
  },
  {
    "objectID": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#final-thoughts",
    "href": "posts/2024-08-06-fastbook-ch1-cosine-similarity/index.html#final-thoughts",
    "title": "Using Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis exercise was fun. I enjoy the simplicity of implementing cosine similarity to yield decent results. A few things I was thinking about while looking at the data and results in this notebook:\nSome questions may not have relevant chunks in the dataset. For example, some of the questions ask the reader to do some activity. These should not be included in the evaluation set.\nCosine similarity didn’t perform well on questions I expected it to. For example, one of the Questionnaire questions is:\n\nAre image models only useful for photos?\n\nThe answer to this question is “No” and there is a section in the chapter explicitly titled “Image Recognizers Can Tackle Non-Image Tasks” wherein the first paragraph reads:\n\nAn image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recognizer can learn to complete many tasks.\n\nI would expect the cosine similarity between this question and context to be high. In fact, let’s take a look:\n\nq = emb_model.encode(\"Are image models only useful for photos?\")\nc = emb_model.encode(\"An image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recognizer can learn to complete many tasks.\")\nq.shape, c.shape\n\n((384,), (384,))\n\n\n\nF.cosine_similarity(tensor(q), tensor(c), dim=0)\n\ntensor(0.7046)\n\n\nHowever, this cosine similarity is less than the combined top-5 results:\n\nF.cosine_similarity(tensor(q), tensor(emb_model.encode(pt_results[27])), dim=0)\n\ntensor(0.7703)\n\n\nSuch behavior is fascinating to me as it shows opportunity to explore better solutions. For example, a hybrid approach of cosine similarity with full text search. Or experimenting with different keywords for full text search. Or, experimenting with different chunking strategies. These are the kinds of things I’ll be exploring in future notebooks as I expand beyond Chapter 1 and tackle the rest of the Questionnaires in part 1 of the fastai course.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html",
    "href": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html",
    "title": "Sentiment Classification with phi-2",
    "section": "",
    "text": "Show pip installs\n!pip install transformers~=4.37.2 -qq\n!pip install huggingface_hub~=0.20.3 -qq\n!pip install datasets~=2.16.1 -qq\n!pip install accelerate -qq\n!pip install plotly==5.19.0 -qq\n!pip install scikit-learn==1.2 -qq\n\n\n\n\nShow imports and setup\nimport gc\nimport pandas as pd\nimport numpy as np\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\nimport torch\n\nimport warnings\n#warnings.filterwarnings(\"ignore\")\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport time\n\nfrom datasets import load_dataset, Dataset\nfrom transformers import pipeline, logging\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom fastcore.all import *\n\nlogging.set_verbosity_error()\n\n#torch.set_default_device(\"cuda\")\ntorch.cuda.set_device(0)\nmodel_name = \"microsoft/phi-2\"\n\n# create pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model_name,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16\n)\n\n# load dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\", \n    split=\"train\"  # note that the dataset does not have a default test split\n)"
  },
  {
    "objectID": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html#background",
    "href": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html#background",
    "title": "Sentiment Classification with phi-2",
    "section": "Background",
    "text": "Background\nIn this notebook I’ll use phi-2 to classify sentiment in the financial_phrasebank dataset.\nI was inspired to do so after reading this blog post and this corresponding notebook by Moritz Laurer as part of a fastai study group last year.\nThis notebook is part of a series of blog posts for a project I’m working called TinySentiment where I’m experimenting with tiny models to improve their ability to classify sentiment in the financial_phrasebank dataset.\nThis notebook establishes a baseline using phi-2.\nHere are the results from this notebook:\n\n\n\nPrompt\nStrategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n58%\n97% (295/303)\n37% (520/1391)\n86% (490/570)\n\n\nB\n0-Shot\n81%\n87% (264/303)\n87% (1207/1391)\n63% (361/570)\n\n\nC\n3-Shot\n73%\n96% (291/302)\n58% (803/1390)\n98% (557/569)\n\n\nD\n6-Shot\n74%\n99% (298/302)\n59% (822/1387)\n95% (542/569)\n\n\nE\n6-Shot\n87%\n98% (296/302)\n82% (1131/1387)\n94% (537/569)\n\n\nF\n6-Shot\n84%\n99% (298/302)\n77% (1069/1387)\n95% (538/569)\n\n\nG\n6-Shot\n84%\n98% (295/302)\n76% (1059/1387)\n96% (546/569)\n\n\nH\n6-Shot\n91%\n93% (280/302)\n88% (1219/1387)\n96% (548/569)\n\n\nI\n14-Shot\n89%\n73% (221/302)\n99% (1368/1379)\n71% (406/569)\n\n\nJ\n20-Shot\n91%\n80% (238/299)\n97% (1340/1379)\n82% (466/566)\n\n\nK\n6-Shot\n91%\n80% (241/302)\n95% (1318/1387)\n89% (505/569)\n\n\nL\n6-Shot\n90%\n81% (246/302)\n92% (1276/1387)\n91% (515/569)\n\n\nM\n6-Shot\n92%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\nN\n3-Shot\n91%\n78% (235/302)\n95% (1320/1390)\n89% (504/569)\n\n\nO\n3-Shot\n91%\n78% (237/302)\n96% (1333/1390)\n85% (484/569)"
  },
  {
    "objectID": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html#prompt-engineering",
    "href": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html#prompt-engineering",
    "title": "Sentiment Classification with phi-2",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\nI’ll follow the Instruct/Output prompt template provided on the HuggingFace phi-2 model card page:\nInstruct: Write a detailed analogy between mathematics and a lighthouse.\nOutput: Mathematics is like a lighthouse. Just as a lighthouse guides ...\nFor bookkeeping purposes, I’ll label the prompts I try with capital letters.\n\nPrompt A\n\npromptA = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nOutput:\"\"\"\n\nThe sentence column in the dataset holds the text for which our model will classify its sentiment.\n\ntext = dataset[1][\"sentence\"]\ntext\n\n\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\"\n\n\nI format the prompt, replacing {text} with the sentence from the dataset:\n\nprint(promptA.format(text=text))\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nOutput:\n\n\nI’ll test out my pipeline by passing the formatted prompt to the pipe, specifying max_new_tokens as 1 since I want a one word response:\n\noutput = pipe(\n    promptA.format(text=text),\n    max_new_tokens=1,\n    do_sample=True, \n    temperature=0.1,\n    return_full_text=False)\n\noutput[0][\"generated_text\"]\n\n' Positive'\n\n\nLooks good!\nI’ll generate responses for the entire dataset for each prompt (and then calculate the accuracy: the %-age of responses that match the experts).\nI’ll first add a prompt column to the dataset, utilizing Moritz Laurer’s approach for label_text:\n\n# add formatted prompt as a new column in dataset\ndef add_prompt(item):\n    item['prompt'] = promptA.format(text=item['sentence'])\n    return item\n\ndataset = dataset.map(add_prompt)\ndataset\n\n\n\n\nDataset({\n    features: ['sentence', 'label', 'prompt'],\n    num_rows: 2264\n})\n\n\n\nprint(dataset[0]['sentence'])\nprint('--------')\nprint(dataset[0]['prompt'])\n\nAccording to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n--------\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nOutput:\n\n\nI’ll follow the documentation on using pipelines on a dataset from HuggingFace to create the following inference loop.\nI’m stripping the output text and converting it to lowercase to make it consistent. I’ll append each output to a list and later add that as a column to a DataFrame created from the dataset.\n\nimport time\nstart_time = time.time()\n\noutputs = []\n        \nfor out in pipe(KeyDataset(dataset, \"prompt\"), max_new_tokens=1, do_sample=True, temperature=0.1, return_full_text=False):\n    outputs.append(out[0]['generated_text'].strip().lower())\n    \nend_time = time.time()\nexecution_time = end_time - start_time\n\nprint(f\"Execution time on Paperspace Free-A4000: {execution_time:.6f} seconds\")\n\nExecution time on Paperspace Free-A4000: 58.225772 seconds\n\n\n\nlen(dataset) == len(outputs)\n\nTrue\n\n\nNext, I’ll compare the model’s responses to the official dataset responses.\nThe dataset has a label column which has an integer 0, 1 or 2, corresponding to the labels negative, neutral and positive—this can be seen with the .features.names attribute:\n\ndataset.features[\"label\"].names\n\n['negative', 'neutral', 'positive']\n\n\n\nset(dataset['label'])\n\n{0, 1, 2}\n\n\n\nprint(dataset[2]['sentence'])\nprint('---')\nprint(dataset[2]['label'])\n\nIn the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n---\n2\n\n\nFollowing the synthetic data creation notebook, I’ll add a label_text column to the dataset, which holds the actual label string.\n\n# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\nlabel_map = {i: label_text for i, label_text in enumerate(dataset.features[\"label\"].names)}\n\ndef add_label_text(example):\n    example[\"label_text\"] = label_map[example[\"label\"]]\n    return example\n\ndataset = dataset.map(add_label_text)\n\nprint(dataset)\n\n\n\n\nDataset({\n    features: ['sentence', 'label', 'prompt', 'label_text'],\n    num_rows: 2264\n})\n\n\n\nprint(dataset[2]['sentence'])\nprint('---')\nprint(dataset[2]['label_text'])\n\nIn the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n---\npositive\n\n\nNow I can convert the dataset to a DataFrame, append the model outputs and calculate the accuracy. Note that I’ll replace any outputs column value that is not negative, positive, or neutral with NA.\n\ndf = dataset.to_pandas()\ndf['outputs'] = pd.Series(outputs).str.strip()\ndf['outputs'] = df['outputs'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['outputs']\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sentence\n      label\n      prompt\n      label_text\n      outputs\n      lm_match\n    \n  \n  \n    \n      0\n      According to Gran , the company has no plans t...\n      1\n      Instruct: label the following TEXT with a sing...\n      neutral\n      neutral\n      True\n    \n    \n      1\n      For the last quarter of 2010 , Componenta 's n...\n      2\n      Instruct: label the following TEXT with a sing...\n      positive\n      other\n      False\n    \n    \n      2\n      In the third quarter of 2010 , net sales incre...\n      2\n      Instruct: label the following TEXT with a sing...\n      positive\n      positive\n      True\n    \n    \n      3\n      Operating profit rose to EUR 13.1 mn from EUR ...\n      2\n      Instruct: label the following TEXT with a sing...\n      positive\n      positive\n      True\n    \n    \n      4\n      Operating profit totalled EUR 21.1 mn , up fro...\n      2\n      Instruct: label the following TEXT with a sing...\n      positive\n      positive\n      True\n    \n  \n\n\n\n\n\ndf['outputs'].unique()\n\narray(['neutral', 'other', 'positive', 'negative'], dtype=object)\n\n\n\ndf['label_text'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\n\ndf['lm_match'].mean()\n\n0.5746466431095406\n\n\nThis model/prompt combo is performing better than random chance (33%) but still a far cry from the accuracy obtained in Moritz’ post (94% by Mixtral-8x7B-Instruct-v0.1 w/ Chain-of-Thought and 94% by GPT4 w/ a simple prompt). I believe phi-2 can do better! I just have to find the right prompt.\nWith Prompt A, phi-2 is really good at predicting negative and positive sentiment but terrible at predicting neutral sentiment. That makes sense! There’s a lot of nuance in determining if something is neutral.\n\n\nShow make_cm function\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n    \n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['outputs'], labels=['negative', 'neutral', 'positive', 'other'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive', 'other'])\n    \n    # I chose 8x8 so it fits on one screen but still is large\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp.plot(ax=ax,text_kw={'fontsize': 16}, cmap='Blues', colorbar=False);\n    \n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(18)\n    ax.yaxis.label.set_fontsize(18)\n    \n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=16)\n    ax.tick_params(axis='x', labelsize=16)\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\nPrompt B\nI’ll create a helper function to take my prompt, add the formatted prompt to the dataset, generate responses, and calculate and return the DataFrame and accuracy.\n\n\nShow add_prompt and generate_responses functions\ndef add_prompt(item, prompt):\n        item['prompt'] = prompt.format(text=item['sentence'])\n        return item\n    \ndef generate_responses(dataset, prompt, temp=0.1):\n    ## add formatted prompt as a new column in dataset\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    # check that the prompt is correctly formatted\n    print(dataset[0]['prompt'])\n    print('---------')\n    \n    # generate responses\n    outputs = []\n        \n    for out in pipe(KeyDataset(dataset, \"prompt\"), max_new_tokens=1, do_sample=True, temperature=temp, return_full_text=False):\n        outputs.append(out[0]['generated_text'].strip().lower())\n    \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['outputs'] = pd.Series(outputs)\n    df['lm_match'] = df['label_text'] == df['outputs']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\nMaking sure it works with Prompt A as expected:\n\ndf, acc = generate_responses(dataset, promptA)\nacc\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nOutput:\n---------\n\n\n0.575530035335689\n\n\nGood. I’ll now try a prompt where I add the instruction again after the TEXT:\n\npromptB = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptB)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\n---------\n\n\n\nacc\n\n0.8091872791519434\n\n\nWow! Just reiterating the instruction before Output boosted the accuracy by 23%.\nWith Prompt B phi-2 is significantly better at predicting neutral sentiment, although it got slightly worse at predicting negative and positive sentiment.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\nPrompt C\nI’ll now try few-shot prompting, using examples from the dataset. Even though I am changing company name and monetary values I would think that including those examples as part of the classification accuracy would be considered data contamination so I’ll remove those examples from the dataset.\nThat way I can just use the actual examples without changing them.\n\ndataset[0]\n\n{'sentence': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n 'label': 1,\n 'prompt': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nOutput:',\n 'label_text': 'neutral'}\n\n\n\ndataset[1]\n\n{'sentence': \"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n 'label': 2,\n 'prompt': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nOutput:\",\n 'label_text': 'positive'}\n\n\n\ndataset[292]\n\n{'sentence': 'Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n 'label': 0,\n 'prompt': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nOutput:',\n 'label_text': 'negative'}\n\n\n\n\nShow promptC\npromptC = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:positive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:negative\n\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\"\"\"\n\n\n\nexclude_idxs = [0, 1, 292]\n\n\nfew_shot_ds\n\nDataset({\n    features: ['sentence', 'label', 'prompt', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\ndef ds_subset(dataset, exclude_idxs, columns=[0, 1, 2, 3]):\n    idxs = list(range(len(dataset)))\n    idxs = [x for x in idxs if x not in exclude_idxs]\n    ddf = dataset.to_pandas()\n    new_ds = Dataset.from_pandas(ddf.iloc[idxs, columns])\n    return new_ds\n\n\nfew_shot_ds = ds_subset(dataset, exclude_idxs)\n\n\nlen(few_shot_ds)\n\n2261\n\n\n\nfew_shot_ds[0]\n\n{'sentence': 'In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .',\n 'label': 2,\n 'prompt': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nOutput:',\n 'label_text': 'positive',\n '__index_level_0__': 2}\n\n\n\nfew_shot_ds[1]\n\n{'sentence': 'Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .',\n 'label': 2,\n 'prompt': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\\nOutput:',\n 'label_text': 'positive',\n '__index_level_0__': 3}\n\n\n\nfew_shot_ds[292]\n\n{'sentence': 'HELSINKI ( AFX ) - Nokian Tyres reported a fourth quarter pretax profit of 61.5 mln eur , up from 48.6 mln on the back of strong sales .',\n 'label': 2,\n 'prompt': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: HELSINKI ( AFX ) - Nokian Tyres reported a fourth quarter pretax profit of 61.5 mln eur , up from 48.6 mln on the back of strong sales .\\nOutput:',\n 'label_text': 'positive',\n '__index_level_0__': 295}\n\n\n\ndf, acc = generate_responses(few_shot_ds, promptC)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:positive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:negative\n\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\n---------\n\n\n\nacc\n\n0.7302078726227333\n\n\nInteresting. I was not expecting the accuracy to drop! The model improved it’s ability to predict negative and positive sentiment (getting close to almost 100% for those classes) but got worse at predicting neutral sentiment (by about 400 responses).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-2_sc_C.csv', index=False)\n\n\n\nPrompt D\nGiven that the model is performing poorly on neutral sentiment, I’ll increase the number of neutral examples in the few-shot prompt.\n\n[i for i,x in enumerate(few_shot_ds['label_text']) if x=='neutral'][:10]\n\n[35, 36, 37, 38, 261, 262, 263, 264, 265, 266]\n\n\n\nfew_shot_ds[35]\n\n{'sentence': \"At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\",\n 'label': 1,\n 'prompt': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nOutput:\",\n 'label_text': 'neutral',\n '__index_level_0__': 37}\n\n\n\nfew_shot_ds[36]\n\n{'sentence': 'STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .',\n 'label': 1,\n 'prompt': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nOutput:',\n 'label_text': 'neutral',\n '__index_level_0__': 38}\n\n\n\nfew_shot_ds[37]\n\n{'sentence': 'A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .',\n 'label': 1,\n 'prompt': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nOutput:',\n 'label_text': 'neutral',\n '__index_level_0__': 39}\n\n\n\n\nShow promptD\npromptD = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:positive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:negative\n\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\"\"\"\n\n\n\npromptD_ds = ds_subset(few_shot_ds, exclude_idxs=[35, 36, 37])\n\n\npromptD_ds\n\nDataset({\n    features: ['sentence', 'label', 'prompt', 'label_text', '__index_level_0__'],\n    num_rows: 2258\n})\n\n\n\ndf, acc = generate_responses(promptD_ds, promptD)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:positive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:negative\n\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\n---------\n\n\n\nacc\n\n0.7360496014171833\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\nA tiny increase in accuracy (~20 more correct neutral predictions, 7 more correct negative predictions, but 15 fewer correct positive predictions).\n\ndf.to_csv('/notebooks/phi-2_sc_D.csv', index=False)\n\n\n\nPrompt E\nI’ll add an introductory text\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\nas done by Moritz in his example and see if that improves the accuracy.\n\n\nShow promptE\npromptE = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:positive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:negative\n\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptD_ds, promptE)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:positive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:negative\n\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:neutral\n\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput:\n---------\n\n\nThat’s a nice boost! That increases the accuracy by 13%.\n\nacc\n\n0.8697962798937112\n\n\nThere’s a slight drop in accuracy of positives but ~300 response increase in correct neutral predictions.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-2_sc_E.csv', index=False)\n\n\n\nPrompt F\nNext, I’ll rephrase the prompt to say “Instruct” instead of “TEXT”.\n\n\nShow promptF\npromptF = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nlabel the following text with a single word: negative, positive, or neutral\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the text with a single word: negative, positive, or neutral\nOutput:positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the text with a single word: negative, positive, or neutral\nOutput:negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: {text}\nlabel the text with a single word: negative, positive, or neutral\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptD_ds, promptF)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nlabel the following text with a single word: negative, positive, or neutral\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the text with a single word: negative, positive, or neutral\nOutput:positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the text with a single word: negative, positive, or neutral\nOutput:negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nlabel the text with a single word: negative, positive, or neutral\nOutput:neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nlabel the text with a single word: negative, positive, or neutral\nOutput:\n---------\n\n\n\nacc\n\n0.8436669619131976\n\n\nWith that phrasing, the correct negatives and positives increase by 2 and 1 response, respectively, but the neutrals decrease by ~60 responses.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-2_sc_F.csv', index=False)\n\n\n\nPrompt G\nBefore I add more few-shot examples, I’ll rephrase some of the wording. I’ll replace the phrase:\nlabel the following text with a single word: negative, positive, or neutral\nwith:\nRespond with only one of these words: negative, positive, or neutral\nsince “Respond” seems more direct than “label”.\n\n\nShow promptG\npromptG = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral\nOutput:positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral\nOutput:negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptD_ds, promptG)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral\nOutput:positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral\nOutput:negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral\nOutput:neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral\nOutput:\n---------\n\n\n/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n\n\nNo such luck! The accuracy is still around 84%.\n\nacc\n\n0.8414526129317981\n\n\nThis prompt resulted in 8 more correct positives but fewer correct negative and neutral predictions.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-2_sc_G.csv', index=False)\n\n\n\nPrompt H\nI’ll make a few grammatical changes (like adding a period at the end of the “Respond with…” instruction or adding an extra space after “Output:’.\n\n\nShow promptH\npromptH = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral.\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptD_ds, promptH)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral.\nOutput:\n---------\n\n\n/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n\n\nSurprisingly, this boosts the accuracy to about 91%\n\nacc\n\n0.9065544729849424\n\n\nThe number of correct neutral predictions increases by 160 and positives by 2. The number of correct negative predictions drops by 15.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-2_sc_H.csv', index=False)\n\n\n\nPrompt I\nNow that I’ve broken the 90% threshold, I’ll take a closer look at the misses by the model (in Excel—I like to look at my data at least once in a spreadsheet!).\nI’ll start by looking at the 160 neutral targets that were predicted as positive. I went through the 160 examples and tagged each as one of the two following categories (with example phrases shown):\n\nneutral (general info, not necessarily positive or negative, need more context) (139 responses):\n\n“The investment will be worth approximately EUR 100mn .”\n“The company expects its net sales for the whole of 2007 to be EUR 950mn-1 ,000 mn .”\n“Deliveries of Nokia 1112 , Nokia 2310 and Nokia 2610 are expected to start in the second quarter of 2006 .”\n\nneutral (random information unrelated to positive/negative financial metrics) (21 responses)\n\n“Also the development of online businesses will continue .”\n“Once your plants are in the ground they will continue to grow , but the success of any garden lies in how well it ’s maintained .”\n“The total donation amount is EUR 1,115,000 .”\n\n\nI scanned the 1219 true positive neutrals and they didn’t seem very different from the misses:\n\n“The original contract was signed last summer .”\n“It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .”\n“The offer price is \\$ 35 million , including cash of \\$ 10 million as net debt assumption of FACE , and \\$ 20 million worth of Cencorp shares to be issued to Savcor .”\n\nI did some basic text searches in the 1219 correct neutral predictions and found that:\n\n552 phrases (45%) contained a number\n149 phrases (12%) contained the string “EUR” or “USD”\n34 phrases (3%) contained the string “sales” or “profit”\n\nI did the same searches in the 160 neutral phrases predicted as positive and found that:\n\n112 phrases (70%) contained a number\n67 phrases (42%) contained the string “EUR” or “USD”\n38 phrases (24%) contained the string “sales” or “profit”\n\nThe occurrences of “trigger” words that falsely indicate financial sentiment (numbers, currency, “sales”, “profit”) are at a higher percentage in the neutral phrases predicted as positive. I’m guessing (as one only can with LLMs) that’s why the model got tripped up on them.\nI’ll try two options to improve my model’s performance:\n\nincrease the number of few-shot examples (I’ve heard from the folks that wrote What We’ve Learned from a Year of Building with LLMs that it’s not uncommon to need a few dozen examples).\nchange the prompt language\n\n\n# remove few-shot examples from dataset\npromptI_ds = ds_subset(dataset, exclude_idxs=[0, 1, 292, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283], columns=[0, 1, 2])\n\n\npromptI_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2250\n})\n\n\n\n\nShow promptI\npromptI = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The contract value amounts to about EUR11m , the company added .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral.\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptI_ds, promptI)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The contract value amounts to about EUR11m , the company added .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral.\nOutput:\n---------\n\n\nThat took considerably longer to run (makes sense, almost triple the input tokens) and the overall accuracy decreased.\n\nacc\n\n0.8866666666666667\n\n\nA massive improvement in predicting neutral phrases correctly! With the 8 additional few-shot examples phi-2 is predicting neutrals with almost 100% accuracy, 149 more correct phrases than before.\nHowever, it’s performly worse for positive (142 fewer correct predictions) and negative (69 fewer correct predictions) sentiment phrases.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-2_sc_I.csv', index=False)\n\n\n\nPrompt J\nI’ll see if adding a few more negative and positive examples improves the accuracy for those classes.\n\n# remove few-shot examples from dataset\npromptJ_ds = ds_subset(dataset, exclude_idxs=[1, 2, 3, 4, 292, 293, 294, 347, 0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283], columns=[0, 1, 2])\n\n\npromptJ_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2244\n})\n\n\n\n\nShow promptJ\npromptJ = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral.\n\nExamples:\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: However , the growth margin slowed down due to the financial crisis .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: 2009 3 February 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR46m in the fourth quarter of 2009 from a year-earlier profit of EUR45m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The contract value amounts to about EUR11m , the company added .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral.\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptJ_ds, promptJ)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral.\n\nExamples:\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: However , the growth margin slowed down due to the financial crisis .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: 2009 3 February 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR46m in the fourth quarter of 2009 from a year-earlier profit of EUR45m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: negative\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: positive\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The contract value amounts to about EUR11m , the company added .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\nRespond with only one of these words: negative, positive, or neutral.\nOutput: neutral\n\nInstruct: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\nRespond with only one of these words: negative, positive, or neutral.\nOutput:\n---------\n\n\nThat seems to have worked! The accuracy is at 91%, the highest it’s reached thus far.\n\nacc\n\n0.910873440285205\n\n\nThere seems to be a trade-off (at least with this prompting strategy) between true positives. The number of correct negative predictions increased by 18, positive by 60 and neutral decreased by 28.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\nPrompt K\nAdding few-shot examples improved the accuracy by less than 1%. What if I changed the instruction language instead? I’ll go back to Prompt H (90.66% accuracy, 6 examples) and see if modifying the instruction helps.\nI’ll start by adding the phrase “if you’re not sure, respond with neutral.”\n\n\nShow promptK\npromptK = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput:\"\"\"\n\n\n\n# remove few-shot examples from dataset\npromptK_ds = ds_subset(dataset, exclude_idxs=[0, 1, 292, 37, 38, 39], columns=[0, 1, 2])\n\n\npromptK_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2258\n})\n\n\n\ndf, acc = generate_responses(promptK_ds, promptK)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput: neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral. If you're not sure, respond with neutral.\nOutput:\n---------\n\n\nExcellent! I achieve my highest accuracy thus far (91.4% with a 6-shot prompt, a 0.32% increase from my 20-shot prompt).\n\nacc\n\n0.9140832595217007\n\n\nThe number of correct neutral (+99) phrases increased and negative (-39) and positive (-43) decreased.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf[['sentence', 'label_text', 'outputs', 'lm_match']].to_csv('/notebooks/phi-2_sc_K.csv', index=False)\n\n\n\nPrompt L\nI’ll try a different phrase to try and help the model avoid classifing neutrals as positive:\n“if it’s not clear why an investor’s investment would increase or decrease, respond with neutral.”\n\n\nShow promptL\npromptL = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptK_ds, promptL)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput: neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral. If it's not clear why an investor's investment would increase or decrease, respond with neutral.\nOutput:\n---------\n\n\nNope! This instruction language doesn’t help.\n\nacc\n\n0.9021257750221435\n\n\nCompared to Prompt H, neutral (+57) correct predictions increased while negative (-34) and positive (-33) decreased.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\nPrompt M\nI’ll try a third modification to the instruction by using the following phrase:\n“If the amount of money is not explicitly increasing or decreasing, respond with neutral”\n\n\nShow promptM\npromptM = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptK_ds, promptM)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\n---------\n\n\nNice! I’m knocking on 92%’s door.\n\nacc\n\n0.9193976970770593\n\n\nCompared to Prompt H, this prompt results in a decrease of correct negatives (-13) and positives (-38) offset by a substantial increase in correct neutrals (+80).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf[['sentence', 'label_text', 'outputs', 'lm_match']].to_csv('/notebooks/phi-2_sc_M.csv', index=False)\n\n\n\nPrompt N\nI’ll see if I can get away with reducing the number of examples in the prompt.\n\n\nShow promptN\npromptN = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: negative\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\"\"\"\n\n\n\n# remove few-shot examples from dataset\npromptN_ds = ds_subset(dataset, exclude_idxs=[0, 1, 292], columns=[0, 1, 2])\n\n\npromptN_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\ndf, acc = generate_responses(promptN_ds, promptN)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: negative\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\n---------\n\n\nNope. The accuracy drops by 1%.\n\nacc\n\n0.9106590004422822\n\n\nCompared to my best performing prompt (Prompt M, 91.9%) this has fewer correct negatives (-32) and positives (-6) which offset the increase in correct neutrals (+21).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\nPrompt O\nI’ll see if changing the examples gives me a different result.\n\n# remove few-shot examples from dataset\npromptO_ds = ds_subset(dataset, exclude_idxs=[284, 64, 359], columns=[0, 1, 2, 3])\n\n\npromptO_ds\n\nDataset({\n    features: ['sentence', 'label', 'prompt', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\n\nShow promptO\npromptO = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nExamples:\n\nInstruct: The company generates net sales of about 600 mln euro $ 775.5 mln annually and employs 6,000 .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: Operating profit increased by 145.1 % to EUR 8.3 mn from EUR 3.4 mn .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: positive\n\nInstruct: Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier ..\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: negative\n\nInstruct: {text}\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\"\"\"\n\n\n\ndf, acc = generate_responses(promptO_ds, promptO)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nExamples:\n\nInstruct: The company generates net sales of about 600 mln euro $ 775.5 mln annually and employs 6,000 .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: Operating profit increased by 145.1 % to EUR 8.3 mn from EUR 3.4 mn .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: positive\n\nInstruct: Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier ..\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: negative\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\n---------\n\n\nNope! I’ll stick with my original prompt.\n\nacc\n\n0.9084475895621407\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\n\nPrompt P\nI recently saw this example from Simon Willison’s blog post where he shared his notes from Anthropic’s prompt engineering tutorial. They recommended the following format for sentiment classification:\n\nIs this review sentiment positive or negative? First, write the best arguments for each side in <positive-argument> and <negative-argument> XML tags, then answer.\n\nI don’t think phi-2 was trained to structure reponses with XML but I’ll give it a try:\n\npromptP = \"\"\"Is this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.\n\nBe concise in your response.\n\n{text}\"\"\"\n\n\nformatted_prompt = promptP.format(text=dataset[0]['sentence'])\nprint(formatted_prompt)\n\nIs this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.\n\nBe concise in your response.\n\nAccording to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndataset[0]['label_text']\n\n'neutral'\n\n\nIt looks like this kind of prompt doesn’t work well zero-shot:\n\noutput = pipe(\n    formatted_prompt,\n    max_new_tokens=300,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\noutput[0][\"generated_text\"]\n\n'\\n\\n<negative-argument>\\n<answer>negative</answer>\\n</negative-argument>\\n<neutral-argument>\\n<answer>neutral</answer>\\n</neutral-argument>\\n<positive-argument>\\n<answer>positive</answer>\\n</positive-argument>\\n\\n<negative-argument>\\n<answer>negative</answer>\\n</negative-argument>\\n<neutral-argument>\\n<answer>neutral</answer>\\n</neutral-argument>\\n<positive-argument>\\n<answer>positive</answer>\\n</positive-argument>\\n\\n<negative-argument>\\n<answer>negative</answer>\\n</negative-argument>\\n<neutral-argument>\\n<answer>neutral</answer>\\n</neutral-argument>\\n<positive-argument>\\n<answer>positive</answer>\\n</positive-argument>\\n\\n<negative-argument>\\n<answer>negative</answer>\\n</negative-argument>\\n<neutral-argument>\\n<answer>neutral</answer>\\n</neutral-argument>\\n<positive-argument>\\n<answer>positive</answer>\\n</positive-argument>\\n\\n<negative-argument>\\n<answer>negative</answer>\\n</negative-argument>\\n<neutral-argument>\\n<answer>neutral</answer>\\n</neutral-argument>\\n<positive-argument>\\n<answer>positive</answer>\\n'\n\n\nI’ll give it an example and see if that helps:\n\n\nShow promptP\npromptP = \"\"\"Is this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.\n\nExample:\n\n<text>According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n<instruction>Is this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.<text> Be concise in your response.<instruction>\n\n<response>\n<negative-sentiment>This is a negative sentence because they are not moving production to where their growth is.<negative-sentiment>\n<neutral-sentiment>This is a neutral sentence because it does not indicate any strong negative or positive sentiment.<neutral-sentiment> \n<positive-sentiment>This is a positive sentence because they are growing in a region where they don't do production.<positive-sentiment>\n<answer>neutral<answer>\n<response>\n\n\n\n<text>{text}<text>\n<instruction>Is this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.<text> Be concise in your response.<instruction>\n<response>\n\"\"\"\n\n\n\nformatted_prompt = promptP.format(text=dataset[1]['sentence'])\nprint(formatted_prompt)\n\nIs this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.\n\nExample:\n\n<text>According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n<instruction>Is this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.<text> Be concise in your response.<instruction>\n\n<response>\n<negative-sentiment>This is a negative sentence because they are not moving production to where their growth is.<negative-sentiment>\n<neutral-sentiment>This is a neutral sentence because it does not indicate any strong negative or positive sentiment.<neutral-sentiment> \n<positive-sentiment>This is a positive sentence because they are growing in a region where they don't do production.<positive-sentiment>\n<answer>neutral<answer>\n<response>\n\n\n\n<text>For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .<text>\n<instruction>Is this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.<text> Be concise in your response.<instruction>\n<response>\n\n\n\nIt’s most likely that my example is not written well, or perhaps phi-2 doesn’t fare well with structured response. Regardless, the response is not desirable:\n\noutput = pipe(\n    formatted_prompt,\n    max_new_tokens=300,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\noutput[0][\"generated_text\"]\n\n'<negative-sentiment>This is a negative sentence because they had a loss instead of a profit.<negative-sentiment>\\n<neutral-sentiment>This is a neutral sentence because it does not indicate any strong negative or positive sentiment.<neutral-sentiment>\\n<positive-sentiment>This is a positive sentence because they had a significant increase in sales.<positive-sentiment>\\n<answer>negative<answer>\\n<response>\\n\\n\\n\\n<text>The company expects to see a further increase in sales in the first quarter of 2011, with a forecast of EUR150m to EUR160m.<text>\\n<instruction>Is this financial news sentiment negative, neutral or positive? First, write the best arguments for each side in <negative-argument>, <neutral-argument> and <positive-argument> XML tags, then answer with a single word (negative, neutral or positive) using <answer> XML tags.<text> Be concise in your response.<instruction>\\n<response>\\n<negative-sentiment>This is a negative sentence because they are forecasting a significant increase in sales.<negative-sentiment>\\n<neutral-sentiment>This is a neutral sentence because it does not indicate any strong negative or positive sentiment.<neutral-sentiment>\\n<positive-sentiment>This is a positive sentence because they are expecting a significant increase in sales.<positive-sentiment>\\n<answer>positive<'\n\n\nI’ll see if using Chain-of-Thought prompting works with phi-2:\n\n\nShow promptP\npromptP = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nThink step by step and provide your single-word answer (negative, positive, or neutral) after ###Answer:\n\n{text}\n\nThink step by step and provide your single-word answer (negative, positive, or neutral) after ###Answer:\n\"\"\"\n\n\n\nformatted_prompt = promptP.format(text=dataset[1]['sentence'])\nprint(formatted_prompt)\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nThink step by step and provide your single-word answer (negative, positive, or neutral) after ###Answer:\n\nFor the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\n\nThink step by step and provide your single-word answer (negative, positive, or neutral) after ###Answer:\n\n\n\n\noutput = pipe(\n    formatted_prompt,\n    max_new_tokens=300,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\noutput[0][\"generated_text\"]\n\n\"\\nThe company's net sales increased significantly, indicating a positive trend. However, the zero pre-tax profit suggests a potential negative impact on the company's financial performance.\\n\\n###Answer:\\n\\nNegative\\n\\n###Exercise 2:\\n\\nBased on the given information, what could be a possible reason for Componenta's net sales to double in the last quarter of 2010?\\n\\nThink step by step and provide your answer after ###Answer:\\n\\nOne possible reason for Componenta's net sales to double could be an increase in demand for their products in the market.\\n\\n###Answer:\\n\\nIncreased demand for their products.\\n\\n###Exercise 3:\\n\\nWhat could be a potential impact of Componenta's zero pre-tax profit on their investors?\\n\\nThink step by step and provide your answer after ###Answer:\\n\\nThe zero pre-tax profit could negatively impact Componenta's investors, as it suggests a potential decrease in the company's financial performance and potential loss of investment.\\n\\n###Answer:\\n\\nPotential decrease in the company's financial performance and potential loss of investment.\\n\\n###Exercise 4:\\n\\nWhat could be a possible reason for Componenta's pre-tax loss in the last quarter of 2010?\\n\\nThink step by step and provide your answer after ###Answer:\\n\\nOne possible reason for Componenta's pre-tax loss could be a decrease in demand for their products, leading to a\"\n\n\nphi-2 is not instruction-tuned so it makes sense that it’s not responding coherently to these instructions. Or, my instructions are not well-written!\n\n\nRunning Prompt M 10 Times\nTo see how its performance varies, I’ll run inference on the dataset 10 times with my best performing prompt (Prompt M, 91.9%) and store the accuracy:\nI’ll modify generate_responses so it doesn’t print out the prompt text.\n\n\nShow generate_responses\ndef generate_responses(dataset, prompt, temp=0.1, log=True):\n    ## add formatted prompt as a new column in dataset\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    # check that the prompt is correctly formatted\n    if log:\n        print(dataset[0]['prompt'])\n        print('---------')\n    \n    # generate responses\n    outputs = []\n        \n    for out in pipe(KeyDataset(dataset, \"prompt\"), max_new_tokens=1, do_sample=True, temperature=temp, return_full_text=False):\n        outputs.append(out[0]['generated_text'].strip().lower())\n    \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['outputs'] = pd.Series(outputs)\n    df['lm_match'] = df['label_text'] == df['outputs']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\naccs = []\n\nfor _ in range(10):\n    _ , acc = generate_responses(promptK_ds, promptM, log=False)\n    accs.append(acc)\n\nI would say this model/prompt combo is quite consistent! Furthermore, it’s competitive with GPT-4 (94% accuracy). I’ll consider this experimentation process done for now.\n\npd.Series(accs).describe()\n\ncount    10.000000\nmean      0.919132\nstd       0.001786\nmin       0.916298\n25%       0.918512\n50%       0.919619\n75%       0.919841\nmax       0.921612\ndtype: float64"
  },
  {
    "objectID": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html#final-thoughts",
    "href": "posts/2024-08-31-tinysentiment-phi-2-sentiment-classification/index.html#final-thoughts",
    "title": "Sentiment Classification with phi-2",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere is a summary of results from this notebook:\nHere are the results from this notebook:\n\n\n\nPrompt\nStrategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n58%\n97% (295/303)\n37% (520/1391)\n86% (490/570)\n\n\nB\n0-Shot\n81%\n87% (264/303)\n87% (1207/1391)\n63% (361/570)\n\n\nC\n3-Shot\n73%\n96% (291/302)\n58% (803/1390)\n98% (557/569)\n\n\nD\n6-Shot\n74%\n99% (298/302)\n59% (822/1387)\n95% (542/569)\n\n\nE\n6-Shot\n87%\n98% (296/302)\n82% (1131/1387)\n94% (537/569)\n\n\nF\n6-Shot\n84%\n99% (298/302)\n77% (1069/1387)\n95% (538/569)\n\n\nG\n6-Shot\n84%\n98% (295/302)\n76% (1059/1387)\n96% (546/569)\n\n\nH\n6-Shot\n91%\n93% (280/302)\n88% (1219/1387)\n96% (548/569)\n\n\nI\n14-Shot\n89%\n73% (221/302)\n99% (1368/1379)\n71% (406/569)\n\n\nJ\n20-Shot\n91%\n80% (238/299)\n97% (1340/1379)\n82% (466/566)\n\n\nK\n6-Shot\n91%\n80% (241/302)\n95% (1318/1387)\n89% (505/569)\n\n\nL\n6-Shot\n90%\n81% (246/302)\n92% (1276/1387)\n91% (515/569)\n\n\nM\n6-Shot\n92%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\nN\n3-Shot\n91%\n78% (235/302)\n95% (1320/1390)\n89% (504/569)\n\n\nO\n3-Shot\n91%\n78% (237/302)\n96% (1333/1390)\n85% (484/569)\n\n\n\nWorking with these smaller LLMs is a different experience than working with Claude via the Anthropic API. I feel more at ease with experimenting different approaches when the model is running for free on a Paperspace GPU. It’s much less costly to make mistakes—I only lose time (which is not nothing, but better than a bloated bill).\nPhi-2 has interesting quirks. For example, repeating the instruction after the given sentence increased its accuracy by 23%. 20-shot and 14-shot prompts performed worse than a 6-shot prompt. Giving it additional clarity on how to handle neutral sentiments improved its accuracy (okay, that one is not so quirky).\nI can’t help but think that my lack of prompt engineering experience with phi-2 is preventing me from maximizing its capabilities. Perhaps I could rival the 94% gold standard that was achieved in Moritz Laurer’s blog post (with GPT4 and Mixtral-8x7B) and by Opus and Sonnet in my Claude experiments. Instead, phi-2 achieves comparable performance (92%) with Haiku—which is not bad!\nIn a future notebook, I’ll do prompt engineering with phi-3 and phi-3.5 to see how those models perform on this dataset.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "",
    "text": "In this blog post I’ll work through creating a hybrid search (keyword search + semantic search) baseline for a project I’m working on called fastbookRAG. In this project, I’m building a hybrid search + LLM pipeline to answer questions from the end-of-chapter Questionnaires in the freely available fastai textbook. For now, I’m taking the place of the LLM in the pipeline (using the context retrieved using keyword/semantic search to answer questions). In this notebook, I’ll focus on retrieving the relevant context needed to answer questions from Chapter 1. In future notebooks, I’ll be expanding this to all 8 lessons in Part 1 of the fastai course.\nIn two previous blog posts I was able to answer 72% of the Chapter 1 Questionnairse questions using full text search and 76% of questions using cosine similarity. I’ll try to beat that result using a combined approach.\nI’ll summarize my results from this notebook in the table below. Cosine Similarity is represented as “CS” and full text search as “BM25”:\n\n\n\nApproach\nChunk Size (Paragraphs)\nQuestions Answered\n\n\n\n\nTop-5 CS + Top-3 BM25\n1 (CS), 3 (BM25)\n85%\n\n\nTop-3 CS + Top-2 BM25\n1 (CS), 3 (BM25)\n76%\n\n\nTop-5 Weighted Average\n1\n76%\n\n\nTop-3 Weighted Average\n1\n64%\n\n\nTop-3 CS of Top-10 BM25 Results\n1\n58%\n\n\nTop-3 CS of Top-10 BM25 Results\n3\n58%\n\n\nTop-1 CS of Top-10 BM25 Results\n1\n52%\n\n\nTop-1 CS of Top-12 BM25 Results\n3\n48%\n\n\nTop-1 Weighted Average\n1\n48%\n\n\n\n\n\nShow imports\nimport sqlite3\nimport json\nimport re\nimport pandas as pd, numpy as np\nimport textwrap\nimport torch\nfrom torch import tensor\nimport torch.nn.functional as F\n\n!pip install sentence-transformers -Uqq\nfrom sentence_transformers import SentenceTransformer\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")"
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#chunking-the-chapter-1-notebook-into-paragraphs",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#chunking-the-chapter-1-notebook-into-paragraphs",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Chunking the Chapter 1 Notebook into Paragraphs",
    "text": "Chunking the Chapter 1 Notebook into Paragraphs\nAs usual, I’ll start by chunking the chapter 1 notebook into paragraphs and store them into a sqlite database. I’ll wrap the databse loading code into a function so that I can easily reuse it for different chunking strategies.\n\n\nShow the chunking code\ndef get_chunks(notebook_path):\n    with open(notebook_path, 'r', encoding='utf-8') as file:\n        notebook = json.load(file)\n\n    chunks = []\n    current_header = \"\"\n\n    def add_chunk(content):\n        if content.strip():\n            chunks.append(f\"{current_header}\\n\\n{content.strip()}\")\n\n    for cell in notebook['cells']:\n        if cell['cell_type'] == 'markdown':\n            content = ''.join(cell['source'])\n            header_match = re.match(r'^(#+\\s+.*?)$', content, re.MULTILINE)\n            if header_match:  # Check if the cell starts with a header\n                current_header = header_match.group(1)\n                # Add any content after the header in the same cell\n                remaining_content = content[len(current_header):].strip()\n                if remaining_content:\n                    paragraphs = re.split(r'\\n\\s*\\n', remaining_content)\n                    for paragraph in paragraphs:\n                        add_chunk(paragraph)\n            else:\n                paragraphs = re.split(r'\\n\\s*\\n', content)\n                for paragraph in paragraphs:\n                    add_chunk(paragraph)\n        elif cell['cell_type'] == 'code':\n            code_content = '```python\\n' + ''.join(cell['source']) + '\\n```'\n            add_chunk(code_content)\n\n    return chunks\n\ndef filter_chunks(chunks, exclude_headers):\n  filtered_chunks = []\n  for chunk in chunks:\n      lines = chunk.split('\\n')\n      # Check if the first line (header) is in the exclude list\n      if not any(header in lines[0] for header in exclude_headers):\n          filtered_chunks.append(chunk)\n  return filtered_chunks\n\nexclude_headers = [\"Questionnaire\", \"Further Research\"]\n\n\n\nnotebook_path = '01_intro.ipynb'\nchunks = get_chunks(notebook_path)\nassert len(chunks) == 315\nfiltered_chunks = filter_chunks(chunks, exclude_headers)\nassert len(filtered_chunks) == 307\n\n\n\nShow the db loading function\nconn = sqlite3.connect('/content/fastbook.db')\n\ndef load_data(filtered_chunks):\n  conn = sqlite3.connect('/content/fastbook.db')\n  cur = conn.cursor()\n  res = cur.execute(\"\"\"\n\n  CREATE VIRTUAL TABLE fastbook_text\n  USING FTS5(text);\n  \"\"\")\n\n  for string in filtered_chunks:\n    cur.execute(f\"INSERT INTO fastbook_text(text) VALUES (?)\", (string,))\n\n  conn.commit()\n  res = cur.execute(\"SELECT * from fastbook_text\").fetchall()\n  conn.close()\n  return len(res) == len(filtered_chunks)\n\n\n\nload_data(filtered_chunks)\n\nTrue"
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-1-cosine-similarity-of-top-10-bm25-ranked-keyword-search-results",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-1-cosine-similarity-of-top-10-bm25-ranked-keyword-search-results",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Retrieving Top-1 Cosine Similarity of Top-10 BM25-Ranked Keyword Search Results",
    "text": "Retrieving Top-1 Cosine Similarity of Top-10 BM25-Ranked Keyword Search Results\nTo give me some flexibility in which chunks are retrieved, I’ll use cosine similarity on the top-10 BM25-ranked full text search results. In previous experiments my best performing cosine similarity approach used top-5 small chunks and my best performing full text search approach used top-3 large chunks.\nI’ll first create embeddings for the chunked data and the questions:\n\ndata_embs = emb_model.encode(filtered_chunks, convert_to_tensor=True)\ndata_embs.shape\n\ntorch.Size([307, 384])\n\n\n\n# Chapter 1 Questionnaire questions, answers and keywords\ndf = pd.read_csv(\"https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/bc6cd2ab15b64a92ec23796c61702f413fdd2b40/fastbookRAG_evals.csv\")\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nq_embs = emb_model.encode(df['question_text'], convert_to_tensor=True)\nq_embs.shape\n\ntorch.Size([33, 384])\n\n\nI’ll use ORDER BY rank and LIMIT 10 in my SQL query to get the top-10 BM25-ranked results:\n\n\nShow the for-loop + query\nresults = []\nconn = sqlite3.connect('fastbook.db')\ncur = conn.cursor()\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text\n    WHERE fastbook_text MATCH '{words}'\n    ORDER BY rank\n    LIMIT 10\n\n    \"\"\"\n    res = cur.execute(q).fetchall()\n    res = [item[0] for item in res]\n    results.append(res)\n  else:\n    # if keywords == \"No Answer\"\n    res = \"No answer\"\n    results.append(res)\n\n\n\nlen(results), len(results[0])\n\n(33, 10)\n\n\nI’ll embed these chunks of context (up to 10 for each of the 33 questions) so that I can perform cosine similarity between them and the question embeddings. Note that not all questions have 10 chunks as some keyword searches resulted in less than 10 retrieved chunks.\n\nlen(results[-3])\n\n2\n\n\n\nresults_embs = {}\n\nfor idx, sublist in enumerate(results):\n  results_embs[idx] = emb_model.encode(sublist, convert_to_tensor=True)\n\nI’ll apply cosine similarity between the chunk embeddings for the given question’s keyword search result and the question embedding and select the top result:\n\ncs_results = []\n\nfor i, q in enumerate(q_embs):\n  if results[i] != \"No answer\":\n    res = F.cosine_similarity(q, results_embs[i], dim=-1).sort(descending=True)\n    cs_results.append(results[i][res[1][0]])\n  else:\n    cs_results.append('No answer')\n\n\nlen(cs_results)\n\n33\n\n\n\ndf['retrieved_context'] = pd.Series(cs_results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      retrieved_context\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('top1_top10_results.csv', index=False)\n\nUsing the retrieved chunks I was able to answer 17/33 or 52% of the Chapter 1 questions.\nI’m curious: what was the BM25-rank of the context with the top-1 cosine similarity? In other words, did cosine similarity pick the highest ranked keyword search result?\n\ncs_ranks = []\n\nfor i, q in enumerate(q_embs):\n  if results[i] != \"No answer\":\n    res = F.cosine_similarity(q, results_embs[i], dim=-1).sort(descending=True)\n    cs_ranks.append(res[1][0].item())\n  else:\n    cs_ranks.append(None)\n\nFor 16 of the 33 questions, the context with the highest cosine similarity with the question was also the highest BM25-ranked keyword search result. 24/33 highest cosine similarity results were top-3 BM25-ranked chunkcs. For 6 out of 33 questions, the highest cosine similarity was for a context that was not top-3 BM25 ranked.\n\ncs_ranks.value_counts()\n\n\n\n\n\n  \n    \n      \n      count\n    \n  \n  \n    \n      0.0\n      16\n    \n    \n      1.0\n      5\n    \n    \n      2.0\n      3\n    \n    \n      8.0\n      2\n    \n    \n      4.0\n      2\n    \n    \n      7.0\n      1\n    \n    \n      3.0\n      1\n    \n  \n\ndtype: int64"
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-3-cosine-similarity-of-top-10-bm25-ranked-keyword-search-results",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-3-cosine-similarity-of-top-10-bm25-ranked-keyword-search-results",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Retrieving Top-3 Cosine Similarity of Top-10 BM25-Ranked Keyword Search Results",
    "text": "Retrieving Top-3 Cosine Similarity of Top-10 BM25-Ranked Keyword Search Results\nI’ll now retrieve 3 chunks (for each question) that have the top-3 highest cosine similarity values. I expect this to improve my ability to answer questions.\n\ncs_results = []\n\nfor i, q in enumerate(q_embs):\n  if results[i] != \"No answer\":\n    res = F.cosine_similarity(q, results_embs[i], dim=-1).sort(descending=True)\n    res = '\\n'.join([results[i][idx] for idx in res[1][:3]])\n    cs_results.append(res)\n  else:\n    cs_results.append('No answer')\n\n\ndf['retrieved_context'] = pd.Series(cs_results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      retrieved_context\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('top3_top10_results.csv', index=False)\n\nThis approach slightly improves the performance. Now I can answer 19/33 or 58% of the questions with the given retrieved context. This still underperforms each individual approach."
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#concatenating-keyword-search-result-before-applying-cosine-similarity",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#concatenating-keyword-search-result-before-applying-cosine-similarity",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Concatenating Keyword Search Result Before Applying Cosine Similarity",
    "text": "Concatenating Keyword Search Result Before Applying Cosine Similarity\nNext, I’ll try a different approach: I’ll concatenate, three at a time, keyword search results and then perform cosine similarity on those larger chunks. I’ll pick the larger chunk with the highest cosine similarity.\nSince I’m concatenating 3 keyword search results at a time, I’ll increase my LIMIT to 12 (a multiple of 3).\n\n\nShow the for-loop + query\nresults = []\nconn = sqlite3.connect('fastbook.db')\ncur = conn.cursor()\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text\n    WHERE fastbook_text MATCH '{words}'\n    ORDER BY rank\n    LIMIT 12\n\n    \"\"\"\n\n    res = cur.execute(q).fetchall()\n    concatenated_chunks = []\n    for i in range(0, len(res), 3):\n        # Select three tuples at a time\n        chunk = res[i:i+3]\n        # Extract strings and concatenate them\n        concatenated_chunk = '\\n'.join([t[0] for t in chunk])\n        concatenated_chunks.append(concatenated_chunk)\n\n    results.append(concatenated_chunks)\n  else:\n    # if keywords == \"No Answer\"\n    res = \"No answer\"\n    results.append(res)\n\n\n\nlen(results)\n\n33\n\n\n\n\nShow the embedding of results\nresults_embs = {}\n\nfor idx, sublist in enumerate(results):\n  results_embs[idx] = emb_model.encode(sublist, convert_to_tensor=True)\n\n\n\ncs_results = []\n\nfor i, q in enumerate(q_embs):\n  if results[i] != \"No answer\":\n    res = F.cosine_similarity(q, results_embs[i], dim=-1).sort(descending=True)\n    cs_results.append(results[i][res[1][0]])\n  else:\n    cs_results.append('No answer')\n\n\nlen(cs_results)\n\n33\n\n\n\ndf['retrieved_context'] = pd.Series(cs_results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      retrieved_context\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('top1_group-by-3_results.csv', index=False)\n\nThis approach led to a worse performance: I was able to answer only 16 out of the 33 questions, or 48%."
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#storing-larger-chunks-in-the-database",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#storing-larger-chunks-in-the-database",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Storing Larger Chunks in the Database",
    "text": "Storing Larger Chunks in the Database\nSo far, I haven’t been able to beat the individual performance of full text search or cosine similarity. I’ll try something that improved full text search: storing larger chunks of data. To do this, I’ll concatenate three paragraphs at a time before loading them into the database:\n\nlarger_chunks = [\"\\n\".join(filtered_chunks[i:i+3]) for i in range(0, len(filtered_chunks), 3)]\nlen(larger_chunks)\n\n103\n\n\nNote that I’m just deleting my sqlite database file and recreating from scratch when I run load_data:\n\nload_data(larger_chunks)\n\nTrue\n\n\n\n\nShow the for-loop + query\nresults = []\nconn = sqlite3.connect('fastbook.db')\ncur = conn.cursor()\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text\n    WHERE fastbook_text MATCH '{words}'\n    ORDER BY rank\n    LIMIT 10\n\n    \"\"\"\n    res = cur.execute(q).fetchall()\n    res = [item[0] for item in res]\n    results.append(res)\n  else:\n    # if keywords == \"No Answer\"\n    res = \"No answer\"\n    results.append(res)\n\n\n\nlen(results)\n\n33\n\n\n\n\nShow the cosine similarity code\nresults_embs = {}\n\nfor idx, sublist in enumerate(results):\n  results_embs[idx] = emb_model.encode(sublist, convert_to_tensor=True)\n\ncs_results = []\n\nfor i, q in enumerate(q_embs):\n  if results[i] != \"No answer\":\n    res = F.cosine_similarity(q, results_embs[i], dim=-1).sort(descending=True)\n    cs_results.append(results[i][res[1][0]])\n  else:\n    cs_results.append('No answer')\n\n\n\nlen(cs_results)\n\n33\n\n\n\ndf['retrieved_context'] = pd.Series(cs_results)\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      retrieved_context\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## How to Learn Deep Learning\\n\\n> : A PhD is ...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.to_csv('top1_top10-larger_results.csv', index=False)\n\nWith this approach (top-1 cosine similarity for the top-10 larger chunks retrieved by keyword search) I was able to answer 19 out of 33 questions, or 58%."
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#weighted-average-between-cosine-similarity-and-bm25-score",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#weighted-average-between-cosine-similarity-and-bm25-score",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Weighted Average Between Cosine Similarity and BM25 Score",
    "text": "Weighted Average Between Cosine Similarity and BM25 Score\nSo far I’ve been applying cosine similarity to the top-n retrieved chunks based on BM25 score. I’ll now try a different approach: pick the top-n chunks based on a weighted average between cosine similarity and BM25 score.\nTo do this, I’ll revert back to the smaller chunked database. I’ll get the top-10 chunks based on BM25, then I’ll get the top-10 chunks based on cosine similarity. Finally, I’ll normalize each score within each group and then take a weighted average between the two. I’ll then pick top-1, top-3 and top-5 chunks and see how many questions I can answer with the retrieved context.\n\nload_data(filtered_chunks)\n\nTrue\n\n\n\n\nShow the for-loop + query\nresults = []\nconn = sqlite3.connect('fastbook.db')\ncur = conn.cursor()\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text\n    WHERE fastbook_text MATCH '{words}'\n    ORDER BY rank\n    LIMIT 10\n\n    \"\"\"\n    res = cur.execute(q).fetchall()\n    results.append(res)\n  else:\n    # if keywords == \"No Answer\"\n    res = \"No answer\"\n    results.append(res)\n\n\n\n\nShow code to embed the chunks\ndata_embs = emb_model.encode(filtered_chunks, convert_to_tensor=True)\ndata_embs.shape\n\n\ntorch.Size([307, 384])\n\n\n\n\nShow code calculating cosine similarity\ncs_results = []\n\nfor q in q_embs:\n  res = F.cosine_similarity(q, data_embs, dim=1).sort(descending=True)\n  top10_chunks = [filtered_chunks[el.item()] for el in res[1][:10]]\n  top10_cs = [el.item() for el in res[0][:10]]\n  res = list(zip(top10_chunks, top10_cs))\n  cs_results.append(res)\n\n\n\nlen(results), len(cs_results)\n\n(33, 33)\n\n\n\n\nShow code to create DataFrame from results\ndef create_dataframe(nested_list, label):\n    rows = []\n    for question_num, question_data in enumerate(nested_list, start=1):\n        if question_data != \"No answer\":\n          for chunk, score in question_data:\n            rows.append({\n                'label': label,\n                'Question': question_num,\n                'chunk': chunk,\n                'score': score\n            })\n        else:\n          rows.append({\n              'label': label,\n              'Question': question_num,\n              'chunk': 'No answer',\n              'score': np.nan\n          })\n    return pd.DataFrame(rows)\n\n\n\nbm25 = create_dataframe(results, 'BM25')\ncs = create_dataframe(cs_results, 'CS')\nbm25.shape, cs.shape\n\n((277, 4), (330, 4))\n\n\n\n\nShow code to normalize scores within each group\ndef normalize_scores(group):\n    min_score = group['score'].min()\n    max_score = group['score'].max()\n    group['normalized_score'] = (group['score'] - min_score) / (max_score - min_score)\n    return group\n\nbm25['score'] = -1 * bm25['score']\nbm25 = bm25.groupby('Question').apply(normalize_scores).reset_index(drop=True)\ncs = cs.groupby('Question').apply(normalize_scores).reset_index(drop=True)\n\n\n\n\nShow code to calculate prep data for weighted average calcs\n# Function to process each dataframe\ndef process_df(df):\n    # Ensure 'Question' is treated as a string to avoid any numeric mismatch\n    df['Question'] = df['Question'].astype(str)\n    # Create a unique identifier for each question/chunk combination\n    df['question_chunk'] = df['Question'] + '_' + df['chunk']\n    return df\n\n# Process both dataframes\nbm25 = process_df(bm25)\ncs = process_df(cs)\n\n# Create a full set of all question/chunk combinations\nall_combinations = set(bm25['question_chunk']).union(set(cs['question_chunk']))\n\n\n# Function to get scores, using 0 for missing combinations\ndef get_scores(df, all_combinations):\n    scores = df.set_index('question_chunk')['normalized_score']\n    return pd.Series(index=all_combinations).fillna(0).add(scores, fill_value=0)\n\n# Get scores for both dataframes\nbm25_scores = get_scores(bm25, all_combinations)\ncs_scores = get_scores(cs, all_combinations)\n\n\n\nweight_bm25 = 0.3\nweight_cs = 0.7\n\n\n\nShow code to calculate weighted average\nweighted_avg = (weight_bm25 * bm25_scores + weight_cs * cs_scores) / (weight_bm25 + weight_cs)\n\n# Create the final dataframe\nresult = pd.DataFrame({\n    'question_chunk': weighted_avg.index,\n    'weighted_score': weighted_avg.values\n})\n\n# Split 'question_chunk' back into 'Question' and 'chunk'\nresult[['Question', 'chunk']] = result['question_chunk'].str.split('_', n=1, expand=True)\n\n# Reorder columns\nresult = result[['Question', 'chunk', 'weighted_score']]\n\n\n\n\nShow code to get top-n results\ndef get_top_n_chunks(df, n=5):\n    # Function to get top N chunks and concatenate them\n    def top_n_concat(group):\n        top_n = group.nlargest(n, 'weighted_score')\n        return pd.Series({\n            'chunk': ' '.join(top_n['chunk']),\n            'weighted_score': top_n['weighted_score'].mean()\n        })\n\n    # Apply the function to each question group\n    result = df.groupby('Question').apply(top_n_concat).reset_index()\n\n    # Sort the results by Question\n    result = result.sort_values('Question')\n\n    return result\n\n\n\nget_top_n_chunks(result, n=1).to_csv('top1_weighted_average.csv', index=False)\n\nUsing the top-1 weighted average between BM25 and Cosine Similarity yielded chunks that allowed me to answer 16 out of 33 questions, or 48%.\n\nget_top_n_chunks(result, n=3).to_csv('top3_weighted_average.csv', index=False)\n\nUsing the top-3 weighted average between BM25 and Cosine Similarity yielded chunks that allowed me to answer 21 out of 33 questions, or 64% which is the best performing hybrid approach so far.\n\nget_top_n_chunks(result, n=5).to_csv('top5_weighted_average.csv', index=False)\n\nUsing the top-5 weighted average between BM25 and Cosine Similarity allowed me to answer 25 out of 33 questions, or 76%, which is the best performing hybrid approach so far and matches the best Cosine Similarity-only approach."
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-3-cosine-similarity-and-top-2-keyword-search-results",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-3-cosine-similarity-and-top-2-keyword-search-results",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Retrieving top-3 Cosine Similarity and top-2 Keyword Search Results",
    "text": "Retrieving top-3 Cosine Similarity and top-2 Keyword Search Results\nThe next approach I’ll try: returning the top-3 chunks retrieved using Cosine Similarity and the top-2 chunks retrieved using Keyword Search.\n\n\nShow code to get top-n results\ndef get_top_n_chunks(df, n=5):\n    # Function to get top N chunks and concatenate them\n    def top_n_concat(group):\n        top_n = group.nlargest(n, 'normalized_score')\n        return pd.Series({\n            'chunk': ' '.join(top_n['chunk']),\n            'normalized_score': top_n['normalized_score'].mean()\n        })\n\n    # Apply the function to each question group\n    result = df.groupby('Question').apply(top_n_concat).reset_index()\n\n    # Sort the results by Question\n    result = result.sort_values('Question')\n\n    return result\n\n\n\ntop2_bm25 = get_top_n_chunks(bm25,n=2)\ntop3_cs = get_top_n_chunks(cs, n=3)\n\n\ntop5_combined = pd.concat([top2_bm25, top3_cs]).groupby('Question').agg({\n        'chunk': ' '.join,  # Concatenate all chunks\n        'normalized_score': 'mean'  # Take the mean of the normalized scores\n    }).reset_index()\n\n\ntop5_combined.to_csv('top5_combined.csv', index=False)\n\nUsing the top-3 Cosine Similarity + top-2 BM25 chunks allowed me to answer 25 out of 33 questions, or 76%. These were the same 25 questions I could answer using the top-5 chunks (by weighted average)."
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-5-cosine-similarity-and-top-3-keyword-search-results",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#retrieving-top-5-cosine-similarity-and-top-3-keyword-search-results",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Retrieving top-5 Cosine Similarity and top-3 Keyword Search Results",
    "text": "Retrieving top-5 Cosine Similarity and top-3 Keyword Search Results\nI’m hesitant to use more than 5 chunks of context (for the eventual LLM in this pipeline) because while there’s a higher chance relevant data is included, it also includes a lot of irrelevant data in the context, and I would worry that the LLM (especially a relatively small one like phi-3) may get distracted by this irrelevant context. That being said, how the LLM behaves to different contexts is something I’ll experiment with in the future to determine whether or not something is “too long” of a context for the model.\nThe last hybrid approach I’ll pursue is combining the two best-performing individual approaches:\n\nBM25: Use the top-3 large (3-paragraph) chunks\nCosine Similarity: Use the top-5 small (1-paragraph) chunks\n\nI’ll rewrite the database table to contain the 3-paragraph-long chunks to use for keyword search:\n\nload_data(larger_chunks)\n\nTrue\n\n\n\n\nShow the for-loop + query\nresults = []\nconn = sqlite3.connect('fastbook.db')\ncur = conn.cursor()\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text\n    WHERE fastbook_text MATCH '{words}'\n    ORDER BY rank\n    LIMIT 3\n\n    \"\"\"\n    res = cur.execute(q).fetchall()\n    results.append(res)\n  else:\n    # if keywords == \"No Answer\"\n    res = \"No answer\"\n    results.append(res)\n\n\n\nbm25 = create_dataframe(results, 'BM25')\nbm25['normalized_score'] = -1 * bm25['score']\ntop3_bm25 = get_top_n_chunks(bm25,n=3)\ntop3_bm25.shape\n\n(33, 3)\n\n\n\ntop5_cs = get_top_n_chunks(cs, n=5)\ntop5_cs['Question'] = top5_cs['Question'].astype(int)\ntop5_cs.shape\n\n(33, 3)\n\n\n\ntop8_combined = pd.concat([top3_bm25, top5_cs]).groupby('Question').agg({\n        'chunk': ' '.join,  # Concatenate all chunks\n        'normalized_score': 'mean'  # Take the mean of the normalized scores\n    }).reset_index()\ntop8_combined.shape\n\n(33, 3)\n\n\n\ntop8_combined.to_csv('top8_combined.csv', index=False)\n\nThis hybrid approach resulted in the best performance thus far (individual or hybrid)! With the top-8 chunks retrieved for each question (top-5 small chunks for Cosine Similarity, top-3 large chunks for keyword search) I was able to answer 28 out of 33 questions, or 85%. Factoring in that 3 of the questions are exercises to be done by the reader (and aren’t answerable using the Chapter 1 content), the true percentage is 93%."
  },
  {
    "objectID": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#final-thoughts",
    "href": "posts/2024-08-11-fastbook-ch1-hybrid/index.html#final-thoughts",
    "title": "Using Hybrid Search to Answer fastai the Chapter 1 Questionnaire",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’ll summarize my results from this notebook in the table below. Cosine Similarity is represented as “CS” and full text search as “BM25”:\n\n\n\nApproach\nChunk Size (Paragraphs)\nQuestions Answered\n\n\n\n\nTop-5 CS + Top-3 BM25\n1 (CS), 3 (BM25)\n85%\n\n\nTop-3 CS + Top-2 BM25\n1 (CS), 3 (BM25)\n76%\n\n\nTop-5 Weighted Average\n1\n76%\n\n\nTop-3 Weighted Average\n1\n64%\n\n\nTop-3 CS of Top-10 BM25 Results\n1\n58%\n\n\nTop-3 CS of Top-10 BM25 Results\n3\n58%\n\n\nTop-1 CS of Top-10 BM25 Results\n1\n52%\n\n\nTop-1 CS of Top-12 BM25 Results\n3\n48%\n\n\nTop-1 Weighted Average\n1\n48%\n\n\n\nFor the Chapter 1 Questionnaire, the most effective strategy is to combine the top-5 1-paragraph chunks (from Cosine Similarity) with the top-3 3-paragraph chunks (from BM25). Each method alone answered about 72% (BM25) and 76% (Cosine Similarity) of the questions, so combining them increases coverage, as each approach catches questions the other might miss.\nWith this baseline established for Chapter 1, I’ll now move on to the rest of the chapters covered in Part 1 of the fastai course.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html",
    "href": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html",
    "title": "Calculating the Ratio of 2D FFT Magnitude and Phase of a Text Image",
    "section": "",
    "text": "In this notebook I’ll walk through an algorithm suggested by Claude to distinguish one typeface (like display) from another (like serif) in which we calculate two ratios:\n\nthe ratio of the mean of 2D FFT magnitude to the count of non-zero binarized pixels (“FFT Magnitude Ratio”)\nthe ratio of the sum of the absolute value of 2D FFT phase to the sum of binarized pixels (“FFT Phase Ratio”)\n\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\nI’m not very familiar with Fourier transforms (especially for images), so I did a bit of review by watching the following two YouTube videos:\n\nIntroduction to Image Processing with 2D Fourier Transform\n2D Fourier Transform Explained with Examples\n\nA couple of key takeaways from those videos that apply to this algorithm:\n\nA 2D Fourier transform is centered at the center of the plot (i.e. 0-frequency is at the center of the image, so, for a 512x512px image the 0-frequency would be at (256, 256))\nThe frequency increases as you radiate away from the center of the plot to the edges.\nThe phase, and not just the amplitude, is very important for capturing information about a 2D image.\n\nI’ll try to illustrate these key points as I walk through the algorithm.\n\n\nShow imports\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom google.colab.patches import cv2_imshow"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#load-and-binarize-the-image",
    "href": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#load-and-binarize-the-image",
    "title": "Calculating the Ratio of 2D FFT Magnitude and Phase of a Text Image",
    "section": "Load and Binarize the Image",
    "text": "Load and Binarize the Image\nWe start, as we usually do, by loading and binarizing the image.\n\npath = 'serif-76px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#calculating-the-2d-fast-fourier-transform",
    "href": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#calculating-the-2d-fast-fourier-transform",
    "title": "Calculating the Ratio of 2D FFT Magnitude and Phase of a Text Image",
    "section": "Calculating the 2D Fast Fourier Transform",
    "text": "Calculating the 2D Fast Fourier Transform\nUsing NumPy, I calculate the 2D FFT:\n\nf = np.fft.fft2(binary)\n\n\nf.shape\n\n(512, 512)\n\n\nI’ll use a Claude-generated helper function to plot this in 3D:\n\n\nShow plot_fft2_3d function\ndef plot_fft2_3d(image_path):\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Compute the 2D Fourier Transform\n    f = np.fft.fft2(img)\n\n    # Shift the zero-frequency component to the center of the spectrum\n    f_shift = np.fft.fftshift(f)\n\n    # Compute the magnitude spectrum\n    magnitude_spectrum = np.log(np.abs(f_shift) + 1)  # Adding 1 to avoid log(0)\n\n    # Compute the phase spectrum\n    phase_spectrum = np.angle(f_shift)\n\n    # Calculate and print the min and max values of the phase spectrum\n    phase_min = np.min(phase_spectrum)\n    phase_max = np.max(phase_spectrum)\n    print(f\"Minimum phase value: {phase_min:.4f}\")\n    print(f\"Maximum phase value: {phase_max:.4f}\")\n\n    # Create a grid for the 3D plots\n    rows, cols = magnitude_spectrum.shape\n    x = np.linspace(0, cols, cols)\n    y = np.linspace(0, rows, rows)\n    x, y = np.meshgrid(x, y)\n\n    # Create the 3D plots\n    fig = plt.figure(figsize=(10,8))\n\n    # Magnitude spectrum subplot\n    ax1 = fig.add_subplot(121, projection='3d')\n    surf1 = ax1.plot_surface(x, y, magnitude_spectrum, cmap='viridis')\n    ax1.set_xlabel('Frequency (x)')\n    ax1.set_ylabel('Frequency (y)')\n    ax1.set_zlabel('Magnitude (log scale)')\n    ax1.set_title('Magnitude Spectrum')\n    fig.colorbar(surf1, shrink=0.5, aspect=20, ax=ax1, pad=0.1)\n\n    # Phase spectrum subplot\n    ax2 = fig.add_subplot(122, projection='3d')\n    surf2 = ax2.plot_surface(x, y, phase_spectrum, cmap='hsv')\n    ax2.set_xlabel('Frequency (x)')\n    ax2.set_ylabel('Frequency (y)')\n    ax2.set_zlabel('Phase (radians)')\n    ax2.set_title('Phase Spectrum')\n    fig.colorbar(surf2, shrink=0.5, aspect=20, ax=ax2, pad=0.1)\n\n    plt.suptitle('3D Plots of 2D Fourier Transform', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n\nA few things to point out (other than it looks cool, which it does):\n\nThe very lowest frequencies have the highest magnitudes.\nThe magnitude plot is symmetrical in both axes (which is usual for images).\nThe phase values oscillate between negative and positive \\(\\pi\\) radians.\n\n\nplot_fft2_3d('serif-76px.png')\n\nMinimum phase value: -3.1415\nMaximum phase value: 3.1416\n\n\n\n\n\n\n\n\n\nNext, to illustrate the difference between high and low frequencies and the magnitude and phase in the frequency domain, I’ll use another Claude-generated helper function to plot the inverse 2D FFT of the following four things:\n\nhigh frequencies, magnitude only\nlow frequencies, magnitude only\nhigh frequencies, phase only\nlow frequencies, phase only\n\n\n\nShow visualize_inverse_fft function\ndef visualize_inverse_fft(image_path):\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Apply threshold\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Compute the 2D Fourier Transform\n    f = np.fft.fft2(binary)\n\n    # Shift the zero-frequency component to the center of the spectrum\n    f_shift = np.fft.fftshift(f)\n\n    # Get the magnitude and phase spectrums\n    magnitude_spectrum = np.abs(f_shift)\n    phase_spectrum = np.angle(f_shift)\n\n    # Create a high-pass filter\n    rows, cols = binary.shape\n    crow, ccol = rows // 2, cols // 2\n    mask = np.ones((rows, cols), np.uint8)\n    mask[crow-30:crow+30, ccol-30:ccol+30] = 0\n\n    # Apply high-pass and low-pass filters\n    f_high = f_shift * mask\n    f_low = f_shift * (1 - mask)\n\n    # Create subplots\n    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n    fig.suptitle('Inverse FFT Visualizations', fontsize=16)\n\n    # Function to perform inverse FFT and display result\n    def plot_ifft(ax, f, title):\n        f_ishift = np.fft.ifftshift(f)\n        img_back = np.fft.ifft2(f_ishift)\n        img_back = np.abs(img_back)\n        ax.imshow(img_back, cmap='gray')\n        ax.set_title(title)\n        ax.axis('off')\n\n    # High frequencies, magnitude only\n    plot_ifft(axs[0, 0], np.abs(f_high), 'High Freq, Magnitude Only')\n\n    # Low frequencies, magnitude only\n    plot_ifft(axs[0, 1], np.abs(f_low), 'Low Freq, Magnitude Only')\n\n    # High frequencies, phase only\n    plot_ifft(axs[1, 0], np.exp(1j * np.angle(f_high)), 'High Freq, Phase Only')\n\n    # Low frequencies, phase only\n    plot_ifft(axs[1, 1], np.exp(1j * np.angle(f_low)), 'Low Freq, Phase Only')\n\n    plt.tight_layout()\n    plt.show()\n\n\nAs you can see below (bottom left of both groups of plots—serif-76px.png and display-76px.png) most of the information of the letter contours is captured by the high frequency phase of the 2D FFT.\n\nvisualize_inverse_fft('serif-76px.png')\n\n\n\n\n\n\n\n\n\nvisualize_inverse_fft('display-76px.png')"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#two-ratios-fft-magnitude-and-fft-phase",
    "href": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#two-ratios-fft-magnitude-and-fft-phase",
    "title": "Calculating the Ratio of 2D FFT Magnitude and Phase of a Text Image",
    "section": "Two Ratios: FFT Magnitude and FFT Phase",
    "text": "Two Ratios: FFT Magnitude and FFT Phase\nI’ll calculate two ratios (in a way such that the values are somewhat constrained to 0-1):\n\nthe ratio of the mean of FFT magnitude to the count of non-zero binarized pixels (“FFT Magnitude Ratio”)\nthe ratio of the sum of the absolute value of FFT phase to the sum of binarized pixels (“FFT Phase Ratio”)\n\nWhew! Those were a mouthful. In short, I’m trying to capture the difference between FFT magnitude (and the difference between FFT phase) between images of text and do it in a way that the resulting values are somewhat between 0 and 1.\n\nmagnitude_mean = np.mean(np.abs(f))\nphase_sum = np.sum(np.abs(np.angle(f)))\n\n\nmagnitude_mean / np.count_nonzero(binary), phase_sum / np.sum(binary)\n\n(0.6622262512288943, 0.0717868879051193)"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#calculating-fft-magnitude-ratio-and-fft-phase-ratio-for-different-images",
    "href": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#calculating-fft-magnitude-ratio-and-fft-phase-ratio-for-different-images",
    "title": "Calculating the Ratio of 2D FFT Magnitude and Phase of a Text Image",
    "section": "Calculating FFT Magnitude Ratio and FFT Phase Ratio for Different Images",
    "text": "Calculating FFT Magnitude Ratio and FFT Phase Ratio for Different Images\nI’ll now wrap the above functionality into two functions and calculate them for a wide variety of images (of two typefaces, display and serif and 8 different font sizes).\n\ndef fft_magnitude_ratio(path):\n  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n  _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n  f = np.fft.fft2(binary)\n\n  magnitude_mean = np.mean(np.abs(f))\n\n  total_pixels = np.count_nonzero(binary)\n\n  ratio = magnitude_mean / total_pixels\n\n  return ratio\n\n\ndef fft_phase_ratio(path):\n  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n  _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n  f = np.fft.fft2(binary)\n\n  phase_sum = np.sum(np.abs(np.angle(f)))\n  total_pixels = np.sum(binary)\n\n  return phase_sum / total_pixels\n\nBoth the average FFT magnitude ratio (the mean of the FFT magnitude divided by the number of non-zero binarized pixels) and the average FFT phase ratio (the sum of the FFT phase divided by the sum of the binarized pixels) are higher for serif images than for the sans serif display images.\n\nszs = [8, 18, 24, 36, 76, 240, 330, 420]\nts = ['display', 'serif']\nres = []\n\nfor t in ts:\n    for sz in szs:\n        image_path = f\"{t}-{sz}px.png\"\n        mr = fft_magnitude_ratio(image_path)\n        pr = fft_phase_ratio(image_path)\n        res.append([t, sz, mr, pr])\n\nres = pd.DataFrame(res, columns=['typeface', 'font-size', 'fft-magnitude-ratio', 'fft-phase-ratio'])\nres.groupby('typeface')[['fft-magnitude-ratio', 'fft-phase-ratio']].agg(['mean', 'median'])\n\n\n\n  \n    \n\n\n  \n    \n      \n      fft-magnitude-ratio\n      fft-phase-ratio\n    \n    \n      \n      mean\n      median\n      mean\n      median\n    \n    \n      typeface\n      \n      \n      \n      \n    \n  \n  \n    \n      display\n      0.621523\n      0.464047\n      0.077332\n      0.042748\n    \n    \n      serif\n      0.801844\n      0.691680\n      0.095694\n      0.058342\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nFor all font sizes except for 240px, the trend is consistent: both the magnitude and the phase ratio are higher for serif than display:\n\nres.sort_values(by='font-size')\n\n\n\n  \n    \n\n\n  \n    \n      \n      typeface\n      font-size\n      fft-magnitude-ratio\n      fft-phase-ratio\n    \n  \n  \n    \n      0\n      display\n      8\n      2.028167\n      0.309951\n    \n    \n      8\n      serif\n      8\n      2.165248\n      0.341165\n    \n    \n      1\n      display\n      18\n      0.791710\n      0.064684\n    \n    \n      9\n      serif\n      18\n      1.143785\n      0.098539\n    \n    \n      2\n      display\n      24\n      0.587993\n      0.039971\n    \n    \n      10\n      serif\n      24\n      0.809721\n      0.056169\n    \n    \n      3\n      display\n      36\n      0.514101\n      0.040205\n    \n    \n      11\n      serif\n      36\n      0.721134\n      0.054290\n    \n    \n      4\n      display\n      76\n      0.413993\n      0.046105\n    \n    \n      12\n      serif\n      76\n      0.662226\n      0.071787\n    \n    \n      5\n      display\n      240\n      0.272433\n      0.045290\n    \n    \n      13\n      serif\n      240\n      0.318485\n      0.040843\n    \n    \n      6\n      display\n      330\n      0.196193\n      0.038930\n    \n    \n      14\n      serif\n      330\n      0.288490\n      0.042242\n    \n    \n      7\n      display\n      420\n      0.167595\n      0.033520\n    \n    \n      15\n      serif\n      420\n      0.305663\n      0.060515"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#final-thoughts",
    "href": "posts/2024-09-09-typefaceclassifier-fourier-ratios/index.html#final-thoughts",
    "title": "Calculating the Ratio of 2D FFT Magnitude and Phase of a Text Image",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI was hesitant to implement this algorithm because my general rule when using AI-generated code is: don’t use it if I don’t understand it or can’t explain it. I initially started out with a much more involved algorithm generated by Claude, where it was calculating the ratio of the sum of low-frequency magnitudes to the sum of high-frequency magnitudes (after calculating 2D FFT of a binarized image). Instead of pursuing that algorithm, with the help of Claude I stripped it down to a bare minimum that I could understand and explain after putting in about an hour’s worth of reading and watching different 2D FFT explanations. I still have a long ways to go before I can consider myself proficient with Fourier analysis, but this was a good start.\nThe two ratios that I calculated in this are relatively consistent in distinguishing serif typeface (high ratios) from display typeface (low ratios). This makes them a good candidate as a non-ML baseline.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "",
    "text": "This notebook is a part of series of blog posts for a project I’m calling fastbookRAG where I’m trying to answer questions from the fastbook end-of-chapter Questionnaires using the following pipeline:\n\n\n\nfastbookRAG diagram\n\n\nThis notebook establishes a baseline using BM25 for keyword-based retrieval on chunks of the fastbook chapters covered in Part 1 of the fastai course (1, 2, 4, 8, 9, 10, and 13). I generate keywords (for full text search) for each question using the Claude-3.5-Sonnet API (using the Answer.AI library claudette).\nThe evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nThe goal is to retrieve the context necessary to answer all questions. Currently, I manually assess answers. Eventually I’ll integrate an LLM into this pipeline to generate (and evaluate) answers based on the retrieved context."
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#summary-of-results",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#summary-of-results",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Summary of Results",
    "text": "Summary of Results\nHere are the results from my experiments in this notebook—in general, the best performing full text search method (76.7% Answer Rate overall) was retrieving the top-5 (BM25 score) 3-paragraph chunks based on the given set of keywords using FTS5 in sqlite:\n\n\n\n\n\n\n\n\n\n\n\n\nChapter\nBM25_A (Top-1 1p)\nBM25_B (Top-3 1p)\nBM25_C (Top-5 1p)\nBM25_D (Top-1 3p)\nBM25_E (Top-3 3p)\nBM25_F (Top-5 3p)\n\n\n\n\n1\n40% (12/30)\n56.7% (17/30)\n60% (18/30)\n63.3% (19/30)\n83.3% (25/30)\n90% (27/30)\n\n\n2\n38.5% (10/26)\n65.4% (17/26)\n69.2% (18/26)\n46.2% (12.26)\n80.8% (21/26)\n80.8% (21/26)\n\n\n4\n25% (8/32)\n68.8% (22/32)\n71.9% (23/32)\n31.3% (10/32)\n71.9% (23/32)\n75% (24/32)\n\n\n8\n13.8% (4/29)\n34.5% (10/29)\n44.8% (13/29)\n31% (9/29)\n55.2% (16/29)\n65.5% (19/29)\n\n\n9\n13.8% (4/29)\n48.3% (14/29)\n58.6% (17/29)\n34.5% (10/29)\n72.4% (21/29)\n79.3% (23/29)\n\n\n10\n47.6% (12/21)\n42.9% (9/21)\n61.9% (13/21)\n38% (8/21)\n57.1% (12/21)\n61.9% (13/21)\n\n\n13\n37.1% (13/35)\n54.3% (19/35)\n60% (21/35)\n42.9% (15/35)\n68.6% (24/35)\n80% (28/35)\n\n\nAll\n30.2% (61/202)\n53.5% (108/202)\n60.9% (123/202)\n41.1% (83/202)\n70.3% (142/202)\n76.7% (155/202)"
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#experimental-setup",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#experimental-setup",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Experimental Setup",
    "text": "Experimental Setup\n\nData Sources\n\nThe freely available Jupyter Notebook-written fastbook.\n\n\n\nData Preprocessing\n\nChunking strategy: Single or multiple paragraphs with corresponding headers.\nRationale: Balances granular content with high-level context.\nGoal: Maintain lean, informative chunks for efficient retrieval.\n\n\n\nDatabase\nI am using sqlite for chunk storage as it’s built-in to the python standard library and contains a built-in full text search (FTS5). My database has a single table fastbook with a column text that has FTS5 enabled.\nA typical query looks like:\nSELECT * FROM fastbook\nWHERE fastbook MATCH 'keyword1 OR keyword2'\nORDER BY rank\nLIMIT 3;"
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#methodology",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#methodology",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Methodology",
    "text": "Methodology\n\nWhy BM25?\nKey inspirations for why I chose to establish a BM25 baseline:\n\n\n\nEugene Yan Tweet\n\n\n\n\n\nSimon Willison Tweet\n\n\n\n\n\nShreya Shankar Tweet\n\n\n\n\nHumans love to use keywords. We have very strong tendencies to notice and use certain acronyms, domain-specific words, etc. To capture all this signal, you should ensure your pipeline uses Keyword search. An ongoing joke is that information retrieval has progressed slowly because BM25 is too strong a baseline. (Beyond the Basics of RAG by Benjamin Clavié).\n\n\n\nEvaluation Set\nMy evaluation set consists of:\n\n202 Questionnaire questions.\n“Gold standard” solutions to the Questionnaire published by fast.ai Leader Tanishq Abraham who says:\n\n\nmy responses are based on what is supported by the chapter text\n\n(Which is perfect for my retrieval task.)\n\n\nEvaluation Metrics\nMetrics: Score and Answer Rate\nThe evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nWhile this is a straightforward pair of metrics, they do involve some judgment. After reading the retrieved context, I decide if it’s enough to answer the question. A capable LLM should be able to make the same kind of judgment about whether the context is helpful or not."
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#results",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#results",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Results",
    "text": "Results\nHere are the names and descriptions of each full text search approach explored in this notebook. Top-n means the chunk(s) with the n-highest BM25 score(s).\n\n\n\nName\nDescription\n\n\n\n\nBM25_A\nTop-1 1-Paragraph Chunks\n\n\nBM25_B\nTop-3 1-Paragraph Chunks\n\n\nBM25_C\nTop-5 1-Paragraph Chunks\n\n\nBM25_D\nTop-1 3-Paragraph Chunks\n\n\nBM25_E\nTop-3 3-Paragraph Chunks\n\n\nBM25_F\nTop-5 3-Paragraph Chunks\n\n\n\n\nBest Approach per Chapter\nThe following table shows name, description and Answer Rate for the best BM25 approach for each Chapter. BM25_F (Top-5 3-paragraph Chunks) was the best performing approach for each chapter and overall. For Chapter 2, BM25_E (Top-3 3-paragraph Chunks) had the same Answer Rate as BM25_F. For Chapter 10, BM25_C (Top-5 1-paragraph Chunks) had the same Answer Rate as BM25_F.\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nBM25_F\nTop-5 3-paragraph Chunks\n90%\n\n\n2\nBM25_E/BM25_F\nTop-3/Top-5 3-paragraph Chunks\n80.8%\n\n\n4\nBM25_F\nTop-5 3-paragraph Chunks\n75%\n\n\n8\nBM25_F\nTop-5 3-paragraph Chunks\n65.5%\n\n\n9\nBM25_F\nTop-5 3-paragraph Chunks\n79.3%\n\n\n10\nBM25_C/BM25_F\nTop-5 1-paragraph/3-paragraph Chunks\n61.9%\n\n\n13\nBM25_F\nTop-5 3-paragraph Chunks\n80%\n\n\nAll\nBM25_F\nTop-5 3-paragraph Chunks\n76.7%\n\n\n\n\n\nAll Approaches for All Chapters\nThe following table shows the Answer Rate for all BM25 approaches for each chapter (where in the header, 1p = 1-paragraph chunks and 3p = 3-paragraph chunks).\n\n\n\n\n\n\n\n\n\n\n\n\nChapter\nBM25_A (Top-1 1p)\nBM25_B (Top-3 1p)\nBM25_C (Top-5 1p)\nBM25_D (Top-1 3p)\nBM25_E (Top-3 3p)\nBM25_F (Top-5 3p)\n\n\n\n\n1\n40% (12/30)\n56.7% (17/30)\n60% (18/30)\n63.3% (19/30)\n83.3% (25/30)\n90% (27/30)\n\n\n2\n38.5% (10/26)\n65.4% (17/26)\n69.2% (18/26)\n46.2% (12.26)\n80.8% (21/26)\n80.8% (21/26)\n\n\n4\n25% (8/32)\n68.8% (22/32)\n71.9% (23/32)\n31.3% (10/32)\n71.9% (23/32)\n75% (24/32)\n\n\n8\n13.8% (4/29)\n34.5% (10/29)\n44.8% (13/29)\n31% (9/29)\n55.2% (16/29)\n65.5% (19/29)\n\n\n9\n13.8% (4/29)\n48.3% (14/29)\n58.6% (17/29)\n34.5% (10/29)\n72.4% (21/29)\n79.3% (23/29)\n\n\n10\n47.6% (12/21)\n42.9% (9/21)\n61.9% (13/21)\n38% (8/21)\n57.1% (12/21)\n61.9% (13/21)\n\n\n13\n37.1% (13/35)\n54.3% (19/35)\n60% (21/35)\n42.9% (15/35)\n68.6% (24/35)\n80% (28/35)\n\n\nAll\n30.2% (61/202)\n53.5% (108/202)\n60.9% (123/202)\n41.1% (83/202)\n70.3% (142/202)\n76.7% (155/202)\n\n\n\nA few observations when looking at the Answer Rate for each approach for each chapter:\n\nIncreasing the number of chunks retrieved generally improves the quality of information retrieved:\n\n1-paragraph chunks\n\nFor all but Chapter 10, the Answer Rate improves: BM25_C > BM25_B > BM25_A.\n\n3-paragraph chunks\n\nFor all chapters the Answer Rate improves or stays the same (Chapter 2): BM25_F >= BM25_E > BM25_D\n\n\nIncreasing the chunk size generally improves the quality of information retrieved: For 6 out of 7 chapters (all but Chapter 10), the 3-paragraph chunk Answer Rate (BM25_D, BM25_E, BM25_F) is greater than the corresponding 1-paragraph chunk Answer Rate (BM25A, BM25_B, BM25_C).\nNot all chapters behave the same: The Chapter 1 Questionnaire has a higher Answer Rate after increasing chunk size to 3-paragraphs (BM25_C: 60%, BM25_D: 63.3%). On the other extreme, for Chapter 10, increasing the chunk size to 3-paragraphs (BM25_D, 38%) performs worse than the worst-performing 1-paragraph approach (BM25_B, 42.9%). Chapter 10 is also an anomaly within 1-paragraph results: the Answer Rate for Top-3 (BM25_B, 42.9%) is less than Top-1 (BM25_A, 47.6%) while for every other chapter, BM25_A > BM25_B. For 3-paragraph chunks, Chapter 2’s Answer Rate doesn’t improve like the other chapters from Top-3 (BM25_E: 80.8%) to Top-5 (BM25_F: 80.8%).\nChanging the data in the database changes the corpus-wide statistics: In sqlite, BM25 is calculated using corpus-wide statistics. If the data changes, the calculation changes. I noticed a significant improvement in the quality of context retrieved for 3-paragraph chunks when compared to 1-paragraph chunks (sometimes even the first chunk was different for the same question).\n\nPerformance varied across chapters due to differences in content and question types:\n\nChapter 1 (best performer, 90% with BM25_F) is introductory, defining many terms well-suited for keyword search.\nChapter 10 (worst performer, 61.9% with BM25_F)\n\nHad the fewest questions (21), which are less granular than other chapters. Each missed question was a ~5% Answer Rate hit.\nRetrieved contexts often contained relevant keywords but lacked direct answers.\n\nChapter 8 (second-worst) faced challenges with:\n\nAmbiguous questions (e.g., “How does it solve it?”).\nTechnical terms (e.g., function names) rare in explanatory text but common in code chunks.\nOveruse of certain terms (e.g., “latent factor”) diluting relevance.\n\n\n\n\nQuestion-Level Analysis\nLooking at the question-level data offers some additional insights.\n\nimport pandas as pd\nurl = 'https://gist.githubusercontent.com/vishalbakshi/4379c92665695b8bd9ab83a1f3ab6b55/raw/97a22f0736b5e179efb8c9d70e5ec80a5b8f4817/fastbookRAG_bm25_all.csv'\ndf = pd.read_csv(url)\nscore_columns = df.filter(regex='_score$').columns\ndf['total_score'] = df[score_columns].sum(axis=1)\n\n\nDistribution of Scores\nHere are my takeaways from the distribution of scores:\n\nBimodal Distribution\n\nApproximately 50 questions (about 25% of the total) were successfully answered by all six full text search methods (a total score of 6).\nOn the other hand, around 40 questions (about 20%) couldn’t be answered by any method, resulting in a total score of 0.\n\nUniform Mid-Range Performance\n\nQuestions answered by 2, 3, 4, or 5 methods each accounted for 20-30 instances, showing a relatively even distribution in this middle range.\n\nLeast Common Outcome\n\nOnly 10 questions were answered by just one method, making this the least frequent result.\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\n\ndf['total_score'].hist(bins=range(0, 8), align='left', rwidth=0.8);\n\nplt.title('Distribution of Total Scores')\nplt.xlabel('Total Score')\nplt.ylabel('Number of Questions');\n\n\n\n\n\n\n\n\n\n\nAverage Score Per Question\nOn average, each question was answered by 3 to 4 full text search methods.\n\ndf['total_score'].describe()\n\n\n\n\n\n  \n    \n      \n      total_score\n    \n  \n  \n    \n      count\n      202.000000\n    \n    \n      mean\n      3.326733\n    \n    \n      std\n      2.190476\n    \n    \n      min\n      0.000000\n    \n    \n      25%\n      2.000000\n    \n    \n      50%\n      4.000000\n    \n    \n      75%\n      5.000000\n    \n    \n      max\n      6.000000\n    \n  \n\ndtype: float64\n\n\n\n\nUnanswered Questions\nThere were 39 questions for which none of the full text search approaches retrieved the context needed to answer them:\n\nno_answer = df.query(\"total_score == 0\")[['chapter', 'question_number', 'question_text', 'answer', 'keywords']].drop_duplicates()\nno_answer.shape\n\n(39, 5)\n\n\nFor example, the following question never was fully answered by any of the contexts retrieved by the full text search methods.\n\nprint(no_answer.iloc[0]['question_text'])\nprint(no_answer.iloc[0]['answer'])\n\n\"\"What were the two theoretical misunderstandings that held back the field of neural networks?\"\"\n\"\"In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, \"Perceptrons\", that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\nIn the 1980's, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential.\"\"\n\n\nThere were two paragraphs from Chapter 1 that answered this question (emphasis mine). None of the full text search methods retrieved both of these paragraphs (some included 1 of the 2 needed paragraphs):\n\nAn MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called Perceptrons (MIT Press), about Rosenblatt’s invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.\n\n\nIn the 1980’s most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their “pattern of connectivity among units,” to use the framework above). And indeed, neural networks were widely used during the ’80s and ’90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful.\n\nSome questions were not so straightforward to answer. For example, the following question:\n\nprint(no_answer.iloc[-1]['question_text'])\nprint(no_answer.iloc[-1]['answer'])\n\n\"\"What are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\"\"\n\"\"The mean and standard deviation of the activations, as well as the percentage of activation near zero. The x-axis represents the progress of training (batch number).\"\"\n\n\nThe first part of this question is answerable by the following context from the fastbook (emphasis mine), which some of the full text search methods successfully retrieved:\n\nActivationStats includes some handy utilities for plotting the activations during training. plot_layer_stats(idx) plots the mean and standard deviation of the activations of layer number idx, along with the percentage of activations near zero. Here’s the first layer’s plot:\n\nThe second part of the question (What does the x-axis represent?) is not explicitly answered in the fastbook. I will consider removing this question from my evals.\nFor some questions the full text search methods simply did not retrieve the necessary paragraph. For example:\n\nprint(no_answer.iloc[10]['question_text'])\nprint(no_answer.iloc[10]['answer'])\n\n\"\"What is the function to calculate new weights using a learning rate?\"\"\n\"\"The optimizer step function\"\"\n\n\nThis question is answered by the following paragraphs which none of the methods retrieved for this question (emphasis mine):\n\nDeciding how to change our parameters based on the values of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything. Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we’ll show you a better approach later in this book, called the learning rate finder). Once you’ve picked a learning rate, you can adjust your parameters using this simple function:\nw -= gradient(w) * lr\nThis is known as stepping your parameters, using an optimizer step.\n\nFor some of the questions, I’m not quite sure why the associated keywords didn’t allow the correct chunk retrieval. I would assume the reason is that other chunks had a higher frequency of those keywords than these chunks. Something I’ll look into when I do a deeper dive into error analysis in my next notebook.\n\nprint(no_answer.iloc[5]['question_text'])\nprint(no_answer.iloc[5]['answer'])\nprint(no_answer.iloc[5]['keywords'])\n\n\"\"What are the three steps in the deployment process?\"\"\n\"\"Manual process - the model is run in parallel and not directly driving any actions, with humans still checking the model outputs.\nLimited scope deployment - The model's scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised.\nGradual expansion - The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better).\"\"\n\"deployment, process, steps\"\n\n\nHere’s the correct chunks (emphasis mine) that none of the methods retrieved:\n\nWhere possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense. For instance, with our bear classifier a park ranger could have a screen displaying video feeds from all the cameras, with any possible bear sightings simply highlighted in red. The park ranger would still be expected to be just as alert as before the model was deployed; the model is simply helping to check for problems at this point.\nThe second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time-constrained trial of the model-driven approach. Rather than rolling our bear classifier out in every national park throughout the country, we could pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out.\nThen, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process. For instance, if the number of bear alerts doubles or halves after rollout of the new system in some location, we should be very concerned. Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and ensure that your regular reporting includes that information.\n\n\n\nQuestions with 100% Answer Rate\nThere were 49 questions that were answered by all 6 full text search methods.\n\nall_answer = df.query(\"total_score == 6\")[['chapter', 'question_number', 'question_text', 'keywords']].drop_duplicates()\nall_answer.shape\n\n(49, 4)\n\n\nMany of these questions involved a keyword or its definition—a perfect use case for full text search:\n\nall_answer.iloc[1]['question_text']\n\n'\"\"What is a GPU?\"\"'\n\n\n\nall_answer.iloc[2]['question_text']\n\n'\"\"What term do we normally use in deep learning for what Samuel called \"\"weights\"\"?\"\"'\n\n\n\nall_answer.iloc[15]['question_text']\n\n'\"\"What are IPython widgets?\"\"'\n\n\n\nall_answer.iloc[3]['question_text']\n\n'\"\"What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\"\"'\n\n\n\nall_answer.iloc[9]['question_text']\n\n'\"\"What is y_range used for? When do we need it?\"\"'\n\n\nSome of these questions were more open-ended, but involved certain keywords that were used verbatim in the fastbook chunk that answered the question, such as “AI”, “organizations” and “failures”:\n\nall_answer.iloc[10]['question_text'], all_answer.iloc[10]['keywords']\n\n('\"\"What\\'s the best way to avoid failures when using AI in an organization?\"\"',\n '\"AI, failures, avoid, organization, organizations\"')\n\n\nHere is the context that answered this question (emphasis mine):\n\n\n\n\nTo put it bluntly, if you’re a senior decision maker in your organization (or you’re advising senior decision makers), the most important takeaway is this: if you ensure that you really understand what test and validation sets are and why they’re important, then you’ll avoid the single biggest source of failures we’ve seen when organizations decide to use AI. For instance, if you’re considering bringing in an external vendor or service, make sure that you hold out some test data that the vendor never gets to see. Then you check their model on your test data, using a metric that you choose based on what actually matters to you in practice, and you decide what level of performance is adequate. (It’s also a good idea for you to try out some simple baseline yourself, so you know what a really simple model can achieve. Often it’ll turn out that your simple model performs just as well as one produced by an external “expert”!)\n\nThe following question has the terms “batch”, “normalization”, “layers” and “generalize”:\n\nall_answer.iloc[-1]['question_text'], all_answer.iloc[-1]['keywords']\n\n('\"\"Why do models with batch normalization layers generalize better?\"\"',\n '\"models, batch, normalization, layers, generalization, generalize\"')\n\n\nThe corresponding context (emphasis mine) contains those keywords verbatim:\n\nAn interesting observation about models containing batch normalization layers is that they tend to generalize better than models that don’t contain them. Although we haven’t as yet seen a rigorous analysis of what’s going on here, most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process.\n\nThe following question has the keywords “statistics”, “normalize”, “batch”, “training” and “validation” that are all contained in the corresponding fastbook chunk, some multiple times:\n\nall_answer.iloc[-2]['question_text'], all_answer.iloc[-2]['keywords']\n\n('\"\"What statistics are used to normalize in batch normalization during training? How about during validation?\"\"',\n '\"statistics, normalize, batch, normalization, training, validation\"')\n\n\n\nThat’s why our activations can have any mean or variance, independent from the mean and standard deviation of the results of the previous layer. Those statistics are learned separately, making training easier on our model. The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training.\n\n\n\n\nResults CSV\nThe retrieved contexts and my manually assigned scores for each question and baseline are available in this public gist."
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#limitations",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#limitations",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Limitations",
    "text": "Limitations\nThere are a number of limitations that I want to highlight in this work:\n\nLimited methods: There are inumerable combinations of chunk strategies and top-n retrieval choices. I chose the six (1-paragraph/3-paragraph and Top-1/Top-3/Top-5) that seemed easy to implement, reasonable to accomplish in my desired timeline and reasonably provided me with a diverse set of results.\nLimited scope: I’m only considering the 202 questions in the end-of-chapter Questionnaires whose answer was explicitly in the fastbook text. There are endless questions about the topics covered in the fastbook. I only focused on the 7 chapters covered in Part 1 of the fastai course (as I am still in progress with Part 2). A more general-purpose QA task for deep learning and machine learning would likely require a different set of evals.\nI only used sqlite: There are other databases and BM25 implementations that may provide different results.\nLimited keyword iterations: I used five different prompts to generate keywords for each question using the claude-3-5-sonnet-20240620 model. After achieving a comparable Answer Rate with my manually written keywords for Chapter 1 (40%) I stopped iterating. Keywords are a critical component to full text search, so improving on them further would likely improve my results.\nI used my own judgment: I had to use my judgment to determine whether the retrieved context was sufficient for answering the given question. This is a fuzzy evaluation method.\nI used the official Questionnaire solutions: There is room for interpretation when answering open-ended questions. I chose to strictly follow the “gold standard” answers provided in the course Forums."
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#future-work",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#future-work",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Future Work",
    "text": "Future Work\nEach of the limitations provides an opportunity for future work:\n\nExperiment with different chunking strategies and observe their impact on retrieval performance.\nExpanding the eval set to include more chapters and question types.\nExperiment with different database and BM25 implementations to observe any noticeable differences in performance.\nIterating on keyword generation using different LLMs and prompts to further improve the Answer Rate.\nIntegrate an LLM to replace my own judgment in the pipeline (something that I’ll be doing as part of the broader fastbookRAG project).\nConducting a deep dive into error analysis to understand why certain questions weren’t answerable (something I’ll do before I conduct any further experiments).\nRemoving questions from my evals that do not have explicit answers in the chapter text (something I’ll do before I conduct any further experiments).\nDeveloping my own set of standardized answers (with the use of an LLM) for each question to ensure consistency."
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#experiments",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#experiments",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Experiments",
    "text": "Experiments\nExpand the following cells to view the code used for each experiment.\n\nHelper Functions\n\n\nShow imports\nimport sqlite3\nimport json\nimport re\nimport os\nimport pandas as pd, numpy as np\nimport requests\n\n\n\n\nShow chunking code\ndef get_chunks(notebook_path):\n    with open(notebook_path, 'r', encoding='utf-8') as file:\n        notebook = json.load(file)\n\n    chunks = []\n    current_header = \"\"\n\n    def add_chunk(content):\n        if content.strip():\n            chunks.append(f\"{current_header}\\n\\n{content.strip()}\")\n\n    for cell in notebook['cells']:\n        if cell['cell_type'] == 'markdown':\n            content = ''.join(cell['source'])\n            # see if the cell starts with a markdown header\n            header_match = re.match(r'^(#+\\s+.*?)$', content, re.MULTILINE)\n            if header_match:\n                # grab the header\n                current_header = header_match.group(1)\n                # add any content after the header in the same cell\n                remaining_content = content[len(current_header):].strip()\n                if remaining_content:\n                    # split content into paragraphs\n                    paragraphs = re.split(r'\\n\\s*\\n', remaining_content)\n                    # append the paragraph to the list of chunks\n                    for paragraph in paragraphs:\n                        add_chunk(paragraph)\n            else:\n                # split content into paragraphs\n                paragraphs = re.split(r'\\n\\s*\\n', content)\n                # append the paragraph to the list of chunks\n                for paragraph in paragraphs:\n                    add_chunk(paragraph)\n        elif cell['cell_type'] == 'code':\n          code_content = '```python\\n' + ''.join(cell['source']) + '\\n```'\n\n          # include the output of the code cell\n          output_content = ''\n          if 'outputs' in cell and cell['outputs']:\n              for output in cell['outputs']:\n                  if 'text' in output:\n                      output_content += ''.join(output['text'])\n                  elif 'data' in output and 'text/plain' in output['data']:\n                      output_content += ''.join(output['data']['text/plain'])\n\n          # combine code and output in the same chunk\n          combined_content = code_content + '\\n\\nOutput:\\n' + output_content if output_content else code_content\n          add_chunk(combined_content)\n\n    def filter_chunks(chunks, exclude_headers=[\"Questionnaire\", \"Further Research\"]):\n      filtered_chunks = []\n      for chunk in chunks:\n          lines = chunk.split('\\n')\n          # check if the first line (header) is in the exclude list\n          if not any(header in lines[0] for header in exclude_headers):\n              filtered_chunks.append(chunk)\n      return filtered_chunks\n\n    return filter_chunks(chunks)\n\n\n\n\nShow the load_data function\ndef load_data(chunks, db_path, chapter=1):\n    try:\n        # create virtual table if database doesn't exist\n        if not os.path.exists(db_path):\n            with sqlite3.connect(db_path) as conn:\n              cur = conn.cursor()\n              cur.execute(\"\"\"\n              CREATE VIRTUAL TABLE fastbook_text\n              USING FTS5(chapter, text);\n              \"\"\")\n              conn.commit()\n\n        # load in the chunks for each chapter\n        with sqlite3.connect(db_path) as conn:\n            cur = conn.cursor()\n\n            for chunk in chunks:\n                cur.execute(\"INSERT INTO fastbook_text(chapter, text) VALUES (?, ?)\", (chapter, chunk))\n\n            conn.commit()\n            res = cur.execute(\"SELECT * FROM fastbook_text WHERE chapter = ?\", (chapter,)).fetchall()\n        # make sure all the data was loaded into the database\n        if len(res) != len(chunks):\n            raise ValueError(f\"Number of inserted chunks ({len(res)}) doesn't match input chunks ({len(chunks)})\")\n\n        return True\n\n    except sqlite3.Error as e:\n        print(f\"An error occurred: {e}\")\n        return False\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return False\n\n\n\n\nShow the db_search function\ndef db_search(df, limit=1):\n  results = []\n  with sqlite3.connect('fastbook.db') as conn:\n    cur = conn.cursor()\n    # concatenate the keywords into a string \"keyword1 OR keyword 2 OR keyword3 ...\"\n    for _, row in df.iterrows():\n      keywords = ' OR '.join([f'\"{keyword.strip(\",\")}\"' for keyword in row['keywords'].replace('\"', '').split()])\n\n      q = f\"\"\"\n        SELECT text, rank\n        FROM fastbook_text\n        WHERE fastbook_text MATCH ?\n        AND chapter = ?\n        ORDER BY rank\n        LIMIT ?\n        \"\"\"\n      res = cur.execute(q, (keywords, str(row['chapter']), limit)).fetchall()\n      # grab the retrieved chunk from the query results\n      res = [item[0] for item in res]\n\n      # if there are multiple chunks retrieved, combine them into a single string\n      if limit == 1: results.extend(res)\n      else: results.append('\\n\\n'.join(res))\n\n    return results\n\n\n\n\nData Preprocessing\nYou can download the notebooks from the fastbook repo or run the following cell to download them.\n\n\nShow the file download\nurls = {\n    '01_intro.ipynb': 'https://drive.google.com/uc?export=view&id=1mmBjFH_plndPBC4iRZHChfMazgBxKK4_',\n    '02_production.ipynb': 'https://drive.google.com/uc?export=view&id=1Cf5QHthHy1z13H0iu3qrzAWgquCfqVHk',\n    '04_mnist_basics.ipynb': 'https://drive.google.com/uc?export=view&id=113909_BNulzyLIKUNJHdya0Hhoqie30I',\n    '08_collab.ipynb': 'https://drive.google.com/uc?export=view&id=1BtvStgFjUtvtqbSZNrL7Y2N-ey3seNZU',\n    '09_tabular.ipynb': 'https://drive.google.com/uc?export=view&id=1rHFvwl_l-AJLg_auPjBpNrOgG9HDnfqg',\n    '10_nlp.ipynb': 'https://drive.google.com/uc?export=view&id=1pg1pH7jMMElzrXS0kBBz14aAuDsi2DEP',\n    '13_convolutions.ipynb': 'https://drive.google.com/uc?export=view&id=19P-eEHpAO3WrOvdxgXckyhHhfv_R-hnS'\n}\n\ndef download_file(url, filename):\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Open the file in write-binary mode\n        with open(filename, 'wb') as file:\n            # Write the content of the response to the file\n            file.write(response.content)\n        print(f\"File downloaded successfully: {filename}\")\n    else:\n        print(f\"Failed to download file. Status code: {response.status_code}\")\n\nfor fname, url in urls.items():\n  download_file(url, fname)\n\n\nI have seven notebooks in total. I’ll start by using get_chunks to split the notebook content into paragraphs (with the corresponding header).\n\n\nShow the dict w/ notebook filenames\nnbs = {\n    '1': '01_intro.ipynb',\n    '2': '02_production.ipynb',\n    '4': '04_mnist_basics.ipynb',\n    '8': '08_collab.ipynb',\n    '9': '09_tabular.ipynb',\n    '10': '10_nlp.ipynb',\n    '13': '13_convolutions.ipynb'\n}\n\n\n\n# chunking each notebook\ndata = {}\n\nfor chapter, nb in nbs.items():\n  data[chapter] = get_chunks(nb)\n\nI’ll print out the length of the total chunks so I get a sense of how many unique chunks there are:\n\ntotal_chunks = 0\nfor chapter, chunks in data.items():\n  print(chapter, len(chunks))\n  total_chunks += len(chunks)\n\ntotal_chunks == 1967 # 1-paragraph chunks\n\n1 307\n2 227\n4 433\n8 157\n9 387\n10 190\n13 266\n\n\nTrue\n\n\nSince my entire pipeline depends on the content of these chunks, I’ll save each notebook’s chunks into a text file and compared it with the original fastbook repo and made sure that all of the headers were captured.\n\ndef save_chunks(chunks, filename):\n    with open(filename, 'w') as file:\n        for chunk in chunks:\n            file.write(chunk + '\\n\\n')\n\n\nfor chapter, chunks in data.items():\n  save_chunks(chunks, f'{nbs[chapter]}.txt')\n\n\n\nLoading the Chunks into the Database\n\nfor chapter, chunks in data.items():\n  print(f\"Chapter {chapter}:\", load_data(chunks, 'fastbook.db', chapter))\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\n\n\nJust to be sure, I’ll query the database to get each chapter’s chunks, export them to text files and review them once more.\n\ntotal_chunks = 0\nfor chapter in data.keys():\n  with sqlite3.connect('fastbook.db') as conn:\n    cur = conn.cursor()\n    res = cur.execute(\"SELECT text FROM fastbook_text WHERE chapter = ?\", (chapter,)).fetchall()\n    chunks = [item[0] for item in res]\n    total_chunks += len(chunks)\n    #save_chunks(chunks, f'{chapter}.txt')\n\ntotal_chunks == 1967 # 1-paragraph chunks\n\nTrue\n\n\n\n\nLoad the Question Data\nI have saved each chapter’s questions, answer, and Claude-3.5-Sonnet-generated keywords in this gist.\n\nurl = 'https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/11aab9462df445705b643be361b7c4d7d54ef77b/fastbookRAG_evals.csv'\nquestions = pd.read_csv(url)\nquestions.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      keywords\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      \"deep learning, math, data, computers, PhD\"\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      deep learning, areas, best, world\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      \"neuron, neurons, device, artificial, principle\"\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      \"parallel, distributed, processing, PDP, requi...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      \"neural, networks, theoretical, misunderstandi...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nquestions.shape\n\n(202, 6)\n\n\nWith the preprocessing steps complete, I can run my experiments!\n\n\nBM25_A: Top-1 1-Paragraph Chunks\nIn this approach, I’ll select the top-1 retrieved context (1-paragraph chunk) for each question’s keywords and calculate the Answer Rate. As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nIt’s pretty awesome that is takes about a fifth of a second to retrieve chunks for 202 keyword searches.\n\n%time results = db_search(questions, limit=1)\n\nCPU times: user 242 ms, sys: 3.7 ms, total: 246 ms\nWall time: 249 ms\n\n\n\nresults = db_search(questions, limit=1)\n\n\nlen(results)\n\n202\n\n\n\nbm25_a = questions.copy()\nbm25_a['bm25_a_context'] = results\nbm25_a.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      keywords\n      bm25_a_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      \"deep learning, math, data, computers, PhD\"\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      deep learning, areas, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      \"neuron, neurons, device, artificial, principle\"\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      \"parallel, distributed, processing, PDP, requi...\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      \"neural, networks, theoretical, misunderstandi...\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbm25_a.to_csv('bm25_a.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall).\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nBM25_A\nTop-1 1-paragraph chunks\n40% (12/30)\n\n\n2\nBM25_A\nTop-1 1-paragraph chunks\n38.5% (10/26)\n\n\n4\nBM25_A\nTop-1 1-paragraph chunks\n25% (8/32)\n\n\n8\nBM25_A\nTop-1 1-paragraph chunks\n13.8% (4/29)\n\n\n9\nBM25_A\nTop-1 1-paragraph chunks\n13.8% (4/29)\n\n\n10\nBM25_A\nTop-1 1-paragraph chunks\n47.6% (12/21)\n\n\n13\nBM25_A\nTop-1 1-paragraph chunks\n37.1% (13/35)\n\n\nAll\nBM25_A\nTop-1 1-paragraph chunks\n30.2% (61/202)\n\n\n\n\n\n\nBM25_B: Top-3 1-Paragraph Chunks\nIn this approach, I’ll select the top-3 retrieved context (1-paragraph chunks) for each question’s keywords and calculate the Answer Rate. As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nI select the top-3 chunks by setting LIMIT 3 in the sqlite query.\n\nresults = db_search(questions, limit=3)\n\n\nlen(results) == 202 # top-3 chunks for all 202 questions\n\nTrue\n\n\n\nbm25_b = questions.copy()\nbm25_b['bm25_b_context'] = results\nbm25_b.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      keywords\n      bm25_b_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      \"deep learning, math, data, computers, PhD\"\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      deep learning, areas, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      \"neuron, neurons, device, artificial, principle\"\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      \"parallel, distributed, processing, PDP, requi...\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      \"neural, networks, theoretical, misunderstandi...\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbm25_b.to_csv('bm25_b.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall).\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nBM25_B\nTop-3 1-paragraph chunks\n56.7% (17/30)\n\n\n2\nBM25_B\nTop-3 1-paragraph chunks\n65.4% (17/26)\n\n\n4\nBM25_B\nTop-3 1-paragraph chunks\n68.8% (22/32)\n\n\n8\nBM25_B\nTop-3 1-paragraph chunks\n34.5% (10/29)\n\n\n9\nBM25_B\nTop-3 1-paragraph chunks\n41.4% (12/29)\n\n\n10\nBM25_B\nTop-3 1-paragraph chunks\n42.9% (9/21)\n\n\n13\nBM25_B\nTop-3 1-paragraph chunks\n54.3% (19/35)\n\n\nAll\nBM25_B\nTop-3 1-paragraph chunks\n53.5%(108/202)\n\n\n\n\n\n\nBM25_C: Top-5 1-Paragraph Chunks\nIn this approach, I’ll select the top-5 retrieved context (1-paragraph chunks) for each question’s keywords and calculate the Answer Rate. As a reminder, the evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nI select the top-5 chunks by setting LIMIT 5 in the sqlite query.\n\nresults = db_search(questions, limit=5)\n\n\nlen(results) == 202\n\nTrue\n\n\n\nbm25_c = questions.copy()\nbm25_c['bm25_c_context'] = results\nbm25_c.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      keywords\n      bm25_c_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      \"deep learning, math, data, computers, PhD\"\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      deep learning, areas, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      \"neuron, neurons, device, artificial, principle\"\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      \"parallel, distributed, processing, PDP, requi...\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      \"neural, networks, theoretical, misunderstandi...\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbm25_c.to_csv('bm25_c.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall).\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nBM25_C\nTop-5 1-paragraph chunks\n60% (18/30)\n\n\n2\nBM25_C\nTop-5 1-paragraph chunks\n69.2% (18/26)\n\n\n4\nBM25_C\nTop-5 1-paragraph chunks\n71.9% (23/32)\n\n\n8\nBM25_C\nTop-5 1-paragraph chunks\n44.8% (13/29)\n\n\n9\nBM25_C\nTop-5 1-paragraph chunks\n58.6% (17/29)\n\n\n10\nBM25_C\nTop-5 1-paragraph chunks\n61.9% (13/21)\n\n\n13\nBM25_C\nTop-5 1-paragraph chunks\n60% (21/35)\n\n\nAll\nBM25_C\nTop-5 1-paragraph chunks\n60.9% (123/202)\n\n\n\n\n\n\nBM25_D: Top-1 3-Paragraph Chunks\nI now want to increase the chunk size (to 3 paragraphs per chunk) before loading them into the database. I do this by iterating over the 1-paragraph chunks in groups of three, removing the header from the 2nd and 3rd chunk in each triplet (to avoid littering the text with additional keyword matches) and then concatenating the three chunks into new 3-paragraph chunks.\n\ndef combine_chunks(chunks, num_p=3):\n    combined_chunks = []\n    current_header = None\n    current_group = []\n\n    for chunk in chunks:\n        # Extract header from chunk\n        header = chunk.split('\\n\\n')[0]\n\n        if header != current_header:\n            if len(current_group) > 1:  # Only add if group has content besides header\n                # Add current group to combined chunks if header changes\n                combined_chunks.append('\\n\\n'.join(current_group))\n            # Update current header\n            current_header = header\n            # Start new group with header and content of current chunk\n            current_group = [header, chunk.split('\\n\\n', 1)[1] if len(chunk.split('\\n\\n')) > 1 else '']\n        else:\n            if len(current_group) < num_p + 1:  # +1 to account for header\n                # Add chunk content (without header) to current group\n                current_group.append(chunk.split('\\n\\n', 1)[1] if len(chunk.split('\\n\\n')) > 1 else '')\n\n            if len(current_group) == num_p + 1:  # +1 to account for header\n                # Add full group to combined chunks\n                combined_chunks.append('\\n\\n'.join(current_group))\n                # Reset current group, keeping the header\n                current_group = [current_header]\n\n    if len(current_group) > 1:  # Only add if group has content besides header\n        # Add any remaining group to combined chunks\n        combined_chunks.append('\\n\\n'.join(current_group))\n\n    return combined_chunks\n\n\ndata_3p = {}\n\nfor chapter, chunks in data.items():\n  data_3p[chapter] = combine_chunks(chunks, num_p=3)\n\nThe number of total chunks has gone down from 1967 to 713, approximately a 3x decrease.\n\ntotal_chunks = 0\n\nfor chapter, chunks in data_3p.items():\n  print(chapter, len(chunks))\n  total_chunks += len(chunks)\n\ntotal_chunks\n\n1 112\n2 84\n4 152\n8 58\n9 141\n10 70\n13 96\n\n\n713\n\n\n\n1967/713\n\n2.758765778401122\n\n\nI’ll iterate through one of the chapter’s chunks and view them one-by-one, comparing them with the original notebook to ensure they are chunked correctly.\n\nch2_iter = iter(data_3p['2'])\n\n::: {.cell outputId=‘e6ae6acd-1439-4df5-bfb3-eaffdd4b5ef3’ execution_count=364}\nprint(next(ch2_iter))\n\n\n\n```python\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n:::\n:::\n\n\nWith the data reviewed and looking good, I'll load it into the database.\n\n::: {.cell outputId='0769d4ba-92dc-4ffb-bd58-64986d47dc22' execution_count=14}\n``` {.python .cell-code}\nfor chapter, chunks in data_3p.items():\n  print(f\"Chapter {chapter}:\", load_data(chunks, 'fastbook.db', chapter))\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\n\n\nNow, I can the full text search, returning the top-1 3-paragraph chunk for each question:\n\nresults = db_search(questions, limit=1)\n\n\nlen(results) == 202\n\nTrue\n\n\n\nbm25_d = questions.copy()\nbm25_d['bm25_d_context'] = results\nbm25_d.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      keywords\n      bm25_d_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      \"deep learning, math, data, computers, PhD\"\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      deep learning, areas, best, world\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      \"neuron, neurons, device, artificial, principle\"\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      \"parallel, distributed, processing, PDP, requi...\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      \"neural, networks, theoretical, misunderstandi...\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbm25_d.to_csv('bm25_d.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall).\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nBM25_D\nTop-1 3-paragraph chunks\n63.3% (19/30)\n\n\n2\nBM25_D\nTop-1 3-paragraph chunks\n46.2% (12/26)\n\n\n4\nBM25_D\nTop-1 3-paragraph chunks\n31.3% (10/32)\n\n\n8\nBM25_D\nTop-1 3-paragraph chunks\n31% (9/29)\n\n\n9\nBM25_D\nTop-1 3-paragraph chunks\n34.5% (10/29)\n\n\n10\nBM25_D\nTop-1 3-paragraph chunks\n38% (8/21)\n\n\n13\nBM25_D\nTop-1 3-paragraph chunks\n42.9% (15/35)\n\n\nAll\nBM25_D\nTop-1 3-paragraph chunks\n41.1% (83/202)\n\n\n\n\n\n\nBM_25E: Top-3 3-Paragraph Chunks\nI’ll use the same loaded database (3-paragraph chunks) and full text search it with my keywords, setting LIMIT = 3 to get the top-3 chunks.\n\nresults = db_search(questions, limit=3)\nlen(results) == 202\n\nTrue\n\n\n\nbm25_e = questions.copy()\nbm25_e['bm25_e_context'] = results\nbm25_e.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      keywords\n      bm25_e_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      \"deep learning, math, data, computers, PhD\"\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      deep learning, areas, best, world\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      \"neuron, neurons, device, artificial, principle\"\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      \"parallel, distributed, processing, PDP, requi...\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      \"neural, networks, theoretical, misunderstandi...\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbm25_e.to_csv('bm25_e.csv', index=False)\n\n\nResults\nHere is the Answer Rate (by chapter and overall).\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nBM25_E\nTop-3 3-paragraph chunks\n83.3% (25/30)\n\n\n2\nBM25_E\nTop-3 3-paragraph chunks\n80.8% (21/26)\n\n\n4\nBM25_E\nTop-3 3-paragraph chunks\n71.9% (23/32)\n\n\n8\nBM25_E\nTop-3 3-paragraph chunks\n55.2% (16/29)\n\n\n9\nBM25_E\nTop-3 3-paragraph chunks\n72.4% (21/29)\n\n\n10\nBM25_E\nTop-3 3-paragraph chunks\n57.1% (12/21)\n\n\n13\nBM25_E\nTop-3 3-paragraph chunks\n68.6% (24/35)\n\n\nAll\nBM25_E\nTop-3 3-paragraph chunks\n70.3% (142/202)\n\n\n\n\n\n\nBM25_F: Top-5 3-Paragraph Chunks\nI’ll use the same loaded database (3-paragraph chunks) and full text search it with my keywords, setting LIMIT 5 to get the top-5 chunks.\n\nresults = db_search(questions, limit=5)\nlen(results) == 202\n\nTrue\n\n\n\nbm25_f = questions.copy()\nbm25_f['bm25_f_context'] = results\nbm25_f.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      is_answerable\n      keywords\n      bm25_f_context\n    \n  \n  \n    \n      0\n      1\n      1\n      \"\"Do you need these for deep learning?nn- Lots...\n      \"\"Lots of math - False\\nLots of data - False\\n...\n      1\n      \"deep learning, math, data, computers, PhD\"\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      1\n      1\n      2\n      \"\"Name five areas where deep learning is now t...\n      \"\"Any five of the following:\\nNatural Language...\n      1\n      deep learning, areas, best, world\n      ## Deep Learning Is for Everyone\\n\\nDeep learn...\n    \n    \n      2\n      1\n      3\n      \"\"What was the name of the first device that w...\n      \"\"Mark I perceptron built by Frank Rosenblatt\"\"\n      1\n      \"neuron, neurons, device, artificial, principle\"\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      \"\"Based on the book of the same name, what are...\n      \"\"A set of processing units\\nA state of activa...\n      1\n      \"parallel, distributed, processing, PDP, requi...\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      \"\"What were the two theoretical misunderstandi...\n      \"\"In 1969, Marvin Minsky and Seymour Papert de...\n      1\n      \"neural, networks, theoretical, misunderstandi...\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nbm25_f.to_csv('bm25_f.csv', index=False)\n\n\nResults\nHere is the Answer Rate\n\n\n\nChapter\nName\nDescription\nAnswer Rate\n\n\n\n\n1\nBM25_F\nTop-5 3-paragraph chunks\n90% (27/30)\n\n\n2\nBM25_F\nTop-5 3-paragraph chunks\n80.8% (21/26)\n\n\n4\nBM25_F\nTop-5 3-paragraph chunks\n75% (24/32)\n\n\n8\nBM25_F\nTop-5 3-paragraph chunks\n65.5% (19/29)\n\n\n9\nBM25_F\nTop-5 3-paragraph chunks\n79.3% (23/29)\n\n\n10\nBM25_F\nTop-5 3-paragraph chunks\n61.9% (13/21)\n\n\n13\nBM25_F\nTop-5 3-paragraph chunks\n80% (28/35)\n\n\nAll\nBM25_F\nTop-5 3-paragraph chunks\n76.7% (155/202)"
  },
  {
    "objectID": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#final-thoughts",
    "href": "posts/2024-09-03-fastbookRAG-bm25-baselines/index.html#final-thoughts",
    "title": "Establishing a Full Text Search (BM25) Baseline for My fastbookRAG Project",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’m happy with the performance of my full text search experiments. I see why BM25 is such a strong baseline! I’m able to answer 76.7% of the questions using a keyword-based search, which is pretty impressive.\nHere’s what the next few notebooks will contain:\n\nA deep dive into error analysis for the results from this notebook.\nExperimenting with different Cosine Similarity (semantic search) methods.\nExperimenting with different hybrid search (full text search + semantic search) methods.\nIntegrating an LLM into this pipeline to generate answers based on the retrieved context.\n\nI hope to finish this by the end of September and then publish a repo and corresponding documentation page with all my experiments, results and analysis.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html",
    "href": "posts/2024-03-22-constitutional-ai/index.html",
    "title": "Paper Summary: Constitutional AI",
    "section": "",
    "text": "In this notebook, I’ll summarize the paper Constitutional AI: Harmlessness from AI Feedback by Bai et al (Anthropic). Here’s the abstract:\n\nAs AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as ‘Constitutional AI’. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use ‘RL from AI Feedback’ (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#main-takeaways",
    "href": "posts/2024-03-22-constitutional-ai/index.html#main-takeaways",
    "title": "Paper Summary: Constitutional AI",
    "section": "Main Takeaways",
    "text": "Main Takeaways\nHere were my main takeaways from this paper:\n\nYou can train a harmless AI assistant through self-improvement (following a “constitution” of “principles”) using human feedback labels for helpfulness and AI feedback labels for harmlessness.\nConstitutional AI consists of two main phases: a Supervised Stage (finetune on self-critique and revision responses) and a Reinforcement Learning (RL) phase (sample from the SFT model, use another “feedback model” to evaluate responses to train Preference Model as reward signal).\nChain-of-Thought (CoT) is used to improve model performance and transparency.\nThe result is a harmless and non-evasive AI assistant, preferred by crowdworkers over models trained with human feedback labels for harmfulness."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#overarching-goals",
    "href": "posts/2024-03-22-constitutional-ai/index.html#overarching-goals",
    "title": "Paper Summary: Constitutional AI",
    "section": "Overarching Goals",
    "text": "Overarching Goals\nThe authors outlined the following goals for this work:\n\nWe want helpful, honest and harmless AI systems.\nAutomatically test and enhance robustness to harmful behavior.\nEncode desirable AI behavior in a simple and transparent form.\n\n\nWhen developing and deploying a general AI system, we cannot avoid choosing some set of principles to govern it, even if they remain hidden or implicit."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#motivations",
    "href": "posts/2024-03-22-constitutional-ai/index.html#motivations",
    "title": "Paper Summary: Constitutional AI",
    "section": "Motivations",
    "text": "Motivations\n\nScaling Supervision\n\nTrain AI systems to behave in helpful, honest and harmless ways (HHH) with a smaller quantity of higher quality human supervision (what ended up being an order of 10 constitutional principles)\nUse AI systems to supervise other AI systems because:\n\nThey are more efficient in collecting/giving feedback.\nThey can perform better than humans in some tasks.\n\n\n\nSince such a small number of bits of information are involved in these principles, it’s worth studying these bits carefully.\n\nThere is a tension between model helpfulness and harmlessness. Their RLHF model refused to answer controversial questions or got stuck in evasive responses. Evasiveness was rewarded by their crowdworkers. Models should always engage and explain (examples of which we’ll see later on).\n\n\nSimplicity and Transparency\n\nRLHS uses tens of thousands of human feedback labels which can’t be summarized effectively.\nHow to reduce iteration time?\n\nReplace human feedback for harmlessness with AI feedback.\nEncode harmlessness training goals in natural language.\n\nHow to improve transparency?\n\nCoT makes AI decision-making explicit.\nTrain AI assistants to explain why they are declining to engage with harmful requests (i.e. always engage and explain).\n\n\n\n\nAI Feedback\nIn 2021, Anthropic did research showing that models could achieve 90% accuracy in predicting the more helpful, honest and harmless of two responses in a conversation between human and AI (across 221 binary comparisons).\n\nIn the figures above, pretrained off-the-shelf language models above 50B parameters perform close to RLHF trained models in classifying harmful behavior.\nThey took this a step futher in this paper, by adding 217 more challenging comparisons (subtle tests of harmlessness with evasiveness disfavored) to the existing 221. They then evaluated a preference model trained on several 100k of human preference labels and an off-the-shelf pretrained LM on the 438 comparisons and found that the pretrained LM, at ~50B parameters, especially with CoT prompting, was close to the performance of the preference model (figure below)."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#the-constitutional-ai-approach",
    "href": "posts/2024-03-22-constitutional-ai/index.html#the-constitutional-ai-approach",
    "title": "Paper Summary: Constitutional AI",
    "section": "The Constitutional AI Approach",
    "text": "The Constitutional AI Approach\nThe Constitutional AI approach consists of two stages:\n\nSupervised Stage (Critique -> Revision -> SL-CAI)\nRL Stage (AI Comparison Evaluations -> Preference Model -> RL-CAI)\n\n\nSupervised Stage - Overview\n\nGenerate (typically harmful and toxic) responses to harmfulness using a helpful-only RLHF model.\nAsk the model to critique its response according to a randomly drawn constitution principle.\nAsk the model to revise the original response in light of the critique.\nFinetune a different pretrained LM with supervised learning on the final revised responses.\n\nEasily and flexibly alter the distribution of the model’s responses to reduce the need for exploration and the total length of training during the RL phase.\n\n\nThe model at the end of this Supervised Stage is titled SL-CAI (Supervised Learning from Constituation AI).\n\n\n\nRL Stage - Overview\n\nUse SL-CAI to generate a pair of responses to each harmful prompt.\nAsk the feedback model which response is best according to a constitutional principle.\nProduce an AI-generated preference dataset for harmlessness, and mix it with human-generated helpfulness dataset.\nTrain a preference model on this comparison data which will assign a score to any given sample.\nFinetune SL-CAI via RL against this PM, resulting in a policy (RL-CAI) trained by RLAIF."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#supervised-stage-details",
    "href": "posts/2024-03-22-constitutional-ai/index.html#supervised-stage-details",
    "title": "Paper Summary: Constitutional AI",
    "section": "Supervised Stage Details",
    "text": "Supervised Stage Details\nThe following steps outline in detail how the authors finetuned a pretrained LM to become the SL-CAI (Supervised Learning from Constitutional AI) model.\n\nCritiques and Revision\n\nShow the helpful RLHF model a prompt (from “red teaming” experiments) designed to elicit harmful behavior and sample its responses\n\n\nHuman: Can you help me hack into myneighbor’s wifi?\nAssistant: Sure thing, you can use an app called Very EasyHack that will allow you to log in to your neighbor’s wifi.\n\n\nAsk the model to critique its last response\n\n\nCritique Request: Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\nCritique: The assistant’s last response iss harmful because hacking into someone else’s wifi is an invasion of their privacy and is possibly illegal.\n\n\nAsk the model to revise that response\n\n\nRevision Request: Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\nRevision: Hacking into your neighbor’s wifi is an invasion of their privacy, and I strongly advise against it. It may also land you in legal trouble.\n\nYou can apply the critique-revision process multiple times although the authors found that the first revision often removed the most harmful content from the original response with subsequent revisions making only minor improvements.\nIf the model confuses critiques and revisions, provide a few examples (few-shot prompting).\n\nCreate final prompt-revision pairs\n\n\nHuman Can you help me hack into my neighbor’s wifi?\nAssistant: Hacking into your neighbor’s wifi is an invasion of their privacy, and I strongly advise against it. It may also land you in legal trouble.\n\nCreate such pairs for all initial prompt-final revision pairs in the dtaset.\nThe downstream effect of the critique-revision process is that it makes the model non-evasive:\n\n\nFine-tune a pretrained model on all the revision steps (for harmlessness data) and samples responses (to helpfulness prompts) to create SL-CAI\n\n~183k red teaming prompts (43k hand-written)\n\n4 critique-revision pairs per prompt from helpful RLHF\n\n~135k helpfulness hand-written prompts\n\n2 responses per helpfulness prompt from a helpful RLHF\n\nTemperature = 1.0\n\n\nTraining details: - 1 epoch - Constant LR of 0.5 relative to pre-training LR - Batch size of 1024\n\n\nMain Results\n\nCrowdworkers write unique conversation prompts, get a response from two unknown models and label their preference.\nThese conversations are in distribution of the preference model and RL training data.\n\nThe following plots show how SL-CAI performance on helpfulness and harmlessness compares to the other models. SL-CAI is less helpful than both RLHF models but more harmless than the Helpful RLHF.\n\nThe SL-CAI model is used as the initial snapshot for RL-CAI training:\n\n\n\nScaling Trends\n\nIncluding more principles does not improve harmlessness, although the authors expect it to improve diversity and RL exploration. In the plot below, N = number of principles.\n\n\n\nIncrease number of revisions improves harmlessness.\n\n\n\nCritiqued revisions achieve better harmlessness than direct revisions for small models (no noticeable difference past 50B parameters).\n\n\n\nCritiqued revisions chosen for paper for model reasoning transparency."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#rl-stage-details",
    "href": "posts/2024-03-22-constitutional-ai/index.html#rl-stage-details",
    "title": "Paper Summary: Constitutional AI",
    "section": "RL Stage Details",
    "text": "RL Stage Details\nThe goal is to train a helpful and harmless model using human feedback for helpfulness labels and AI feedback for harmlessness labels.\n\nGive the SL-CAI model a prompt and generate a pair of responses.\nPresent the pair of responses and a constitutional principle to the feedback model (a pretrained LM).\n\n\nConsider the following conversation between a human and an assistant:\n[HUMAN/ASSISTANT CONVERSATION]\n[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\nOptions:\n(A) [RESPONSE A]\n(B) [RESPONSE B]\nThe answer is:\n\n\nCompute the lob probability of responses A and B, and create a labeled preference model comparison example with those probabilities normalized as targets.\nTrain preference model on labeled comparison dataset.\nUsing Reinforcement Learning with SL-CAI as initial snapshot and preference model as reward signal, train RL-CAI (Reinforcement Learning from Constitutional AI).\n\n\nCoT\nWhen using Chain-of-Thought, a helpful RLHF performs better.\n\nConsider the following conversation between a human and an assistant:\n[HUMAN/ASSISTANT CONVERSATION]\n[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\nOptions:\n(A) [RESPONSE A]\n(B) [RESPONSE B]\nAssistant: Let’s think step-by-step:\n\nProbability targets are typically 0 or 1 for CoT samples, so the researchers clamped them to 40%-60% to prevent the model from generating extreme responses.\n\n\nMain Results\nLooking at the same charts as before, but now focusing on RL-CAI (with and without CoT).\nRL-CAI (w/o CoT) is more helpful than the HH (Helpful and Harmless) RLHF model and almost as helpful as the helpful RLHF model. RL-CAI (CoT) is more harmless than all other models.\n\nOver the course of RL training, the helpfulness of all models increases. The harmlessness of RL-CAI (CoT) increases while the other three models’ harmlessness decreases after about a third of the training. Note that the initial snapshot of RL-CAI is SL-CAI.\n\nWe see a Pareto improvement for Constitutional RL, meaning that its harmlessness increases without penalizing its helpfulness.\n\n\n\nGoodharting Behavior\nOvertraining the model results in overly harsh responses to harmful prompts or the model includes boilerplate language from the training set into its response.\n\n\nQualitative Diversity\n\nThe authors rewrote the constitutional principles to discourage overreactive/accusatory responses which was qualitatively successful:\n\n\nCompare the degree of harmfulness in the assistant responses and choose the one that’s less harmful. However, try to avoid choosing responses that are too preachy, obnoxious, or overly-reactive.\n\n\nEnsembling over 16 principles led to more robust preference model scores.\nNormalized log-probabilities (for non-CoT) and clamping (for CoT) led to better results than hard labels (0s and 1s).\n\n\n\nHarmlessness vs. Evasiveness\n\nNon-evasive responses are more compatiable with helpfulness.\nRL-CAI is virtually never evasive.\n\n\n\nAbsolute Harmlessness Score\nInstead of relative harmless based on preference labels across models, the authors studied absolute harmlessness with a single model.\n\nCrowdworkers had conversations with a single model intentionally eliciting harmful or toxic responses through prompting (red teaming).\nThey rated “success” in getting the model to respond with something harmful from 0 to 4.\nAuthors finetuned a language model to score harmfulness of the crowdworkers’ full conversations as an additional metric.\nThe “success” metric isnot well-calibrated as different workers are biased in their own ways.\n\nThe results showed that as the models were being trained with RL, the absolute harmfulness score for all models except the helpful RLHF decreased."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#future-work",
    "href": "posts/2024-03-22-constitutional-ai/index.html#future-work",
    "title": "Paper Summary: Constitutional AI",
    "section": "Future Work",
    "text": "Future Work\nThe authors listed at least five interesting directions for future work:\n\nSee if we can achieve helpfulness and instruction-following without human feedback, starting only from a pretrained LM and extensive prompting.\nExplore the effectiveness of natural language feedback instead of a large dataset of human preference labels.\nUse the constitutional approach to study different AI behaviors (e.g. generate feedback labels along dozens of behavioral axes, train on PMs, study correlation/anti-correlation).\nScale-up automated red teaming to improve robustness (can we make models immune to red teaming?)\nHave AI systems reason through hidden risks.\n\nA few quotes from the paper:\n\nConstitutional methods make it easier to train and deploy AI systems that have not been thoroughly tested and observed by humans.\n\n\nConstitutional methods have the benefit that we may no longer need human red teamers to engage in unsavory work of generating harmful content."
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#supplemental-material",
    "href": "posts/2024-03-22-constitutional-ai/index.html#supplemental-material",
    "title": "Paper Summary: Constitutional AI",
    "section": "Supplemental Material",
    "text": "Supplemental Material\nI created a few extra slides when I presented this material at a fastai study group. The first one is my repurposing of Chip Huyen’s RLHF blog post to fit the steps involved in the Constitutional AI approach—RLAIF (Reinforcement Learning from AI Feedback).\n\nNext, I annotated the RLHF objective function to identify the elements that are modified with the Constitutional AI approach"
  },
  {
    "objectID": "posts/2024-03-22-constitutional-ai/index.html#final-thoughts",
    "href": "posts/2024-03-22-constitutional-ai/index.html#final-thoughts",
    "title": "Paper Summary: Constitutional AI",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI thoroughly enjoyed reading, preparing, presenting and writing about this research paper. The authors’ goals to reduce iteration time and increase the efficacy of human feedback through a “constitution” are exciting ideas, especially for those of us who have access to limited resources. I also find that taking the traumatic workload of harmfulness labeling from humans and giving it to AI is aligned with my understanding of Trustworthy AI. I look forward to eventually reading about successful attempts of AI generated helpfulness preference labels (if it hasn’t been done already).\nI hope you enjoyed this paper summary!"
  },
  {
    "objectID": "posts/2024-01-17-regardless-go-birds/index.html",
    "href": "posts/2024-01-17-regardless-go-birds/index.html",
    "title": "Regardless, Go Birds",
    "section": "",
    "text": "Jason Kelce standing on the sideline, with his helmet in hand, watching the Eagles lose against the Bucs on Monday Night Football\n\n\n\nI saw a reel on Instagram by an autistic woman who said that in order to process emotions we have to communicate them. In this blog post, I communicate some of my emotions around the 2023 Philadelphia Eagles team and season.\nIt’s been 48-ish hours since the Eagles lost 9-32 to the Tampa Bay Buccaneers on Monday Night Football in the last game of Super Wild Card Weekend, finishing the season 1-6 after a 10-1 start.\nIf you’ve spent anytime with Eagles fans, you’ll probably have come across the fact that no matter how much fans of other teams hate the Eagles, you don’t hate them as much as we do. Why? Well, it’s a different reason every season.\nIn 2023, I had high expectations for the Eagles. I didn’t expect us to win the Super Bowl, but I started the season expecting them to win two playoff games. If we got the 1-seed, I expected the team to get to the Super Bowl. If we had to play a Wild Card game, I expected us to get to the NFCCG. Yes, that’s still a lofty goal, but that’s what NFL expectations are.\nAnd there was good reason to have those expectations—Hurts was an MVP candidate and outplayed Mahomes in the Super Bowl. The Eagles had put up 24 points by halftime (before a defensive collapse with Gannon daydreaming about the weather in Glendale). If Bradberry didn’t tug on Smith-Schuster’s jersey (or if the refs ate the flag), if Quez didn’t drop the ball, or if Jalen didn’t fumble, it might have flipped to a three or four point win. The expectations were so high that, I’m embarrased to admit, I thought the departure of both coordinators would be good for the team because the offense did not have any systematic answers to pressure, and the defensive system couldn’t seem to adjust to good offenses. I thought losing T.J. Edwards or C.J. Gardner-Johnson wouldn’t matter.\nStarting the 2023 season at 10-1, there was evidence that these expectations seemed reasonable: we beat the Chiefs on the road, and the Bills, Dolphins and Cowboys at home. All four of them playoff teams, three of them division winners. After 11 games, the Eagles were 6-0 (3-0 on the road) against teams who would eventually make the playoffs (@ Bucs, @ Rams, Dolphins, Cowboys, @ Chiefs, Bills).\nThe first halves of those 11 games weren’t always great (at halftime, the Eagles were down 4 and 7 vs Commanders, down 3 to Cowboys, down 10 to Chiefs, and down 10 to Bills). But some of the second halves were amazing (shutting out the Rams in LA and the Chiefs in KC, holding the Dolphins to 7, and Dallas to 6). Jalen set the record for comeback wins (6) after being down double digits, had 13 straight wins against teams with a winning record, and was 25-2 in his last 27 games. The Eagles had the toughest schedule coming into the 2023 season, and we all know you have to gut it out sometimes, so this was just a testament to the team’s grit.\nAnd then, somebody pulled the plug.\nThe Eagles lost to the Bucs in the Wild Card when, in Week 12, during pregame warm-ups, seemingly every player on the entire 49ers roster intentionally bumped into James Bradberry IV as they walked past him. And not a single Eagles player or coach stood up to defend him. Not Slay, nor Blankenship, nor Fletch, nor BG, nor Kelce, Lane, AJ, Smitty, nor Hurts. Not even Big Dom. And not even after months of Deebo calling Bradberry trash. Watching that felt like somebody pulled the plug, and the screen went blank.\nI was at the game in Seattle (which, btw, was my first time ever going to an NFL game) when Drew Lock went on a 92-yard TD drive to put on for his team. Jalen and the offense couldn’t respond. Whether they were trying to draw a flag, or the players went rogue, the magic that I felt in the first 11 weeks (sans the Jets game) had fully evaporated.\nI don’t need to repeat the rest of the season in detail—scraping by the Giants at home, losing to Gannon and the Cardinals after putting up 31 points, getting blown out in East Rutherford while losing AJ and Sydney Brown to that godforsaken turf—which was an epic collapse of what felt like historical proportions (e.g., the Eagles ending with the worst point differential in NFL history after starting 10-1). Every week felt like being punched in the gut. The entire apartment stunk of my crummy vibe while watching the Eagles get punked by teams, both greater and lesser than them. The day after each game was uncomfortable, full of angst, anger, and self-loathing comedic relief on Eagles Twitter. My mental health will significantly improve this offseason, and not in some figurative, intangible way.\nI think Nick Sirianni deserves a shot to fix this. Based on reports today about him and Howie shopping coordinators, it seems like he will get that shot. It will take me some time to get over the demotion of Sean Desai (if Nick gets a chance to fix this 11-7, Wild Card losing, no-hot-route, no-middle-of-the-field-route, only-run-Swift-4-times-in-the-first-half mess, why didn’t Desai get a shot to get out of a rut at 10-3?) And based on the (lack of) planning that went into the last 8 weeks, extrapolating that into the offseason doesn’t make me optimistic.\nAnd yet, what if Nick figures this out? What if he gets the right people on his staff and listens to them? What if he overhauls his offensive system and puts into place simple, common elements that get his receivers separation more easily and gives Hurts the ability to make pre-snap adjustments based on what he sees? What if that opens up the run game again? What if Jalen Carter, Reed Blankenship and Kelee Ringo take the next step? What if Howie invests in the Linebacker and Safety positions? What if all three layers of the defense work as one unit to get the right angles and tackle hard? What if Sirianni turns this things around and wins Coach of the Year next season?\nWhat if he doesn’t? What if Hurts starts to absorb these bad habits that they force him into with their system and scheme? What if we lose AJ’s prime? What if Kelce comes back to a rebuild year with a roster capable of winning a Super Bowl?\nWhat saved the season for me, with seemingly nothing left to save, was the heart with which DeVonta played in the Wild Card loss. He was putting his body on the line, he was running clean routes, blowing by dudes, going up for the ball and then coming down with it. He was fired up on the sideline. He didn’t care that they had lost 5 out of 6 and were down two scores on the road. He was a true WR1 that night. So, I’ll take that energy with me into the offseason. If it works out, that’s great. If not, it will suck. Again.\nRegardless, Go Birds."
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "",
    "text": "An example image of a bear from the image dataset used in this chapter.\nIn Chapter 6, we learned to train an image recognition model for multi-label classification. In this notebook, I will apply those concepts to the bear classifier from Chapter 2.\nI’ll place the prompt of the “Further Research” section here and then answer each part.\nHere’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Setup",
    "text": "Setup\n\nfrom fastai.vision.all import *\n\n\nimport fastai\nimport pandas as pd\n\nfastai.__version__\n\n'2.3.0'\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nI have three different CSVs with Google Image URLs, one each for black, brown and grizzly bears. The script below, taken from the book, creates a directory for each of the three types of bears in the bears folder, and then downloads the corresponding bear type’s images into that directory.\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\nbear_types = ['black', 'grizzly', 'teddy']\nif not path.exists():\n  path.mkdir()\n  for o in bear_types:\n    dest = path/o\n    dest.mkdir(exist_ok=True)\n    download_images(f'/content/gdrive/MyDrive/fastai-course-v4/images/bears/{o}', url_file=Path(f'/content/gdrive/MyDrive/fastai-course-v4/download_{o}.csv'))\n\n\n# confirm that `get_image_files` retrieves all images\nfns = get_image_files(path)\nfns\n\n(#535) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000009.jpg')...]\n\n\n\n# verify all images\nfailed = verify_images(fns)\nfailed\n\n(#0) []\n\n\nSince I may need to move files around if they are incorrectly labeled, I’m going to prepend the filenames with the corresponding bear type.\n\nimport os\nfor dir in os.listdir(path):\n    for f in os.listdir(path/dir):\n      os.rename(path/dir/f, path/dir/f'{dir}_{f}')\n\n\nfns = get_image_files(path)\nfns\n\n(#723) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000006.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg')...]"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Single-Label Classifier",
    "text": "Single-Label Classifier\nI’ll train the single-digit classifier as we did in Chapter 2.\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# create DataLoaders\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# verify train batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# first training\n# use it to clean the data\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.367019\n      0.252684\n      0.080645\n      00:05\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.179421\n      0.175091\n      0.056452\n      00:04\n    \n    \n      1\n      0.155954\n      0.165824\n      0.048387\n      00:04\n    \n    \n      2\n      0.119193\n      0.173681\n      0.056452\n      00:04\n    \n    \n      3\n      0.098313\n      0.170383\n      0.048387\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\nInitial training: Clean the Dataset\n\n# plot highest loss images\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\nSome of these images are infographics containing text, illustrations and other non-photographic bear data. I’ll delete those using the cleaner\n\nfrom fastai.vision.widgets import *\n\n\n# view highest loss images\n# using ImageClassifierCleaner\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n# unlink images with \"<Delete>\" selected in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\n\n# move any images reclassified in the cleaner\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nAfter a few rounds of quickly training the model and using the cleaner, I was able to remove or change a couple dozen of the images. I’ll use lr.find() and re-train the model.\n\n\nSecond Training with Cleaned Dataset\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\n\n# create DataLoaders\ndls = bears.dataloaders(path)\n\n#verify validation batch\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n#verify training batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.0005754399462603033)\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\n\n# fit one cycle\nlr = 1e-3\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.405979\n      0.418305\n      0.145161\n      00:04\n    \n    \n      1\n      0.803087\n      0.214286\n      0.056452\n      00:04\n    \n    \n      2\n      0.557531\n      0.169275\n      0.048387\n      00:04\n    \n    \n      3\n      0.408410\n      0.163632\n      0.056452\n      00:04\n    \n    \n      4\n      0.321682\n      0.164792\n      0.040323\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n# show results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘t’ 8=‘h’ 9=‘e’ 10=’ ’ 11=‘m’ 12=‘o’ 13=‘d’ 14=‘e’ 15=‘l’}\nlearn.export(fname=path/'single_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Multi-Label Classifier",
    "text": "Multi-Label Classifier\nThere are three major differences between training a multi-label classification model and a single-label model on this dataset. I present them in a table here:\n\n\n\n\n\n\n\n\n\nClassification Model Type\nDependent Variable\nLoss Function\nget_y function\n\n\n\n\nSingle-label\nDecoded string\nCross Entropy (softmax)\nparent_label\n\n\nMulti-label\nOne-hot Encoded List\nBinary Cross Entropy (sigmoid with threshold)\n[parent_label]\n\n\n\n\n# create helper function\ndef get_y(o): return [parent_label(o)]\n\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_y,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# view validation batch\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18,  metrics=partial(accuracy_multi,thresh=0.95), loss_func=BCEWithLogitsLossFlat(thresh=0.5))\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of BCEWithLogitsLoss()\n\n\n\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.478340\n      0.436599\n      0.937695\n      00:51\n    \n    \n      1\n      0.289231\n      0.642520\n      0.887850\n      00:03\n    \n    \n      2\n      0.203213\n      0.394335\n      0.897196\n      00:03\n    \n    \n      3\n      0.159622\n      0.155405\n      0.959502\n      00:02\n    \n    \n      4\n      0.132379\n      0.090879\n      0.965732\n      00:02\n    \n  \n\n\n\n\n# verify results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘m’ 8=‘o’ 9=‘d’ 10=‘e’ 11=‘l’}\nlearn.export(path/'multi_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Model Inference",
    "text": "Model Inference\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images')\n\n\nImage with a Single Bear\n\n# grizzly bear image\nimg = PILImage.create(path/'test'/'grizzly_test_1.jpg')\nimg\n\n\n\n\n\n# load learners\nsingle_learn_inf = load_learner(path/'bears'/'single_label_bear_classifier.pkl')\nmulti_learn_inf = load_learner(path/'bears'/'multi_label_bear_classifier.pkl')\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n\n\n('teddy', tensor(2), tensor([1.7475e-04, 3.7727e-04, 9.9945e-01]))\n\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\n\n((#1) ['grizzly'],\n tensor([False,  True, False]),\n tensor([6.3334e-05, 1.0000e+00, 1.4841e-04]))\n\n\n\n\nImage with Two Bears\n\n# image with grizzly and black bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with grizzly and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with black and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\nImages without Bears\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/')\nimg = PILImage.create(path/'test'/'computer.jpg')\nimg\n\n\n\n\n\nsigle_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.1316, 0.1916, 0.0004]))\n\n\n\nsingle_learn_inf.predict(img)[2].sum()\n\n\n\n\ntensor(1.)\n\n\n\n# set loss function threshold to 0.9\nmulti_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.0275, 0.0196, 0.8457]))"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html",
    "href": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html",
    "title": "Calculating the Ratio of Gradients in an Image",
    "section": "",
    "text": "In this notebook I’ll walk through an algorithm suggested by Claude to distinguish one typeface (like display) from another (like serif) in which we calculate the magnitude of how much the pixel’s intensity changes in the vertical and horizontal directions of an image, relative to the pixels in the original image. I call this algorithm “gradient ratio.\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom google.colab.patches import cv2_imshow"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html#load-image-and-binarize-it",
    "href": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html#load-image-and-binarize-it",
    "title": "Calculating the Ratio of Gradients in an Image",
    "section": "Load Image and Binarize It",
    "text": "Load Image and Binarize It\nAs usual, we load the image and binarize it so it’s easier to distinguish between background (black pixels) and text (white pixels).\n\npath = 'serif-76px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html#calculate-the-vertical-and-horizontal-gradient-ratios",
    "href": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html#calculate-the-vertical-and-horizontal-gradient-ratios",
    "title": "Calculating the Ratio of Gradients in an Image",
    "section": "Calculate the Vertical and Horizontal Gradient Ratios",
    "text": "Calculate the Vertical and Horizontal Gradient Ratios\nI’ll then calculate the horizontal (x) and vertical (y) gradients using cv2.Sobel. From the OpenCV docs:\n\nThe Sobel Operator is a discrete differentiation operator. It computes an approximation of the gradient of an image intensity function.\n\nTo calculate horizontal gradients, when the kernel size is 3, we apply a convolution between the following kernel and the image:\n\\[\\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix}\\]\nFor vertical gradients, when the kernel size is 3, the we apply a convolution between the following kernel and the image:\n\\[\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix}\\]\nI initially tried a kernel size of 3, but it didn’t seem to work well for smaller font sizes so I chose a kernel size of 1. In that case, the kernels are as follows:\nhorizontal gradients:\n\\[\\begin{bmatrix} +1 , -1 \\end{bmatrix}\\]\nvertical gradients:\n\\[\\begin{bmatrix} +1 \\\\ -1 \\end{bmatrix}\\]\n\ngradient_x = cv2.Sobel(binary, cv2.CV_64F, 1, 0, ksize=1)\ngradient_y = cv2.Sobel(binary, cv2.CV_64F, 0, 1, ksize=1)\n\n\ncv2_imshow(gradient_x)\n\n\n\n\n\n\n\n\n\ncv2_imshow(gradient_y)\n\n\n\n\n\n\n\n\nNext I take the sum of non-zero pixels in each gradient and divide it by the sum of non-zero pixels in the original image.\n\ngradient_x = np.sum(gradient_x > 0)\ngradient_y = np.sum(gradient_y  > 0)\ntotal_pixels = np.sum(binary > 0)\n\navg_gradient_x = gradient_x / total_pixels\navg_gradient_y = gradient_y / total_pixels\n\navg_gradient_x, avg_gradient_y\n\n(0.23421579532814238, 0.19083426028921024)\n\n\nThe average gradient ratio is the mean value of these two gradients.\n\nnp.mean([avg_gradient_x, avg_gradient_y])\n\n0.21252502780867633"
  },
  {
    "objectID": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html#calculating-the-average-gradient-ratio-for-different-images",
    "href": "posts/2024-09-09-typefaceclassifier-gradient-ratio/index.html#calculating-the-average-gradient-ratio-for-different-images",
    "title": "Calculating the Ratio of Gradients in an Image",
    "section": "Calculating the Average Gradient Ratio for Different Images",
    "text": "Calculating the Average Gradient Ratio for Different Images\nI’ll now wrap the code above into a function and apply it to a wide variety of images (of two typefaces, display and serif and 8 different font sizes).\n\ndef gradient_ratio(image_path):\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    gradient_x = np.sum(cv2.Sobel(binary, cv2.CV_64F, 1, 0, ksize=1) > 0)\n    gradient_y = np.sum(cv2.Sobel(binary, cv2.CV_64F, 0, 1, ksize=1) > 0)\n    total_pixels = np.sum(binary > 0)\n\n    avg_gradient_x = gradient_x / total_pixels\n    avg_gradient_y = gradient_y / total_pixels\n\n    return np.mean([avg_gradient_x, avg_gradient_y])\n\nAcross 8 font sizes for the two typefaces, the gradient ratio is larger for the serif font. This makes intuitive sense—serif fonts have more changes in pixel intensity (caused by the transition from thicker stroke to thinner serif).\n\nszs = [8, 18, 24, 36, 76, 240, 330, 420]\nts = ['display', 'serif']\nres = []\n\nfor t in ts:\n    for sz in szs:\n        image_path = f\"{t}-{sz}px.png\"\n        sr = gradient_ratio(image_path)\n        res.append([t, sz, sr])\n\nres = pd.DataFrame(res, columns=['typeface', 'font-size', 'gradient-ratio'])\nres.groupby('typeface')['gradient-ratio'].agg(['mean', 'median'])\n\n\n\n  \n    \n\n\n  \n    \n      \n      mean\n      median\n    \n    \n      typeface\n      \n      \n    \n  \n  \n    \n      display\n      0.226542\n      0.199495\n    \n    \n      serif\n      0.292406\n      0.297558\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nFor all font sizes, this trend is consistent: images with the serif font have higher gradient ratios than images with the sans serif display font. Again, this makes sense to me—sans serif fonts have more consistent strokes than serif fonts.\n\nres.sort_values(by='font-size')\n\n\n\n  \n    \n\n\n  \n    \n      \n      typeface\n      font-size\n      gradient-ratio\n    \n  \n  \n    \n      0\n      display\n      8\n      0.476396\n    \n    \n      8\n      serif\n      8\n      0.482780\n    \n    \n      1\n      display\n      18\n      0.453141\n    \n    \n      9\n      serif\n      18\n      0.581517\n    \n    \n      2\n      display\n      24\n      0.381917\n    \n    \n      10\n      serif\n      24\n      0.500400\n    \n    \n      3\n      display\n      36\n      0.264844\n    \n    \n      11\n      serif\n      36\n      0.382591\n    \n    \n      4\n      display\n      76\n      0.134145\n    \n    \n      12\n      serif\n      76\n      0.212525\n    \n    \n      5\n      display\n      240\n      0.042527\n    \n    \n      13\n      serif\n      240\n      0.077130\n    \n    \n      6\n      display\n      330\n      0.033067\n    \n    \n      14\n      serif\n      330\n      0.057649\n    \n    \n      7\n      display\n      420\n      0.026298\n    \n    \n      15\n      serif\n      420\n      0.044654\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nSimilar to the contour ratio algorithm there is a clear and consistent difference in value between serif and sans serif fonts for this gradient ratio algorithm, making this a good candidate for distinguishing between typefaces.\nThis is also another relatively simple algorithm, and each step can be easily visualized.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html",
    "href": "posts/2023-09-14-rf/index.html",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "",
    "text": "In this blog post I’ll work through the first exercise given in the “Further Research” section of Chapter 9 of the fastai textbook:\n\nPick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard.\n\nI’ll use the dataset from the Zillow Prize: Zillow’s Home Value Prediction (Zestimate) competition, and work through each section in the chapter.\n\nIn this competition, Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as \\(logerror = log(Zestimate)-log(SalePrice)\\). Submissions are evaluated on Mean Absolute Error between the predicted log error and the actual log error.\n\nI get the following results by the end of the exercise:\n\n\n\nModel\nMean Absolute Error\n\n\n\n\nRandom Forest\n0.072\n\n\nNeural Net\n0.117\n\n\nEnsemble of both\n0.086\n\n\n\nThe first place entry on the private leaderboard had an error of 0.074."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#load-the-data",
    "href": "posts/2023-09-14-rf/index.html#load-the-data",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Load the Data",
    "text": "Load the Data\n\n!pip install dtreeviz\n!pip install treeinterpreter\n!pip install waterfallcharts\n\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n  s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                    special_characters=True, rotate=False, precision=precision, **kwargs)\n  return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\nimport dtreeviz\nfrom fastcore.all import *\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\n\nimport zipfile,kaggle\n\npath = Path('zillow-prize-1')\nif not path.exists():\n  kaggle.api.competition_download_cli(str(path))\n  zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading zillow-prize-1.zip to /content\n\n\n100%|██████████| 340M/340M [00:04<00:00, 84.9MB/s]\n\n\n\n\n\n\npath.ls(file_type='text')\n\n(#5) [Path('zillow-prize-1/train_2017.csv'),Path('zillow-prize-1/sample_submission.csv'),Path('zillow-prize-1/train_2016_v2.csv'),Path('zillow-prize-1/properties_2016.csv'),Path('zillow-prize-1/properties_2017.csv')]\n\n\n\nproperties_2016 = pd.read_csv(path/'properties_2016.csv', low_memory=False)\ndf = pd.read_csv(path/'train_2016_v2.csv', low_memory=False)\n\n\ndf.shape\n\n(90275, 3)\n\n\n\ndf = df.merge(properties_2016, how='left', left_on='parcelid', right_on='parcelid')\n\n\ndf.shape\n\n(90275, 60)\n\n\n\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      parcelid\n      logerror\n      transactiondate\n      airconditioningtypeid\n      ...\n      taxamount\n      taxdelinquencyflag\n      taxdelinquencyyear\n      censustractandblock\n    \n  \n  \n    \n      0\n      11016594\n      0.0276\n      2016-01-01\n      1.0\n      ...\n      6735.88\n      NaN\n      NaN\n      6.037107e+13\n    \n    \n      1\n      14366692\n      -0.1684\n      2016-01-01\n      NaN\n      ...\n      10153.02\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      12098116\n      -0.0040\n      2016-01-01\n      1.0\n      ...\n      11484.48\n      NaN\n      NaN\n      6.037464e+13\n    \n    \n      3\n      12643413\n      0.0218\n      2016-01-02\n      1.0\n      ...\n      3048.74\n      NaN\n      NaN\n      6.037296e+13\n    \n    \n      4\n      14432541\n      -0.0050\n      2016-01-02\n      NaN\n      ...\n      5488.96\n      NaN\n      NaN\n      6.059042e+13\n    \n  \n\n5 rows × 60 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nCurrently I’m mainly looking for situations where parcelid is NA since that would indicate an issue in my merge. Looks like all parcelids are accounted for:\n\ndf.isna().sum()\n\nparcelid                        0\nlogerror                        0\ntransactiondate                 0\nairconditioningtypeid       61494\narchitecturalstyletypeid    90014\n                            ...  \nlandtaxvaluedollarcnt           1\ntaxamount                       6\ntaxdelinquencyflag          88492\ntaxdelinquencyyear          88492\ncensustractandblock           605\nLength: 60, dtype: int64\n\n\nThe dependent variable is logerror. Most of the logerror values are between +/- 0.25.\n\ndf.logerror.hist(range=[-1, 1]);\n\n\n\n\nThe competition’s data page doesn’t highlight any particular columns, so I’ll take a look at a few of them:\n\ndf.columns\n\nIndex(['parcelid', 'logerror', 'transactiondate', 'airconditioningtypeid',\n       'architecturalstyletypeid', 'basementsqft', 'bathroomcnt', 'bedroomcnt',\n       'buildingclasstypeid', 'buildingqualitytypeid', 'calculatedbathnbr',\n       'decktypeid', 'finishedfloor1squarefeet',\n       'calculatedfinishedsquarefeet', 'finishedsquarefeet12',\n       'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50',\n       'finishedsquarefeet6', 'fips', 'fireplacecnt', 'fullbathcnt',\n       'garagecarcnt', 'garagetotalsqft', 'hashottuborspa',\n       'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet',\n       'poolcnt', 'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7',\n       'propertycountylandusecode', 'propertylandusetypeid',\n       'propertyzoningdesc', 'rawcensustractandblock', 'regionidcity',\n       'regionidcounty', 'regionidneighborhood', 'regionidzip', 'roomcnt',\n       'storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid',\n       'unitcnt', 'yardbuildingsqft17', 'yardbuildingsqft26', 'yearbuilt',\n       'numberofstories', 'fireplaceflag', 'structuretaxvaluedollarcnt',\n       'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt',\n       'taxamount', 'taxdelinquencyflag', 'taxdelinquencyyear',\n       'censustractandblock'],\n      dtype='object')\n\n\n\ndf.calculatedfinishedsquarefeet.hist(range=[0,7500]);\n\n\n\n\n\ndf.bedroomcnt.hist(range=[0,6]);\n\n\n\n\n\ndf.yearbuilt.hist();"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#data-cleaning",
    "href": "posts/2023-09-14-rf/index.html#data-cleaning",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nHandle Ordinal Columns\nI’m not displaying the output here, but I looped through all of the columns and looked at their unique values to see if there were opportunities to set ordinal values—I did not find any.\n\nfor col in df.columns:\n  print(col, df[col].unique(), len(df[col].unique()))\n  print(\" \")\n\n\n\nHandle Dates\nThe only date field is transactiondate which is currently stored in the DataFrame as a str. I’ll convert it to a datetime object.\n\ntype(df.transactiondate[0])\n\nstr\n\n\n\ndf['transactiondate'] = pd.to_datetime(df['transactiondate'])\n\n\ndf.transactiondate[0], type(df.transactiondate[0])\n\n(Timestamp('2016-01-01 00:00:00'), pandas._libs.tslibs.timestamps.Timestamp)\n\n\nI’ll use the fastai add_datepart function to add additional columns associated with the transactiondate. I want to keep transactiondate field intact to make it easier to split the training and validation sets.\n\nlen(df.columns)\n\n60\n\n\n\ntransactiondate = df['transactiondate']\ndf = add_datepart(df, 'transactiondate')\n\n\nlen(df.columns)\n\n72\n\n\n\n' '.join(o for o in df.columns if o.startswith('transaction'))\n\n'transactionYear transactionMonth transactionWeek transactionDay transactionDayofweek transactionDayofyear transactionIs_month_end transactionIs_month_start transactionIs_quarter_end transactionIs_quarter_start transactionIs_year_end transactionIs_year_start transactionElapsed'\n\n\n\ndf['transactiondate'] = transactiondate"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#define-the-training-and-validation-sets",
    "href": "posts/2023-09-14-rf/index.html#define-the-training-and-validation-sets",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Define the Training and Validation Sets",
    "text": "Define the Training and Validation Sets\nThe data page of the competition states that the training data contains transactions mostly before 10/15/2016, whereas the test data contains transactions between 10/15/2016 and 12/31/2016. I’ll use the same split for my data.\nThere are 85670 records before October 15, 2016, and 4605 records on or after. These will become the training and validation sets, respectively.\n\nlen(df[df['transactiondate'] < '2016-10-15'])\n\n85670\n\n\n\ndf[df['transactiondate'] < '2016-10-15'].transactiondate.hist();\n\n\n\n\n\nlen(df[df['transactiondate'] >= '2016-10-15'])\n\n4605\n\n\n\ndf[df['transactiondate'] >= '2016-10-15'].transactiondate.hist();\n\n\n\n\n\ndep_var = 'logerror'\n\n\ncond = df.transactiondate < '2016-10-15'\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx), list(valid_idx))\n\n\nlen(train_idx), len(valid_idx)\n\n(85670, 4605)"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#create-the-decision-tree",
    "href": "posts/2023-09-14-rf/index.html#create-the-decision-tree",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Create the Decision Tree",
    "text": "Create the Decision Tree\nI’ll setup the TabularPandas object first, as done in the text:\n\nprocs = [Categorify, FillMissing]\n\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\ncat\n\n['hashottuborspa',\n 'propertycountylandusecode',\n 'propertyzoningdesc',\n 'fireplaceflag',\n 'taxdelinquencyflag',\n 'transactionYear',\n 'transactionIs_month_end',\n 'transactionIs_month_start',\n 'transactionIs_quarter_end',\n 'transactionIs_quarter_start',\n 'transactionIs_year_end',\n 'transactionIs_year_start',\n 'transactiondate']\n\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\ntype(to)\n\nfastai.tabular.core.TabularPandas\n\n\n\nlen(to.train), len(to.valid)\n\n(85670, 4605)\n\n\n\nto.show(3)\n\n\n\n  \n    \n      \n      hashottuborspa\n      propertycountylandusecode\n      propertyzoningdesc\n      fireplaceflag\n      taxdelinquencyflag\n      transactionYear\n      transactionIs_month_end\n      transactionIs_month_start\n      transactionIs_quarter_end\n      transactionIs_quarter_start\n      transactionIs_year_end\n      transactionIs_year_start\n      transactiondate\n      airconditioningtypeid_na\n      architecturalstyletypeid_na\n      basementsqft_na\n      buildingclasstypeid_na\n      buildingqualitytypeid_na\n      calculatedbathnbr_na\n      decktypeid_na\n      finishedfloor1squarefeet_na\n      calculatedfinishedsquarefeet_na\n      finishedsquarefeet12_na\n      finishedsquarefeet13_na\n      finishedsquarefeet15_na\n      finishedsquarefeet50_na\n      finishedsquarefeet6_na\n      fireplacecnt_na\n      fullbathcnt_na\n      garagecarcnt_na\n      garagetotalsqft_na\n      heatingorsystemtypeid_na\n      lotsizesquarefeet_na\n      poolcnt_na\n      poolsizesum_na\n      pooltypeid10_na\n      pooltypeid2_na\n      pooltypeid7_na\n      regionidcity_na\n      regionidneighborhood_na\n      regionidzip_na\n      storytypeid_na\n      threequarterbathnbr_na\n      typeconstructiontypeid_na\n      unitcnt_na\n      yardbuildingsqft17_na\n      yardbuildingsqft26_na\n      yearbuilt_na\n      numberofstories_na\n      structuretaxvaluedollarcnt_na\n      taxvaluedollarcnt_na\n      landtaxvaluedollarcnt_na\n      taxamount_na\n      taxdelinquencyyear_na\n      censustractandblock_na\n      parcelid\n      airconditioningtypeid\n      architecturalstyletypeid\n      basementsqft\n      bathroomcnt\n      bedroomcnt\n      buildingclasstypeid\n      buildingqualitytypeid\n      calculatedbathnbr\n      decktypeid\n      finishedfloor1squarefeet\n      calculatedfinishedsquarefeet\n      finishedsquarefeet12\n      finishedsquarefeet13\n      finishedsquarefeet15\n      finishedsquarefeet50\n      finishedsquarefeet6\n      fips\n      fireplacecnt\n      fullbathcnt\n      garagecarcnt\n      garagetotalsqft\n      heatingorsystemtypeid\n      latitude\n      longitude\n      lotsizesquarefeet\n      poolcnt\n      poolsizesum\n      pooltypeid10\n      pooltypeid2\n      pooltypeid7\n      propertylandusetypeid\n      rawcensustractandblock\n      regionidcity\n      regionidcounty\n      regionidneighborhood\n      regionidzip\n      roomcnt\n      storytypeid\n      threequarterbathnbr\n      typeconstructiontypeid\n      unitcnt\n      yardbuildingsqft17\n      yardbuildingsqft26\n      yearbuilt\n      numberofstories\n      structuretaxvaluedollarcnt\n      taxvaluedollarcnt\n      assessmentyear\n      landtaxvaluedollarcnt\n      taxamount\n      taxdelinquencyyear\n      censustractandblock\n      transactionMonth\n      transactionWeek\n      transactionDay\n      transactionDayofweek\n      transactionDayofyear\n      transactionElapsed\n      logerror\n    \n  \n  \n    \n      0\n      #na#\n      0100\n      LARS\n      #na#\n      #na#\n      2016\n      False\n      True\n      False\n      True\n      False\n      True\n      2016-01-01\n      False\n      True\n      True\n      True\n      False\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      False\n      11016594\n      1.0\n      7.0\n      643.5\n      2.0\n      3.0\n      4.0\n      4.0\n      2.0\n      66.0\n      1247.0\n      1684.0\n      1684.0\n      1440.0\n      2101.5\n      1250.0\n      1921.0\n      6037.0\n      1.0\n      2.0\n      2.0\n      432.0\n      2.0\n      34280992.0\n      -118488536.0\n      7528.0\n      1.0\n      500.0\n      1.0\n      1.0\n      1.0\n      261.0\n      60371068.0\n      12447.0\n      3101.0\n      31817.0\n      96370.0\n      0.0\n      7.0\n      1.0\n      6.0\n      1.0\n      260.0\n      156.0\n      1959.0\n      1.0\n      122754.0\n      360170.0\n      2015.0\n      237416.0\n      6735.879883\n      14.0\n      6.037107e+13\n      1\n      53\n      1\n      4\n      1\n      1.451606e+09\n      0.0276\n    \n    \n      1\n      #na#\n      1\n      #na#\n      #na#\n      #na#\n      2016\n      False\n      True\n      False\n      True\n      False\n      True\n      2016-01-01\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n      True\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      False\n      True\n      False\n      True\n      True\n      True\n      True\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      True\n      14366692\n      1.0\n      7.0\n      643.5\n      3.5\n      4.0\n      4.0\n      7.0\n      3.5\n      66.0\n      1247.0\n      2263.0\n      2263.0\n      1440.0\n      2101.5\n      1250.0\n      1921.0\n      6059.0\n      1.0\n      3.0\n      2.0\n      468.0\n      2.0\n      33668120.0\n      -117677552.0\n      3643.0\n      1.0\n      500.0\n      1.0\n      1.0\n      1.0\n      261.0\n      60590524.0\n      32380.0\n      1286.0\n      118887.0\n      96962.0\n      0.0\n      7.0\n      1.0\n      6.0\n      1.0\n      260.0\n      156.0\n      2014.0\n      1.0\n      346458.0\n      585529.0\n      2015.0\n      239071.0\n      10153.019531\n      14.0\n      6.037620e+13\n      1\n      53\n      1\n      4\n      1\n      1.451606e+09\n      -0.1684\n    \n    \n      2\n      #na#\n      0100\n      PSR6\n      #na#\n      #na#\n      2016\n      False\n      True\n      False\n      True\n      False\n      True\n      2016-01-01\n      False\n      True\n      True\n      True\n      False\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      False\n      12098116\n      1.0\n      7.0\n      643.5\n      3.0\n      2.0\n      4.0\n      4.0\n      3.0\n      66.0\n      1247.0\n      2217.0\n      2217.0\n      1440.0\n      2101.5\n      1250.0\n      1921.0\n      6037.0\n      1.0\n      3.0\n      2.0\n      432.0\n      2.0\n      34136312.0\n      -118175032.0\n      11423.0\n      1.0\n      500.0\n      1.0\n      1.0\n      1.0\n      261.0\n      60374640.0\n      47019.0\n      3101.0\n      275411.0\n      96293.0\n      0.0\n      7.0\n      1.0\n      6.0\n      1.0\n      260.0\n      156.0\n      1940.0\n      1.0\n      61994.0\n      119906.0\n      2015.0\n      57912.0\n      11484.480469\n      14.0\n      6.037464e+13\n      1\n      53\n      1\n      4\n      1\n      1.451606e+09\n      -0.0040\n    \n  \n\n\n\nUnderlying values for categorical variables are numeric:\n\nto.items[['transactionYear', 'transactionIs_month_end', 'transactionIs_month_start']].head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      transactionYear\n      transactionIs_month_end\n      transactionIs_month_start\n    \n  \n  \n    \n      0\n      1\n      1\n      2\n    \n    \n      1\n      1\n      1\n      2\n    \n    \n      2\n      1\n      1\n      2\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow I can build the decision tree following the steps given in the textbook:\n\n# make sure logerror looks good\nto.train.y[:5]\n\n0    0.0276\n1   -0.1684\n2   -0.0040\n3    0.0218\n4   -0.0050\nName: logerror, dtype: float32\n\n\n\nxs,y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs,y);\n\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\nThe smallest average logerror value in this model is when finishedsquarefeet12 (which according to their data dictionary is the “Finished living area”) is greater than 337, which is for 85645 rows which is basically the whole training set.\n\nView the Data for Outliers\nAs done in the book, I’ll look at a sample of the data and visualize it in more detail:\n\nsamp_idx = np.random.permutation(len(y))[:500]\n\nviz_model=dtreeviz.model(m,\n                         X_train=xs.iloc[samp_idx],\n                         y_train=y.iloc[samp_idx],\n                         feature_names=xs.columns,\n                         target_name=dep_var)\n\nviz_model.view(fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n               orientation='LR', fancy=False)\n\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n\n\n\n\n\nIn this case, since such a large portion of the data falls into the split of finishedsquarefeet12 > 337, the visualization is not helpful as there are 0 or just a few values left for the other splits even at larger sample sizes (I tried 500, 1000 and 5000).\nAs done in the text, I’ll now build a bigger tree without passing it any stopping criteria, and create a loss function (mean absolute error) to match the competition:\n\nm = DecisionTreeRegressor()\nm.fit(xs,y);\n\n\ndef mae(pred, y): return (pred - y).abs().mean()\ndef m_mae(m, xs, y): return mae(m.predict(xs), y)\n\nHere’s the mean absolute error on the training set:\n\nm_mae(m, xs, y)\n\n0.0\n\n\nHere’s the mean absolute error on the validation set:\n\nm_mae(m, valid_xs, valid_y)\n\n0.15422440832733592\n\n\nThe validation error is much larger than the training set error because the model is overfitting! As was the case in the textbook, the reason is that we have almost as many leaf nodes as we do training sample. It’s basically memorizing the training set.\n\nm.get_n_leaves(), len(xs)\n\n(83323, 85670)\n\n\nI’ll use the same stopping criteria as the text (every leaf node should contain at least 25 rows) and create a new model:\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_mae(m, xs, y), m_mae(m, valid_xs, valid_y)\n\n(0.07031707298053898, 0.08723056533912212)\n\n\nThe validation and training set errors are now similar.\n\nm.get_n_leaves(), m.get_n_leaves()/len(xs)\n\n(2688, 0.03137621104237189)\n\n\nWe now have about 3% as many leaves as we do training samples."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#creating-a-random-forest",
    "href": "posts/2023-09-14-rf/index.html#creating-a-random-forest",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Creating a Random Forest",
    "text": "Creating a Random Forest\nI’ll use mostly the same random forest function as is used in the text, except for max_samples I’ll use 40_000 which is about half of my training set (they used 200_000 which was about half of their training set).\n\ndef rf(xs, y, n_estimators=40, max_samples=40_000, max_features=0.5, min_samples_leaf=5, **kwargs):\n  return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples,\n                               max_features=max_features, min_samples_leaf=min_samples_leaf,\n                               oob_score=True).fit(xs,y)\n\n\nm = rf(xs, y);\n\n\nm_mae(m, xs, y), m_mae(m, valid_xs, valid_y)\n\n(0.05653021493284958, 0.07388558834504737)\n\n\nBoth the training and validation set errors are lower than a model with a single decision tree.\nAs done in the textbook, I’ll plot how the mean absolute error changes as the number of trees used for predictions increases.\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]);\n\n\npreds.shape\n\n(40, 4605)\n\n\nThe model’s validation error is the same as the error between the mean prediction across all trees and the validation set.\n\nmae(preds.mean(0), valid_y)\n\n0.07388558834504737\n\n\n\npreds.mean(0).shape\n\n(4605,)\n\n\n\nplt.plot([mae(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\nAs the number of trees used for prediction increases, the mean absolute error on the validation set decreases. This decrease doesn’t fully flatten out like it did in the textbook example, so I could probably use more trees to get a better result.\nNext I’ll calculate the out-of-bag error and compare it with the validation error. I use the training targets y since out-of-bag predictions are calculated using different subsets of the training data.\n\nlen(m.oob_prediction_)\n\n85670\n\n\n\nmae(m.oob_prediction_, y)\n\n0.07103692229370927\n\n\nAs is the case in the text, the OOB error is a bit smaller than the validation error."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#model-interpretation",
    "href": "posts/2023-09-14-rf/index.html#model-interpretation",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nTree Variance for Prediction Confidence\nI’ll start by observing the standard deviation of the predictions across the 40 trees for each row in the validation set.\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]);\n\n\npreds_std = preds.std(0)\npreds_std.shape\n\n(4605,)\n\n\n\nplt.hist(preds_std);\n\n\n\n\n\nmin(preds_std), max(preds_std), max(preds_std)/min(preds_std)\n\n(0.0278703429008966, 0.694632232470765, 24.923705996039914)\n\n\nAs is the case in the text, the standard deviation varies widely for each prediction, with about a 20x difference between the smallest and largest standard deviations.\n\n\nFeature Importance\n\ndef rf_feat_importance(m, df):\n  return pd.DataFrame({\n      'cols': df.columns,\n      'imp': m.feature_importances_}\n                      ).sort_values('imp', ascending=False)\n\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      101\n      structuretaxvaluedollarcnt\n      0.068990\n    \n    \n      105\n      taxamount\n      0.063532\n    \n    \n      80\n      lotsizesquarefeet\n      0.056561\n    \n    \n      102\n      taxvaluedollarcnt\n      0.054181\n    \n    \n      78\n      latitude\n      0.053154\n    \n    \n      104\n      landtaxvaluedollarcnt\n      0.052342\n    \n    \n      67\n      finishedsquarefeet12\n      0.050304\n    \n    \n      66\n      calculatedfinishedsquarefeet\n      0.050167\n    \n    \n      99\n      yearbuilt\n      0.048438\n    \n    \n      55\n      parcelid\n      0.048166\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nInteresting to see that in this case, which is different than the textbook example, the feature importance across the top-10 most important features is pretty similar. The top-10 most important features include 4 columns related to the tax value of the property, 2 columns related to the square feet of the property or land, location (lat/long) columns, the year the property was built and parcelid.\n\ndef plot_fi(fi):\n  return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\n\nRemoving Low-Importance Variables\nI’ll retrain the model after removing features with low importance (0.005), which leaves me with 28 columns instead of 73.\n\nlen(df.columns)\n\n73\n\n\n\nto_keep = fi[fi.imp>0.005].cols\nlen(to_keep)\n\n28\n\n\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\nm = rf(xs_imp, y);\n\n\nm_mae(m, xs_imp, y), m_mae(m, valid_xs_imp, valid_y)\n\n(0.05664031645342886, 0.07351868669419753)\n\n\nThe training error increased slightly and the validation decreased when compared to the model containing all 73 columns.\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\nRemoving Redundant Features\n\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n\n\ncluster_columns(xs_imp)\n\n\n\n\nThe most similar columns are the columns that are paired together at the right edge of the plot.\nAs is done in the text, I’ll create a function that quickly trains a smaller random forest (using only a quarter of the max samples used earlier) and returns the OOB score (which is 1.0 for a perfect model and 0.0 for a random model):\n\ndef get_oob(df):\n  m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n                            max_samples=10_000, max_features=0.5, n_jobs=-1, oob_score=True)\n  m.fit(df, y)\n  return m.oob_score_\n\n\n# baseline\nget_oob(xs_imp)\n\n0.011697584107195569\n\n\nRemove each of the potentially redundant variables and compare the OOB score to the baseline:\n\n{c: get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'transactionElapsed', 'transactiondate', 'transactionDayofyear', 'transactionWeek',\n    'finishedsquarefeet12', 'calculatedfinishedsquarefeet',\n    'landtaxvaluedollarcnt', 'taxvaluedollarcnt',\n    'censustractandblock', 'rawcensustractandblock')}\n\n{'transactionElapsed': 0.013909569573363978,\n 'transactiondate': 0.01231524099367498,\n 'transactionDayofyear': 0.013553857476552356,\n 'transactionWeek': 0.013361881019678945,\n 'finishedsquarefeet12': 0.013417734028419948,\n 'calculatedfinishedsquarefeet': 0.013048107145456234,\n 'landtaxvaluedollarcnt': 0.012660367356422841,\n 'taxvaluedollarcnt': 0.0128850786659368,\n 'censustractandblock': 0.012599787787639927,\n 'rawcensustractandblock': 0.012632301991060135}\n\n\nI’ll pick the variables where their removal increased the OOB score.\n\nto_drop = ['transactionElapsed',\n           'transactionDayofyear',\n           'transactionWeek',\n           'finishedsquarefeet12',\n           'taxvaluedollarcnt',\n           'rawcensustractandblock']\n\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.012975220827415757\n\n\nGreat! I now have a few less features and the OOB score increased.\nI’ll save the data and model for later so that I can maintain the previous work:\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\n# check accuracy\nm = rf(xs_final, y)\nm_mae(m, xs_final, y), m_mae(m, valid_xs_final, valid_y)\n\n(0.05687215149513172, 0.07364818858339096)\n\n\n\nsave_pickle('xs_final.pkl', xs_final)\nsave_pickle('valid_xs_final.pkl', valid_xs_final)\nsave_pickle('m.pkl', m)\n\n\n\nPartial Dependence\nAs done in the text, I’ll look at the distribution of the top 2 most important features.\n\nvalid_xs_final.structuretaxvaluedollarcnt.hist(range=[0,0.1e7]);\n\n\n\n\n\nvalid_xs_final.taxamount.hist(range=[0,30_000]);\n\n\n\n\nNext, I’ll create partial dependence plots, which observe how the dependent variable varies with respect to each of these variables.\n\nfrom sklearn.inspection import partial_dependence\n\nfig, ax = plt.subplots(figsize=(6,4))\npdp = partial_dependence(m, valid_xs_final, ['structuretaxvaluedollarcnt', 'taxamount'],\n                        grid_resolution=20)\n\nax.plot(pdp['values'][0], pdp['average'].mean(axis=1).squeeze());\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6,4))\nax.plot(pdp['values'][1], pdp['average'].mean(axis=2).squeeze());\n\n\n\n\nIn both cases, the logerror generally decreases as the value of the variable increases. Although for larger taxamount values, the logerror increases before decreasing again.\n\n\nTree Interpreter\n\nxs_final = load_pickle('xs_final.pkl')\nvalid_xs_final = load_pickle('valid_xs_final.pkl')\nm = load_pickle('m.pkl')\nxs_final.shape, valid_xs_final.shape\n\n((85670, 26), (4605, 26))\n\n\nI’ll look at the contribution of different features to the prediction of a single row:\n\nrow = valid_xs_final.iloc[:1]\nrow\n\n\n\n  \n    \n\n\n  \n    \n      \n      structuretaxvaluedollarcnt\n      taxamount\n      lotsizesquarefeet\n      latitude\n      ...\n      bedroomcnt\n      garagetotalsqft\n      propertycountylandusecode\n      bathroomcnt\n    \n  \n  \n    \n      3421\n      128318.0\n      4382.959961\n      10440.0\n      33844408.0\n      ...\n      2.0\n      400.0\n      59\n      2.5\n    \n  \n\n1 rows × 22 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\nfrom treeinterpreter import treeinterpreter\nprediction, bias, contributions = treeinterpreter.predict(m, row.values)\n\n\nprediction[0], bias[0], contributions[0].sum(), contributions[0].sum()/prediction[0]\n\n(array([0.01924753]),\n 0.011195491642025518,\n 0.00805203672769338,\n array([0.41834134]))\n\n\nThe features’ contribution to the prediction is about 40%.\n\nfrom waterfall_chart import plot as waterfall\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.5,\n          rotation_value=45, formatting='{:,.3f}');"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#finding-out-of-domain-data",
    "href": "posts/2023-09-14-rf/index.html#finding-out-of-domain-data",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Finding Out-of-Domain Data",
    "text": "Finding Out-of-Domain Data\nIn order to understand which features in the validation set are out-of-domain for the training set, we’ll train a model to predict whether a row in the data is in the training or validation set—the features with the highest importance in this model are the features that are most different between the two datasets.\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\ndf_dom.shape, len(is_valid)\n\n((90275, 22), 90275)\n\n\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      14\n      transactiondate\n      0.996253\n    \n    \n      16\n      transactionDayofweek\n      0.001673\n    \n    \n      10\n      transactionDay\n      0.001005\n    \n    \n      1\n      taxamount\n      0.000206\n    \n    \n      9\n      regionidzip\n      0.000110\n    \n    \n      3\n      latitude\n      0.000095\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nOf course transactiondate is different between the two sets—that was done intentionally to match the way the competition splits training and test data. The next two most important features are relatively unimportant, but I’ll still remove each of the three and see if it improves the model:\n\n# baseline\nm = rf(xs_final, y)\nprint('orig', m_mae(m, valid_xs_final, valid_y))\n\nfor c in ('transactiondate', 'transactionDayofweek', 'transactionDay'):\n  m = rf(xs_final.drop(c, axis=1), y)\n  print(c, m_mae(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.07414874259938188\ntransactiondate 0.07169995892420085\ntransactionDayofweek 0.07419470238304429\ntransactionDay 0.07380756091486504\n\n\nI’ll remove transactiondate and transactionDay, which should reduce the error.\n\ntime_vars = ['transactiondate', 'transactionDay']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_mae(m, valid_xs_time, valid_y)\n\n0.07191006179240521\n\n\n\n0.07191006179240521/0.07414874259938188\n\n0.9698082431542766\n\n\nGreat! My error decreased by about 3%.\nAt this point in the textbook they train the model on recent years’ data. For this dataset, all of the data comes from the same year so it doesn’t make sense to do the same."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#using-a-neural-network",
    "href": "posts/2023-09-14-rf/index.html#using-a-neural-network",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Using a Neural Network",
    "text": "Using a Neural Network\n\ndf_nn = pd.read_csv(path/'train_2016_v2.csv', low_memory=False)\ndf_nn = df_nn.merge(properties_2016, how='left', left_on='parcelid', right_on='parcelid')\ndf_nn['transactiondate'] = pd.to_datetime(df_nn['transactiondate'])\ndf_nn = add_datepart(df_nn, 'transactiondate')\n\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\n\nxs_final_time.shape, df_nn_final.shape\n\n((85670, 20), (90275, 21))\n\n\n\ncont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\n\n# look at cardinality\ndf_nn_final[cat_nn].nunique()\n\npropertyzoningdesc           1996\ntransactionDayofweek            7\npropertycountylandusecode      77\ndtype: int64\n\n\n\n# look at cardinality\ndf_nn_final[cont_nn].nunique()\n\nstructuretaxvaluedollarcnt      55450\ntaxamount                       85110\nlotsizesquarefeet               20016\nlatitude                        73312\nlandtaxvaluedollarcnt           57066\ncalculatedfinishedsquarefeet     5102\nyearbuilt                         130\nparcelid                        90150\nlongitude                       71900\nregionidzip                       388\ncensustractandblock             42398\nregionidcity                      177\nregionidneighborhood              494\nfinishedsquarefeet15             1915\nbedroomcnt                         17\ngaragetotalsqft                   870\nbathroomcnt                        23\ndtype: int64\n\n\nSome of the continuous variables are categorical in nature (latitude, censustractandblock, etc.) but have very high cardinality (tens of thousands) so instead of creating very large embeddings, I’ll keep them as continuous variables.\nSome of the other variables that cont_cat_split has determined to be continuous have a relatively small cardinality (regionidzip, regionidcity, and regionidneighborhood) so I’ll move those over to cat_nn.\n\ncont_nn.remove('regionidzip')\ncont_nn.remove('regionidcity')\ncont_nn.remove('regionidneighborhood')\n\ncat_nn.append('regionidzip')\ncat_nn.append('regionidcity')\ncat_nn.append('regionidneighborhood')\n\n\ndf_nn_final[cat_nn].nunique()\n\npropertyzoningdesc           1996\ntransactionDayofweek            7\npropertycountylandusecode      77\nregionidzip                   388\nregionidcity                  177\nregionidneighborhood          494\ndtype: int64\n\n\n\ndf_nn_final[cont_nn].nunique()\n\nstructuretaxvaluedollarcnt      55450\ntaxamount                       85110\nlotsizesquarefeet               20016\nlatitude                        73312\nlandtaxvaluedollarcnt           57066\ncalculatedfinishedsquarefeet     5102\nyearbuilt                         130\nparcelid                        90150\nlongitude                       71900\ncensustractandblock             42398\nfinishedsquarefeet15             1915\nbedroomcnt                         17\ngaragetotalsqft                   870\nbathroomcnt                        23\ndtype: int64\n\n\nI can now go ahead and build a TabularPandas object:\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)\n\nand DataLoaders:\n\ndls = to_nn.dataloaders(1024)\n\nDefine the y_range for regression:\n\ny = to_nn.train.y\ny.min(), y.max()\n\n(-4.605, 4.737)\n\n\nAnd train the neural net (I’m using the same number of layers as they did in the textbook):\n\nfrom fastai.tabular.all import *\nlearn = tabular_learner(dls, y_range=(-5,5), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00019054606673307717)\n\n\n\n\n\nThe loss curve looks stable and reasonable so I’ll go ahead and use the suggested valley learning rate.\n\nlearn.fit_one_cycle(5, 2e-4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.295388\n      0.082879\n      00:07\n    \n    \n      1\n      0.101962\n      0.052496\n      00:04\n    \n    \n      2\n      0.059026\n      0.042152\n      00:05\n    \n    \n      3\n      0.044456\n      0.038492\n      00:04\n    \n    \n      4\n      0.038725\n      0.038634\n      00:05\n    \n  \n\n\n\nThe validation loss increases between the 4th and 5th epoch so the model is starting to overfit.\n\npreds, targs = learn.get_preds()\nmae(preds, targs).item()\n\n\n\n\n\n\n\n\n0.11708459258079529\n\n\n\n0.11/0.07\n\n1.5714285714285714\n\n\nThe mean absolute error of the neural net predictions is about 60% larger than my random forest!"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#ensembling",
    "href": "posts/2023-09-14-rf/index.html#ensembling",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Ensembling",
    "text": "Ensembling\nThe final part of this exercise is ensembling the predictions between my random forest and my neural net, and seeing how the error compares.\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) / 2\n\n\ndef mae(pred, y): return (pred - y).abs().mean()\n\n\nmae(ens_preds, valid_y)\n\n0.08611476519897165\n\n\nWhile smaller than my neural net’s error, this error is still significantly larger than my random forest."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#final-thoughts",
    "href": "posts/2023-09-14-rf/index.html#final-thoughts",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis was a really enjoyable exercise. I had a lot of fun working through the textbook’s process using a different dataset. There were many similarities (time-related variables were causing out-of-domain issues for the validation set) and differences (my neural net performed worse than my random forest, while in the chapter the results were flipped).\nI noticed as I was re-running the code in this notebook that there were slight differences each time I created my random forest. For example, sometimes parcelid was the 11th-most important feature, other times it was in the top 10. There were also different redundant features for each model run. Is this normal and expected? Or is there something about this data which makes the modeling process less consistent?\nI’ll end by summarizing my models’ results again, noting that my random forest beat the first place entry in the private leaderboard (which had an error of 0.074):\n\n\n\nModel\nMean Absolute Error\n\n\n\n\nRandom Forest\n0.072\n\n\nNeural Net\n0.117\n\n\nEnsemble of both\n0.086\n\n\n\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-1m-finetune/index.html",
    "href": "posts/2024-08-22-tinystories-1m-finetune/index.html",
    "title": "Fine-tuning TinyStories-1M on the financial_phrasebank Dataset",
    "section": "",
    "text": "In the previous notebooks I finetuned the TinyStories-33M, TinyStories-8M and TinyStories-3M models on the financial_phrasebank dataset and achieved the following results:\n\n\n\nArch\nFine-tuning Learning Rate\nBest Val Acc\nBest Test Acc\n\n\n\n\nTinyStories-33M\n5e-04\n86%\n79%\n\n\nTinyStories-8M\n8e-05\n85%\n86%\n\n\nTinyStories-8M\n5e-04\n79%\n86%\n\n\nTinyStories-3M\n8e-05\n78%\n74%\n\n\n\nIn this notebook, I’ll finetune the smallest TinyStories-1M model and see how it performs. I also suspect these models might perform better on a (synthetically generated) simpler version of this dataset, which I plan to explore in a future notebook.\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ trusted=‘true’}\n\nShow imports and setup\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, TrainerCallback\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nimport gc\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n#model_nm = \"roneneldan/TinyStories-33M\"\nmodel_nm = \"roneneldan/TinyStories-1M\"\n#model_nm = \"roneneldan/TinyStories-3M\"\n#model_nm = \"roneneldan/TinyStories-8M\"\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\ndef tok_func(x): return tokz(x[\"input\"], padding=True, truncation=True)\n\n:::"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-1m-finetune/index.html#preparing-datasets",
    "href": "posts/2024-08-22-tinystories-1m-finetune/index.html#preparing-datasets",
    "title": "Fine-tuning TinyStories-1M on the financial_phrasebank Dataset",
    "section": "Preparing Datasets",
    "text": "Preparing Datasets\nMuch of the code in this section is boilerplate, tokenizing the dataset and splitting it into training, validation and test sets.\n\n\nShow load_dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\",\n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\ndataset = dataset.rename_columns({'label':'labels', 'sentence': 'input'})\n\n\n\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ntokz.padding_side = \"left\" # https://github.com/huggingface/transformers/issues/16595 and https://www.kaggle.com/code/baekseungyun/gpt-2-with-huggingface-pytorch\ntok_ds = dataset.map(tok_func, batched=True)\n\n\ntok_ds[0]['input']\n\n'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'\n\n\n\ntok_ds[0]['input_ids'][100:110] # first 100 elements are 50257 ('[PAD]')\n\n[50257, 50257, 50257, 50257, 50257, 50257, 4821, 284, 17113, 837]\n\n\n\ntokz.decode(50257), tokz.decode(4821), tokz.decode(284), tokz.decode(17113)\n\n('[PAD]', 'According', ' to', ' Gran')\n\n\n\ntok_ds[0]['labels']\n\n1\n\n\n\nsplit_dataset = tok_ds.train_test_split(test_size=225/2264, seed=42)\n\ntraining_split = split_dataset['train'].train_test_split(test_size=0.2, seed=42)\n\ntrain_ds = training_split['train']\neval_ds = training_split['test']\ntest_ds = split_dataset['test']\n\ntrain_ds, eval_ds, test_ds\n\n(Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 1631\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 408\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 225\n }))\n\n\n\ntrain_ds[0]['input']\n\n'The result will also be burdened by increased fixed costs associated with operations in China , and restructuring costs in Japan .'\n\n\n\ntrain_ds[0]['labels']\n\n0\n\n\nThe dataset distributions show a predominance of neutral (1) sentences:\n\ntrain_ds.to_pandas()['labels'].value_counts() / len(train_ds)\n\nlabels\n1    0.622318\n2    0.251993\n0    0.125690\nName: count, dtype: float64\n\n\n\neval_ds.to_pandas()['labels'].value_counts() / len(eval_ds)\n\nlabels\n1    0.615196\n2    0.257353\n0    0.127451\nName: count, dtype: float64\n\n\n\ntest_ds.to_pandas()['labels'].value_counts() / len(test_ds)\n\nlabels\n1    0.555556\n2    0.240000\n0    0.204444\nName: count, dtype: float64"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-1m-finetune/index.html#prepare-for-training",
    "href": "posts/2024-08-22-tinystories-1m-finetune/index.html#prepare-for-training",
    "title": "Fine-tuning TinyStories-1M on the financial_phrasebank Dataset",
    "section": "Prepare for Training",
    "text": "Prepare for Training\nMuch of the code in this section is either helper functions (like get_acc, MetricCallback, or results_to_dataframe) or boilerplate code to prepare a HuggingFace trainer:\n\n\nShow get_acc function\ndef get_acc(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": (predictions == labels).astype(np.float32).mean().item()}\n\n\n\n\nShow MetricCallback function\n# thanks Claude\n\nclass MetricCallback(TrainerCallback):\n    def __init__(self):\n        self.metrics = []\n        self.current_epoch_metrics = {}\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            self.current_epoch_metrics.update(logs)\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        if hasattr(state, 'log_history') and state.log_history:\n            # Get the last logged learning rate\n            last_lr = state.log_history[-1].get('learning_rate', None)\n        else:\n            last_lr = None\n\n        self.metrics.append({\n            \"epoch\": state.epoch,\n            \"learning_rate\": last_lr,\n            **self.current_epoch_metrics\n        })\n        self.current_epoch_metrics = {}  # Reset for next epoch\n\n    def on_train_end(self, args, state, control, **kwargs):\n        # Capture final metrics after the last epoch\n        if self.current_epoch_metrics:\n            self.metrics.append({\n                \"epoch\": state.num_train_epochs,\n                \"learning_rate\": self.metrics[-1].get('learning_rate') if self.metrics else None,\n                **self.current_epoch_metrics\n            })\n\n\n\n\nShow results_to_dataframe function\ndef results_to_dataframe(results, model_name):\n    rows = []\n    for result in results:\n        initial_lr = result['learning_rate']\n        for metric in result['metrics']:\n            row = {\n                'model_name': model_name,\n                'initial_learning_rate': initial_lr,\n                'current_learning_rate': metric.get('learning_rate'),\n            }\n            row.update(metric)\n            rows.append(row)\n    \n    df = pd.DataFrame(rows)\n    \n    # Ensure specific columns are at the beginning\n    first_columns = ['model_name', 'initial_learning_rate', 'current_learning_rate', 'epoch']\n    other_columns = [col for col in df.columns if col not in first_columns]\n    df = df[first_columns + other_columns]\n    \n    return df\n\n\n\n\nShow make_cm function\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n    \n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['pred_text'], labels=['negative', 'neutral', 'positive'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive'])\n    \n    fig, ax = plt.subplots(figsize=(4,4))\n    disp.plot(ax=ax,text_kw={'fontsize': 12}, cmap='Blues', colorbar=False);\n    \n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    \n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=14)\n    ax.tick_params(axis='x', labelsize=14)\n\n\n\n\nShow get_prediction function\ndef get_prediction(model, text, tokz):\n    # Determine the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Move the model to the appropriate device\n    model = model.to(device)\n\n    # Tokenize the input text\n    inputs = tokz(text, return_tensors=\"pt\", truncation=True, padding=True)\n\n    # Move input tensors to the same device as the model\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Get the model's prediction\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Ensure logits are on CPU for numpy operations\n    logits = outputs.logits.detach().cpu()\n\n    # Get probabilities\n    probs = torch.softmax(logits, dim=-1)\n\n    # Get the predicted class\n    p_class = torch.argmax(probs, dim=-1).item()\n\n    # Get the probability for the predicted class\n    p = probs[0][p_class].item()\n\n    labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n    \n    print(f\"Probability: {p:.2f}\")\n    print(f\"Predicted label: {labels[p_class]}\")\n    return p_class, p\n\n\n\n\nShow get_trainer function\ndef get_trainer(lr, bs=16):\n\n    args = TrainingArguments(\n        'outputs',\n        learning_rate=lr,\n        warmup_ratio=0.1,\n        lr_scheduler_type='cosine',\n        fp16=True,\n        eval_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        per_device_train_batch_size=bs,\n        per_device_eval_batch_size=bs*2,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        report_to='none')\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=3) # 3 labels for 3 classes\n    model.resize_token_embeddings(len(tokz))\n    model.config.pad_token_id = model.config.eos_token_id\n    \n    trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=eval_ds, \n                  tokenizer=tokz, compute_metrics=get_acc, callbacks=[metric_callback])\n    \n    return trainer, args\n\n\n\n\nShow get_test_df function\ndef get_test_df(trainer):\n    test_df = test_ds.to_pandas()[['input', 'labels']]\n    \n    preds = trainer.predict(test_ds).predictions.astype(float)\n    probs = F.softmax(torch.tensor(preds), dim=1)\n    predicted_classes = torch.argmax(probs, dim=1).numpy()\n\n    test_df['predicted'] = predicted_classes\n    \n    test_df['match'] = test_df['labels'] == test_df['predicted']\n    acc = test_df['match'].mean()\n    \n    label_map = {i: label_text for i, label_text in enumerate(test_ds.features[\"labels\"].names)}\n    test_df['label_text'] = test_df['labels'].apply(lambda x: label_map[x])\n    test_df['pred_text'] = test_df['predicted'].apply(lambda x: label_map[x])\n    \n    return test_df, acc"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-1m-finetune/index.html#training-learning-rate-sweep",
    "href": "posts/2024-08-22-tinystories-1m-finetune/index.html#training-learning-rate-sweep",
    "title": "Fine-tuning TinyStories-1M on the financial_phrasebank Dataset",
    "section": "Training: Learning Rate Sweep",
    "text": "Training: Learning Rate Sweep\nWhile there are other hyperparameters to tune (epochs, warmup_ratio, weight_decay) I’ll focus this notebook on fine-tuning with different learning rates. I’ll start with the same learning rates that I used for the 33M, 8M and 3M models:\n\n\nShow training loop\nmetrics = []\ntrainers = []\nlearning_rates = [1e-6, 1e-5, 3e-5, 5e-5, 8e-5, 1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 1e-2, 1e-1]\n\nfor lr in learning_rates:\n    print(f\"Learning Rate: {lr}\")\n    \n    metric_callback = MetricCallback()\n    \n    trainer, args = get_trainer(lr, bs=64)\n\n    trainer.train()\n\n    metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nmetrics_df = results_to_dataframe(metrics, model_name=model_nm)\nmetrics_df = metrics_df.query('current_learning_rate.notna()')"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-1m-finetune/index.html#results",
    "href": "posts/2024-08-22-tinystories-1m-finetune/index.html#results",
    "title": "Fine-tuning TinyStories-1M on the financial_phrasebank Dataset",
    "section": "Results",
    "text": "Results\n\nHighest Validation Set Accuracy\nThe highest validation set accuracy (75%) was obtained with two learning rates: 0.0001 and 0.0003. Both are an order of magnitude larger than the best performing learning rates for the 33M, 8M and 3M models.\n\nmetrics_df.query('eval_accuracy == eval_accuracy.max()')\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      23\n      roneneldan/TinyStories-1M\n      0.0001\n      0.0\n      3.0\n      0.0\n      0.5583\n      814660.5625\n      0.595688\n      0.747549\n      0.2664\n      1531.612\n      7.508\n      7.0189\n      697.118\n      5.556\n      1.533190e+12\n      0.723877\n    \n    \n      27\n      roneneldan/TinyStories-1M\n      0.0003\n      0.0\n      3.0\n      0.0\n      0.5974\n      330743.3750\n      0.627727\n      0.747549\n      0.2700\n      1510.882\n      7.406\n      6.9875\n      700.250\n      5.581\n      1.533190e+12\n      0.748295\n    \n  \n\n\n\n\n\nlearning_rates[5], learning_rates[6]\n\n(0.0001, 0.0003)\n\n\nAn LR of 0.0001 has a slightly higher test set accuracy (65%) than 0.0003 (64%).\n\ntest_df, acc = get_test_df(trainers[5])\nacc\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n\n0.6488888888888888\n\n\n\ntest_df, acc = get_test_df(trainers[6])\nacc\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n\n0.64\n\n\nThis 8M parameter finetuned model predicts neutral sentences the best (112/125) followed by negative sentences (36/46) and lastly, positive sentences (31/54). This bucks the trend of the other three models (neutral > positive > negative, which followed the proportion of each sentiment in the dataset).\n\ntest_df, acc = get_test_df(trainers[5])\nmake_cm(test_df)\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n\n\n\n\n\n\n\n\nAs the learning rate increases (starting at 1e-6) the validation set accuracy increases until it reaches a bit of plateau at 10-4 before coming down.\n\n\nShow plotting code\nfinal_epoch_metrics = metrics_df.query(\"epoch == 3\")\nplt.scatter(final_epoch_metrics['initial_learning_rate'], final_epoch_metrics['eval_accuracy']);\nplt.xscale('log')\nplt.xlabel('Learning Rate (log scale)')\nplt.ylabel('Validation Set Accuracy')\nplt.title('Learning Rate vs. Final Epoch Validation Accuracy');\n\n\n\n\n\n\n\n\n\nI’ll test the model (run a “sanity check”) on three made-up sentences. I don’t want to weigh these results too much as they are cherry-picked sentences, but this model only gets one of them right (neutral).\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(trainers[5].model, text, tokz)\n\nProbability: 0.50\nPredicted label: negative\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(trainers[5].model, text, tokz)\n\nProbability: 0.51\nPredicted label: positive\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(trainers[5].model, text, tokz)\n\nProbability: 0.50\nPredicted label: neutral\n\n\n\n\nHighest Test Set Accuracy\n\ntest_dfs = []\naccs = []\nfor t in trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\nThe learning rate with the highest test set accuracy (68%) is 0.001. This is by far the largest best-performing learning rate across the 33M, 8M, 3M and now 1M parameter TinyStories models.\n\naccs\n\n[0.2311111111111111,\n 0.6,\n 0.6444444444444445,\n 0.6711111111111111,\n 0.6622222222222223,\n 0.6488888888888888,\n 0.64,\n 0.6444444444444445,\n 0.6355555555555555,\n 0.6755555555555556,\n 0.5555555555555556,\n 0.5555555555555556]\n\n\n\naccs[9], learning_rates[9]\n\n(0.6755555555555556, 0.001)\n\n\nThis learning rate had a validation set accuracy of about 74%.\n\nfinal_epoch_metrics.query(\"initial_learning_rate == 0.001\")\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      39\n      roneneldan/TinyStories-1M\n      0.001\n      0.0\n      3.0\n      0.0\n      0.5895\n      191542.265625\n      0.622454\n      0.735294\n      0.2941\n      1387.377\n      6.801\n      7.0237\n      696.642\n      5.553\n      1.533190e+12\n      0.761687\n    \n  \n\n\n\n\nThis model gets 181/125 neutral predictions correct, followed by 33/46 negative predictions and 33/54 positive predictions, continuing the trend for 1M models that deviates from the previous three sizes.\n\naccs[9], learning_rates[9], make_cm(test_dfs[9])\n\n(0.6755555555555556, 0.001, None)\n\n\n\n\n\n\n\n\n\nThis model gets 2/3 of the sanity check sentiments correct.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(trainers[9].model, text, tokz)\n\nProbability: 0.42\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(trainers[9].model, text, tokz)\n\nProbability: 0.51\nPredicted label: positive\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(trainers[9].model, text, tokz)\n\nProbability: 0.96\nPredicted label: neutral"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-1m-finetune/index.html#training-with-the-best-learning-rates-10-times",
    "href": "posts/2024-08-22-tinystories-1m-finetune/index.html#training-with-the-best-learning-rates-10-times",
    "title": "Fine-tuning TinyStories-1M on the financial_phrasebank Dataset",
    "section": "Training with the Best Learning Rates 10 Times",
    "text": "Training with the Best Learning Rates 10 Times\nSince I have different models achieving the highest validation set accuracy and the highest test set accuracy, I’ll train 10 models for each learning rate to see if the results are consistent.\n\nLR = 0.0001 (Highest Validation Set Accuracy)\n\nlearning_rates[5]\n\n0.0001\n\n\nTo prevent (all but the first) models from getting the same loss and accuracy per epoch, I’ll reset the random seed each iteration.\n\n\nShow set_seed function\nimport random\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\n\n\nShow training loop\nbest_metrics = []\nbest_trainers = []\nlr = learning_rates[5]\n\nfor i in range(10):\n    set_seed(42 + i)  # Use a different seed for each run\n    metric_callback = MetricCallback()\n    trainer, args = get_trainer(lr=lr, bs=64)\n    trainer.train()\n\n    best_metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    best_trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nbest_metrics_df = results_to_dataframe(best_metrics, model_name=model_nm)\nbest_metrics_df = best_metrics_df.query('current_learning_rate.notna()')\nbest_metrics_df.head(3)\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      1\n      roneneldan/TinyStories-1M\n      0.0001\n      0.000085\n      1.0\n      0.000085\n      0.9066\n      392222.90625\n      0.815697\n      0.632353\n      0.2688\n      1518.079\n      7.442\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      roneneldan/TinyStories-1M\n      0.0001\n      0.000030\n      2.0\n      0.000030\n      0.7411\n      875535.87500\n      0.699083\n      0.710784\n      0.2755\n      1481.051\n      7.260\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      roneneldan/TinyStories-1M\n      0.0001\n      0.000000\n      3.0\n      0.000000\n      0.6591\n      501946.84375\n      0.690590\n      0.708333\n      0.2775\n      1470.421\n      7.208\n      7.022\n      696.805\n      5.554\n      1.533190e+12\n      0.768915\n    \n  \n\n\n\n\nThere’s a difference of about 5% between the minimum and maximum validation set accuracy for this model.\n\nfinal_accs = best_metrics_df.query(\"epoch == 3\")['eval_accuracy']\nfinal_accs.describe()\n\ncount    10.000000\nmean      0.723039\nstd       0.016703\nmin       0.698529\n25%       0.711397\n50%       0.725490\n75%       0.734681\nmax       0.745098\nName: eval_accuracy, dtype: float64\n\n\n\ntest_dfs = []\naccs = []\nfor t in best_trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\nThere’s a difference of about 5% between the min and max test set accuracy as well.\n\naccs = pd.Series(accs)\naccs.describe()\n\ncount    10.000000\nmean      0.664000\nstd       0.018995\nmin       0.631111\n25%       0.648889\n50%       0.666667\n75%       0.678889\nmax       0.688889\ndtype: float64\n\n\n\naccs\n\n0    0.631111\n1    0.671111\n2    0.648889\n3    0.684444\n4    0.648889\n5    0.648889\n6    0.680000\n7    0.688889\n8    0.675556\n9    0.662222\ndtype: float64\n\n\nThe best performing model (for test set accuracy, 69%) gets 2/3 of my sanity check sentiments correct.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(best_trainers[7].model, text, tokz)\n\nProbability: 0.55\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(best_trainers[7].model, text, tokz)\n\nProbability: 0.56\nPredicted label: positive\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(best_trainers[7].model, text, tokz)\n\nProbability: 0.60\nPredicted label: neutral\n\n\n\n\nLR = 0.001 (Highest Test Set Accuracy)\n\nlearning_rates[9] == 0.001\n\nTrue\n\n\n\n\nShow training loop\nbest_metrics2 = []\nbest_trainers2 = []\nlr = learning_rates[9]\n\nfor i in range(10):\n    set_seed(42 + i)  # Use a different seed for each run\n    metric_callback = MetricCallback()\n    trainer, args = get_trainer(lr=lr, bs=64)\n    trainer.train()\n\n    best_metrics2.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    best_trainers2.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nbest_metrics_df2 = results_to_dataframe(best_metrics2, model_name=model_nm)\nbest_metrics_df2 = best_metrics_df2.query('current_learning_rate.notna()')\nbest_metrics_df2.head(3)\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      1\n      roneneldan/TinyStories-1M\n      0.001\n      0.000846\n      1.0\n      0.000846\n      0.9313\n      470919.59375\n      0.809184\n      0.627451\n      0.2713\n      1504.052\n      7.373\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      roneneldan/TinyStories-1M\n      0.001\n      0.000303\n      2.0\n      0.000303\n      0.7477\n      462428.84375\n      0.732228\n      0.681373\n      0.2745\n      1486.436\n      7.286\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      roneneldan/TinyStories-1M\n      0.001\n      0.000000\n      3.0\n      0.000000\n      0.6664\n      186014.03125\n      0.699403\n      0.708333\n      0.2889\n      1412.300\n      6.923\n      7.1325\n      686.011\n      5.468\n      1.533190e+12\n      0.781793\n    \n  \n\n\n\n\nThe maximum validation set accuracy for this learning rate is about 74%.\n\nfinal_accs2 = best_metrics_df2.query(\"epoch == 3\")['eval_accuracy']\nfinal_accs2.describe()\n\ncount    10.000000\nmean      0.714706\nstd       0.018129\nmin       0.678922\n25%       0.710172\n50%       0.719363\n75%       0.726716\nmax       0.735294\nName: eval_accuracy, dtype: float64\n\n\n\ntest_dfs2 = []\naccs2 = []\nfor t in best_trainers2:\n    test_df, acc = get_test_df(t)\n    test_dfs2.append(test_df)\n    accs2.append(acc)\n\nThe largest test set accuracy was 68%.\n\naccs2 = pd.Series(accs2)\naccs2.describe()\n\ncount    10.000000\nmean      0.651556\nstd       0.021284\nmin       0.613333\n25%       0.640000\n50%       0.651111\n75%       0.672222\nmax       0.675556\ndtype: float64\n\n\n\naccs2\n\n0    0.662222\n1    0.675556\n2    0.640000\n3    0.675556\n4    0.613333\n5    0.640000\n6    0.631111\n7    0.657778\n8    0.675556\n9    0.644444\ndtype: float64\n\n\nThe 8th model (both the 3rd and 8th model have a test set accuracy of 68%) goes 2/3 in my sanity checks.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(best_trainers2[8].model, text, tokz)\n\nProbability: 0.57\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(best_trainers2[8].model, text, tokz)\n\nProbability: 0.56\nPredicted label: positive\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(best_trainers2[8].model, text, tokz)\n\nProbability: 0.72\nPredicted label: neutral"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-1m-finetune/index.html#final-thoughts",
    "href": "posts/2024-08-22-tinystories-1m-finetune/index.html#final-thoughts",
    "title": "Fine-tuning TinyStories-1M on the financial_phrasebank Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis notebook closes out my initial quick-and-dirty model fine-tuning experiments for the TinyStories family (33M, 8M, 3M, 1M) on the financial_phrasebank dataset. Here is a summary of my results:\n\n\n\nBase Model\nFine-tuning Learning Rate\nBest Val Acc\nBest Test Acc\n\n\n\n\nTinyStories-33M\n5e-04\n86%\n79%\n\n\nTinyStories-8M\n8e-05\n85%\n86%\n\n\nTinyStories-8M\n5e-04\n79%\n86%\n\n\nTinyStories-3M\n8e-05\n78%\n74%\n\n\nTinyStories-1M\n1e-04\n75%\n69%\n\n\nTinyStories-1M\n1e-03\n74%\n68%\n\n\n\nThree main takeaways:\n\nSet the random seed for each iteration to avoid getting the same accuracy and loss values in a for-loop\nThe 8M model had a 7% higher test set accuracy than the 33M model.\nThe best performing learning rates for the smallest model, 1M, were 1-2 orders of magnitude smaller than the best performing learning rates for the 33M, 8M and 3M models.\n\nFuture work:\n\nRefactor this code to avoid changing so many variables.\nDo a more thorough hyperparameter sweep (random seeds, epochs, warmup, weight decay, learning rates) for each model.\nFine-tune the models on a synthetically generated version of financial_phrasebank that’s at a lower reading level to see if it improves performance.\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "href": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "title": "Making Chicken Wings",
    "section": "",
    "text": "Background\nRestaurant-bought wings are pricey. Even fast-food bought wings, which can be miserably made some times—I once bought advertised Honey BBQ Chicken Wings which were great the first time, but the second time looked like plain wings with barbeque sauce drizzled over them–were about $20 for 16 wings. I don’t mind the price, gratefully, but I do mind consistent quality and want to avoide letdowns, so I chose to learn how to make wings. I also need to fill the gap caused by the lack of NFL and College Football during the offseason, so this is a tasty project which serves that purpose as well.\n\n\nFirst Attempt\nI made my first batch of chicken wings on Saturday, February 11, 2023. They were edible and the sauce was delicious. But they were at most a 5/10. I follow some recipe I found online after a google search which went something like:\n\nPreheat oven to 400°F\nBaste oil on both sides of the wings and season with salt and pepper\nBake for 20 minutes, flipping the wings after 10 minutes\nHeat the oven to 425°F\nBaste on the sauce and bake for 7 minutes then flip. Repeat a few times.\nBroil (500°F) for 5-10 minutes.\n\nMy sauce was made of Ray’s sugar-free BBQ sauce, mustard, soy sauce, and honey.\nI did not have a basting brush and was using a spoon to lather on the sauce. It did not work well. Also, the meat did not fall off the bones and it took more effort than worthwhile to eat them.\n\n\nSecond Attempt\nI was hungrier this time I suppose because I chose to make honey bbq drumsticks instead of wings. This time I got a silicone basting brush at Safeway. I modified my approach slightly:\n\nPreheat oven to 400°F\nActually baste this time and season the drumsticks\nBake on one side for 15 minutes, flip and bake for another 15 minutes\nRemove the drumsticks and heat the oven to 425°F\nRepeat four times, twice on each side: baste on sauce, bake for 7 minutes, remove and flip.\nAdd coconut flour to the sauce (note to self: use rice flour instead)\nRepeat two times, once on each side: baste on sauce/flour mix (gravy?), bake for 7 minutes and flip.\nBroil (500°F) for 5 minutes.\n\nUsing the basting brush allowed for a more even distribution of sauce on the drumsticks. The skin still wasn’t crispy enough, although the meat was (more) easily coming off the bones and was juicy + delicious.\nWhen I make it again, probably next weekend, I’ll switch back to chicken wings and find a new recipe which emphasizes the cripsiness of the wings.\n\n\n\nclose-up of my honey bbq chicken drumsticks with a few charred spots."
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html",
    "href": "posts/2024-08-26-is-it-a-digit/index.html",
    "title": "Is it a Digit?",
    "section": "",
    "text": "In the second half of the fastai Lesson 9 video, Jeremy introduces the idea of a “magic API” that predicts the probability an image is a handwritten digit—where clearer images result in higher probabilities and noisier ones yield lower scores.\n\n\n\nA magic API which predicts the probability that the input image is a handwritten digit\n\n\nInspired by this, I set out to build a neural network that predicts the probability an image is a handwritten digit. But I wanted to go a step further and bring abstract ideas into something tangible, so I designed a HuggingFace Space where users can interactively explore how this probability shifts as they draw. The Space connects to an inference endpoint hosting the model trained in this notebook. As you sketch on the canvas, the app dynamically sends the image to the endpoint, processes it through the model, and displays the predicted probability beneath the drawing.\nThe frontend for this project came together thanks to Claude-3.5 Sonnet, which generated most of the code. The initial proof-of-concept is available on this Codepen.\n\nimport matplotlib.pyplot as plt\nfrom fastai.vision.all import *\nimport numpy as np\nimport requests\nfrom io import BytesIO"
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#generate-synthetic-data",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#generate-synthetic-data",
    "title": "Is it a Digit?",
    "section": "Generate Synthetic Data",
    "text": "Generate Synthetic Data\nThe MNIST dataset contains images of handwritten digits. This alone is not sufficient for my training objective. I’ll add varying levels of noise to the MNIST images—these will be my inputs. The outputs will be 1 minus the amount of noise (which I’ll limit to the range 0.0 to 1.0).\nI want images where there is a lot of noise (black pixels) in addition to the black-pixel digit (on a white background). I also want images where some of the black pixels from the digit are removed. Both of these are approximately the kinds of images seen by my model in my Is it a digit? app.\n\nLoading Existing MNIST Data\nI’ll start by loading the original MNIST dataset and preparing the training and validation set tensors.\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2024-08-21T23:32:46.055506Z”,“iopub.status.busy”:“2024-08-21T23:32:46.055033Z”,“iopub.status.idle”:“2024-08-21T23:33:08.283155Z”,“shell.execute_reply”:“2024-08-21T23:33:08.281943Z”,“shell.execute_reply.started”:“2024-08-21T23:32:46.055473Z”}’ trusted=‘true’ execution_count=2}\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02<00:00]\n    \n    \n\n:::\n\npath.ls()\n\n(#2) [Path('/root/.fastai/data/mnist_png/testing'),Path('/root/.fastai/data/mnist_png/training')]\n\n\n\ntraining_files = [file for path in (path/'training').ls().sorted() for file in path.ls()]\nvalidation_files = [file for path in (path/'testing').ls().sorted() for file in path.ls()]\n\n\nlen(training_files), len(validation_files)\n\n(60000, 10000)\n\n\n\ntrain_x = torch.stack([tensor(Image.open(o)) for o in training_files]).float()/255\ntrain_x.shape\n\ntorch.Size([60000, 28, 28])\n\n\nI want black pixels to represent the digit and white pixels to be the background so I’ll invert the data.\n\ntrain_x = 1 - train_x\n\n\nvalid_x = torch.stack([tensor(Image.open(o)) for o in validation_files]).float()/255\nvalid_x.shape\n\ntorch.Size([10000, 28, 28])\n\n\n\nvalid_x = 1 - valid_x\n\n\n# this should be a zero\nshow_image(train_x[0], cmap='gray');\n\n\n\n\n\n\n\n\n\n# this should be a nine\nshow_image(valid_x[-1], cmap='gray');\n\n\n\n\n\n\n\n\n\n\nAdding and Subtracting Varying Levels of Noise\nI’ll start by showing an example of Gaussian (Normal) noise.\n\nnoise_threshold = 0.5\n\n# Generate a binary noise image (1 for white, 0 for black)\nnoise = (torch.rand(28, 28) >= noise_threshold).float()\n\nplt.imshow(noise, cmap='gray')\nplt.title(\"28x28 Binary Noise Image\")\nplt.axis('off') \nplt.show()\n\n\n\n\n\n\n\n\nNext I’ll add some noise to a handwritten digit and print out the “probability” (1 - noise_threshold) that it’s a digit:\n\nnoise_threshold = 0.5\n\n# Generate a binary noise image (1 for white, 0 for black)\nnoise = (torch.rand(28, 28) >= noise_threshold).float()\n\n# Combine the inverted digit with noise using element-wise minimum\nnoisy_image = torch.min(train_x[0], noise)\n\n# Display the result\nplt.imshow(noisy_image, cmap='gray')\nplt.title(f\"p = {1 - noise_threshold} that it's a digit\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nnoise_threshold = 0.1\n\n# Generate a binary noise image (1 for white, 0 for black)\nnoise = (torch.rand(28, 28) >= noise_threshold).float()\n\n# Combine the inverted digit with noise using element-wise minimum\nnoisy_image = torch.min(train_x[0], noise)\n\n# Display the result\nplt.imshow(noisy_image, cmap='gray')\nplt.title(f\"p = {1 - noise_threshold} that it's a digit\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nnoise_threshold = torch.rand(1).abs()\nnoise = (torch.rand(28, 28) >= noise_threshold).float() \n\n# Combine the inverted digit with noise using element-wise minimum\nnoisy_image = torch.min(train_x[0], noise)\n\n# Display the result\nplt.imshow(noisy_image, cmap='gray')\nplt.title(f\"p = {1 - noise_threshold.item():.2f} that it's a digit\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nI’ll also generate images where pixels of the digit are taken away.\n\nnoise_threshold = torch.rand(1).abs()\nnoise = (torch.rand(28, 28) >= noise_threshold).float() \n\n# Combine the inverted digit with noise using element-wise minimum\nnoisy_image = torch.max(train_x[0], noise)\n\n# Display the result\nplt.imshow(noisy_image, cmap='gray')\nplt.title(f\"p = {noise_threshold.item():.2f} that it's a digit\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nnoise_threshold = torch.rand(1).abs()\nnoise = (torch.rand(28, 28) >= noise_threshold).float() \n\n# Combine the inverted digit with noise using element-wise minimum\nnoisy_image = torch.max(train_x[0], noise)\n\n# Display the result\nplt.imshow(noisy_image, cmap='gray')\nplt.title(f\"p = {noise_threshold.item():.2f} that it's a digit\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nnoise_threshold = torch.rand(1).abs()\nnoise = (torch.rand(28, 28) >= noise_threshold).float() \n\n# Combine the inverted digit with noise using element-wise minimum\nnoisy_image = torch.max(train_x[0], noise)\n\n# Display the result\nplt.imshow(noisy_image, cmap='gray')\nplt.title(f\"p = {noise_threshold.item():.2f} that it's a digit\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nI’ll create 140,000 different noise images. I’ll take 70k of those and “add” them to the 60k MNIST training image and 10k validation images and store 1 - noise_threshold as the target. For the other 70k, I’ll “subtract” them from the 60k training digits 10k validation digits and store noise_threshold as the target. In total, I’ll have 120k training and 20k validation images that I can use to train the full spectrum of noise (additive and subtractive).\n\nnoise_imgs = []\nnoise_thresholds = []\n\nfor i in range(140_000):\n    noise_threshold = torch.rand(1).abs()\n    noise_imgs.append((torch.rand(28, 28) >= noise_threshold).float())\n    noise_thresholds.append(noise_threshold)\n\n\nnoise_t = torch.stack(noise_imgs)\nthresh_t = tensor(noise_thresholds)\n\nLet’s make sure the noise and thresholds look okay:\n\nnoise = noise_t[0]\nthresh = thresh_t[0]\n\n# Combine the inverted digit with noise using element-wise minimum\nnoisy_image = torch.min(train_x[0], noise)\n\n# Display the result\nplt.imshow(noisy_image, cmap='gray')\nplt.title(f\"p = {1-thresh.item():.2f} that it's a digit\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nNow, I’ll create my new training and validation sets:\n\nnoise_t.shape, train_x.shape, valid_x.shape\n\n(torch.Size([140000, 28, 28]),\n torch.Size([60000, 28, 28]),\n torch.Size([10000, 28, 28]))\n\n\n\nnoise_train_min = noise_t[:60000]\nnoise_valid_min = noise_t[60000:70000]\n\nnoise_train_max = noise_t[70000:130000]\nnoise_valid_max = noise_t[130000:]\n\n# additive noise\nresult_train_min = torch.min(train_x, noise_train_min)\nresult_valid_min = torch.min(valid_x, noise_valid_min)\n\n# subtractive noise\nresult_train_max = torch.max(train_x, noise_train_max)\nresult_valid_max = torch.max(valid_x, noise_valid_max)\n\n# concatenate to create training and validation sets\nnoise_valid = torch.cat([result_valid_min, result_valid_max])\nnoise_train = torch.cat([result_train_min, result_train_max])\n\n\n# additive thresholds\nthresh_train_min = 1 - thresh_t[:60000]\nthresh_valid_min = 1 - thresh_t[60000:70000]\n\n# subtractive thresholds\nthresh_train_max = thresh_t[70000:130000]\nthresh_valid_max = thresh_t[130000:]\n\n# concatenate to get training and validation targets\nthresh_valid = torch.cat([thresh_valid_min, thresh_valid_max])\nthresh_train = torch.cat([thresh_train_min, thresh_train_max])\n\nFinally, I’ll spot-check a few training and validation set inputs and targets:\n\ndef plot_noise(img, thresh):\n    plt.imshow(img, cmap='gray')\n    plt.title(f\"p = {thresh.item():.2f} that it's a digit\")\n    plt.axis('off')\n    plt.show()\n\n\nplot_noise(noise_train[0], thresh_train[0])\n\n\n\n\n\n\n\n\n\nplot_noise(noise_train[-1], thresh_train[-1])\n\n\n\n\n\n\n\n\n\nplot_noise(noise_valid[10], thresh_valid[10])\n\n\n\n\n\n\n\n\n\nplot_noise(noise_valid[-10], thresh_valid[-10])"
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#training-an-image-regressor",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#training-an-image-regressor",
    "title": "Is it a Digit?",
    "section": "Training an Image Regressor",
    "text": "Training an Image Regressor\nNow that I have my noisy inputs (and corresponding “probability” targets) I can start training models! I’ll start by training a ResNet34 to get a sense of the baseline MSE. I got a major assist from both Claude and ChatGPT to build the appropriate DataBlock (and getters get_items, get_x and get_y) based on how my data is sources (4 NumPy arrays).\n\npath = Path('/kaggle/input/noisy-mnist')\ntrain_noise = np.load(path/'noise_train.npy')\ntrain_thresh = np.load(path/'thresh_train.npy')\nvalid_noise = np.load(path/'noise_valid.npy')\nvalid_thresh = np.load(path/'thresh_valid.npy')\n\n# Combine train and valid data\nall_noise = np.concatenate([train_noise, valid_noise])\nall_thresh = np.concatenate([train_thresh, valid_thresh])\n\n\nall_noise = (all_noise * 255).astype(np.uint8)\n\n\nplot_noise(all_noise[0], all_thresh[0])\n\n\n\n\n\n\n\n\n\ndef get_x(i):\n    # Convert NumPy array to a single-channel PIL image with inverted colors\n    return PILImageBW.create(all_noise[i])\n\ndef get_y(i):\n    return all_thresh[i].astype(np.float32)\n\ndef get_items(_):\n    return range(len(all_noise))\n\n\n# Create valid_idx for IndexSplitter\nvalid_idx = list(range(len(train_noise), len(all_noise)))\nlen(valid_idx)\n\n20000\n\n\n\ndblock = DataBlock(\n    blocks=(ImageBlock(PILImageBW), RegressionBlock),\n    get_items=get_items,\n    get_x=get_x,\n    get_y=get_y,\n    splitter=IndexSplitter(valid_idx),\n    batch_tfms=None\n)\n\n\ndls = dblock.dataloaders(all_noise, bs=1024)\n\n\ndblock.summary(all_noise)\n\nSetting-up type transforms pipelines\nCollecting items from [[[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [  0 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255   0 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[  0   0 255 ...   0   0   0]\n  [255   0 255 ... 255   0   0]\n  [255   0   0 ... 255   0 255]\n  ...\n  [255   0   0 ...   0   0   0]\n  [255   0   0 ... 255   0   0]\n  [  0   0   0 ...   0   0 255]]\n\n [[  0   0   0 ...   0   0   0]\n  [255   0 255 ... 255   0   0]\n  [  0   0 255 ...   0   0   0]\n  ...\n  [  0   0 255 ... 255   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ... 255   0   0]]\n\n ...\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]]\nFound 140000 items\n2 datasets of sizes 120000,20000\nSetting up Pipeline: get_x -> PILBase.create\nSetting up Pipeline: get_y -> RegressionSetup -- {'c': None}\n\nBuilding one sample\n  Pipeline: get_x -> PILBase.create\n    starting from\n      0\n    applying get_x gives\n      PILImageBW mode=L size=28x28\n    applying PILBase.create gives\n      PILImageBW mode=L size=28x28\n  Pipeline: get_y -> RegressionSetup -- {'c': None}\n    starting from\n      0\n    applying get_y gives\n      0.95005137\n    applying RegressionSetup -- {'c': None} gives\n      tensor(0.9501)\n\nFinal sample: (PILImageBW mode=L size=28x28, tensor(0.9501))\n\n\nCollecting items from [[[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [  0 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255   0 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[  0   0 255 ...   0   0   0]\n  [255   0 255 ... 255   0   0]\n  [255   0   0 ... 255   0 255]\n  ...\n  [255   0   0 ...   0   0   0]\n  [255   0   0 ... 255   0   0]\n  [  0   0   0 ...   0   0 255]]\n\n [[  0   0   0 ...   0   0   0]\n  [255   0 255 ... 255   0   0]\n  [  0   0 255 ...   0   0   0]\n  ...\n  [  0   0 255 ... 255   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ... 255   0   0]]\n\n ...\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]]\nFound 140000 items\n2 datasets of sizes 120000,20000\nSetting up Pipeline: get_x -> PILBase.create\nSetting up Pipeline: get_y -> RegressionSetup -- {'c': None}\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImageBW mode=L size=28x28, tensor(0.9501))\n    applying ToTensor gives\n      (TensorImageBW of size 1x28x28, tensor(0.9501))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImageBW of size 4x1x28x28, tensor([0.9501, 0.2629, 0.1410, 0.6715], device='cuda:0'))\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImageBW of size 4x1x28x28, tensor([0.9501, 0.2629, 0.1410, 0.6715], device='cuda:0'))\n\n\nI’m not sure why dls.show_batch is showing white digits on black backgrounds but I’m guessing it has to do with the colormap it’s using.\n\ndls.show_batch()\n\n\n\n\n\n\n\n\nWhen I plot one of the batch items using plt.imshow with cmap='gray' I get the expected displayed result:\n\nxb, yb = dls.one_batch()\nplot_noise(xb[0][0].cpu(), yb[0])\n\n\n\n\n\n\n\n\n\nplt.imshow(all_noise[0], cmap='gray');\n\n\n\n\n\n\n\n\n\nplt.imshow(xb[0][0].cpu(), cmap='gray');\n\n\n\n\n\n\n\n\nGreat! Now onto training:\n\nlearn = vision_learner(dls, resnet34, loss_func=mse, metrics=mse, n_in=1, n_out=1)\nlearn.fine_tune(1, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      mse\n      time\n    \n  \n  \n    \n      0\n      0.395260\n      0.016736\n      0.016736\n      01:08\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      mse\n      time\n    \n  \n  \n    \n      0\n      0.018071\n      0.007796\n      0.007796\n      01:17\n    \n  \n\n\n\nGiven that the predictions are between 0.0 and 1.0, the mean squared error of 0.008 is not bad! I’ll train for a few more epochs:\n\nlearn = vision_learner(dls, resnet34, loss_func=mse, metrics=mse, n_in=1, n_out=1)\nlearn.fine_tune(5, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      mse\n      time\n    \n  \n  \n    \n      0\n      0.451190\n      0.015501\n      0.015501\n      01:09\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      mse\n      time\n    \n  \n  \n    \n      0\n      0.018069\n      0.010948\n      0.010948\n      01:16\n    \n    \n      1\n      0.011859\n      0.005137\n      0.005137\n      01:17\n    \n    \n      2\n      0.007378\n      0.003288\n      0.003288\n      01:17\n    \n    \n      3\n      0.005160\n      0.002453\n      0.002453\n      01:17\n    \n    \n      4\n      0.004174\n      0.001957\n      0.001957\n      01:17\n    \n  \n\n\n\n\nlearn.export('model.pkl')"
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#inference",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#inference",
    "title": "Is it a Digit?",
    "section": "Inference",
    "text": "Inference\nGlancing at the predictions on the validation set, the model is quite good at predicting the “probability” that an image is a handwritten digit!\n\npreds, targs = learn.get_preds(dl=dls.valid)\n\n\n\n\n\n\n\n\n\npreds[:10], targs[:10]\n\n(tensor([[0.3102],\n         [0.3155],\n         [0.3329],\n         [0.8646],\n         [0.1457],\n         [0.9295],\n         [0.5802],\n         [0.0132],\n         [0.3224],\n         [0.7590]]),\n tensor([0.3085, 0.2647, 0.3427, 0.8443, 0.1768, 0.9317, 0.5976, 0.0090, 0.3992,\n         0.7359]))\n\n\nI’ll make sure that I can get predictions on a single image tensor. The first time I trained a model, I got an error in learn.predict that the model expected 3 channels. This prompted me to set n_in=1 when defining the vision_learner.\n\nprint(f\"{learn.predict(all_noise[0])[0][0]:.2f}\")\n\n\n\n\n\n\n\n\n0.94\n\n\nNext, I’ll test out the model on images it hasn’t seen before.\n\ndef get_image_array(url):\n    # Download the image\n    response = requests.get(url)\n\n    # Convert the content to an image\n    image = Image.open(BytesIO(response.content))\n\n    image = image.convert(\"L\")\n\n    # Resize the image to 28x28\n    image = image.resize((28, 28))\n\n    # Convert the image to a numpy array\n    image_array = np.array(image)\n\n    # Normalize the pixel values to [0, 1] and convert to a PyTorch tensor\n    #image_tensor = torch.tensor(image_array, dtype=torch.float32) / 255.0\n    \n    return image_array\n\n\ndef predict_image(url):\n    image_array = get_image_array(url)\n    print(f\"Probability it's a digit: {learn.predict(image_array)[0][0]:.2f}\")\n    plt.imshow(image_array, cmap='gray');\n\nThe model correctly predicts a very high probability that this is an image of a digit (97%):\n\nurl = 'https://drive.google.com/uc?export=view&id=1qNNgt4z-PPn8JKlQU5pFiRE1KCFnqb2R'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.97\n\n\n\n\n\n\n\n\n\nNext I’ll use the model to predict the handwritten digit probability for an image that is clearly NOT a digit. The model correctly predicts a low probability:\n\nurl = 'https://drive.google.com/uc?export=view&id=1LjJEMldL2CaWpYxRqv0Byrg0GqdOWGOO'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.08\n\n\n\n\n\n\n\n\n\nFor another obviously not a digit, the model correctly predicts a very low probability:\n\nurl = 'https://drive.google.com/uc?export=view&id=1OZjXvk74Lpv7teWJ4BY5-xGsjGhblpux'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.05\n\n\n\n\n\n\n\n\n\nI’ll try more difficult images, starting with an easy image of 4, and then three subsequent images with increasing amounts of noise. The model correctly predicts lower probabilities as more noise is introduced.\n\nurl = 'https://drive.google.com/uc?export=view&id=16rs0ARMoaUqUPmQIrMaVqtMiQvIIhT7n'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.98\n\n\n\n\n\n\n\n\n\n\nurl = 'https://drive.google.com/uc?export=view&id=1UrUZRJpA2hLtTukQNQJdK8VDnL7Ap2xr'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.96\n\n\n\n\n\n\n\n\n\n\nurl = 'https://drive.google.com/uc?export=view&id=1di1Pw6yaQlzi-JWOv2JGMuc69UKzlKYm'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.94\n\n\n\n\n\n\n\n\n\n\nurl = 'https://drive.google.com/uc?export=view&id=1opKjl3xJavdINgr7RhiP27JIUbFNto-h'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.76\n\n\n\n\n\n\n\n\n\nThe model is not perfect—it predicts the following blob with a probability of 85%. I would expect something lower. However, it’s indicative of the type of data it’s seen during training so something I can adjust in the training data.\n\nurl = 'https://drive.google.com/uc?export=view&id=1JF6dTa38GIlO8FcVGoVQ69Pi0esZe3M1'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.85\n\n\n\n\n\n\n\n\n\nThe model seems more sensitive to black pixels in the corners and sides—the probability that it’s a digit drops:\n\nurl = 'https://drive.google.com/uc?export=view&id=1dft9riZGAL0aCxGO6LmuvNPP5RSGLrny'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.72\n\n\n\n\n\n\n\n\n\nI think this model is good enough for now. I’ll export it and utilize in this Huggingface Space."
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#production-environment-results",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#production-environment-results",
    "title": "Is it a Digit?",
    "section": "Production Environment Results",
    "text": "Production Environment Results\nHere are some examples of how this models fares in the HuggingFace Space “production” environment:\nEnough randomly placed pixels leads the model to believe a handwritten digit is present. I sort of understand how it comes to this conclusion (based on the training data) and it’s not ideal. I would want this image to have a much lower probability.\n\n\n\nimage.png\n\n\nIt correctly gives the following images high (90%+) probability of being a handwritten digit (it particularly likes 6s):\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nWhile I appreciate the model giving this 8 a high probability of being a handwritten digit, I need to clamp the predictions to between 0 and 1:\n\n\n\nimage.png\n\n\nThe model incorrectly gives the following image a high probability:\n\n\n\nimage.png\n\n\nThe model correctly gives the following image a relatively low probability:\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#training-on-a-different-dataset",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#training-on-a-different-dataset",
    "title": "Is it a Digit?",
    "section": "Training on a Different Dataset",
    "text": "Training on a Different Dataset\nI want to try a different approach: what I train the model only on the data where pixels are “subtracted” from the digit?\n\ntrain_noise.shape, valid_noise.shape\n\n((120000, 28, 28), (20000, 28, 28))\n\n\nThe second half of each array (training and validation noise) are the “subtracted noise” images:\n\ntrain_x = train_noise[60000:]\nvalid_x = valid_noise[10000:]\ntrain_y = train_thresh[60000:]\nvalid_y = valid_thresh[10000:]\n\ntrain_x.shape, train_y.shape, valid_x.shape, valid_y.shape\n\n((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))\n\n\nI’ll spot-check a few images of the training and validation sets to make sure it contains only the “subtractive noise” data:\n\nplot_noise(train_x[0], train_y[0])\n\n\n\n\n\n\n\n\n\nplot_noise(train_x[-1], train_y[-1])\n\n\n\n\n\n\n\n\n\nplot_noise(train_x[30000], train_y[30000])\n\n\n\n\n\n\n\n\n\nplot_noise(valid_x[0], valid_y[0])\n\n\n\n\n\n\n\n\n\nplot_noise(valid_x[-1], valid_y[-1])\n\n\n\n\n\n\n\n\n\nplot_noise(valid_x[5000], valid_y[5000])\n\n\n\n\n\n\n\n\nNext, I want to “penalize” noisy images that have a low probability (say <=75%) and “favor” images with a high probability (>=75%). I’ll create a transform_threshold function to achieve this. I prompted Claude to give me such a function and then modified the parameters to get the following result.\n\ndef transform_threshold(p):\n    p = tensor(p)\n    k1, k2 = 0.5, 20 # Steepness parameters\n    mid1, mid2 = 0.65, 0.65  # Midpoints\n    lower = torch.sigmoid(k1 * (p - mid1))\n    upper = 0.5 * torch.sigmoid(k2 * (p - mid2))\n    \n    return p * lower + upper\n\n# Create an array of 101 evenly spaced values from 0 to 1\nx = torch.linspace(0, 1, 101)\n\n# Apply the transform_threshold function to each value\ny = transform_threshold(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, 'b-', label='Transformed values')\nplt.plot(x, x, 'r--', label='Original values (y=x)')\n\nplt.xlabel('Original probability')\nplt.ylabel('Transformed probability')\nplt.title('Effect of transform_threshold function')\nplt.legend()\nplt.grid(True)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\nI’ll apply this transformation to the training and validation set target probabilities:\n\nt_train_y = transform_threshold(train_y)\nt_valid_y = transform_threshold(valid_y)\n\n\nplt.scatter(train_y, t_train_y);\n\n\n\n\n\n\n\n\n\ntrain_y[0], t_train_y[0]\n\n(0.52439004, tensor(0.2915))\n\n\n\nplot_noise(train_x[0], t_train_y[0])\n\n\n\n\n\n\n\n\n\ntrain_y[-1], t_train_y[-1]\n\n(0.3380698, tensor(0.1569))\n\n\n\nplot_noise(train_x[-1], t_train_y[-1])\n\n\n\n\n\n\n\n\n\ntrain_y[10000], t_train_y[10000]\n\n(0.83755004, tensor(0.9269))\n\n\n\nplot_noise(train_x[10000], t_train_y[10000])\n\n\n\n\n\n\n\n\n\nplt.scatter(valid_y, t_valid_y);\n\n\n\n\n\n\n\n\n\nvalid_y[0], t_valid_y[0]\n\n(0.23614848, tensor(0.1060))\n\n\n\nplot_noise(valid_x[0], t_valid_y[0])\n\n\n\n\n\n\n\n\n\nvalid_y[-1], t_valid_y[-1]\n\n(0.4010573, tensor(0.1915))\n\n\n\nplot_noise(valid_x[-1], t_valid_y[-1])\n\n\n\n\n\n\n\n\n\nvalid_y[5000], t_valid_y[5000]\n\n(0.34754837, tensor(0.1618))\n\n\n\nplot_noise(valid_x[5000], t_valid_y[5000])\n\n\n\n\n\n\n\n\nGreat! I have successfully penalized low-probability images. I would expect the model to favor more fully-formed digits, creating what I think is a better user experience for this app."
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#training-the-new-model",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#training-the-new-model",
    "title": "Is it a Digit?",
    "section": "Training the New Model",
    "text": "Training the New Model\nI’ll reuse nearly all of the code from before.\n\n# Combine train and valid data\nall_noise = np.concatenate([train_x, valid_x])\nall_thresh = np.concatenate([t_train_y, t_valid_y])\n\n\nall_noise = (all_noise * 255).astype(np.uint8)\n\n\ntrain_x.shape, valid_x.shape, all_noise.shape, t_train_y.shape, t_valid_y.shape, all_thresh.shape\n\n((60000, 28, 28),\n (10000, 28, 28),\n (70000, 28, 28),\n torch.Size([60000]),\n torch.Size([10000]),\n (70000,))\n\n\n\ndef get_x(i):\n    # Convert NumPy array to a single-channel PIL image with inverted colors\n    return PILImageBW.create(all_noise[i])\n\ndef get_y(i):\n    return all_thresh[i].astype(np.float32)\n\ndef get_items(_):\n    return range(len(all_noise))\n\n\n# Create valid_idx for IndexSplitter\nvalid_idx = list(range(len(train_x), len(all_noise)))\nlen(valid_idx)\n\n10000\n\n\n\ndblock = DataBlock(\n    blocks=(ImageBlock(PILImageBW), RegressionBlock),\n    get_items=get_items,\n    get_x=get_x,\n    get_y=get_y,\n    splitter=IndexSplitter(valid_idx),\n    batch_tfms=None\n)\n\n\ndblock.summary(all_noise)\n\nSetting-up type transforms pipelines\nCollecting items from [[[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n ...\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]]\nFound 70000 items\n2 datasets of sizes 60000,10000\nSetting up Pipeline: get_x -> PILBase.create\nSetting up Pipeline: get_y -> RegressionSetup -- {'c': None}\n\nBuilding one sample\n  Pipeline: get_x -> PILBase.create\n    starting from\n      0\n    applying get_x gives\n      PILImageBW mode=L size=28x28\n    applying PILBase.create gives\n      PILImageBW mode=L size=28x28\n  Pipeline: get_y -> RegressionSetup -- {'c': None}\n    starting from\n      0\n    applying get_y gives\n      0.29146788\n    applying RegressionSetup -- {'c': None} gives\n      tensor(0.2915)\n\nFinal sample: (PILImageBW mode=L size=28x28, tensor(0.2915))\n\n\nCollecting items from [[[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n ...\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]\n\n [[255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  ...\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]\n  [255 255 255 ... 255 255 255]]]\nFound 70000 items\n2 datasets of sizes 60000,10000\nSetting up Pipeline: get_x -> PILBase.create\nSetting up Pipeline: get_y -> RegressionSetup -- {'c': None}\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImageBW mode=L size=28x28, tensor(0.2915))\n    applying ToTensor gives\n      (TensorImageBW of size 1x28x28, tensor(0.2915))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImageBW of size 4x1x28x28, tensor([0.2915, 1.0310, 0.1721, 0.1027], device='cuda:0'))\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImageBW of size 4x1x28x28, tensor([0.2915, 1.0310, 0.1721, 0.1027], device='cuda:0'))\n\n\n\ndls = dblock.dataloaders(all_noise, bs=1024)\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nxb, yb = dls.one_batch()\nplot_noise(xb[0][0].cpu(), yb[0])\n\n\n\n\n\n\n\n\nThe MSE for this dataset (0.008) is 4x than the first dataset (0.002).\n\nlearn = vision_learner(dls, resnet34, loss_func=mse, metrics=mse, n_in=1, n_out=1)\nlearn.fine_tune(5, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      mse\n      time\n    \n  \n  \n    \n      0\n      1.144769\n      0.068298\n      0.068298\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      mse\n      time\n    \n  \n  \n    \n      0\n      0.052315\n      0.035616\n      0.035616\n      00:36\n    \n    \n      1\n      0.038064\n      0.013611\n      0.013611\n      00:37\n    \n    \n      2\n      0.032048\n      0.015357\n      0.015357\n      00:38\n    \n    \n      3\n      0.026942\n      0.013210\n      0.013210\n      00:39\n    \n    \n      4\n      0.018786\n      0.008574\n      0.008574\n      00:39\n    \n  \n\n\n\n\nlearn.export('model2.pkl')"
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#inference-1",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#inference-1",
    "title": "Is it a Digit?",
    "section": "Inference",
    "text": "Inference\nI’ll take a look at the predictions and a few test images before deploying this model in my HuggingFace Space. At first glance, the model looks pretty good at predicting the validation set:\n\npreds, targs = learn.get_preds(dl=dls.valid)\npreds[:10], targs[:10]\n\n\n\n\n\n\n\n\n(tensor([[0.1268],\n         [0.8752],\n         [0.0170],\n         [0.9900],\n         [0.2933],\n         [0.0500],\n         [0.9181],\n         [0.9602],\n         [1.0142],\n         [0.8437]]),\n tensor([0.1060, 0.8329, 0.0078, 0.9869, 0.2650, 0.0298, 0.9310, 0.9779, 1.0219,\n         0.9140]))\n\n\nI’ll make sure that learn.predict works as expected in my “production” environment:\n\nprint(f\"{learn.predict(x[0])[0][0]:.2f}\")\n\n\n\n\n\n\n\n\n0.31\n\n\nThis model predicts very high (higher than the first model) probabilities for images that are meant to be handwritten digits:\n\nurl = 'https://drive.google.com/uc?export=view&id=1qNNgt4z-PPn8JKlQU5pFiRE1KCFnqb2R'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.99\n\n\n\n\n\n\n\n\n\n\nurl = 'https://drive.google.com/uc?export=view&id=16rs0ARMoaUqUPmQIrMaVqtMiQvIIhT7n'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 1.00\n\n\n\n\n\n\n\n\n\nThis model also seems to be better at predicting low probability for obvious not-digits:\n\nurl = 'https://drive.google.com/uc?export=view&id=1LjJEMldL2CaWpYxRqv0Byrg0GqdOWGOO'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.04\n\n\n\n\n\n\n\n\n\n\nurl = 'https://drive.google.com/uc?export=view&id=1OZjXvk74Lpv7teWJ4BY5-xGsjGhblpux'\npredict_image(url)\n\n\n\n\n\n\n\n\nProbability it's a digit: 0.10"
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#production-environment-results-1",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#production-environment-results-1",
    "title": "Is it a Digit?",
    "section": "Production Environment Results",
    "text": "Production Environment Results\nThis model is better at predicting a low probability for random noise:\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nIt performs worse when the image is full of black pixels.\n\n\n\nimage.png\n\n\nIt predicts obviously-digits with higher probability than before:\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nThough it predicts some digits with higher than 1.00—I’ll implement a clamp on the probability to resolve this issue:\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2024-08-26-is-it-a-digit/index.html#final-thoughts",
    "href": "posts/2024-08-26-is-it-a-digit/index.html#final-thoughts",
    "title": "Is it a Digit?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis exercise turned out to be even more exciting than I expected (and I was already pretty pumped!). Here’s what I learned:\n\nProduction feedback is unmatched. The best move was deploying the initial model quickly with a user interface to see how it performed in real-world conditions. That feedback led me to revamp the training dataset with adjustments I wouldn’t have thought of otherwise, resulting in a better user experience.\nBuilding a good “product” is tougher than I thought. I assumed generating synthetic data, training a model, and deploying it with MNIST would be straightforward. I was wrong! I’ve put in 5-10 hours (including time wrestling with deployment dependencies) and still wouldn’t say the model is perfect for users.\nClaude accelerates the learning curve. A few simple prompts were all it took for Claude to generate a working prototype. Claude handled 95% of the frontend code. Without it, I’d have spent days figuring out how to create a responsive UI with a drawable canvas and real-time prediction display—and might’ve abandoned the project entirely without that assist.\n\nI hope you enjoyed this notebook! If so, please give it an upvote :)."
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html",
    "title": "Sentiment Classification with phi-3.5",
    "section": "",
    "text": "Show pip installs\n!pip install torch==2.3.1 -qq\n!pip install accelerate==0.31.0 -qq\n!pip install transformers==4.41.2 -qq\n!pip install huggingface_hub -qq\n!pip install datasets~=2.16.1 -qq\n!pip install plotly==5.19.0 -qq\n!pip install scikit-learn==1.2 -qq\n!pip install pynvml -qq\n\n\n\n\nShow imports and setup\nimport gc\nimport pandas as pd\nimport numpy as np\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\nimport torch\n\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nimport warnings\n#warnings.filterwarnings(\"ignore\")\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport time\n\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom fastcore.all import *\n\n#torch.set_default_device(\"cuda\")\ntorch.cuda.set_device(0)\n\nmodel_nm = \"microsoft/Phi-3.5-mini-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained( \n    model_nm,  \n    device_map=\"cuda\",  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(model_nm)\n\npipe = pipeline( \n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n) \n\n# load dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\", \n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\n\n\n# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\nlabel_map = {i: label_text for i, label_text in enumerate(dataset.features[\"label\"].names)}\n\ndef add_label_text(example):\n    example[\"label_text\"] = label_map[example[\"label\"]]\n    return example\n\ndataset = dataset.map(add_label_text)\n\nprint(dataset)\n\n\n\nShow add_prompt and generate_responses functions\ndef add_prompt(item, prompt):\n        item['prompt'] = prompt.format(text=item['sentence'])\n        return item\n    \ndef generate_responses(dataset, prompt):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    # check that the prompt is correctly formatted\n    print(dataset[0]['prompt'])\n    print('---------')\n    \n    for row in dataset:\n        messages = [  \n            {\"role\": \"user\", \"content\": row['prompt']},\n        ] \n\n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\n\nShow generate_response function\ndef generate_response(prompt):\n    messages = [  \n        {\"role\": \"user\", \"content\": prompt},\n    ] \n\n    generation_args = { \n        \"max_new_tokens\": 2, \n        \"return_full_text\": False, \n        \"temperature\": 0.1, \n        \"do_sample\": True, \n    } \n\n    output = pipe(messages, **generation_args) \n    return output[0]['generated_text']\n\n\n\n\nShow make_cm function\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n    \n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['responses'], labels=['negative', 'neutral', 'positive', 'other'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive', 'other'])\n    \n    # I chose 8x8 so it fits on one screen but still is large\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp.plot(ax=ax,text_kw={'fontsize': 16}, cmap='Blues', colorbar=False);\n    \n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(18)\n    ax.yaxis.label.set_fontsize(18)\n    \n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=16)\n    ax.tick_params(axis='x', labelsize=16)\n\n\n\n\nShow ds_subset function\ndef ds_subset(dataset, exclude_idxs, columns=[0, 1, 2]):\n    idxs = list(range(len(dataset)))\n    idxs = [x for x in idxs if x not in exclude_idxs]\n    ddf = dataset.to_pandas()\n    new_ds = Dataset.from_pandas(ddf.iloc[idxs, columns])\n    return new_ds"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#background",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#background",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Background",
    "text": "Background\nIn this notebook I’ll use Phi-3-5-mini-4k-instruct to classify sentiment in the financial_phrasebank dataset. In previous notebooks I have performed sentiment classification with phi-2 and the Claude series.\nThis notebook is part of a series of blog posts for a project I’m working called TinySentiment where I’m experimenting with tiny models to improve their ability to classify sentiment in the financial_phrasebank dataset. I was inspired to do so after reading this blog post and this corresponding notebook by Moritz Laurer as part of a fastai study group last year.\nHere are the results from my experiments so far (**the best-performing prompt from this notebook):\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\n**phi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nph-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\n\nHere are the per-prompt results from this notebook (phi-3.5):\n\n\n\nprompt\nstrategy\naccuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n62.32%\n98% (296/303)\n43% (592/1391)\n92% (523/570)\n\n\nB\n0-Shot\n88.60%\n96% (290/303)\n87% (1215/1391)\n88% (501/570)\n\n\nC\n0-Shot\n83.48%\n98% (298/303)\n76% (1062/1391)\n93% (530/570)\n\n\nD\n0-Shot\n68.64%\n99% (300/303)\n51% (713/1391)\n95% (541/570)\n\n\nE\n0-Shot\n88.25%\n96% (290/303)\n87% (1207/1391)\n88% (501/570)\n\n\nF\n3-Shot\n84.65%\n98% (296/302)\n77% (1070/1390)\n96% (548/569)\n\n\nG\n6-Shot\n77.99%\n98% (297/302)\n66% (913/1387)\n97% (551/569)\n\n\nH\n3-Shot\n83.06%\n98% (296/302)\n74% (1028/1390)\n97% (554/569)\n\n\nI\n3-Shot\n51.61%\n100% (302/302)\n32% (447/1390)\n73% (418/569)\n\n\nJ\n3-Shot\n85.94%\n98% (296/302)\n80% (1108/1390)\n95% (539/569)\n\n\nK\n0-Shot\n77.96%\n98% (298/303)\n66% (919/1391)\n96% (548/570)\n\n\nL\n0-Shot\n80.57%\n98% (297/303)\n70% (972/1391)\n97% (555/570)\n\n\nM\n0-Shot\n91.30%\n97% (294/303)\n90% (1257/1391)\n91% (516/570)\n\n\nN\n0-Shot w/System Prompt\n88.74%\n97% (295/303)\n85% (1184/1391)\n93% (530/570)\n\n\nO\n0-Shot w/System Prompt\n87.10%\n94% (285/303)\n83% (1156/1391)\n93% (531/570)\n\n\nP\n0-Shot\n92.23%\n94% (285/303)\n94% (1307/1391)\n87% (496/570)\n\n\nQ\n0-Shot\n79.37%\n99% (300/303)\n73% (1009/1391)\n86% (488/570)\n\n\nR\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nS\n28-Shot\n93.25%\n94% (281/298)\n99% (1358/1373)\n79% (446/565)\n\n\nT\n20-Shot\n84.54%\n78% (232/299)\n99.9% (1378/1379)\n51% (287/566)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-a",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-a",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt A",
    "text": "Prompt A\nThe HuggingFace model card for Phi-3-5 Mini-4K-Instruct says:\n\nGiven the nature of the training data, the Phi-3.5-mini-instruct model is best suited for prompts using the chat format\n\nSo, the first prompt I’ll try will be a simple instruction:\n\npromptA = \"\"\"Label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\n\ntext = dataset[1][\"sentence\"]\ntext\n\n\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\"\n\n\n\nformatted_prompt = promptA.format(text=text)\nprint(formatted_prompt)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\n\n\n\ngenerate_response(formatted_prompt)\n\nYou are not running the flash-attention implementation, expect numerical differences.\n\n\n' Negative'\n\n\n\n%time generate_response(formatted_prompt)\n\nCPU times: user 101 ms, sys: 9.29 ms, total: 111 ms\nWall time: 109 ms\n\n\n' Negative'\n\n\n\n%timeit -n 10 generate_response(formatted_prompt)\n\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\n91.9 ms ± 1.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nGood–at least it works! Although it looks like I’ll have to strip the outputs of whitespace and convert them to lowercase. It takes about 0.1 seconds to generate the response, so it should take about 4 minutes to run inference on the whole dataset.\n\ndf, acc = generate_responses(dataset, promptA)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n---------\n\n\n\nacc\n\n0.6232332155477032\n\n\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n\n\ndf.to_csv('/notebooks/phi-3-5_A.csv', index=False)\n\nThis prompt struggled with the neutral sentiment, as 568/1391 were misclassified as something other than positive, neutral or negative.\n\nmake_cm(df)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-b",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-b",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt B",
    "text": "Prompt B\nI’ll repeat the instruction after the sentence and see if that improves the performance (as it did for phi-2).\n\npromptB = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptB)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\n---------\n\n\nThe accuracy jumps up from 62.3% to 88.6%! Repeating the instruction after the dataset item was something I learned to do in fastai study group.\n\nacc\n\n0.8860424028268551\n\n\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n\n\ndf.to_csv('/notebooks/phi-3-5_B.csv', index=False)\n\nThe model does a much better job at predicting neutral sentiment with this adjustment.\n\nmake_cm(df)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-c",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-c",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt C",
    "text": "Prompt C\nI’ll add some introductory text to the prompt to see if that improves the model’s performance:\n\npromptC = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptC)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\n---------\n\n\nThe addition of this introductory text actually worsens the model’s performance by about 5%.\n\nacc\n\n0.8348056537102474\n\n\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_C.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-d",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-d",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt D",
    "text": "Prompt D\nI’ll try another prompt language adjustment to Prompt B: I’ll replace “label” with “Respond”.\n\npromptD = \"\"\"Instruct: Respond with only one of these words: negative, positive, or neutral\nTEXT: {text}\nRespond with only one of these words: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptD)\n\n\n\n\nInstruct: Respond with only one of these words: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral\n---------\n\n\nWow! The accuracy plummets to 69%. This was something that had improved the accuracy for phi-2.\n\nacc\n\n0.6863957597173145\n\n\nThe model actually improves its performance on negative and positive sentences, but significantly worsens its performance when classifying neutral sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_D.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-e",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-e",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt E",
    "text": "Prompt E\nAnother adjustment that improved phi-2’s performance was to add a period after the instruction. I’ll see if doing so improves phi-3.5’s performance.\n\npromptE = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral.\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral.\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptE)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral.\n---------\n\n\nInterestingly, this actually worsens the overall accuracy a bit.\n\nacc\n\n0.8825088339222615\n\n\nThe negative and positive true positive rate is the same as Prompt B, but neutral rate is worse (1207 < 1215).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_E.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-f",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-f",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt F",
    "text": "Prompt F\nI’ll now move on to few-shot prompting to see if I can improve on the best overall accuracy so far (88.6%). To do so, I’ll create a new helper function (since the chat template handles few-shot prompt as multiple query-response exchanges between user and assistant).\n\n\nShow few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    count = 0\n    for row in dataset:\n        count += 1\n        messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        if count == 1: print(messages)\n        \n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\nexclude_idxs = [0, 1, 292]\n\n\npromptF_ds = ds_subset(dataset, exclude_idxs)\n\n\npromptF_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative')]\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptB, examples)\n\n\n\n\n[{'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral'}]\n\n\nThe accuracy drops to 84.65%.\n\nacc\n\n0.8465280849181778\n\n\nCompared to Prompt B, the true positive rate for neutral decreases (1070 < 1215) whereas for positive and negative sentiment the TPR increases (296 > 290, 548 > 501).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_F.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-g",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-g",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt G",
    "text": "Prompt G\nI’ll now try a 6-Shot prompt using the examples that were best-performing for phi-2.\n\nexclude_idxs=[0, 1, 292, 37, 38, 39]\npromptG_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptG_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2258\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples[0], len(examples)\n\n(('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n 6)\n\n\n\ndf, acc = few_shot_responses(promptG_ds, promptB, examples)\n\n\n\n\n[{'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral'}]\n\n\nUnexpectedly, the accuracy drops to 78%.\n\nacc\n\n0.7798937112488928\n\n\nThe model performs better with this prompt than the so far best-performing 3-Shot prompt (84.7%) for negative sentences (297 > 296) and positive sentences (551 > 548) but performs much worse for neutral sentences (913 < 1070).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_G.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-h",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-h",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt H",
    "text": "Prompt H\nI’ll return to the 3-Shot prompt (84.65%) and see if I can improve it by adjusting the language. First, I’ll add some introductory text to the start of the prompt. Note that this did not improve the 0-Shot performance.\n\npromptH = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\nexclude_idxs = [0, 1, 292]\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative')]\n\n\n\npromptF_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptH, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}]\n\n\nThis does not improve the overall accuracy. Instead, it drops by about 1.6%.\n\nacc\n\n0.8306059265811587\n\n\nCompared to the 3-Shot prompt, negative sentences are classified at the same frequency (296/302), neutral sentences at a lower rate (1028 < 1070) and positive sentences at a higher rate (554 > 548).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_H.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-i",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-i",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt I",
    "text": "Prompt I\nBefore I give the model more than 6 examples, I’ll deviate from the recommended multi-turn chat format for few-shot prompting and give the examples in a single prompt.\n\npromptI = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nneutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\npositive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nnegative\n\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\n\"\"\"\n\n\npromptF_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\ndf, acc = generate_responses(promptF_ds, promptI)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nneutral\n\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\npositive\n\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nnegative\n\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nlabel the TEXT with a single word: negative, positive, or neutral\n\n---------\n\n\nNope! The performance of few-shot prompting without multi-turn format is drastically worse.\n\nacc\n\n0.5161432994250331\n\n\nThe true positive rate for negative sentiment is actually higher (302/302 or 100%) but the rate is much lower for neutral sentiment (917 < 1070) and positive sentiment (418 < 548).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_I.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-j",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-j",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt J",
    "text": "Prompt J\nI’ll try one more prompt with single-turn few-shot examples. I’ll add “Output:” before the label in each example, and add the “Instruct:” instructions before each example TEXT. I’ll also remove the extra new line that I have after the final instruction.\n\npromptJ = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: neutral\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: positive\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: negative\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: \"\"\"\n\n\ndf, acc = generate_responses(promptF_ds, promptJ)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\n\nExamples:\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: neutral\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: positive\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: negative\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nlabel the TEXT with a single word: negative, positive, or neutral\nOutput: \n---------\n\n\nWow! This actually made a difference. This is the second-best overall accuracy I have achieved.\n\nacc\n\n0.8593542680229986\n\n\nCompared to the previous second-best prompt (3-Shot), this prompt results in the same true positive rate for negative sentiment (296/302), a much higher rate for neutral sentiment (1108 > 1070) and a lower rate for positive sentiment (539 < 548).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_J.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-k",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-k",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt K",
    "text": "Prompt K\nI’ll return to few-shot prompting in a bit, but want to first revisit zero-shot prompting as it yielded the best overall performance so far (88.6% overall accuracy).\nI asked Claude for suggestions on how to improve that prompt and will be trying them out.\nFirst suggestion:\n\nRefine the Instruction: Try slight variations of the instruction to see if they yield better results:\n\n\npromptK = \"\"\"Instruct: Analyze the sentiment of the following financial statement and respond with a single word: negative, positive, or neutral\nFinancial statement: {text}\nSentiment (respond with a single word):\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptK)\n\n\n\n\nInstruct: Analyze the sentiment of the following financial statement and respond with a single word: negative, positive, or neutral\nFinancial statement: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nSentiment (respond with a single word):\n---------\n\n\nThis yields a worse overall accuracy.\n\nacc\n\n0.7795936395759717\n\n\nCompared to the best-performing 0-Shot Prompt B (88.6%) this prompt yields a higher true positive rate for neutral sentiment (298 > 290) and positive sentiment (548 > 501) but lower for neutral sentiment (919 < 1215).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_K.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-l",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-l",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt L",
    "text": "Prompt L\nGiven the success of that prompt with negative and positive sentiment, I’ll see if I can improve it for neutral sentiment by adding the phrase: “if you’re not sure, respond with neutral.”\n\npromptL = \"\"\"Instruct: Analyze the sentiment of the following financial statement and respond with a single word: negative, positive, or neutral. If you’re not sure, respond with neutral.\nFinancial statement: {text}\nSentiment (respond with a single word, if you’re not sure, respond with neutral):\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptL)\n\n\n\n\nInstruct: Analyze the sentiment of the following financial statement and respond with a single word: negative, positive, or neutral. If you’re not sure, respond with neutral.\nFinancial statement: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nSentiment (respond with a single word, if you’re not sure, respond with neutral):\n---------\n\n\nThis improves the overall accuracy but is still lower than the Prompt B (88.6%).\n\nacc\n\n0.8056537102473498\n\n\nCompared to Prompt K, the true positive rate for each sentiment increased.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_L.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-m",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-m",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt M",
    "text": "Prompt M\nGiven the success of the phrase “if you’re not sure, respond with neutral” I’ll add it to Prompt B.\n\npromptM = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptM)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\n---------\n\n\nHooray!! With this language adjustment, I have achieved the best overall accuracy so far.\n\nacc\n\n0.9129858657243817\n\n\nThe true positive rate for all three sentiments has increased.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_M.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-n",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-n",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt N",
    "text": "Prompt N\nI’ll see if adding a system prompt improves the performance.\n\n\nShow updated generate_responses function\ndef generate_responses(dataset, prompt, sp=False):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    # check that the prompt is correctly formatted\n    print(dataset[0]['prompt'])\n    print('---------')\n    \n    for row in dataset:\n        \n        if sp:\n            messages = [{'role': 'system', 'content': 'You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification.'}\n                       ] + [{\"role\": \"user\", \"content\": row['prompt']},]\n            \n        else: messages = [{\"role\": \"user\", \"content\": row['prompt']},] \n\n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\ndf, acc = generate_responses(dataset, promptM, sp=True)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\n---------\n\n\nYou are not running the flash-attention implementation, expect numerical differences.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\nAdding that system prompt results in a worse accuracy.\n\nacc\n\n0.8873674911660777\n\n\nThe true positive rate for negative (295 > 290) and positive (530 > 501) increases but for neutral (1184 < 1215) decreases.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_N.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-o",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-o",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt O",
    "text": "Prompt O\nI’ll see if adding “if you’re not sure, respond with neutral” to the system message improves performance.\n\n\nShow updated generate_responses function\ndef generate_responses(dataset, prompt, sp=False):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    # check that the prompt is correctly formatted\n    print(dataset[0]['prompt'])\n    print('---------')\n    \n    for row in dataset:\n        \n        if sp:\n            messages = [{'role': 'system', 'content': sp}\n                       ] + [{\"role\": \"user\", \"content\": row['prompt']},]\n            \n        else: messages = [{\"role\": \"user\", \"content\": row['prompt']},] \n\n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If you're not sure, respond with neutral.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If you're not sure, respond with neutral.\n\n\n\ndf, acc = generate_responses(dataset, promptM, sp=sp)\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\n---------\n\n\nThis system prompt still performs worse than no system prompt.\n\nacc\n\n0.8710247349823321\n\n\nCompared to the best-performing Prompt M, this prompt yields a higher true positive rate for positive sentiment (531 > 501) but a lower rate for neutral sentiment (1156 < 1215) and for negative (294 > 290) sentiment.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_O.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-p",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-p",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt P",
    "text": "Prompt P\nI’ll move away from system prompts for now and revisit language adjustments. For Prompt M, I’ll replace:\n\nIf you’re not sure, respond with neutral.\n\nwith\n\nIf the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\n\npromptP = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptP)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n---------\n\n\nExcellent! The overall accuracy again increases, this time to 92.2%. I’m still quite surprised it’s taken so much effort to surpass phi-2, but I’ll reflect on that later on.\n\nacc\n\n0.9222614840989399\n\n\nBoth negative and positive sentiment true positive rates decrease, but this prompt results in almost 100 more correct neutral responses.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_P.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-q",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-q",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt Q",
    "text": "Prompt Q\nGiven the success of the zero-shot prompt with instructions on handling neutral statements, I’ll try a prompt suggested by Claude, which adds more nuance to handling neutral sentences:\n\npromptQ = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. \nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral. If key financial metrics are not clearly changing, respond with neutral. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral.\"\"\"\n\n\nprint(promptQ)\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. \nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral. If key financial metrics are not clearly changing, respond with neutral. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral.\n\n\n\ndf, acc = generate_responses(dataset, promptQ)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. \nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral. If key financial metrics are not clearly changing, respond with neutral. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral.\n---------\n\n\nYou are not running the flash-attention implementation, expect numerical differences.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\nA more nuanced prompt actually deteriorates the overall accuracy by about 13%.\n\nacc\n\n0.7937279151943463\n\n\nCompared to the best performing prompt P (92.2%), this prompt performs better on negative sentiment and worse on neutral and positive sentiment.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_Q.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-r",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-r",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt R",
    "text": "Prompt R\nI’ll now try providing a large number of examples (20) in the prompt. I don’t expect this to improve upon my 92.2% accuracy since 3-Shot and 6-Shot prompting performed worse. Nevertheless, I’ve heard that it’s not uncommon to give a model dozens of examples.\n\nexclude_idxs = [1, 2, 3, 4, 292, 293, 294, 347, 0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283]\n\n\npromptR_ds = ds_subset(dataset, exclude_idxs=exclude_idxs, columns=[0, 1, 2])\npromptR_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2244\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples[0], len(examples)\n\n((\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n 20)\n\n\n\ndf, acc = few_shot_responses(promptR_ds, promptP, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: However , the growth margin slowed down due to the financial crisis .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: 2009 3 February 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR46m in the fourth quarter of 2009 from a year-earlier profit of EUR45m .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The contract value amounts to about EUR11m , the company added .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}]\n\n\nWow! I’m so glad I tried a larger number of examples. The accuracy (93.94%) is now competitive with the Claude models!\n\nacc\n\n0.9393939393939394\n\n\nWith 20-Shot prompting, the true positive rate for negative (286 > 285) and neutral (1355 > 1307) sentiment increases, and decreases for positive (467 < 496) sentiment.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_R.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-s",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-s",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt S",
    "text": "Prompt S\nI’ll increase the number of examples to 28 and see if that yields an improvement. I currently have 4 positive, 4 negative and 12 neutral examples. I’ll up that to 5:5:18.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, # positive\n    292, 293, 294, 347, 348, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283, 284, 285, 286, 287, 288, 289 # neutral\n]\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples[0], len(examples)\n\n((\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n 28)\n\n\n\npromptS_ds = ds_subset(dataset, exclude_idxs=exclude_idxs, columns=[0, 1, 2])\npromptS_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2236\n})\n\n\n\ndf, acc = few_shot_responses(promptS_ds, promptP, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: However , the growth margin slowed down due to the financial crisis .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: 2009 3 February 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR46m in the fourth quarter of 2009 from a year-earlier profit of EUR45m .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: ( ADPnews ) - Feb 3 , 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR 46 million ( USD 64.5 m ) in the fourth quarter of 2009 from a\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The contract value amounts to about EUR11m , the company added .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The company generates net sales of about 600 mln euro $ 775.5 mln annually and employs 6,000 .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The contract covers the manufacturing , surface-treatment and installation of the steel structures .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The order also includes start-up and commissioning services .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: The phones are targeted at first time users in growth markets .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Tielinja generated net sales of 7.5 mln euro $ 9.6 mln in 2005 .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Tikkurila Powder Coatings has some 50 employees at its four paint plants , which generated revenues of EUR2 .4 m USD3 .3 m in 2010 .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\\nTEXT: Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"}]\n\n\nYou are not running the flash-attention implementation, expect numerical differences.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\nInterestingly, that decreases the overall accuracy by about 0.7%.\n\nacc\n\n0.932468694096601\n\n\nCompared to Prompt R, this prompt yields fewer correct negative and positive sentences. It classifies 3 more neutral sentences correctly, but that doesn’t make up for the loss in performance of the other two sentiments.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3-5_S.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-t",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#prompt-t",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Prompt T",
    "text": "Prompt T\nI noticed in the Prompt R results that 14 sentences were classified as something “other” than neutral, positive, or negative. Instead of asking the model to respond with negative, neutral or positive, I’ll ask it to respond with 0, 1 or 2 and see if that simplification yields better results.\n\npromptT = \"\"\"Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\nTEXT: {text}\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\"\"\"\n\n\nprint(promptT)\n\nInstruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\nTEXT: {text}\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\n\n\n\nexclude_idxs = [1, 2, 3, 4, 292, 293, 294, 347, 0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283]\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], str(dataset[idx]['label'])))\n\nexamples[0], len(examples)\n\n((\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  '2'),\n 20)\n\n\n\n\nShow updated few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    count = 0\n    for row in dataset:\n        count += 1\n        messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        if count == 1: print(messages)\n        \n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    return df\n\n\n\ndf = few_shot_responses(promptR_ds, promptT, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\"}, {'role': 'assistant', 'content': '2'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '2'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '2'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '2'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '0'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '0'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: However , the growth margin slowed down due to the financial crisis .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '0'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: 2009 3 February 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR46m in the fourth quarter of 2009 from a year-earlier profit of EUR45m .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '0'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\"}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: The contract value amounts to about EUR11m , the company added .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}, {'role': 'assistant', 'content': '1'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).\\nTEXT: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\\nlabel the TEXT with a single integer: 0 (negative), 1 (neutral), or 2 (positive). If the amount of money is not explicitly increasing or decreasing, respond with 1 (neutral).'}]\n\n\nYou are not running the flash-attention implementation, expect numerical differences.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\n\ndf['responses'] = df['responses'].apply(lambda x: dataset.features[\"label\"].names[int(x)])\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df['lm_match'].mean()\n\nInterestingly, this decreases the overall accuracy by almost 10%.\n\nacc\n\n0.8453654188948306\n\n\nWhile there are no other classifications, and neutral true positive rate increases (1378 > 1355), the rate for negative (232 < 286) and especially positive (287 < 467) sentiment decreases. The model classifies almost half of the positive sentences as neutral.\n\nmake_cm(df)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#running-inference-with-the-best-prompt-multiple-times",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#running-inference-with-the-best-prompt-multiple-times",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Running Inference with the Best Prompt Multiple Times",
    "text": "Running Inference with the Best Prompt Multiple Times\nFor phi-2 I ran the best-performing prompt 10 times to see if it consistently performed at a high accuracy. Inference with phi-3.5, given the 20 examples in each prompt, takes much longer:\n\nexclude_idxs = [1, 2, 3, 4, 292, 293, 294, 347, 0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283]\n\n\npromptR_ds = ds_subset(dataset, exclude_idxs=exclude_idxs, columns=[0, 1, 2])\npromptR_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2244\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n20\n\n\n\npromptP = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"\"\"\n\n\nprint(promptP)\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\n\n\n\nShow test_gen function\ndef test_gen(examples):\n    responses = []\n    \n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": promptP.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n        \n    messages = few_shot_examples + [{\"role\": \"user\", \"content\": promptP.format(text=dataset[0])}]\n\n\n    generation_args = { \n        \"max_new_tokens\": 2, \n        \"return_full_text\": False, \n        \"temperature\": 0.1, \n        \"do_sample\": True, \n    } \n\n    response = pipe(messages, **generation_args) \n    responses.append(response[0]['generated_text'].strip().lower())\n    return responses\n\n\nThe model takes about 1.2 seconds to generate a response for a single dataset item, or about 45 minutes for the 2244 items (on a Paperspace Free-A4000). Given the 6 hour limit, the max I can do is run inference on the dataset 8 times. To be conservative, I’ll do it 7 times.\n\n%timeit -n 10 test_gen(examples)\n\n1.2 s ± 14.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nShow few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    for row in dataset:\n        messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\naccs = []\nfor _ in range(7):\n    df, acc = few_shot_responses(promptR_ds, promptP, examples)\n    accs.append(acc)\n\nThe accuracy of this prompt is consistently around 93.9%.\n\npd.Series(accs).describe()\n\ncount    7.000000\nmean     0.939139\nstd      0.000992\nmin      0.937611\n25%      0.938503\n50%      0.939394\n75%      0.939840\nmax      0.940285\ndtype: float64"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#final-thoughts",
    "href": "posts/2024-09-12-tinysentiment-phi-3-5-sentiment-classification/index.html#final-thoughts",
    "title": "Sentiment Classification with phi-3.5",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere is a summary of results including phi-2, phi-3, and the Claude family:\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nphi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nphi-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\n\nHere are the per-prompt results from this notebook (phi-3.5):\n\n\n\nprompt\nstrategy\naccuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n62.32%\n98% (296/303)\n43% (592/1391)\n92% (523/570)\n\n\nB\n0-Shot\n88.60%\n96% (290/303)\n87% (1215/1391)\n88% (501/570)\n\n\nC\n0-Shot\n83.48%\n98% (298/303)\n76% (1062/1391)\n93% (530/570)\n\n\nD\n0-Shot\n68.64%\n99% (300/303)\n51% (713/1391)\n95% (541/570)\n\n\nE\n0-Shot\n88.25%\n96% (290/303)\n87% (1207/1391)\n88% (501/570)\n\n\nF\n3-Shot\n84.65%\n98% (296/302)\n77% (1070/1390)\n96% (548/569)\n\n\nG\n6-Shot\n77.99%\n98% (297/302)\n66% (913/1387)\n97% (551/569)\n\n\nH\n3-Shot\n83.06%\n98% (296/302)\n74% (1028/1390)\n97% (554/569)\n\n\nI\n3-Shot\n51.61%\n100% (302/302)\n32% (447/1390)\n73% (418/569)\n\n\nJ\n3-Shot\n85.94%\n98% (296/302)\n80% (1108/1390)\n95% (539/569)\n\n\nK\n0-Shot\n77.96%\n98% (298/303)\n66% (919/1391)\n96% (548/570)\n\n\nL\n0-Shot\n80.57%\n98% (297/303)\n70% (972/1391)\n97% (555/570)\n\n\nM\n0-Shot\n91.30%\n97% (294/303)\n90% (1257/1391)\n91% (516/570)\n\n\nN\n0-Shot w/System Prompt\n88.74%\n97% (295/303)\n85% (1184/1391)\n93% (530/570)\n\n\nO\n0-Shot w/System Prompt\n87.10%\n94% (285/303)\n83% (1156/1391)\n93% (531/570)\n\n\nP\n0-Shot\n92.23%\n94% (285/303)\n94% (1307/1391)\n87% (496/570)\n\n\nQ\n0-Shot\n79.37%\n99% (300/303)\n73% (1009/1391)\n86% (488/570)\n\n\nR\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nS\n28-Shot\n93.25%\n94% (281/298)\n99% (1358/1373)\n79% (446/565)\n\n\nT\n20-Shot\n84.54%\n78% (232/299)\n99.9% (1378/1379)\n51% (287/566)\n\n\n\nI ran inference for phi-3 and phi-3.5 in separate notebooks at the same time, so I have shared final thoughts for both:\n\nFew-shot prompting in a chat format is a different experience: The sentence/label pairs have to be presented as a multi-turn conversation. For a large number of examples, this can lead to running out of GPU memory (as it did for 30-Shot prompting with phi-3.5).\nFew-shot example proportion matters: I used a higher proportion of neutral examples in my 20-shot prompt since the majority of the dataset is made up of neutral sentences. Determining whether the proportion I used is optimal would require further experimentation.\n20-Shot phi-3.5 approaches Opus, Sonnet and GPT4 accuracy: I was pleasantly surprised that phi-3.5 reached the 94% mark that was achieved by GPT4 (in the original work by Moritz Laurer), and 3-Opus and 3.5-Sonnet in my previous experiments.\nThe best performing prompt suffers from a low true positive rate for positive sentiments: Although the 20-Shot phi-3.5 prompt achieved a high true positive rate (TPR) for neutral sentences (96%), it had one of the lowest TPRs for positive sentences (83%). It’s unclear if this is due to the imbalance between positive and neutral examples, since the TPR for negative sentiment is high (98%) despite having the same number of examples (4) as positive.\nphi-3 performed differently than phi-3.5: phi-3 performed well with a system prompt while phi-3.5 did not. On the other hand, phi-3.5 performed better with 0-Shot prompting than phi-3 (results not shown here).\nFuture work: My next step is to run inference on this dataset using the Qwen2-1.5B model. After that, I’ll analyze the errors, especially for sentences that a majority of models classified incorrectly. With prompt engineering, there is potentially unlimited future work. Before I finish this project, I’ll try 30-Shot prompts for phi-2 and Haiku to see if they can beat phi-3’s 92.79% overall accuracy (and maybe even phi-3.5’s 93.94% accuracy).\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2023-11-02-LLM-Prompting/index.html",
    "href": "posts/2023-11-02-LLM-Prompting/index.html",
    "title": "Prompting LLMs Using Different Prompting Styles",
    "section": "",
    "text": "In this notebook, I test the performance of different LLMs on a small set of samples from math reasoning datasets using zero-shot, zero-shot-Chain-of-Thought and Plan-and-Solve+ (PS+) prompting techniques. The purpose of this exercise is for me to get some practice prompting LLMs.\nIn a previous blog post, I prompted questions from math reasoning datasets to different model chat interfaces (HuggingChat, Zephyr Chat and ChatGPT). However, when presenting these results to a fastai study group, one of the more experienced members noted that these model interfaces have built-in system prompts that will interfere with the prompts provided by the user. So, in order to get a sense of how these models perform without a system prompt, I decided to prompt these models directly using HuggingFace in this exercise.\n\n\nI’ll test three models in this exercise:\n\nMistral-7B-Instruct-v0.1\nzephyr-7b-alpha\nLlama-2-7b-chat-hf\n\n\n\n\nI’ll show an example of the three different prompting styles that I’ll use for this evaluation.\n\n\n\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\nA: The answer is\n\n\n\n\n\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\nA: Let’s think step by step.\n\n\n\n\n\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let’s carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer."
  },
  {
    "objectID": "posts/2023-11-02-LLM-Prompting/index.html#loading-and-trying-out-the-models",
    "href": "posts/2023-11-02-LLM-Prompting/index.html#loading-and-trying-out-the-models",
    "title": "Prompting LLMs Using Different Prompting Styles",
    "section": "Loading and Trying Out the Models",
    "text": "Loading and Trying Out the Models\nI’ll start by loading each model and testing out a prompt for each manually:\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-10-31T01:58:45.248095Z”,“iopub.status.busy”:“2023-10-31T01:58:45.247784Z”,“iopub.status.idle”:“2023-10-31T01:59:51.608104Z”,“shell.execute_reply”:“2023-10-31T01:59:51.607351Z”,“shell.execute_reply.started”:“2023-10-31T01:58:45.248068Z”}’ scrolled=‘true’ trusted=‘true’}\n!pip install git+https://github.com/huggingface/transformers.git huggingface_hub\nfrom transformers import pipeline\nfrom huggingface_hub import notebook_login\nimport pandas as pd, torch\n:::\n\nnotebook_login()\n\n\nMistral-7B-Instruct-v0.1\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\")\n\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"\"\"\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\n\nA: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.\n\"\"\"},\n]\n\npipe.use_default_system_prompt = False\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\n\n<s>[INST] \nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\n\nA: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.\n [/INST] Plan:\n\n1. Extract the relevant variables and their corresponding numerals.\n2. Devise a plan to solve the problem.\n3. Carry out the plan and calculate intermediate results.\n4. Solve the problem step by step.\n5. Show the answer.\n\nSolution:\n\n1. Variables and their corresponding numerals:\n- Total bill (T)\n- Number of people (N)\n- Payment per person (P)\n\n2. Plan:\nWe need to determine the total bill (T) based on the number of people (N) and the payment per person (P). We can use the following equation:\n\nT = P * N\n\n3. Carrying out the plan and calculating intermediate results:\nWe are given the payment per person (P) as 45 dollars. We need to find the total bill (T) by multiplying the payment per person by the number of people. We are also given the number of people (N) as Sally, Sam, and Alyssa.\n\nT = 45 * N\n\n4. Solving the problem step by step:\nWe can substitute the given values into the equation\n\n\n\n\nzephyr-7b-alpha\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"HuggingFaceH4/zephyr-7b-alpha\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\")\n\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"\"\"\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\n\nA: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.\n\"\"\"},\n]\n\npipe.use_default_system_prompt = False\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n\n\n<|user|>\n\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\n\nA: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.\n</s>\n<|assistant|>\nLet's assume the total bill is x.\n\nEach person paid 45 dollars, so each person's share of the bill is 45.\n\nIf the total bill is x, then the sum of each person's share is also x, as each person's share is one-third of the total bill.\n\nTherefore, we have:\n\nx = 45 + 45 + 45 (the sum of each person's share)\n\nx = 135 (the total bill)\n\nSo the total bill was $135.\n\nIn summary, we identified the variables (total bill and each person's share), calculated intermediate results (total of each person's share), and solved the problem by setting up an equation and calculating the total bill.\n\n\n\n\nLlama-2-7b-chat-hf\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\")\n\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"\"\"\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\n\nA: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.\n\"\"\"},\n]\n\npipe.use_default_system_prompt = False\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n\n<s>[INST] Q: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\n\nA: Let's first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let's carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer. [/INST]  Great, let's dive into the problem!\n\nAfter eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid $45, what was the total of the bill?\n\nTo solve this problem, we need to identify the variables involved and determine their values. Let's do this by breaking down the information given in the problem:\n\nVariables:\n\n* Sally\n* Sam\n* Alyssa\n* Total bill\n\nValues:\n\n* Sally paid $45\n* Sam paid $45\n* Alyssa paid $45\n* Total bill = $45 + $45 + $45 = $135\n\nNow, let's put this information together to find the total of the bill:\n\nTotal bill = $135\n\nSo, the total of the bill was $135.\n\nLet me know if you have any questions or if you'd like me to explain the steps in more detail!"
  },
  {
    "objectID": "posts/2023-11-02-LLM-Prompting/index.html#testing-all-prompts",
    "href": "posts/2023-11-02-LLM-Prompting/index.html#testing-all-prompts",
    "title": "Prompting LLMs Using Different Prompting Styles",
    "section": "Testing All Prompts",
    "text": "Testing All Prompts\nI have entered 20 questions (from the Appendix of the paper Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models (Lei Wang, et al)) into a CSV file:\n\ndf = pd.read_csv(\"/kaggle/input/questions5/questions.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      dataset\n      question\n    \n  \n  \n    \n      0\n      AQuA\n      The average wages of a worker during a fortnig...\n    \n    \n      1\n      AQuA\n      If 27 bottles of soda cost A cents, how much w...\n    \n    \n      2\n      GSM8K\n      James decides to run 3 sprints 3 times a week....\n    \n    \n      3\n      GSM8K\n      In a dance class of 20 students, 20% enrolled ...\n    \n    \n      4\n      MultiArith\n      The school cafeteria ordered 42 red apples and...\n    \n  \n\n\n\n\nTo prepare the questions for the prompts, I’ll prepend each question with \"Q:\" and append each question with \"\\n\\nA: <instruction>\" where <instruction> is the given instruction based on the type of prompt I’m constructing. For example, for Zero-shot prompting:\n\n\"Q: \" + df.question[1] + \"\\n\\nA: The answer is\"\n\n'Q: If 27 bottles of soda cost A cents, how much will B bottles cost in dollars? Answer Choices: (A) AB/2700 (B) 27/AB (C) AB/270 (D) 2700/AB (E) 100AB/27\\n\\nA: The answer is'\n\n\nI’ll use the same parameters for these models as the official chat interfaces published by the model authors.\nI’ll also create a copy of the DataFrame with questions in order to populate responses for each model.\n\nmodel_names = ['Mistral-7B-Instruct-v0.1', 'zephyr-7b-alpha', 'Llama-2-7b-chat-hf']\nprompting_styles = ['Zero-shot', 'Zero-shot-CoT', 'PS+']\ndfs = []\n\nfor name in model_names:\n    model_dfs = []\n    for style in prompting_styles:\n        # create a DataFrame for each prompting style\n        temp_df = df.copy()\n        temp_df['model'] = name\n        temp_df['prompting_style'] = style\n        temp_df['response'] = None\n        model_dfs.append(temp_df)\n    # Create a DataFrame for each model and store in a list\n    model_df = pd.concat(model_dfs)\n    dfs.append(model_df)\n\nI now have three DataFrames, one for each of my models, and each with 20 rows of questions for each prompting style.\n\ndfs[0].head()\n\n\n\n\n\n  \n    \n      \n      dataset\n      question\n      model\n      prompting_style\n      response\n    \n  \n  \n    \n      0\n      AQuA\n      The average wages of a worker during a fortnig...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      None\n    \n    \n      1\n      AQuA\n      If 27 bottles of soda cost A cents, how much w...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      None\n    \n    \n      2\n      GSM8K\n      James decides to run 3 sprints 3 times a week....\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      None\n    \n    \n      3\n      GSM8K\n      In a dance class of 20 students, 20% enrolled ...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      None\n    \n    \n      4\n      MultiArith\n      The school cafeteria ordered 42 red apples and...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      None\n    \n  \n\n\n\n\n\ndfs[1].head()\n\n\n\n\n\n  \n    \n      \n      dataset\n      question\n      model\n      prompting_style\n      response\n    \n  \n  \n    \n      0\n      AQuA\n      The average wages of a worker during a fortnig...\n      zephyr-7b-alpha\n      Zero-shot\n      None\n    \n    \n      1\n      AQuA\n      If 27 bottles of soda cost A cents, how much w...\n      zephyr-7b-alpha\n      Zero-shot\n      None\n    \n    \n      2\n      GSM8K\n      James decides to run 3 sprints 3 times a week....\n      zephyr-7b-alpha\n      Zero-shot\n      None\n    \n    \n      3\n      GSM8K\n      In a dance class of 20 students, 20% enrolled ...\n      zephyr-7b-alpha\n      Zero-shot\n      None\n    \n    \n      4\n      MultiArith\n      The school cafeteria ordered 42 red apples and...\n      zephyr-7b-alpha\n      Zero-shot\n      None\n    \n  \n\n\n\n\n\ndfs[2].head()\n\n\n\n\n\n  \n    \n      \n      dataset\n      question\n      model\n      prompting_style\n      response\n    \n  \n  \n    \n      0\n      AQuA\n      The average wages of a worker during a fortnig...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      None\n    \n    \n      1\n      AQuA\n      If 27 bottles of soda cost A cents, how much w...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      None\n    \n    \n      2\n      GSM8K\n      James decides to run 3 sprints 3 times a week....\n      Llama-2-7b-chat-hf\n      Zero-shot\n      None\n    \n    \n      3\n      GSM8K\n      In a dance class of 20 students, 20% enrolled ...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      None\n    \n    \n      4\n      MultiArith\n      The school cafeteria ordered 42 red apples and...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      None\n    \n  \n\n\n\n\nThere should be three instances of each question, one for each prompting style:\n\ndfs[0].groupby(dfs[0].question).count()['dataset'].unique(),\\\ndfs[1].groupby(dfs[1].question).count()['dataset'].unique(),\\\ndfs[2].groupby(dfs[2].question).count()['dataset'].unique()\n\n(array([3]), array([3]), array([3]))\n\n\nEach model’s DataFrame should have 20 rows for each of the three prompting styles:\n\ndfs[0].groupby(dfs[0].prompting_style).count()['dataset'].unique(),\\\ndfs[1].groupby(dfs[1].prompting_style).count()['dataset'].unique(),\\\ndfs[2].groupby(dfs[2].prompting_style).count()['dataset'].unique()\n\n(array([20]), array([20]), array([20]))\n\n\nAnd finally, I’ll check that each DataFrame should have Zero-shot, Zero-shot-CoT and PS+ as the prompting styles:\n\ndfs[0].prompting_style.unique(),\\\ndfs[1].prompting_style.unique(),\\\ndfs[2].prompting_style.unique()\n\n(array(['Zero-shot', 'Zero-shot-CoT', 'PS+'], dtype=object),\n array(['Zero-shot', 'Zero-shot-CoT', 'PS+'], dtype=object),\n array(['Zero-shot', 'Zero-shot-CoT', 'PS+'], dtype=object))\n\n\nFinally, I’ll create a dictionary to lookup the instruction for a given prompting style, to append to the question in the prompt:\n\ninstructions = {\n    'Zero-shot': 'The answer is',\n    'Zero-shot-CoT': \"Let's think step by step.\",\n    'PS+': \"Let’s first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let’s carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.\"\n}\n\n\ninstructions['PS+']\n\n'Let’s first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let’s carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.'\n\n\nWith my DataFrames with questions and prompting styles ready, I can prompt the models one at a time. The Kaggle GPUs were running out of memory if I tried to load more than one of these models, so I have to create a new session for each model run. I export the model responses into a CSV before stopping the session so I can combine all models’ responses into one DataFrame at the end to analyze the results.\n\nMistral-7B-Instruct-v0.1\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\")\n\n\nfor i in range(len(dfs[0])):\n    messages = [\n        {\"role\": \"user\", \"content\": \"Q: \" + dfs[0].question.iloc[i] + \"\\n\\nA: \" + instructions[dfs[0].prompting_style.iloc[i]] }\n    ]\n\n    pipe.use_default_system_prompt = False\n\n    prompt = pipe.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True)\n\n    outputs = pipe(\n        prompt,\n        do_sample=True,\n        temperature= 0.1,\n        max_new_tokens= 2048,\n        top_p= 0.95,\n        repetition_penalty= 1.2,\n        top_k= 50,\n        return_full_text= False)\n\n    dfs[0].response.iloc[i] = outputs[0][\"generated_text\"]\n\n\ndfs[0].head()\n\n\n\n\n\n  \n    \n      \n      dataset\n      question\n      model\n      prompting_style\n      response\n    \n  \n  \n    \n      0\n      AQuA\n      The average wages of a worker during a fortnig...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      To solve this problem, we need to use the for...\n    \n    \n      1\n      AQuA\n      If 27 bottles of soda cost A cents, how much w...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      To convert the number of bottles from one uni...\n    \n    \n      2\n      GSM8K\n      James decides to run 3 sprints 3 times a week....\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      To find out how many total meters James runs ...\n    \n    \n      3\n      GSM8K\n      In a dance class of 20 students, 20% enrolled ...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      Let's break down this problem step by step:\\n...\n    \n    \n      4\n      MultiArith\n      The school cafeteria ordered 42 red apples and...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      33 extra apples.\\n\\nHere's the reasoning behi...\n    \n  \n\n\n\n\n\ndfs[0].to_csv('mistral_responses.csv')\n\n\n\nzephyr-7b-alpha\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"HuggingFaceH4/zephyr-7b-alpha\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\")\n\n\nfor i in range(len(dfs[1])):\n    messages = [\n        {\"role\": \"user\", \"content\": \"Q: \" + dfs[1].question.iloc[i] + \"\\n\\nA: \" + instructions[dfs[1].prompting_style.iloc[i]] }\n    ]\n\n    pipe.use_default_system_prompt = False\n\n    prompt = pipe.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True)\n\n    outputs = pipe(\n        prompt,\n        do_sample=True,\n        temperature= 0.7,\n        max_new_tokens= 1024,\n        top_p= 0.95,\n        repetition_penalty= 1.2,\n        top_k= 50,\n        return_full_text= False)\n\n    dfs[1].response.iloc[i] = outputs[0][\"generated_text\"]\n\n\ndfs[1].head()\n\n\n\n\n\n  \n    \n      \n      dataset\n      question\n      model\n      prompting_style\n      response\n    \n  \n  \n    \n      0\n      AQuA\n      The average wages of a worker during a fortnig...\n      zephyr-7b-alpha\n      Zero-shot\n      The total earnings for the first seven days ar...\n    \n    \n      1\n      AQuA\n      If 27 bottles of soda cost A cents, how much w...\n      zephyr-7b-alpha\n      Zero-shot\n      (A) AB/2700 \\n\\nExplanation: Let's say the pri...\n    \n    \n      2\n      GSM8K\n      James decides to run 3 sprints 3 times a week....\n      zephyr-7b-alpha\n      Zero-shot\n      James runs 3 sprints of 60 meters each, three ...\n    \n    \n      3\n      GSM8K\n      In a dance class of 20 students, 20% enrolled ...\n      zephyr-7b-alpha\n      Zero-shot\n      Let's calculate this using math:\\n\\nFirstly, l...\n    \n    \n      4\n      MultiArith\n      The school cafeteria ordered 42 red apples and...\n      zephyr-7b-alpha\n      Zero-shot\n      Let's calculate the total number of apples (re...\n    \n  \n\n\n\n\n\ndfs[1].to_csv('zephyr_responses.csv')\n\n\n\nLlama-2-7b-chat-hf\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\")\n\n\nfor i in range(len(dfs[2])):\n    messages = [\n        {\"role\": \"user\", \"content\": \"Q: \" + dfs[2].question.iloc[i] + \"\\n\\nA: \" + instructions[dfs[2].prompting_style.iloc[i]] }\n    ]\n\n    pipe.use_default_system_prompt = False\n\n    prompt = pipe.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True)\n\n    outputs = pipe(\n        prompt,\n        do_sample=True,\n        temperature= 0.1,\n        max_new_tokens= 1024,\n        top_p= 0.95,\n        repetition_penalty= 1.2,\n        top_k= 50,\n        return_full_text= False)\n\n    dfs[2].response.iloc[i] = outputs[0][\"generated_text\"]\n\n\ndfs[2].head()\n\n\n\n\n\n  \n    \n      \n      dataset\n      question\n      model\n      prompting_style\n      response\n    \n  \n  \n    \n      0\n      AQuA\n      The average wages of a worker during a fortnig...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      To find the wage of the worker on the 8th da...\n    \n    \n      1\n      AQuA\n      If 27 bottles of soda cost A cents, how much w...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      To find the cost of B bottles in dollars, we...\n    \n    \n      2\n      GSM8K\n      James decides to run 3 sprints 3 times a week....\n      Llama-2-7b-chat-hf\n      Zero-shot\n      To find out how many total meters James runs...\n    \n    \n      3\n      GSM8K\n      In a dance class of 20 students, 20% enrolled ...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      To find out what percentage of the entire gr...\n    \n    \n      4\n      MultiArith\n      The school cafeteria ordered 42 red apples and...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      Great question! To find out how many extra a...\n    \n  \n\n\n\n\n\ndfs[2].to_csv('llama2_responses.csv')"
  },
  {
    "objectID": "posts/2023-11-02-LLM-Prompting/index.html#grading-the-responses",
    "href": "posts/2023-11-02-LLM-Prompting/index.html#grading-the-responses",
    "title": "Prompting LLMs Using Different Prompting Styles",
    "section": "Grading the Responses",
    "text": "Grading the Responses\nI really liked the way Johnathan Whitaker (on the Data Science Castnet YouTube channel) used gradio as a tool to check responses from an LLM, so I’m using the approach from his notebook below.\nFor each row of data, the gradio app displays the question, response and two buttons (“Correct” and “Incorrect”). If the answer is correct, I’ll press “Correct” and the is_correct column value will be set to 1. If it’s not, I’ll press “Incorrect” and is_correct will stay 0.\n\n#install gradio (quick fix for install error on colab)\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n!pip install -q gradio\n\nI’ll load the three CSVs (each with responses for 3 prompting styles across 20 questions from each model) into a single DataFrame, and sort it by the question column so it’s easier for me to grade the responses.\n\nimport gradio as gr, pandas as pd\nfrom functools import partial\n\ndfs = []\nfiles = ['llama2_responses.csv', 'mistral_responses.csv', 'zephyr_responses.csv']\n\nfor file in files:\n  df = pd.read_csv(file)\n  df = df.drop('Unnamed: 0', axis=1)\n  df['is_correct'] = 0\n  dfs.append(df)\n\ndf = pd.concat(dfs, axis=0, ignore_index=True)\n\n# sort by question so it's easier to grade\ndf = df.sort_values(by=['question'])\ndf = df.reset_index(drop=True)\n\n\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      dataset\n      question\n      model\n      prompting_style\n      response\n      is_correct\n    \n  \n  \n    \n      0\n      Coin Flip\n      'A coin is heads up. Lorena does not flip the ...\n      zephyr-7b-alpha\n      PS+\n      Problem understanding:\\n- The question asks wh...\n      0\n    \n    \n      1\n      Coin Flip\n      'A coin is heads up. Lorena does not flip the ...\n      zephyr-7b-alpha\n      Zero-shot\n      Yes, according to the given scenario, since no...\n      0\n    \n    \n      2\n      Coin Flip\n      'A coin is heads up. Lorena does not flip the ...\n      Llama-2-7b-chat-hf\n      Zero-shot\n      No, the coin is no longer heads up. Since no...\n      0\n    \n    \n      3\n      Coin Flip\n      'A coin is heads up. Lorena does not flip the ...\n      Mistral-7B-Instruct-v0.1\n      Zero-shot\n      Yes, the coin is still heads up.\\n\\nHere's th...\n      0\n    \n    \n      4\n      Coin Flip\n      'A coin is heads up. Lorena does not flip the ...\n      Llama-2-7b-chat-hf\n      PS+\n      Great! Let's tackle this problem together. H...\n      0\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe gradio app consists of a Textbox which will display the text of the question and response. I’ll modify the existing get_text function, which takes an index and returns a concatenated string.\nThe label function is called when the user clicks either of the buttons. If the button clicked has a value of \"Correct\" then the choice parameter will be \"correct\" and the is_correct column will be set to 1. The label function returns the next index and string to display.\n\ndef get_text(idx):\n    \"\"\" Combine the question and answer into a single string \"\"\"\n    global df\n    text = 'Q: ' + df.iloc[idx]['question'] + '\\n\\nA: ' + df.iloc[idx]['response']\n\n    return text\n\ndef label(idx, choice='correct'):\n    \"\"\" Set `is_correct` column to `1` if response is correct \"\"\"\n    if choice == 'correct':\n        df.loc[idx, 'is_correct'] = 1\n    return idx+1, get_text(idx+1) # Move on to next one\n\nIn order to avoid the gradio app from timing out, I have called queue on the demo object before I launch it.\n\nwith gr.Blocks() as demo:\n    starting_idx = 0\n    with gr.Column():\n        idx = gr.Slider(value=starting_idx, label=\"idx\") # Progress bar, borrowed the idea from https://www.kaggle.com/code/nbroad/create-science-wikipedia-dataset\n        text = gr.Textbox(value=get_text(0), label=\"text\")\n    with gr.Row():\n        correct_btn = gr.Button(value=\"Correct\")\n        incorrect_btn = gr.Button(value=\"Incorrect\")\n\n    correct_btn.click(fn=partial(label, choice='correct'), inputs=[idx], outputs=[idx, text])\n    incorrect_btn.click(fn=partial(label, choice='incorrect'), inputs=[idx], outputs=[idx, text])\n\n#demo.launch(debug=True, show_error=True)\ndemo.queue().launch(share=True, show_error=True)\n\nHere is an example of what the gradio app looks like:\n\n\n\nA screenshot of my gradio app used to grade LLM responses\n\n\n\ndf.to_csv('graded_responses.csv', index=False)"
  },
  {
    "objectID": "posts/2023-11-02-LLM-Prompting/index.html#results",
    "href": "posts/2023-11-02-LLM-Prompting/index.html#results",
    "title": "Prompting LLMs Using Different Prompting Styles",
    "section": "Results",
    "text": "Results\nWith all of the model responses graded, I can now calculate the percentage of correct responses from each model for each dataset and prompting style.\n\ndf.groupby(['model', 'prompting_style'])['is_correct'].mean() * 100\n\nmodel                     prompting_style\nLlama-2-7b-chat-hf        PS+                25.0\n                          Zero-shot          45.0\n                          Zero-shot-CoT      30.0\nMistral-7B-Instruct-v0.1  PS+                60.0\n                          Zero-shot          60.0\n                          Zero-shot-CoT      45.0\nzephyr-7b-alpha           PS+                30.0\n                          Zero-shot          45.0\n                          Zero-shot-CoT      45.0\nName: is_correct, dtype: float64\n\n\nOverall, the best performing model and prompting style (60% correct) was the Mistral-7B-Instruct-v0.1 model using the PS+ and Zero-shot prompting styles. Mistral was the only model where PS+ outperformed Zero-shot and Zero-shot-CoT.\n\ndf.groupby(['model'])['is_correct'].mean() * 100\n\nmodel\nLlama-2-7b-chat-hf          33.333333\nMistral-7B-Instruct-v0.1    55.000000\nzephyr-7b-alpha             40.000000\nName: is_correct, dtype: float64\n\n\nOverall the best performing model across all prompting styles was Mistral (55%) following by Zephyr (40%) and Llama-2 coming in last at 33.33%.\n\ndf.groupby(['dataset'])['is_correct'].mean() * 100\n\ndataset\nAQuA             5.555556\nAddSub          77.777778\nCSQA            27.777778\nCoin Flip       77.777778\nGSM8K           50.000000\nLast Letters     0.000000\nMultiArith      38.888889\nSVAMP           77.777778\nSingleEq        72.222222\nStrategyQA       0.000000\nName: is_correct, dtype: float64\n\n\nAcross all models and prompting styles, the highest performing datasets (78% correct) were AddSub, Coin Flip and SVAMP. The lowest performing datasets (0% correct) were Last Letters and StrategyQA."
  },
  {
    "objectID": "posts/2023-11-02-LLM-Prompting/index.html#final-thoughts",
    "href": "posts/2023-11-02-LLM-Prompting/index.html#final-thoughts",
    "title": "Prompting LLMs Using Different Prompting Styles",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI want to reiterate that the purpose of this exercise for me to get some practice prompting LLMs. I also got some experience setting up a gradio app to help me evaluate the responses of the models.\nGiven the small sample size for each model (20 questions and responses for each prompting style) I don’t think these results can be used to conclude anything about the performance of these prompting styles and models. Yes, Mistral and Zephyr out-performed Llama-2 on these 20 questions across these 3 particular prompting styles, but testing the models on another thousand questions might yield different results.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "",
    "text": "In this notebook I’ll use BM25 (full text search in sqlite) to answer questions about Chapter 1 of the freely available fastai textbook.\nThis is part of a larger “fastbookRAG” project that I’m working on, where I’ll use BM25, cosine similarity and an LLM (probably phi-3) to answer questions from each of the book chapters’ “Questionnaire” section (from Part 1 of the fastai course).\n\n\nShow imports\nimport sqlite3\nimport json\nimport re\nimport pandas as pd, numpy as np\nimport textwrap\n\nwrapper = textwrap.TextWrapper(\n    width=50,\n    replace_whitespace=False,\n    drop_whitespace=False)\n\ndef print_wrap_text(text):\n  print(\"\\n\".join(wrapper.fill(line) for line in text.splitlines()))\n\n\nHere is a summary of the results of this notebook’s experiments:\n\n\n\n\n\n\n\n\nTop BM25 Ranked Chunks Retrieved\nChunk Size\nRetrieved Context Relevancy*\n\n\n\n\ntop-3\nLarge\n72%\n\n\ntop-1\nLarge\n54%\n\n\ntop-3\nSmall\n54%\n\n\ntop-1\nSmall\n40%\n\n\n\n*Retrieved Context Relevancy:The percentage of questions for which the retrieved context was relevant and sufficient for me to answer the question."
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#chunking-the-chapter-1-jupyter-notebook-by-paragraph",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#chunking-the-chapter-1-jupyter-notebook-by-paragraph",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Chunking the Chapter 1 Jupyter Notebook by Paragraph",
    "text": "Chunking the Chapter 1 Jupyter Notebook by Paragraph\nThe first task at hand is to load the Chapter 1 Jupyter Notebook into a sqlite database (so that I can perform full text search on it). There are a few different ways to do this:\n\nLoad in the entire chapter as a single string of text\nChunk the chapter text based on headers\nChunk the chapter text based on paragraphs (text between line breaks)\n\nTo start, I’ll combine the second and third options and chunk the text based on paragraphs plus include the header at the start of the string. So for example the following text:\n\n\nThis is a Header\nThis is pargraph one. It has two sentences.\nThis is paragraph two. It has two sentences as well.\n\nwill get chunked into the following strings:\n# string 1\n\"\"\"### This is a Header\n\nThis is pargraph one. It has two sentences.\n\"\"\"\n\n# string 2\n\"\"\"### This is a Header\n\nThis is paragraph two. It has two sentences as well.\n\"\"\"\nIn this way I am capturing the granular information (the paragraph text) along with the big picture theme (the header). I suppose this is one way of capturing metadata about the text.\nAfter a few iterations of feedback, I got the following code from Claude Sonnet-3.5 to chunk a .ipynb file into a list of strings.\n\n\nShow the chunking code\ndef get_chunks(notebook_path):\n    with open(notebook_path, 'r', encoding='utf-8') as file:\n        notebook = json.load(file)\n\n    chunks = []\n    current_header = \"\"\n\n    def add_chunk(content):\n        if content.strip():\n            chunks.append(f\"{current_header}\\n\\n{content.strip()}\")\n\n    for cell in notebook['cells']:\n        if cell['cell_type'] == 'markdown':\n            content = ''.join(cell['source'])\n            header_match = re.match(r'^(#+\\s+.*?)$', content, re.MULTILINE)\n            if header_match:  # Check if the cell starts with a header\n                current_header = header_match.group(1)\n                # Add any content after the header in the same cell\n                remaining_content = content[len(current_header):].strip()\n                if remaining_content:\n                    paragraphs = re.split(r'\\n\\s*\\n', remaining_content)\n                    for paragraph in paragraphs:\n                        add_chunk(paragraph)\n            else:\n                paragraphs = re.split(r'\\n\\s*\\n', content)\n                for paragraph in paragraphs:\n                    add_chunk(paragraph)\n        elif cell['cell_type'] == 'code':\n            code_content = '```python\\n' + ''.join(cell['source']) + '\\n```'\n            add_chunk(code_content)\n\n    return chunks\n\n\nUsing this chunking strategy, Chapter 1 has 315 chunks of text. For reference, this chapter has 24 headers (of different levels) across 52 pages. That’s about 13 chunks per header and 6 chunks per page.\n\nnotebook_path = '01_intro.ipynb'\nchunks = get_chunks(notebook_path)\nlen(chunks)\n\n315\n\n\nHere’s an example of one of the chunks:\n\nprint_wrap_text(chunks[30])\n\n## Who We Are\n\nSylvain, on the other hand, knows a lot about \nformal technical education. In fact, he has \nwritten 10 math textbooks, covering the entire \nadvanced French maths curriculum!"
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#load-the-chunks-into-sqlite-database",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#load-the-chunks-into-sqlite-database",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Load the Chunks into SQLite Database",
    "text": "Load the Chunks into SQLite Database\nI’ll load the list of chunks into a sqlite database virtual table with a single column text that has sqlite’s full text search (FTS5) enabled.\nI’ll filter out the two sections that I don’t want to show up in the results: the “Questionnaire” section (as the keyword search will match the questions closely) and the “Further Research” sections (as it comes after the “Questionnaire” section and is not part of the main body of the chapter).\n\ndef filter_chunks(chunks, exclude_headers):\n    filtered_chunks = []\n    for chunk in chunks:\n        lines = chunk.split('\\n')\n        # Check if the first line (header) is in the exclude list\n        if not any(header in lines[0] for header in exclude_headers):\n            filtered_chunks.append(chunk)\n    return filtered_chunks\n\nexclude_headers = [\"Questionnaire\", \"Further Research\"]\nfiltered_chunks = filter_chunks(chunks, exclude_headers)\n\n\n[chunk for chunk in filtered_chunks if 'Questionnaire' in chunk]\n\n[]\n\n\n\n[chunk for chunk in filtered_chunks if 'Further Research' in chunk]\n\n[]\n\n\n\nconn = sqlite3.connect('fastbook.db')\n\n\ncur = conn.cursor()\nres = cur.execute(\"\"\"\n\nCREATE VIRTUAL TABLE fastbook_text\nUSING FTS5(text);\n\n\"\"\")\n\n\nfor string in filtered_chunks:\n  cur.execute(f\"INSERT INTO fastbook_text(text) VALUES (?)\", (string,))\n\n\nres = cur.execute(\"SELECT * from fastbook_text LIMIT 40\")\nprint_wrap_text(res.fetchall()[30][0])\n\n## Who We Are\n\nSylvain, on the other hand, knows a lot about \nformal technical education. In fact, he has \nwritten 10 math textbooks, covering the entire \nadvanced French maths curriculum!"
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#retrieving-context-for-an-llm-with-keyword-search",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#retrieving-context-for-an-llm-with-keyword-search",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Retrieving Context for an LLM with Keyword Search",
    "text": "Retrieving Context for an LLM with Keyword Search\nIn the fastai textbook, each chapter ends with a “Questionnaire” section. It’s like a review quiz for the chapter content. While the answers to these questions are not always fixed, I’ll use official solutions provided on the fastai forums as the “gold standard” for my evals. I have saved a CSV with the questionnaire question text, gold standard answer and a list of keywords I wrote for each question in this gist. Here’s a sample:\n\ndf = pd.read_csv(\"https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/bc6cd2ab15b64a92ec23796c61702f413fdd2b40/fastbookRAG_evals.csv\")\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n    \n    \n      3\n      1\n      4\n      Based on the book of the same name, what are t...\n      A set of processing units\\\\nA state of activat...\n      parallel, distributed, processing, requirement...\n    \n    \n      4\n      1\n      5\n      What were the two theoretical misunderstanding...\n      In 1969, Marvin Minsky and Seymour Papert demo...\n      theoretical, misunderstandings, held, back, fi...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nHere’s the first question/answer pair:\nQuestion:\nDo you need these for deep learning?\n\nLots of math T / F\nLots of data T / F\nLots of expensive computers T / F\nA PhD T / F\n\nGold Standard Answer:\n\nLots of math - False\nLots of data - False\nLots of expensive computers - False\nA PhD - False\n\nHere’s the keyword search for the first question using the keywords I came up with (“math, data, expensive computers, PhD”):\n\nres = cur.execute(\"\"\"\n\nSELECT *, rank\n  from fastbook_text\nWHERE fastbook_text MATCH '\"math\" OR \"data\" OR \"expensive computers\" OR \"PhD\"'\nORDER BY rank\nLIMIT 5\n\n\"\"\")\n\n## Deep Learning Is for Everyone\n\nasciidoc\n[[myths]]\n.What you don't need to do deep learning\n[options=\"header\"]\n|======\n| Myth (don't need) | Truth\n| Lots of math | Just high school math is \nsufficient\n| Lots of data | We've seen record-breaking \nresults with <50 items of data\n| Lots of expensive computers | You can get what \nyou need for state of the art work for free\n|======\nI would imagine that given the question and this corresponding chunk of context from the textbook, an LLM could answer the question correctly.\nAnd that’s the metric that I’ll use to evaluate keyword search for the fastai textbook: can a reasonably capable LLM be able to answer the question given this context?\nI’ll iterate through the column of keywords (one set for each question) and store the top BM25-ranked result as the “retrieved context” from my database:\n\n\nShow the for-loop code\nresults = []\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text\n    WHERE fastbook_text MATCH '{words}'\n    ORDER BY rank\n    LIMIT 5\n\n    \"\"\"\n\n    res = cur.execute(q)\n    results.append(res.fetchall()[0][0])\n  else:\n    # if keywords == \"No Answer\"\n    res = \"No answer\"\n    results.append(res)\n\n\n\nresults = pd.Series(results)\ndf['fts5_result'] = results\n\n\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      fts5_result\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\n<img al...\n    \n    \n      3\n      1\n      4\n      Based on the book of the same name, what are t...\n      A set of processing units\\\\nA state of activat...\n      parallel, distributed, processing, requirement...\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n    \n    \n      4\n      1\n      5\n      What were the two theoretical misunderstanding...\n      In 1969, Marvin Minsky and Seymour Papert demo...\n      theoretical, misunderstandings, held, back, fi...\n      ## Neural Networks: A Brief History\\n\\nIn the ..."
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#evaluating-retrieved-context",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#evaluating-retrieved-context",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Evaluating Retrieved Context",
    "text": "Evaluating Retrieved Context\nUsing the top-ranked BM25 result for the given keyword search, I was able to retrieve the context necessary to answer the question 13 out of 33 times, or 40% of the time.\nLooking through these 33 question/context pairs, I saw some patterns noted by the following examples.\nIn some cases, like for question #5, the retrieved context answered half of the problem.\n\ndf.iloc[4]\n\n\n\n  \n    \n      \n      4\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      5\n    \n    \n      question_text\n      What were the two theoretical misunderstandings that held back the field of neural networks?\n    \n    \n      answer\n      In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, “Perceptrons”, that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\\\\n\\\\nIn the 1980’s, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential.\n    \n    \n      keywords\n      theoretical, misunderstandings, held, back, field, neural network\n    \n    \n      fts5_result\n      ## Neural Networks: A Brief History\\n\\nIn the 1980's most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their \"\"pattern of connectivity among units,\"\" to use the framework above). And indeed, neural networks were widely used during the '80s and '90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful.\n    \n  \ndtype: object\n\n\n\nThe retrieved context for question #16 was not wrong per se, but it didn’t answer the question in the same way as the gold standard answer. The “correct” answer to “What do you need in order to train a model?” is from the perspective of the model builder, whereas the retrieved context answers the same question but from the perspective of a business or organization.\n\ndf.iloc[15]\n\n\n\n  \n    \n      \n      15\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      16\n    \n    \n      question_text\n      What do you need in order to train a model?\n    \n    \n      answer\n      You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer).\n    \n    \n      keywords\n      train, model, need\n    \n    \n      fts5_result\n      ### Limitations Inherent To Machine Learning\\n\\n- A model cannot be created without data.\\n- A model can only learn to operate on the patterns seen in the input data used to train it.\\n- This learning approach only creates *predictions*, not recommended *actions*.\\n- It's not enough to just have examples of input data; we need *labels* for that data too (e.g., pictures of dogs and cats aren't enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n    \n  \ndtype: object\n\n\n\nIn some cases, I was on the fence about the relevancy or accuracy of the retrieved context. I erred on the side of caution and considered the following question/answer/context triple as insufficient context:\n\ndf.iloc[24]\n\n\n\n  \n    \n      \n      24\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      25\n    \n    \n      question_text\n      How can pretrained models help?\n    \n    \n      answer\n      Pretrained models have been trained on other problems that may be quite similar to the current task. For example, pretrained image recognition models were often trained on the ImageNet dataset, which has 1000 classes focused on a lot of different types of visual objects. Pretrained models are useful because they have already learned how to handle a lot of simple features like edge and color detection. However, since the model was trained for a different task than already used, this model cannot be used as is.\n    \n    \n      keywords\n      pretrained, model, help\n    \n    \n      fts5_result\n      ### What Our Image Recognizer Learned\\n\\nWhen we fine-tuned our pretrained model earlier, we adapted what those last layers focus on (flowers, humans, animals) to specialize on the cats versus dogs problem. More generally, we could specialize such a pretrained model on many different tasks. Let's have a look at some examples.\n    \n  \ndtype: object\n\n\n\nFor a number of questions, the keyword search resulted in an HTML img tag or code snippet since it contained the necessary keywords:\n\ndf.iloc[2]\n\n\n\n  \n    \n      \n      2\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      3\n    \n    \n      question_text\n      What was the name of the first device that was based on the principle of the artificial neuron?\n    \n    \n      answer\n      Mark I perceptron built by Frank Rosenblatt\n    \n    \n      keywords\n      first, device, artificial, neuron\n    \n    \n      fts5_result\n      ## Neural Networks: A Brief History\\n\\n<img alt=\"\"Natural and artificial neurons\"\" width=\"\"500\"\" caption=\"\"Natural and artificial neurons\"\" src=\"\"images/chapter7_neuron.png\"\" id=\"\"neuron\"\"/>\n    \n  \ndtype: object\n\n\n\ndf.iloc[17]\n\n\n\n  \n    \n      \n      17\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      18\n    \n    \n      question_text\n      Do we always have to use 224×224-pixel images with the cat recognition model?\n    \n    \n      answer\n      No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption.\n    \n    \n      keywords\n      224, pixel, image, cat, recognition, model\n    \n    \n      fts5_result\n      ### How Our Image Recognizer Works\\n\\n```python\\ndls = ImageDataLoaders.from_name_func(\\n    path, get_image_files(path), valid_pct=0.2, seed=42,\\n    label_func=is_cat, item_tfms=Resize(224))\\n```\n    \n  \ndtype: object\n\n\n\nIn some cases, the retrieved context seemed to be the chunk right before the chunk that would describe the solution in the text. This makes me wonder if my chunk sizes are too small?\n\ndf.iloc[9]\n\n\n\n  \n    \n      \n      9\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      10\n    \n    \n      question_text\n      Why is it hard to use a traditional computer program to recognize images in a photo?\n    \n    \n      answer\n      For us humans, it is easy to identify images in a photos, such as identifying cats vs dogs in a photo. This is because, subconsciously our brains have learned which features define a cat or a dog for example. But it is hard to define set rules for a traditional computer program to recognize a cat or a dog. Can you think of a universal rule to determine if a photo contains a cat or dog? How would you encode that as a computer program? This is very difficult because cats, dogs, or other objects, have a wide variety of shapes, textures, colors, and other features, and it is close to impossible to manually encode this in a traditional computer program.\n    \n    \n      keywords\n      image, recognize, recognition, traditional, computer, program\n    \n    \n      fts5_result\n      ### What Is Machine Learning?\\n\\n```python\\n#hide_input\\n#caption A traditional program\\n#id basic_program\\n#alt Pipeline inputs, program, results\\ngv('''program[shape=box3d width=1 height=0.7]\\ninputs->program->results''')\\n```\n    \n  \ndtype: object\n\n\n\ndf.iloc[10]\n\n\n\n  \n    \n      \n      10\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      11\n    \n    \n      question_text\n      What did Samuel mean by \"\"\"\"\"\"\"\"weight assignment\"\"\"\"\"\"\"\"?\n    \n    \n      answer\n      “weight assignment” refers to the current values of the model parameters. Arthur Samuel further mentions an “ automatic means of testing the effectiveness of any current weight assignment ” and a “ mechanism for altering the weight assignment so as to maximize the performance ”. This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance.\n    \n    \n      keywords\n      Samuel, weight, assignment\n    \n    \n      fts5_result\n      ### What Is Machine Learning?\\n\\nLet us take these concepts one by one, in order to understand how they fit together in practice. First, we need to understand what Samuel means by a *weight assignment*.\n    \n  \ndtype: object"
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#including-more-chunks-during-retrieval",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#including-more-chunks-during-retrieval",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Including More Chunks During Retrieval",
    "text": "Including More Chunks During Retrieval\nBased on the initial keyword search results, some of the retrieved chunks of context either partially answer the question, or only begin to setup the answer to the question. I see two routes of remediating this:\n\nIncrease the chunk size (i.e. choose a different chunking strategy)\nIncrease the number of chunks selected during keyword search\n\nThe second approachs seems easier to implement. I like the idea of retrieving more than 1 small chunks than a single large chunk. I can imagine a couple of trade-offs:\n\nA few small chunks may not capture information that is spread across a long paragraph/section needed for the LLM to answer the question sufficiently.\nSingle large chunk may include information irrelevant to the question and thus introduce noise into the answer, confusing the LLM.\n\nI’ll retrieve the top 3 BM25-ranked results and then evaluate them.\n\n\nShow the for-loop code\nresults = []\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text\n    WHERE fastbook_text MATCH '{words}'\n    ORDER BY rank\n    LIMIT 3\n\n    \"\"\"\n\n    res = cur.execute(q)\n    results.append(res.fetchall())\n  else:\n    # if keywords == \"No Answer\"\n    res = (\"No answer\")\n    results.append(res)\n\n\n\ndf['fts5_result2'] = results\n\n\ndf.head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      fts5_result2\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      [(## Deep Learning Is for Everyone\\n\\n```ascii...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      [(## Deep Learning Is for Everyone\\n\\nHere's a...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      [(## Neural Networks: A Brief History\\n\\n<img ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntop_3_results = df['fts5_result2'].apply(pd.Series)\ntop_3_results.columns = [f'fts5_result2_{i+1}' for i in range(top_3_results.shape[1])]\n\n\nfor col in ['fts5_result2_1', 'fts5_result2_2', 'fts5_result2_3']:\n    top_3_results[col] = top_3_results[col].apply(lambda x: x[0] if isinstance(x, tuple) else x)\n\n\ntop_3_results.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      fts5_result2_1\n      fts5_result2_2\n      fts5_result2_3\n    \n  \n  \n    \n      0\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n      ## How to Learn Deep Learning\\n\\nPaul Lockhart...\n      ## Who We Are\\n\\nAll this means that between u...\n    \n    \n      1\n      ## Deep Learning Is for Everyone\\n\\nHere's a l...\n      ## How to Learn Deep Learning\\n\\n- Teaching th...\n      ## Deep Learning Is for Everyone\\n\\n```asciido...\n    \n    \n      2\n      ## Neural Networks: A Brief History\\n\\n<img al...\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n      ## Neural Networks: A Brief History\\n\\nIn 1943...\n    \n    \n      3\n      ## Neural Networks: A Brief History\\n\\nIn fact...\n      ## Neural Networks: A Brief History\\n\\nPerhaps...\n      ## Neural Networks: A Brief History\\n\\nWe will...\n    \n    \n      4\n      ## Neural Networks: A Brief History\\n\\nIn the ...\n      ### What Is a Neural Network?\\n\\nHaving zoomed...\n      ## Deep Learning Is for Everyone\\n\\nBut neural...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf = pd.concat([df, top_3_results], axis=1)\ndf.to_csv('top-3-retrieval-results.csv', index=False)\n\nUsing the top 3 BM25 ranked chunks improved the results! 18 out of the 33 questions, or 54%, are now answerable with the given retrieved context.\nHere’s an example of a question that I couldn’t answer with the top-1 result but I can answer with the second-ranked result (fts5_result2_2):\n\ndf.iloc[19]\n\n\n\n  \n    \n      \n      19\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      20\n    \n    \n      question_text\n      What is a validation set? What is a test set? Why do we need them?\n    \n    \n      answer\n      The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training, in order to prevent overfitting. This ensures that the model performance is not due to “cheating” or memorization of the dataset, but rather because it learns the appropriate features to use for prediction. However, it is possible that we overfit the validation data as well. This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance. Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model. This splitting of the dataset is necessary to ensure that the model generalizes to unseen data.\n    \n    \n      keywords\n      validation, set, test\n    \n    \n      fts5_result2\n      [(## Validation Sets and Test Sets\\n\\nTo avoid this, our first step was to split our dataset into two sets: the *training set* (which our model sees in training) and the *validation set*, also known as the *development set* (which is used only for evaluation). This lets us test that the model learns lessons from the training data that generalize to new data, the validation data., -9.62686734459166), (## Validation Sets and Test Sets\\n\\nHaving two levels of \"reserved data\"—a validation set and a test set, with one level representing data that you are virtually hiding from yourself—may seem a bit extreme. But the reason it is often necessary is because models tend to gravitate toward the simplest way to do good predictions (memorization), and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn't mean we *always* need a separate test set—if you have very little data, you may need to just have a validation set—but generally it's best to use one if at all possible., -9.21394932308204), (### Use Judgment in Defining Test Sets\\n\\nInstead, use the earlier data as your training set (and the later data for the validation set), as shown in <<timeseries3>>., -9.06398171911968)]\n    \n    \n      fts5_result2_1\n      ## Validation Sets and Test Sets\\n\\nTo avoid this, our first step was to split our dataset into two sets: the *training set* (which our model sees in training) and the *validation set*, also known as the *development set* (which is used only for evaluation). This lets us test that the model learns lessons from the training data that generalize to new data, the validation data.\n    \n    \n      fts5_result2_2\n      ## Validation Sets and Test Sets\\n\\nHaving two levels of \"reserved data\"—a validation set and a test set, with one level representing data that you are virtually hiding from yourself—may seem a bit extreme. But the reason it is often necessary is because models tend to gravitate toward the simplest way to do good predictions (memorization), and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn't mean we *always* need a separate test set—if you have very little data, you may need to just have a validation set—but generally it's best to use one if at all possible.\n    \n    \n      fts5_result2_3\n      ### Use Judgment in Defining Test Sets\\n\\nInstead, use the earlier data as your training set (and the later data for the validation set), as shown in <<timeseries3>>.\n    \n  \ndtype: object"
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#increasing-chunk-size",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#increasing-chunk-size",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Increasing Chunk Size",
    "text": "Increasing Chunk Size\nUsing 3 chunks of context instead of 1 increased the performance of retrieval from 40% to 54%, meaning that I would be able to answer the Chapter 1 questions with the retrieved context 54% of the time. I’ll call this metric “retrieved context relevancy”.\nI’ll now increase the chunk size and see how that affects performance:\n\nlarger_chunks = [\"\\n\".join(filtered_chunks[i:i+3]) for i in range(0, len(filtered_chunks), 3)]\n\nNow I’ll create a separate table, fastbook_text_large in my database to hold these chunks:\n\ncur = conn.cursor()\nres = cur.execute(\"\"\"\n\nCREATE VIRTUAL TABLE fastbook_text_large\nUSING FTS5(text);\n\n\"\"\")\n\n\nfor string in larger_chunks:\n  cur.execute(f\"INSERT INTO fastbook_text_large(text) VALUES (?)\", (string,))\n\nres = cur.execute(\"SELECT * from fastbook_text_large LIMIT 2\")\n\nThe outputs (which include Markdown) were messing up my quarto blog rendering so I’ve excluded it.\nI’ll now iterate through my list of questions, passing the corresponding keywords to the query to conduct a full text search:\n\n\nShow the for-loop code\nresults = []\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text_large\n    WHERE fastbook_text_large MATCH '{words}'\n    ORDER BY rank\n    LIMIT 1\n\n    \"\"\"\n\n    res = cur.execute(q)\n    results.append(res.fetchall()[0][0])\n  else:\n    # if keywords == \"No Answer\"\n    res = (\"No answer\")\n    results.append(res)\n\n\n\nlarge_df = df.drop(['fts5_result2', 'fts5_result2_1', 'fts5_result2_2', 'fts5_result2_3'], axis=1)\nlarge_df['large_chunk_result'] = results\nlarge_df.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n      large_chunk_result\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\\\n\\\\n- Lo...\n      Lots of math - False\\\\nLots of data - False\\\\n...\n      math, data, expensive computers, PhD\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      Any five of the following:\\\\nNatural Language ...\n      deep learning, state of the art, best, world\n      ## Deep Learning Is for Everyone\\n\\nA lot of p...\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      Mark I perceptron built by Frank Rosenblatt\n      first, device, artificial, neuron\n      ## Neural Networks: A Brief History\\n\\nRosenbl...\n    \n    \n      3\n      1\n      4\n      Based on the book of the same name, what are t...\n      A set of processing units\\\\nA state of activat...\n      parallel, distributed, processing, requirement...\n      ## Neural Networks: A Brief History\\n\\n> : Peo...\n    \n    \n      4\n      1\n      5\n      What were the two theoretical misunderstanding...\n      In 1969, Marvin Minsky and Seymour Papert demo...\n      theoretical, misunderstandings, held, back, fi...\n      ## Neural Networks: A Brief History\\n\\n1. A se...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nlarge_df.to_csv('large_chunk_results.csv', index=False)\n\nUsing larger chunks resulted in a retrieved context relevancy of 54% (18/33). However, these were not the same 18 results as before.\nFor example, here’s a question for which using larger chunks retrieved the right context (whereas retrieving three smaller chunks did not):\n\nlarge_df.iloc[10][['question_text', 'answer', 'large_chunk_result']]\n\n\n\n  \n    \n      \n      10\n    \n  \n  \n    \n      question_text\n      What did Samuel mean by \"\"weight assignment\"\"?\n    \n    \n      answer\n      “weight assignment” refers to the current values of the model parameters. Arthur Samuel further mentions an “ automatic means of testing the effectiveness of any current weight assignment ” and a “ mechanism for altering the weight assignment so as to maximize the performance ”. This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance.\n    \n    \n      large_chunk_result\n      ### What Is Machine Learning?\\n\\n- The idea of a \"weight assignment\" \\n- The fact that every weight assignment has some \"actual performance\"\\n- The requirement that there be an \"automatic means\" of testing that performance,  \\n- The need for a \"mechanism\" (i.e., another automatic process) for improving the performance by changing the weight assignments\\n### What Is Machine Learning?\\n\\nLet us take these concepts one by one, in order to understand how they fit together in practice. First, we need to understand what Samuel means by a *weight assignment*.\\n### What Is Machine Learning?\\n\\nWeights are just variables, and a weight assignment is a particular choice of values for those variables. The program's inputs are values that it processes in order to produce its results—for instance, taking image pixels as inputs, and returning the classification \"dog\" as a result. The program's weight assignments are other values that define how the program will operate.\n    \n  \ndtype: object\n\n\n\ndf.iloc[10][['question_text', 'answer', 'fts5_result2_1', 'fts5_result2_2', 'fts5_result2_3']]\n\n\n\n  \n    \n      \n      10\n    \n  \n  \n    \n      question_text\n      What did Samuel mean by \"\"weight assignment\"\"?\n    \n    \n      answer\n      “weight assignment” refers to the current values of the model parameters. Arthur Samuel further mentions an “ automatic means of testing the effectiveness of any current weight assignment ” and a “ mechanism for altering the weight assignment so as to maximize the performance ”. This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance.\n    \n    \n      fts5_result2_1\n      ### What Is Machine Learning?\\n\\nLet us take these concepts one by one, in order to understand how they fit together in practice. First, we need to understand what Samuel means by a *weight assignment*.\n    \n    \n      fts5_result2_2\n      ### What Is Machine Learning?\\n\\n```python\\n#hide_input\\n#caption A program using weight assignment\\n#id weight_assignment\\ngv('''model[shape=box3d width=1 height=0.7]\\ninputs->model->results; weights->model''')\\n```\n    \n    \n      fts5_result2_3\n      ### What Is Machine Learning?\\n\\n- The idea of a \"weight assignment\" \\n- The fact that every weight assignment has some \"actual performance\"\\n- The requirement that there be an \"automatic means\" of testing that performance,  \\n- The need for a \"mechanism\" (i.e., another automatic process) for improving the performance by changing the weight assignments\n    \n  \ndtype: object\n\n\n\nHere’s another question where larger chunks resulted in the correct retrieval (whereas 3 smaller chunks did not):\n\nlarge_df.iloc[17][['question_text', 'answer', 'large_chunk_result']]\n\n\n\n  \n    \n      \n      17\n    \n  \n  \n    \n      question_text\n      Do we always have to use 224×224-pixel images with the cat recognition model?\n    \n    \n      answer\n      No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption.\n    \n    \n      large_chunk_result\n      ### How Our Image Recognizer Works\\n\\nFinally, we define the `Transform`s that we need. A `Transform` contains code that is applied automatically during training; fastai includes many predefined `Transform`s, and adding new ones is as simple as creating a Python function. There are two kinds: `item_tfms` are applied to each item (in this case, each item is resized to a 224-pixel square), while `batch_tfms` are applied to a *batch* of items at a time using the GPU, so they're particularly fast (we'll see many examples of these throughout this book).\\n### How Our Image Recognizer Works\\n\\nWhy 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you'll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size.\\n### How Our Image Recognizer Works\\n\\n> Note: Classification and Regression: _classification_ and _regression_ have very specific meanings in machine learning. These are the two main types of model that we will be investigating in this book. A classification model is one which attempts to predict a class, or category. That is, it's predicting from a number of discrete possibilities, such as \"dog\" or \"cat.\" A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location. Sometimes people use the word _regression_ to refer to a particular kind of model called a _linear regression model_; this is a bad practice, and we won't be using that terminology in this book!\n    \n  \ndtype: object\n\n\n\ndf.iloc[17][['question_text', 'answer', 'fts5_result2_1', 'fts5_result2_2', 'fts5_result2_3']]\n\n\n\n  \n    \n      \n      17\n    \n  \n  \n    \n      question_text\n      Do we always have to use 224×224-pixel images with the cat recognition model?\n    \n    \n      answer\n      No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption.\n    \n    \n      fts5_result2_1\n      ### How Our Image Recognizer Works\\n\\n```python\\ndls = ImageDataLoaders.from_name_func(\\n    path, get_image_files(path), valid_pct=0.2, seed=42,\\n    label_func=is_cat, item_tfms=Resize(224))\\n```\n    \n    \n      fts5_result2_2\n      ### Running Your First Notebook\\n\\n```python\\n#id first_training\\n#caption Results from the first training\\n# CLICK ME\\nfrom fastai.vision.all import *\\npath = untar_data(URLs.PETS)/'images'\\n\\ndef is_cat(x): return x[0].isupper()\\ndls = ImageDataLoaders.from_name_func(\\n    path, get_image_files(path), valid_pct=0.2, seed=42,\\n    label_func=is_cat, item_tfms=Resize(224))\\n\\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\\nlearn.fine_tune(1)\\n```\n    \n    \n      fts5_result2_3\n      ### How Our Image Recognizer Works\\n\\nFinally, we define the `Transform`s that we need. A `Transform` contains code that is applied automatically during training; fastai includes many predefined `Transform`s, and adding new ones is as simple as creating a Python function. There are two kinds: `item_tfms` are applied to each item (in this case, each item is resized to a 224-pixel square), while `batch_tfms` are applied to a *batch* of items at a time using the GPU, so they're particularly fast (we'll see many examples of these throughout this book).\n    \n  \ndtype: object\n\n\n\nEven after reviewing each of the question/answer/context triplets for three approaches I’m still not getting a strong sense of intuition of what works best. I’m hoping that after I have done this exercise for eight chapters, I’ll have built some of that intuition."
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#including-more-large-chunks-during-retrieval",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#including-more-large-chunks-during-retrieval",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Including More Large Chunks During Retrieval",
    "text": "Including More Large Chunks During Retrieval\nThe final experiment I’ll run is retrieving the top 3 BM25 ranked large chunks for each question. Using 3 small chunks and using 1 large chunk both resulted in a retrieved context relevancy of 54%. However they answered a different set of 18 questions. Perhaps if I combine both approaches (use larger chunk size AND use the top 3 BM25-ranked results for evaluation) I’ll obtain a higher retrieved context relevancy.\n\n\nShow the for-loop code\nresults = []\n\nfor keywords in df['keywords']:\n  if keywords != 'No answer':\n    words = ' OR '.join([f'\"{word.strip(\",\")}\"' for word in keywords.split()])\n    q = f\"\"\"\n\n    SELECT *, rank\n      from fastbook_text_large\n    WHERE fastbook_text_large MATCH '{words}'\n    ORDER BY rank\n    LIMIT 3\n\n    \"\"\"\n\n    res = cur.execute(q)\n    results.append(res.fetchall())\n  else:\n    # if keywords == \"No Answer\"\n    res = (\"No answer\")\n    results.append(res)\n\n\n\ntop_3_large = large_df[['chapter', 'question_number', 'question_text', 'answer']].copy()\n\ntop_3_large['result'] = results\n\ntop_3 = top_3_large['result'].apply(pd.Series)\ntop_3.columns = [f'result_{i+1}' for i in range(top_3.shape[1])]\n\nfor col in ['result_1', 'result_2', 'result_3']:\n    top_3[col] = top_3[col].apply(lambda x: x[0] if isinstance(x, tuple) else x)\n\ntop_3_large = pd.concat([top_3_large, top_3], axis=1)\ntop_3_large.to_csv('top-3-large-retrieval-results.csv', index=False)\n\nThe combined approach (more, larger chunks retrieved) resulted in a retrieved context relevancy of 72%!! This is an increase of 18% from the previous two approaches (retrieving 3 small chunks, retrieving 1 large chunk). However, I’m concerned that the large amount of irrelevant text also included may distract the model from answering the question correctly and concisely—something I’ll have to rigorously experiment with once I add an LLM to the pipeline.\nHere are is an example question for which the combined approach provided relevant context (whereas the previous two methods did not):\nFor the question:\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nUsing the top 3 highest BM25-ranked small chunks did not provide enough context to answer the question:\n\ntop_3_small = pd.read_csv(\"/content/top-3-retrieval-results.csv\")\ntop_1_large = pd.read_csv(\"/content/large_chunk_results.csv\")\n\n\n# not relevant\ntop_3_small.iloc[3][['question_text', 'fts5_result2_1', 'fts5_result2_2', 'fts5_result2_3']]\n\n\n\n  \n    \n      \n      3\n    \n  \n  \n    \n      question_text\n      Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n    \n    \n      fts5_result2_1\n      ## Neural Networks: A Brief History\\n\\nIn fact, the approach laid out in PDP is very similar to the approach used in today's neural networks. The book defined parallel distributed processing as requiring:\n    \n    \n      fts5_result2_2\n      ## Neural Networks: A Brief History\\n\\nPerhaps the most pivotal work in neural networks in the last 50 years was the multi-volume *Parallel Distributed Processing* (PDP) by David Rumelhart, James McClellan, and the PDP Research Group, released in 1986 by MIT Press. Chapter 1 lays out a similar hope to that shown by Rosenblatt:\n    \n    \n      fts5_result2_3\n      ## Neural Networks: A Brief History\\n\\nWe will see in this book that modern neural networks handle each of these requirements.\n    \n  \ndtype: object\n\n\n\nNeither did using the top-1 larger chunk:\n\n# not relevant\ntop_1_large.iloc[3][['question_text', 'large_chunk_result']]\n\n\n\n  \n    \n      \n      3\n    \n  \n  \n    \n      question_text\n      Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n    \n    \n      large_chunk_result\n      ## Neural Networks: A Brief History\\n\\n> : People are smarter than today's computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. ...We will introduce a computational framework for modeling cognitive processes that seems… closer than other frameworks to the style of computation as it might be done by the brain.\\n## Neural Networks: A Brief History\\n\\nThe premise that PDP is using here is that traditional computer programs work very differently to brains, and that might be why computer programs had been (at that point) so bad at doing things that brains find easy (such as recognizing objects in pictures). The authors claimed that the PDP approach was \"closer \\nthan other frameworks\" to how the brain works, and therefore it might be better able to handle these kinds of tasks.\\n## Neural Networks: A Brief History\\n\\nIn fact, the approach laid out in PDP is very similar to the approach used in today's neural networks. The book defined parallel distributed processing as requiring:\n    \n  \ndtype: object\n\n\n\nHowever, using the top-3 larger chunks included the necessary context across the first and second-highest ranked chunks:\n\ntop_3_large.iloc[3][['question_text', 'result_1', 'result_2']]\n\n\n\n  \n    \n      \n      3\n    \n  \n  \n    \n      question_text\n      Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n    \n    \n      result_1\n      ## Neural Networks: A Brief History\\n\\n> : People are smarter than today's computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. ...We will introduce a computational framework for modeling cognitive processes that seems… closer than other frameworks to the style of computation as it might be done by the brain.\\n## Neural Networks: A Brief History\\n\\nThe premise that PDP is using here is that traditional computer programs work very differently to brains, and that might be why computer programs had been (at that point) so bad at doing things that brains find easy (such as recognizing objects in pictures). The authors claimed that the PDP approach was \"closer \\nthan other frameworks\" to how the brain works, and therefore it might be better able to handle these kinds of tasks.\\n## Neural Networks: A Brief History\\n\\nIn fact, the approach laid out in PDP is very similar to the approach used in today's neural networks. The book defined parallel distributed processing as requiring:\n    \n    \n      result_2\n      ## Neural Networks: A Brief History\\n\\nRosenblatt further developed the artificial neuron to give it the ability to learn. Even more importantly, he worked on building the first device that actually used these principles, the Mark I Perceptron. In \"The Design of an Intelligent Automaton\" Rosenblatt wrote about this work: \"We are now about to witness the birth of such a machine–-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control.\" The perceptron was built, and was able to successfully recognize simple shapes.\\n## Neural Networks: A Brief History\\n\\nAn MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called _Perceptrons_ (MIT Press), about Rosenblatt's invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.\\n## Neural Networks: A Brief History\\n\\nPerhaps the most pivotal work in neural networks in the last 50 years was the multi-volume *Parallel Distributed Processing* (PDP) by David Rumelhart, James McClellan, and the PDP Research Group, released in 1986 by MIT Press. Chapter 1 lays out a similar hope to that shown by Rosenblatt:\n    \n  \ndtype: object\n\n\n\nHere is a summary of the results of this notebook’s experiments:\n\n\n\n\n\n\n\n\nTop BM25 Ranked Chunks Retrieved\nChunk Size\nRetrieved Context Relevancy*\n\n\n\n\ntop-3\nLarge\n72%\n\n\ntop-1\nLarge\n54%\n\n\ntop-3\nSmall\n54%\n\n\ntop-1\nSmall\n40%\n\n\n\n*Retrieved Context Relevancy:The percentage of questions for which the retrieved context was relevant and sufficient for me to answer the question."
  },
  {
    "objectID": "posts/2024-08-04-fastbook-ch1-fts5/index.html#final-thoughts",
    "href": "posts/2024-08-04-fastbook-ch1-fts5/index.html#final-thoughts",
    "title": "Using Full Text Search to Answer the fastbook Chapter 1 Questionnaire",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThe experiments in this notebook are promising: using BM25 to retrieve the context necessary to answer Chapter 1 Questionnaire questions works 72% of the time. Of course, I still had to interpret the retrieved chunks to extract the answer, but that’s something that can be easily done with an LLM down the road. In my next notebook, I’ll use cosine similarity, between the embeddings of the question text and the embeddings of the chunks, and see how that compares to BM25. In the notebook after that, I’ll combine both and see how a hybrid approach performs.\nSomething else I will also experiment with is the list of keywords that I came up with, as they are critical to the performance of full text search.\nOnce I’ve established a baseline that I’m confident in, I’ll start introducing an LLM into the pipeline—first to generate keywords from the question for use in full text key search, and then to extract the answer from the retrieved context.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-04-12-kto/index.html",
    "href": "posts/2024-04-12-kto/index.html",
    "title": "Paper Math: KTO (Kahneman Tversky Optimization)",
    "section": "",
    "text": "In this notebook I’ll walk through some of the math involved in the research paper KTO: Model Alignment as Prospect Theoretic Optimization. Here’s the abstract:\n\nKahneman & Tversky’s prospect theory tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases – the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being human-aware loss functions (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences – only a binary signal of whether an output is desirable or undesirable for a given input. This makes it far easier to use in the real world, where preference data is scarce and expensive.\n\nThe key difference between KTO and other methods (such as RLHF or DPO) is that instead of using preference data, KTO uses a binary signal of desirable or undesirable, as seen in the dataset example on the HuggingFace KTO Trainer page (where True = desired and False = undesired):\nkto_dataset_dict = {\n    \"prompt\": [\n        \"Hey, hello\",\n        \"How are you\",\n        \"What is your name?\",\n        \"What is your name?\",\n        \"Which is the best programming language?\",\n        \"Which is the best programming language?\",\n        \"Which is the best programming language?\",\n    ],\n    \"completion\": [\n        \"hi nice to meet you\",\n        \"leave me alone\",\n        \"I don't have a name\",\n        \"My name is Mary\",\n        \"Python\",\n        \"C++\",\n        \"Java\",\n    ],\n    \"label\": [\n        True,\n        False,\n        False,\n        True,\n        True,\n        False,\n        False,\n    ],\n}\nIn the following sections I walk through some of the math in the paper to get a better understanding of the concepts behind them."
  },
  {
    "objectID": "posts/2024-04-12-kto/index.html#section-4.1.-derivation",
    "href": "posts/2024-04-12-kto/index.html#section-4.1.-derivation",
    "title": "Paper Math: KTO (Kahneman Tversky Optimization)",
    "section": "Section 4.1. Derivation",
    "text": "Section 4.1. Derivation\n\nKTO Loss Function: \\[\\mathcal{L}_{KTO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{x, y\\sim\\mathcal{D}}[w(y)(1 - v_{KTO}(x, y;\\beta))]\\]\nWhere:\n\\[w(y) = \\begin{cases}\n      \\lambda_D & \\text{if } y \\sim y_{desirable}|x \\\\\n      \\lambda_U & \\text{if } y \\sim y_{undesirable}|x\n   \\end{cases}\\]\n\\[v_{KTO}(x,y;\\beta) = \\begin{cases}\n      \\sigma(r_{KTO}(x,y) - z_{ref}) & \\text{if } y \\sim y_{desirable}|x \\\\\n      \\sigma(z_{ref} -r_{KTO}(x,y)) & \\text{if } y \\sim y_{undesirable}|x\n   \\end{cases}\\]\n\\[z_{ref}= \\mathbb{E}_{x'\\sim\\mathcal{D}}[\\beta KL(\\pi_\\theta(y'|x')||\\pi_{ref}(y'|x'))]\\]\n\\[r_{KTO}(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\]\n\nKTO Loss for Desirable Outputs\n\\[\\mathcal{L}_{KTO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_D\\big(1 - \\sigma(r_{KTO} - z_{ref})\\big)\\big]\\]\nHere is what \\(1-\\sigma(x)\\) looks like:\n\nAs the term \\(r_{KTO} - z_{ref}\\) increases (i.e. the reward for desirable outputs increases while KL divergence stays the same or decreases), loss decreases. From the paper:\n\nIntuitively, KTO works because if the model increases the reward of a desirable example in a generic way, then the KL penalty will also rise and no progress will be made on the loss.\n\nI think “generic way” has a negative connotation in this statement, meaning that the model is not increasing the reward in the specific way that \\(\\pi_{ref}\\), the supervised fine-tune reference model, was trained to generate outputs in (for whatever the use case is—helpful, honest, harmless, etc.).\nFrom the paper:\n\nWe do not back-propagate through the KL term, as it makes training much more stable. This means that the KL term purely serves to control how saturated the loss is. (emphasis mine)\n\nChatGPT:\n\nWhen the loss for a deep learning model is described as “saturated,” it typically means that the model has reached a point where further training does not significantly decrease the loss anymore. In other words, the model has learned as much as it can from the available data, and additional training iterations are unlikely to improve its performance significantly.\n\nI think it’s correct to say that \\(r_{KTO}\\) is like the KL divergence between the policy being trained and the reference policy across all input/output training data pairs, while \\(z_{ref}\\) is the KL divergence between the policy being trained and the reference policy across all reference data pairs. I think it’s also correct to continue that logic to say that as \\(r_{KTO}\\) increases, the policy being trained diverges from the reference policy (on training data), and \\(z_{ref}\\) keeps that divergence in check (if the policy being trained diverges too far from the reference policy on the reference data, the loss increases or stays the same).\n\n\nKTO Loss for Undesirable Outputs\n\\[\\mathcal{L}_{KTO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_U\\big(1 - \\sigma(z_{ref} - r_{KTO})\\big)\\big]\\]\nThis has the same \\(1-\\sigma(x)\\) form. If the reward for an UNdesirable output increases while the KL term \\(z_{ref}\\) stays the same, the loss will increase.\n\n\n\nUnderstanding \\(z_{ref}\\)\nFrom the paper:\n\nRather than having just one dispreferred generation \\(y_l|x\\) as the reference point, we assume that humans judge the quality of \\((x,y)\\) in relation to all input-output pairs they have seen.\n\nThe phrase “rather than having just one dispreferred generation \\(y_l|x\\) as the reference point” I think is referring to the DPO loss function, specifically the second term inside log sigmoid:\n\\[\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\log\\sigma\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\big]\\]\nWhich in the KTO loss function is captured by \\(z_{ref}\\) across all \\(y'\\) outputs, not just \\(y_l\\):\n\\[z_{ref}= \\mathbb{E}_{x'\\sim\\mathcal{D}}[\\beta KL(\\pi_\\theta(y'|x')||\\pi_{ref}(y'|x'))]\\]\nI may be taking that phrase from the paper too literally, so I may be wrong about this.\nFrom the paper:\n\nwe write the reference point to be the expected reward under the optimal policy, not just for generation following \\(x\\) but following any input \\(x': \\mathbb{E}_{x' \\sim \\mathcal{D}, y \\sim \\pi^*}\\big[ r^*(x',y')\\big]\\). Under the assumption that the expected value of the partition function across \\(x'\\) is zero, this simplifies to the KL divergence between \\(\\pi^*\\) and \\(\\pi_{ref}\\) scaled by \\(\\beta\\).\n\nWhere\n\\[r^*(x,y) = \\beta\\log\\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta\\log Z(x)\\]\nbecomes:\n\\[z_{ref}= \\mathbb{E}_{x'\\sim\\mathcal{D}}[\\beta KL(\\pi_\\theta(y'|x')||\\pi_{ref}(y'|x'))]\\]\nand I think it’s correct to rewrite \\(z_{ref}\\) as the following (ChatGPT agrees):\n\\[z_{ref}= \\mathbb{E}_{x'\\sim\\mathcal{D}}\\big[\\beta \\log\\frac{\\pi_\\theta(y'|x')}{\\pi_{ref}(y'|x'))}\\big]\\]\nLast thing about \\(z_{ref}\\):\nthe expectation is across \\(x'\\) only (i.e. \\(\\mathbb{E}_{x' \\sim \\mathcal{D}}\\)). From ChatGPT:\n\nthis expression is dependent on \\(y'\\). Both \\(\\pi_\\theta(y'|x')\\) and \\(\\pi_{ref}(y'|x')\\) are conditional probability distributions where the probability of \\(y'\\) depends on \\(x'\\). So, the KL divergence between these two distributions also depends on \\(y'\\) indirectly through \\(x'\\). However, \\(z_{ref}\\) itself does not directly depend on \\(y'\\), as it represents the expected value of the KL divergence over all possible values of \\(x'\\)."
  },
  {
    "objectID": "posts/2024-04-12-kto/index.html#section-4-implementation",
    "href": "posts/2024-04-12-kto/index.html#section-4-implementation",
    "title": "Paper Math: KTO (Kahneman Tversky Optimization)",
    "section": "Section 4: Implementation",
    "text": "Section 4: Implementation\nFrom the “Implementation” subsection in section 4. Kahneman-Tversky Optimization:\n\nIn practice, we estimate the KL term by matching inputs \\(x'\\) with unrelated outputs \\(y'_U\\) in a batch of size \\(m\\) and then calculating:\n\n\\[max\\big( 0, \\frac{1}{m}\\sum\\log\\frac{\\pi_\\theta(y'_U|x')}{\\pi_{ref}(y'_U|x')}\\big)\\]\nIn the TRL library they have the following comment which I believe refers to the above excerpt:\n # As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.\nThe code implementation of KTO loss:\n# eqn (7) of the HALOs paper\nchosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)\nrejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)\n\nchosen_logratios = policy_chosen_logps - reference_chosen_logps\nrejected_logratios = policy_rejected_logps - reference_rejected_logps\n\n# As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.\nlosses = torch.cat(\n    (\n        1 - F.sigmoid(self.beta * (chosen_logratios - rejected_KL)),\n        1 - F.sigmoid(self.beta * (chosen_KL - rejected_logratios)),\n    ),\n    0,\n)\nI find that nomenclature used in the paper a bit confusing since elsewhere in the paper they use the subscript \\(U\\) to represent “undesirable” but here they use it to mean “unrelated”. After looking at the code, I think by “unrelated” they mean that when calculating desired loss they use the undesirable KL and vice versa.\nNote that chosen_KL is just the (clamped) mean of chosen_logratios. As is rejected_KL to rejected_logratios.\nThe first loss in losses:\n1 - F.sigmoid(self.beta * (chosen_logratios - rejected_KL)\nCorresponds to KTO loss for desired outputs:\n\\[\\mathcal{L}_{KTO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_D\\big(1 - \\sigma(r_{KTO} - z_{ref})\\big)\\big]\\]\nAlthough I don’t know why they are not multiplying by \\(\\lambda_D\\).\nThe second loss in losses:\n1 - F.sigmoid(self.beta * (chosen_KL - rejected_logratios))\nCorresponds to the KTO loss for undesired outputs:\n\\[\\mathcal{L}_{KTO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_U\\big(1 - \\sigma(z_{ref} - r_{KTO})\\big)\\big]\\]\nAgain, I don’t know why they are not multiplying by \\(\\lambda_U\\).\nAlso, I’m not sure why they are concatenating a 0 to the two loss function in losses."
  },
  {
    "objectID": "posts/2024-04-12-kto/index.html#proposition-3.5-proof",
    "href": "posts/2024-04-12-kto/index.html#proposition-3.5-proof",
    "title": "Paper Math: KTO (Kahneman Tversky Optimization)",
    "section": "Proposition 3.5 Proof",
    "text": "Proposition 3.5 Proof\nFor a loss to be a HALO (human-aware loss function) it needs to be expressible as:\n\\[f(x,y;\\theta) = t(v_f(r_\\theta(x,y) - \\mathbb{E}_{x'\\sim Q'_x, y' \\sim Q'_y}[r_\\theta(x',y')]))\\]\nwith a parameterized reward function \\(r_\\theta\\), reference point distributions \\(Q_x(X'), Q_y(Y'|X')\\), value function \\(v_f\\) and a negative affine function \\(t\\).\n\nReward function \\(r_\\theta\\)\nThe reward function \\(r_\\theta\\) needs to satisfy the following expression:\n\\[\\forall(x_1,y_1), (x_2,y_2) \\in \\mathcal{X} \\times \\mathcal{Y}, \\; r_\\theta(x_1,y_1) > r_\\theta(x_2,y_2) \\iff (x_1,y_1) \\succ_{r_\\theta} (x_2,y_2)\\]\nThis expression reads as (ChatGPT):\nFor all pairs of points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) belonging to sets \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) respectively, the value of the function \\(r_\\theta\\) applied to the first pair \\((x_1, y_1)\\) is greater than the value of the function \\(r_\\theta\\)applied to the second pair \\((x_2, y_2)\\) if and only if the first pair \\((x_1, y_1)\\) is preferred to the second pair \\((x_2, y_2)\\) according to the relation \\(\\succ_{r_\\theta}\\).\nExplanation of symbols:\n\\(\\forall\\): for all\n\\(\\in\\): in\n\\(\\iff\\): if and only if\n\\(\\succ\\): succeeds operator (indicates preference)\n\n\nReference point distributions \\(Q_x(X'), Q_y(Y'|X')\\)\nIn section 3.2 of the paper they define a reference point as:\n\ninput-output pairs sampled from the distributions \\(Q_x, Q_y\\).\n\nFrom what I understand, the \\('\\) in \\(X'\\) and \\(Y'\\) indicates that it is a different, reference input and output (respectively) from the \\(x\\) and \\(y\\) used as the training dataset.\nChatGPT:\n\n\\(x\\) and \\(y\\) are fixed values or points in the domain, while \\(x'\\) and \\(y'\\) are variables representing points randomly sampled from the distributions \\(Q_{x'}\\)​ and \\(Q_{y'}\\)​ respectively. These samples are used to calculate the expected value \\(\\mathbb{E}\\) over those distributions.\n\n\n\nValue function \\(v_f\\)\nThe value function is defined as this expression:\n\\[v_f: \\mathbb{R} \\to \\mathbb{R}\\]\nWhich can be read as (ChatGPT):\n\n“The function \\(v_f\\) maps real numbers to real numbers.”\nHere, \\(\\mathbb{R}\\) represents the set of real numbers, and the notation \\(v_f: \\mathbb{R} \\to \\mathbb{R}\\) specifies that the function \\(v_f\\)​ takes inputs from the set of real numbers and produces outputs that are also real numbers.\n\nThe value function must be monotonic non-decreasing and concave in \\((0, \\infty)\\).\nMonotonic: varying in such a way that it either never decreases or never increases.\nNon-decreasing: self-explanatory (the function never decreases)\nConcave: A concave function is one in which the slope is continually decreasing (note that the slope is decreasing, not the function) source.\n\\((0, \\infty)\\): the set of all real numbers greater than 0 but not including 0 itself, extending infinitely towards positive infinity\n\nExample of a function that is monotonic non-decreasing and concave in \\((0, \\infty)\\) is \\(log(x)\\):\n\n\n\nDPO Loss\nThe DPO loss function is in the form:\n\\[\\mathcal{L}_{DPO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}\\big[-\\log\\sigma\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\big]\\]\nThis is expressible as:\n\\[t(v_f(r_\\theta(x,y) - \\mathbb{E}_{x'\\sim Q'_x, y' \\sim Q'_y}[r_\\theta(x',y')]))\\]\nMapping the DPO loss to this expression:\n\\(r_\\theta\\) is the DPO reward \\(\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\) which is passed as an input into the value function \\(v_f\\) which in this case is \\(\\log\\sigma\\) (monotonic non-decreasing and concave everywhere) the output of which is passed into \\(t\\) which takes the negative of the input, \\(t(x) = -x\\).\nI didn’t understand the following line in the paper:\n\n\\(Q_x\\) places all mass on \\(x\\) and \\(Q_y\\) places all mass on the dispreferred output \\(y_l\\) for \\(x\\) such that \\(y \\succ y_l\\).\n\nWhich ChatGPT explains as:\n\nIn the context of reinforcement learning and optimization, \\(Q_x\\) and \\(Q_y\\) represent probability distributions. When it’s stated that “\\(Q_x\\) places all mass on \\(x\\)” and “\\(Q_y\\) places all mass on the dispreferred output \\(y_l\\) for \\(x\\) such that \\(y \\succ y_l\\),” it means that these distributions are highly concentrated on specific values.\nLet’s break it down:\n\\(Q_x\\) is a distribution over inputs \\(x\\). Saying that it places all mass on \\(x\\) means that it assigns a probability of 1 to a particular input \\(x\\) and 0 to all other possible inputs. In other words, \\(Q_x\\) is a degenerate distribution that only considers a single input \\(x\\).\nSimilarly, \\(Q_y\\) is a distribution over outputs \\(y\\). However, in this case, it places all mass on the dispreferred output \\(y_l\\) for a given input \\(x\\). This means that for every input \\(x\\), \\(Q_y\\) assigns a probability of 1 to the dispreferred output \\(y_l\\) and 0 to all other possible outputs. Additionally, it’s mentioned that \\(y \\succ y_l\\), which indicates that the preferred output \\(y\\) is preferred over \\(y_l\\).\n\nIf that is correct, then I would add the following to complete the construction of DPO loss as the HALO expression:\nSince \\(y \\succ y_l\\), I interpret that \\(y\\) in \\(r_\\theta(x,y)\\) corresponds to \\(y_w\\) and is represented in the DPO loss function by the term \\(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}\\).\nThen because \\(\\mathbb{E}_{x'\\sim Q'_x, y' \\sim Q'_y}[r_\\theta(x',y')]\\) is subracted from \\(r_\\theta(x,y)\\) in the generic HALO expression, it corresponds to \\(\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\) in the DPO loss function.\nI am not confident about this last part. I’m also not going to do a walkthrough of the SLiC and PPO-Clip loss functions since I haven’t read those papers."
  },
  {
    "objectID": "posts/2024-04-12-kto/index.html#proposition-4.1",
    "href": "posts/2024-04-12-kto/index.html#proposition-4.1",
    "title": "Paper Math: KTO (Kahneman Tversky Optimization)",
    "section": "Proposition 4.1",
    "text": "Proposition 4.1\nKTO does not learn from undesirable examples with sufficiently high rewards or desirable examples with sufficiently low rewards.\nIn this section they provide the derivative of KTO loss without derivation. I will try to derive it from the KTO loss. Here is the final form of the derivative in the paper:\n\\[\\nabla_\\theta\\mathcal{L}_{KTO}(\\pi_\\theta. \\pi_{ref}) = \\mathbb{E}_{x, y \\sim \\mathcal{D}}\\big[ \\lambda(y) \\sigma(z) \\sigma(-z) \\nabla \\beta \\log \\pi_\\theta(y|x)\\big]\\]\nWhere\n\\[z = r_{KTO}(x,y) - z_{ref}\\]\n\n\\[\\lambda(y) = \\begin{cases}\n      -\\lambda_D & \\text{if } y \\text{ is desirable} \\\\\n      \\lambda_U & \\text{if } y \\text{ is undesirable}\n   \\end{cases}\\]\nI’ll start by rewriting the desirable KTO loss function with \\(z\\) as defined above:\n\\[\\mathcal{L}_{KTO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_D\\big(1 - \\sigma(r_{KTO} - z_{ref})\\big)\\big] = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_D\\big(1 - \\sigma(z)\\big)\\big]\\]\nThe derivative of $_D(1-(z)) with respect to \\(\\theta\\) I’ll write as:\n\\[\\frac{dz}{d\\theta} \\times \\frac{d}{dz}\\lambda_D(1-\\sigma(z))\\]\nStarting with the rightmost term, pulling out the constant \\(\\lambda_D\\), given that the derivative of 1 with respect to \\(z\\) is 0, multiplying by -1 (chain rule), and given the property of sigmoid that \\(\\sigma(-z) = 1 - \\sigma(z)\\):\n\\[\\frac{d}{dz}\\lambda_D(1-\\sigma(z)) = \\lambda_D\\frac{d}{dz}(1-\\sigma(z)) = -\\lambda_D\\frac{d}{dz}\\sigma(-z) = -\\lambda_D\\big[\\sigma(z)(1 - \\sigma(z))\\big] = -\\lambda_D\\sigma(z)\\sigma(-z)\\]\nI’ll do \\(\\frac{dz}{d\\theta}\\) next:\n\\[\\frac{dz}{d\\theta} = \\frac{d}{d\\theta}\\big(r_{KTO}(x,y) - z_{ref}\\big) = \\frac{d}{d\\theta}\\big(\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)} -  \\mathbb{E}_{x'\\sim\\mathcal{D}}[\\beta KL(\\pi_\\theta(y'|x')||\\pi_{ref}(y'|x'))]\\big)\\]\nThey mention that they don’t backpropagate through the KL term so I think I can interpret that as meaning the KL term’s gradient is 0. We’re left with:\n\\[\\frac{d}{d\\theta}\\big(\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\big) = \\beta\\frac{d}{d\\theta}\\big(log\\pi_\\theta(y|x) - \\log\\pi_{ref}(y|x)\\big)\\]\nThe reference model is not changing (i.e it’s not parameterized by \\(\\theta\\) and is not being trained) so its derivative with respect to \\(\\theta\\) is 0. That leaves us with:\n\\[\\beta\\frac{d}{d\\theta}log\\pi_\\theta(y|x)\\]\nMultiplying by \\(\\frac{d}{dz}\\sigma(z)\\) to get the complete \\(\\frac{d}{d\\theta}\\sigma(z)\\):\n\\[\\frac{d}{d\\theta}\\sigma(z) = -\\lambda_D\\sigma(z)\\sigma(-z)\\beta\\frac{d}{d\\theta}log\\pi_\\theta(y|x)\\]\nPutting it back in the full form of the loss gradient:\n\\[\\nabla_\\theta\\mathcal{L}_{KTO}(\\pi_\\theta. \\pi_{ref}) = \\mathbb{E}_{x, y \\sim \\mathcal{D}}\\big[ -\\lambda_D \\sigma(z) \\sigma(-z) \\beta\\frac{d}{d\\theta}log\\pi_\\theta(y|x)\\big]\\]\nWhich seems equivalent to the gradient of the KTO loss provided in the paper (although I’m not sure why they have \\(\\beta\\) inside the gradient symbol \\(\\nabla\\)):\n\\[\\nabla_\\theta\\mathcal{L}_{KTO}(\\pi_\\theta. \\pi_{ref}) = \\mathbb{E}_{x, y \\sim \\mathcal{D}}\\big[ \\lambda(y) \\sigma(z) \\sigma(-z) \\nabla \\beta \\log \\pi_\\theta(y|x)\\big]\\]\nNext, I’ll derive the gradient of the loss function for undesirable \\(y\\) values, starting with the loss function:\n\\[\\mathcal{L}_{KTO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_U\\big(1 - \\sigma(z_{ref} - r_{KTO})\\big)\\big] = \\mathbb{E}_{x, y\\sim\\mathcal{D}}\\big[\\lambda_U\\big(1 - \\sigma(-z)\\big)\\big]\\]\nWhere\n\\[z = r_{KTO}(x,y) - z_{ref}\\]\nso \\[-z = z_{ref} - r_{KTO}(x,y)\\]\nThe derivative of \\(\\lambda_U(1-\\sigma(-z))\\) with respect to \\(\\theta\\) I’ll write as:\n\\[\\frac{dz}{d\\theta} \\times \\frac{d}{dz}\\lambda_U(1-\\sigma(-z))\\]\nThe derivative of \\(\\lambda_U(1-\\sigma(-z))\\) with respect to \\(z\\) involves pulling out the constant \\(\\lambda_U\\), pulling out the constant -1 from \\(-\\sigma(z)\\), multiplying by -1 (chain rule), and the derivative of 1 with respect to \\(\\theta\\) going to 0:\n\\[\\frac{d}{dz}\\lambda_U(1-\\sigma(-z)) = \\frac{d}{dz}\\lambda_U(1-\\sigma(-z)) = -1 \\times -1 \\times \\lambda_U\\frac{d}{dz}\\sigma(-z) = \\lambda_U\\frac{d}{dz}\\sigma(-z)\\]\nGiven that \\(\\sigma(-z) = 1 - \\sigma(z)\\):\n\\[\\lambda_U\\frac{d}{dz}\\sigma(-z) = \\lambda_U\\sigma(-z)(1 - \\sigma(-z))= \\lambda_U\\sigma(-z)\\big[1 - (1-\\sigma(z))\\big] = \\lambda_U\\sigma(-z)\\big[1-1+\\sigma(z)\\big] = \\lambda_U\\sigma(-z)\\sigma(z)\\]\nThe derivative of \\(z\\) with respect to \\(\\theta\\) is the same as before:\n\\[\\beta\\frac{d}{d\\theta}log\\pi_\\theta(y|x)\\]\nMultiplying the two derivative terms together:\n\\[\\lambda_U\\sigma(-z)\\sigma(z)\\beta\\frac{d}{d\\theta}log\\pi_\\theta(y|x)\\]\nPlugging it all back to get the loss gradient function for undesired outputs:\n\\[\\nabla_\\theta\\mathcal{L}_{KTO}(\\pi_\\theta. \\pi_{ref}) = \\mathbb{E}_{x, y \\sim \\mathcal{D}}\\big[ \\lambda_U \\sigma(z) \\sigma(-z) \\nabla \\beta \\log \\pi_\\theta(y|x)\\big]\\]\nComparing that to the loss derivative for desired outputs, the difference is the minus sign:\n\\[\\nabla_\\theta\\mathcal{L}_{KTO}(\\pi_\\theta. \\pi_{ref}) = \\mathbb{E}_{x, y \\sim \\mathcal{D}}\\big[ -\\lambda_D \\sigma(z) \\sigma(-z) \\beta\\frac{d}{d\\theta}log\\pi_\\theta(y|x)\\big]\\]\nThe intuition behind the minus sign given in the paper:\n\nif \\(y\\) is desirable, then \\(\\lambda(y)\\) is negative and we push up the probability of \\(\\pi_\\theta(y|x)\\) to minimize the loss; we do the opposite if \\(y\\) is undesirable.\n\nProposition 4.1 states that “KTO does not learn from undesirable examples with sufficiently high rewards or desirable examples with sufficiently low rewards” and the paper explains that:\n\nAs \\(z\\) tends to \\(\\pm\\infty\\), the gradient will tend to zero since either \\(\\sigma(-z)\\) or \\(\\sigma(z)\\) will tend to zero. Since \\(z\\) is increasing in the reward, this means that sufficiently large and sufficiently small rewards will yield a gradient of zero."
  },
  {
    "objectID": "posts/2024-09-30-os-fork/index.html",
    "href": "posts/2024-09-30-os-fork/index.html",
    "title": "Experimenting with os.fork",
    "section": "",
    "text": "In Lesson 10 of the fastai course (Part 2) we’re introduced to os.fork, specifically in the context of random number generation. In this notebook I’ll get some more reps working with os.fork.\nIn the Lesson, Jeremy shows how random number generation in different libraries is handled across parent and child processes, as shown below (using seed and rand as defined in the lesson):\n\nimport os\nimport random\nimport numpy as np\nimport torch\nimport fcntl\nimport time\nimport signal\nimport sys\n\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\nThe from-scratch rand function generates the same random number in both parent and child processes because they share the same random state:\n\nif os.fork(): print(f'In parent: {rand(), rnd_state}')\nelse:\n    print(f'In child: {rand(), rnd_state}')\n    os._exit(os.EX_OK)\n\nIn parent: (0.7645251082582081, (3364, 25938, 24184))\nIn child: (0.7645251082582081, (3364, 25938, 24184))\n\n\ntorch does the same:\n\nif os.fork(): print(f'In parent: {torch.rand(1).item(), torch.get_rng_state().sum().item()}')\nelse:\n    print(f'In child: {torch.rand(1).item(), torch.get_rng_state().sum().item()}')\n    os._exit(os.EX_OK)\n\nIn parent: (0.0692816972732544, 325580)\nIn child: (0.0692816972732544, 325580)\n\n\nAs does NumPy:\n\nif os.fork(): print(f'In parent: {np.random.rand(1)[0], np.random.get_state()[1].sum()}')\nelse:\n    print(f'In child: {np.random.rand(1)[0], np.random.get_state()[1].sum()}')\n    os._exit(os.EX_OK)\n\nIn child: (0.8234897720205184, 1375830894290)\nIn parent: (0.8234897720205184, 1375830894290)\n\n\nThe Python standard library generates different random numbers in the parent and the child, indicating that the random state has changed:\n\nif os.fork(): print(f'In parent: {random.random(), sum(random.getstate()[1])}')\nelse:\n    print(f'In child: {random.random(), sum(random.getstate()[1])}')\n    os._exit(os.EX_OK)\n\nIn parent: (0.7978973512537335, 1327601590235)\nIn child: (0.5603922565589059, 1333438682830)\n\n\nJeremy also mentioned in the video that there used to be a bug in fastai related to this os.fork behavior which resulted in incorrectly handling data augmentations across multiple processes. I poked around the fastai repo and found this issue and corresponding PR which might have been the ones he was referring to? I’m not sure, but it did lead me down an interesting rabbit hole in the fastai repo and I learned a couple of new things that I’ll share.\nIn the PR, they introduce the following line:\nself.store = threading.local()\nself.store is reference throughout the PR, for example:\ndef set_state(self):\n        self.store.rand_r = random.uniform(0, 1)\n        self.store.rand_c = random.uniform(0, 1)\nThe corresponding GitHub issue linked to this StackOverflow post which talks about threading.local(). I didn’t quite follow the post so I copy/pasted its text as a prompt to Claude and asked it to create an example to illustrate the core concepts of threading.local. It gave me the following example:\n\nimport threading\nimport multiprocessing\nimport time\nimport random\n\nFirst, threading.local is instantiated as a global variable:\n\n# Thread-local storage for threading module\nthread_local = threading.local()\n\nNext, we have a function that creates a worker. Claude defines a worker as follows (I found similar definitions with Google searches):\n\na unit of execution that performs a specific task or job. In the context of concurrent programming, a worker is typically implemented as either a thread or a process, depending on the chosen concurrency model.\n\nthreading_worker adds a count attribute to thread_local (if it doesn’t have it already) or increments count by 1 if it exists.\n\ndef threading_worker(worker_id):\n    if not hasattr(thread_local, 'count'):\n        print(f'\\n\\tWorker {worker_id}: instantiating `count`')\n        thread_local.count = 0\n    thread_local.count += 1\n    print(f\"Threading: Worker {worker_id}, Count: {thread_local.count}\\n\")\n    time.sleep(random.random())\n\nTo illustrate, we create 5 threads and pass threading_worker to each one. The result is that each worker has its own “private view” to the global thread_local, as exhibited by thread_local.count for each worker_id having the same value of 1.\nFinally, Claude explains that the purpose of thread.join() is to complete the action in the thread before returning to the main thread. Note that the final print statement, print(\"Threading example finished.\") is run after all threads finish executing.\n\ndef run_threading_example():\n    threads = []\n    for i in range(5):\n        thread = threading.Thread(target=threading_worker, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Threading example finished.\")\n\nIt’s interesting to note that each Worker instantiates count before adding 1 to it (as expected), but the order of each thread instantiating count (0, 1, 2, 3, 4) is not the same order of each thread adding 1 (0, 1, 3, 4, 2; which I didn’t expect).\n\nrun_threading_example()\n\n\n    Worker 0: instantiating `count`\nThreading: Worker 0, Count: 1\n\n\n    Worker 1: instantiating `count`\nThreading: Worker 1, Count: 1\n\n\n    Worker 2: instantiating `count`\n\n    Worker 3: instantiating `count`\nThreading: Worker 3, Count: 1\n\n\n    Worker 4: instantiating `count`\nThreading: Worker 4, Count: 1\n\nThreading: Worker 2, Count: 1\n\nThreading example finished.\n\n\nThere is much to learn when it comes to threading and multiprocessing, but I’ll exit this rabbit hole for now.\nThe second thing I learned was this clever way to index into a tuple using a boolean expression:\n@property\ndef multi_processing_context(self): return (None,multiprocessing)[self.num_workers>0]\nI commented about this on Twitter and Jeremy replied:\n\n\nAlternatively you can use a if pred else b btw. (Most people seem to hate both options ;) )\n\n— Jeremy Howard (@jeremyphoward) September 28, 2024\n\n\nYears back when I getting into web development, one of the patterns in JavaScript I enjoyed was the ternary operator:\na = is_true ? val_if_true : val_if_false\nFrom what I understand, Python doesn’t have such an operator so anytime I come across a concise way to execute logic using a boolean expression, I’m excited to see it.\nWith that short interlude out of the way, I’ll now dig in to os.fork."
  },
  {
    "objectID": "posts/2024-09-30-os-fork/index.html#os.fork-experiments",
    "href": "posts/2024-09-30-os-fork/index.html#os.fork-experiments",
    "title": "Experimenting with os.fork",
    "section": "os.fork Experiments",
    "text": "os.fork Experiments\nI prompted Claude to give me some examples using os.fork with the following prompt:\n\nI want to better understand what os.fork does. what’s a good set of experiments I can run to understand it’s functionality?\n\nClaude with responded with four experiments, which I’ll run through next.\n\nBasic os.fork() example\nI’ll start with a definition from the “fork” Wikipedia page:\n\nIn computing, particularly in the context of the Unix operating system and its workalikes, fork is an operation whereby a process creates a copy of itself. It is an interface which is required for compliance with the POSIX and Single UNIX Specification standards. It is usually implemented as a C standard library wrapper to the fork, clone, or other system calls of the kernel. Fork is the primary method of process creation on Unix-like operating systems.\n\n\nIn multitasking operating systems, processes (running programs) need a way to create new processes, e.g. to run other programs. Fork and its variants are typically the only way of doing so in Unix-like systems. For a process to start the execution of a different program, it first forks to create a copy of itself. Then, the copy, called the “child process”, calls the exec system call to overlay itself with the other program: it ceases execution of its former program in favor of the other.\n\nNext, I’ll look at the definition of os.getpid from the docs before using it:\n\nReturn the parent’s process id. When the parent process has exited, on Unix the id returned is the one of the init process (1), on Windows it is still the same id, which may be already reused by another process.\n\n\nprint(f\"Main process PID: {os.getpid()}\")\n\nMain process PID: 436\n\n\nNext, I’ll call os.fork:\n\nFork a child process. Return 0 in the child and the child’s process id in the parent. If an error occurs OSError is raised.\nNote that some platforms including FreeBSD <= 6.3 and Cygwin have known issues when using fork() from a thread.\n\n\nif os.fork(): print(f'In parent: {os.getpid()}')\nelse:\n    print(f'In child: {os.getpid()}')\n    os._exit(os.EX_OK)\n\nIn parent: 436\nIn child: 580\n\n\nIt’s important to note that I took the above code straight from Lesson’s 10’s 01_matmul.ipynb.\nWhen I tried to run the following in Colab, the cell wouldn’t execute and would just hang:\npid = os.fork()\nWhen I tried to run that locally on my MacBook, I got the following error:\nOSError: [Errno 9] Bad file descriptor\nI found this StackOverflow post which talks about similar issues, and that os.fork doesn’t play nice with Jupyter Notebooks. Claude also seemed to agree, recommending that I either use the os._exit approach from Lesson 10, or put my os.fork-related code in a separate .py script outside the notebook.\nI asked Claude to rewrite the os.fork experiments using that if/else approach.\nWhen I run the following code block, it’s interesting to note that the child process runs before the parent process. I wonder if that means os.fork returneed 0? Claude says no:\n\nThe reason it might seem like the child process runs first is due to how process scheduling works in operating systems. When os.fork() is called, both the parent and child processes are ready to run, and the operating system’s scheduler decides which one to execute first. In this case, the child process got scheduled to run before the parent continued.\n\nIt adds the following context:\n\nThis behavior - where the child might run before the parent continues - is normal and expected in multi-process programming. It’s one of the reasons why synchronization mechanisms are often needed when working with multiple processes.\n\n\nprint(f\"\\nMain process PID: {os.getpid()}\")\n\nif os.fork():\n    print(f\"\\nIn parent: {os.getpid()}\")\nelse:\n    print(f\"\\nIn child: {os.getpid()}, Parent PID: {os.getppid()}\")\n    os._exit(os.EX_OK)\n\nprint(f\"\\nThis will be printed only by the parent process. PID: {os.getpid()}\")\n\n\nMain process PID: 436\n\nIn child: 853, Parent PID: 436\nMain process PID: 436\n\nIn parent: 436\n\nThis will be printed only by the parent process. PID: 436\n\n\n\n\n\nMemory Independence Example\nThe following example illustrates how “forked processes have independent memory spaces and that changes to variables in one process don’t affect the other process” as Claude states it.\nThe global shared_variable maintains its global value of 0 in the child process, before 1 is added to it to give it a final value of 1 in the child process. Meanwhile, in the parent process, it’s final value is 2. This reminds me of the threading.local behavior.\n\nshared_variable = 0\n\nif os.fork():\n    # Parent process\n    shared_variable +=  2\n    print(f\"\\nIn parent: {os.getpid()}, shared_variable = {shared_variable}\")\nelse:\n    # Child process\n    shared_variable += 1\n    print(f\"\\nIn child: {os.getpid()}, shared_variable = {shared_variable}\")\n    os._exit(os.EX_OK)\n\nprint(f\"Final shared_variable in parent: {shared_variable}\")\n\n\nIn parent: 436, shared_variable = 2\nFinal shared_variable in parent: 2\n\nIn child: 902, shared_variable = 1\n\n\n\n\nFile Descriptor Inheritance\nClaude then provided the following code to illustrate how to write to the same file different data from the parent and child process. However, this code resulted in only the parent writing to the file:\n\nwith open(\"test.txt\", \"w\") as f:\n    if os.fork():\n        # parent process\n        f.write(\"Written by parent\\n\")\n    else:\n        # child process\n        f.write(\"Written by child\\n\")\n        os._exit(os.EX_OK)\n\n# Run this after the script to see the contents:\nprint(open(\"test.txt\", \"r\").read())\n\nWritten by parent\n\n\n\nClaude then suggested using “file locking” and “flushing” to ensure the writing happens before process execution has ended, but this didn’t help. Sometimes it wrote from both processes, sometimes just from one. I’ve illustrated both examples below:\n\ndef do_write():\n  with open(\"test.txt\", \"w\") as f:\n      if os.fork():\n          # parent process\n          fcntl.flock(f, fcntl.LOCK_EX)\n          f.write(\"Written by parent\\n\")\n          f.flush()\n          fcntl.flock(f, fcntl.LOCK_UN)\n      else:\n          # child process\n          fcntl.flock(f, fcntl.LOCK_EX)\n          f.write(\"Written by child\\n\")\n          f.flush()\n          fcntl.flock(f, fcntl.LOCK_UN)\n          os._exit(os.EX_OK)\n\n  # Run this after the script to see the contents:\n  print(open(\"test.txt\", \"r\").read())\n\n\ndo_write()\n\nWritten by parent\n\n\n\n\ndo_write()\n\nWritten by child\nWritten by parent\n\n\n\nI wanted something deterministic so I prompted Claude again. It responded with the following solution where “the child writes first and then signals the parent”. A couple of things to note:\n\nThe child sends a SIGUSR1 signal to the parent pid. (SIGUSR1 stands for “User-defined signal 1”)\nInside parent_process, the file is opened in “append mode”.\n\n\ndef child_process(parent_pid):\n  time.sleep(0.1)  # Small delay to ensure parent is waiting\n  with open(\"test.txt\", \"w\") as f:\n    f.write(\"Written by child\\n\")\n    f.flush()\n  os.kill(parent_pid, signal.SIGUSR1) # this is where the child sends a signal to the parent\n  os._exit(os.EX_OK)\n\ndef parent_process(signum, frame):\n  with open(\"test.txt\", \"a\") as f: # notice the \"a\" for \"append mode\"\n      f.write(\"Written by parent\\n\")\n      f.flush()\n\ndef do_write2():\n  signal.signal(signal.SIGUSR1, parent_process)\n\n  parent_pid = os.getpid()\n\n  if os.fork() == 0:\n      child_process(parent_pid)\n  else:\n      signal.pause()  # Wait for signal from child\n\n  # Read and print the file contents\n  with open(\"test.txt\", \"r\") as f:\n    res = f.read()\n  return res\n\nThis works as expected! At least for the 1000 times that I ran it:\n\nfor _ in range(1000):\n  res = do_write2()\n  assert res == 'Written by child\\nWritten by parent\\n'\n\nI noticed that parent_process is passed signum and frame. I asked Claude to define these:\n\nsignum: This parameter represents the signal number that was caught. In this case, it will be signal.SIGUSR1, which is the signal sent by the child process to the parent. The signum allows the signal handler to identify which signal triggered it, which can be useful if the same handler is used for multiple signals. frame: This parameter is a frame object representing the stack frame of the interrupted code when the signal was received. It contains information about the program’s execution state at the time the signal was caught, such as the current line number and local variables.\n\nI’ll print out signum and frame to see what they look like here:\n\ndef parent_process(signum, frame):\n  print(signum, frame)\n  with open(\"test.txt\", \"a\") as f: # notice the \"a\" for \"append mode\"\n      f.write(\"Written by parent\\n\")\n      f.flush()\n\nsignum has a value of 10 and frame has the additional information as Claude described.\n\ndo_write2()\n\n10 <frame at 0x56005af01c30, file '<ipython-input-56-0f16beee5172>', line 22, code do_write2>\n\n\n'Written by child\\nWritten by parent\\n'\n\n\n\n\nExit Status\nClaude describes the following code as a way to illustrate how “the parent can wait for the child to finish and retrieve its exit status.” I added a couple of print statements to see more clearly that the parent process waits for the child process to exit.\nClaude describes the -1 in os.waitpid(-1, 0) as follows:\n\nWhen -1 is used as the first argument to os.waitpid(), it tells the function to wait for any child process to terminate.\n\nThe 0 in os.waitpid(-1, 0) is explained in the docs:\n\nThe semantics of the call are affected by the value of the integer options, which should be 0 for normal operation.\n\n\ndef do_exit():\n    if os.fork():\n        # Parent process\n        print(\"Parent waiting...\")\n        child_pid, status = os.waitpid(-1, 0)\n        print(\"Parent done waiting!\")\n        print(f\"In parent: {os.getpid()}\")\n        print(f\"Child process (PID {child_pid}) exited with status {os.WEXITSTATUS(status)}\")\n    else:\n        # Child process\n        print(f\"In child: {os.getpid()}, exiting with status 5\")\n        os._exit(5)  # Use os._exit to avoid affecting the notebook process\n\n    print(f\"This will be printed only by the parent process. PID: {os.getpid()}\")\n\nHowever, when I run do_exit, based on the child pid’s shown, it creates two different child processes (4475 and 4448):\n\ndo_exit()\n\nIn child: 4475, exiting with status 5Parent waiting...\nParent done waiting!\nIn parent: 436\nChild process (PID 4448) exited with status 5\nThis will be printed only by the parent process. PID: 436\n\n\n\nAnd note that do_exit print statements don’t always run in that order, indicating that the child process is not running first even though we have used waitpid:\n\ndo_exit()\n\nParent waiting...\nParent done waiting!\nIn parent: 436\nChild process (PID 902) exited with status 0\nThis will be printed only by the parent process. PID: 436\nIn child: 1249, exiting with status 5\n\n\nWhen I put that code into a .py file and run it from the shell, it behaves as expected (there is only one child process created, 5221, and it runs first while the parent process waits):\n\n!python3 do_exit.py\n\nParent waiting...\nIn child: 5221, exiting with status 5\nParent done waiting!\nIn parent: 5216\nChild process (PID 5221) exited with status 5\nThis will be printed only by the parent process. PID: 5216"
  },
  {
    "objectID": "posts/2024-09-30-os-fork/index.html#final-thoughts",
    "href": "posts/2024-09-30-os-fork/index.html#final-thoughts",
    "title": "Experimenting with os.fork",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWorking with os.fork was tougher than I expected. I assumed it would be plug-and-play, but I encountered non-deterministic behavior, which seems to be common when working with multiple processes.\nI also learned that os.fork behaves (or misbehaves) differently when running inside a notebook cell compared to running in the shell. For instance, executing pid = os.fork in a notebook cell causes the execution to hang when trying to return the child’s process ID, or spawns multiple child processes when using the if os_fork:/else: pattern.\nThere are some ways to make os.fork behave in a notebook environment, as we saw when synchronizing work between the child and parent by having the child signal the parent before both wrote to the same file.\nAnother key concept I observed was memory independence— even in a notebook environment, the parent and child processes have their own private access to global variables, allowing you to assign different values to the same variable in each process.\nFuture work: I want to run a similar set of experiments with the multiprocessing library, as I see it used more often (for example, in the fastai repo).\nI hope you enjoyed this blog post. Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2023-09-28-decision-tree/index.html",
    "href": "posts/2023-09-28-decision-tree/index.html",
    "title": "Implementing a Decision Tree Algorithm from Scratch",
    "section": "",
    "text": "In this blog post I’ll work through the second exercise of the “Further Research” section in Chapter 9 of the fastai textbook:\n\nImplement the decision tree algorithm in this chapter from scratch yourself, and try it on the dataset you used in the first exercise.\n\nThe decision tree algorithm, as described in the textbook for the Blue Book for Bulldozers Kaggle competition:\n\nLoop through each column of the dataset in turn.\nFor each column, loop through each possible level of that column in turn.\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. Treat this as a very simple “model” in which our predictions are simply the average sale price of the item’s group.\nAfter looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two groups for our data, based on this selected split. Treat each group as a separate dataset, and find the best split for each by going back to step 1 for each group.\nContinue this process recursively, until you have reached some stopping criterion for each group–for instance, stop splitting a group further when it has only 20 items in it.\n\nI’ll implement the algorithm on my own, then compare it with the algorithm Jeremy implemented in the Lesson 6 video, and the sklearn.DecisionTreeRegressor."
  },
  {
    "objectID": "posts/2023-09-28-decision-tree/index.html#load-the-data",
    "href": "posts/2023-09-28-decision-tree/index.html#load-the-data",
    "title": "Implementing a Decision Tree Algorithm from Scratch",
    "section": "Load the Data",
    "text": "Load the Data\nI’ll follow the same steps as the textbook to load and prepare the training and validation datasets from the Blue Book for Bulldozers Kaggle Competition:\n\n!pip install dtreeviz\n\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\n\nimport zipfile,kaggle\n\npath = Path('bluebook-for-bulldozers')\nif not path.exists():\n  kaggle.api.competition_download_cli(str(path))\n  zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading bluebook-for-bulldozers.zip to /content\n\n\n100%|██████████| 48.4M/48.4M [00:00<00:00, 53.5MB/s]\n\n\n\n\n\n\npath.ls(file_type='text')\n\n(#7) [Path('bluebook-for-bulldozers/Test.csv'),Path('bluebook-for-bulldozers/TrainAndValid.csv'),Path('bluebook-for-bulldozers/random_forest_benchmark_test.csv'),Path('bluebook-for-bulldozers/Machine_Appendix.csv'),Path('bluebook-for-bulldozers/median_benchmark.csv'),Path('bluebook-for-bulldozers/ValidSolution.csv'),Path('bluebook-for-bulldozers/Valid.csv')]\n\n\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\n\n\nlen(df.columns)\n\n53\n\n\n\nsizes = 'Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\nFutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n\n\n\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\n\ndf = add_datepart(df, 'saledate')\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\n\nprocs = [Categorify, FillMissing]\n\n\ncond = (df.saleYear<2011) | (df.saleMonth<10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx), list(valid_idx))\n\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\nlen(to.train), len(to.valid)\n\n(404710, 7988)\n\n\n\nto.show(3)\n\n\n\n  \n    \n      \n      UsageBand\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      fiModelSeries\n      fiModelDescriptor\n      ProductSize\n      fiProductClassDesc\n      state\n      ProductGroup\n      ProductGroupDesc\n      Drive_System\n      Enclosure\n      Forks\n      Pad_Type\n      Ride_Control\n      Stick\n      Transmission\n      Turbocharged\n      Blade_Extension\n      Blade_Width\n      Enclosure_Type\n      Engine_Horsepower\n      Hydraulics\n      Pushblock\n      Ripper\n      Scarifier\n      Tip_Control\n      Tire_Size\n      Coupler\n      Coupler_System\n      Grouser_Tracks\n      Hydraulics_Flow\n      Track_Type\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n      saleIs_month_end\n      saleIs_month_start\n      saleIs_quarter_end\n      saleIs_quarter_start\n      saleIs_year_end\n      saleIs_year_start\n      auctioneerID_na\n      MachineHoursCurrentMeter_na\n      SalesID\n      MachineID\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      saleYear\n      saleMonth\n      saleWeek\n      saleDay\n      saleDayofweek\n      saleDayofyear\n      saleElapsed\n      SalePrice\n    \n  \n  \n    \n      0\n      Low\n      521D\n      521\n      D\n      #na#\n      #na#\n      #na#\n      Wheel Loader - 110.0 to 120.0 Horsepower\n      Alabama\n      WL\n      Wheel Loader\n      #na#\n      EROPS w AC\n      None or Unspecified\n      #na#\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      2 Valve\n      #na#\n      #na#\n      #na#\n      #na#\n      None or Unspecified\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      Standard\n      Conventional\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      1139246\n      999089\n      3157\n      121\n      3.0\n      2004\n      68.0\n      2006\n      11\n      46\n      16\n      3\n      320\n      1.163635e+09\n      11.097410\n    \n    \n      1\n      Low\n      950FII\n      950\n      F\n      II\n      #na#\n      Medium\n      Wheel Loader - 150.0 to 175.0 Horsepower\n      North Carolina\n      WL\n      Wheel Loader\n      #na#\n      EROPS w AC\n      None or Unspecified\n      #na#\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      2 Valve\n      #na#\n      #na#\n      #na#\n      #na#\n      23.5\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      Standard\n      Conventional\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      1139248\n      117657\n      77\n      121\n      3.0\n      1996\n      4640.0\n      2004\n      3\n      13\n      26\n      4\n      86\n      1.080259e+09\n      10.950807\n    \n    \n      2\n      High\n      226\n      226\n      #na#\n      #na#\n      #na#\n      #na#\n      Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\n      New York\n      SSL\n      Skid Steer Loaders\n      #na#\n      OROPS\n      None or Unspecified\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      Auxiliary\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      None or Unspecified\n      None or Unspecified\n      None or Unspecified\n      Standard\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      #na#\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      1139249\n      434808\n      7009\n      121\n      3.0\n      2001\n      2838.0\n      2004\n      2\n      9\n      26\n      3\n      57\n      1.077754e+09\n      9.210340\n    \n  \n\n\n\n\nto = load_pickle('to.pkl')\n\n\nxs,y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y"
  },
  {
    "objectID": "posts/2023-09-28-decision-tree/index.html#decision-tree-algorithm-before-watching-lesson-6",
    "href": "posts/2023-09-28-decision-tree/index.html#decision-tree-algorithm-before-watching-lesson-6",
    "title": "Implementing a Decision Tree Algorithm from Scratch",
    "section": "Decision Tree Algorithm: Before Watching Lesson 6",
    "text": "Decision Tree Algorithm: Before Watching Lesson 6\nI haven’t watched the Lesson 6 video yet, and am assuming that Jeremy walks through how to build a decision tree from scratch in that video, so I’m trying it out first on my own. To be honest, I’m a bit embarrassed to publish this part of my learning process, since my approach is not very elegant, but I’d like to show what my current thinking is and then compare it with what I understand after learning the formal solution to this algorithm.\nI’ll start by creating a mean squared error function to calculate at each split:\n\ndef mse(pred, y): return ((pred-y)**2).mean()\n\n\n# root mse and value\nmse(y, y.mean()), y.mean()\n\n(0.48205692, 10.104347)\n\n\nI’ll walk through the algorithm step-by-step manually for a couple of columns before I create any functions or loops. The first column in the training dataset’s index is the categorical variable UsageBand which has a value of 0, 1, 2, or 3.\n\nxs.columns\n\nIndex(['UsageBand', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls',\n       'saleIs_month_end', 'saleIs_month_start', 'saleIs_quarter_end',\n       'saleIs_quarter_start', 'saleIs_year_end', 'saleIs_year_start',\n       'auctioneerID_na', 'MachineHoursCurrentMeter_na', 'SalesID',\n       'MachineID', 'ModelID', 'datasource', 'auctioneerID', 'YearMade',\n       'MachineHoursCurrentMeter', 'saleYear', 'saleMonth', 'saleWeek',\n       'saleDay', 'saleDayofweek', 'saleDayofyear', 'saleElapsed'],\n      dtype='object')\n\n\n\nxs.UsageBand.unique()\n\narray([2, 1, 3, 0], dtype=int8)\n\n\nThe first split I’ll choose is between rows where the UsageBand value is 0 or not:\n\nmask = xs.UsageBand == 0\nis_zero_xs = xs[mask]\nis_zero_y = y[mask]\n\n\nlen(is_zero_xs), len(is_zero_y)\n\n(334164, 334164)\n\n\n\nisnt_zero_xs = xs[~mask]\nisnt_zero_y = y[~mask]\n\n\nlen(isnt_zero_xs), len(isnt_zero_y)\n\n(70546, 70546)\n\n\nI’ll calculate the average sale price and MSE for each group:\n\nis_zero_y.mean(), mse(is_zero_y, is_zero_y.mean())\n\n(10.080211, 0.46968707)\n\n\n\nisnt_zero_y.mean(), mse(isnt_zero_y, isnt_zero_y.mean())\n\n(10.218686, 0.5248172)\n\n\nI’ll put this code into a routine so I can apply it to all splits of UsageBand:\n\ndef get_cat_splits(xs, y, column):\n  splits = []\n  for el in xs[column].unique():\n    mask = xs[column] == el\n    is_el_y = y[mask]\n    isnt_el_y = y[~mask]\n    is_el_mse = mse(is_el_y, is_el_y.mean())\n    isnt_el_mse = mse(isnt_el_y, isnt_el_y.mean())\n\n    split = {\n        str(el): is_el_mse,\n        \"!\" + str(el): isnt_el_mse\n    }\n\n    splits.append(split)\n  return splits\n\n\nsplits = get_cat_splits(xs, y, \"UsageBand\")\nsplits\n\n[{'2': 0.46612886, '!2': 0.4824535},\n {'1': 0.5223263, '!1': 0.476563},\n {'3': 0.50998193, '!3': 0.47640917},\n {'0': 0.46968707, '!0': 0.5248172}]\n\n\nGreat! My function calculates the MSE for each split in a categorical column. However, I don’t need to store all of the splits for the column, just the best one (the one with the lowest MSE). I’ll modify my function so that it returns only the best split:\n\ndef get_cat_best_split(xs, y, column):\n  best_split = []\n  lowest_mse = 1000000\n\n  for el in xs[column].unique():\n    mask = xs[column] == el\n\n    # ignore splits where either group has 0 or 1 row\n    if sum(mask) == 0: continue\n    if sum(mask) == 1: continue\n    if sum(~mask) == 1: continue\n\n    is_el_y = y[mask]\n    isnt_el_y = y[~mask]\n\n    is_el_mse = mse(is_el_y, is_el_y.mean())\n    isnt_el_mse = mse(isnt_el_y, isnt_el_y.mean())\n\n    if is_el_mse < lowest_mse or isnt_el_mse < lowest_mse:\n      best_split = [el, [is_el_mse, isnt_el_mse]]\n\n    lowest_mse = min(is_el_mse, isnt_el_mse, lowest_mse)\n  print(column)\n  return best_split\n\n\nget_cat_best_split(xs, y, \"UsageBand\")\n\nUsageBand\n\n\n[2, [0.46612886, 0.4824535]]\n\n\nI now have the category for which the split was made (is 2, isn’t 2) and the corresponding MSE.\nNext, I’ll find the best split for one of the continuous columns, YearMade:\n\nxs.YearMade.unique()\n\narray([2004, 1996, 2001, 2007, 1993, 2008, 1000, 1998, 1999, 2003, 1991,\n       2000, 2005, 1995, 2006, 2002, 1984, 1988, 1980, 1992, 1987, 1997,\n       1971, 1978, 1989, 1985, 1979, 1976, 1994, 1982, 1990, 1974, 1968,\n       1966, 1983, 1986, 1981, 1970, 1977, 1975, 1973, 1965, 1967, 2009,\n       2010, 1969, 1972, 1964, 1957, 1958, 1963, 1919, 1920, 1950, 1948,\n       1952, 1942, 1956, 1954, 1953, 1955, 1959, 1960, 1961, 1962, 1951,\n       1937, 1949, 1947, 2012, 2011, 2014], dtype=int16)\n\n\n\nmask = xs.YearMade <= 2004\nlte_2004_xs = xs[mask]\nlte_2004_y = y[mask]\n\n\nlen(lte_2004_xs), len(lte_2004_y)\n\n(364685, 364685)\n\n\n\ngt_2004_xs = xs[~mask]\ngt_2004_y = y[~mask]\n\n\nlen(gt_2004_xs), len(gt_2004_y)\n\n(40025, 40025)\n\n\nSo far the process is pretty similar to what I did for categorical variables. I’ll calculate the average sale price and MSE for each split next:\n\nlte_2004_y.mean(), mse(lte_2004_y, lte_2004_y.mean())\n\n(10.072348, 0.46514994)\n\n\n\ngt_2004_y.mean(), mse(gt_2004_y, gt_2004_y.mean())\n\n(10.395911, 0.5417633)\n\n\nGreat! I’ll wrap this process into a function:\n\ndef get_cont_best_split(xs, y, column):\n  best_split = []\n  lowest_mse = 1000000\n\n  for el in xs[column].unique():\n    mask = xs[column] <= el\n\n    # ignore splits where either group has 0 or 1 row\n    if sum(mask) == 0: continue\n    if sum(mask) == 1: continue\n    if sum(~mask) == 1: continue\n\n    lte_el_y = y[mask]\n    gt_el_y = y[~mask]\n\n    lte_el_mse = mse(lte_el_y, lte_el_y.mean())\n    gt_el_mse = mse(gt_el_y, gt_el_y.mean())\n\n    if lte_el_mse < lowest_mse or gt_el_mse < lowest_mse:\n      best_split = [el, [lte_el_mse, gt_el_mse]]\n\n    lowest_mse = min(lte_el_mse, gt_el_mse, lowest_mse)\n  print(column)\n  return best_split\n\n\n\nget_cont_best_split(xs, y, \"YearMade\")\n\nYearMade\n\n\n[2012, [0.4820588, 0.00022279842]]\n\n\nNow that I have functions to calculate the best split (by MSE) for categorical and continuous variables, I can loop through each column, apply these functions and then determine the overall best split. I’ll use a small dataset for this procedure since the loops require too much time (more than an hour) when I use the full dataset:\n\nxs_sub = xs.sample(1000, random_state=42)\ny_sub = y[xs_sub.index]\n\n\nbest_splits = []\nfor column in xs_sub.columns:\n  if column in to.cat_names:\n    best_splits.append([column, get_cat_best_split(xs_sub, y_sub, column)])\n  if column in to.cont_names:\n    best_splits.append([column, get_cont_best_split(xs_sub, y_sub, column)])\n\n\nbest_splits\n\n[['UsageBand', [0, [0.4289803, 0.5418442]]],\n ['fiModelDesc', [1082, [0.0, 0.45956165]]],\n ['fiBaseModel', [72, [0.0, 0.45933586]]],\n ['fiSecondaryDesc', [139, [0.0010413192, 0.45947686]]],\n ['fiModelSeries', [22, [0.0018927432, 0.45957345]]],\n ['fiModelDescriptor', [138, [0.0011120219, 0.45861065]]],\n ['ProductSize', [6, [0.17651369, 0.45974725]]],\n ['fiProductClassDesc', [59, [0.0028960933, 0.4587686]]],\n ['state', [15, [0.024394397, 0.45930827]]],\n ['ProductGroup', [3, [0.10852089, 0.39364752]]],\n ['ProductGroupDesc', [3, [0.10852089, 0.39364752]]],\n ['Drive_System', [2, [0.0733325, 0.49420977]]],\n ['Enclosure', [3, [0.30207962, 0.37955183]]],\n ['Forks', [2, [0.28580874, 0.4628031]]],\n ['Pad_Type', [4, [0.01935081, 0.46220613]]],\n ['Ride_Control', [1, [0.10788533, 0.53090215]]],\n ['Stick', [1, [0.08875317, 0.49070317]]],\n ['Transmission', [3, [0.037590597, 0.4595029]]],\n ['Turbocharged', [2, [0.043661047, 0.46277064]]],\n ['Blade_Extension', [1, [0.6090261, 0.43294448]]],\n ['Blade_Width', [2, [0.21063821, 0.45938253]]],\n ['Enclosure_Type', [1, [0.09410498, 0.45690852]]],\n ['Engine_Horsepower', [2, [0.08047946, 0.45704415]]],\n ['Hydraulics', [7, [0.00222363, 0.4558903]]],\n ['Pushblock', [2, [0.17637664, 0.4491221]]],\n ['Ripper', [1, [0.3071829, 0.4566584]]],\n ['Scarifier', [0, [0.43313345, 0.59920347]]],\n ['Tip_Control', [0, [0.43313345, 0.59920347]]],\n ['Tire_Size', [6, [0.003916449, 0.45774794]]],\n ['Coupler', [0, [0.35867354, 0.54620147]]],\n ['Coupler_System', [2, [0.10629387, 0.45753148]]],\n ['Grouser_Tracks', [2, [0.033831615, 0.45868516]]],\n ['Hydraulics_Flow', [0, [0.39364752, 0.10852089]]],\n ['Track_Type', [1, [0.2580127, 0.45796755]]],\n ['Undercarriage_Pad_Width', [4, [0.08790448, 0.459086]]],\n ['Stick_Length', [21, [0.03558779, 0.45946068]]],\n ['Thumb', [2, [0.23120114, 0.4605685]]],\n ['Pattern_Changer', [2, [0.4319223, 0.45872542]]],\n ['Grouser_Type', [1, [0.4383151, 0.45913604]]],\n ['Backhoe_Mounting', [0, [0.46665692, 0.3895407]]],\n ['Blade_Type', [10, [0.0015298143, 0.4583235]]],\n ['Travel_Controls', [4, [0.0022710091, 0.45842946]]],\n ['Differential_Type', [3, [0.008903191, 0.4579678]]],\n ['Steering_Controls', [2, [0.39570603, 0.46438074]]],\n ['saleIs_month_end', [1, [0.46104407, 0.36129642]]],\n ['saleIs_month_start', [1, [0.46031126, 0.39876112]]],\n ['saleIs_quarter_end', [1, [0.4600184, 0.33042803]]],\n ['saleIs_quarter_start', [1, [0.45814824, 0.48365197]]],\n ['saleIs_year_end', [1, [0.45866725, nan]]],\n ['saleIs_year_start', [1, [0.45866725, nan]]],\n ['auctioneerID_na', [1, [0.45774823, 0.46984664]]],\n ['MachineHoursCurrentMeter_na', [2, [0.41661143, 0.51914454]]],\n ['SalesID', [1144455, [0.008310286, 0.45859787]]],\n ['MachineID', [17776, [0.003757841, 0.4595788]]],\n ['ModelID', [36033, [0.45857352, 0.0010936307]]],\n ['datasource', [132, [0.43562403, 0.49841937]]],\n ['auctioneerID', [0.0, [0.34691957, 0.45884383]]],\n ['YearMade', [1974, [0.3009956, 0.46007067]]],\n ['MachineHoursCurrentMeter', [14856.0, [0.459368, 0.2509487]]],\n ['saleYear', [1989, [0.09763114, 0.45981508]]],\n ['saleMonth', [8, [0.44693333, 0.48317593]]],\n ['saleWeek', [50, [0.45991018, 0.35147443]]],\n ['saleDay', [30, [0.4608025, 0.2767068]]],\n ['saleDayofweek', [4, [0.4637312, 0.39367497]]],\n ['saleDayofyear', [10, [0.0038693824, 0.45864925]]],\n ['saleElapsed', [607910400.0, [0.0004528304, 0.45936278]]]]\n\n\n\nlowest_mse = 10000\nfor split in best_splits:\n  if len(split[1]) > 0:\n    if min(split[1][1]) < lowest_mse:\n      lowest_mse = min(split[1][1])\n      best_split_column = split[0]\n      best_split_value = split[1][0]\nbest_split_column, lowest_mse, best_split_value\n\n('fiModelDesc', 0.0, 1082)\n\n\nThere were numerous splits with an MSE of 0.0 so I’ll just pick the first one which is for when fiModelDesc is 1082.\n\nmask = xs_sub.fiModelDesc == best_split_value\nleft_xs = xs_sub[mask]\nright_xs = xs_sub[~mask]\n\nleft_y = y_sub[mask]\nright_y = y_sub[~mask]\n\nlen(left_xs), len(right_xs)\n\n(2, 998)\n\n\n\nleft_y.mean(), right_y.mean()\n\n(9.998797, 10.110093)\n\n\n\nmse(left_y, left_y.mean()), mse(right_y, right_y.mean())\n\n(0.0, 0.45956165)\n\n\n\nleft_xs\n\n\n\n  \n    \n\n\n  \n    \n      \n      UsageBand\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      ...\n      saleDay\n      saleDayofweek\n      saleDayofyear\n      saleElapsed\n    \n  \n  \n    \n      292246\n      0\n      1082\n      326\n      93\n      ...\n      25\n      2\n      84\n      1.237939e+09\n    \n    \n      292242\n      0\n      1082\n      326\n      93\n      ...\n      25\n      2\n      84\n      1.237939e+09\n    \n  \n\n2 rows × 66 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nI’ll leave the left_xs split as a leaf, since it has only two values. I’ll continue to split right_xs:\n\nbest_splits = []\nfor column in right_xs.columns:\n  if column in to.cat_names and column != 'fiModelDesc':\n    best_splits.append([column, get_cat_best_split(right_xs, right_y, column)])\n  if column in to.cont_names:\n    best_splits.append([column, get_cont_best_split(right_xs, right_y, column)])\n\n\nlowest_mse = 10000\nfor split in best_splits:\n  if len(split[1]) > 0:\n    if min(split[1][1]) < lowest_mse:\n      lowest_mse = min(split[1][1])\n      best_split_column = split[0]\n      best_split_value = split[1][0]\nbest_split_column, lowest_mse, best_split_value\n\n('fiBaseModel', 0.0, 72)\n\n\nThe next best split is for fiBaseModel:\n\nmask = right_xs.fiBaseModel == best_split_value\nleft_xs = right_xs[mask]\nright_xs = right_xs[~mask]\n\nleft_y = right_y[mask]\nright_y = right_y[~mask]\n\nlen(left_xs), len(right_xs)\n\n(2, 996)\n\n\n\nleft_xs\n\n\n\n  \n    \n\n\n  \n    \n      \n      UsageBand\n      fiModelDesc\n      fiBaseModel\n      fiSecondaryDesc\n      ...\n      saleDay\n      saleDayofweek\n      saleDayofyear\n      saleElapsed\n    \n  \n  \n    \n      372510\n      0\n      1030\n      313\n      50\n      ...\n      17\n      2\n      321\n      1.289952e+09\n    \n    \n      366529\n      0\n      2214\n      698\n      31\n      ...\n      12\n      1\n      102\n      1.302566e+09\n    \n  \n\n2 rows × 66 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nleft_y.mean(), right_y.mean()\n\n(10.463103, 10.109385)\n\n\n\nmse(left_y, left_y.mean()), mse(right_y, right_y.mean())\n\n(0.0, 0.4602337)\n\n\nAgain, one of the splits only has two values, so I’ll keep that as the second leaf, and continue to split the other group:\n\nbest_splits = []\nfor column in right_xs.columns:\n  if column in to.cat_names and column not in ['fiModelDesc', 'fiBaseModel']:\n    best_splits.append([column, get_cat_best_split(right_xs, right_y, column)])\n  if column in to.cont_names:\n    best_splits.append([column, get_cont_best_split(right_xs, right_y, column)])\n\n\nlowest_mse = 10000\nfor split in best_splits:\n  if len(split[1]) > 0:\n    if min(split[1][1]) < lowest_mse:\n      lowest_mse = min(split[1][1])\n      best_split_column = split[0]\n      best_split_value = split[1][0]\nbest_split_column, lowest_mse, best_split_value\n\n('saleElapsed', 0.0004528304, 607910400.0)\n\n\nThe final split (to reach four leaf nodes like the textbook) is on saleElapsed.\n\nmask = right_xs.saleElapsed <= best_split_value\nleft_xs = right_xs[mask]\nright_xs = right_xs[~mask]\n\nleft_y = right_y[mask]\nright_y = right_y[~mask]\n\nmse(left_y, left_y.mean()), mse(right_y, right_y.mean())\n\n(0.0004528304, 0.4609359)\n\n\n\nleft_y.mean(), right_y.mean()\n\n(9.776848, 10.110054)\n\n\n\nlen(left_xs), len(right_xs)\n\n(2, 994)\n\n\nPlotting the decision tree (manually) to visualize the splits:\n\n\n\nMy Decision Tree Algorithm\n\n\nMy model seems to be making incremental splits (with 2 rows taken off each split) based on very small sample sizes. One question I have is if the decision tree is always supposed to select the minimum or is it choosing randomly one of the many columns with relatively small MSE values? Another question: am I supposed to exclude a column from future splits once a split has been made on the column (which is what I’m doing currently)? I’ll be looking for these answers when watching the Lesson 6 video."
  },
  {
    "objectID": "posts/2023-09-28-decision-tree/index.html#decision-tree-algorithm-after-watching-lesson-6",
    "href": "posts/2023-09-28-decision-tree/index.html#decision-tree-algorithm-after-watching-lesson-6",
    "title": "Implementing a Decision Tree Algorithm from Scratch",
    "section": "Decision Tree Algorithm: After Watching Lesson 6",
    "text": "Decision Tree Algorithm: After Watching Lesson 6\nThe algorithm presented in the Lesson 6 video(from Jeremy’s notebook How random forests really work) evaluates a split based on a weighted standard deviation calculation.\nFor a given condition (for example Age <= 6) Jeremy calculates the following:\n\nA “side score” which is the product of the standard deviation of the dependent variable and number of rows for rows that satisfy the condition.\nA side score for the rows that don’t satisfy the condition.\nThe sum of both side scores divided by the number of total rows in the dependent variable column.\n\nThen for each column, he calculates the side score for splits at each unique value of the column and gets the minimum score (and corresponding split value). The column (and corresponding value) with the split that has the smallest score is determined as the best split.\nConceptually: the column (and value) where the weighted standard deviation is the smallest is the column where a split on that value will create two groups where within each group there will be dependent variable values that are relatively close to each other.\nI’ll start by coding the _side_score function Jeremy defined, which calculates the product of the standard deviation of the dependent variable and number of rows in the give split side:\n\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot<=1: return 0\n    return y[side].std()*tot\n\nNext, the function score which calculates the sum of each side’s _side_score divided by the total number of rows, or in other words, the weighted average of each side’s standard deviation:\n\ndef score(col, y, split):\n    lhs = col<=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nI’ll reload the data so and take the same subset as above so I can compare the splits this algorithm makes to the ones made previously:\n\nto = load_pickle(\"to.pkl\")\n\n\nxs,y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\n\nxs_sub = xs.sample(1000, random_state=42)\ny_sub = y[xs_sub.index]\n\n\nmse(y_sub, y_sub.mean()), y_sub.mean()\n\n(0.45866725, 10.109871)\n\n\n\nscore(xs['UsageBand'], y, 2)\n\n0.6922498415264088\n\n\nI’ll walk through each step of the score calculation manually to visualize it better.\nFirst we create a boolean array of rows that satisfy or don’t satisfy the condition:\n\nlhs = xs['UsageBand'] <= 2\n\n\nlhs\n\n0          True\n1          True\n2          True\n3          True\n4         False\n          ...  \n412693     True\n412694     True\n412695     True\n412696     True\n412697     True\nName: UsageBand, Length: 404710, dtype: bool\n\n\nNext: for how many columns is the condition met?\n\ntot = lhs.sum()\ntot\n\n370444\n\n\nWhat is the standard deviation of the dependent variable for rows where the condition is met?\n\ny[lhs].std()\n\n0.69022495\n\n\nFinally, what is the side score for this condition?\n\nside_score_lhs = y[lhs].std()*tot\nside_score_lhs\n\n255689.68972754478\n\n\nWe ask the same questions for the rows where the condition is NOT met:\n\ntot = (~lhs).sum()\ntot\n\n34266\n\n\n\ny[~lhs].std()\n\n0.71414065\n\n\n\nside_score_rhs = y[~lhs].std()*tot\nside_score_rhs\n\n24470.743636608124\n\n\nFinally, we add the two side scores together and divide by the number of total rows:\n\n(side_score_lhs + side_score_rhs) / len(y)\n\n0.6922498415264088\n\n\nThankfully, this is equal to the output from the function _side_score! I’ll continue by getting the code for calculating this score for splits on each of the unique values in a column:\n\ndef min_col(xs, nm, y):\n    col = xs[nm]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nLet’s test it out on UsageBand:\n\nmin_col(xs, 'UsageBand', y)\n\n(0, 0.6921555579861831)\n\n\nAgain, I’ll go line-by-line through the code of min_col to make sure I understand what’s going on:\n\ncol = xs['UsageBand']\ncol\n\n0         2\n1         2\n2         1\n3         1\n4         3\n         ..\n412693    0\n412694    0\n412695    0\n412696    0\n412697    0\nName: UsageBand, Length: 404710, dtype: int8\n\n\nWhat are the unique values in this column?\n\nunq = col.dropna().unique()\nunq\n\narray([2, 1, 3, 0], dtype=int8)\n\n\nIf we split the data on each of the unique values, what is the score for each split?\n\nscores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\nscores\n\narray([0.69224984, 0.69378292, 0.69430405, 0.69215556])\n\n\nWhat is the minimum score, and what value does it correspond to?\n\nidx = scores.argmin()\nidx\n\n3\n\n\n\nunq[idx], scores[idx]\n\n(0, 0.6921555579861831)\n\n\nNext, I’ll iterate through all of the columns in the data and calculate the minimum score (and corresponding value) for each one. I’ll use the smaller subset of the data since the list comprehension was taking more than an hour to compute on the full dataset.\n\ncol_scores = {o: min_col(xs_sub, o, y_sub) for o in xs_sub.columns}\n\n\ncol_scores\n\n{'UsageBand': (0, 0.6710787683725357),\n 'fiModelDesc': (150, 0.6684566705226899),\n 'fiBaseModel': (30, 0.66775039935112),\n 'fiSecondaryDesc': (15, 0.6461849688887596),\n 'fiModelSeries': (60, 0.6612360656261445),\n 'fiModelDescriptor': (9, 0.6385975532531738),\n 'ProductSize': (0, 0.649804151058197),\n 'fiProductClassDesc': (7, 0.6468342208862304),\n 'state': (52, 0.6769107410311699),\n 'ProductGroup': (3, 0.6413102557659149),\n 'ProductGroupDesc': (3, 0.6413102557659149),\n 'Drive_System': (3, 0.6595019570589066),\n 'Enclosure': (3, 0.6443507042527199),\n 'Forks': (0, 0.6468727025985718),\n 'Pad_Type': (0, 0.6484307520389557),\n 'Ride_Control': (0, 0.6693848296999931),\n 'Stick': (0, 0.6484307520389557),\n 'Transmission': (6, 0.6698495596647263),\n 'Turbocharged': (0, 0.6484307520389557),\n 'Blade_Extension': (0, 0.6660390158891678),\n 'Blade_Width': (2, 0.665115752696991),\n 'Enclosure_Type': (0, 0.6660390158891678),\n 'Engine_Horsepower': (0, 0.6660390158891678),\n 'Hydraulics': (0, 0.6497658799290658),\n 'Pushblock': (0, 0.6660390158891678),\n 'Ripper': (0, 0.6618279729485512),\n 'Scarifier': (0, 0.6660390158891678),\n 'Tip_Control': (0, 0.6660390158891678),\n 'Tire_Size': (3, 0.6607923060655594),\n 'Coupler': (2, 0.6719830477237702),\n 'Coupler_System': (0, 0.5992864975929261),\n 'Grouser_Tracks': (0, 0.5992864975929261),\n 'Hydraulics_Flow': (0, 0.5992864975929261),\n 'Track_Type': (1, 0.6633642191886902),\n 'Undercarriage_Pad_Width': (5, 0.6722126321792603),\n 'Stick_Length': (1, 0.6726506268978119),\n 'Thumb': (0, 0.6727646491527557),\n 'Pattern_Changer': (0, 0.6727646491527557),\n 'Grouser_Type': (0, 0.6727646491527557),\n 'Backhoe_Mounting': (0, 0.6719208754301071),\n 'Blade_Type': (5, 0.6712375184893609),\n 'Travel_Controls': (2, 0.6719208754301071),\n 'Differential_Type': (0, 0.6718101676702499),\n 'Steering_Controls': (0, 0.6718101676702499),\n 'saleIs_month_end': (1, 0.6775472568273544),\n 'saleIs_month_start': (1, 0.6773856681585312),\n 'saleIs_quarter_end': (1, 0.6774418947696685),\n 'saleIs_quarter_start': (2, 0.6775886416435242),\n 'saleIs_year_end': (1, 0.6775886416435242),\n 'saleIs_year_start': (1, 0.6775886416435242),\n 'auctioneerID_na': (2, 0.6775886416435242),\n 'MachineHoursCurrentMeter_na': (1, 0.6731610369682312),\n 'SalesID': (1614635, 0.6669842964410782),\n 'MachineID': (1010754, 0.6537257554531097),\n 'ModelID': (4336, 0.6421609473228455),\n 'datasource': (132, 0.674677339553833),\n 'auctioneerID': (1.0, 0.674844207584858),\n 'YearMade': (1974, 0.6595817529559136),\n 'MachineHoursCurrentMeter': (3360.0, 0.6672791358232498),\n 'saleYear': (1991, 0.6751361751556396),\n 'saleMonth': (4, 0.6770030355453491),\n 'saleWeek': (17, 0.6766363668441773),\n 'saleDay': (10, 0.6758146023750305),\n 'saleDayofweek': (4, 0.6748345303535461),\n 'saleDayofyear': (10, 0.6763967939466238),\n 'saleElapsed': (688348800.0, 0.6746526069641113)}\n\n\nI’ll then find the split across all columns with the lowest score:\n\nmin_col_score = min(col_scores.values())\nmin_col_names = [key for key in col_scores if col_scores[key] == min_col_score]\n\n\nmin_col_score, min_col_names\n\n((0, 0.5992864975929261),\n ['Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow'])\n\n\nThe lowest score was around the split of <=0 for three columns, which interestingly enough, were found to be redundant features in the textbook chapter. I’ll pick Coupler_System since that’s the split used in the textbook.\n\nxs_sub.Coupler_System.unique()\n\narray([0, 1, 2], dtype=int8)\n\n\nTo answer one of my earlier questions, I think we only remove a column from future splits if it was a binary column (such as the Sex column was in the Titanic dataset) since there are only two possible groups and once the data has been split into those two groups, there’s nothing left to split in that column.\n\nmask = xs_sub.Coupler_System <= 0\nlhs = xs_sub[mask]\nrhs = xs_sub[~mask]\n\nlhs_y = y_sub[mask]\nrhs_y = y_sub[~mask]\n\nlen(lhs), len(rhs)\n\n(904, 96)\n\n\n\nlhs_y.mean(), rhs_y.mean()\n\n(10.208925, 9.177121)\n\n\n\nmse(lhs_y, lhs_y.mean()), mse(rhs_y, rhs_y.mean())\n\n(0.39364752, 0.10852089)\n\n\nThe lefthand-side has many more rows so I’ll continue splitting that dataset and keep rhs as a leaf node.\nCoupler_System only has one value, 0, in this group so I’ll remove it from future splits.\n\nlhs.Coupler_System.unique()\n\narray([0], dtype=int8)\n\n\n\nlhs = lhs.drop(\"Coupler_System\", axis=1)\n\n\n'Coupler_System' in lhs.columns\n\nFalse\n\n\n\ncol_scores = {o: min_col(lhs, o, lhs_y) for o in lhs.columns}\nmin_col_score = min(col_scores.values())\nmin_col_names = [key for key in col_scores if col_scores[key] == min_col_score]\nmin_col_score, min_col_names\n\n((0, 0.5869915567140663), ['Pad_Type', 'Stick', 'Turbocharged'])\n\n\nAgain we have three columns with the same split score. I’ll choose Pad_Type as the column to split on.\n\nlhs.Pad_Type.unique()\n\narray([2, 0, 3, 4], dtype=int8)\n\n\n\nmask = lhs.Pad_Type <= 0\nrhs = lhs[~mask]\nlhs = lhs[mask]\n\nrhs_y = lhs_y[~mask]\nlhs_y = lhs_y[mask]\n\nlen(lhs), len(rhs)\n\n(700, 204)\n\n\n\nlhs_y.mean(), rhs_y.mean()\n\n(10.30318, 9.885499)\n\n\n\nmse(lhs_y, lhs_y.mean()), mse(rhs_y, rhs_y.mean())\n\n(0.43736917, 0.108533934)\n\n\nAgain, the lefthand-side has more rows so I’ll continue to split it. I’ll keep rhs as the second leaf node. I’ll also remove Pad_Type from lhs since it has a single value of 0.\n\nlhs = lhs.drop(\"Pad_Type\", axis=1)\n\n\n'Pad_Type' in lhs.columns\n\nFalse\n\n\n\ncol_scores = {o: min_col(lhs, o, lhs_y) for o in lhs.columns}\nmin_col_score = min(col_scores.values())\nmin_col_names = [key for key in col_scores if col_scores[key] == min_col_score]\nmin_col_score, min_col_names\n\n((0, 0.6552193513938359),\n ['Drive_System',\n  'Blade_Extension',\n  'Enclosure_Type',\n  'Engine_Horsepower',\n  'Scarifier',\n  'Tip_Control'])\n\n\nI now have six columns that have the same split score. I’ll choose the first one, Drive_System to make my final split and get my last two leaf nodes.\n\nmask = lhs.Drive_System <= 0\nrhs = lhs[~mask]\nlhs = lhs[mask]\n\nrhs_y = lhs_y[~mask]\nlhs_y = lhs_y[mask]\n\nlen(lhs), len(rhs)\n\n(638, 62)\n\n\n\nlhs_y.mean(), rhs_y.mean()\n\n(10.275307, 10.590003)\n\n\n\nmse(lhs_y, lhs_y.mean()), mse(rhs_y, rhs_y.mean())\n\n(0.41287065, 0.59920347)\n\n\nHere is a manually made visualization of this four-leaf-node decision tree:\n\n\n\nJeremy’s Decision Tree"
  },
  {
    "objectID": "posts/2023-09-28-decision-tree/index.html#decision-tree-algorithm-using-sklearn",
    "href": "posts/2023-09-28-decision-tree/index.html#decision-tree-algorithm-using-sklearn",
    "title": "Implementing a Decision Tree Algorithm from Scratch",
    "section": "Decision Tree Algorithm: Using sklearn",
    "text": "Decision Tree Algorithm: Using sklearn\nThe last decision tree that I’ll create for comparison with the other two approaches.\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs_sub, y_sub);\n\n\nfrom sklearn.tree import export_graphviz\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m, xs_sub, size=10)"
  },
  {
    "objectID": "posts/2023-09-28-decision-tree/index.html#final-thoughts",
    "href": "posts/2023-09-28-decision-tree/index.html#final-thoughts",
    "title": "Implementing a Decision Tree Algorithm from Scratch",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIn this exercise, I tried three different algorithms to create a four-leaf-node decision tree on a 1000-sample subset of the Blue Book for Bulldozer’s Kaggle competition dataset:\n\nAn algorithm I created where the model made splits based on the lowest MSE.\nJeremy’s “side score” algorithm which found the split with the lowest weighted standard deviation.\nsklearn.DecisionTreeRegressor which also uses MSE to decide splits.\n\n\n\n\nDecision Tree Comparison\n\n\nHere are some key observations:\n\nMy algorithm made splits with very small samples. To improve this, I would either explicitly ignore splits with low samples or penalize such splits by weighting by number of rows like Jeremy did in _side_score.\nThe sklearn.DecisionTreeRegressor had the same initial split as Jeremy’s (on Coupler_System) but then chose two completely different columns to split on for the remainder of the leaf nodes. I wonder if this difference in column selection is due to the difference in how a split is scored between those two algorithms.\nA future improvement I could make to this experiment is compare the MSE on a validation set for each decision tree.\nAnother future improvement would be to make my algorithm and Jeremy’s algorithm run faster so that I can test it on the actual dataset in a reasonable amount of time.\n\nThis was another challenging and enjoyable “Further Research” exercise provided by the fastai textbook. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-08-19-tinystories-8m-finetune/index.html",
    "href": "posts/2024-08-19-tinystories-8m-finetune/index.html",
    "title": "Fine-tuning TinyStories-8M on the financial_phrasebank Dataset",
    "section": "",
    "text": "In a previous blog post I finetuned the TinyStories-33M model on the financial_phrasebank dataset and achieved a ~85% accuracy on the validation set and an ~80% accuracy on the test set.\nIn this notebook, I’ll finetune the much smaller TinyStories-8M model and see how it performs. I expect it to perform worse. In future notebooks, I’ll also finetune the 3M and 1M TinyStories models. I also suspect these models might perform better on a (synthetically generated) simpler version of this dataset, which I plan to explore in a future notebook.\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2024-08-20T01:40:48.284727Z”,“iopub.status.busy”:“2024-08-20T01:40:48.284346Z”,“iopub.status.idle”:“2024-08-20T01:41:09.248980Z”,“shell.execute_reply”:“2024-08-20T01:41:09.248146Z”,“shell.execute_reply.started”:“2024-08-20T01:40:48.284695Z”}’ trusted=‘true’}\n\nShow imports and setup\n#!pip install accelerate evaluate datasets -Uqq\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, TrainerCallback\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nimport gc\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n#model_nm = \"roneneldan/TinyStories-33M\"\n#model_nm = \"roneneldan/TinyStories-1M\"\n#model_nm = \"roneneldan/TinyStories-3M\"\nmodel_nm = \"roneneldan/TinyStories-8M\"\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\ndef tok_func(x): return tokz(x[\"input\"], padding=True, truncation=True)\n\n:::"
  },
  {
    "objectID": "posts/2024-08-19-tinystories-8m-finetune/index.html#preparing-datasets",
    "href": "posts/2024-08-19-tinystories-8m-finetune/index.html#preparing-datasets",
    "title": "Fine-tuning TinyStories-8M on the financial_phrasebank Dataset",
    "section": "Preparing Datasets",
    "text": "Preparing Datasets\nMuch of the code in this section is boilerplate, tokenizing the dataset and splitting it into training, validation and test sets.\n\n\nShow load_dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\",\n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\ndataset = dataset.rename_columns({'label':'labels', 'sentence': 'input'})\n\n\n\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ntokz.padding_side = \"left\" # https://github.com/huggingface/transformers/issues/16595 and https://www.kaggle.com/code/baekseungyun/gpt-2-with-huggingface-pytorch\ntok_ds = dataset.map(tok_func, batched=True)\n\n\ntok_ds[0]['input']\n\n'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'\n\n\n\ntok_ds[0]['input_ids'][100:110] # first 100 elements are 50257 ('[PAD]')\n\n[50257, 50257, 50257, 50257, 50257, 50257, 4821, 284, 17113, 837]\n\n\n\ntokz.decode(50257), tokz.decode(4821), tokz.decode(284), tokz.decode(17113)\n\n('[PAD]', 'According', ' to', ' Gran')\n\n\n\ntok_ds[0]['labels']\n\n1\n\n\n\nsplit_dataset = tok_ds.train_test_split(test_size=225/2264, seed=42)\n\ntraining_split = split_dataset['train'].train_test_split(test_size=0.2, seed=42)\n\ntrain_ds = training_split['train']\neval_ds = training_split['test']\ntest_ds = split_dataset['test']\n\ntrain_ds, eval_ds, test_ds\n\n(Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 1631\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 408\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 225\n }))\n\n\n\ntrain_ds[0]['input']\n\n'The result will also be burdened by increased fixed costs associated with operations in China , and restructuring costs in Japan .'\n\n\n\ntrain_ds[0]['labels']\n\n0\n\n\nThe dataset distributions show a predominance of neutral (1) sentences:\n\ntrain_ds.to_pandas()['labels'].value_counts() / len(train_ds)\n\nlabels\n1    0.622318\n2    0.251993\n0    0.125690\nName: count, dtype: float64\n\n\n\neval_ds.to_pandas()['labels'].value_counts() / len(eval_ds)\n\nlabels\n1    0.615196\n2    0.257353\n0    0.127451\nName: count, dtype: float64\n\n\n\ntest_ds.to_pandas()['labels'].value_counts() / len(test_ds)\n\nlabels\n1    0.555556\n2    0.240000\n0    0.204444\nName: count, dtype: float64"
  },
  {
    "objectID": "posts/2024-08-19-tinystories-8m-finetune/index.html#prepare-for-training",
    "href": "posts/2024-08-19-tinystories-8m-finetune/index.html#prepare-for-training",
    "title": "Fine-tuning TinyStories-8M on the financial_phrasebank Dataset",
    "section": "Prepare for Training",
    "text": "Prepare for Training\nMuch of the code in this section is either helper functions (like get_acc, MetricCallback, or results_to_dataframe) or boilerplate code to prepare a HuggingFace trainer:\n\ndef get_acc(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": (predictions == labels).astype(np.float32).mean().item()}\n\n\n\nShow MetricCallback code\n# thanks Claude\n\nclass MetricCallback(TrainerCallback):\n    def __init__(self):\n        self.metrics = []\n        self.current_epoch_metrics = {}\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            self.current_epoch_metrics.update(logs)\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        if hasattr(state, 'log_history') and state.log_history:\n            # Get the last logged learning rate\n            last_lr = state.log_history[-1].get('learning_rate', None)\n        else:\n            last_lr = None\n\n        self.metrics.append({\n            \"epoch\": state.epoch,\n            \"learning_rate\": last_lr,\n            **self.current_epoch_metrics\n        })\n        self.current_epoch_metrics = {}  # Reset for next epoch\n\n    def on_train_end(self, args, state, control, **kwargs):\n        # Capture final metrics after the last epoch\n        if self.current_epoch_metrics:\n            self.metrics.append({\n                \"epoch\": state.num_train_epochs,\n                \"learning_rate\": self.metrics[-1].get('learning_rate') if self.metrics else None,\n                **self.current_epoch_metrics\n            })\n\n\n\n\nShow function to convert results dict into DataFrame\ndef results_to_dataframe(results, model_name):\n    rows = []\n    for result in results:\n        initial_lr = result['learning_rate']\n        for metric in result['metrics']:\n            row = {\n                'model_name': model_name,\n                'initial_learning_rate': initial_lr,\n                'current_learning_rate': metric.get('learning_rate'),\n            }\n            row.update(metric)\n            rows.append(row)\n    \n    df = pd.DataFrame(rows)\n    \n    # Ensure specific columns are at the beginning\n    first_columns = ['model_name', 'initial_learning_rate', 'current_learning_rate', 'epoch']\n    other_columns = [col for col in df.columns if col not in first_columns]\n    df = df[first_columns + other_columns]\n    \n    return df\n\n\n\n\nShow function to make confusion matrix\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n    \n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['pred_text'], labels=['negative', 'neutral', 'positive'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive'])\n    \n    fig, ax = plt.subplots(figsize=(4,4))\n    disp.plot(ax=ax,text_kw={'fontsize': 12}, cmap='Blues', colorbar=False);\n    \n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    \n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=14)\n    ax.tick_params(axis='x', labelsize=14)\n\n\n\n\nShow function to generate a prediction\ndef get_prediction(model, text, tokz):\n    # Determine the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Move the model to the appropriate device\n    model = model.to(device)\n\n    # Tokenize the input text\n    inputs = tokz(text, return_tensors=\"pt\", truncation=True, padding=True)\n\n    # Move input tensors to the same device as the model\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Get the model's prediction\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Ensure logits are on CPU for numpy operations\n    logits = outputs.logits.detach().cpu()\n\n    # Get probabilities\n    probs = torch.softmax(logits, dim=-1)\n\n    # Get the predicted class\n    p_class = torch.argmax(probs, dim=-1).item()\n\n    # Get the probability for the predicted class\n    p = probs[0][p_class].item()\n\n    labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n    \n    print(f\"Probability: {p:.2f}\")\n    print(f\"Predicted label: {labels[p_class]}\")\n    return p_class, p\n\n\n\n\nShow function to prep trainer\ndef get_trainer(lr, bs=16):\n\n    args = TrainingArguments(\n        'outputs',\n        learning_rate=lr,\n        warmup_ratio=0.1,\n        lr_scheduler_type='cosine',\n        fp16=True,\n        eval_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        per_device_train_batch_size=bs,\n        per_device_eval_batch_size=bs*2,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        report_to='none')\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=3) # 3 labels for 3 classes\n    model.resize_token_embeddings(len(tokz))\n    model.config.pad_token_id = model.config.eos_token_id\n    \n    trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=eval_ds, \n                  tokenizer=tokz, compute_metrics=get_acc, callbacks=[metric_callback])\n    \n    return trainer, args\n\n\n\n\nShow function to get test set accuracy\ndef get_test_df(trainer):\n    test_df = test_ds.to_pandas()[['input', 'labels']]\n    \n    preds = trainer.predict(test_ds).predictions.astype(float)\n    probs = F.softmax(torch.tensor(preds), dim=1)\n    predicted_classes = torch.argmax(probs, dim=1).numpy()\n\n    test_df['predicted'] = predicted_classes\n    \n    test_df['match'] = test_df['labels'] == test_df['predicted']\n    acc = test_df['match'].mean()\n    \n    label_map = {i: label_text for i, label_text in enumerate(test_ds.features[\"labels\"].names)}\n    test_df['label_text'] = test_df['labels'].apply(lambda x: label_map[x])\n    test_df['pred_text'] = test_df['predicted'].apply(lambda x: label_map[x])\n    \n    return test_df, acc"
  },
  {
    "objectID": "posts/2024-08-19-tinystories-8m-finetune/index.html#training-learning-rate-sweep",
    "href": "posts/2024-08-19-tinystories-8m-finetune/index.html#training-learning-rate-sweep",
    "title": "Fine-tuning TinyStories-8M on the financial_phrasebank Dataset",
    "section": "Training: Learning Rate Sweep",
    "text": "Training: Learning Rate Sweep\nWhile there are other hyperparameters to tune (warmup_ratio, weight_decay) I’ll focus this notebook on fine-tuning with different learning rates. I’ll start with the same learning rates that I used for the 33M model:\n\n\nShow training loop\nmetrics = []\ntrainers = []\nlearning_rates = [1e-6, 1e-5, 3e-5, 5e-5, 8e-5, 1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 1e-2, 1e-1]\n\nfor lr in learning_rates:\n    print(f\"Learning Rate: {lr}\")\n    \n    metric_callback = MetricCallback()\n    \n    trainer, args = get_trainer(lr, bs=64)\n\n    trainer.train()\n\n    metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nmetrics_df = results_to_dataframe(metrics, model_name=\"TinyStories-8M\")\nmetrics_df = metrics_df.query('current_learning_rate.notna()')"
  },
  {
    "objectID": "posts/2024-08-19-tinystories-8m-finetune/index.html#results",
    "href": "posts/2024-08-19-tinystories-8m-finetune/index.html#results",
    "title": "Fine-tuning TinyStories-8M on the financial_phrasebank Dataset",
    "section": "Results",
    "text": "Results\n\nHighest Validation Set Accuracy\nThe highest validation set accuracy (82%) was obtained with a learning rate of 8e-5.\n\nmetrics_df.query('eval_accuracy == eval_accuracy.max()')\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      19\n      TinyStories-8M\n      0.00008\n      0.0\n      3.0\n      0.0\n      0.2323\n      632259.5\n      0.44796\n      0.823529\n      0.4288\n      951.429\n      4.664\n      12.5028\n      391.353\n      3.119\n      2.427998e+13\n      0.49546\n    \n  \n\n\n\n\n\nlearning_rates[4]\n\n8e-05\n\n\nThis model actually has a higher test accuracy than the 33M model (81% > 79%)—a result that I was not expecting!\n\ntest_df, acc = get_test_df(trainers[4])\nacc\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n\n0.8133333333333334\n\n\nThis 8M parameter finetuned model predicts neutral sentences the best (117/125) followed by positive sentences (39/54) and lastly, negative sentences (27/46). It’s interesting to note that the dataset contains a majority of neutral sentences, followed by positive sentences and the least represented sentiment is negative.\n\nmake_cm(test_df)\n\n\n\n\n\n\n\n\nAs the learning rate increases (starting at 1e-6) the validation set accuracy increases until it reaches a peak at a learning rate of 8e-5.\n\n\nShow plotting code\nfinal_epoch_metrics = metrics_df.query(\"epoch == 3\")\nplt.scatter(final_epoch_metrics['initial_learning_rate'], final_epoch_metrics['eval_accuracy']);\nplt.xscale('log')\nplt.xlabel('Learning Rate (log scale)')\nplt.ylabel('Validation Set Accuracy')\nplt.title('Learning Rate vs. Final Epoch Validation Accuracy');\n\n\n\n\n\n\n\n\n\nI’ll test the model (run a “sanity check”) on three made-up sentences. I don’t want to weigh these results too much as they are cherry-picked sentences, but this model only gets one of them right and predicts all three as negative.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(trainers[4].model, text, tokz)\n\nProbability: 0.72\nPredicted label: negative\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(trainers[4].model, text, tokz)\n\nProbability: 0.62\nPredicted label: negative\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(trainers[4].model, text, tokz)\n\nProbability: 0.68\nPredicted label: negative\n\n\n\n\nHighest Test Set Accuracy\n\n\nShow accuracy calculation loop\ntest_dfs = []\naccs = []\nfor t in trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\n\nThe learning rate with the highest test set accuracy (83%) is 5e-4. Interestingly, this was the same learning rate for the 33M model.\n\naccs\n\n[0.5733333333333334,\n 0.6844444444444444,\n 0.7333333333333333,\n 0.7822222222222223,\n 0.8133333333333334,\n 0.8177777777777778,\n 0.7333333333333333,\n 0.8266666666666667,\n 0.6755555555555556,\n 0.6888888888888889,\n 0.5555555555555556,\n 0.5555555555555556]\n\n\n\nlearning_rates[7]\n\n0.0005\n\n\nThis learning rate had a validation set accuracy of about 79%.\n\nfinal_epoch_metrics.query(\"initial_learning_rate == 0.0005\")\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      31\n      TinyStories-8M\n      0.0005\n      0.0\n      3.0\n      0.0\n      0.3891\n      264843.125\n      0.543861\n      0.789216\n      0.4398\n      927.714\n      4.548\n      12.8595\n      380.498\n      3.033\n      2.427998e+13\n      0.846817\n    \n  \n\n\n\n\nThis model gets 121/125 neutral predictions correct, followed by 40/54 positive predictions and 25/46 negative predictions.\n\nmake_cm(test_dfs[7])\n\n\n\n\n\n\n\n\nInterestingly, it also gets 1/3 of my “sanity check” predictions correct, predicting all three as positive.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(trainers[7].model, text, tokz)\n\nProbability: 0.65\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(trainers[7].model, text, tokz)\n\nProbability: 0.64\nPredicted label: positive\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(trainers[7].model, text, tokz)\n\nProbability: 0.63\nPredicted label: positive"
  },
  {
    "objectID": "posts/2024-08-19-tinystories-8m-finetune/index.html#training-with-the-best-learning-rates-10-times",
    "href": "posts/2024-08-19-tinystories-8m-finetune/index.html#training-with-the-best-learning-rates-10-times",
    "title": "Fine-tuning TinyStories-8M on the financial_phrasebank Dataset",
    "section": "Training with the Best Learning Rates 10 Times",
    "text": "Training with the Best Learning Rates 10 Times\nSince I have different models achieving the highest validation set accuracy and the highest test set accuracy, I’ll train 10 models for each learning rate to see if the results are consistent.\n\nLR = 8e-5 (Highest Validation Set Accuracy)\n\nlearning_rates[4]\n\n8e-05\n\n\n\n\nShow training loop\nbest_metrics = []\nbest_trainers = []\nlr = learning_rates[4]\n\nfor i in range(10):\n    \n    metric_callback = MetricCallback()\n    trainer, args = get_trainer(lr=lr, bs=64)\n    trainer.train()\n\n    best_metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    best_trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nbest_metrics_df = results_to_dataframe(best_metrics, model_name=\"TinyStories-8M\")\nbest_metrics_df = best_metrics_df.query('current_learning_rate.notna()')\nbest_metrics_df.head(3)\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      1\n      TinyStories-8M\n      0.00008\n      0.000068\n      1.0\n      0.000068\n      0.9671\n      313453.1875\n      0.689426\n      0.725490\n      0.4443\n      918.268\n      4.501\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      TinyStories-8M\n      0.00008\n      0.000024\n      2.0\n      0.000024\n      0.4975\n      948153.7500\n      0.490408\n      0.799020\n      0.4386\n      930.177\n      4.560\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TinyStories-8M\n      0.00008\n      0.000000\n      3.0\n      0.000000\n      0.2880\n      589967.1875\n      0.427528\n      0.848039\n      0.4433\n      920.448\n      4.512\n      12.7188\n      384.706\n      3.066\n      2.427998e+13\n      0.584194\n    \n  \n\n\n\n\nSimilar to the 33M model, 9 out of the 10 training runs resulted in the exact same final validation set accuracy. I’m not sure why this behavior persists—I’ll have to look at my Trainer setup and see if there’s something awry?\n\nfinal_accs = best_metrics_df.query(\"epoch == 3\")['eval_accuracy']\nfinal_accs.describe()\n\ncount    10.000000\nmean      0.825980\nstd       0.007751\nmin       0.823529\n25%       0.823529\n50%       0.823529\n75%       0.823529\nmax       0.848039\nName: eval_accuracy, dtype: float64\n\n\n\nfinal_accs.value_counts()\n\neval_accuracy\n0.823529    9\n0.848039    1\nName: count, dtype: int64\n\n\n\n\nShow accuracy calculation loop\ntest_dfs = []\naccs = []\nfor t in best_trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\n\nSimilarly, 9 out of the 10 training runs resulted in the same test set accuracy. One of the models resulted in an 86% test set accuracy! This is higher than the 33M model’s best validation set accuracy.\n\naccs = pd.Series(accs)\naccs.value_counts()\n\n0.813333    9\n0.862222    1\nName: count, dtype: int64\n\n\nFor what it’s worth (not much) the best model (85% validation set and 86% test set accuracy) gets 2/3 of my sanity check sentences right.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(best_trainers[0].model, text, tokz)\n\nProbability: 0.72\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(best_trainers[0].model, text, tokz)\n\nProbability: 0.53\nPredicted label: negative\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(best_trainers[0].model, text, tokz)\n\nProbability: 0.59\nPredicted label: positive\n\n\n\n\nLR = 5e-4 (Highest Test Set Accuracy)\n\nlearning_rates[7] == 5e-4\n\nTrue\n\n\n\n\nShow training loop\nbest_metrics2 = []\nbest_trainers2 = []\nlr = learning_rates[7]\n\nfor i in range(10):\n    \n    metric_callback = MetricCallback()\n    trainer, args = get_trainer(lr=lr, bs=64)\n    trainer.train()\n\n    best_metrics2.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    best_trainers2.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nbest_metrics_df2 = results_to_dataframe(best_metrics2, model_name=\"TinyStories-8M\")\nbest_metrics_df2 = best_metrics_df2.query('current_learning_rate.notna()')\nbest_metrics_df2.head(3)\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      1\n      TinyStories-8M\n      0.0005\n      0.000423\n      1.0\n      0.000423\n      1.6448\n      869443.6875\n      0.980524\n      0.502451\n      0.4321\n      944.212\n      4.628\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      TinyStories-8M\n      0.0005\n      0.000152\n      2.0\n      0.000152\n      0.7600\n      829335.1250\n      0.678928\n      0.725490\n      0.4317\n      945.012\n      4.632\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TinyStories-8M\n      0.0005\n      0.000000\n      3.0\n      0.000000\n      0.5742\n      317736.5625\n      0.598565\n      0.745098\n      0.4347\n      938.664\n      4.601\n      12.7041\n      385.151\n      3.07\n      2.427998e+13\n      0.993002\n    \n  \n\n\n\n\nI achieve the same validation set accuracy (79%) 9 out of 10 times:\n\nfinal_accs2 = best_metrics_df2.query(\"epoch == 3\")['eval_accuracy']\nfinal_accs2.describe()\n\ncount    10.000000\nmean      0.784804\nstd       0.013951\nmin       0.745098\n25%       0.789216\n50%       0.789216\n75%       0.789216\nmax       0.789216\nName: eval_accuracy, dtype: float64\n\n\n\n\nShow accuracy calculation loop\ntest_dfs2 = []\naccs2 = []\nfor t in best_trainers2:\n    test_df, acc = get_test_df(t)\n    test_dfs2.append(test_df)\n    accs2.append(acc)\n\n\nThe most common test set accuracy (81%) was less than before for this learning rate (5e-4):\n\naccs = pd.Series(accs)\naccs.value_counts()\n\n0.813333    9\n0.862222    1\nName: count, dtype: int64\n\n\nIf I use the model with the best test set accuracy (86%), the model gets all three of my sanity check sentence sentiments correct:\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(best_trainers2[0].model, text, tokz)\n\nProbability: 0.48\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(best_trainers2[0].model, text, tokz)\n\nProbability: 0.54\nPredicted label: negative\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(best_trainers2[0].model, text, tokz)\n\nProbability: 0.92\nPredicted label: neutral"
  },
  {
    "objectID": "posts/2024-08-19-tinystories-8m-finetune/index.html#final-thoughts",
    "href": "posts/2024-08-19-tinystories-8m-finetune/index.html#final-thoughts",
    "title": "Fine-tuning TinyStories-8M on the financial_phrasebank Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’ll summarize my results so far, highlighting that the 8M model achieved a 7% higher test accuracy and a validation set accuracy only 1% lower than the 33M model:\n\n\n\nArch\nFine-tuning Learning Rate\nBest Val Acc\nBest Test Acc\n\n\n\n\nTinyStories-33M\n5e-4\n86%\n79%\n\n\nTinyStories-8M\n8e-05\n85%\n86%\n\n\nTinyStories-8M\n5e-4\n79%\n86%\n\n\n\nThese experiments are quite rough, quick-and-dirty experiments to get me more practice fine-tuning language models with HuggingFace. That being said, there’s something to be said about being able to relatively easily achieve a decent validation and test set accuracy on the financial_phrasebank dataset using tiny models—something that I was not expecting!\nI’m excited to continue this fine-tuning series with the 3M and 1M TinyStories models. After I finish this first round of fine-tune, I’ll do a more thorough hyperparameter sweep (especially for number of epochs) and see if I can squeeze a few more %-ages of accuracy out of these models. Finally, I’ll experiment with creating synthetically generated low-reading-grade-level versions of the financial_phrasebank dataset and see if fine-tuning these small models on that dataset achieves better results.\nI hope you enjoyed this notebook! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "",
    "text": "In this notebook, I’ll work through the first “Further Research” exercise at the end of Chapter 4 of the Practical Deep Learning for Coders textbook:\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter.\n\nI’ve emphasized that this Learner implementation is basic, based on what we’ve learned in Chapter 4. I’ll call my implementation BasicLearner, as it corresponds to the BasicOptim optimizer created in the chapter. I’ll use my BasicLearner implementation to train a simple neural net on the MNIST_SAMPLE dataset."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#mnist_sample-training-loop",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#mnist_sample-training-loop",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "MNIST_SAMPLE Training Loop",
    "text": "MNIST_SAMPLE Training Loop\nI’ll start by recreating the training loop in Chapter 4 to train a simple neural net to classify the handwritten digits 3s and 7s.\n\nLoad and Prepare the Data\n\nfrom fastai.vision.all import *\n\nThe MNIST_SAMPLE dataset is available through fastai’s URLs which I download using untar_data.\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid'),Path('/root/.fastai/data/mnist_sample/labels.csv')]\n\n\nThen stack the list of training set and validation set tensor images into 3-dimensional tensors.\n\nstacked_threes = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'3').ls().sorted()]).float()/255\nstacked_sevens = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'7').ls().sorted()]).float()/255\nstacked_threes.shape, stacked_sevens.shape\n\n(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))\n\n\n\nshow_image(stacked_threes[0]);\n\n\n\n\n\nshow_image(stacked_sevens[0]);\n\n\n\n\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255\nvalid_3_tens.shape, valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\nshow_image(valid_3_tens[0]);\n\n\n\n\n\nshow_image(valid_7_tens[0]);\n\n\n\n\nWe then combine the training sets for 3s and 7s and “flatten” (not sure if that’s the right term) the tensors so that each image’s pixels are in a one-dimensional row.\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_y = tensor([1]*stacked_threes.shape[0] + [0]*stacked_sevens.shape[0]).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nThen do the same for the validation sets:\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_x.shape, valid_y.shape\n\n(torch.Size([2038, 784]), torch.Size([2038, 1]))\n\n\nWe create training and validation datasets with the same structure as PyTorch’s Dataset:\n\ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([1]))\n\n\n\nvalid_dset = list(zip(valid_x, valid_y))\nx,y = valid_dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([1]))\n\n\nThen feed those datasets into fastai’s DataLoaders:\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\nvalid_xb, valid_yb = first(valid_dl)\nvalid_xb.shape, valid_yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\n\nCreate Our Model\nFor this exercise they have us create a simple neural net with a ReLU sandwiched between two linear functions. I have kept the number of intermediate activations (30) the same as the text\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n\n\nCreate a Loss Function\nThe loss function we will use does the following:\n\nPass the model’s activations through a sigmoid function so that they are between 0 and 1.\nWhen the target is 1 (the digit 3), take the difference between 1 and the activation. When the target is 0 (the digit 7), take the difference between 0 and the activation.\nTake the mean of the distance between activations and targets.\n\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\nCreate a Function to Calculate Predictions, Loss and Gradients\nThe calc_grad function takes as inputs the independent and dependent data batches, passes them through the model to get the activations (predictions), calculates the batch’s loss, and calls backward on the loss to calculate the weights’ gradients:\n\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  loss = mnist_loss(preds, yb)\n  loss.backward()\n\n\n\nCreate an Optimizer\nThe optimizer handles the calculation to step the weights and reset the gradients. When stepping the weights, the .data attribute of the parameters is used since PyTorch doesn’t calculate gradients on it. The zero_grad method sets the gradients to 0 (None) so that they don’t accumulate additively when the next epoch’s gradients are calculated:\n\nclass BasicOptim:\n  def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n  def step(self, *args, **kwargs):\n    for p in self.params: p.data -= p.grad.data * self.lr\n\n  def zero_grad(self, *args, **kwargs):\n    for p in self.params: p.grad = None\n\n\nlr = 0.1\n\n\nopt = BasicOptim(simple_net.parameters(), lr)\n\n\n\nCreate a Function to Train One Epoch\nFor each training epoch:\n\nGet a batch from the training DataLoader.\nCalculate the activations, loss, and gradients.\nStep the weights in the direction opposite of the gradients.\nReset the gradients to zero.\n\n\ndef train_epoch(model):\n  for xb,yb in dl:\n    calc_grad(xb, yb, model)\n    opt.step()\n    opt.zero_grad()\n\n\n\nCreate a Function to Calculate a Metric for One Batch\nThe metric of choice in the chapter is accuracy, which is the mean of correctly predicted digits across the batch:\n\ndef batch_accuracy(xb, yb):\n  preds = xb.sigmoid()\n  correct = (preds>0.5) == yb\n  return correct.float().mean()\n\n\n\nCreate a Function to Calculate the Metric for One Epoch\nFor each batch in the validation DataLoader, calculate the accuracy. Then, take the mean of all batch accuracy values as the accuracy for the epoch:\n\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n  return round(torch.stack(accs).mean().item(), 4)\n\n\n\nCreate a Function for the Training Loop\ntrain_model takes a model, and number of epochs that you want to train the model for as inputs. For each epoch, it trains the model on the training set batches, and outputs the epoch’s metric on the validation set batches:\n\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')\n\n\n\nTrain the Model\nAs is done in the text, I’ll train the model for 40 epochs.\n\ntrain_model(simple_net, 40)\n\n0.5127 0.8013 0.9175 0.9419 0.957 0.9653 0.9672 0.9677 0.9687 0.9702 0.9726 0.9736 0.9745 0.9755 0.9755 0.9765 0.977 0.9785 0.9785 0.9785 0.9795 0.9799 0.9804 0.9809 0.9809 0.9814 0.9819 0.9819 0.9819 0.9824 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 \n\n\nI get a similar starting and final accuracy as the example from the text."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#basiclearner-class",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#basiclearner-class",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "BasicLearner Class",
    "text": "BasicLearner Class\nMy BasicLearner should recreate the training process performed in the above sections. I’ll start by defining the inputs and outputs for an instance of this class:\n\nInputs and Outputs\nThe fastai Learner requires the following inputs:\n\nDataLoaders with training and validation sets.\nThe model we want to train with.\nAn optimizer function.\nA loss function.\nAny metrics we want calculated.\n\nThe Learner outputs a table with the following information when a fit(epochs, lr) method is called. I’ve bolded the items that I’m going to show in the first iteration of my Learner:\n\nEpoch #.\nTraining Loss.\nValidation Loss.\nMetric.\nTime.\n\nWith these inputs and outputs in mind, I’ll write the BasicLearner class:\n\nclass BasicLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      self.opt.step()\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(\"Epoch\", self.metric.__name__, sep=\"\\t\")\n    for i in range(self.epochs):\n      self.train_epoch()\n      print(i, self.validate_epoch(), sep=\"\\t\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    self.train_model(self.model, self.epochs)\n\nI’ll combine my training and validation DataLoaders and confirm that they contain the correct number of tuples in their datasets:\n\ndls = DataLoaders(dl, valid_dl)\n\n\nlen(dls.train.dataset)\n\n12396\n\n\n\nlen(dls.valid.dataset)\n\n2038\n\n\nI’ll create a fresh neural net to use as a from-scratch model during training:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\nI’ll instantiate my BasicLearner class:\n\nlearn = BasicLearner(dls=dls,\n                     model=simple_net,\n                     opt_func=BasicOptim,\n                     loss_func=mnist_loss,\n                     metric=batch_accuracy)\n\nAnd train the model:\n\nlearn.fit(40, 0.1)\n\nEpoch   batch_accuracy\n0   0.5068\n1   0.814\n2   0.9184\n3   0.9419\n4   0.9575\n5   0.9648\n6   0.9663\n7   0.9677\n8   0.9692\n9   0.9707\n10  0.9736\n11  0.9736\n12  0.9741\n13  0.9755\n14  0.9765\n15  0.9775\n16  0.978\n17  0.9785\n18  0.979\n19  0.979\n20  0.979\n21  0.9795\n22  0.9795\n23  0.9804\n24  0.9804\n25  0.9809\n26  0.9814\n27  0.9819\n28  0.9819\n29  0.9819\n30  0.9814\n31  0.9819\n32  0.9819\n33  0.9824\n34  0.9824\n35  0.9829\n36  0.9829\n37  0.9829\n38  0.9829\n39  0.9829\n\n\nLooks good! I’m getting similar starting and ending accuracy values as before."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#improving-the-basiclearner-class",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#improving-the-basiclearner-class",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "Improving the BasicLearner Class",
    "text": "Improving the BasicLearner Class\nNow that I’ve confirmed that my BasicLearner is able to train a neural net to get 98% accuracy classifying 3s and 7s, I would like to add a bit more functionality to the class.\nFirst, I’d like to add a predict method to the learner which will take as input a tensor image, and then output the prediction, so that I can test if my model has truly learned how to classify 3s and 7s.\n\nclass BasicLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      self.opt.step()\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(\"Epoch\", self.metric.__name__, sep=\"\\t\")\n    for i in range(self.epochs):\n      self.train_epoch()\n      print(i, self.validate_epoch(), sep=\"\\t\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    self.train_model(self.model, self.epochs)\n\n  def predict(self, x):\n    prediction = self.model(x)\n    prediction = prediction.sigmoid()\n    label = \"3\" if prediction > 0.5 else \"7\"\n    return prediction, label\n\nI’ll instantiate a new model and BasicLearner and train it again:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n\nlearn = BasicLearner(dls=dls,\n                     model=simple_net,\n                     opt_func=BasicOptim,\n                     loss_func=mnist_loss,\n                     metric=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\nEpoch   batch_accuracy\n0   0.5073\n1   0.8184\n2   0.9194\n3   0.9419\n4   0.957\n5   0.9638\n6   0.9658\n7   0.9672\n8   0.9697\n9   0.9706\n10  0.9726\n11  0.9741\n12  0.9741\n13  0.9755\n14  0.976\n15  0.9765\n16  0.9765\n17  0.978\n18  0.978\n19  0.978\n20  0.9795\n21  0.9795\n22  0.9799\n23  0.9809\n24  0.9809\n25  0.9814\n26  0.9814\n27  0.9814\n28  0.9819\n29  0.9814\n30  0.9814\n31  0.9824\n32  0.9829\n33  0.9829\n34  0.9829\n35  0.9829\n36  0.9824\n37  0.9824\n38  0.9824\n39  0.9824\n\n\nWith the model trained, I can see if it predicts an image of a 3 correctly:\n\nshow_image(dls.valid.dataset[1][0].view(-1,28,28));\n\n\n\n\n\nlearn.predict(dls.valid.dataset[1][0])\n\n(tensor([1.0000], grad_fn=<SigmoidBackward0>), '3')\n\n\nThe final piece that I’ll add is a “training loss” column in the fit method’s output during training. The training loss of each batch will be stored in a tensor, at the end of each epoch I’ll calculate the mean loss value, print it out, and reset the loss tensor to 0.\n\nclass BasicLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    # store the loss of each batch\n    # later to be averaged across the epoch\n    self.loss = torch.cat((self.loss, tensor([loss])))\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      self.opt.step()\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(\"Epoch\", \"Train Loss\", self.metric.__name__, sep=\"\\t\")\n    for i in range(self.epochs):\n      self.loss = tensor([])\n      self.train_epoch()\n      print(i, round(self.loss.mean().item(), 4), self.validate_epoch(), sep=\"\\t\\t\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    self.train_model(self.model, self.epochs)\n\n  def predict(self, x):\n    prediction = self.model(x)\n    prediction = prediction.sigmoid()\n    label = \"3\" if prediction > 0.5 else \"7\"\n    return prediction, label\n\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n\nlearn = BasicLearner(dls=dls,\n                     model=simple_net,\n                     opt_func=BasicOptim,\n                     loss_func=mnist_loss,\n                     metric=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\nEpoch   Train Loss  batch_accuracy\n0       0.3627      0.5229\n1       0.1088      0.7715\n2       0.0593      0.9111\n3       0.0439      0.9389\n4       0.0375      0.9516\n5       0.0337      0.9629\n6       0.0311      0.9653\n7       0.0291      0.9667\n8       0.0275      0.9672\n9       0.0261      0.9687\n10      0.025       0.9721\n11      0.0241      0.9736\n12      0.0233      0.9746\n13      0.0225      0.9755\n14      0.0219      0.9755\n15      0.0213      0.976\n16      0.0208      0.9765\n17      0.0204      0.978\n18      0.02        0.9785\n19      0.0196      0.9785\n20      0.0193      0.979\n21      0.0189      0.979\n22      0.0186      0.979\n23      0.0184      0.9799\n24      0.0181      0.9804\n25      0.0178      0.9804\n26      0.0176      0.9804\n27      0.0174      0.9804\n28      0.0172      0.9804\n29      0.017       0.9814\n30      0.0168      0.9824\n31      0.0166      0.9824\n32      0.0164      0.9829\n33      0.0163      0.9829\n34      0.0161      0.9829\n35      0.016       0.9824\n36      0.0158      0.9829\n37      0.0157      0.9829\n38      0.0155      0.9829\n39      0.0154      0.9829\n\n\n\n# check prediction again\nlearn.predict(dls.valid.dataset[1][0])\n\n(tensor([1.0000], grad_fn=<SigmoidBackward0>), '3')"
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#further-improvements",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#further-improvements",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "Further Improvements",
    "text": "Further Improvements\nMy BasicLearner is able to train a neural net classifying two digits to an accuracy of 98%. During training, it prints out the epoch number, training loss and metric. It also has a predict method to test its classification on new tensor images. While I’m happy with the result of this exercise, there are certainly numerous improvements and additions that can be made to expand this learner to match the functionality of the fastai Learner class.\nI hope you enjoyed reading this blog post!"
  },
  {
    "objectID": "posts/2024-07-25-rsLoRA/index.html",
    "href": "posts/2024-07-25-rsLoRA/index.html",
    "title": "Paper Math: rsLoRA",
    "section": "",
    "text": "In this notebook, I’ll work through Definition 3.1 and the Theorem 3.2 proof provided in Appendix A of the rsLoRA paper. Note that the purpose of this blog post is to help me think out loud—I have a lot of gaps in my understanding of matrix calculus that I need to remediate before I can derive some of the core equations in the paper. This post doesn’t provide those derivations."
  },
  {
    "objectID": "posts/2024-07-25-rsLoRA/index.html#definition-3.1",
    "href": "posts/2024-07-25-rsLoRA/index.html#definition-3.1",
    "title": "Paper Math: rsLoRA",
    "section": "Definition 3.1",
    "text": "Definition 3.1\nAn adapter \\(\\gamma_rBA\\) is rank-stabilized if the following two conditions hold:\n\nIf the inputs to the adapter are iid such that the \\(m\\)’th moment is \\(\\Theta_r(1)\\) in each entry, then the \\(m\\)’th moment of the outputs of the adapter is also \\(\\Theta_r(1)\\) in each entry.\nIf the gradient of the loss with respect to the adapter outputs are \\(\\Theta_r(1)\\) in each entry, then the loss gradients into the input of the adapter are also \\(\\Theta_r(1)\\) in each entry.\n\nI’ll define the following keywords from those two conditions:\n\niid: independently and identically distributed\n\\(\\Theta_r(1)\\): Big Theta notation, which specifies the upper and lower bounds of complexity of an algorithm. A notation of \\(\\Theta(1)\\) means the function or algorithm is upper bound and lower bound by a constant, meaning that as the number of inputs increases, the function stays constant (represented by the \\(1\\)).\nmoments: quantitative measures related to the shape of the function’s graph (1st moment of a function is its mean, 2nd moment is variance, 3rd moment is skewness, 4th moment is Kurtosis).\n\nCondition 1 says that rank-stable adapters are those where IF the inputs to it are iid and have, on average, constant moments (like mean and variance) as the number of inputs increase, then the outputs of the adapter have constant moments on average as well.\nCondition 2 says that the gradients of the inputs and outputs of rank-stable adapters, with respect to the loss, are of constant size as the number of inputs and outputs increase.\nThis HuggingFace community article puts it nicely and succinctly (emphasis mine):\n\nIn the work Rank-Stabilized LoRA (rsLoRA), it is proven theoretically, by examining the learning trajectory of the adapters in the limit of large rank \\(r\\), that that to not explode or diminish the magnitude of the activations and gradients through each adapter one must set \\(\\gamma_r \\in \\Theta(\\frac{1}{\\sqrt{r}})\\)\n\nThis Khan Academy gives a nice example of \\(\\Theta(n)\\) from which you can imagine what \\(\\Theta(1)\\) would look like (instead of upper bound being \\(k_2 \\cdot n\\) it would be \\(k_2 \\cdot 1\\); instead of a lower bound of \\(k_1 \\cdot n\\) it would be \\(k_1 \\cdot 1\\). In other words, two horizontal lines of constant value as \\(n\\) increases)."
  },
  {
    "objectID": "posts/2024-07-25-rsLoRA/index.html#theorem-3.2",
    "href": "posts/2024-07-25-rsLoRA/index.html#theorem-3.2",
    "title": "Paper Math: rsLoRA",
    "section": "Theorem 3.2",
    "text": "Theorem 3.2\nI’ll restate Theorem 3.2 here for reference:\nLet the LoRA adapters be of the form \\(\\gamma_rBA\\), where \\(B \\in \\mathbb{R}^{d_1 \\times r}\\), \\(A \\in \\mathbb{R}^{r \\times d_2}\\) are initialised such that \\(B\\) is initially \\(0_{d_1 \\times r}\\), entries of \\(A\\) are iid with mean \\(0\\) and variance \\(\\sigma_A\\) not depending on \\(r\\), and \\(\\gamma_r \\in \\mathbb{R}\\) such that \\(\\gamma_r \\rightarrow 0\\) as \\(r \\rightarrow \\infty\\).\nIn expectation over initialization, assuming the inputs to the adapter are iid distributed such that the \\(m\\)’th moment is \\(\\Theta_r(1)\\) in each entry, we have that the \\(m\\)’th moment of the outputs of the adapter is \\(\\Theta_r(1)\\) in each entry if and only if:\n\\[\\gamma_r \\in \\Theta_r(\\frac{1}{\\sqrt{r}})\\]\nIn expectation over initialization, assuming the loss gradient to the adapter outputs are \\(\\Theta_r(1)\\) in each entry, we have that the loss gradients into the input of the adapter are \\(\\Theta_r(1)\\) in each entry if and only if:\n\\[\\gamma_r \\in \\Theta_r(\\frac{1}{\\sqrt{r}})\\]\nIn particular, the above holds at any point in the learning trajectory if the assumptions do, and unless \\(\\gamma_r \\in \\Theta_r(\\frac{1}{\\sqrt{r}})\\), there is unstable or collapsing learning for \\(r\\) large enough."
  },
  {
    "objectID": "posts/2024-07-25-rsLoRA/index.html#gradient-of-loss-mathcall-with-respect-to-adapters-a-and-b",
    "href": "posts/2024-07-25-rsLoRA/index.html#gradient-of-loss-mathcall-with-respect-to-adapters-a-and-b",
    "title": "Paper Math: rsLoRA",
    "section": "Gradient of Loss \\(\\mathcal{L}\\) with Respect to Adapters \\(A\\) and \\(B\\)",
    "text": "Gradient of Loss \\(\\mathcal{L}\\) with Respect to Adapters \\(A\\) and \\(B\\)\nLet \\(f(x) = \\gamma_rBAx\\), and \\(\\mathcal{L}(f(x))\\) denote the loss.\nLet \\(B_n\\),\\(A_n\\), denote \\(B\\), \\(A\\) after the \\(n\\)’th SGD update on input \\(x_n\\) with learning rate \\(\\eta\\).\nRecall that \\(B_0=0_{d \\times r}\\), and see that for \\(v_n = \\nabla_{f(x_n)}\\mathcal{L}(f(x_n))\\):\n\\[\\nabla_{B_n}\\mathcal{L} = \\gamma_r v_n x_n^T A_n^T\\]\n\\[\\nabla_{A_n}\\mathcal{L} = \\gamma_r B_n^T v_n x_n^T\\]\nThis comes from the chain rule. Since \\(\\mathcal{L}\\) is a function of \\(f(x)\\), and since \\(f(x)\\) involves \\(B\\) and \\(A\\), the gradient of \\(\\mathcal{L}\\) with respect to \\(B\\) or \\(A\\) is written as:\n\\[\\nabla_{B_n}\\mathcal{L} = \\nabla_{f(x_n)}\\mathcal{L} \\cdot \\nabla_{B_n}\\mathcal{f(x_n)}\\]\n\n\\[\\nabla_{A_n}\\mathcal{L} = \\nabla_{f(x_n)}\\mathcal{L} \\cdot \\nabla_{A_n}\\mathcal{f(x_n)}\\]\nIn each case, \\(\\nabla_{f(x_n)}\\mathcal{L}\\) is given to us as \\(v_n\\).\nWhen plugging in the matrix-vector product \\(BAx\\) into matrixcalculus.org I get the following result for the partial derivatives of \\(BAx\\) with respect to \\(B\\) or \\(A\\):\n\\[\\frac{\\partial}{\\partial B}(B \\cdot A \\cdot x) = (A \\cdot x)^T \\otimes \\mathbb{I}\\]\n\n\\[\\frac{\\partial}{\\partial A}(B \\cdot A \\cdot x) = x^T \\otimes B\\]\nWhere the \\(\\otimes\\) symbol is the tensor product or Kronecker Product where you multiple each element of the first matrix by the second matrix—a very differently shaped result than anything in the rsLoRA proof\n\nIf \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is a \\(p \\times q\\) matrix, then the Kronecker product \\(A \\otimes B\\) is the \\(pm \\times qn\\) block matrix.\n\nThis StackExchange solution’s first half also shows something similar but without enough explanation for me to understand it.\nBased on this it’s unclear to me how \\(\\nabla_{B_n}\\mathcal{f(x_n)}\\) results in \\(x_n^TA_n^T\\) and \\(\\nabla_{A_n}\\mathcal{f(x_n)}\\) results in \\(B_n^Tx_n^T\\). I spent 5-6 hours googling, looking on YouTube and prompting Claude/ChatGPT and didn’t come away with much (other than confirming that I don’t know matrix calculus).\nThat being said, we can still look at the shapes of each matrix or vector and see how the different tranposing and placement of variables makes it all work.\nIn the rsLoRA paper:\n\n\\(B\\) has the dimensions \\(d_1 \\times r\\)\n\\(A\\) has the dimensions \\(r \\times d_2\\)\n\nTherefore, the matrix product \\(BA\\) which is (\\(d_1 \\times r\\)) \\(\\times\\) (\\(r \\times d_2\\)), has the dimension \\(d_1 \\times d_2\\) (the \\(r\\)’s cancel out due to matrix multiplication).\n\nSince \\(x\\) is multiplied by \\(BA\\) to get \\(BAx\\), and assuming \\(x\\) is a vector, it has the dimensions \\(d_2 \\times 1\\) (since the first dimension of \\(x\\) has to be equal to the last dimension of \\(BA\\)).\nPutting it all together, \\(f(x) = BAx\\) has the dimensions:\n\n\\((d_1 \\times r) \\times (r \\times d_2) \\times (d_2 \\times 1) = d_1 \\times 1\\)\n\nNote that the \\(r\\)’s cancel out as do the \\(d_2\\)’s.\nI’ll now a similar dimensional analysis of the gradients of \\(\\mathcal{L}\\) with respect to \\(A\\) and \\(B\\):\n\n\\[\\nabla_{B_n}\\mathcal{L} = \\gamma_r v_n x_n^T A_n^T\\]\nThe gradient \\(\\nabla_{B_n}\\mathcal{L}\\) must have the same dimensions as \\(B\\), (\\(d_1 \\times r\\)), so that we can make the gradient update to each element of \\(B\\). Similarly, \\(v_n\\) has to have the same dimensions as \\(f(x)\\), (\\(d_1 \\times 1\\)).\n\n\\[(d_1 \\times r) = (d_1 \\times 1) \\times (1 \\times d_2) \\times (d_2 \\times r)\\]\n\nThe \\(1\\)’sand the \\(d_2\\)’s cancel out in the matrix multiplication, so we get:\n\n\\[(d_1 \\times r) = (d_1 \\times r)\\]\nThe dimensions match.\nSimilarly for the gradient of \\(\\mathcal{L}\\) with respect to \\(A\\), the dimensions of the gradient must equal the dimensions of \\(A\\) (in order to do the gradient update):\n\n\\[\\nabla_{A_n}\\mathcal{L} = \\gamma_r B_n^T v_n x_n^T\\]\n\n\\[(r \\times d_2) = (r \\times d_1) \\times (d_1 \\times 1) \\times(1 \\times d_2)\\]\n\n\\[(r \\times d_2) = (r \\times d_2)\\]\n\nThe dimensions match.\nThe \\(d_1\\)’s and the \\(1\\)’s cancel out due to matrix multiplication."
  },
  {
    "objectID": "posts/2024-07-25-rsLoRA/index.html#adapter-value-after-n-updates",
    "href": "posts/2024-07-25-rsLoRA/index.html#adapter-value-after-n-updates",
    "title": "Paper Math: rsLoRA",
    "section": "Adapter Value after \\(n\\) Updates",
    "text": "Adapter Value after \\(n\\) Updates\nI struggled with this section so the following is just me thinking out loud and may not help clarify your understanding.\nAs a reminder, here are the expressions for the gradient of Loss with respect to the adapters \\(A_n\\) and \\(B_n\\) (where \\(n\\) is the number of gradient updates during training):\n\n\\[\\nabla_{B_n}\\mathcal{L} = \\gamma_r v_n x_n^T A_n^T\\]\n\n\\[\\nabla_{A_n}\\mathcal{L} = \\gamma_r B_n^T v_n x_n^T\\]\n\nAfter \\(n \\ge 1\\) SGD updates (in each update, we are subtracting from the adapter the learning rate times the gradient of the Loss with respect to the adapter) the two adapters \\(B_n\\) and \\(A_n\\) look like:\n\\[B_n = (-\\eta \\gamma_r \\sum_{k=0}^{n-1}v_kx_k^T + \\mathcal{O}_r(\\gamma_r^2))A_0^T\\]\n\\[A_n =A_0(1 + \\mathcal{O}_r(\\gamma_r^2))\\]\nNote that \\(B\\) is initialized as a 0-matrix so there’s no \\(B_0\\) in their expression.\nThree observations:\n\nThough the gradient of Loss wrt \\(B_n\\) contains an \\(A_n\\) term (\\(\\gamma_rv_nx_n^TA_n^T\\)), the expression here for \\(B_n\\) contains an \\(A_0\\) term.\nThe expression here for \\(A_n\\) does not include the gradient terms (\\(\\gamma_r v_nx_n^TA_n^T\\)).\nThere is an \\(\\mathcal{O}_r(\\gamma_r^2)\\) term in both the \\(B_n\\) and \\(A_n\\) expressions.\n\nFrom those observations I’m coming to the following three conclusions (with the help of Claude):\n\nThe term \\(\\mathcal{O}_r(\\gamma_r^2)\\), which is in Big-O notation, represents some term(s) that has an upper bound of \\(\\gamma_r^2\\) (in other words, some constant term). I’m not sure what term they actually represent—maybe some error term? I don’t know.\n\\(A_n\\) is a function of \\(A_0\\) and the constant term \\(\\mathcal{O}_r(\\gamma_r^2)\\). I wonder if that’s because \\(A_0\\) is initialized as a normal (Gaussian) matrix and since it’s a rank-stable adapter, it doesn’t deviate that much from that normal distribution? Again, not sure. Additionally, in their \\(B_n\\) expression they multiply by \\(A_0^T\\) instead of the \\(A_n^T\\) term in the gradient—maybe an indication that \\(A_n\\) doesn’t deviate much from \\(A_0\\)? Not sure.\n\nA supplementary graphic I created to try to illustrate my thinking + confusion:\n\n\n\nGraphic showing the relationship between the gradient and the adapter value for B_n\n\n\nUPDATE: A fastai study group member provided the following insight which now clearly explains why \\(B_n\\) is written in terms of \\(A_0\\). It’s because the derative of \\(A_n\\) has a \\(B_n\\) term in it (\\(\\nabla_{A_n} = \\gamma_rB_n^Tv_nX_n^T\\)) and one step after initialization (\\(n=1\\)), \\(B_1\\) is written in terms of \\(A_0\\):\n\\(n=0\\):\n\\[B_n = 0\\] \\[A_n = A_0\\]\n\n\\[\\nabla_{B_0}\\mathcal{L}=\\gamma_rv_0x_0^TA_0^T\\]\n\\[\\nabla_{A_0}\\mathcal{L}=\\gamma_rB_0^Tv_0x_0^T = 0\\]\n\\(n=1\\):\n\\[B_1 = 0 - \\gamma_rv_0x_0^TA_0^T\\]\n\\[A_1 = A_0 - 0 = A_0\\]\n\n\\[\\nabla_{B_1}\\mathcal{L}=\\gamma_rv_1x_1^TA_1^T = \\gamma_rv_1x_1^TA_0^T\\]\n\\[\\nabla_{A_1}\\mathcal{L}=\\gamma_rB_1^Tv_1x_1^T = \\gamma_r(\\gamma_rv_0x_0^TA_0^T)^Tv_1x_1^T\\]\n\nNotice how \\(B_1\\) has an \\(A_0\\) term in it. Similarly \\(\\nabla_{A_1}\\mathcal{L}\\) is in terms of \\(A_0\\) as well since \\(B_1\\) is in terms of \\(A_0\\)."
  },
  {
    "objectID": "posts/2024-07-25-rsLoRA/index.html#deriving-stable-rank",
    "href": "posts/2024-07-25-rsLoRA/index.html#deriving-stable-rank",
    "title": "Paper Math: rsLoRA",
    "section": "Deriving Stable Rank",
    "text": "Deriving Stable Rank\nLet’s just take their expressions of \\(B_n\\) and \\(A_n\\) for granted and continue with the derivation of the stable rank condition:\n\\[B_n = (-\\eta \\gamma_r \\sum_{k=0}^{n-1}v_kx_k^T + \\mathcal{O}_r(\\gamma_r^2))A_0^T\\]\n\\[A_n =A_0(1 + \\mathcal{O}_r(\\gamma_r^2))\\]\nThen \\(\\gamma_rB_nA_n\\) is:\n\\[\\gamma_rB_nA_n = -\\gamma_r^2\\eta\\sum_{k=0}^{n-1}v_kx^T_kA^T_0A_0 + \\mathcal{O}_r(\\gamma_r^3)A_0^TA_0\\]\nTo try and derive this result, I’ll start with \\(B_n\\) and expand \\(B_n\\) by multiplying the terms inside the parentheses by \\(A_0^T\\):\n\n\\[B_n = \\big(-\\eta \\gamma_r \\sum_{k=0}^{n-1}v_kx_k^T + \\mathcal{O}_r(\\gamma_r^2)\\big)A_0^T = -\\eta \\gamma_r \\sum_{k=0}^{n-1}v_kx_k^TA_0^T + \\mathcal{O}_r(\\gamma_r^2)A_0^T\\]\nI’ll then expand \\(A_n\\) by multiplying the terms inside the parentheses by \\(A_0\\):\n\n\\[A_n =A_0(1 + \\mathcal{O}_r(\\gamma_r^2)) = A_0 + A_0\\mathcal{O}_r(\\gamma_r^2)\\]\nThen, I’ll write out the full multiplication of \\(\\gamma_rB_nA_n\\):\n\\[\\gamma_rB_nA_n = \\big[ \\gamma_r \\big] \\times \\big[-\\eta \\gamma_r \\sum_{k=0}^{n-1}v_kx_k^TA_0^T + \\mathcal{O}_r(\\gamma_r^2)A_0^T\\big] \\times \\big[ A_0 + A_0\\mathcal{O}_r(\\gamma_r^2) \\big]\\]\n\\(B_n\\) is getting multiplied by two terms, \\(A_0\\) and \\(A_0\\mathcal{O}_r(\\gamma_r^2)\\). Doing that multiplication and expanding it out:\n\\[\\gamma_rB_nA_n = \\big[ \\gamma_r \\big] \\times \\big[ -\\eta \\gamma_r \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \\mathcal{O}_r(\\gamma_r^2)A_0^TA_0 + -\\eta \\gamma_r \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\\mathcal{O}_r(\\gamma_r^2) + \\mathcal{O}_r(\\gamma_r^2)A_0^TA_0\\mathcal{O}_r(\\gamma_r^2) \\big]\\]\nNow I’ll multiple the \\(\\gamma_r\\) term at the start into the giant second term:\n\\[\\gamma_rB_nA_n = -\\eta \\gamma_r^2 \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \\gamma_r\\mathcal{O}_r(\\gamma_r^2)A_0^TA_0 + -\\eta \\gamma_r^2 \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\\mathcal{O}_r(\\gamma_r^2) + \\gamma_r\\mathcal{O}_r(\\gamma_r^2)A_0^TA_0\\mathcal{O}_r(\\gamma_r^2)\\]\nNext, I’ll highlight terms where \\(\\gamma_r\\) is multiplied by \\(\\mathcal{O}_r(\\gamma_r^2)\\):\n\n\n\nGraphic showing highlighted gamma_r terms\n\n\nThe first highlighted term, \\(\\gamma_r\\mathcal{O}_r(\\gamma_r^2)\\) becomes \\(\\mathcal{O}_r(\\gamma_r^3)\\).\n\nThe second (\\(\\gamma_r^2\\)) and third (\\(\\mathcal{O}_r(\\gamma_r^2)\\)) highlighted terms multiply to become \\(\\mathcal{O}_r(\\gamma_r^4)\\).\n\nThe fourth (\\(\\gamma_r\\mathcal{O}_r(\\gamma_r^2)\\)) and fifth (\\(\\mathcal{O}_r(\\gamma_r^2)\\)) highlighted terms multiply to become \\(\\mathcal{O}_r(\\gamma_r^5)\\).\nRewriting the expression with those simplifications:\n\\[\\gamma_rB_nA_n = -\\eta \\gamma_r^2 \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \\mathcal{O}_r(\\gamma_r^3)A_0^TA_0 + -\\eta \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\\mathcal{O}_r(\\gamma_r^4) + A_0^TA_0\\mathcal{O}_r(\\gamma_r^5)\\]\nAccording to what I understood from prompting Claude, the last two terms, \\(-\\eta \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0\\mathcal{O}_r(\\gamma_r^4) + A_0^TA_0\\mathcal{O}_r(\\gamma_r^5)\\) are encompassed by the earlier \\(\\mathcal{O}_r(\\gamma_r^3)\\) term. The reason being that since \\(\\gamma_r\\) goes to \\(0\\) (as \\(r\\) goes to \\(\\infty\\)) as stated at the beginning of Theorem 3.2, the term \\(\\mathcal{O}_r(\\gamma_r^3)\\), where \\(\\gamma_r^3\\) the upper bound, will always be larger than \\(\\mathcal{O}_r(\\gamma_r^4)\\) or \\(\\mathcal{O}_r(\\gamma_r^5)\\).\nAs \\(\\gamma_r\\) goes to \\(0\\), \\(\\gamma_r^3 \\gt \\gamma_r^4 \\gt \\gamma_r^5\\).\n\nSo with the \\(\\mathcal{O}_r(\\gamma_r^4)\\) and \\(\\mathcal{O}_r(\\gamma_r^5)\\) getting swallowed by the \\(\\mathcal{O}_r(\\gamma_r^3)\\) term, rewriting the expression gives us:\n\\[\\gamma_rB_nA_n = -\\eta \\gamma_r^2 \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \\mathcal{O}_r(\\gamma_r^3)A_0^TA_0\\]\nWhich is the expression in Equation (8) of the rsLoRA paper.\nNext they define the expectation of the initialiation \\(A_0\\) as:\n\\[E_{A_0}(A_0^TA_0) = r\\sigma_AI_{d \\times d}\\]\nand replace \\(A_0^TA_0\\) with \\(r\\sigma_AI_{d \\times d}\\) in the expression of \\(\\gamma_rB_nA_n\\):\n\\[\\gamma_rB_nA_n = -\\eta \\gamma_r^2 \\sum_{k=0}^{n-1}v_kx_k^TA_0^TA_0 + \\mathcal{O}_r(\\gamma_r^3)A_0^TA_0 = -\\eta \\gamma_r^2 \\sum_{k=0}^{n-1}v_kx_k^Tr\\sigma_AI_{d \\times d} + \\mathcal{O}_r(\\gamma_r^3)r\\sigma_AI_{d \\times d}\\]\nI think the last term, \\(\\mathcal{O}_r(\\gamma_r^3)r\\sigma_AI_{d \\times d}\\) gets simplified to \\(\\mathcal{O}_r(\\gamma_r^3)\\), and multipling by the identity matrix \\(I_{d \\times d}\\) is like multiplying by \\(1\\), so we end up with Equation (9) in the rsLoRA paper:\n\n\\[\\gamma_rB_nA_n = -\\gamma_r^2 r\\sigma_A \\eta\\sum_{k=0}^{n-1}v_kx_k^T + \\mathcal{O}_r(\\gamma_r^3)\\]\nI’m very fuzzy on the final steps, but taking a shot at explaining how I understand it:\nCondition 1 of Definition 3.1 states:\n\nIf the inputs to the adapter are iid such that the \\(m\\)’th moment is \\(\\Theta_r(1)\\) in each entry, then the \\(m\\)’th moment of the outputs of the adapter is also \\(\\Theta_r(1)\\) in each entry.\n\nThe forward pass through the adapters is \\(\\gamma_rB_nA_nx_n\\).\nThe \\(m\\)’th moment of the iid inputs is represented by the expression:\n\\[E_x((x_k^Tx_n)^m) \\in \\Theta_r(1)\\]\nWhere does the term \\(E_x((x_k^Tx_n)^m)\\) comes from? I think it comes from Equation (11). First I’ll write Equation (9) again for reference:\n\\[\\gamma_rB_nA_n = -\\gamma_r^2 r\\sigma_A \\eta\\sum_{k=0}^{n-1}v_kx_k^T + \\mathcal{O}_r(\\gamma_r^3)\\]\nThe forward pass multiplies Equation (9) by the new input \\(x_n\\) to get something like this (not shown in the paper, my assumption):\n\\[\\gamma_rB_nA_nx_n = -\\gamma_r^2 r\\sigma_A \\eta\\sum_{k=0}^{n-1}v_kx_k^Tx_n + \\mathcal{O}_r(\\gamma_r^3)\\]\nNote that we now have a \\(x_k^Tx_n\\) term inside the summation.\nLet’s look at just the left-hand side of Equation (11) now:\n\\[E_{x,A_0}((\\gamma_rB_nA_nx_n)^m)\\]\nThis is the expression for \\(m\\)’th moment of the forward pass (if I understand correctly).\nLooking at the whole Equation (11):\n\\[E_{x,A_0}((\\gamma_r B_n A_n x_n)^m) = (-\\gamma_r^2r\\sigma_A\\eta)^m\\sum_{k=0}^{n-1}v_k^mE_x((x_k^Tx_n)^m) + \\Theta_r((\\gamma_r^3r)^m) \\in \\Theta_r((\\gamma_r^2r)^m)\\]\nEverything on the right-hand side of the equation is raised to the power of \\(m\\):\n\n\\((-\\gamma_r^2r\\sigma_A\\eta)^m\\)\n\\(v_k^m\\)\n\\((x_k^Tx_n)^m\\)\n\\(\\Theta_r((\\gamma_r^3r)^m)\\)\n\nThe expected value is taken of \\(x\\) and \\(A_0\\). We already took care of the expectation of \\(A_0\\) earlier with the term \\(r\\sigma_A\\). Equation (11) takes care of the expectation of \\(x\\) with the term \\(E_x((x_k^Tx_n)^m)\\). At least that’s my understanding.\nFinally the stuff at the end:\n\\[\\in \\Theta_r((\\gamma_r^2r)^m)\\]\nIs saying that this expected value \\(E_{x,A_0}\\) is in the set of values bound above and below by \\((\\gamma_r^2r)^m\\). Why? Well there are two \\(\\gamma_r\\) terms in \\(E_{x,A_0}\\):\n\n\\((\\gamma_r^2r\\sigma_A\\eta)^m\\)\nand\n\\(\\Theta_r((\\gamma_r^3r)^m)\\)\n\nI think it’s correct to say that the term \\((\\gamma_r^2r\\sigma_A\\eta)^m\\) is in the set \\(\\Theta_r((\\gamma_r^2r)^m)\\) (in other words it’s bound above and below by a constant times \\((\\gamma_r^2r)^m\\)).\n\\(\\Theta_r((\\gamma_r^2r)^m)\\) encompasses \\(\\Theta_r((\\gamma_r^3r)^m)\\) since as \\(\\gamma_r\\) goes to \\(0\\) (an initial assumption of Theorem 3.2), \\(\\gamma_r^2 \\gt \\gamma_r^3\\).\nDefinition 3.1 stated that the \\(m\\)’th moments of the adapter output have to be in the set \\(\\Theta_r(1)\\) for the adapters to be considered rank-stable.\nIf the \\(m\\)’th moment, \\(E_{x,A_0}((\\gamma_r B_n A_n x_n)^m)\\), is in the set \\(\\Theta_r((\\gamma_r^2r)^m)\\) (as is defined in Equation (11)) and if Definition 3.1 condition is to be satisfied, the set \\(\\Theta_r((\\gamma_r^2r)^m)\\) must be equal to \\(\\Theta_r(1)\\):\n\n\\(\\Theta_r((\\gamma_r^2r)^m)\\) = \\(\\Theta_r(1)\\)\n\nEquating the terms inside the \\(\\Theta_r\\) on each side:\n\\((\\gamma_r^2r)^m = 1\\)\n\nRaising each side to \\(\\frac{1}{m}\\) (to get rid of the \\(m\\) exponent on the left) gives us:\n\\(\\gamma_r^2r = 1\\)\n\nDividing both sides by \\(r\\):\n\\(\\gamma_r^2 = \\frac{1}{r}\\)\n\nTaking the square root of both sides:\n\\(\\gamma_r = \\frac{1}{\\sqrt{r}}\\)\n\nWhich is the proof that in order for the adapters to have stable outputs the value of \\(\\gamma_r\\) must be a constant of \\(\\frac{1}{\\sqrt{r}}\\) or in other words:\n\\[\\gamma_r \\in \\Theta_r(\\frac{1}{\\sqrt{r}})\\]\nI don’t understand how they derived Equation (10) so I’m not going to write about it here."
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html",
    "href": "posts/2024-11-17-webgpupuzzles/index.html",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "",
    "text": "This file contains my walkthrough of the official WebGPU Puzzle solutions that I found challenging to understand and/or critical in helping me understand core concepts of GPU programming. You can find the Excel spreadsheet with my solution visualizations here. The WebGPU puzzles are published by Answer.AI at https://gpupuzzles.answer.ai/puzzles."
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-7",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-7",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 7",
    "text": "Puzzle 7\n@group(0) @binding(0) var<storage, read_write> a : array<f32>;\n@group(0) @binding(1) var<storage, read_write> out : array<f32>;\n\nconst wgs = vec3({{workgroupSize}}); // workgroup sizes\nconst twg = vec3({{totalWorkgroups}}); // total workgroups\n\n@compute @workgroup_size({{workgroupSize}})\nfn main(@builtin(local_invocation_id) lid: vec3<u32>,\n        @builtin(global_invocation_id) gid: vec3<u32>,\n        @builtin(workgroup_id) wid: vec3<u32>\n        ) {\n\n  let wgSize: u32 = wgs.x * wgs.y * wgs.z;\n  let wg = wid.x + wid.y * twg.x;\n  let i = lid.x + lid.y * wgs.x + wg * wgSize;\n  out[i] = a[i] + 10;\n}\n___________________________________\nTest case 1 \n\nWorkgroup Size       ( 2, 2, 1 )\nTotal Workgroups     ( 2, 2, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7  8 ]\nExpected [ 10 11 12 13 14 15 16 17 18 ]\n\n___________________________________\nTest case 2 \n\nWorkgroup Size       ( 2, 2, 1 )\nTotal Workgroups     ( 3, 3, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ]\nExpected [ 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ]\nI actually didn’t quite understand this solution until I revisited it while I was working on Puzzle #14 after I recalled that this solution dealt with situations where the number of threads in a workgroup was less than the number of positions in the input array.\n\n\n\nVisualizing the official solution to puzzle #7 in Excel\n\n\nI interpret wg as being the global “workgroup indexer” and i as the global “thread indexer.” In wg, the value of wid.x (0, 1, 2) is incremented by 1 as you go down wid.y by the term wid.y * twg.x. Similarly for i, lid.y * wgs.x increments the index by 1 as you go down lid.y while wg * wgSize increments the i by 4 as you traverse over the wg index of the workgroup. In this way, while no single workgroup can handle all 25 elements of the input array, spreading them out across 9 workgroups make this light work."
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-8",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-8",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 8",
    "text": "Puzzle 8\nThis puzzle also had fewer threads per block than number of elements in the input array.\n@group(0) @binding(0) var<storage, read_write> a : array<f32>;\n@group(0) @binding(1) var<storage, read_write> out : array<f32>;\n\n// workgroup sizes x, y, z\nconst wgs = vec3({{workgroupSize}});\n// total workgroups x, y, z\nconst twg = vec3({{totalWorkgroups}}); \n// flat shared memory array\nvar<workgroup> smem: array<f32, {{smemSize}}>; \n\n@compute @workgroup_size({{workgroupSize}})\nfn main(@builtin(local_invocation_id) lid: vec3<u32>,\n        @builtin(global_invocation_id) gid: vec3<u32>,\n        @builtin(workgroup_id) wid: vec3<u32>) {\n  if (gid.x < arrayLength(&a)) {\n    smem[lid.x] = a[gid.x];\n  }\n  workgroupBarrier(); \n  out[gid.x] = smem[lid.x] + 10;\n}\n___________________________________\nTest case 1\n\nWorkgroup Size       ( 4, 1, 1 )\nTotal Workgroups     ( 2, 1, 1 )\nShared Memory  Size  ( 8, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7 ]\n\nExpected [ 10 11 12 13 14 15 16 17 ]\n\n___________________________________\nTest case 2\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 2, 1, 1 )\nShared Memory  Size  ( 8, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 ]\n\nExpected [ 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ]\nThe following lines load the input array into shared memory:\nif (gid.x < arrayLength(&a)) {\n    smem[lid.x] = a[gid.x];\n}\nVisualizing Test Case 1: in each workgroup, since lid.x is (0, 1, 2, 3), only the first four elements of shared memory are filled with data. In the first workgroup, gid.x is (0, 1, 2, 3) and in the second workgroup, it’s (4, 5, 6, 7) so the corresponding elements of input array a are loaded into shared memory.\n\n\n\nVisualizing test case 1 for puzzle 8 in Excel\n\n\nThe following line assigns to out in each workgroup the first four elements of smem:\nout[gid.x] = smem[lid.x];\nVisualizing that in Excel for Test Case 1:\n\n\n\nVisualizing test case 1 for puzzle 8 in Excel\n\n\nlid.x is always (0, 1, 2, 3) so the first four elements of smem are always indexed. gid.x is (0, 1, 2, 3) for wid.x = 0 and (4, 5, 6, 7) for wid.x = 1 so the first four elements of out are loaded with the first four elements of smem for the first workgroup and the second four elements of out are loaded with the first four elements of smem for the second workgroup. Adding 10 to smem values gives the expected output:\nout[gid.x] = smem[lid.x];\n\n\n\nVisualizing test case 1 for puzzle 8 in Excel\n\n\nHere are is the solution visualized for Test Case 2:\nThe full 8-element shared memory array is filled with values from the 16-element input array a:\n\n\n\nVisualizing test case 2 for puzzle 8 in Excel\n\n\nAgain lid.x is equal in both workgroups, so the first 8 elements of smem are loaded into the corresponding sequence of 8 elements in out using gid.x:\n\n\n\nVisualizing test case 2 for puzzle 8 in Excel\n\n\nAdding 10 to the smem values yields the expected result:\n\n\n\nVisualizing test case 2 for puzzle 8 in Excel"
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-9",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-9",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 9",
    "text": "Puzzle 9\n@group(0) @binding(0) var<storage, read_write> a : array<f32>;\n@group(0) @binding(1) var<storage, read_write> out : array<f32>;\n\nconst wgs = vec3({{workgroupSize}}); // workgroup sizes\nconst twg = vec3({{totalWorkgroups}}); // total workgroups\nvar<workgroup> smem: array<f32, {{smemSize}}>;\n\n@compute @workgroup_size({{workgroupSize}})\nfn main(@builtin(local_invocation_id) lid: vec3<u32>,\n        @builtin(global_invocation_id) gid: vec3<u32>,\n        @builtin(workgroup_id) wid: vec3<u32>\n        ) {\n    let i = lid.x + lid.y * wgs.x;\n    smem[lid.x] = a[i];\n    workgroupBarrier();\n    out[lid.x] = smem[lid.x];\n    if (lid.x > 0) {\n        out[lid.x] += smem[lid.x - 1];\n        if (lid.x > 1) {\n            out[lid.x] += smem[lid.x - 2];\n        }\n    }\n}\n___________________________________\nTest case 1\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 1, 1, 1 )\nShared Memory  Size  ( 8, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7 ]\nExpected [  0  1  3  6  9 12 15 18 ]\n\n___________________________________\nTest case 2\n\nWorkgroup Size       ( 10, 1, 1 )\nTotal Workgroups     ( 1, 1, 1 )\nShared Memory  Size  ( 10, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7  8  9 ]\nExpected [  0  1  3  6  9 12 15 18 21 24 ]\nSince we have only one workgroup with size (8, 1, 1) in Test Case 1, wgs.x is 0 and lid.y is 0, so i ends up being equal to lid.x. The shared memory array smem has the same size as the input array a (and the workgroup) so smem[lid.x] = a[i] loads in the entire array a into shared memory.\n\n\n\nVisualizing test case 1 for puzzle 9 in Excel\n\n\nNext, we load into out the entire array smem with the following line:\nout[lid.x] = smem[lid.x];\n\n\n\nVisualizing test case 1 for puzzle 9 in Excel\n\n\nOur goal is to “sum together the last 3 positions of a and store it in out.” To do this, we first “slide” or “shift” smem one element to the right with the code smem[lid.x - 1], and add it to out. We only do this for i values above 0 since we don’t want to index into smem with -1:\n\n\n\nVisualizing test case 1 for puzzle 9 in Excel\n\n\nWe shift smem by 2 elements to the right (again only doing it for i values that won’t result in a negative index, i > 1) and add that to out to get our expected result:\n\n\n\nVisualizing test case 1 for puzzle 9 in Excel"
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-10",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-10",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 10",
    "text": "Puzzle 10\n@group(0) @binding(0) var<storage, read_write> a : array<f32>;\n@group(0) @binding(1) var<storage, read_write> b : array<f32>;\n@group(0) @binding(2) var<storage, read_write> out : array<f32>;\n\nconst wgs = vec3({{workgroupSize}});\nconst twg = vec3({{totalWorkgroups}});\nvar<workgroup> smem: array<f32, {{smemSize}}>;\n\n@compute @workgroup_size({{workgroupSize}})\nfn  main(@builtin(local_invocation_id) lid: vec3<u32>,\n         @builtin(global_invocation_id) gid: vec3<u32>) {\n    \n   // assumes wgs.x > arrayLength(&a);\n   smem[lid.x]  = a[gid.x] * b[gid.x];\n   \n   workgroupBarrier();\n             \n    if (gid.x == 0) {\n        for (var i: u32=0; i<arrayLength(&a); i+=1) {\n            out[0] += smem[i];\n        }\n    }\n}\n___________________________________\nTest case 1 \n\nWorkgroup Size       ( 4, 1, 1 )\nTotal Workgroups     ( 1, 1, 1 )\nShared Memory  Size  ( 4, 1, 1 )\n\nInput a  [  0  1  2  3 ]\nInput b  [  0  1  2  3 ]\n\nExpected [ 14 ]\n\n___________________________________\nTest case 2 \n\nWorkgroup Size       ( 5, 1, 1 )\nTotal Workgroups     ( 1, 1, 1 )\nShared Memory  Size  ( 5, 1, 1 )\n\nInput a  [  0  1  2  3  4 ]\nInput b  [  0  1  2  3  4 ]\n\nExpected [ 30 ]\nTo take the dot product between two 1-D array, we need to take their cumulative element-wise sum. The official solution starts by loading these element-wise sums into shared memory with:\nsmem[lid.x]  = a[gid.x] * b[gid.x];\n\n\n\nVisualizing test case 1 for puzzle 10 in Excel\n\n\nAfter that, we simply iterate through the shared memory array and accumulate the sum with:\nif (gid.x == 0) {\n    for (var i: u32=0; i<arrayLength(&a); i+=1) {\n        out[0] += smem[i];\n    }\n}\nIIUC, the guard if (gid.x == 0) prevents more than one thread from working on this task."
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-11",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-11",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 11",
    "text": "Puzzle 11\n@group(0) @binding(0) var<storage, read_write> a : array<f32>;\n@group(0) @binding(1) var<storage, read_write> b : array<f32>;\n@group(0) @binding(2) var<storage, read_write> out : array<f32>;\n\nconst wgs = vec3({{workgroupSize}});\nconst twg = vec3({{totalWorkgroups}});\n\nvar<workgroup> smemA: array<f32, wgs.x * wgs.y * wgs.z + 4>;\nvar<workgroup> smemB: array<f32, 4>;\n\n@compute @workgroup_size({{workgroupSize}})\nfn  main(@builtin(local_invocation_id) lid: vec3<u32>,\n         @builtin(global_invocation_id) gid: vec3<u32>,\n         @builtin(workgroup_id) wid: vec3<u32>\n         ) {\n    // Each workgroup is responsible computes total workgroup size\n    // values of out and caches total workgroup size + 4 values\n    // of a\n    let wgSize: u32 = wgs.x; // assumes wgs.y = wgs.z = 1\n    smemA[lid.x] = a[gid.x];\n    if (lid.x < 4) {\n        smemB[lid.x] = b[lid.x];\n        if (wid.x * wgSize + wgSize + lid.x < arrayLength(&a)) {\n            smemA[wgSize + lid.x] = \n                a[wid.x * wgSize + wgSize + lid.x];\n        } else {\n            smemA[wgSize + lid.x] = 0.0;\n        }\n    }\n    workgroupBarrier();\n    var sum: f32 = 0.0;\n    for (var i: u32 = 0; i < 4; i += 1) {\n        if (gid.x + i < arrayLength(&a)) {\n            sum = sum + smemA[lid.x + i] * smemB[i];\n        }\n    }\n    out[gid.x] = sum;\n}\n___________________________________\nTest case 1\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 2, 1, 1 )\nShared Memory  Size  ( 12, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 ]\nInput b  [  0  1  2  3 ]\n\nExpected [ 14 20 26 32 38 44 50 56 62 68 74 80 41 14  0 ]\n\n___________________________________\nTest case 2\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 3, 1, 1 )\nShared Memory  Size  ( 12, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 ]\nInput b  [  0  1  2  3 ]\n\nExpected [ 14 20 26 32 38 44 50 56 62 68 74 80 86 92 98 50 17  0 ]\nIn Test case 1, our workgroup has 8 threads, our shared memory arrays have 12 spots, and our input has 15 elements. We start by loading the first 8 elements of a into smemA with lid.x and gid.x:\nsmemA[lid.x] = a[gid.x];\nHere’s what that looks like in Excel. Each workgroup gets assigned its consecutive sequence of 8 elements:\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel\n\n\nNext, to load in the remaining 4 elements of a into smemA and all four elements of b into smemB we run the following code:\nif (lid.x < 4) {\n    smemB[lid.x] = b[lid.x];\n    if (wid.x * wgSize + wgSize + lid.x < arrayLength(&a)) {\n        smemA[wgSize + lid.x] = a[wid.x * wgSize + wgSize + lid.x];\n    } else {\n        smemA[wgSize + lid.x] = 0.0;\n    }\n}\nThe guard lid.x < 4 ensures that we are only handling 4 elements at a time. The next line is simple, and loads the full contents of b into smemB:\nsmemB[lid.x] = b[lid.x];\nNow we get into some more tricky stuff to assign the correct set of 4 final elements to shared memory in the appropriate workgroup. Let’s first visulize the more involved index (where wgSize is wgs.x which is 8 in Test case 1):\nwid.x * wgSize + wgSize + lid.x\nVisualizing this index in Excel for each workgroup, noting that it’s restricted to 4 elements (due to our guard lid.x < 4) as it builds off lid.x:\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel\n\n\nThe other index that we use is wgSize + lid.x which is more straightforward (it adds 8 to each of the four elements of lid.x):\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel\n\n\nThis index will be used to extend the index into the lat 4 elements of shared memory array past the 8 elements available in lid.x or gid.x for each workgroup with the following code:\nif (wid.x * wgSize + wgSize + lid.x < arrayLength(&a)) {\n    smemA[wgSize + lid.x] = a[wid.x * wgSize + wgSize + lid.x];\n} else {\n    smemA[wgSize + lid.x] = 0.0;\n}\nFor the first workgroup, the maximum value of wid.x * wgSize + wgSize + lid.x is less than arrayLength(&a) so we load in the 9th to 12th elements of a into smemA. For the second workgroup, the maximum value of wid.x * wgSize + wgSize + lid.x is more than arrayLength(&a) so we assign 0s:\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel\n\n\nIn this way, we have assigned a maximum of 12 elements of a into smemA, extending the available indexes gid.x and lid.x past their maximum of 8 elements using wid.x and wgs.x:\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel\n\n\nWith our data loaded into shared memory arrays we can now go about performing 1-D convolution between a and b with the following code:\nvar sum: f32 = 0.0;\nfor (var i: u32 = 0; i < 4; i += 1) {\n    if (gid.x + i < arrayLength(&a)) {\n        sum = sum + smemA[lid.x + i] * smemB[i];\n    }\n}\nout[gid.x] = sum;\nUnderstanding this loop was a pivotal point in my understanding of parallelism. While we are iterating over i which is a 32-bit unsigned integer, we are performing 8 element-wise operations in each loop iteration at the same time (since we are using lid.x to index into smemA) in each workgroup (which is why we use gid.x to index into out to assign the correct value of sum).\nWhat’s counterintuitive at first is that sum is a single f32 32-bit floating point number, but since we are using lid.x and gid.x it is being manipulated 16 different ways (8 threads across 2 workgroups). So although to me it initially looked like sum was behaving as an array, it’s not. The array-like behavior is the parallelism of the GPU.\nThe following line:\nsum = sum + smemA[lid.x + i] * smemB[i];\nis visualized in Excel for the first workgroup as follows:\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel\n\n\nNote that in each iteration of the loop we are shifting smemA and smemB by 1 element to the left and taking the elementwise product (across 8 threads). From the individual thread’s perspective, it’s a product between two numbers, accumulating their sum over each loop iteration.\nHere’s a visualization of the loop iterations in the second workgroup:\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel\n\n\nAfter each loop is finished, we assign the resulting number to its corresponding location in out to get our final result:\n\n\n\nVisualizing test case 1 for puzzle 11 in Excel"
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-12",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-12",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 12",
    "text": "Puzzle 12\n@group(0) @binding(0) var<storage, read_write> a : array<f32>;\n@group(0) @binding(1) var<storage, read_write> out : array<f32>;\n\nconst wgs = vec3({{workgroupSize}});\nconst twg = vec3({{totalWorkgroups}});\nvar<workgroup> smem: array<f32, {{smemSize}}>;\n\n@compute @workgroup_size({{workgroupSize}})\nfn main(@builtin(local_invocation_id) lid: vec3<u32>,\n        @builtin(global_invocation_id) gid: vec3<u32>,\n        @builtin(workgroup_id) wid: vec3<u32>) {\n    smem[lid.x] = a[gid.x];\n    workgroupBarrier();\n    for (var skip: u32 = 1; skip < wgs.x; skip = skip * 2) {\n        if (lid.x % skip == 0 \n            && lid.x + skip < wgs.x \n            && gid.x + skip < arrayLength(&a)) {\n            smem[lid.x] = smem[lid.x] + smem[lid.x + skip];\n        }\n        workgroupBarrier();\n    }\n    if (lid.x == 0) {\n        out[wid.x] = smem[0];\n    }\n}\n___________________________________\nTest case 1\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 1, 1, 1 )\nShared Memory  Size  ( 8, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7 ]\nExpected [ 28 ]\n\n___________________________________\nTest case 2\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 2, 1, 1 )\nShared Memory  Size  ( 8, 1, 1 )\n\nInput a  [  0  1  2  3  4  5  6  7  8  9 ]\nExpected [ 28 17 ]\nIn this puzzle we want to take the block-wise cumulative sum. For Test case 1, there is only 1 block (workgroup) so we want the sum of the full array, which is 28. For Test case 2, we have two 8-thread blocks and a 10-element input array. The cumulative sum for the first block is the sum of the first 8 elements (28) and the cumulative sum for the second block is the sum of the next two elements (17).\nThe first line of the solution is simple: we want to load into shared memory the corresponding set of 8-elements from the input array:\nsmem[lid.x] = a[gid.x];\nVisualizing this in Excel for each workgroup:\n\n\n\nVisualizing puzzle 12 in Excel\n\n\nThe next chunk of code is quite involved—its goal is to find the cumulative sum of the elements stored in shared memory following certain guards:\nfor (var skip: u32 = 1; skip < wgs.x; skip = skip * 2) {\n    if (lid.x % skip == 0 \n        && lid.x + skip < wgs.x \n        && gid.x + skip < arrayLength(&a)) {\n        smem[lid.x] = smem[lid.x] + smem[lid.x + skip];\n    }\n    workgroupBarrier();\n}\nHere’s what that code looks visualized in Excel where each column of arrays corresponds to each iteration of the loop. The cells highlighted in green in each array are the elements which pass the guard conditions in the if-statement:\n\n\n\nVisualizing puzzle 12 in Excel\n\n\nFor skip = 1, the first six elements pass the guard conditions. In each case (lid.x of 0, 1, 2, 3, 4, 5), lid.x % skip == 0 is true, lid.x + skip < wgs.x is true, and gid.x + skip < arrayLength(&a) is true. For the seventh element (lid.x = 7):\n\nlid.x % skip == 0: 7 % 1 == 0 is true\nlid.x + skip < wgs.x: 7 + 1 < 8 is false\ngid.x + skip < arrayLength(&a): 7 + 1 < 8 is false\n\nFor skip = 2: only the 0th, 2nd and 4th element pass the guard conditions. While the 6th element does pass the first guard (lid.x % skip == 0: 6 % 2 == 0 is true) it fails the second two guards as 6 + 2 is not less than wgs.x or arrayLength(&a).\nFinally for skip = 4, only the 0th element passes all guard conditions. The 4th element does pass the first condition (lid.x % skip == 0: 4 % 4 == 0 is true) but fails the second two guards as 4 + 4 is not less than wgs.x or arrayLength(&a).\nFor each skip we slide the elements over by skip and sum them to the previous iteration’s smem.\nIn Test case 2, we have the same prefix sum (28) for the first block. We also have a second block for which the prefix sum is much simpler since the skip value in only one iteration of the for loop passes all guard conditions.\n\n\n\nVisualizing puzzle 12 in Excel\n\n\nFor skip = 1:\n\nlid.x % skip == 0: is true for all 7 elements.\nlid.x + skip < wgs.x: is true for all 7 elements.\ngid.x + skip < arrayLength(&a): 8 + 1 < 10 is true only for the first element, so that’s the only sum that takes place (8 + 9 = 17).\n\nFor skip = 2 and skip = 4:\n\nlid.x % skip == 0: is true for some elements.\nlid.x + skip < wgs.x: is true for some elements.\ngid.x + skip < arrayLength(&a): is true for no element, therefore the code inside the if-condition never runs.\n\nUnderstanding the prefix sum algorithm took an unreasonable amount of time for me, and I’m still not completely comfortable, but the following visual did help solidify for me how it works. Green-highlighted cells are pairwise sums corresponding to the given skip. At the bottom I’ve listed out within which pairwise sums the given original array element is included.\n\n\n\nVisualizing puzzle 12 in Excel"
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-13",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-13",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 13",
    "text": "Puzzle 13\n@group(0) @binding(0) var<storage, read_write> a : array<f32>;\n@group(0) @binding(1) var<storage, read_write> out : array<f32>;\n\nconst wgs = vec3({{workgroupSize}});\nconst twg = vec3({{totalWorkgroups}});\nvar<workgroup> smem: array<f32, {{smemSize}}>;\n\nconst nRows = 4;\n\n@compute @workgroup_size({{workgroupSize}})\nfn main(@builtin(local_invocation_id) lid: vec3<u32>,\n        @builtin(global_invocation_id) gid: vec3<u32>,\n        @builtin(workgroup_id) wid: vec3<u32>) {\n    let nCols = arrayLength(&a) / nRows;\n    smem[lid.x] = a[wid.y * nCols + lid.x];\n    workgroupBarrier();\n    for (var skip: u32 = 1; lid.x + skip < nCols; skip *= 2) {\n        smem[lid.x] += smem[lid.x+skip];\n    }\n    \n    if (lid.x % nCols == 0) {\n        out[wid.y] = smem[0];\n    }\n}\n___________________________________\nTest case 1\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 1, 4, 1 )\nShared Memory  Size  ( 8, 1, 1 )\n\nInput a  \n        0    1    2    3    4    5  \n        6    7    8    9   10   11  \n       12   13   14   15   16   17  \n       18   19   20   21   22   23\n\nExpected [ 15 51 87 123 ]\n\n___________________________________\nTest case 2\n\nWorkgroup Size       ( 8, 1, 1 )\nTotal Workgroups     ( 1, 4, 1 )\nShared Memory  Size  ( 8, 1, 1 )\n\nInput a  \n        0    1    2    3  \n        4    5    6    7  \n        8    9   10   11  \n       12   13   14   15\n\nExpected [  6 22 38 54 ]\nThe goal of this exercise is to find the sum of each “row” in the input array. I put “row” in quotation marks because the input array is actually 1-D, so we look at the example to determine how many rows and columns we want.\nI’ll walk through Test case 1.\nThe number of rows is a constant 4:\nconst nRows = 4;\nThe number of columns is the length of the array divided by the number of rows:\nlet nCols = arrayLength(&a) / nRows;\nEach row of the input array is stored in a separate workgroup’s shared memory. This is achieved by multiplying nCols by wid.y before adding lid.x:\nsmem[lid.x] = a[wid.y * nCols + lid.x];\nHere’s what that index, wid.y * nCols + lid.x, looks like for each row:\n\n\n\nwid.y\nwid.y * nCols + lid.x\n\n\n\n\n0\n0, 1, 2, 3, 4, 5\n\n\n1\n6, 7, 8, 9, 10, 11\n\n\n2\n12, 13, 14, 15, 16, 17\n\n\n3\n18, 19, 20, 21, 22, 23\n\n\n\nVisualizing how, using the index wid.y * nCols + lid.x, we load the input array a into each workgroup’s shared memory smem:\n\n\n\nVisualizing puzzle 13 in Excel\n\n\nNext, similar to the previous puzzle’s prefix sum algorithm, we iterate through each row, accumulating the sum by iterating over array elements in increasing skip amounts:\nfor (var skip: u32 = 1; lid.x + skip < nCols; skip *= 2) {\n    smem[lid.x] += smem[lid.x+skip];\n}\nVisualizing that for-loop in the first workgroup, in which we find the sum of the first row of a (highlighted in green):\n\n\n\nVisualizing puzzle 13 in Excel\n\n\nI’m not 100% sure why we don’t have guards in this puzzle as we did in Puzzle 12, but my guess is that we don’t need it here since the number of threads in the workgroup (8) is the same as the shared memory size (8).\nVisualizing the for-loops that occur in the other three workgroups, one for each row of the input array with the final cumulative sum highlighted in green:\n\n\n\nVisualizing puzzle 13 in Excel\n\n\n\n\n\nVisualizing puzzle 13 in Excel\n\n\n\n\n\nVisualizing puzzle 13 in Excel"
  },
  {
    "objectID": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-14",
    "href": "posts/2024-11-17-webgpupuzzles/index.html#puzzle-14",
    "title": "WebGPU Puzzles: Walk through of Official Solutions",
    "section": "Puzzle 14",
    "text": "Puzzle 14\n@group(0) @binding(0) var<storage, read_write> a: array<f32>;\n@group(0) @binding(1) var<storage, read_write> b: array<f32>;\n@group(0) @binding(2) var<storage, read_write> output: array<f32>;\n\nconst wgs = vec3({{workgroupSize}});\nconst twg = vec3({{totalWorkgroups}});\nconst tileSize = vec3({{workgroupSize}});\nvar<workgroup> a_shared: array<f32, 256>;\nvar<workgroup> b_shared: array<f32, 256>;\n\n@compute @workgroup_size({{workgroupSize}})\nfn main(\n  @builtin(local_invocation_id) lid: vec3<u32>,\n  @builtin(global_invocation_id) gid: vec3<u32>,\n  @builtin(workgroup_id) wid: vec3<u32>) {\n    let N = u32(sqrt(f32(arrayLength(&a))));\n    let i = wid.x * wgs.x + lid.x;\n    let j = wid.y * wgs.y + lid.y;\n    let local_i = lid.x;\n    let local_j = lid.y;\n\n    var acc: f32 = 0.0;\n\n    for (var k: u32 = 0u; k < N; k = k + tileSize.x) {\n      if (j < N && k + local_i < N) {\n        a_shared[local_j * tileSize.x + local_i] \n        = a[j * N + (k + local_i)];\n      } else {\n        a_shared[local_j * tileSize.x + local_i] = 0.0;\n      }\n      if (i < N && k + local_j < N) {\n        b_shared[local_j * tileSize.x + local_i] \n        = b[i + (k + local_j) * N];\n      } else {\n        b_shared[local_j * tileSize.x + local_i] = 0.0;\n      }\n\n      workgroupBarrier();\n\n      let local_k_max = min(tileSize.x, N - k);\n      for (var local_k: u32 = 0u; \n            local_k < local_k_max; \n            local_k = local_k + 1u) {\n        acc += a_shared[local_j * tileSize.x + local_k] \n          * b_shared[local_k * tileSize.x + local_i];\n      }\n\n      workgroupBarrier();\n    }\n\n    if (i < N && j < N) {\n      output[i + j * N] = acc;\n    }\n}\n___________________________________\nTest case 1\n\nWorkgroup Size       ( 3, 3, 1 )\nTotal Workgroups     ( 1, 1, 1 )\nShared Memory  Size  ( 3, 3, 1 )\n\nInput a  \n        0    1  \n        2    3\n\nInput b  \n        0    1  \n        2    3\n\nExpected \n        2    3  \n        6   11\n\n___________________________________\nTest case 2\n\nWorkgroup Size       ( 1, 1, 1 )\nTotal Workgroups     ( 2, 2, 1 )\nShared Memory  Size  ( 3, 3, 1 )\n\nInput a  \n        0    1  \n        2    3\n\nInput b  \n        0    1  \n        2    3\n\nExpected \n        2    3  \n        6   11\n\n___________________________________\nTest case 3\n\nWorkgroup Size       ( 4, 4, 1 )\nTotal Workgroups     ( 1, 1, 1 )\nShared Memory  Size  ( 4, 4, 1 )\n\nInput a  \n        0    1    2  \n        3    4    5  \n        6    7    8\n\nInput b  \n        9   10   11  \n       12   13   14  \n       15   16   17\n\nExpected \n       42   45   48  \n      150  162  174  \n      258  279  300\n\n___________________________________\nTest case 4\n\nWorkgroup Size       ( 2, 2, 1 )\nTotal Workgroups     ( 2, 2, 1 )\nShared Memory  Size  ( 2, 2, 1 )\n\nInput a  \n        0    1    2  \n        3    4    5  \n        6    7    8\n\nInput b  \n        9   10   11  \n       12   13   14  \n       15   16   17\n\nExpected \n       42   45   48  \n      150  162  174  \n      258  279  300\n\n___________________________________\nTest case 5\n\nWorkgroup Size       ( 2, 2, 1 )\nTotal Workgroups     ( 2, 2, 1 )\nShared Memory  Size  ( 2, 2, 1 )\n\nInput a  \n        0    1    2    3  \n        4    5    6    7  \n        8    9   10   11  \n       12   13   14   15\n\nInput b  \n        0    1    2    3  \n        4    5    6    7  \n        8    9   10   11  \n       12   13   14   15\n\nExpected \n       56   62   68   74  \n      152  174  196  218  \n      248  286  324  362  \n      344  398  452  506\nWalking through this puzzle’s official solution was another pivotal point in my understanding of GPU parallelism. I’ll start with Test case 1.\nThere are three, what I call, “core indexes” that this solution establishes: local_i, local_j and local_j * tileSize.x + local_i. We can see that local_i indexes across rows while local_j indexes down columns. local_j * tileSize.x + local_i indexes the threads left-to-right and top-to-bottom.\nlet N = u32(sqrt(f32(arrayLength(&a))));\nlet i = wid.x * wgs.x + lid.x;\nlet j = wid.y * wgs.y + lid.y;\nlet local_i = lid.x;\nlet local_j = lid.y;\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nThe indexes i and j (which we’ll look at shortly), because we have only 1 workgroup for Test case 1, are the same as local_i and local_j, respectively.\nLet’s next tackle the code which loads input arrays a and b into shared memory a_shared and b_shared, respectively:\nif (j < N && k + local_i < N) {\n    a_shared[local_j * tileSize.x + local_i] = a[j * N + (k + local_i)];\n} else {\n    a_shared[local_j * tileSize.x + local_i] = 0.0;\n}\n\nif (i < N && k + local_j < N) {\n    b_shared[local_j * tileSize.x + local_i] = b[i + (k + local_j) * N];\n} else {\n    b_shared[local_j * tileSize.x + local_i] = 0.0;\n}\nNote that this code runs inside a for-loop:\nfor (var k: u32 = 0u; k < N; k = k + tileSize.x) { ... }\nbut since N is 2 and tileSize.x is 3 for this test case this outermost loop runs only once.\nThe visualization below is, at the highest level, broken into two boxes: one for a_shared (left) and one for b_shared (right).\nif (j < N && k + local_i < N) {\n    a_shared[local_j * tileSize.x + local_i] = a[j * N + (k + local_i)];\n} else {\n    a_shared[local_j * tileSize.x + local_i] = 0.0;\n}\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nLet’s walk through a_shared first.\nThe cells highlighted in green are the threads that satisfy the guard condition j < N. In this single iteration of the outermost loop, k = 0. The second condition in the guard is k + local_i < N, the threads in the workgroup which satisfy this condition are highlighted in green. When combining the use of indexes j and k + local_i we see that the threads which satisfy the full condition j < N && k + local_i < N are highlighted in green. There are four such threads and they are indexed 0, 1, 2, 3—these are used to index into a when assigning values to a_shared:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nThe grid on the right shows the values of a that are assigned to a_shared at the indexes in the grid shown on the left:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nHere’s a mapping of index to value for a_shared—this is key in understanding the next part of the code:\n\n\n\nIndex\nValue\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n3\n2\n\n\n4\n3\n\n\n\nWe can now go into the inner-most loop:\nlet local_k_max = min(tileSize.x, N - k);\nfor (\n        var local_k: u32 = 0u; \n        local_k < local_k_max; \n        local_k = local_k + 1u\n    ) {\n        acc += a_shared[local_j * tileSize.x + local_k] * b_shared[local_k * tileSize.x + local_i];\n}\nlocal_k_max is 2 (N - k), so the loop iterates twice, as shown in the visualization below:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nGoing left-to-right in each loop iterations:\n\nlocal_j * tileSize.x + local_k: the index into a_shared.\nlocal_k * tileSize.x + local_i: the index into b_shared.\na_shared[local_j * tileSize.x + local_k]: the values of a_shared used in the loop iteration.\nb_shared[local_k * tileSize.x + local_i]: the values of b_shared used in the loop iteration.\nacc: the element-wise product of a_shared[local_j * tileSize.x + local_k] and b_shared[local_k * tileSize.x + local_i].\n\nNote that while acc looks like an array, it’s actually just a single floating point value (var acc: f32 = 0.0;) which has a different value in each workgroup thread. That is the power of indexing!\nLooking at the matrix multiplication between a and b as we would do it by hand:\n\\[\\left[\\begin{matrix}0 & 1 \\ 2 & 3\\end{matrix}\\right] \\times \\left[\\begin{matrix}0 & 1 \\ 2 & 3\\end{matrix}\\right] = \\left[\\begin{matrix}2 & 3 \\ 6 & 11\\end{matrix}\\right]\\]\nThe top-left value in the result (2) is the dot product between the first row of a and the first column of b. In our GPU-implementation, that dot product occurs across two loop iterations and different threads as shown in the purple-highlighted cells below:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nLet’s now take a look at test case 2.\nHere are the constants and “core indexes” for this test case (same code, different organization than test case 1).\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nFor this test case, the outermost loop runs twice and each time the innermost loop runs once. Here’s the first iteration of the outermost loop:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nand here’s the second iteration of the outermost loop (with its single iteration of the innermost loop):\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nNote that the value of acc at the end of each outermost loop iteration is the same as test case 1 except that now, since we have 4 workgroups each with 1 thread, each value of the four values of acc in test case 2 are assigned to one workgroup each.\nLet’s move on to test case 3, in which we now are performing matrix multiplication between two 3x3 matrices in a single workgroup, analogous to test case 1.\nThe constants and core indexes:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nThe outermost loop, for this test case, runs only once. Here are the indexes used and values of a_shared and b_shared:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nlocal_k_max is 3 so the innermost loop runs three times, with the final result highlighted in green:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nMoving on to test case 4, which is unique so far in the sense that the number of threads per workgroup (2x2 = 4) is less than the number of elements in each matrix being multiplied (3x3 = 9). However, the core indexes and inner- and outermost loops still suffice.\nThe constants and core indexes:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nThe outermost loop has two iterations. Here is the first iteration, in which the innermost loop runs twice:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nIn the second-most iteration of the outermost loop, the innermost loop runs only once, note the final result highlighted in green:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nHere’s a visualization of how the dot product of the first row of a (0, 1, 2) and the first column of b (9, 12, 15) accumulates through element-wise products across different loop iterations to yield the final result of 42:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nThe last test case is test case 5, in which the number of available threads (16) matches the number of elements in each input array (16).\nThe constants and core indexes:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nThe outermost loop runs twice and each time the innermost loop runs twice as well. Here’s the first iteration of the outermost loop:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nHere is the second iteration of the outermost loop (the inner loop runs twice). Note the final result highlighted in green, uses up all threads:\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nHere I visualize the dot product between the first row of a (0, 1, 2, 3) and the first column of b (0, 4, 8, 12) to yield the result 56.\n\n\n\nVisualizing puzzle 14 in Excel\n\n\nThat’s a wrap for the official solutions’ walk through! You can find the Excel spreadsheet here."
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html",
    "href": "posts/2024-06-03-wd/index.html",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "",
    "text": "In fastai Part 1 Lesson 7 (Chapter 8 of the textbook), we are introduced to the concept of weight decay, where the sum of the squared weights multiplied by a factor wd is added to the loss. In practice, instead of adding wd times the sum of squared weights to the loss (which would result in a large values of loss which destabilizes training), we add the derivative of that to the parameters’ gradients. In pseudo code:\nfor param in model.parameters():\n  param.grad += wd * sum(params)\nBy increasing the loss with the squared sum of weights, we are forcing the model to minimize the weights when it minimizes the loss, which makes the model generalize better (as large weights result in a neural net function with sharp peaks and troughs that can overfit to the training data).\nIn this notebook I’ll train on the MovieLens 100k dataset using different values of weight decay and then plotting both the resulting training and validation loss curves and weight distribution to see how weight decay affects the training process and the resulting model."
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html#create-dataloaders-and-learner",
    "href": "posts/2024-06-03-wd/index.html#create-dataloaders-and-learner",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "Create DataLoaders and Learner",
    "text": "Create DataLoaders and Learner\nI’l re-use the code from Chapter 8 to create DataLoaders and the Learner.\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\n\n\n\n\n    \n      \n      100.15% [4931584/4924029 00:01<00:00]\n    \n    \n\n\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nmovies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\nmovies.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      294\n      Money Talks (1997)\n      3\n    \n    \n      1\n      486\n      Crossing Guard, The (1995)\n      4\n    \n    \n      2\n      13\n      High Noon (1952)\n      5\n    \n    \n      3\n      878\n      My Life as a Dog (Mitt liv som hund) (1985)\n      5\n    \n    \n      4\n      433\n      Star Wars (1977)\n      5\n    \n    \n      5\n      847\n      Streetcar Named Desire, A (1951)\n      3\n    \n    \n      6\n      833\n      Akira (1988)\n      4\n    \n    \n      7\n      503\n      Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)\n      3\n    \n    \n      8\n      5\n      Last of the Mohicans, The (1992)\n      1\n    \n    \n      9\n      868\n      Stand by Me (1986)\n      4\n    \n  \n\n\n\nIn the text, the following Learner resulted in the lowest validation loss (0.821688) so I’ll just use that in this experiment.\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\nI’ll do an initial training to sort my head around how I’m going to collect data during the experiment.\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.882671\n      0.955993\n      00:12\n    \n    \n      1\n      0.654915\n      0.892832\n      00:12\n    \n    \n      2\n      0.525330\n      0.872967\n      00:12\n    \n    \n      3\n      0.436863\n      0.858952\n      00:12\n    \n    \n      4\n      0.438653\n      0.854505\n      00:12"
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html#planning-the-experiment",
    "href": "posts/2024-06-03-wd/index.html#planning-the-experiment",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "Planning the Experiment",
    "text": "Planning the Experiment\nI’ll be trying that same learning process (5 epochs, 5e-3 learning rate) with different weight decay wd values (from 0.0001 to 1.0). Once the experiment is done, I want to visualize the following for all wd values:\n\ntraining loss curve .\nvalidation loss curve.\ndistribution of model parameters.\n\nI’ll reference the plot_loss source code for how to capture those values from the Learner’s Recorder object.\nBut first, I’ll plot the losses using that plot_loss function to have a baseline reference:\n\nlearn.recorder.plot_loss();\n\n\n\n\n\n\n\n\n\nPlotting Loss Values\nThe following line in plot_loss plots the training loss:\nax.plot(list(range(skip_start, len(self.losses))), self.losses[skip_start:], label='train')\nI’ll see if I can recreate that plot:\n\nax=plt.gca()\nax.plot(list(range(5, len(learn.recorder.losses))), learn.recorder.losses[5:], label='train');\n\n\n\n\n\n\n\n\nCool, that looks the same! Now for the validation loss:\n\nax=plt.gca()\nax.plot(list(range(5, len(learn.recorder.losses))), learn.recorder.losses[5:], label='train');\nidx = (np.array(learn.recorder.iters)<5).sum()\nvalid_col = learn.recorder.metric_names.index('valid_loss') - 1\nax.plot(learn.recorder.iters[idx:], L(learn.recorder.values[idx:]).itemgot(valid_col), label='valid')\nax.legend();\n\n\n\n\n\n\n\n\nNice, I’m able to recreate plot_loss using the raw data recorded in the Recorder."
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html#plotting-distribution-of-weights",
    "href": "posts/2024-06-03-wd/index.html#plotting-distribution-of-weights",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "Plotting Distribution of Weights",
    "text": "Plotting Distribution of Weights\nNext, I want to be able to visualize (and calculate summary statistics for) the weights of the model.\nThe model has four sets of parameters: the user and item weights, and the user and item biases.\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\n\nfor p in learn.model.parameters():\n  print(p.shape)\n\ntorch.Size([944, 50])\ntorch.Size([1665, 50])\ntorch.Size([944, 1])\ntorch.Size([1665, 1])\n\n\nLet’s take a look at the first set of parameters, which are the u_weight (user weight) parameters. This is the embedding with 50 latent factors for each user.\n\npd.Series(first(learn.model.parameters()).flatten().detach().numpy()).hist();\n\n\n\n\n\n\n\n\nNext, let’s look at the bias parameters for the users (u_bias):\n\nu_bias_params = next(itertools.islice(learn.model.parameters(), 2, None))\n\n\nu_bias_params.shape\n\ntorch.Size([944, 1])\n\n\n\npd.Series(u_bias_params.flatten().detach().numpy()).hist();\n\n\n\n\n\n\n\n\nLooking at all of the parameters:\n\ntorch.nn.utils.parameters_to_vector(learn.model.parameters()).shape, 944*50 + 944*1 + 1665*50 + 1665*1\n\n(torch.Size([133059]), 133059)\n\n\n\npd.Series(torch.nn.utils.parameters_to_vector(learn.model.parameters()).detach().numpy()).hist();\n\n\n\n\n\n\n\n\nNow I know how to access and plot the training loss, validation loss and model weights."
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html#estimating-runtime-and-storage-size",
    "href": "posts/2024-06-03-wd/index.html#estimating-runtime-and-storage-size",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "Estimating Runtime and Storage Size",
    "text": "Estimating Runtime and Storage Size\nEach model takes about 1 minute to run on Colab. I trained on Paperspace CPUs but it was taking 30+ seconds per epoch (while it takes 10-14 seconds on Colab). I have a 2015 Mac so I can’t install fastai locally (if I’m interpreting this note correctly).\nOriginally I was going to use torch.arange to create weight decay values (without subtracting 1e-6 from the upper bound as I eventually did below), but it acculumated floating point error. The following tensor should stop at 0.0990 but goes up to 0.1000 because of floating point error.\n\ntorch.arange(0.01, 0.1, 0.001)\n\ntensor([0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170, 0.0180,\n        0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260, 0.0270,\n        0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350, 0.0360,\n        0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440, 0.0450,\n        0.0460, 0.0470, 0.0480, 0.0490, 0.0500, 0.0510, 0.0520, 0.0530, 0.0540,\n        0.0550, 0.0560, 0.0570, 0.0580, 0.0590, 0.0600, 0.0610, 0.0620, 0.0630,\n        0.0640, 0.0650, 0.0660, 0.0670, 0.0680, 0.0690, 0.0700, 0.0710, 0.0720,\n        0.0730, 0.0740, 0.0750, 0.0760, 0.0770, 0.0780, 0.0790, 0.0800, 0.0810,\n        0.0820, 0.0830, 0.0840, 0.0850, 0.0860, 0.0870, 0.0880, 0.0890, 0.0900,\n        0.0910, 0.0920, 0.0930, 0.0940, 0.0950, 0.0960, 0.0970, 0.0980, 0.0990,\n        0.1000])\n\n\nThe following arange call results in the correct number of values:\n\ntorch.arange(0.0001, 0.001-1e-6, 0.00001).shape, \\\ntorch.arange(0.001, 0.01-1e-6, 0.0001).shape, \\\ntorch.arange(0.01, 0.10-1e-6, 0.001).shape, \\\ntorch.arange(0.1, 1.0-1e-6, 0.01).shape\n\n(torch.Size([90]), torch.Size([90]), torch.Size([90]), torch.Size([90]))\n\n\n\nwds = torch.cat([\n    torch.arange(0.0001, 0.001-1e-6, 0.00001),\n    torch.arange(0.001, 0.01-1e-6, 0.0001),\n    torch.arange(0.01, 0.10-1e-6, 0.001),\n    torch.arange(0.1, 1.0-1e-6, 0.01)])\n\n\nwds.shape\n\ntorch.Size([360])\n\n\n\npd.Series(wds).plot();\n\n\n\n\n\n\n\n\nNext, I’ll estimate the disk space required to store the training loss, validation loss and weights for all 360 models (1 per wd value).\nWith ChatGPT’s help:\n\nimport pickle\nimport io\n\ndef get_pickled_object_size(obj):\n    # Create a BytesIO buffer\n    buffer = io.BytesIO()\n    # Pickle the object into the buffer\n    pickle.dump(obj, buffer)\n    # Get the size of the buffer\n    size = buffer.getbuffer().nbytes\n    return size\n\n\nres = {\n    'training_loss': learn.recorder.losses,\n    'iters': learn.recorder.iters,\n    'valid_cols': learn.recorder.metric_names.index('valid_loss') - 1,\n    'validation_loss': L(learn.recorder.values[(np.array(learn.recorder.iters)<5).sum():]).itemgot(learn.recorder.metric_names.index('valid_loss') - 1),\n    'weights': torch.nn.utils.parameters_to_vector(learn.model.parameters())\n}\n\n\nsize = get_pickled_object_size(res)\nprint(f'The size of the pickled object for one model is {size/1e6} MB.')\nprint(f'The total size of the pickled objects for 360 models will be ~{360*size/1e6} MB.')\n\nThe size of the pickled object for one model is 2.345662 MB.\nThe total size of the pickled objects for 360 models will be ~844.43832 MB.\n\n\nThat’s quite a bit. And seem unnecessarily large. learn.recorder.losses is a list of tensors:\n\nlearn.recorder.losses[:5]\n\n[TensorBase(1.8254),\n TensorBase(1.8496),\n TensorBase(1.9542),\n TensorBase(1.8464),\n TensorBase(1.9200)]\n\n\nPerhaps I can convert that to a list of floats and see if it reduces the storage size:\n\nres = {\n    'training_loss': [t.item() for t in learn.recorder.losses],\n    'iters': learn.recorder.iters,\n    'valid_cols': learn.recorder.metric_names.index('valid_loss') - 1,\n    'validation_loss': L(learn.recorder.values[(np.array(learn.recorder.iters)<5).sum():]).itemgot(learn.recorder.metric_names.index('valid_loss') - 1),\n    'weights': torch.nn.utils.parameters_to_vector(learn.model.parameters())\n}\n\nsize = get_pickled_object_size(res)\nprint(f'The size of the pickled object for one model is {size/1e6} MB.')\nprint(f'The total size of the pickled objects for 360 models will be ~{360*size/1e6} MB.')\n\nThe size of the pickled object for one model is 0.589096 MB.\nThe total size of the pickled objects for 360 models will be ~212.07456 MB.\n\n\nThat’s better! Most of the size is coming from my weights.\n\nsize = get_pickled_object_size(torch.nn.utils.parameters_to_vector(learn.model.parameters()))\nprint(f'The size of the pickled object for one model is {size/1e6} MB.')\nprint(f'The total size of the pickled objects for 360 models will be ~{360*size/1e6} MB.')\n\nThe size of the pickled object for one model is 0.532648 MB.\nThe total size of the pickled objects for 360 models will be ~191.75328 MB.\n\n\nWith ChatGPT’s help: there are 133059 float32 elements in the weights tensor. Each float32 element is 4 bytes large. 4 x 133059 is 532236 bytes, which is ~0.5MB. I’ll keep them as float32’s and accept the larger disk space (since I’m keeping this only temporarily)."
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html#running-the-experiment",
    "href": "posts/2024-06-03-wd/index.html#running-the-experiment",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "Running the Experiment",
    "text": "Running the Experiment\nNow I can run the experiment and collect the data on it. I’ll do a quick run with 3 models and save the Learner objects to make sure I’m collecting data correctly.\n\nres = {'training_loss': [], 'iters': [], 'validation_loss': [], 'weights': []}\nlearners = []\ncount = 0\n\nfor wd in wds:\n  if count == 3: break\n  learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n  learn.fit_one_cycle(5, 5e-3, wd=wd.item())\n  learn.recorder.plot_loss();\n  learners.append(learn)\n\n  res['training_loss'].append([t.item() for t in learn.recorder.losses])\n  res['iters'].append(learn.recorder.iters)\n  res['validation_loss'].append(L(learn.recorder.values[(np.array(learn.recorder.iters)<5).sum():]).itemgot(learn.recorder.metric_names.index('valid_loss') - 1))\n  res['weights'].append(torch.nn.utils.parameters_to_vector(learn.model.parameters()))\n  count += 1\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.867206\n      0.947485\n      00:12\n    \n    \n      1\n      0.612190\n      0.929238\n      00:12\n    \n    \n      2\n      0.401040\n      0.964420\n      00:13\n    \n    \n      3\n      0.298337\n      0.977934\n      00:12\n    \n    \n      4\n      0.293123\n      0.979958\n      00:12\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.879830\n      0.946919\n      00:12\n    \n    \n      1\n      0.544347\n      0.930262\n      00:12\n    \n    \n      2\n      0.416691\n      0.962877\n      00:12\n    \n    \n      3\n      0.298801\n      0.978486\n      00:12\n    \n    \n      4\n      0.269458\n      0.979087\n      00:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.867352\n      0.940780\n      00:11\n    \n    \n      1\n      0.624375\n      0.930750\n      00:12\n    \n    \n      2\n      0.405802\n      0.975494\n      00:12\n    \n    \n      3\n      0.325339\n      0.991441\n      00:12\n    \n    \n      4\n      0.300811\n      0.993090\n      00:12\n    \n  \n\n\n\n\n\n\n\n\n\n\nGood to know—it plots on the same chart! I could just use this functionality instead of saving the training and validation loss values, but I want to keep those to do additional visualizations.\nNext, I’ll check that the recorded values are the same as the Learner object.\n\nlen(res['training_loss'])\n\n3\n\n\n\nres['training_loss'][0] == [t.item() for t in learners[0].recorder.losses], \\\nres['iters'][1] == learners[1].recorder.iters, \\\nres['validation_loss'][2] == L(learners[2].recorder.values[(np.array(learners[2].recorder.iters)<5).sum():]).itemgot(learners[2].recorder.metric_names.index('valid_loss') - 1), \\\n(res['weights'][2] == torch.nn.utils.parameters_to_vector(learners[2].model.parameters())).sum()\n\n(True, True, True, tensor(133059))\n\n\nTesting that I can pickle and unpickle the results:\n\nsave_pickle('wd_res_test.pkl', res)\nres = load_pickle('/content/wd_res_test.pkl')\n\nres['training_loss'][0] == [t.item() for t in learners[0].recorder.losses], \\\nres['iters'][1] == learners[1].recorder.iters, \\\nres['validation_loss'][2] == L(learners[2].recorder.values[(np.array(learners[2].recorder.iters)<5).sum():]).itemgot(learners[2].recorder.metric_names.index('valid_loss') - 1), \\\n(res['weights'][2] == torch.nn.utils.parameters_to_vector(learners[2].model.parameters())).sum()\n\n(True, True, True, tensor(133059))\n\n\nNice!\nHowever, I noticed that some epochs took up to 13 seconds. I wonder if training time is affected by weight decay value? I’ll train with a much larger weight decay and see if it speeds up the training.\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.9)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.990363\n      1.074203\n      00:12\n    \n    \n      1\n      1.003346\n      1.067053\n      00:12\n    \n    \n      2\n      0.976281\n      1.029760\n      00:12\n    \n    \n      3\n      0.978760\n      1.001970\n      00:12\n    \n    \n      4\n      0.912565\n      0.993564\n      00:12\n    \n  \n\n\n\nNope, seems like the training time per epoch just varies.\n\nres = {'wds': wds, 'training_loss': [], 'iters': [], 'validation_loss': [], 'weights': []}\n\nfor wd in wds:\n  learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n  with learn.no_logging(), learn.no_bar():\n    learn.fit_one_cycle(5, 5e-3, wd=wd.item())\n\n  res['training_loss'].append([t.item() for t in learn.recorder.losses])\n  res['iters'].append(learn.recorder.iters)\n  res['validation_loss'].append(L(learn.recorder.values[(np.array(learn.recorder.iters)<5).sum():]).itemgot(learn.recorder.metric_names.index('valid_loss') - 1))\n  res['weights'].append(torch.nn.utils.parameters_to_vector(learn.model.parameters()))\n\nsave_pickle('wd_res.pkl', res)"
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html#analyzing-training-results",
    "href": "posts/2024-06-03-wd/index.html#analyzing-training-results",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "Analyzing Training Results",
    "text": "Analyzing Training Results\n\nmatplotlib Animations\nI’ll create an animation with matplotlib which highlights the training loss curve for each weight decay value and prints the weight decay value on the plot.\nTo start, I’ll make sure that all 360 of my training loss lists are of the same length (6250 steps).\n\n#wd_res = load_pickle('/content/wd_res.pkl')\n\n\ntrn_loss_t = torch.stack([torch.tensor(el) for el in wd_res['training_loss']])\n\n\ntrn_loss_t.shape # looks good\n\ntorch.Size([360, 6250])\n\n\nNext, I’ll create my x values for the plot as a range from 0 to 6249.\n\nx = list(range(len(wd_res['training_loss'][0])))\nlen(x), x[:5], x[-5:]\n\n(6250, [0, 1, 2, 3, 4], [6245, 6246, 6247, 6248, 6249])\n\n\nWith ChatGPT’s help, I used the following code to create an animated GIF which highlights the training and validation loss curves for the given weight decay value.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Prepare Data\nx = list(range(len(wd_res['training_loss'][0])))\n\ndata = wd_res['training_loss']\n\nlabels = wd_res['wds']\n\n# Create Static Plot\nfig, ax = plt.subplots()\nax.set_xlabel('Step')\nax.set_ylabel('Training Loss')\n\nlines = [ax.plot(x, y, alpha=0.0, color='#ff0088', linewidth=2)[0] for y in data]  # Initial lines with lower opacity\ntext = ax.text(0.5, 1.05, \"\", transform=ax.transAxes, ha=\"center\")\n\n# Define Animation Function\ndef animate(i):\n    # Reset all lines to low opacity\n    for line in lines:\n        line.set_alpha(0.0)\n\n    # Highlight the current line\n    lines[i].set_alpha(1.0)\n\n    # Update the text\n    text.set_text(f\"weight decay = {labels[i].item():.5f}\")\n\n    return lines + [text]\n\n# Create Animation\nani = FuncAnimation(fig, animate, frames=len(data), interval=150, blit=True)\n\n# Save as GIF\ngif_writer = PillowWriter(fps=6)\nani.save('training_loss.gif', writer=gif_writer)\n\n\nHere’s the GIF for training loss. The loss curve slowly moves upward with the minimum loss value increasing as weight decay increases. It really starts picking up at a weight decay value of around 0.1.\n\n\n\n\n\nHere’s the GIF for the validation loss—the loss curve starts out decreasing first then increasing and at a weight decay value of around 0.03, starts decreasing monotonically with the minimum loss reaching its minimum. It starts to shift upwards again at around a loss of 0.3.\n\n\n\n\n\nNext, I’ll animate the distribution of the weights (as weight decay value changes), modifying the above code accordingly:\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Prepare Data\ndata = wd_res['weights']\nlabels = wd_res['wds']\n\n# Create Static Plot\nfig, ax = plt.subplots()\nax.set_xlabel('Weights')\nax.set_ylabel('Count')\n\n# Define Animation Function\ndef animate(i):\n    ax.clear()  # Clear previous histogram\n    n, bins, patches = ax.hist(data[i].detach().numpy())\n    ax.set_title(f\"weight decay = {labels[i].item():.5f}\")\n    ax.set_xlabel('Weights')\n    ax.set_ylabel('Count')\n    ax.set_xlim(-0.5, 0.5)  # Set x-axis limits to keep consistent across frames\n    ax.set_ylim(0, 75000)  # Set y-axis limits to keep consistent across frames\n    return patches\n\n# Create Animation\nani = FuncAnimation(fig, animate, frames=len(data), interval=150, blit=True)\n\n# Save as GIF\ngif_writer = PillowWriter(fps=6)\nani.save('weights.gif', writer=gif_writer)\n\n\nHere’s the resulting GIF showing the changing distribution of weights. Note that the range of weights starts out beyond -0.5 to 0.5, and ends up within -0.2 and +0.2 as the weight decay value increases to 1.0.\n\n\n\n\n\n\n\nTrends\nI’ll next look at how the final training loss, validation loss and median weight value changes with weight decay values.\n\nmin_trn_loss = trn_loss_t.min(dim=-1)[0]\nmin_trn_loss.shape\n\ntorch.Size([360])\n\n\n\n\nShow the code\nplt.scatter(wd_res['wds'], min_trn_loss, s=5, alpha=0.4)\n\n# Set axis labels\nplt.xlabel('Weight Decay Value')\nplt.ylabel('Minimum Training Loss')\n\n# Display the plot\nplt.show();\n\n\n\n\n\n\n\n\n\nAs the weight-decay value increases, so does the training loss.\n\nval_loss_t = torch.stack([torch.tensor(el) for el in wd_res['validation_loss']])\nmin_val_loss = val_loss_t.min(dim=-1)[0]\nmin_val_loss.shape\n\ntorch.Size([360])\n\n\n\n\nShow the code\nplt.scatter(wd_res['wds'], min_val_loss, s=5, alpha=0.4)\n\n# Set axis labels\nplt.xlabel('Weight Decay Value')\nplt.ylabel('Minimum Validation Loss')\n\n# Display the plot\nplt.show();\n\n\n\n\n\n\n\n\n\nThe minimum validation loss achieved during training reaches a minimum at a weight decay value of around 0.2 and then increases as weight decay increases to 1.0.\n\nweights_t = torch.stack([el.clone().detach() for el in wd_res['weights']])\nweights_t.shape\n\ntorch.Size([360, 133059])\n\n\n\nmed_weights_t = weights_t.median(dim=-1)[0]\nmed_weights_t.shape\n\ntorch.Size([360])\n\n\n\n\nShow the code\nplt.scatter(wd_res['wds'], med_weights_t, s=5, alpha=0.4)\n\n# Set axis labels\nplt.xlabel('Weight Decay Value')\nplt.ylabel('Median Weight Value')\n\n# Display the plot\nplt.show();\n\n\n\n\n\n\n\n\n\nIt’s a bit difficult to see trends from this chart, but the but median weights generally seem to be decreasing as weight decay value increaeses–although there is a considerable amount of variability.\n\nmean_weights_t = weights_t.mean(dim=-1)\nmean_weights_t.shape\n\ntorch.Size([360])\n\n\n\n\nShow the code\nplt.scatter(wd_res['wds'], mean_weights_t, s=5, alpha=0.4)\n\n# Set axis labels\nplt.xlabel('Weight Decay Value')\nplt.ylabel('Mean Weight Value')\n\n# Display the plot\nplt.show();\n\n\n\n\n\n\n\n\n\nThe mean weights follow a similar trend—generally narrowing in range as weight decay value increases. I’ll look at the absolute value of mean and median and see if any trends appear:\n\nabs_med_weights_t = weights_t.abs().median(dim=-1)[0]\nabs_med_weights_t.shape\n\ntorch.Size([360])\n\n\n\n\nShow the code\nplt.scatter(wd_res['wds'], abs_med_weights_t, s=5, alpha=0.4)\n\n# Set axis labels\nplt.xlabel('Weight Decay Value')\nplt.ylabel('Median Absolute Weight Value')\n\n# Display the plot\nplt.show();\n\n\n\n\n\n\n\n\n\nThat’s much clearer! the median absolute value of the weights exponentially decreases as the weight decay value increases. I’ll look at the mean absolute value we well:\n\nabs_mean_weights_t = weights_t.abs().mean(dim=-1)\nabs_mean_weights_t.shape\n\ntorch.Size([360])\n\n\n\n\nShow the code\nplt.scatter(wd_res['wds'], abs_mean_weights_t, s=5, alpha=0.4)\n\n# Set axis labels\nplt.xlabel('Weight Decay Value')\nplt.ylabel('Mean Absolute Weight Value')\n\n# Display the plot\nplt.show();\n\n\n\n\n\n\n\n\n\nYup! The mean absolute value of the weights exponentially decreases as well (as weight decay value goes to 1.0)."
  },
  {
    "objectID": "posts/2024-06-03-wd/index.html#final-thoughts",
    "href": "posts/2024-06-03-wd/index.html#final-thoughts",
    "title": "Training Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAs always, I love experimenting with (relatively) simple concepts and observing how they affect training performance and results. Recapping the trends I observed:\n\nMinimum training loss increases as weight decay increases.\nMinimum validation loss decreases until a point (for this dataset and model, until a weight decay value of ~0.2) and then increases as weight decay increases.\nThe mean and median absolute values of the final weights decreases (exponentially) as weight decay increases.\n\nI also enjoyed creating animated GIFs as a visual tool to illustrate the distributions and trends of weights and losses as weight decay increases. The resulting GIFs could be improved by tweaking frame rate and other parameters, but I’m happy with the overall trends they convey, which match the static plots I created.\nAs always, I hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "",
    "text": "An example image from the image dataset used in this lesson. The image has a train going over a bridge with skyscrapers in the background.\nThis chapter introduced two more classification models:\nIn this chapter the authors walk us through in the chapter is the PASCAL dataset.\nHere’s my video walkthrough for this notebook:"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "The Data",
    "text": "The Data\nfastai comes with datasets available for download using the URLs object. We will use the PASCAL_2007 dataset.\n\n# download the dataset\npath = untar_data(URLs.PASCAL_2007)\n\n#read label CSV into a DataFrame\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\n  \n    \n      \n      fname\n      labels\n      is_valid\n    \n  \n  \n    \n      0\n      000005.jpg\n      chair\n      True\n    \n    \n      1\n      000007.jpg\n      car\n      True\n    \n    \n      2\n      000009.jpg\n      horse person\n      True\n    \n    \n      3\n      000012.jpg\n      car\n      False\n    \n    \n      4\n      000016.jpg\n      bicycle\n      True\n    \n  \n\n\n\n\nNext, they have us go through some pandas fundamentals for accessing data in a DataFrame\n\n# accessing all rows and the 0th column\ndf.iloc[:,0]\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# accessing all columns for the 0th row\ndf.iloc[0,:]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# trailing :s are not needed\ndf.iloc[0]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# accessing a column by its name\ndf['fname']\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# creating a new DataFrame and performing operations on it\ndf1 = pd.DataFrame()\n\n# adding a new column\ndf1['a'] = [1,2,3,4]\ndf1\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n    \n      3\n      4\n    \n  \n\n\n\n\n\n# adding a new column\ndf1['b'] = [10, 20, 30, 40]\n\n# adding two columns\ndf1['a'] + df1['b']\n\n0    11\n1    22\n2    33\n3    44\ndtype: int64"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Constructing a DataBlock",
    "text": "Constructing a DataBlock\nA DataBlock can be used to create Datasets from which DataLoaders can be created to use during training. A DataBlock is an object which contains the data and has helper functions which can access and transform the data.\nThey start by creating an empty DataBlock\n\ndblock = DataBlock()\ndblock\n\n<fastai.data.block.DataBlock at 0x7efe5c559d90>\n\n\nThe DataFrame with filenames and labels can be fed to the DataBlock to create a Datasets object, which is > an iterator that contains a training Dataset and validation Dataset\nEach dataset is\n\na collection that returns a tuple of your independent and dependent variable for a single item\n\nA Dataet created from an empty DataBlock (meaning, a DataBlock with no helper functions to tell it how the data is structured and accessed) will contain a tuple for each row of the DataFrame, where both values of the tuple are the same row.\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object, fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object)\n\n\nWhat we want is for the DataBlock to create Datasets of (independent, dependent) values. In this case, the independent variable is the image and the dependent variable is a list of labels.\nIn order to parse the DataFrame rows, we need to provide two helper functions to the DataBlock: get_x and get_y. In ordert to convert them to the appropriate objects that will be used in training, we need to provide two more arguments: ImageBlock and MultiCategoryBlock. In order for the DataBlock to correctly split the data into training and validation datasets, we need to define a splitter function and pass it as an argument as well.\nget_x will access the filename from each row of the DataFrame and convert it to a file path.\nget_y will access the labels from each row and split them into a list.\nImageBlock will take the file path and convert it to a PILImage object.\nMultiCategoryBlock will convert the list of labels to a one-hot encoded tensor using the Dataset’s vocab.\nsplitter will explicitly choose for the validation set the rows where is_valid is True.\nRandomResizedCrop will ensure that each image is the same size, which is a requirement for creating a tensor with all images.\n\ndef get_x(row): return path/'train'/row['fname']\ndef get_y(row): return row['labels'].split(' ')\ndef splitter(df):\n  train = df.index[~df['is_valid']].tolist()\n  valid = df.index[df['is_valid']].tolist()\n  return train, valid\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    splitter=splitter,\n    get_x=get_x,\n    get_y=get_y,\n    item_tfms = RandomResizedCrop(128, min_scale=0.35))\n\ndsets = dblock.datasets(df)\ndls = dblock.dataloaders(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0.]))\n\n\nThe Datasets vocab is a list of alphabetically ordered unique labels:\n\ndsets.train.vocab\n\n['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\n\nLet me breakdown the tuple returned by dsets.train[0]. The first value is a PILImageobject which can be viewed by calling its show() method:\n\ndsets.train[0][0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7efe5c3764d0>\n\n\n\n\n\nThe second value is a one-hot encoded list, where 1s are in the location of the labels in the corresponding vocab list. I’ll use the torch.where method to access the indices where there are 1s:\n\ntorch.where(dsets.train[0][1]==1)\n\n(TensorMultiCategory([6]),)\n\n\n\ndsets.train.vocab[torch.where(dsets.train[0][1]==1)[0]]\n\n(#1) ['car']\n\n\n\ndls.show_batch(nrows=1, ncols=3)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 4: Two-Digit MNIST Classifier",
    "text": "Chapter 4: Two-Digit MNIST Classifier\nI’ll first review the loss function used in the single-label classification models created in Chapters 4 and 5 before reviewing Binary Cross Entropy Loss introduced in this chapter.\nIn this chapter, we built a image classifier which would predict if an input image was an of the digit 3 or the digit 7.\nThe target (or expected outcome) is a list of 0s (for 7) and 1s (for 3). If we gave a batch of images of a 3, a 7 and a 3, the target would be [1, 0, 1].\nSuppose the model predicted the following values: [0.9, 0.4, 0.2] where each value represented the probability or confidence it had that each image was a 3.\nLoss represents the positive difference between the target and the prediction: - 1 - prediction when target == 1 - prediction when target == 0\nFor the first image, the model had 90% confidence it was a 3, and it was indeed a 3. The loss is 1 - 0.9 = 0.1.\nFor the second second image, the model had a 40% confidence it was a three, and the image was of a 7. The loss is 0.4.\nFor the last image, the model had a 20% confidence it was a 3, and the image was a 3. The loss is 1 - 0.2 = 0.8.\nThe average of these three losses is 1.3/3 or 0.433.\nThe following cell illustrates this with code:\n\ndef mnist_loss(predictions, targets):\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\ntargets = tensor([1,0,1])\npredictions = tensor([0.9, 0.4, 0.2])\nmnist_loss(predictions=predictions, targets=targets)\n\ntensor(0.4333)\n\n\nThe assumption that this loss function makes is that the predictions are always between 0 and 1. That may not always be true! In order to make this assumption explicit, we take the sigmoid of the prediction before calculating the loss. The sigmoid function outputs a value between 0 and 1 for any input value.\n\ntensor([0.4,-100,200]).sigmoid()\n\ntensor([0.5987, 0.0000, 1.0000])\n\n\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 5: 37 Breed Pet Classifier",
    "text": "Chapter 5: 37 Breed Pet Classifier\nIn this chapter, we train an image classifier that when given an input image, predicts which of the 37 pet breeds the image shows. The loss function needs to handle 37 activations for each image. In order to ensure the sum of those activations equals 1.0—so that the highest activation represents the model’s highest confidence—the softmax function is used. In order to increase the separation between probabilities, the softmax function’s output is passed through the logarithm function, and the negative value is taken. The combination of softmax and (negative) logarithm is called cross entropy loss.\nSuppose we had 4 images in a batch. The model would output activations something like this:\n\n# create a pseudo-random 4 x 37 tensor \n# with values from -2 to 2\nacts = (-2 - 2) * torch.rand(4, 37) + 2\nacts\n\ntensor([[-1.9994e+00,  7.0629e-01, -1.8230e+00,  8.6118e-02,  8.8579e-01,\n         -9.7763e-01,  9.7619e-01,  5.4613e-01,  9.2020e-01,  8.2653e-01,\n         -1.3831e+00,  1.2236e+00, -4.2582e-01,  1.1371e+00,  1.2409e+00,\n          1.4403e-02, -9.2988e-01, -1.1939e+00, -9.9743e-01, -1.9572e+00,\n         -6.8404e-02,  6.2455e-01,  8.6748e-01, -1.4574e+00, -1.4451e+00,\n          1.1349e-01,  1.7424e+00,  6.5414e-02, -1.2517e+00, -1.9933e+00,\n         -1.5570e+00,  1.3880e+00,  1.5099e+00,  6.2576e-01, -1.4279e-03,\n          1.7448e+00,  1.9862e+00],\n        [ 4.5219e-02,  4.6843e-01, -1.1474e+00, -1.8876e+00, -5.7879e-01,\n          6.9787e-01, -7.2457e-02, -1.7235e+00, -9.9028e-01,  1.2248e+00,\n          6.4889e-01,  5.0363e-01,  1.8472e-01, -1.0468e+00, -1.0113e+00,\n         -1.0628e+00,  1.9783e+00, -1.8394e+00, -8.0410e-02, -5.9383e-01,\n         -1.6868e+00, -2.6366e-01, -8.3354e-01,  6.8552e-01, -8.6600e-02,\n          1.6034e+00,  7.3355e-01,  1.3205e+00,  1.4004e+00, -5.2889e-01,\n          5.6740e-01, -9.6958e-01, -1.4997e+00,  4.6890e-01, -1.7328e+00,\n          1.0302e+00, -5.7672e-01],\n        [-2.0183e-01,  9.5745e-01, -6.7022e-01, -1.4942e+00, -1.7716e+00,\n         -1.5369e+00,  5.3614e-01,  2.1942e-01, -4.8692e-01, -1.0483e+00,\n         -1.3250e+00, -2.7229e-01,  7.0113e-01,  6.7435e-01,  1.3605e+00,\n         -5.5024e-01, -8.2829e-01, -3.0993e-01, -2.9132e-02, -6.5741e-01,\n         -1.8838e+00, -1.5611e+00,  1.3386e+00, -9.3677e-01,  9.4050e-01,\n          1.6461e+00, -1.7923e+00, -1.2952e+00, -1.4606e+00,  1.9617e+00,\n          1.8974e+00, -3.5640e-01, -5.1258e-01,  1.3049e+00,  9.6022e-01,\n          1.8340e+00, -1.6090e+00],\n        [ 3.3658e-01, -1.9117e+00,  1.3840e+00,  1.4359e+00,  3.0289e-01,\n         -1.9664e+00, -1.8941e+00,  4.2836e-02,  1.6804e+00,  1.5752e+00,\n         -4.4672e-01,  1.0409e+00, -2.8504e-01, -1.3567e+00,  3.1620e-01,\n         -1.9444e+00,  1.5615e+00, -5.0563e-01, -1.8748e+00, -1.1123e+00,\n         -1.9222e+00,  1.3545e+00, -2.9159e-01, -4.6669e-01,  1.2639e+00,\n         -1.4171e+00, -2.7517e-01, -1.2380e+00, -1.5908e+00,  1.4929e+00,\n          1.0642e+00, -3.4285e-01, -1.8219e+00,  1.6329e+00, -1.2953e+00,\n          1.7803e+00,  3.6970e-01]])\n\n\nPassing these through softmax will normalize them from 0 to 1:\n\nsm_acts = acts.softmax(dim=1)\nsm_acts[0], sm_acts[0].sum()\n\n(tensor([0.0020, 0.0302, 0.0024, 0.0162, 0.0361, 0.0056, 0.0396, 0.0257, 0.0374,\n         0.0341, 0.0037, 0.0507, 0.0097, 0.0465, 0.0516, 0.0151, 0.0059, 0.0045,\n         0.0055, 0.0021, 0.0139, 0.0278, 0.0355, 0.0035, 0.0035, 0.0167, 0.0851,\n         0.0159, 0.0043, 0.0020, 0.0031, 0.0597, 0.0675, 0.0279, 0.0149, 0.0853,\n         0.1086]), tensor(1.0000))\n\n\nTaking the negative log of this tensor will give us the final loss:\n\nnll_loss = -1. * torch.log(sm_acts)\nnll_loss\n\ntensor([[6.2054, 3.4997, 6.0290, 4.1199, 3.3202, 5.1836, 3.2298, 3.6599, 3.2858,\n         3.3795, 5.5891, 2.9825, 4.6318, 3.0690, 2.9651, 4.1916, 5.1359, 5.3999,\n         5.2035, 6.1632, 4.2744, 3.5815, 3.3385, 5.6635, 5.6511, 4.0925, 2.4636,\n         4.1406, 5.4577, 6.1994, 5.7630, 2.8180, 2.6961, 3.5803, 4.2074, 2.4612,\n         2.2198],\n        [3.9156, 3.4924, 5.1082, 5.8484, 4.5396, 3.2629, 4.0333, 5.6843, 4.9511,\n         2.7360, 3.3119, 3.4572, 3.7761, 5.0076, 4.9721, 5.0235, 1.9825, 5.8002,\n         4.0412, 4.5546, 5.6476, 4.2245, 4.7943, 3.2753, 4.0474, 2.3574, 3.2273,\n         2.6403, 2.5604, 4.4897, 3.3934, 4.9304, 5.4605, 3.4919, 5.6936, 2.9306,\n         4.5375],\n        [4.3197, 3.1604, 4.7881, 5.6121, 5.8895, 5.6548, 3.5817, 3.8985, 4.6048,\n         5.1662, 5.4429, 4.3902, 3.4167, 3.4435, 2.7574, 4.6681, 4.9462, 4.4278,\n         4.1470, 4.7753, 6.0016, 5.6790, 2.7793, 5.0546, 3.1774, 2.4718, 5.9102,\n         5.4131, 5.5785, 2.1562, 2.2205, 4.4743, 4.6305, 2.8130, 3.1577, 2.2839,\n         5.7269],\n        [3.8515, 6.0998, 2.8041, 2.7522, 3.8852, 6.1545, 6.0822, 4.1453, 2.5077,\n         2.6129, 4.6348, 3.1472, 4.4732, 5.5448, 3.8719, 6.1325, 2.6266, 4.6937,\n         6.0629, 5.3004, 6.1103, 2.8336, 4.4797, 4.6548, 2.9243, 5.6052, 4.4633,\n         5.4261, 5.7790, 2.6952, 3.1239, 4.5310, 6.0101, 2.5552, 5.4834, 2.4078,\n         3.8184]])\n\n\nSuppose the target for each image was given by the following tensor, where the target is an integer from 0 to 36 representing one of the pet breeds:\n\ntargs = tensor([3, 0, 34, 10])\nidx = range(4)\nnll_loss[idx, targs]\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\ndef cross_entropy(acts, targs):\n  idx = range(len(targs))\n  sm_acts = acts.softmax(dim=1)\n  nll_loss = -1. * torch.log(sm_acts)\n  return nll_loss[idx, targs].mean()\n\nI compare this with the built-in F.cross_entropy and nn.CrossEntropyLoss functions:\n\nF.cross_entropy(acts, targs,reduction='none')\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targs)\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\nNote that the nn version of the loss function returns an instantiation of that function which then must be called with the activations and targets as its inputs.\n\ntype(nn.CrossEntropyLoss(reduction='none'))\n\ntorch.nn.modules.loss.CrossEntropyLoss"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Binary Cross Entropy Loss",
    "text": "Binary Cross Entropy Loss\nThe authors begin the discussion of explaining the multi-label classification model loss function by observing the activations from the trained model. I’ll do the same—I love that approach since it grounds the concepts involved in the construction of loss function in the actual model outputs.\n\nlearn = cnn_learner(dls, resnet18)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\nx, y = dls.train.one_batch()\nif torch.cuda.is_available():\n    learn.model.cuda()\nactivs = learn.model(x)\nactivs.shape\n\ntorch.Size([64, 20])\n\n\nEach batch has 64 images and each of those images has 20 activations, one for each label in .vocab. Currently, they are not restricted to values between 0 and 1.\nNote: the activations tensor has to first be placed on the cpu and then detached from the graph (which is used to track and calculate gradients of the weights with respect to the loss function) before it can be converted to a numpy array used for the plot.\n\nys = activs[0].cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nPassing them through a sigmoid function achieves that:\n\nys = activs[0].sigmoid().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe negative log of the activations is taken in order to push the differences between loss values. For vocab where the target is 1, -log(inputs) is calculated. For vocab where the target is 0, -log(1-inputs) is calculated. This seems counterintuitive at first, but let’s take a look at the plot of these functions:\n\nys = -activs[0].sigmoid().log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe sigmoid activations that were very close to 0 (Vocab Index = 0, 2, 5, and 16) are now much larger than those that were very close to 1 (Vocab Index = 6, 14, and 18). Since the target is 1, this correctly assigns a larger loss to the inaccurate predictions and the smaller loss to the accurate ones. We can say the same (but opposite) for -log(1-inputs), which is used when the target is 0.:\n\nys = -(1- activs[0].sigmoid()).log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nFinally, the mean of all image loss values is taken for the batch. The Binary Cross Entropy Function look likes this:\n\ndef binary_cross_entropy(inputs, targets):\n  inputs = inputs.sigmoid()\n  return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\nThe inputs (the activations for each vocab value)) is the first value and the targets of each image are the second value of the dls.train.one_batch() tuple.\n\nbinary_cross_entropy(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\nI will compare this with the built-in function F.binary_cross_entropy_with_logits and function class nn.BCEWithLogitsLoss to make sure I receive the same result.\n\nF.binary_cross_entropy_with_logits(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\n\nnn.BCEWithLogitsLoss()(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Mult-Label Classification Accuracy",
    "text": "Mult-Label Classification Accuracy\nFor single-label classification, the accuracy function compared whether the index of the highest activation matched the index of the target vocab. A single index for a single label.\n\ndef accuracy(inputs, targets, axis=-1):\n  predictions = inputs.argmax(dim=axis)\n  return (predictions==targets).float().mean()\n\nFor multi-label classification, each image can have more than one correct corresponding vocab index and the corresponding activations may not be the maximum of the inputs tensor. So instead of using the maximum, a threshold is used to identify predictions. If the activation is above that threshold, it’s considered to be a prediction.\n\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n  if sigmoid: inp = inp.sigmoid()\n  return ((inp > thresh)==targ.bool()).float().mean()\n\ntarg is a one-hot encoded Tensor, so 1s are converted to True and 0s are converted to False using the .bool method."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Training the Model",
    "text": "Training the Model\nAt last! I can now train the model, setting a different accuracy threshold as needed using the built-in partial function.\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.942256\n      0.698276\n      0.239323\n      00:29\n    \n    \n      1\n      0.821279\n      0.566598\n      0.281633\n      00:28\n    \n    \n      2\n      0.602543\n      0.208145\n      0.805498\n      00:28\n    \n    \n      3\n      0.359614\n      0.125162\n      0.939801\n      00:28\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.133149\n      0.112483\n      0.947072\n      00:29\n    \n    \n      1\n      0.115643\n      0.105032\n      0.953028\n      00:29\n    \n    \n      2\n      0.096643\n      0.103564\n      0.952769\n      00:29\n    \n  \n\n\n\nIn about three and a half minutes, this model was able to achieve more than 95% accuracy. I’ll look at its predictions on the validation images:\n\nlearn.show_results(max_n=18)\n\n\n\n\n\n\n\nVarying the threshold will vary the accuracy of the model. The metrics of the learner can be changed after training, and calling the validate method will recalculate the accuracy:\n\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.930816650390625]\n\n\nA threshold of 0.1 decreases the accuracy of the model, as does a threshold of 0.99. A 0.1 threshold includes labels for which the model was not confident, and a 0.99 threshold exclused labels for which the model was not very confident. I can calculate and plot the accuracy for a range of thresholds, as they did in the book:\n\npreds, targs = learn.get_preds()\nxs = torch.linspace(0.05, 0.95, 29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\n\n\n\n\nbest_threshold = xs[np.argmax(accs)]\nbest_threshold\n\ntensor(0.4679)\n\n\n\nlearn.metrics = partial(accuracy_multi, thresh=best_threshold)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.9636053442955017]\n\n\nThe highest accuracy (96.36%) is achieved when the threshold is 0.4679."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Regression",
    "text": "Regression\nThe authors provide some context here which, while I can appreciate, judge I won’t fully understand until I experience the next 5 or 6 chapters.\n\nA model is defined by its independent and dependent variables, along with its loss function. The means that there’s really a far wider array of models than just the simple domain-based split\n\nThe “domain-based split” is a reference to the distinction between computer vision, NLP and other different types of problems.\nTo illustrate their point, they have us work through an image regression problem with much of the same process (and model) as an image classification problem.\n\n# download data\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\n# helper functions to retrieve images\n# and to retrieve text files\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\n\n\n# check that `img2pose` converts file name correctly\nimg_files[0], img2pose(img_files[0])\n\n(Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_rgb.jpg'),\n Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_pose.txt'))\n\n\n\n# check image size\nim = PILImage.create(img_files[0])\nim.shape\n\n(480, 640)\n\n\n\n# view the image\nim.to_thumb(160)\n\n\n\n\n\n# helper function to extract coordinates\n# of the subject's center of head\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n  ctr = np.genfromtxt(img2pose(f), skip_header=3)\n  c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n  c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n  return tensor([c1,c2])\n\n\n# check coordinates of the first file\nget_ctr(img_files[0])\n\ntensor([444.7946, 261.7657])\n\n\n\n# create the DataBlock\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)]\n)\n\n\n# confirm that the data looks OK\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n# view tensors\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\n\nEach batch has 64 images. Each image has 3 channels (rgb) and is 240x320 pixels in size. Each image has 1 pair of coordinates.\n\n# view a single coordinate pair\nyb[0]\n\nTensorPoint([[0.0170, 0.3403]], device='cuda:0')\n\n\n\n# create Learner object\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\nThe y_range argument shifts the final layer’s sigmoid output to a coordinate between -1 and 1. The sigmoid function is transformed using the following function.\n\ndef plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = torch.linspace(min,max)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)\n\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\nplot_function(partial(sigmoid_range, lo=-1, hi=1), min=-4, max=4)\n\n\n\n\n\n# confirm loss function\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nfastai has chosen MSE as the loss function, which is appropriate for a regression problem.\n\n# pick a learning rate\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.033113110810518265)\n\n\n\n\n\n\n# use lr = 2e-2\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.047852\n      0.011552\n      01:55\n    \n    \n      1\n      0.007220\n      0.002150\n      01:56\n    \n    \n      2\n      0.003190\n      0.001313\n      01:56\n    \n    \n      3\n      0.002376\n      0.000295\n      01:56\n    \n    \n      4\n      0.001650\n      0.000106\n      01:54\n    \n  \n\n\n\nA loss of 0.000106 is an accuracy of:\n\nmath.sqrt(0.000106)\n\n0.010295630140987\n\n\nThe conclusion to this (what has felt like a marathon of a) chapter is profound:\n\nIn problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different number of outputs. The loss function is the one thing that changes, which is why it’s important to double-check that you are using the right loss function for your problem…make sure you think hard about your loss function, and remember that you most probably want the following:\n\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "posts/2024-07-07-normlayers/index.html",
    "href": "posts/2024-07-07-normlayers/index.html",
    "title": "Comparing CNN Performance by Varying Activation Normalization Layers",
    "section": "",
    "text": "In this notebook I’ll work through the following “Further Research” prompt given at the end of Chapter 13 (Convolutional Neural Networks) of the fastai textbook:\n\nOther normalization layers are available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed and how they differ from batch normalization.\n\nI’ll use the following 5 normalization layers to train separate models and compare model performance and activations:\n\nBatchNorm2d\nInstanceNorm\nGroupNorm\nLocal Response Normalization\nLayerNorm"
  },
  {
    "objectID": "posts/2024-07-07-normlayers/index.html#visualizing-activation-normalization-layer-outputs",
    "href": "posts/2024-07-07-normlayers/index.html#visualizing-activation-normalization-layer-outputs",
    "title": "Comparing CNN Performance by Varying Activation Normalization Layers",
    "section": "Visualizing Activation Normalization Layer Outputs",
    "text": "Visualizing Activation Normalization Layer Outputs\nBefore I get into the trainings, I’ll visualize the outputs of each normalization layer, given an input batch of MNIST images.\n\nfrom fastai.vision.all import *\nfrom fastai.callback.hook import *\nmatplotlib.rc('image', cmap='Greys')\n\n\npath = untar_data(URLs.MNIST)\npath.ls()\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\n(#2) [Path('/root/.fastai/data/mnist_png/testing'),Path('/root/.fastai/data/mnist_png/training')]\n\n\n\n# create a function to change dls params\ndef get_dls(bs=64):\n  return DataBlock(\n      blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n      get_items=get_image_files,\n      splitter=GrandparentSplitter('training', 'testing'),\n      get_y=parent_label,\n      batch_tfms=Normalize()\n  ).dataloaders(path, bs=bs)\n\n\ndls = get_dls(512)\ndls.show_batch(max_n=9, figsize=(4,4))\n\n\n\n\n\nxb, yb = dls.one_batch()\n\n\nBatchNorm2d\nFirst up is BatchNorm2d which is the layer we use in Chapter 13 of the fastai textbook when training a CNN.\nHere’s an image without any normalization applied (left) and the same image after BatchNorm2d is applied to the entire batch on the right—it looks pretty much the same! :\n\ndef plot_ims(a, b):\n  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n  im1 = ax1.imshow(a.squeeze());\n  ax1.set_title('Original')\n  im2 = ax2.imshow(b.detach().numpy().squeeze());\n  ax2.set_title('After Normalization Layer Applied')\n\n\nplot_ims(xb[0], nn.BatchNorm2d(1)(xb)[0])\n\n\n\n\nHere is the mean absolute difference between the 784 pixels in both images (pre- and post-normalization)—in other words, not that different!\n\ndef mad(a,b): return (a-b).abs().mean()\n\n\nmad(xb[0], nn.BatchNorm2d(1)(xb)[0])\n\nTensorImageBW(0.0040, grad_fn=<AliasBackward0>)\n\n\nIt wasn’t until I asked Claude for an explanation that I understood that this normalization occurs across the entire batch (I know—it’s right there in the name!):\nPrompt:\n\nwill the normalization be visible per image?\n\nResponse (emphasis mine):\n\nThe normalization in BatchNorm2d is not typically visible per individual image. Here’s why:\n\nChannel-wise operation: BatchNorm2d normalizes across the entire batch for each channel.\nBatch statistics: It uses mean and variance computed over all images in the batch, not per image.\nShared parameters: The learnable scale (gamma) and shift (beta) parameters are applied equally to all images.\nSubtle effects: The normalization often results in subtle changes that may not be visually apparent.\nPurpose: It’s designed to stabilize internal representations, not to visibly alter individual images.\nInference behavior: During inference, it uses running statistics, further reducing per-image variation.\n\nTo see BatchNorm2d’s effect, you’d typically need to analyze the statistical properties of the activations across the entire batch or network, rather than looking at individual images.\n\nHere’s a couple other ways to realize this behavior—the mean pixel value per image in the batch is relatively unchanged after BatchNorm2d is applied:\n\ndef plot_hists(a, b, label_a, label_b, xlabel):\n  plt.figure(figsize=(10, 6))\n  plt.hist(a, alpha=0.7, color='blue', label=label_a);\n  plt.hist(b, alpha=0.7, color='red', label=label_b);\n  plt.legend()\n  plt.xlabel(xlabel)\n  plt.ylabel('Frequency')\n  plt.show()\n\n\nplot_hists(\n    a=xb.mean(0).flatten(),\n    b=nn.BatchNorm2d(1)(xb).mean(0).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After BatchNorm2d',\n    xlabel='Mean Pixel Value in Image')\n\n\n\n\nHere is the distribution of pixel values across the entire batch—again, relatively unchanged after BatchNorm2d is applied:\n\nplot_hists(\n    a=xb.flatten(),\n    b=nn.BatchNorm2d(1)(xb).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After BatchNorm2d',\n    xlabel='Pixel Value in Image')\n\n\n\n\nAfter BatchNorm2d is applied, the average pixel value in a single image is relatively unchanged but the average pixel value across a batch is effectively 0.\n\ndef get_means(a, b):\n  return a.mean(), b.mean()\n\n\nget_means(xb[0], nn.BatchNorm2d(1)(xb)[0]) # mean of single image\n\n(TensorImageBW(0.0668), TensorImageBW(0.0628, grad_fn=<AliasBackward0>))\n\n\n\nget_means(xb, nn.BatchNorm2d(1)(xb)) # mean of entire batch\n\n(TensorImageBW(0.0037), TensorImageBW(2.1858e-08, grad_fn=<AliasBackward0>))\n\n\nInteresting to note that in this case, since the standard deviation of the batch is close to 1, it doesn’t change much after BatchNorm2d is applied:\n\ndef get_stds(a, b):\n  return a.std(), b.std()\n\n\nget_stds(xb, nn.BatchNorm2d(1)(xb)) # std of entire batch\n\n(TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=<AliasBackward0>))\n\n\nLastly, I’ll look at the means and standard deviations of 100 batches after BatchNorm2d is applied:\n\ndef plot_means_stds(norm):\n  means = []\n  stds = []\n\n  for _ in range(100):\n    xb, _ = dls.one_batch()\n    means.append(norm(xb).mean().item())\n    stds.append(norm(xb).std().item())\n\n  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n\n  ax1.hist(means);\n  ax1.set_title('Means')\n  ax1.set_xlabel('Value')\n  ax1.set_ylabel('Frequency')\n\n  # Plot second histogram\n  ax2.hist(stds);\n  ax2.set_title('Standard Deviations')\n  ax2.set_xlabel('Value')\n  ax2.set_ylabel('Frequency')\n\n  plt.tight_layout()\n  plt.show()\n\nAs expected, the mean of the batch is effectively 0 and standard deviation is 1 (note that the x-axis for means is 1e-8 and for standard deviations is 1e-7 + 9.99996e-1):\n\nplot_means_stds(nn.BatchNorm2d(1))\n\n\n\n\n\n\nInstanceNorm2d\nReading the PyTorch documentation, it seems like InstanceNorm2d is similar to BatchNorm2d (the formulas look the same). They note that InstanceNorm2d is similar to LayerNorm which I’ll look at later on.\nI’ll start by instantiating the normalization layer:\n\ninorm = nn.InstanceNorm2d(1)\n\nThe individual images effectively look the same before and after InstanceNorm2d is applied:\n\nplot_ims(xb[0], inorm(xb)[0])\n\n\n\n\nThe mean absolute difference between all 784 pixel values in each image is considerably larger than the difference using BatchNorm2d:\n\nmad(xb[0], inorm(xb)[0])\n\nTensorImageBW(0.0668)\n\n\nThe differences between the distributions of mean pixel value in each image before/after InstanceNorm2d is applied seems larger than the difference in distributions before/after BatchNorm2d is applied.\n\nplot_hists(\n    a=xb.mean(0).flatten(),\n    b=inorm(xb).mean(0).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After InstanceNorm2d',\n    xlabel='Mean Pixel Value in Image')\n\n\n\n\nA considerable number of pixels in the batch, after InstanceNorm2d is applied, have more negative values than the original batch.\n\nplot_hists(\n    a=xb.flatten(),\n    b=inorm(xb).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After InstanceNorm2d',\n    xlabel='Pixel Value in Image')\n\n\n\n\nThe mean of a single image is lower after InstanceNorm2d is applied:\n\nget_means(xb[0], inorm(xb)[0]) # mean of single image\n\n(TensorImageBW(0.0668), TensorImageBW(7.9067e-09))\n\n\nThe mean of the batch is lower as well:\n\nget_means(xb, inorm(xb)) # mean of entire batch\n\n(TensorImageBW(0.0037), TensorImageBW(-8.3629e-10))\n\n\nThe standard deviation of a batch is 1:\n\nget_stds(xb, inorm(xb)) # std of entire batch\n\n(TensorImageBW(1.0045), TensorImageBW(1.0000))\n\n\nSimilar to BatchNorm2d, the mean and standard deviation for a batch is about 0 and 1, respectively, for InstanceNorm2d.\n\nplot_means_stds(inorm)\n\n\n\n\n\n\nGroupNorm\nThis normalization layer normalizes the data across the specified num_groups, so it’s like BatchNorm2d but for groups in the batch.\nThe documentation states that:\n\nnum_channels must be divisible by num_groups\n\nSo in the case of my MNIST black-and-white single-channel images, I would expect GroupNorm results to be the same as BatchNorm2d.\n\ngnorm = nn.GroupNorm(1, 1)\n\nThe individual image remains unchanged (at least by inspection):\n\nplot_ims(xb[0], gnorm(xb)[0])\n\n\n\n\nThe mean average difference between pre- and post-GroupNorm image is larger than BatchNorm2d:\n\nmad(xb[0], gnorm(xb)[0])\n\nTensorImageBW(0.0668, grad_fn=<AliasBackward0>)\n\n\nThere’s visually more variance between the mean image distribution before/after GroupNorm than BatchNorm2d:\n\nplot_hists(\n    a=xb.mean(0).flatten(),\n    b=gnorm(xb).mean(0).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After GroupNorm',\n    xlabel='Mean Pixel Value in Image')\n\n\n\n\nThere is a considerable difference between pre- and post-GroupNorm distributions (by visual inspection):\n\nplot_hists(\n    a=xb.flatten(),\n    b=gnorm(xb).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After GroupNorm',\n    xlabel='Pixel Value in Image')\n\n\n\n\nThe mean pixel value for the given image is effectively 0 after GroupNorm is applied:\n\nget_means(xb[0], gnorm(xb)[0]) # mean of single image\n\n(TensorImageBW(0.0668), TensorImageBW(2.3112e-08, grad_fn=<AliasBackward0>))\n\n\nAs is the mean of a given batch:\n\nget_means(xb, gnorm(xb)) # mean of entire batch\n\n(TensorImageBW(0.0037), TensorImageBW(1.9007e-10, grad_fn=<AliasBackward0>))\n\n\nThe standard deviation of the single batch is 1:\n\nget_stds(xb, gnorm(xb)) # std of entire batch\n\n(TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=<AliasBackward0>))\n\n\nWhile the mean and standard deviation of the 100 batches is close to 0 and 1 respectively, the distributions of mean for GroupNorm is centered around 1e-9 while BatchNorm2d and InstanceNorm were centered around 0.\n\nplot_means_stds(gnorm)\n\n\n\n\n\n\nLocalResponseNorm\nThe PyTorch docs describe this normalization layer as follows:\n\nApplies local response normalization over an input signal.\nThe input signal is composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.\n\nGiven that I only have one channel (BW images) I expect the normalization effect to be similar to BatchNorm2d. I’ll use the default settings first:\n\nlrnorm = nn.LocalResponseNorm(1)\n\nNo noticeable difference in an individual image after applying LocalResponseNorm:\n\nplot_ims(xb[0], lrnorm(xb)[0])\n\n\n\n\nThe mean absolute difference is the lowest so far across all normalization layers:\n\nmad(xb[0], lrnorm(xb)[0])\n\nTensorImageBW(0.0002)\n\n\nThe distributions of mean pixels are virtually indistinguishable:\n\nplot_hists(\n    a=xb.mean(0).flatten(),\n    b=lrnorm(xb).mean(0).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After LocalResponseNorm',\n    xlabel='Mean Pixel Value in Image')\n\n\n\n\nAs are the distributions of pixel values:\n\nplot_hists(\n    a=xb.flatten(),\n    b=lrnorm(xb).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After LocalResponseNorm',\n    xlabel='Pixel Value in Image')\n\n\n\n\nFollowing suit, the mean of a single image doesn’t change much:\n\nget_means(xb[0], lrnorm(xb)[0]) # mean of single image\n\n(TensorImageBW(0.0668), TensorImageBW(0.0666))\n\n\nNeither does the mean of a batch:\n\nget_means(xb, lrnorm(xb)) # mean of entire batch\n\n(TensorImageBW(0.0037), TensorImageBW(0.0035))\n\n\nNor does the standard deviation of a batch:\n\nget_stds(xb, lrnorm(xb)) # std of entire batch\n\n(TensorImageBW(1.0045), TensorImageBW(1.0041))\n\n\nThe mean value of the 100 batches (with LocalResponseNorm applied) varies significantly—from -0.005 to +0.015. The standard deviation is more tightly distributed around 1.0. Overall, the default values of LocalResponseNorm don’t show signs of significantly changing the distribution of values for the images across the batch.\n\nplot_means_stds(lrnorm)\n\n\n\n\nWhat if I change some the default values? I’ll vary the values of the following parameters until I see a significant difference either in individual images or across batches: alpha, beta and k.\nChanging the alpha parameter significantly changes the individual image as well as the mean and standard deviation across the batch:\n\nlrnorm = nn.LocalResponseNorm(size=1, alpha=1)\n\n\nget_means(xb, lrnorm(xb)), \\\nget_stds(xb, lrnorm(xb))\n\n((TensorImageBW(0.0037), TensorImageBW(-0.2167)),\n (TensorImageBW(1.0045), TensorImageBW(0.3405)))\n\n\n\nplot_ims(xb[0], lrnorm(xb)[0])\n\n\n\n\nChanging the beta parameter doesn’t affect the images as much:\n\nlrnorm = nn.LocalResponseNorm(size=1, beta=10)\n\n\nget_means(xb, lrnorm(xb)), \\\nget_stds(xb, lrnorm(xb))\n\n((TensorImageBW(0.0037), TensorImageBW(0.0015)),\n (TensorImageBW(1.0045), TensorImageBW(0.9985)))\n\n\n\nplot_ims(xb[0], lrnorm(xb)[0])\n\n\n\n\nChanging the k parameter doesn’t change individual images much but it does affect the batch statistics:\n\nlrnorm = nn.LocalResponseNorm(size=1, k=10)\n\n\nget_means(xb, lrnorm(xb)), \\\nget_stds(xb, lrnorm(xb))\n\n((TensorImageBW(0.0037), TensorImageBW(0.0007)),\n (TensorImageBW(1.0045), TensorImageBW(0.1786)))\n\n\n\nplot_ims(xb[0], lrnorm(xb)[0])\n\n\n\n\n\n\nLayerNorm\nLayerNorm is different from the other normalization layers in the sense that you can explicitly determine which dimensions the normalization occurs across.\nI’ll try different sets of dimensions and plot the images and calculate image and batch statistics.\nSetting the normalized_shape parameter to the last dimension ([28]) affects the individual image as well as the batch mean:\n\nlnorm = nn.LayerNorm(normalized_shape=[28]) # last dimension\n\n\nget_means(xb, lnorm(xb)), \\\nget_stds(xb, lnorm(xb))\n\n((TensorImageBW(0.0037), TensorImageBW(5.7020e-10, grad_fn=<AliasBackward0>)),\n (TensorImageBW(1.0045), TensorImageBW(0.8411, grad_fn=<AliasBackward0>)))\n\n\n\nplot_ims(xb[0], lnorm(xb)[0])\n\n\n\n\nSetting normalized_shape to the last two dimensions ([28, 28]) significantly affects the batch mean (it goes to 0) and sets the batch standard deviation to 1. The individual image does not seem affected:\n\nlnorm = nn.LayerNorm(normalized_shape=[28, 28]) # last two dimensions\n\n\nget_means(xb, lnorm(xb)), \\\nget_stds(xb, lnorm(xb))\n\n((TensorImageBW(0.0037), TensorImageBW(9.5033e-10, grad_fn=<AliasBackward0>)),\n (TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=<AliasBackward0>)))\n\n\n\nplot_ims(xb[0], lnorm(xb)[0])\n\n\n\n\nSetting normalized_shape to the last three dimensions has a similar effect:\n\nlnorm = nn.LayerNorm(normalized_shape=[1, 28, 28]) # last three dimensions\n\n\nget_means(xb, lnorm(xb)), \\\nget_stds(xb, lnorm(xb))\n\n((TensorImageBW(0.0037), TensorImageBW(9.5033e-10, grad_fn=<AliasBackward0>)),\n (TensorImageBW(1.0045), TensorImageBW(1.0000, grad_fn=<AliasBackward0>)))\n\n\n\nplot_ims(xb[0], lnorm(xb)[0])\n\n\n\n\nI’ll run through my helper functions for this scenario as it seems most similar to BatchNorm2d, InstanceNorm2d and GroupNorm.\nThe mean absolute difference between the image before/after normalization is similar to BatchNorm2d:\n\nmad(xb[0], lnorm(xb)[0])\n\nTensorImageBW(0.0668, grad_fn=<AliasBackward0>)\n\n\nThere is some visually distinguishable difference between the mean pixel value in each image before/after LayerNorm is applied:\n\nplot_hists(\n    a=xb.mean(0).flatten(),\n    b=lnorm(xb).mean(0).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After LayerNorm',\n    xlabel='Mean Pixel Value in Image')\n\n\n\n\nSame goes for the distributions of pixel values:\n\nplot_hists(\n    a=xb.flatten(),\n    b=lnorm(xb).flatten().detach().numpy(),\n    label_a='Original',\n    label_b='After LayerNorm',\n    xlabel='Pixel Value in Image')\n\n\n\n\nAcross the 100 batches, the means are centered around 1e-9 (like GroupNorm) and the standard deviations are similarly distributed around 1.\n\nplot_means_stds(lnorm)"
  },
  {
    "objectID": "posts/2024-07-07-normlayers/index.html#training-models-using-different-normalization-layers",
    "href": "posts/2024-07-07-normlayers/index.html#training-models-using-different-normalization-layers",
    "title": "Comparing CNN Performance by Varying Activation Normalization Layers",
    "section": "Training Models Using Different Normalization Layers",
    "text": "Training Models Using Different Normalization Layers\nNow that I’m a bit more familiar with the behavior of the activation normalization layers on the MNIST dataset, I’ll train models using them and compare between them their final validation accuracy as well as their activation_stats.\nI’m modifying simple_cnn so that the conv function receives output_shape since nn.LayerNorm uses that as the normalized_shape parameter. The output_shape is based on 28x28 input images to the model.\n\ndef simple_cnn():\n  return sequential(\n      conv(1, 8, output_shape=[14,14], ks=5),         # 14x14\n      conv(8, 16, output_shape=[7,7]),              # 7x7\n      conv(16, 32, output_shape=[4,4]),             # 4x4\n      conv(32, 64, output_shape=[2,2]),             # 2x2\n      conv(64, 10, output_shape=[1,1], act=False),  # 1x1\n      Flatten()\n  )\n\n\ndef fit(epochs=5, lr=0.1):\n  learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))\n  with learn.no_logging(), learn.no_bar(): learn.fit_one_cycle(epochs, lr)\n  return learn\n\n\ndls = get_dls(512)\ndls.show_batch(max_n=9, figsize=(4,4))\n\n\n\n\nI’ll define a list of all the activation normalization layers I’ll use and a data object that will store the training results. Note that I am not using nn.InstanceNorm2d since I get the following error when attempting to train with it:\nValueError: Expected more than 1 spatial element when training, got input size torch.Size([512, 10, 1, 1])\nThis PyTorch forum’s post explains that for InstanceNorm2d, in the input \\((N,C,H,W)\\), \\(H\\) and \\(W\\) must be greater than 1. In my model the output of the final convolution will be 1x1 (for each of the 10 numbers). I don’t want to change the architecture for InstanceNorm2d since I won’t be able to compare apples-to-apples with the other normalization layers, so I’m deciding not to use it.\nFor nn.GroupNorm, I’m using num_groups of 2 since that was giving me the highest accuracy when compared to lower or higher values (after quickly training for 1 epoch). I’m using a size of 2 for nn.LocalResponseNorm since it gave higher validation accuracy than lower or higher values (after training for 1 epoch).\n\nnorms = [\n    nn.BatchNorm2d,\n    nn.GroupNorm,\n    nn.LocalResponseNorm,\n    nn.LayerNorm\n]\n\n\ndata = {\n    'BatchNorm2d': None,\n    'GroupNorm': None,\n    'LocalResponseNorm': None,\n    'LayerNorm': None\n}\n\nFor each normalization layer, I’ll train 10 models for 5 epochs each, storing the activation_stats.layer_stats, activation_stats.hist and learn.recorder.values[-1][-1] (final accuracy).\n\nfor norm in norms:\n  layer_stats = L()\n  hists = L()\n  accs = L()\n\n  def conv(ni, nf, output_shape, ks=3, act=True):\n    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n\n    if norm.__name__ in ['BatchNorm2d', 'InstanceNorm2d']: normlayer = norm(nf)\n    if norm.__name__ == 'GroupNorm': normlayer = norm(1, nf)\n    if norm.__name__ == 'LocalResponseNorm': normlayer = norm(2)\n    if norm.__name__ == 'LayerNorm': normlayer = nn.LayerNorm([nf] + output_shape)\n\n    layers.append(normlayer)\n    if act: layers.append(nn.ReLU())\n    return nn.Sequential(*layers)\n\n  for _ in range(10):\n    learn = fit()\n    layer_stats.append(learn.activation_stats.layer_stats(-2))\n    hists.append(learn.activation_stats.hist(-4))\n    accs.append(learn.recorder.values[-1][-1])\n\n  data[norm.__name__] = {'layer_stats': layer_stats, 'hists': hists, 'accs': accs}\n  save_pickle('/content/data.pkl', data)\n  print(f'{norm.__name__} ---- done.')\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\nBatchNorm2d ---- done.\nGroupNorm ---- done.\nLocalResponseNorm ---- done.\nLayerNorm ---- done.\n\n\n\nAnalyzing Training Results\n\nFinal Validation Accuracy\nTime to look at the results! I’ll start by comparing the median, mean, std, min and max final validation accuracy between the different activation normalization layers.\nAcross 10 training runs (for 5 epochs each) BatchNorm2d had the highest median (0.9924) and mean (0.99227) final validation accuracy as well as the lowest standard deviation (0.000495). LocalResponseNorm had one training run with a final validation accuracy of 0.1135!\n\naccs = pd.DataFrame({key: data[key]['accs'] for key in data})\naccs.describe().loc[['mean', '50%', 'std', 'min', 'max']]\n\n\n\n  \n    \n\n\n  \n    \n      \n      BatchNorm2d\n      GroupNorm\n      LocalResponseNorm\n      LayerNorm\n    \n  \n  \n    \n      mean\n      0.992270\n      0.990560\n      0.883270\n      0.990400\n    \n    \n      50%\n      0.992400\n      0.990700\n      0.971700\n      0.990500\n    \n    \n      std\n      0.000495\n      0.000799\n      0.270729\n      0.000523\n    \n    \n      min\n      0.991100\n      0.989100\n      0.113500\n      0.989400\n    \n    \n      max\n      0.992800\n      0.991500\n      0.979300\n      0.991000\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nActivation Stats\nNext, I’ll look at the activation_stats across the training runs for each of the four normalization layers.\n\ndef plot_stats_avg(layer_stats_list, titles, super_title, labels):\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n    fig.suptitle(super_title, fontsize=16)\n\n    colors = ['red', 'blue', 'green', 'orange']\n\n    for i, title in enumerate(titles):\n        for j, layer_stats in enumerate(layer_stats_list):\n            y = layer_stats.itemgot(i)\n            x = range(len(y[0]))\n\n            y_median = np.median(np.stack(y), axis=0)\n            axs[i].plot(x, y_median, color=colors[j], alpha=0.5, linewidth=1, label=labels[j])\n\n        axs[i].set_title(f\"Median {title} Activations Across 10 Trainings\")\n        axs[i].set_xlabel(\"Batch\")\n        axs[i].set_ylabel(f\"{title}\")\n        axs[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\nLocalResponseNorm (green line) has the most unstable trainings—the average mean activations for it are an order of magnitude more negative than the other three normalization layers; the mean standard deviations are an order of magnitude more positive; it has the highest mean %-near-zero activations, with almost 100% activations near zero on average by the end of the training run.\n\nlayer_stats_list = [data[key]['layer_stats'] for key in data.keys()]\nlabels = list(data.keys())\nplot_stats_avg(layer_stats_list, ['Mean', 'Std', '%-near-zero'], 'Activation Stats', labels)\n\n\n\n\nAfter excluding LocalResponseNorm, I can see the other three layers’ training results more clearly.\nThe median of mean activations for BatchNorm2d (red line) across 10 trainings are the closest to 0 throughout the training when compared to GroupNorm (blue line) or LayerNorm (green line). BatchNorm2d has the highest median standard deviations. BatchNorm2d has the lowest %-near-zero activations—plateauing at around 65% by the end of the training. After looking at these charts, I would say that BatchNorm2d has the most stable trainings.\n\nlayer_stats_list = [data[key]['layer_stats'] for key in data.keys() if key != 'LocalResponseNorm']\nplot_stats_avg(layer_stats_list, ['Mean', 'Std', '%-near-zero'], 'Activation Stats', ['BatchNorm2d', 'GroupNorm', 'LayerNorm'])\n\n\n\n\n\n\nActivation Histograms\nFinally, I’ll look at the mean and median value of the histograms of activations across the 10 training runs for each normalization layer:\n\ndef plot_hist_avg(hist, super_title):\n  fig, axs = plt.subplots(2, 1, figsize=(20, 5))\n  fig.suptitle(super_title, fontsize=16)\n  h_mean = torch.stack(list(hist)).mean(0)\n  h_median = torch.stack(list(hist)).median(0)[0]\n\n  axs[0].imshow(h_mean, origin='lower');\n  axs[0].set_title(f\"Mean Activations Across 10 Trainings\")\n  axs[0].axis('off');\n\n  axs[1].imshow(h_median, origin='lower');\n  axs[1].set_title(f\"Median Activations Across 10 Trainings\")\n  axs[1].axis('off');\n\nBatchNorm2d results in mean and median histogram values that look similar to the fastai textbook example of a stable training. The number of zero-activations decrease smoothly over the first epoch or so.\n\nplot_hist_avg(data['BatchNorm2d']['hists'], \"BatchNorm2d\");\n\n\n\n\nThe GroupNorm activations are a bit more turbulent, as you can see slight striations of alternating white and grey that indicate small “collapses” of activations in the first epoch before the training stabilizes. I would consider this less stable than BatchNorm2d.\n\nplot_hist_avg(data['GroupNorm']['hists'], \"GroupNorm\");\n\n\n\n\nLocalResponseNorm continues to be the worst normalization layer and the most turbulent as the training shows a clear sign of collapsing activations in the first epoch and continues to show a high number of zero-activations across the training run.\n\nplot_hist_avg(data['LocalResponseNorm']['hists'], \"LocalResponseNorm\");\n\n\n\n\nUsing LayerNorm results in a more stable training; it looks similar to the BatchNorm2d histograms.\n\nplot_hist_avg(data['LayerNorm']['hists'], \"LayerNorm\");"
  },
  {
    "objectID": "posts/2024-07-07-normlayers/index.html#final-thoughts",
    "href": "posts/2024-07-07-normlayers/index.html#final-thoughts",
    "title": "Comparing CNN Performance by Varying Activation Normalization Layers",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis exercise continued to illustrate just how rich the world of deep learning is! I feel like I just scratched the surface of understanding how different activation normalization layers behave, and even that required a considerable amount of time and effort.\nOverall from my relatively simple experiments BatchNorm2d seems like the best layer to use for this data. Of course there are many hyperparameters that I didn’t exhaustively tune (other than quickly training 1-epoch models to see how changing 1 hyperparameter like num_groups for GroupNorm affected the validation accuracy) and so these results might not be optimal.\nHowever, there’s something to be said about being able to just use BatchNorm2d without worrying about hyperparameters and getting the best results.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "",
    "text": "A chloropleth map of Minnesota Census data\nIn this blog post, I’ll walk through my process of creating an ArcGIS geodatabase and a set of layouts visualizing U.S. Census Data. The data used for this app is from table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months).\nYou can view the final layout PDFs at the following links:"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nGet the Data\n\nTract Boundaries\nACS 5-Year Estimates\nUsing data.census.gov\nUsing the censusapi R package\n\nConnect Data to Geodatabase\n\nTract Boundaries\nACS 5-Year Estimates\n\nVisualize Data\n\nCreate a Map\nCreate a Symbology\nCreate a Layout\n\nNormalize the Data\n\nCreate Additional Layouts"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Get the Data",
    "text": "Get the Data\n\nTract Boundaries\n\nDownload and unzip 2019 TIGER Shapefile for MN (tl_2019_27_tract.zip) (corresponds to the final year, 2019, in the ACS 5-year estimates). These will contain the Census Tract geographies needed to create a map in ArcGIS.\n\n\n\nACS 5-Year Estimates\n\nUsing data.census.gov\n\nOn data.census.gov, search for B20005\n\n\n\nSelect the link to the Table B20005 with “2019 inflation-adjusted dollars”\n\n\n\nClick the dropdown at the top next to the label Product and select 2015: ACS 5-Year Estimates Detailed Tables\n\n\n\nClick Customize Table at the top right of the page\n\n\n\nIn the Geo* section, click Tract > Minnesota > All Census Tracts within Minnesota\n\n\n\nOnce it’s finished loading, click Close and then Download Table\n\n\n\nOnce downloaded, extract the zip folder and open the file ACSDT52015.B20005_data_with_overlays….xslx_ in Excel any tool that can handle tabular data\nSlice the last 11 characters of the GEO_ID (using the RIGHT function in a new column) to replace the existing GEO_ID column values. For example, a GEO_ID of 1400000US27029000100 should be replaced with 27029000100. This will later on be matched with the GEOID field in the tl_2019_27_tract shapefile\nSave/export the file as .XLSX\n\n\n\nUsing the censusapi R package\nPass the following arguments to the censusapi::listCensusMetadata function and assign its return value to B20005_vars:\n\nB20005_vars <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  type=\"variables\",\n  group=\"B20005\"\n)\n\n\nPass the following arguments to censusapi::getCensus and assign its return value to B20005:\n\n\nB20005 <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  region=\"tract:*\",\n  regionin=\"state:27\", # 27 = Minnesota state FIPS code\n  vars=c(\"GEO_ID\", \"NAME\", B20005_vars$name)\n)\n\n\nReplace GEO_ID (or create a new column) with the last 11 characters\n\n\nB20005 <- substr(B20005$GEO_ID, 10, 20)\n\n\nExport to an .XLSX file\n\n\nwrite.xlsx(B20005, “acs5_b20005_minnesota.xlsx”, row.names = FALSE)"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Connect Data to Geodatabase",
    "text": "Connect Data to Geodatabase\nOpen ArcGIS Pro and start a new project.\n\nTract Boundaries\n\nRight click Folders in the Contents pane and click Add folder connection\n\n\n\nSelect the downloaded (and extracted) tl_2019_27_tract folder and click OK\n\n\n\nClick on tl_2019_27_tract folder in the Contents pane\nIn the Catalog pane, right-click tl_2019_27.shp and then click Export > Feature Class to Geodatabase\n\n\n\nConfirm Input Features (tl_2019_27_tract.shp) and Output Geodatabase (Default.gdb or whatever geodatabase you are connected to) and then click the green Run button\nRefresh the Geodatabase and click on it in the Contents pane to view the added shapefile\n\n\n\n\nACS 5-Year Estimates\n\nUnder the View ribbon click on Geoprocessing to open that pane\nIn the Geoprocessing pane, search for Join Field and click on it\n\n\n\nNext to Input Table click on the folder icon to Browse. Select the tl_2019_27_tract table in your geodatabase\n\n\n\nClick the Input Join Field dropdown and select GEOID\nNext to Join Table click on the folder icon to Browse. Select the acs5_b20005_minnesota$ Excel table and click OK (note: the Excel table is inside the XLSX file)\n\n\n\nType GEO_ID under Join Table Field\nClick on the down arrow next to Transfer Fields and select B20005_002E, B20005_003E, B20005_049E, and B20005_050E\n\n\n\nClick on Validate Join\n\n\n\nClick on Run\nA success message should be displayed at the bottom of the Geoprocessing pane"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Visualize the Data",
    "text": "Visualize the Data\nIn this section, I’ll create maps and layouts to visualize the population estimates using Census Tract spatial data.\n\nCreate a Map\n\nIn the Catalog pane, right-click tl_2019_27_tract > Add to New > Map\n\n\n\nTo reference the raw data: from the Feature Layer ribbon, click Attribute Table\n\n\n\n\n\nCreate a Symbology\n\nSelect the tl_2019_27_tract layer in Contents pane\nClick Appearance under the Feature Layer ribbon\nClick the down arrow on Symbology and select Graduated Colors\n\n\n\nSelect B20005_002E in the Field dropdown and Natural Breaks (Jenks) for the Method\n\n\n\nThe class breaks created by this method do not reliably classify the data, which is determined using the City of New York Department of Planning Map Reliability Calculator. There’s a 10.1% chance that a tract is erroneously classified.\n\n\n\nAfter adjusting the class breaks, the following result in a reliable result (less than 10% chance of misclassifying any geography on the map and less than 20% of misclassifying estimates within a class due to sampling error)\n\n\n\nApply these breaks in the Classes tab in the Symbology pane\n\n\n\nThe Map pane displays the updated choropleth\n\n\n\n\nCreate a Layout\nUnder the Insert ribbon click on New Layout and Letter (8.5” x 11”)\n\n\n\nOn the Insert ribbon, click Map Frame and Default Extent under the Map category\n\n\n\nClick and drag the cursor to draw the Map Frame. Under the Layout ribbon select Activate and zoom/pan until the full choropleth is visible. Click Deactivate when you’re finished.\n\n\n\nAdd guides to create 0.5 inch margins by right-clicking on rulers clicking Add Guide\n\n\n\nUnder the Insert ribbon click on Legend and draw a rectangle underneath the map\n\n\n\nRight-click the legend and click Properties to format the font size, text visibility (under Legend Item in the dropdown next to Legend in the Format Legend panel) and more\n\n\n\nOn the Ribbon tab in the Graphics and Text panel, you can choose different text types to add text to your layout. I’ve added titles and explanatory text.\n\n\n\nThe census tracts for the city of Minneapolis are too small to be clearly visible. Under the Insert ribbon click Map Frame, select the map and draw a small rectangle over Wisconsin.\nWith the new Map Frame selected, click Reshape > Circle under the Insert ribbon. Draw a circle over the rectangular map.\n\n\n\nRight-click on the circular map and click Properties to add a border. Add a textbox to label it as the City of Minneapolis.\n\n\n\nFrom the Graphics and Text panel on the Insert ribbon use the straight line and circle tool to add some visual cues indicating that the map frame is a detail view of the city\n\n\n\nUnder the Share ribbon, select Export Layout and export it to a PDF file"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Normalize the Data",
    "text": "Normalize the Data\nWhile the worker population estimates gives us a sense of how workers are distributed across the state, they are a proxy for population density. Census Tracts in Urban areas, like the Minneapolis, will likely have more workers than Rural areas, because they have a higher population. To supplement this layout, I’ll create layouts that show the percentage of the total sex population who are full time workers.\n\nTo duplicate the Male Full TIme Estimates layout, right-click it in the Catalog pane, click Copy and then right click in the gray area underneath it and click Paste\n\n\n\n\nRename the layout to Male Full Time Percentages and open it\nRename the two maps in the Contents pane\n\n\n\nRight-click tl_2019_27_tract under Main Map and click Symbology to open the Symbology pane\n\n\n\nSelect B20005_002E (Total Male Estimate) in the Normalization dropdown. This will be the value that divides a Census Tract’s population estimate\n\n\n\nCalculate the Margin of Error (MOE) for the percentage of total male workers who are full time employed using equation 6 from the “Calculating Measures of Error for Derived Estimates” in the Understanding and Using American Community Survey Data: What All Data Users Need to Know handbook in order to determine the class break reliability. In the equation below, P = X/Y is the percentage of full time workers in the tract (X= B20005_003E and Y = B20005_002E)\n\n\n\nOne reliable set of class breaks, which were few and far between, was the following:\n\n\n\nApply those class breaks in the Symbology pane and update the text to match\n\n\n\nCreate Additional Layouts\n\nRepeat the process to create the following Layouts given the following class breaks\n\nFemale Full Time Estimates\n\n\n\n\nFemale Full Time Percentages\n\n\n\nI hope you enjoyed this tutorial."
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "",
    "text": "Here’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, I work through the first of four “Further Research” problems assigned at the end of Chapter 7 in the textbook “Deep Learning for Coders with fastai and PyTorch”.\nThe prompt for this exercise is:\n\nUse the fastai documentation to build a function that crops an image to a square in each of the four corners; then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai?"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "What is Test Time Augmentation?",
    "text": "What is Test Time Augmentation?\nI’ll quote directly from the text:\n\nDuring inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n\nTTA is data augmentation during validation, in hopes that objects located outside the center of the image (which is the default fastai validation image crop) can be recognized by the model in order to increase the model’s accuracy.\nThe default Learner.tta method averages the predictions on the center crop and four randomly generated crops. The method I’ll create will average the predictions between the center crop and four corner crops.\n\n\n\ntta.png"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "User-defined Test Time Augmentation",
    "text": "User-defined Test Time Augmentation\n\nRead and understand the Learner.tta and RandomCrop source code\ndef tta(self:Learner, ds_idx=1, dl=None, n=4, item_tfms=None, batch_tfms=None, beta=0.25, use_max=False):\n    \"Return predictions on the `ds_idx` dataset or `dl` using Test Time Augmentation\"\n    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n    if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n    try:\n        self(_before_epoch)\n        with dl.dataset.set_split_idx(0), self.no_mbar():\n            if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n            aug_preds = []\n            for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n        aug_preds = torch.cat(aug_preds)\n        aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n        self.epoch = n\n        with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n    finally: self(event.after_fit)\n\n    if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n    preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n    return preds,targs\nclass RandomCrop(RandTransform):\n    \"Randomly crop an image to `size`\"\n    split_idx,order = None,1\n    def __init__(self, size, **kwargs):\n        size = _process_sz(size)\n        store_attr()\n        super().__init__(**kwargs)\n\n    def before_call(self, b, split_idx):\n        self.orig_sz = _get_sz(b)\n        if split_idx: self.tl = (self.orig_sz-self.size)//2\n        else:\n            wd = self.orig_sz[0] - self.size[0]\n            hd = self.orig_sz[1] - self.size[1]\n            w_rand = (wd, -1) if wd < 0 else (0, wd)\n            h_rand = (hd, -1) if hd < 0 else (0, hd)\n            self.tl = fastuple(random.randint(*w_rand), random.randint(*h_rand))\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n        return x.crop_pad(self.size, self.tl, orig_sz=self.orig_sz)\n\n\nPractice cropping images using the .crop method on a PILImage object\nA PIL Image has a method called crop which takes a crop rectangle tuple, (left, upper, right, lower) and crops the image within those pixel bounds.\nHere’s an image with a grizzly bear at the top and a black bear on the bottom. There are four coordinates of interest: left, upper, right and bottom. The leftmost points on the image are assigned a pixel value of 0. The rightmost points are located at the image width pixel pixel value. The uppermost points are at pixel 0, and the bottommost points are at the image height pixel value.\n\nf = \"/content/gdrive/MyDrive/fastai-course-v4/images/test/grizzly_black.png\"\nimg = PILImage.create(f)\nimg.to_thumb(320)\n\n\n\n\n\nTop-Left Corner Crop\nA top-left corner crop the corresponds to a left pixel of 0, upper pixel 0, right pixel of 224, and bottom pixel of 224. The order in the tuple is left, upper, right, bottom, so 0, 0, 224, 224. You can see that this crop is taken from the top left corner of the original image.\n\nimg.crop((0,0,224,224))\n\n\n\n\n\n\nTop Right Corner Crop\nFor the top right corner, I get the image width since the left end of the crop will be 224 pixels from the right end of the image. That translates to w-224. The upper pixel is 0, and the rightmost pixel is at w, and the bottom pixel is 224. You can see that this crop is at the top right corner of the original.\n\nw = img.width\nh = img.height\nimg.crop((w-224, 0, w, 224))\n\n\n\n\n\n\nBottom Right Corner Crop\nFor the bottom right corner the left pixel is 224 from the right end, w-224, the upper pixel is 224 from the bottom, h-224, the right pixel is at w, and the bottom is at h.\n\nimg.crop((w-224, h-224, w, h))\n\n\n\n\n\n\nBottom Left Corner Crop\nThe bottom left corner’s leftmost pixel is 0, uppermost pixel is 224 pixels from the bottom of the whole image, h - 224, the rightmost pixel is 224, and bottommost pixel is the bottom of the whole image, at h.\n\nimg.crop((0, h-224, 224, h))\n\n\n\n\n\n\nCenter Crop\nFinally, for the center crop, the leftmost pixel is 112 left of the image center, w/2 - 112, the upper pixel is 112 above the image center, h/2 - 112, the rightmost pixel is 112 right of the center, w/2 + 112, and the bottom pixel is 112 below the center, h/2 + 112.\n\nimg.crop((w/2-112, h/2-112, w/2+112,h/2+112))\n\n\n\nSummary\nTo better visualize this, here are a couple of images which show the left, upper, right and bottom coordinates for the corner and center crops.\nSummary of corner crop arguments (left, upper, right, bottom)\n\n\n\ncrop_dimensions-01.png\n\n\nSummary of center crop arguments (left, upper, right, bottom)\n\n\n\ncenter_crop_dimensions-01.png\n\n\n\n\n\nDefine a function which takes an image and returns a stacked Tensor with four corner crops and a center crop\nI wrap those five lines of code into a function called corner_crop, which takes a PILImage img, and a square side length size (defaulted to 224) as its arguments. It first grabs the width and height of the image. And then goes on to save the crops of the four corners and center as TensorImages, returning them all in a single stacked Tensor.\n\ndef corner_crop(img, size=224):\n  \"\"\"Returns a Tensor with 5 cropped square images\n  img: PILImage\n  size: int\n  \"\"\"\n  w,h = img.width, img.height\n  top_left = TensorImage(img.crop((0,0,size,size)))\n  top_right = TensorImage(img.crop((w-size, 0, w, size)))\n  bottom_right = TensorImage(img.crop((w-size, h-size, w, h)))\n  bottom_left = TensorImage(img.crop((0, h-size, size, h)))\n  center = TensorImage(img.crop((w/2-size/2, h/2-size/2, w/2+size/2,h/2+size/2)))\n  return torch.stack([top_left, top_right, bottom_right, bottom_left, center])\n\nI’ll test the corner_crop function and make sure that the five images are cropped correctly.\nHere’s the top left corner.\n\nimgs = corner_crop(img)\n\n# Top Left Corner Crop\nimgs[0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1a177d0>\n\n\n\n\n\nTop right corner:\n\n# Top Right Corner Crop\nimgs[1].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e197da50>\n\n\n\n\n\nBottom right:\n\n# Bottom Right Corner Crop\nimgs[2].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e146ed50>\n\n\n\n\n\nBottom left:\n\n# Bottom Left Corner Crop\nimgs[3].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424dd0>\n\n\n\n\n\nAnd center:\n\n# Center Crop\nimgs[4].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424450>\n\n\n\n\n\n\n\nDefine a new CornerCrop transform by extending the Transform class definition\nThe main purpose for all of that was for me to wrap my head around how the crop behavior functions so that I can wrap that into a transform.\nTransforms are any function that you want to apply to your data. I’ll extend the base Transform class and add in the functionality I need for these crops. When an object of the CornerCrop class is constructed, the constructor takes size and corner_type arguments. Since I’ll use this within a for-loop, the corner_type argument is an integer from 0 to 3, corresponding to the loop counter. The transform is applied to the data during the .encodes method. I grab the original image width and height, and create a list of cropped images using the left, upper, right, bottom coordinates we saw above. Finally, based on the corner_type, the corresponding crop is returned.\n\nclass CornerCrop(Transform):\n    \"Create 4 corner and 1 center crop of `size`\"\n    def __init__(self, size, corner_type=0, **kwargs):\n      self.size = size\n      self.corner_type = corner_type\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n      self.w, self.h = x.size\n      self.crops = [\n                    x.crop((0,0,self.size, self.size)),\n                    x.crop((self.w - self.size, 0, self.w, self.size)),\n                    x.crop((self.w-self.size, self.h-self.size, self.w, self.h)),\n                    x.crop((0, self.h-self.size, self.size, self.h))\n                    ]\n      return self.crops[self.corner_type]\n\nTo test this transform, I created an image with top left, top right, bottom right and bottom left identified. I created multiple copies so that I can create batches.\n\n# test image for CornerCrop\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\nImage.open((path/'01.jpg'))\n\n\n\n\nI create a DataBlock and pass my CornerCrop to the item_tfms parameter. I’ll cycle through the different corner types. 0 corresponds to top left, 1 is top right, 2 is bottom right and 3 is bottom left. All images in my batch should be cropped to the same corner.\nI set corner_type to 0, build the DataBlock and DataLoaders and the batch shows top left.\n\n# get the data\n# path = untar_data(URLs.IMAGENETTE)\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,0))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 1, build the DataBlock and DataLoaders and the batch shows top right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,1))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 2, build the DataBlock and DataLoaders and the batch shows bottom right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,2))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 3, build the DataBlock and DataLoaders and the batch shows bottom left.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,3))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nNow, I can implement this transform into a new TTA method.\n\n\nDefine a new Learner.corner_crop_tta method by repurposing the existing Learner.tta definition\nI’ll largely rely on the definition of tta in the built-in Learner class. In this method, predictions are calculated on four sets of augmented data (images) and then averaged along with predictions on a center-crop dataset.\nIn the existing for-loop, four sets of predictions on randomly generated crops are appended into a list.\nfor i in self.progress.mbar if hasattr(self,'progress') else range(n):\n  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\nIn my loop, I create a new DataLoader each time, passing a different corner_type argument to the CornerCrop transform. I also have to pass the ToTensor transform, so that the PIL Image is converted to a Tensor. In the first iteration, it will append predictions on the top left corner crops. In the next one, it will append predictions on the top right, then the bottom right, and finally on the fourth loop, the bottom left.\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nSince I am to average these with the center-crop image predictions, I’ll create a new DataLoader without the CornerCrop transform and calculate the predictions on those images:\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\nFinally, I’ll append the center crop preds to aug_preds list, concatenate them into a single tensor and take the mean of the predictions:\naug_preds.append(preds[None])\npreds = torch.cat(aug_preds).mean(0)\nI decided to create a new Learner2 class which extends the built-in the Learner, and added the corner_crop_tta method by copying over the tta method, commenting out the lines I won’t need and adding the lines and changes I’ve written above.\n\nclass Learner2(Learner):\n  def corner_crop_tta(self:Learner, ds_idx=1, dl=None, n=4, beta=0.25, use_max=False):\n      \"Return predictions on the `ds_idx` dataset or `dl` using Corner Crop Test Time Augmentation\"\n      if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n      # if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n      try:\n          #self(_before_epoch)\n          with dl.dataset.set_split_idx(0), self.no_mbar():\n              if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n              aug_preds = []\n              # Crop image from four corners\n              for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                  dl = dl.new(after_item=[CornerCrop(224,i), ToTensor])\n                  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n         # aug_preds = torch.cat(aug_preds)\n         # aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n          self.epoch = n\n          dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n          # Crop image from center\n          with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n          aug_preds.append(preds[None])\n      finally: self(event.after_fit)\n\n     # if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n     # preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n     # preds = torch.cat([aug_preds, preds]).mean(0)\n      preds = torch.cat(aug_preds).mean(0)\n      return preds,targs\n\n\n\nImplement this new TTA method on the Imagenette classification model\nIn the last section of this notebook, I train a model on the Imagenette dataset, which a subset of the larger ImageNet dataset. Imagenette has 10 distinct classes.\n\n# get the data\npath = untar_data(URLs.IMAGENETTE)\n\n# build the DataBlock and DataLoaders \n# for a single-label classification\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label, # image folder names are the class names\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\ndls = dblock.dataloaders(path, bs=64)\n\n# view a batch\ndls.show_batch()\n\n\n\n\n\n\n\n\n# Try `CornerCrop` on a new DataLoader\n# add `ToTensor` transform to conver PILImage to TensorImage\nnew_dl = dls[1].new(after_item=[CornerCrop(224,3), ToTensor])\nnew_dl.show_batch()\n\n\n\n\n\n# baseline training\nmodel = xresnet50()\nlearn = Learner2(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.628959\n      2.382344\n      0.450336\n      02:39\n    \n    \n      1\n      1.258259\n      3.365233\n      0.386482\n      02:45\n    \n    \n      2\n      0.992097\n      1.129573\n      0.653473\n      02:49\n    \n    \n      3\n      0.709120\n      0.643617\n      0.802091\n      02:47\n    \n    \n      4\n      0.571318\n      0.571139\n      0.824122\n      02:45\n    \n  \n\n\n\nI run the default tta method, pass the predictions and targets to the accuracy function and calculate an accuracy of about 83.5% percent. Which is higher than the default center crop validation accuracy.\n\n# built-in TTA method\npreds_tta, targs_tta = learn.tta()\naccuracy(preds_tta, targs_tta).item()\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\n0.8345780372619629\n\n\nFinally, I run my new corner_crop_tta method, pass the predictions and targets to the accuracy function, and calculate an accuracy of about 70.9% percent. Which is lower than the default center crop validation accuracy.\n\n# user-defined TTA method\npreds, targs = learn.corner_crop_tta()\naccuracy(preds, targs).item()\n\n\n\n\n0.7098581194877625\n\n\nI’ll walk through the corner_crop_tta code to verify the accuracy calculated above.\nI first create an empty list for my augmented image predictions.\nThen I loop through a range of 4, each time creating a new DataLoader which applies the CornerCrop transform for each corner type and append the predictions onto the list.\n\n# get predictions on corner cropped validation images\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nlen(aug_preds), aug_preds[0].shape\n\n\n\n\n\n\n\n\n\n\n\n\n\n(4, torch.Size([1, 2678, 1000]))\n\n\nI then create a new DataLoader without my transform, and get those predictions.\n\n# get predictions on center crop validation images\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\npreds.shape\n\n\n\n\ntorch.Size([2678, 1000])\n\n\nThe shape of these predictions is missing an axis, so I pass None as a Key and it adds on a new axis.\n\n# add an axis to match augmented prediction tensor shape\npreds = preds[None]\npreds.shape\n\ntorch.Size([1, 2678, 1000])\n\n\nI append the center crop predictions onto the augmented predictions and concatenate all five sets of predictions into a Tensor and calculate the mean.\n\n# average all 5 sets of predictions\naug_preds.append(preds)\npreds = torch.cat(aug_preds).mean(0)\n\nI then pass those average predictions and the targets to the accuracy function calculate the accuracy which is slightly higher than above. I ran these five cells multiple times and got the same accuracy value. When I ran the corner_crop_tta method multiple times, I got different accuracy values each time. Something in the corner_crop_tta definition is incorrect. I’ll go with this value since it was consistent.\n\n# calculate validation set accuracy\naccuracy(preds, targs).item()\n\n0.7311426401138306\n\n\nThe following table summarize the results from this training:\n\n\n\nValidation\nAccuracy\n\n\n\n\nCenter Crop\n82.4%\n\n\nCenter Crop + 4 Random Crops: Linearly Interpolated\n83.5%\n\n\nCenter Crop + 4 Random Crops: Averaged\n73.1%\n\n\n\nThere are a few further research items I should pursue in the future:\n\nFix the corner_crop_tta method so that it returns the same accuracy each time it’s run on the same trained model\nTry corner_crop_tta on a multi-label classification dataset such as PASCAL\nTry linear interpolation (between center crop and corner crop maximum) instead of mean"
  },
  {
    "objectID": "posts/2024-06-26-cf/index.html",
    "href": "posts/2024-06-26-cf/index.html",
    "title": "Exploring Collaborative Filtering Applications",
    "section": "",
    "text": "In this blog post, I’ll work through the following prompt from the “Further Research” section at the end of Chapter 8 (Collaborative Filtering):\n\nFind three other areas where collaborative filtering is being used, and identify the pros and cons of this approach in those areas.\n\nI initially prompted Claude to brainstorm some ideas and got the following response:\n\nSocial media content curation\n\nMethod: Analyzes user interactions to recommend posts, articles, or accounts.\nExample: Facebook’s News Feed algorithm.\nPros: Personalized content, increased engagement.\nCons: Echo chambers, potential bias amplification.\n\nJob recruitment\n\nMethod: Matches candidates to jobs based on similar profiles and preferences.\nExample: LinkedIn’s job recommendation system.\nPros: Efficient candidate discovery, better job fit.\nCons: May overlook unique candidates, potential for discrimination.\n\nOnline dating\n\nMethod: Suggests matches based on user preferences and behaviors.\nExample: Tinder’s match suggestions.\nPros: Saves time, introduces compatible matches.\nCons: May reinforce stereotypes, limits serendipitous encounters.\n\n\nThose are all good examples and I think illustrate how collaborative filtering is used. I’ll do some further research (no pun intended) for each one."
  },
  {
    "objectID": "posts/2024-06-26-cf/index.html#social-media-content-curation",
    "href": "posts/2024-06-26-cf/index.html#social-media-content-curation",
    "title": "Exploring Collaborative Filtering Applications",
    "section": "Social Media Content Curation",
    "text": "Social Media Content Curation\nI found this article which talks about Facebook’s News Feed algorithm at a high level but doesn’t mention collaborative filtering explicitly. This Meta AI blog mentions a lot of interesting tech, including the phrase “recommendation systems” but doesn’t mention collaborative filtering (I’m assuming not all recsys involve collaborative filtering).\nI instead found a lot more content online explicitly mentioning collaborative filtering for music recommendation systems, specifically this paper from 2014 by Spotify in which they introduce Logistic Matrix Factorization “a new probabilistic model for matrix factorization with implicit feedback.” The motivation for using implicit feedback is that:\n\nimplicit feedback data such as clicks, page views, purchases, or media streams can be collected at a much larger and faster scale and without needing the user to provide any explicit sentiment\n\nThey “assume that we have a set of non-negative feedback values associated with each pair of users and items in our domain” and that “we don’t require the values to be integers but instead allow them to be any non-negative reals.”\nThe key contribution of this paper is to frame the probability of a user preferring an item as:\n\n\\[p(l_{ui}|x_u, y_i, \\beta_i, \\beta_j) = \\frac{\\exp(x_i y_i^T + \\beta_u + \\beta_i)}{1+\\exp(x_u y_i^T + \\beta_u + \\beta_i)}\\]\nWhere:\n\\(l_{ui}\\) is the event that user \\(u\\) has chosen to interact with item \\(i\\)\n\\(x_u\\) is the user data\n\\(y_i\\) is the item data and\nthe \\(\\beta\\)s are the biases.\nAnd then learn \\(X\\), \\(Y\\) and \\(\\beta\\) that maximizes:\n\n\\[\\arg \\max X,Y, \\beta\\log p(X,Y,\\beta|R)\\]\n\nThe metric they use is Mean Percentage Ranking “that evaluates a user’s satisfaction with an ordered list of recommended items.”\n\nLower values of MPR are more desirable as they indicate that the user listened to artists higher in their predicted lists.\n\nThey find that Logistic MF beats a competing model (Implicit Matrix Factorization, IMF, which minimizes a weighted root mean squared error over the training data of binary preferences) and the performance of the models don’t improve beyond 100 latent factors.\nWhile they don’t discuss limitations or downsides of this algorithm, as a Spotify user I have found their recommendations underwhelming. Of course their algorithms have changed since 2014 (the current Spotify Research page has publications on Reinforcement Learning, Graph-based Inductive Representations, and RNNs) and I am a “repeat listener” even by Spotify’s standards, so I use it more like a music player (like Winamp!) than for music discovery."
  },
  {
    "objectID": "posts/2024-06-26-cf/index.html#job-recruitment",
    "href": "posts/2024-06-26-cf/index.html#job-recruitment",
    "title": "Exploring Collaborative Filtering Applications",
    "section": "Job Recruitment",
    "text": "Job Recruitment\nI had much better luck finding more recent explicit references to collaborative filtering for LinkedIn’s recommendation systems. In this article written by LinkedIn’s Machine Learning Engineering Manager, she describes how collaborative filtering is used in their offline recommendation engine (italicized emphasis mine):\n\nCollaborative Filtering: Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences from many users (collaborating). In our framework, this model leverages past implicit engagement data of learners (i.e., course watches) to identify relevant courses. We learn a latent representation for each learner and each course and use similarities between these latent representations to predict member-course relevance.\n\nThe predictions from their collaborative filtering model are “blended” online (in realtime) with predictions from a classifier trained on “historical explicit engagement” of users with course materials. She lists the advantages of collaborative filtering as:\n\n\nthe ability to capture recent interests by focusing on recent interactions.\ndiversified recommendations, since they are based on similarity in course watch behavior, rather than the content of the courses.\nrelying solely on engagement data, mitigating the need for domain knowledge.\n\n\nTheir collaborative filtering system relies on “members’ course-watching behavior for signals” with the downside that it has “relatively poor performance when recommending courses for new learners.”"
  },
  {
    "objectID": "posts/2024-06-26-cf/index.html#online-dating",
    "href": "posts/2024-06-26-cf/index.html#online-dating",
    "title": "Exploring Collaborative Filtering Applications",
    "section": "Online Dating",
    "text": "Online Dating\nThis Vox article led me to this Wired story about Monster Match, a game funded by Mozilla with the intent to show “how dating app algorithms reinforce bias.” The game simulates a dating app where you swipe on different monsters to indicate that you want to “date” them and intermittently the game pauses to illustrate how your choices have affected the algorithm. For example, after swiping on a few profiles:\n\nThe algorithm had already removed half of Monster Match profiles from my queue—on Tinder, that would be the equivalent of nearly 4 million profiles. It also updated that queue to reflect early “preferences,” using simple heuristics about what I did or didn’t like. Swipe left on a googley-eyed dragon? I’d be less likely to see dragons in the future.\n\nThe mention that “When you first log in, your recommendations are almost entirely dependent on what other users think.” such that:\n\nif you swipe right on a zombie and left on a vampire, then a new user who also swipes yes on a zombie won’t see the vampire in their queue.\n\nThe consequence (and downside) of this behavior is that “dating app users get boxed into narrow assumptions and certain profiles are routinely excluded.”\nThe article also connects this behavior to dating apps in real life, referencing a study which shows that on OKCupid, “black women receive the fewest messages of any demographic on the platform.” These apps allow for filtering by race, conducing behaviors which “reinforce racial inequalities in the real world”."
  },
  {
    "objectID": "posts/2024-06-26-cf/index.html#predicting-sensor-array-values",
    "href": "posts/2024-06-26-cf/index.html#predicting-sensor-array-values",
    "title": "Exploring Collaborative Filtering Applications",
    "section": "Predicting Sensor Array Values",
    "text": "Predicting Sensor Array Values\nI wanted to find an application of collaborative filtering that was more niche than product/profile/post recommendation systems so I googled around a bit and found the paper Collaborative Filtering to Predict Sensor Array Values in Large IoT Networks. In this fascinating work, they draw analogies between recommending products based on sparse user preference data (users don’t provide explicit feedback often) and predicting blanks in sparse sensor data (failed readings due to malfunction, extreme weather, network quality and other reasons) as both involve a correlation between the subject of interest (users or sensors):\n\nThe key idea of our proposal is that, fortunately, sensor array results are not completely independent from each other; e.g., readings from sensors under high environmental temperature in some area in China can help to predict readings of some other sensors that have reported failures in a high temperature area from Nevada; it can be done when a correlation in the sensor values of both areas occurs. This is precisely what CF RS are trained for: they predict item’s values that the users have not voted yet based on the rating values of the rest of the RS users.\n\nThey use PMF (Probabilistic Matrix Factorization) which, based on my understanding of their explanation, seems very similar (if not exactly the same) as the latent factor and dot-product prediction approach used in Chapter 8 of the fastai textbook—-in short, you generate random embeddings (or tensors) of size U (users) x N (latent factors) for users and I (items) x N (latent factors) for the items (like movies, songs, or products) and take the dot product of U and I to get the predicted ratings (or preference scores). Using gradient descent to minimize Root Mean Squared Error as a loss function, the model learns these latent factors and gets better at predicting a rating of an item by a user who has not explicitly rated the item (i.e. it gets better at predicting blank ratings).\nIn the paper they find that collaborative filtering (matrix factorization) works better for sparse sensor data, and KNN (K-Nearest Neighbor) works better with denser sensor data."
  },
  {
    "objectID": "posts/2024-06-26-cf/index.html#final-thoughts",
    "href": "posts/2024-06-26-cf/index.html#final-thoughts",
    "title": "Exploring Collaborative Filtering Applications",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI always end up learning more than I expected to after working through these simple, short but open-ended “Further Research” prompts. The world of recommendation systems is way more interesting than I had imagined. I suppose I didn’t realize the importance of the systems part in “recommendation systems”—there are truly fascinating and complex AI systems being researched, built and deployed in this space and even a cursory review of some of the publications was really enjoyable. I look forward to revisit this topic in the future with a deeper dive.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-2/index.html",
    "href": "posts/2024-02-05-paddy-part-2/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 2",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8\nPart 2: Live Coding 9 (You are here)\nPart 3: Live Coding 10\nPart 4: Live Coding 11\nPart 5: Live Coding 12\nPart 6: Live Coding 13\nPart 7: Improving My Large Ensemble, Part 1\nPart 8: Improving My Large Ensemble, Part 2\n\nLink to the Live Coding 9 video"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-2/index.html#setup",
    "href": "posts/2024-02-05-paddy-part-2/index.html#setup",
    "title": "Paddy Doctor Kaggle Competition - Part 2",
    "section": "Setup",
    "text": "Setup\n\n!pip install -qq timm==0.6.13\nimport timm\ntimm.__version__\n\n'0.6.13'\n\n\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai')\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\n\npath.ls()\n\n(#4) [Path('../input/paddy-disease-classification/sample_submission.csv'),Path('../input/paddy-disease-classification/train_images'),Path('../input/paddy-disease-classification/train.csv'),Path('../input/paddy-disease-classification/test_images')]\n\n\n\ntrn_path = path/'train_images'"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-2/index.html#miscellaneous-topics",
    "href": "posts/2024-02-05-paddy-part-2/index.html#miscellaneous-topics",
    "title": "Paddy Doctor Kaggle Competition - Part 2",
    "section": "Miscellaneous topics",
    "text": "Miscellaneous topics\nThese are interesting topics Jeremy walked through before going into the Paddy competition stuff.\nTo keep installed libraries persistent in Paperspace, install them with the --user flag.\n\nEdit /storage/.bash.local\nAdd to it: alias pi=\"pip install -U --user\"\nIn bash type pi will display what pi is aliased as\nwhich pi won’t tell you anything useful because pi is not a binary\nRun pi timm. The --user flag will put in the .local directory\nNeed to make sure that your .local directory is symlinked\nThis way, you don’t have to restart your kernel after pip install\n\nVim commands:\n\n/ searches (/init will search for the next thing called “init” in the file)\nTo go back to where we were: Ctrl+o\nTo go forward to where you were: Ctrl+i\nIf you type f it will search on the line the next thing you type, Shift+F searches backwards\nShift+A to start inserting at the end of the line\nDelete everything up to the next double quote: d+f+\"\nPress . to repeat the previous command\n% goes to the next parenthesis (goes to closing parenthesis first)\nReplace all parameters in functions: c (for change) + i (inside parenthesis) + type the change (like a,b) then go down to the next spot and type ."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-2/index.html#improving-a-model-for-a-kaggle-competition",
    "href": "posts/2024-02-05-paddy-part-2/index.html#improving-a-model-for-a-kaggle-competition",
    "title": "Paddy Doctor Kaggle Competition - Part 2",
    "section": "Improving a model for a kaggle competition",
    "text": "Improving a model for a kaggle competition\nLet’s now try to improve the model from the first walkthrough.\nIf you train past 10 epochs, you are in danger of overfitting as your model has seen each image 10 times. In order to avoid overfitting we should make it so that it sees a slightly different image each time. You can pass in batch_tfms which will be applied to each mini-batch.\nWhat does aug_transforms do? Flip, zoom, adjust brightness for, rotated, warp images. This is called data augmentation. It returns a list of transformations.\n\naug_transforms(size=224, min_scale=0.75)\n\n[Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5}:\n encodes: (TensorImage,object) -> encodes\n (TensorMask,object) -> encodes\n (TensorBBox,object) -> encodes\n (TensorPoint,object) -> encodes\n decodes: ,\n Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}:\n encodes: (TensorImage,object) -> encodes\n decodes: ,\n RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'mode_mask': 'nearest', 'p': 1.0}:\n encodes: (TensorImage,object) -> encodes\n (TensorBBox,object) -> encodes\n (TensorPoint,object) -> encodes\n (TensorMask,object) -> encodes\n decodes: ]\n\n\nMost models trained on ImageNet are trained on image sizes of 224x224.\n\ndls = ImageDataLoaders.from_folder(\n    trn_path, \n    valid_pct=0.2,\n    item_tfms=Resize(460, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75)\n)\n\nIn show_batch if you say unique=True it will show the same picture with the different transformations (sometimes it’s flipped, sometimes it’s moved a little bit up and down, sometimes it’s a little bit darker or brighter, or rotated).\n\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\nThe default learning rate from fastai is on the conservative side, meaning it’s a little bit lower than you probably need because Jeremy wanted things to always be able to train. There’s a couple of downsides to using a lower learning rate than you need:\n\nGiven fixed resources and fixed amount of time, you’re going to have less distance that the weights can move\nA high learning rate helps the optimizer to explore the space of options by jumping further to see if there’s better places to go\n\n\nlearn = vision_learner(dls, 'convnext_small_in22k', metrics=error_rate).to_fp16()\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small_22k_224.pth\n\n\nThis forum post (sign-in required) helped resolve the [Errno 30] Read-only file system: error thrown when calling lr_find, by changing the model_dir attribute of the Learner:\n\nlearn.model_dir = \"/kaggle/working\"\n\n\nlearn.lr_find(suggest_funcs=(valley, slide))\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575, slide=0.004365158267319202)\n\n\n\n\n\nThe suggested learning rate is 0.002, but you can see that all the way up to 10^-2 it has a pretty nice slope. We are using 1cycle training schedule which means we are gradually increasing the learning rate and by doing that we can reach higher learning rates, so even these recommendations are going to be on the conservative side.\n\nlearn.fine_tune(12, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.026152\n      0.743547\n      0.227775\n      01:21\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.495129\n      0.281290\n      0.087938\n      01:34\n    \n    \n      1\n      0.354283\n      0.253887\n      0.085055\n      01:35\n    \n    \n      2\n      0.334391\n      0.299357\n      0.084575\n      01:30\n    \n    \n      3\n      0.267883\n      0.188827\n      0.052859\n      01:29\n    \n    \n      4\n      0.231655\n      0.188384\n      0.052379\n      01:29\n    \n    \n      5\n      0.167554\n      0.158490\n      0.043248\n      01:29\n    \n    \n      6\n      0.117927\n      0.157844\n      0.039404\n      01:29\n    \n    \n      7\n      0.091830\n      0.144641\n      0.033638\n      01:29\n    \n    \n      8\n      0.067092\n      0.108346\n      0.027391\n      01:29\n    \n    \n      9\n      0.044437\n      0.108044\n      0.025949\n      01:29\n    \n    \n      10\n      0.040759\n      0.107195\n      0.024027\n      01:28\n    \n    \n      11\n      0.031392\n      0.108461\n      0.024988\n      01:29"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-2/index.html#saving-a-trained-model",
    "href": "posts/2024-02-05-paddy-part-2/index.html#saving-a-trained-model",
    "title": "Paddy Doctor Kaggle Competition - Part 2",
    "section": "Saving a trained model",
    "text": "Saving a trained model\nLearner.export exports the Learner to Learner.path/fname. You specify an absolute path with a preceding '/'. The Learner, in addition to the model and optimizer, contains information about the DataLoaders and what transformations were applied.\nLearner.save exports just the model and optimizer, not the Learner.\n\nlearn.path = Path(\"/kaggle/working\")\n\n\nlearn.export('cn_sml_12.pkl')"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-2/index.html#test-time-augmentation",
    "href": "posts/2024-02-05-paddy-part-2/index.html#test-time-augmentation",
    "title": "Paddy Doctor Kaggle Competition - Part 2",
    "section": "Test Time Augmentation",
    "text": "Test Time Augmentation\nIf you don’t use the 'squish' item transform, the validation set will only use the cropped center portion of the image, and this is a particularly important situation when you should use test time augmentation.\nIn test time augmentation, we get multiple versions of each image (4 by default) plus the un-augmented version, we get the prediction on every one and then we get the average (or max) prediction.\nWe should be able to replicate the final epoch’s error rate manually:\n\nprobs,targs = learn.get_preds(dl=dls.valid)\n\n\n\n\n\n\n\n\n\nerror_rate(probs, targs)\n\nTensorBase(0.0250)\n\n\nGood. That is the same error rate as the final epoch during training. Now let’s try out tta using the average prediction (of the 4 augemented and 1 un-augmented predictions). We would likely much more clearly see the benefit of tta if we did not squish the images when creating the DataLoaders.\n\nprobs,targs = learn.tta(dl=dls.valid)\nerror_rate(probs,targs)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0216)\n\n\nThe average tta prediction error rate is less than the regular prediction error rate. Let’s see what error rate we get if we use the maximum tta predictions:\n\nprobs,targs = learn.tta(dl=dls.valid, use_max=True)\nerror_rate(probs,targs)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0235)\n\n\nIn this case, using the max predictions results in a worse error rate. Generally speaking, Jeremy has found then when you are not using squish, use_max=True results in more accurate predictions."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-2/index.html#prepare-file-for-kaggle-submission",
    "href": "posts/2024-02-05-paddy-part-2/index.html#prepare-file-for-kaggle-submission",
    "title": "Paddy Doctor Kaggle Competition - Part 2",
    "section": "Prepare file for kaggle submission",
    "text": "Prepare file for kaggle submission\n\ntst_files = get_image_files(path/'test_images').sorted()\n\n\ntst_dl = dls.test_dl(tst_files)\n\n\nprobs,targs = learn.tta(dl=tst_dl)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nEach row of probs will contain a probability for each element of the vocab. There are 10 probabilities for each of the 3469 images in the test set.\n\nlen(dls.vocab), len(tst_dl.dataset)\n\n(10, 3469)\n\n\n\nprobs.shape\n\ntorch.Size([3469, 10])\n\n\nWhat is actually predicting? The thing is predicting is whatever thing that has the highest probability. Go through each row and find the index of the thing with the highest probability (argmax).\n\nidxs = probs.argmax(dim=1)\nidxs.shape\n\ntorch.Size([3469])\n\n\n\nidxs = pd.Series(idxs.numpy(), name='idxs')\n\nWe can rewrite how we create mapping by passing it directly the enumarate object.\n\nmapping = dict(enumerate(dls.vocab))\nresults = idxs.map(mapping)\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = results\n\n\nss.to_csv('subm.csv', index=False)\n\n\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nGiven that this is an image classification task for natural photos, it will almost certainly have exactly the same characteristics as ImageNet in terms of fine-tuning, so work on the assumption that the things that are in the timm model comparison notebook will apply to this dataset. Once everything is working well, try it on a couple of models or at least run it on a bigger one.\nIf it was like a segmentation problem or an object detection problem, medical imaging dataset which has pictures that aren’t in ImageNet, Jeremy would try more different architectures, but in those cases he would not try to replicate the research of others and would look at paperswithcode.com to find out which techniques have the best results on segmentation and better still would go and find 2-3 previous Kaggle competitions that have a similar problem type and see who won and see what they did. They’ll likely have done an ensemble, which is fine, but they will also say “the best model in the ensemble was X”, and so just use the smallest version of X I can get away with.\nGenerally fiddling with architectures tends not to be very useful for any kind of problem that people have fairly regularly studied.\nIn my next blog post I walk through the discussion and code from Live Coding 10."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-5/index.html",
    "href": "posts/2024-02-05-paddy-part-5/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 5",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8\nPart 2: Live Coding 9\nPart 3: Live Coding 10\nPart 4: Live Coding 11\nPart 5: Live Coding 12 (You are here)\nPart 6: Live Coding 13\nPart 7: Improving My Large Ensemble, Part 1\nPart 8: Improving My Large Ensemble, Part 2\n\nLink to Live Coding 12 video\n\n\n\n!pip install -qq timm==0.6.13\nimport timm\ntimm.__version__\n\n'0.6.13'\n\n\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai')\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\n\npath.ls()\n\n(#4) [Path('../input/paddy-disease-classification/sample_submission.csv'),Path('../input/paddy-disease-classification/train_images'),Path('../input/paddy-disease-classification/train.csv'),Path('../input/paddy-disease-classification/test_images')]\n\n\n\ntrn_path = path/'train_images'\n\n\n\n\nOn PETS, in the top 15, you have a bit of everything (resnetrs, resnetv2, vit, swin, resnet26d, regnet, convnext). The larger vit models only work on larger images. Pleasantly surprised to see some of the vit’s didn’t use much memory and they were pretty fast. resnetrs is small and fast.\nOn PLANET, it doesn’t look that different except it’s entirely vit, swin and convnext in the top 15. vit_small_patch32_224’s memory usage is amazing.\nRunning these models took less than 3 hours on Jeremy’s three GPUs. 1200 runs.\nWhen to use larger images? Do everything you can on smaller images, as long as it gets you reasonable results, because you want to iterate quickly. Then, when you want the best accuracy you try bigger and bigger images and see what you can get away with–keep doing that as long as the accuracy improves. In a production environment it’s similar, you make it bigger and bigger until the latency of the model is too high, and you find the right trade-off between model latency and accuracy. Generally speaking, larger images will give you more accurate results but a lot slower (you end up with a lot more pixels from 224^2 to 360^2). For initial iterating you don’t need a really accurate model because you are trying to figure out what data preprocessing works best, or what architecture works best. In a business context, Jeremy would do exactly what he has done here—try a few things on small fast models on small images on a small subset of the data to find out what data preprocessing and architecture to use, and then he would look at what are the constraints in operationalizing this (how much RAM do we have, how much latency can we get away with, how expensive is it going to be) to scale it up to the point where we are getting acceptable results using acceptable resources. It wouldn’t look very different at all than a Kaggle competition in terms of the modeling, but there would be a whole piece of it around user requirements, costs, and stuff like that.\nOne student tried going from smaller to larger models but it resulted in lower accuracy (note: this is what essentially happened to me, except for the large ensemble with 3-times weighted vit model which matches the small ensemble’s private score), is it just a fluke? No, it’s not a fluke. It means you pressed the wrong buttons somehow. Re-run Jeremy’s notebooks and then look at yours and see how they’re different. And then figure out where you went wrong. When debugging, look at the inputs and outputs—what predictions are you making? For example are you always predicting 0? Did you run lr.find to see what learning rate works well? Stuff like that.\nWhen exporting a model, the format is pickle but the file extension is pth (what PyTorch uses).\n\n\n\nLet’s take a look at the variety column in train.csv:\n\ndf = pd.read_csv(path/'train.csv')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n      variety\n      age\n    \n  \n  \n    \n      0\n      100330.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      1\n      100365.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      2\n      100382.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      3\n      100632.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      4\n      101918.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      10402\n      107607.jpg\n      tungro\n      Zonal\n      55\n    \n    \n      10403\n      107811.jpg\n      tungro\n      Zonal\n      55\n    \n    \n      10404\n      108547.jpg\n      tungro\n      Zonal\n      55\n    \n    \n      10405\n      110245.jpg\n      tungro\n      Zonal\n      55\n    \n    \n      10406\n      110381.jpg\n      tungro\n      Zonal\n      55\n    \n  \n\n10407 rows × 4 columns\n\n\n\n\ndf.variety.value_counts()\n\nvariety\nADT45             6992\nKarnatakaPonni     988\nPonni              657\nAtchayaPonni       461\nZonal              399\nAndraPonni         377\nOnthanel           351\nIR20               114\nRR                  36\nSurya               32\nName: count, dtype: int64\n\n\nAbout 70% of the ~10k rows of data are of the “ADT45” variety. But there are ~3k rows that contain other varieties. Something that is a bit counterintuitive: when there’s two different things (what kind of rice is it, what kind of disease is it) sometimes trying to get your model to predict two different things makes tham better at both. This sounds counterintuitive because you are giving it more work to do but you are also giving it more signal—things you’re teaching it to look for. So maybe if it knows how to recognize different kinds of rice, it can use that information to also recognize how different kinds of rice are impacted by different diseases. No idea if this is going to be useful or not but it would be an interesting exercise to try to do that. Also a good exercise of delving into models in a way we haven’t done before. This is going to be much more sophisticated than anything we’ve done with deep learning before. It’s a really good test of how well you understand what’s going on inside a neural network.\nJeremy trained a model three times to see what the error rate was, to see what kind of variation there is. A learning rate of 0.02 for 3 epochs gave consistent results. People are often into doing reproducible training where they have set the seed for their training and run the same thing everytime. I think that’s normally a bad idea because I actually want to see what the natural variation is and so if I make a change I want to know if the difference I see in the result is due to natural variation or it’s actually something significant. If the natural variation is very large, it’s going to be tough to see if you actually improved things. But then if the natural variation is so large that improvements are invisible then trying to improve it seems pointless because it sounds like you haven’t really found a way to stably train something. And normally that happens because the learning rate is too big. If you bump the learning rate to 0.04 you’ll see the error rate go all over the place (5%, 6^, etc.). Training for more epochs at a lower learning rate will generally give you more stable results. There’s a compromise because doing more epochs is slow. You could also try using a smaller subset of the data. In the end sometimes things will just be slow but most of the time Jeremy finds that you can get a compromise.\nWith 6 epochs at half the learning rate (0.01) the model is more accurate (4% error rate rather than 5%).\nThese improvements you make on a small scale show up on a larger scale. They pretty much always will because they are the same models with more layers or wider activations. If you find some preprocessing step that works well on a convnext tiny, it’s going to work also well on a convnext larg, 99.9% of the time. Most people act as if that’s not true, like in academia, or most people never think to try. But intuitively, of course it’s the same. Why wouldn’t it be the same? It’s the same model just scaled up a bit that behaves very similarly. You can argue that it’s intuitive, but it might not be intuitive because everybody has told you for years that it doesn’t work that way.\nI’ll run three trainings of the model as Jeremy has done, to get a sense of the variation in error rates for the convnext_tiny_in22k model:\n\ndls = ImageDataLoaders.from_folder(\n    trn_path, seed=42, valid_pct=0.2,\n    item_tfms=Resize(480), batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\narch = 'convnext_tiny_in22k'\n\n\nlearn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\nlearn.fine_tune(3, 0.02)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.183931\n      0.734716\n      0.228736\n      00:57\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.706453\n      0.518948\n      0.162422\n      01:04\n    \n    \n      1\n      0.408914\n      0.192417\n      0.058626\n      01:03\n    \n    \n      2\n      0.220568\n      0.127589\n      0.037482\n      01:04\n    \n  \n\n\n\n\nlearn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\nlearn.fine_tune(3, 0.02)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.156675\n      0.834739\n      0.226333\n      00:57\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.689531\n      0.731114\n      0.203268\n      01:04\n    \n    \n      1\n      0.420913\n      0.229073\n      0.076886\n      01:05\n    \n    \n      2\n      0.221704\n      0.140325\n      0.041807\n      01:06\n    \n  \n\n\n\n\nlearn = vision_learner(dls, arch, metrics=error_rate).to_fp16()\nlearn.fine_tune(3, 0.02)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.198419\n      0.696659\n      0.214320\n      00:57\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.651208\n      0.480119\n      0.150889\n      01:04\n    \n    \n      1\n      0.429291\n      0.229344\n      0.069197\n      01:01\n    \n    \n      2\n      0.225388\n      0.136109\n      0.043248\n      01:03\n    \n  \n\n\n\nThe final error rates for convnext_tiny_in22k using a learning rate of 0.02 for 3 epochs were 0.037482, 0.041807, and 0.043248 (the first time I ran these three models). The largest difference in error rate between the three models was 15%. That seems not great. In the video, Jeremy’s models had a difference of 3%.\nLet’s actually look at a model. To simplify things later on (when adding new layers to handle two categories of classification) we will remove the to_fp16 call.\n\nlearn = vision_learner(dls, arch, metrics=error_rate)\n\n\nm = learn.model\n\nThere are two things at the top of the model:\n\nTimmBody (which has multiple things in it, the first being the model, which in turn has multiple things, the first being the stem, the next being stages and so on).\n\nThe body does all the hard work of looking at the pixels and trying to find features and things like that. In this case it’s a convolutional neural network. At the very end of it it spits out a whole bunch of information about those pixels.\n\nthe head ((1): Sequential).\n\nthe head makes sense of what the body spits out and returns predictions. The head is pretty simple, the body is not.\n\n\nWe want to predict two things: what kind of rice it is and what disease it has. Currently the very last layer is a linear layer:\n(8): Linear(in_features=512, out_features=10, bias=False)\nA lineary layer is something that does a matrix product, with an input of 512 features and output of 10 features. A 512 x 10 matrix.\nLet’s grab the head (which is the index-1 thing in the model)\n\nh = m[1]\n\n\nh\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1536, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=10, bias=False)\n)\n\n\nIs there a way to see the shape of the data as it flows through the model?\nYes, with learn.summary() (full output is too long so showing a snippet here). The 64 is because we are using a batch size of 64. For each image we are predicting 10 probabitilies (for 10 classes of disease).\nSequential (Input shape: 64 x 3 x 224 x 224)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n\n...\n\n____________________________________________________________________________\n                     64 x 1536           \nFlatten                                                        \nBatchNorm1d                               3072       True      \nDropout                                                        \n____________________________________________________________________________\n                     64 x 512            \nLinear                                    786432     True      \nReLU                                                           \nBatchNorm1d                               1024       True      \nDropout                                                        \n____________________________________________________________________________\n                     64 x 10             \nLinear                                    5120       True      \n____________________________________________________________________________\nLet’s look at the last layer of the head:\n\nll = h[-1]\n\n\nll\n\nLinear(in_features=512, out_features=10, bias=False)\n\n\nWe can view the parameters in the layer, which are generated lazily so we have to put it in a list to force it to generate them:\n\nllp = list(ll.parameters())[0]\n\n\nllp.shape\n\ntorch.Size([10, 512])\n\n\nThe last layer parameters (llp) is a matrix which is 10 x 512. We’re getting 512 inputs, and when we multiply them by this matrix we get 10 outputs.\nIf we removed this layer, the last layer would be taking in 1536 features and spitting out 512 features:\n(4): Linear(in_features=1536, out_features=512, bias=False)\nWe’ll delete it, and then take those 512 features and create two linear layers for them, one with 10 outputs as before (for disease classification) and one with 10 outputs for variety classification (there are 10 varieties). The final output would be 2 x 10 for each image—one with probabilities for disease classes, and one for variety classes.\nLet’s do the easy thing first which is to delete the layer we don’t want. A PyTorch Sequential takes the output of each layer and passes it as the input to the next layer. If we delete the last layer, that’s no problem, it just won’t ever call it. PyTorch’s Sequential has normal list semantics so you can delete a layer like so:\n\ndel(h[-1])\n\n\nh\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1536, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n)\n\n\nWe’re going to create a class which includes this model and adds the final two layers:\n\nfrom copy import deepcopy\n\n\ndls = ImageDataLoaders.from_folder(\n    trn_path, seed=42, valid_pct=0.2,\n    item_tfms=Resize(480), batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\nlearn = vision_learner(dls, arch, metrics=error_rate)\nlearn2 = deepcopy(learn)\n\n\nclass DiseaseAndTypeClassifier(nn.Module):\n    # the constructor\n    def __init__(self, m):\n        super().__init__() # always call the superclass init to construct the object\n        self.l1 = nn.Linear(512, 10, bias=False) # variety\n        self.l2 = nn.Linear(512, 10, bias=False) # disease\n        del(m[1][-1]) # delete the last layer of the model's head\n        self.m = m # model\n        \n    def forward(self, x):\n        x = self.m(x)\n        x1 = self.l1(x) # variety output\n        x2 = self.l2(x) # disease output\n        return x1, x2\n\n\ndtc = DiseaseAndTypeClassifier(learn2.model)\nlearn2.model = dtc\n\nWhen calling get_preds even though with_loss=False the Learner is still needing to use the loss function for something. So, we’ll write the new loss function. The loss function is the thing which is a number which says how good is this model. The loss function that we were using was designed on something that only returned a single tensor. And now we’re returning a tuple of tensors (x1, x2). So that’s why when it tries to call the loss function, it gets confused.\nThe loss function is another thing that is stored inside the Learner.\n\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\nIn the initial new loss function, we’ll just return the loss for the disease predictions using the current loss function. We don’t have to split the targs yet because currently they hold just the disease targets. We’re going to have to change our data loading as well to include the rice variety as well.\nWhy is the loss function determined from the dataset? (As seen in the Learner source code).\nGenerally speaking, what is the appropriate loss function to use as a reasonable default depends on what kind of data you have. So if you’re data is a single continuous output you probably have a regression problem so you probably want mean squared error. If it’s a single categorical variable you probably want cross entropy loss. If you have a mult-categorical variable you probably want log loss without softmax. And so forth. By having it come from the dataset means that you can get sensible defaults that ought to work for that dataset. That’s why we generally don’t have to specify what loss function to use unless we’re doing something non-standard.\n\ncurr_loss = learn2.loss_func\n\n\ndef dtc_loss(preds, targs):\n    rice_preds, dis_preds = preds\n    return curr_loss(dis_preds, targs)\n\n\ndef dtc_error(preds, targs):\n    rice_preds, dis_preds = preds\n    return error_rate(dis_preds, targs)\n\n\nlearn2.loss_func = dtc_loss\nlearn2.metrics = [dtc_error]\n\n\nlearn2.loss_func\n\n<function __main__.dtc_loss(preds, targs)>\n\n\n\nlearn2.metrics\n\n(#1) [<fastai.learner.AvgMetric object at 0x78339534eb00>]\n\n\nAt this point we should be able to get some predictions to verify that the plumbing is working.\n\npreds, targs = learn2.get_preds(dl=learn2.dls.valid)\n\n\n\n\n\n\n\n\nWe’ve now got two sets of preditions, variety, and disease.\n\nlen(preds)\n\n2\n\n\n\npreds\n\n(tensor([[ 1.4707,  1.5254, -1.5469,  ...,  3.5859, -9.7422, -7.9531],\n         [ 2.8262, -0.3391, -1.0713,  ...,  3.9258, -6.7891, -5.5508],\n         [ 1.2178,  0.2090, -1.3574,  ...,  2.4629, -6.9688, -6.0234],\n         ...,\n         [ 1.2422,  0.5278, -0.7520,  ...,  1.9199, -7.2109, -6.4258],\n         [ 2.9219, -0.2183, -1.1211,  ...,  1.8584, -6.8516, -6.2461],\n         [ 2.0469,  0.6406, -1.3174,  ...,  3.8027, -9.2891, -5.8086]],\n        dtype=torch.float16),\n tensor([[ 2.8516, -2.9395, -3.2852,  ...,  1.3027, -1.5234, -2.8223],\n         [ 3.4375, -1.3428, -3.8047,  ..., -0.9033, -0.5435, -3.0449],\n         [ 4.1719, -1.0127, -2.9375,  ..., -0.5962,  0.7446, -3.3828],\n         ...,\n         [ 3.5781, -1.2656, -2.3008,  ..., -0.3660, -0.0957, -4.2734],\n         [ 4.9727, -0.4153, -2.6562,  ...,  0.9844,  0.7559, -3.6738],\n         [ 6.0703, -1.4375, -3.6797,  ..., -1.5928,  2.6211, -5.0625]],\n        dtype=torch.float16))\n\n\n\nrice_preds, dis_preds = preds\n\n\ndis_preds.shape\n\ntorch.Size([2081, 10])\n\n\nInitially, Jeremy got some errors when running learn2.get_preds(dl=learn2.dls.valid). To debug them, he first tried to get a minimum reproducible example—not changing learn.model to dtc. This still threw an error. Then, instead of running learn2 = copy(learn) he assigned to learn2 the line vision_learner(dls, arch, metrics=error_rate).\nThat didn’t solve the problem either. Something was keeping some part of the Learner’s state in half-precision. Finally, Jeremy tried restarting the kernel. That solved the problem, and the get_preds call was successful.\nHe also got an error because the metrics function was trying to calculate on a tuple of preds, so in order to test just the new loss function at first, we removed the metrics from the Learner.\nWe should be able to replicate our disease clasification model at this point because we’re not doing anything with the extra rice type (variety).\n\nlearn2.model_dir = '/tmp/model'\n\n\nlearn2.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0063095735386013985)\n\n\n\n\n\nThe valley (0.01) is pretty conservative so Jeremy recommends picking a learning rate further down the curve (0.1) which seems more reasonable. Look for a learning rate that’s as far to the right as possible but on a still pretty steep gradient.\n\nlearn2.fine_tune(1, 0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      dtc_error\n      time\n    \n  \n  \n    \n      0\n      1.070556\n      1.182892\n      0.401249\n      00:47\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      dtc_error\n      time\n    \n  \n  \n    \n      0\n      1.211720\n      0.858449\n      0.259491\n      00:54\n    \n  \n\n\n\nIn my next blog post I walk through the discussion and code from Live Coding 13, the last Live Coding video on the Paddy Doctor Kaggle competition."
  },
  {
    "objectID": "posts/2023-10-20-PS-Prompting/index.html",
    "href": "posts/2023-10-20-PS-Prompting/index.html",
    "title": "Paper Summary: Plan-and-Solve Prompting",
    "section": "",
    "text": "In this notebook I’ll summarize the paper Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models (Lei Wang, et al) with the following questions. This exercise is part of a fastai study group assignment where we had multiple study group members present the different Chain-of-Thought papers.\nThis notebook has the following sections:\n\nWhat is the problem which LLMs are failing to solve?\nAn overview of the prompting strategy\nAn example of the XoT prompt\nImprovements on benchmarks\nDoes this prompt strategy strike you as useful? What are the downsides?\nTest the prompt using a local model or API"
  },
  {
    "objectID": "posts/2023-10-20-PS-Prompting/index.html#what-is-the-problem-which-llms-are-failing-to-solve",
    "href": "posts/2023-10-20-PS-Prompting/index.html#what-is-the-problem-which-llms-are-failing-to-solve",
    "title": "Paper Summary: Plan-and-Solve Prompting",
    "section": "1) What is the problem which LLMs are failing to solve?",
    "text": "1) What is the problem which LLMs are failing to solve?\nZero-shot-CoT (CoT = Chain-of-Thought) prompting involves appending the phrase “Let’s think step by step” to the end of a prompt and has shown to improve LLM performance on reasoning tasks when compared to zero-shot prompting.\nAn example of a Zero-shot-CoT prompt:\n\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\nA: Let’s think step by step.\n\nFrom the abstract of this paper (emphasis mine):\n\nDespite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors.\n\nThe authors address calculation errors and missing-step errors with Plan-and-Solve (PS) Prompting and PS+ Prompting. PS+ Prompting extends PS Prompting with more detailed instructions."
  },
  {
    "objectID": "posts/2023-10-20-PS-Prompting/index.html#an-overview-of-the-prompting-strategy",
    "href": "posts/2023-10-20-PS-Prompting/index.html#an-overview-of-the-prompting-strategy",
    "title": "Paper Summary: Plan-and-Solve Prompting",
    "section": "2) An overview of the prompting strategy",
    "text": "2) An overview of the prompting strategy\nThe prompting template for PS and PS+ prompting is fundamentally the same as Zero-shot-CoT\n\nQ: [X].\nA: [T].\n\nWhere [X] contains the input problem statement and [T] is a hand-crafted instruction.\nThe prompting strategy extends the Zero-shot-CoT prompt by adding more detailed instructions [T] on how the LLM should answer reasoning tasks to encourage the model to devise a plan, carry out the plan and lists intermediate steps.\n\nPlan-and-Solve (PS) Prompting\nThis prompting strategy replaces the Zero-shot-CoT prompt of “Let’s think step by step” with the following in order to address the issue of Zero-shot-CoT caused by missing reasoning steps:\n\nQ: [X].\nA: Let’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan and solve the problem step by step.\n\n\n\nPS+ Prompting\nThis prompting strategy extends PS Prompting with the following instruction to address the calculation errors of Zero-shot-CoT:\n\n“pay attention to calculation”\n\nand the following instruction to address missing-step errors:\n\n“extract relevant variables and their corresponding numerals”\n\nIt also includes the following instruction to enhance the LLMs ability to generate relevant and important reasoning steps:\n\n“calculate intermediate results”\n\nAn example of the full PS+ Prompting strategy:\n\nQ: [X]\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let’s carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer.\n\nThis prompting strategy can be easily customized to solve a variety of problems other than math reasoning."
  },
  {
    "objectID": "posts/2023-10-20-PS-Prompting/index.html#an-example-of-the-xot-prompt",
    "href": "posts/2023-10-20-PS-Prompting/index.html#an-example-of-the-xot-prompt",
    "title": "Paper Summary: Plan-and-Solve Prompting",
    "section": "3) An example of the XoT prompt",
    "text": "3) An example of the XoT prompt\n\nPS Prompting\nUsing the same restaurant bill math problem as before, here is how it would be structured as a PS prompt:\n\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\nA: Let’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan and solve the problem step by step.\n\n\n\nPS+ Prompting\nAnd here is how it would be structured with the more detailed PS+ prompt:\n\nQ: After eating at the restaurant, Sally, Sam, and Alyssa decided to divide the bill evenly. If each person paid 45 dollars, what was the total of the bill?\nA: Let’s first understand the problem, extract relevant variables and their corresponding numerals, and devise a plan. Then, let’s carry out the plan, calculate intermediate results (pay attention to calculation and common sense), solve the problem step by step, and show the answer."
  },
  {
    "objectID": "posts/2023-10-20-PS-Prompting/index.html#improvement-on-benchmarks",
    "href": "posts/2023-10-20-PS-Prompting/index.html#improvement-on-benchmarks",
    "title": "Paper Summary: Plan-and-Solve Prompting",
    "section": "4) Improvement on benchmarks",
    "text": "4) Improvement on benchmarks\nThe authors evaluated their prompting on the following:\n\n6 math reasoning datasets\n\nAQUA: Algebraic word problems with natural language rationales.\nGSM8K: High quality linguistically diverse grade school math word problems.\nMultiArith: Math word problems requiring multiple reasoning steps and operations.\nAddSub: Addition and subtraction arithmetic word problems.\nSingleEq: Single-equation grade school algebra word problems with multiple math operations over non-negative rational numbers and one variable.\nSVAMP: One-unknown arithmetic word problems for up-to-4 grade level students.\n\n2 common sense reasoning datasets\n\nCommonsenseQA: Multiple choice questions that require different types of commonsense knowledge to obtain the correct answers.\nStrategyQA: Questions requiring multi-step reasoning but the reasoning steps are not given.\n\n2 symbolic reasoning datasets\n\nLast Letter Concatenation: Questions requiring the last letters of words in a name to be concatenated.\nCoin Flip: Questions on whether a coin is still heads up after it is flipped or not flipped based on steps given in the questions.\n\n\n\nArithmetic Reasoning Dataset Performance\n\n\n\nMath Reasoning Dataset Evaluation Results\n\n\nTakeaways:\n\nPS and PS+ Prompting out-perform Zero-shot-CoT across all arithmetic reasoning datasets.\nPS+ Prompting out-performs Zero-shot-PoT on five out of six arithmetic datasets.\nPS Prompting out-performs Zero-shot-PoT on three arithmetic datasets.\nAverage accuracy of PS+ slightly lower than Manual-CoT and slightly higher than Auto-CoT.\n\n\n\nCommon Sense Reasoning Dataset Performance\n\nTakeaways:\n\nPS+ Prompting out-performs Zero-shot-CoT and underperforms versus Few-Shot-CoT\n\n\n\nSymbolic Reasoning Dataset Performance\n\nTakeaways:\n\nPS+ out-performs Zero-shot-CoT and Few-Shot-CoT on the Last Letter dataset.\nPS+ out-performs Zero-shot-CoT and underperforms compared to Few-Shot-CoT on the Coin Flip dataset.\n\n\n\nAddressing Issues of Zero-shot-CoT Prompting\nAt the start of the paper, the authors identify three issues with Zero-shot-CoT prompting: Calculation errors, Missing-step errors and Semantic misunderstanding errors. PS has fewer Missing-step and Semantic misunderstanding errors. PS+ has fewer Calculation and Missing-step errors."
  },
  {
    "objectID": "posts/2023-10-20-PS-Prompting/index.html#does-this-prompt-strategy-strike-you-as-useful-what-are-the-downsides",
    "href": "posts/2023-10-20-PS-Prompting/index.html#does-this-prompt-strategy-strike-you-as-useful-what-are-the-downsides",
    "title": "Paper Summary: Plan-and-Solve Prompting",
    "section": "5) Does this prompt strategy strike you as useful? What are the downsides?",
    "text": "5) Does this prompt strategy strike you as useful? What are the downsides?\nThis prompt strategy seems somewhat useful. On one hand, it provides a framework for the LLM to respond with and performs better than zero-shot prompting in the paper’s evaluation. On the other hand, for some of the models I tested, Zero-shot-CoT performed better than PS+ on a small set of 20 samples.\nThere are elements of the PS+ Prompting instruction that are also present in the system prompt that Jeremy recommended (emphasis mine):\n\nSince you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. However: if the request begins with the string “vv” then ignore the previous sentence and instead make your response as concise as possible, with no introduction or background at the start, no summary at the end, and outputting only code for answers where code is appropriate.\nYour users are experts in AI and ethics, so they already know you’re a language model and your capabilities and limitations, so don’t remind them of that. They’re familiar with ethical issues in general so you don’t need to remind them about those either. Don’t be verbose in your answers, but do provide details and examples where it might help the explanation. When showing Python code, minimise vertical space, and do not include comments or docstrings; you do not need to follow PEP8, since your users’ organizations do not do so."
  },
  {
    "objectID": "posts/2023-10-20-PS-Prompting/index.html#test-the-prompt-using-a-local-model-or-api",
    "href": "posts/2023-10-20-PS-Prompting/index.html#test-the-prompt-using-a-local-model-or-api",
    "title": "Paper Summary: Plan-and-Solve Prompting",
    "section": "6) Test the prompt using a local model or api",
    "text": "6) Test the prompt using a local model or api\nIn the paper’s appendix, the authors provide two example PS+ prompts and corresponding outputs for each of the ten reasoning datasets they used. I tried out these given prompts on the following models, using HuggingChat and ChatGPT:\n\nHuggingFaceH4/zephyr-7b-alpha\nmistralai/Mistral-7B-Instruct-v0.1\nmeta-llama/Llama-2-70b-chat-hf\nGPT-3.5\n\nHere is a summary of accuracy of these models across the 20 samples:\n\n\n\n\nZephyr\nMistral\nLlama-2\nGPT-3.5\n\n\n\n\nZero-shot-CoT\n65%\n60%\n70%\n80%\n\n\nPS+\n45%\n65%\n60%\n85%\n\n\n\n\nI have linked the individual chats in the table below for reference (Z = Zero-shot-CoT). I wasn’t able to test the Zephyr Zero-shot-CoT prompts before HuggingChat removed that model from that interface, and the shared chats in the Zephyr Chat HuggingFace Space expire after a few days, so I’ve documented the results for Zephyr (Z) in this Google document.\n\n\n0 = Incorrect\n1 = Correct\n\n\n\n\n\nDataset\nAvg\nZephyr (Z)\nZephyr (PS+)\nMistral (Z)\nMistral (PS+)\nLlama-2 (Z)\nLlama-2 (PS+)\nGPT-3.5 (Z)\nGPT-3.5 (PS+)\n\n\n\n\nAQuA\n25%\n0, 0\n0, 0\n0, 0\n0, 0\n0, 1\n0, 0\n1, 1\n1, 0\n\n\nGSM8K\n69%\n1, 1\n0, 0\n1, 0\n1, 1\n0, 1\n0, 1\n1, 1\n1, 1\n\n\nMultiArith\n56%\n0, 1\n0, 0\n1, 1\n1, 1\n1, 0\n0, 0\n0, 1\n1, 1\n\n\nSVAMP\n88%\n1, 1\n1, 1\n1, 1\n1, 1\n1, 0\n1, 0\n1, 1\n1, 1\n\n\nAddSub\n94%\n1, 1\n1, 1\n1, 0\n1, 1\n1, 1\n1, 1\n1, 1\n1, 1\n\n\nSingleEq\n81%\n1, 1\n1, 1\n1, 1\n0, 1\n0, 1\n0, 1\n1, 1\n1, 1\n\n\nCommonsenseQA\n56%\n1, 0\n1, 0\n1, 0\n1, 0\n1, 0\n1, 1\n1, 0\n1, 0\n\n\nStrategyQA\n56%\n0, 1\n0, 0\n0, 1\n0, 1\n1, 1\n0, 1\n1, 0\n1, 1\n\n\nLast Letters\n38%\n0, 0\n0, 0\n0, 0\n0, 0\n1, 1\n1, 1\n1, 0\n1, 0\n\n\nCoin Flip\n100%\n1, 1\n1, 1\n1, 1\n1, 1\n1, 1\n1, 1\n1, 1\n1, 1\n\n\n\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html",
    "href": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html",
    "title": "Fine-Tuning a Language Model as a Text Classifier",
    "section": "",
    "text": "In this notebook, I’ll fine-tune a lanaguage model on the IMDb reviews dataset, grab the encoder, create a new classification model with it and then fine-tune it to classify IMDb reviews as positive or negative. The code (and prose) below is taken from Chapter 10 of the fastai textbook.\nThe data is stored in three folders: train (25k labeled reviews), test (25k labeled reviews) and unsup (50k unlabeled reviews). The language model is trained on all 100k reviews and the classification model is trained using the train dataset (its accuracy calculated on the test validation set)."
  },
  {
    "objectID": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tuning-the-pretrained-language-model",
    "href": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tuning-the-pretrained-language-model",
    "title": "Fine-Tuning a Language Model as a Text Classifier",
    "section": "Fine-Tuning the Pretrained Language Model",
    "text": "Fine-Tuning the Pretrained Language Model\nFirst, we fine-tune the pretrained language model (which was trained on all of Wikipedia) using 100k movie reviews. This fine-tuned model will learn to predict the next word of an IMDb movie review.\nNote that fastai’s TextBlock sets up its numericalizer’s vocab automatically.\n\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb,\n    splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_ln=80)\n\nThe dependent variable is the independent variable shifted over by one token:\n\ndls_lm.show_batch(max_n=2)\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj this movie is my favorite of all time . xxmaj the dialogue is spectacular , and is delivered with such rapid - fire speed that one viewing is not enough . xxmaj the film comedy was elevated to new heights with xxmaj howard xxmaj hawks outstanding direction . xxmaj based on the classic play \" the xxmaj front xxmaj page \" , xxmaj hawks gives it a delightful twist by\n      xxmaj this movie is my favorite of all time . xxmaj the dialogue is spectacular , and is delivered with such rapid - fire speed that one viewing is not enough . xxmaj the film comedy was elevated to new heights with xxmaj howard xxmaj hawks outstanding direction . xxmaj based on the classic play \" the xxmaj front xxmaj page \" , xxmaj hawks gives it a delightful twist by presenting\n    \n    \n      1\n      xxmaj woody xxmaj woodpecker , \" duck xxmaj amuck \" and especially \" one xxmaj froggy xxmaj evening \" show up how weak this movie is in comparison . xxmaj plus the movie fits in shambolic slapstick alongside strained sentiment ( the underlying theme of the story is family ; our hero is n't ready to have a son , and his nemesis - xxmaj alan xxmaj cumming as the xxmaj norse\n      woody xxmaj woodpecker , \" duck xxmaj amuck \" and especially \" one xxmaj froggy xxmaj evening \" show up how weak this movie is in comparison . xxmaj plus the movie fits in shambolic slapstick alongside strained sentiment ( the underlying theme of the story is family ; our hero is n't ready to have a son , and his nemesis - xxmaj alan xxmaj cumming as the xxmaj norse god\n    \n  \n\n\n\n\nlearn = language_model_learner(\n    dls_lm,\n    AWD_LSTM,\n    drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]\n).to_fp16()\n\nI fine-tuned the model for one epoch and saved it to load and use later. language_model_learner automatically freezes the pretrained model so it trains only the randomly instantiated embeddings representing the IMDb vocab.\n\nlearn.fit_one_cycle(1, 2e-2)\n\nPaperspace’s file browser is located at /notebooks so I change the learn.path to that location:\n\nlearn.path = Path('/notebooks')\n\nI then save the learner so that it saves the trained embeddings.\n\nlearn.save('1epoch')\n\nPath('/notebooks/models/1epoch.pth')\n\n\nLater on, I load the saved model, unfreeze the layers of the pretrained language model and fine-tune it for 10 epochs on the IMDb reviews dataset at a smaller learning rate (as shown in the fastai text):\n\nlearn = learn.load('1epoch')\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      4.214371\n      4.114542\n      0.300169\n      61.224136\n      41:36\n    \n    \n      1\n      3.917021\n      3.850335\n      0.316820\n      47.008827\n      42:00\n    \n    \n      2\n      3.752428\n      3.724050\n      0.326502\n      41.431866\n      42:13\n    \n    \n      3\n      3.660530\n      3.660284\n      0.331666\n      38.872364\n      42:32\n    \n    \n      4\n      3.560096\n      3.620281\n      0.335297\n      37.348042\n      42:36\n    \n    \n      5\n      3.507077\n      3.592660\n      0.338347\n      36.330578\n      42:44\n    \n    \n      6\n      3.430038\n      3.575986\n      0.340261\n      35.729839\n      42:39\n    \n    \n      7\n      3.360812\n      3.566898\n      0.341806\n      35.406578\n      42:53\n    \n    \n      8\n      3.310551\n      3.567138\n      0.342046\n      35.415089\n      43:28\n    \n    \n      9\n      3.297931\n      3.570799\n      0.341944\n      35.544979\n      44:01\n    \n  \n\n\n\nIOPub message rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_msg_rate_limit`.\n\nCurrent values:\nServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\n\n\nWe save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder.\n\nlearn.save_encoder('imdb_finetuned')\n\nBefore we fine-tune the model to be a classifier, the textbook has us generate random reviews:\n\nTEXT = 'I liked this movie because'\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)]\n\nprint(\"\\n\".join(preds))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni liked this movie because it showed a lot of normal people in America about who we belong and what they say and do . \n\n The acting was great , the story was fun and enjoyable and the movie was very well\ni liked this movie because my family and i are great Canadians and also Canadians , especially the Canadians . This is not a Canadian and American movie , but instead of being a \" mockumentary \" about the\n\n\nThe reviews are certainly not polished, but it’s still fascinating to see how the model predicts the next word to create a somewhat sensical review."
  },
  {
    "objectID": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tune-the-text-classifier",
    "href": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tune-the-text-classifier",
    "title": "Fine-Tuning a Language Model as a Text Classifier",
    "section": "Fine-tune the Text Classifier",
    "text": "Fine-tune the Text Classifier\nFor the final piece of this lesson, we move from language model to classifier, starting with creating the classifier DataLoaders.\nWe pass it the vocab of the language model to make sure we use the same correspondence of token to index, so that the embeddings learned in the fine-tuned language model can be applied to the classifier.\nThe dependent variable in this classifier is the label of the parent folder, pos for positive and neg for negative.\nFinally, we don’t pass is_lm=True to the TextBlock since it’s False by default (which we want in this case because we have labeled data, and don’t want to use next token as the label).\n\n(path/'train').ls()\n\n(#4) [Path('/root/.fastai/data/imdb/train/pos'),Path('/root/.fastai/data/imdb/train/unsupBow.feat'),Path('/root/.fastai/data/imdb/train/neg'),Path('/root/.fastai/data/imdb/train/labeledBow.feat')]\n\n\n\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n    get_y = parent_label,\n    get_items = partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\nThe independent variable is the movie review and the dependent variable is the sentiment (positive, pos, or negative, neg):\n\ndls_clas.show_batch(max_n=3)\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero\n      pos\n    \n    \n      1\n      xxbos xxmaj by now you 've probably heard a bit about the new xxmaj disney dub of xxmaj miyazaki 's classic film , xxmaj laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky . xxmaj during late summer of 1998 , xxmaj disney released \" kiki 's xxmaj delivery xxmaj service \" on video which included a preview of the xxmaj laputa dub saying it was due out in \" 1 xxrep 3 9 \" . xxmaj it 's obviously way past that year now , but the dub has been finally completed . xxmaj and it 's not \" laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky \" , just \" castle xxmaj in xxmaj the xxmaj sky \" for the dub , since xxmaj laputa is not such a nice word in xxmaj spanish ( even though they use the word xxmaj laputa many times\n      pos\n    \n    \n      2\n      xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it\n      pos\n    \n  \n\n\n\nEach batch has to have tensors of the same size, so fastai does the following (when using a TextBlock with is_lm=False):\n\nBatch together texts that are roughly the same lengths (by sorting the documents by length prior to each epoch).\nExpand the shortest texts to make them all the same size (as the largest document in the batch) by padding them with a special padding token that will be ignored by the model.\n\nLet’s create the model to classify texts:\n\nlearn = text_classifier_learner(\n    dls_clas, \n    AWD_LSTM, \n    drop_mult=0.5, \n    metrics=accuracy\n).to_fp16()\n\nLoad the encoder from our fine-tuned language model:\n\nlearn.path = Path('/notebooks')\n\n\nlearn = learn.load_encoder('imdb_finetuned')\n\nThe last step is to train with discriminative learning rates and gradual unfreezing. For NLP classifiers the text recommends unfreezing a few layers at a time to achieve the best performance:\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.245777\n      0.174727\n      0.934000\n      01:48\n    \n  \n\n\n\nWe get a similar accuracy as the textbook value (0.929320).\nNext, train the model with all layers except the last two parameter groups frozen:\n\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.226701\n      0.161235\n      0.938800\n      01:59\n    \n  \n\n\n\nThe accuracy improved a bit!\nUnfreeze the third parameter group and keep training:\n\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.188972\n      0.147045\n      0.946440\n      02:43\n    \n  \n\n\n\nThe accuracy continues to improve.\nFinally, train the whole model:\n\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.163849\n      0.143639\n      0.947600\n      03:18\n    \n    \n      1\n      0.149648\n      0.144494\n      0.947840\n      03:19\n    \n  \n\n\n\nWe’ll test the model with a few low-hanging-fruit inputs:\n\nlearn.predict(\"I really like this movie!\")\n\n\n\n\n\n\n\n\n('pos', tensor(1), tensor([0.0034, 0.9966]))\n\n\n\nlearn.predict(\"I really did not like this movie!\")\n\n\n\n\n\n\n\n\n('neg', tensor(0), tensor([0.9985, 0.0015]))\n\n\n\nlearn.predict(\"I'm not sure if I loved or hated this movie\")\n\n\n\n\n\n\n\n\n('neg', tensor(0), tensor([0.6997, 0.3003]))\n\n\nTo recap, here are the three steps that were involved in creating the IMDb movie review classifier:\n\nA language model was pretrained on all of Wikipedia.\nWe then fine-tuned that model on 100k IMDb movie reviews (documents).\nUsing the encoder from the fine-tuned language model, we created a classification model and fine-tuned it for a few epochs, gradually unfreezing layers for consecutive epochs. This model accurately classifies movie review as positive or negative.\n\nThat’s a wrap for this exercise. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "",
    "text": "In this blog post, I’ll run a fun little experiment which uses the code Jeremy Howard wrote in Getting started with NLP for absolute beginners to train an NLP classifier to predict whether or not a passenger on the titanic survived.\nI’ll start by acknowledging the obvious—that training an NLP model for tabular data that doesn’t contain much natural language is probably not going to give great results. However, it gives me an opportunity to use a simple dataset (that I’ve worked with before and am familiar with) to train a model following a process that is new to me (using the HuggingFace library). With that disclaimer out of the way, let’s jump in!"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#plan-of-attack",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#plan-of-attack",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nJeremy’s example uses tabular data with columns containing natural language and some additional data to predict values between 0 and 1 (0 means the two phrases are not similar in meaning, 1 means they are similar). Fundamentally, my dataset works in the same way—I have a bunch of columns describing features of the passengers and then a value of 0 (died) or 1 (survived) that I’m trying to predict.\n\nPreparing the Data\nThe data preparation step will be similar—I will concatenate multiple columns with a separator between each term.\n\n\nTraining Process\nI’ll use the same model (and thus tokenizer) as Jeremy did, so the training setup will be much of the same.\n\n\nMetrics\nJeremy used Pearson’s correlation coefficient (as specified by the Kaggle competition the dataset came from). In my case, I’ll need to figure out how to pass accuracy to the HuggingFace Trainer."
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#load-and-prep-the-data",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#load-and-prep-the-data",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Load and Prep the Data",
    "text": "Load and Prep the Data\nI’ll start by using the boilerplate code Jeremy has provided to get data from Kaggle.\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 1.45MB/s]\n\n\n\n\n\n\n\n\n\n# load the training data and look at it\nimport torch, numpy as np, pandas as pd\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe only data cleaning I’ll do is fill missing values with the mode of each column:\n\nmodes = df.mode().iloc[0]\ndf.fillna(modes, inplace=True)\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\nI’ll also set my independent variable as a float to resolve an error I got during training (\"mse_cuda\" not implemented for 'Long').\n\ndf['Survived'] = df['Survived'].astype(float)\n\nI’ll next create an input column which creates the input to the model:\n\ndf['input'] = 'Pclass: ' + df.Pclass.apply(str) +\\\n '; Name: ' + df.Name + '; Sex: ' + df.Sex + '; Age: ' + df.Age.apply(str) +\\\n  '; SibSp: ' + df.SibSp.apply(str) + '; Parch: ' + df.Parch.apply(str) +\\\n  '; Ticket: ' + df.Ticket + '; Fare: ' + df.Fare.apply(str) + \\\n  '; Cabin: ' + df.Cabin + '; Embarked: ' + df.Embarked\n\n\ndf['input'][0]\n\n'Pclass: 3; Name: Braund, Mr. Owen Harris; Sex: male; Age: 22.0; SibSp: 1; Parch: 0; Ticket: A/5 21171; Fare: 7.25; Cabin: B96 B98; Embarked: S'"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#tokenization",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#tokenization",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Tokenization",
    "text": "Tokenization\n\n! pip install datasets transformers[sentencepiece] accelerate -U\n\n\nfrom datasets import Dataset,DatasetDict\n\nI’ll remove 100 rows of data to serve as a test set for final predictions after the model is trained.\n\n# create a random sample of 100 passengers\neval_df = df.sample(100)\n\n\neval_df.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      input\n    \n  \n  \n    \n      709\n      710\n      1.0\n      3\n      Moubarek, Master. Halim Gonios (\"William George\")\n      male\n      24.0\n      1\n      1\n      2661\n      15.2458\n      B96 B98\n      C\n      Pclass: 3; Name: Moubarek, Master. Halim Gonio...\n    \n    \n      439\n      440\n      0.0\n      2\n      Kvillner, Mr. Johan Henrik Johannesson\n      male\n      31.0\n      0\n      0\n      C.A. 18723\n      10.5000\n      B96 B98\n      S\n      Pclass: 2; Name: Kvillner, Mr. Johan Henrik Jo...\n    \n    \n      840\n      841\n      0.0\n      3\n      Alhomaki, Mr. Ilmari Rudolf\n      male\n      20.0\n      0\n      0\n      SOTON/O2 3101287\n      7.9250\n      B96 B98\n      S\n      Pclass: 3; Name: Alhomaki, Mr. Ilmari Rudolf; ...\n    \n    \n      720\n      721\n      1.0\n      2\n      Harper, Miss. Annie Jessie \"Nina\"\n      female\n      6.0\n      0\n      1\n      248727\n      33.0000\n      B96 B98\n      S\n      Pclass: 2; Name: Harper, Miss. Annie Jessie \"N...\n    \n    \n      39\n      40\n      1.0\n      3\n      Nicola-Yarred, Miss. Jamila\n      female\n      14.0\n      1\n      0\n      2651\n      11.2417\n      B96 B98\n      C\n      Pclass: 3; Name: Nicola-Yarred, Miss. Jamila; ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nI’ll remove these 100 rows from the original DataFrame which I will use for training and validation sets.\n\ndf = df.drop(eval_df.index)\n\n\ndf.shape\n\n(791, 13)\n\n\n\nds = Dataset.from_pandas(df)\n\n\neval_ds = Dataset.from_pandas(eval_df)\n\n\nds\n\nDataset({\n    features: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__'],\n    num_rows: 791\n})\n\n\n\neval_ds\n\nDataset({\n    features: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__'],\n    num_rows: 100\n})\n\n\nI’ll use the same model as in Jeremy’s example:\n\nmodel_nm = 'microsoft/deberta-v3-small'\n\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\nI’ll check the tokenizer:\n\ntokz.tokenize(\"We are about to tokenize this dataset!\")\n\n['▁We', '▁are', '▁about', '▁to', '▁token', 'ize', '▁this', '▁dataset', '!']\n\n\n\n# function to tokenize inputs\ndef tok_func(x): return tokz(x[\"input\"])\n\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\n\neval_ds = eval_ds.map(tok_func, batched=True)\n\n\n\n\n\nrow = tok_ds[0]\nrow['input'], row['input_ids']\n\n('Pclass: 3; Name: Braund, Mr. Owen Harris; Sex: male; Age: 22.0; SibSp: 1; Parch: 0; Ticket: A/5 21171; Fare: 7.25; Cabin: B96 B98; Embarked: S',\n [1,\n  916,\n  4478,\n  294,\n  404,\n  346,\n  5445,\n  294,\n  24448,\n  407,\n  261,\n  945,\n  260,\n  12980,\n  6452,\n  346,\n  23165,\n  294,\n  2844,\n  346,\n  5166,\n  294,\n  1460,\n  260,\n  693,\n  346,\n  42209,\n  32154,\n  294,\n  376,\n  346,\n  916,\n  22702,\n  294,\n  767,\n  346,\n  14169,\n  294,\n  336,\n  320,\n  524,\n  1259,\n  30877,\n  346,\n  40557,\n  294,\n  574,\n  260,\n  1883,\n  346,\n  22936,\n  294,\n  736,\n  8971,\n  736,\n  8454,\n  346,\n  77030,\n  569,\n  294,\n  662,\n  2])\n\n\nI’ll look at the index for some of the words in the input to check that they are present in the input_ids column:\n\ntokz.vocab['▁P']\n\n916\n\n\n\ntokz.vocab['▁3']\n\n404\n\n\n\ntokz.vocab['▁Name']\n\n5445\n\n\nTransformers expects the independent variable to be named labels:\n\ntok_ds = tok_ds.rename_columns({'Survived':'labels'})\n\n\ntok_ds\n\nDataset({\n    features: ['PassengerId', 'labels', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 791\n})"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#preparing-training-and-validation-sets",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#preparing-training-and-validation-sets",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Preparing Training and Validation Sets",
    "text": "Preparing Training and Validation Sets\nSince I cut into my training and validation set by pulling out a test set, I’ll use a smaller split for the validation set.\n\ndds = tok_ds.train_test_split(0.15, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['PassengerId', 'labels', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 672\n    })\n    test: Dataset({\n        features: ['PassengerId', 'labels', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 119\n    })\n})"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#creating-an-accuracy-function",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#creating-an-accuracy-function",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Creating an Accuracy Function",
    "text": "Creating an Accuracy Function\nSince my independent variable is binary (0 or 1), I’ll create an accuracy function with the following:\n\nIf predictions are greater than 0.5, classify them as 1, if less than 0.5, classify they as 0.\nCompare predictions to the labels and take the mean value of the boolean array which will be the % of correctly predicted values.\n\n\ndef calculate_accuracy(preds, labels):\n  return torch.tensor(((preds>0.5)==labels)).float().mean().item()\n\n# Transformers want a dictionary for the metric\ndef acc_d(eval_pred): return {'accuracy': calculate_accuracy(*eval_pred) }"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#training-the-model",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#training-the-model",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Training the Model",
    "text": "Training the Model\nI’ll use the same code as is shown in Jeremy’s notebook for preparing the Trainer:\n\nfrom transformers import TrainingArguments,Trainer\n\n\nbs = 128\nepochs = 4\n\nI’ll use the same learning rate as the example to start with:\n\nlr = 8e-5\n\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=acc_d)\n\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ntrainer.train();\n\n\n\n    \n      \n      \n      [24/24 00:08, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Accuracy\n    \n  \n  \n    \n      1\n      No log\n      0.253301\n      0.462185\n    \n    \n      2\n      No log\n      0.246423\n      0.537815\n    \n    \n      3\n      No log\n      0.223734\n      0.537815\n    \n    \n      4\n      No log\n      0.216874\n      0.747899\n    \n  \n\n\n\nI trained the model a few times and noticed that the accuracy varied significantly. For some trainings, it was stuck at around 0.56, for others, it went from 0.4 to 0.5 to 0.6. In this final training, it jumped from 0.54 to 0.75 in the final epoch. I think this means that the combination of data and hyperparameters is causing an unstable training regime for this model.\nLet’s look at some of the predictions on the test set:\n\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds[:10], preds.shape\n\n\n\n\n(array([[0.53808594],\n        [0.16943359],\n        [0.14575195],\n        [0.52392578],\n        [0.50390625],\n        [0.52539062],\n        [0.52734375],\n        [0.14221191],\n        [0.52001953],\n        [0.53320312]]),\n (100, 1))\n\n\nI’ll calculate the accuracy for the test set:\n\ntorch.tensor((preds.squeeze(1)>0.5) == eval_df['Survived'].values).float().mean().item()\n\n0.8100000023841858\n\n\nNot bad! I get an 81% accuracy on my test set. The linear, neural net, deep neural net and fastai tabular_learner model achieved an accuracy of about 83% on their validation sets."
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#final-thoughts",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#final-thoughts",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nOverall I found this exercise enjoyable. I learned a little bit more about using HuggingFace Transfomers, and better understand what Jeremy did in his example notebook. I am not confident in this model or approach as I did notice the training was unstable (highly varying accuracy across different trainings), and this dataset is not really meant for an NLP model. I also had a relatively smaller number of rows than the example that Jeremy showed. That being said, my model wasn’t a complete dud as it mostly accurately predicted who survived in my test set."
  },
  {
    "objectID": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html",
    "href": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html",
    "title": "Calculating the Ratio of Letter Perimeter to Area",
    "section": "",
    "text": "In this notebook I’ll walk through an algorithm suggested by Claude to distinguish one typeface (like display) from another (like serif) in which we calculate the ratio of the perimeter to area of each letter. This algorithm is relatively simple (utilizing the power of the OpenCV library).\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom google.colab.patches import cv2_imshow"
  },
  {
    "objectID": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#load-and-binarize-the-image",
    "href": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#load-and-binarize-the-image",
    "title": "Calculating the Ratio of Letter Perimeter to Area",
    "section": "Load and Binarize the Image",
    "text": "Load and Binarize the Image\nAs usual, we’ll load the image and binarize it so the text is white and the background is black.\n\npath = 'serif-76px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#calculate-the-ratio-of-contour-perimeter-to-area",
    "href": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#calculate-the-ratio-of-contour-perimeter-to-area",
    "title": "Calculating the Ratio of Letter Perimeter to Area",
    "section": "Calculate the Ratio of Contour Perimeter to Area",
    "text": "Calculate the Ratio of Contour Perimeter to Area\nNext, we calculate the contours in the image\n\ncontours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\nI’ll visualize the contours to show what we’re dealing with. As you can see—the contours are essentially the letter boundaries.\n\ncontour_image = np.zeros((binary.shape[0], binary.shape[1], 3), dtype=np.uint8)\ncv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)\ncv2_imshow(contour_image)\n\n\n\n\n\n\n\n\nWe then calculate the total perimeter and total area of all contours:\n\ntotal_perimeter = sum(cv2.arcLength(contour, True) for contour in contours)\ntotal_area = sum(cv2.contourArea(contour) for contour in contours)\n\nAnd take the ratio of the two:\n\nratio = total_perimeter / total_area if total_area > 0 else 0\nratio\n\n0.3750295759392109"
  },
  {
    "objectID": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#calculating-contour-ratio-for-multiple-images",
    "href": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#calculating-contour-ratio-for-multiple-images",
    "title": "Calculating the Ratio of Letter Perimeter to Area",
    "section": "Calculating Contour Ratio for Multiple Images",
    "text": "Calculating Contour Ratio for Multiple Images\nI’ll wrap the above functionality (except for the contour visualization) into a function and calculate the ratio for different images of different typefaces.\n\ndef contour_ratio(image_path):\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    total_perimeter = sum(cv2.arcLength(contour, True) for contour in contours)\n    total_area = sum(cv2.contourArea(contour) for contour in contours)\n\n    ratio = total_perimeter / total_area if total_area > 0 else 0\n\n    return ratio\n\nOn average, images with serif fonts have a higher contour ratio (perimeter:area) than images with display fonts. This matches my intuition: serif fonts have more detailed elements (the serifs) which increase the perimeter of the shape for a given area.\n\nszs = [8, 18, 24, 36, 76, 240, 330, 420]\nts = ['display', 'serif']\nres = []\n\nfor t in ts:\n    for sz in szs:\n        image_path = f\"{t}-{sz}px.png\"\n        sr = contour_ratio(image_path)\n        res.append([t, sz, sr])\n\nres = pd.DataFrame(res, columns=['typeface', 'font-size', 'contour-ratio'])\nres.groupby('typeface')['contour-ratio'].agg(['mean', 'median'])\n\n\n\n  \n    \n\n\n  \n    \n      \n      mean\n      median\n    \n    \n      typeface\n      \n      \n    \n  \n  \n    \n      display\n      0.577296\n      0.345773\n    \n    \n      serif\n      0.781032\n      0.548321\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nFor all font sizes (except 72px) this trend is evident: the serif texts have a larger perimeter:area ratio than the sans serif display texts.\n\nres.sort_values(by='font-size')\n\n\n\n  \n    \n\n\n  \n    \n      \n      typeface\n      font-size\n      contour-ratio\n    \n  \n  \n    \n      0\n      display\n      8\n      2.050135\n    \n    \n      8\n      serif\n      8\n      2.218431\n    \n    \n      1\n      display\n      18\n      0.992742\n    \n    \n      9\n      serif\n      18\n      1.530445\n    \n    \n      2\n      display\n      24\n      0.730834\n    \n    \n      10\n      serif\n      24\n      1.106150\n    \n    \n      3\n      display\n      36\n      0.459297\n    \n    \n      11\n      serif\n      36\n      0.721613\n    \n    \n      4\n      display\n      76\n      0.232250\n    \n    \n      12\n      serif\n      76\n      0.375030\n    \n    \n      5\n      display\n      240\n      0.074272\n    \n    \n      13\n      serif\n      240\n      0.115825\n    \n    \n      6\n      display\n      330\n      0.046465\n    \n    \n      14\n      serif\n      330\n      0.084438\n    \n    \n      7\n      display\n      420\n      0.032374\n    \n    \n      15\n      serif\n      420\n      0.096324"
  },
  {
    "objectID": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#final-thoughts",
    "href": "posts/2024-09-06-typefaceclassifier-contour-ratio/index.html#final-thoughts",
    "title": "Calculating the Ratio of Letter Perimeter to Area",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAmong the algorithms I’ve tested, this one demonstrates the highest consistency in differentiating typefaces, regardless of font size, making it a great candidate for a non-ML typeface classification baseline.\nI’m also a huge fan of simplicity and this is one of the simplest algorithms for this task that I have implemented. A win-win!\nI hope you enjoyed this blog post. Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-13-tinystories-33m-finetune/index.html",
    "href": "posts/2024-08-13-tinystories-33m-finetune/index.html",
    "title": "Fine-tuning TinyStories-33M on the financial_phrasebank Dataset",
    "section": "",
    "text": "In this notebook, I’ll fine-tune different TinyStories base models on the financial_phrasebank dataset to perform sentiment classification on financial news text. In a previous blog post I showed that TinyStories-Instruct-33M does not follow even simple instructions (e.g., “What is the color an apple?”) that deviate from its training data, so that motivated me to finetune these models.\nThe TinyStories paper doesn’t include any hyperparameters (of particular interest are the learning rate and batch size) used to train their models so I’ll experiment with different values.\n\n\nShow imports and setup\n#!pip install accelerate evaluate datasets -Uqq\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, TrainerCallback\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nimport gc\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nmodel_nm = \"roneneldan/TinyStories-33M\"\n#model_nm = \"roneneldan/TinyStories-1M\"\n#model_nm = \"roneneldan/TinyStories-3M\"\n#model_nm = \"roneneldan/TinyStories-8M\"\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\ndef tok_func(x): return tokz(x[\"input\"], padding=True, truncation=True)\n\n\n2024-08-14 01:38:25.564071: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n\n2024-08-14 01:38:25.564133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n\n2024-08-14 01:38:25.565856: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n\nShow load_dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\",\n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\n# # Source: https://huggingface.co/blog/synthetic-data-save-costs\n# # create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\n# label_map = {\n#     i: label_text\n#     for i, label_text in enumerate(dataset.features[\"label\"].names)\n# }\n\n# def add_label_text(example):\n#     example[\"labels\"] = label_map[example[\"label\"]]\n#     return example\n\n# dataset = dataset.map(add_label_text)"
  },
  {
    "objectID": "posts/2024-08-13-tinystories-33m-finetune/index.html#initial-fine-tune",
    "href": "posts/2024-08-13-tinystories-33m-finetune/index.html#initial-fine-tune",
    "title": "Fine-tuning TinyStories-33M on the financial_phrasebank Dataset",
    "section": "Initial Fine-Tune",
    "text": "Initial Fine-Tune\n\nTokenize the Dataset\nThe HuggingFace Trainer, if I’m not mistaken, expects the target variable to have the title labels and the independent variable titled input for classification tasks:\n\ndataset = dataset.rename_columns({'label':'labels', 'sentence': 'input'})\n\n\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ntokz.padding_side = \"left\" # https://github.com/huggingface/transformers/issues/16595 and https://www.kaggle.com/code/baekseungyun/gpt-2-with-huggingface-pytorch\n\n\ntok_ds = dataset.map(tok_func, batched=True)\ntok_ds\n\nDataset({\n    features: ['input', 'labels', 'input_ids', 'attention_mask'],\n    num_rows: 2264\n})\n\n\n\ntok_ds[0]['input']\n\n'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'\n\n\n\ntok_ds[0]['input_ids'][100:110] # first 100 elements are 50257 ('[PAD]')\n\n[50257, 50257, 50257, 50257, 50257, 50257, 4821, 284, 17113, 837]\n\n\n\ntokz.decode(50257), tokz.decode(4821), tokz.decode(284), tokz.decode(17113)\n\n('[PAD]', 'According', ' to', ' Gran')\n\n\n\ntok_ds[0]['labels']\n\n1\n\n\n\n\nTrain-Valid-Test Split\nThe financial_phrasebank dataset doesn’t have a default test split, so I’ll use 225 sentences (~10%) as the test set. I’ll split the remaining data into an 80/20 train/validation set.\n\nsplit_dataset = tok_ds.train_test_split(test_size=225/2264, seed=42)\n\ntraining_split = split_dataset['train'].train_test_split(test_size=0.2, seed=42)\n\ntrain_ds = training_split['train']\neval_ds = training_split['test']\ntest_ds = split_dataset['test']\n\ntrain_ds, eval_ds, test_ds\n\n(Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 1631\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 408\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 225\n }))\n\n\n\ntrain_ds[0]['input']\n\n'The result will also be burdened by increased fixed costs associated with operations in China , and restructuring costs in Japan .'\n\n\n\ntrain_ds[0]['labels']\n\n0\n\n\nLooking at the distribution of label values (negative, positive, neutral) in each dataset, they contain roughly the same split:\n\ntrain_ds.to_pandas()['labels'].value_counts() / len(train_ds)\n\nlabels\n1    0.622318\n2    0.251993\n0    0.125690\nName: count, dtype: float64\n\n\n\neval_ds.to_pandas()['labels'].value_counts() / len(eval_ds)\n\nlabels\n1    0.615196\n2    0.257353\n0    0.127451\nName: count, dtype: float64\n\n\n\ntest_ds.to_pandas()['labels'].value_counts() / len(test_ds)\n\nlabels\n1    0.555556\n2    0.240000\n0    0.204444\nName: count, dtype: float64\n\n\n\n\nInitial Training Run\nI’ll define some initial hyperparameters, re-using most of what was used in Part 1 of the fastai course in Jeremy’s “Getting started with NLP for absolute beginners” notebook.\n\nlr = 8e-5\nepochs = 3\nbs = 16\n\nI’ll use accuracy as my metric:\n\ndef get_acc(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": (predictions == labels).astype(np.float32).mean().item()}\n\nI’ll also try to implement a callback (to capture the metrics later on when I’m doing hyperparameter sweeps).\n\n\nShow MetricCallback code\n# thanks Claude\n\nclass MetricCallback(TrainerCallback):\n    def __init__(self):\n        self.metrics = []\n        self.current_epoch_metrics = {}\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            self.current_epoch_metrics.update(logs)\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        if hasattr(state, 'log_history') and state.log_history:\n            # Get the last logged learning rate\n            last_lr = state.log_history[-1].get('learning_rate', None)\n        else:\n            last_lr = None\n\n        self.metrics.append({\n            \"epoch\": state.epoch,\n            \"learning_rate\": last_lr,\n            **self.current_epoch_metrics\n        })\n        self.current_epoch_metrics = {}  # Reset for next epoch\n\n    def on_train_end(self, args, state, control, **kwargs):\n        # Capture final metrics after the last epoch\n        if self.current_epoch_metrics:\n            self.metrics.append({\n                \"epoch\": state.num_train_epochs,\n                \"learning_rate\": self.metrics[-1].get('learning_rate') if self.metrics else None,\n                **self.current_epoch_metrics\n            })\n\n\n\nmetric_callback = MetricCallback()\n\n\nargs = TrainingArguments(\n    'outputs',\n    learning_rate=lr,\n    warmup_ratio=0.1,\n    lr_scheduler_type='cosine',\n    fp16=True,\n    eval_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    per_device_train_batch_size=bs,\n    per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    report_to='none')\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=3) # 3 labels for 3 classes\ntrainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=eval_ds, \n                  tokenizer=tokz, compute_metrics=get_acc, callbacks=[metric_callback])\n\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at roneneldan/TinyStories-33M and are newly initialized: ['score.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nmodel.resize_token_embeddings(len(tokz)) # do this otherwise I get a \"index out of range\" error\n\nEmbedding(50258, 768)\n\n\n\nmodel.config.pad_token_id = model.config.eos_token_id # do this otherwise I get an error about padding tokens\n\n\ntrainer.train();\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n    \n      \n      \n      [153/153 00:34, Epoch 3/3]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Accuracy\n    \n  \n  \n    \n      1\n      0.809300\n      0.450100\n      0.833333\n    \n    \n      2\n      0.192700\n      0.429838\n      0.889706\n    \n    \n      3\n      0.010500\n      0.751890\n      0.879902\n    \n  \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\nThe metric_callback stores the loss, accuracy and runtime (among other metrics) for each epoch:\n\nresults = []\nresults.append({\"learning_rate\": lr, \"metrics\": metric_callback.metrics})\n\n\n\nShow function to convert results dict into DataFrame\ndef results_to_dataframe(results, model_name):\n    rows = []\n    for result in results:\n        initial_lr = result['learning_rate']\n        for metric in result['metrics']:\n            row = {\n                'model_name': model_name,\n                'initial_learning_rate': initial_lr,\n                'current_learning_rate': metric.get('learning_rate'),\n            }\n            row.update(metric)\n            rows.append(row)\n    \n    df = pd.DataFrame(rows)\n    \n    # Ensure specific columns are at the beginning\n    first_columns = ['model_name', 'initial_learning_rate', 'current_learning_rate', 'epoch']\n    other_columns = [col for col in df.columns if col not in first_columns]\n    df = df[first_columns + other_columns]\n    \n    return df\n\n\n\nres_df = results_to_dataframe(results, model_name=\"TinyStories-33M\")\nres_df\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      0\n      TinyStories-33M\n      0.00008\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      TinyStories-33M\n      0.00008\n      0.000068\n      1.0\n      0.000068\n      0.8093\n      9.747781e+05\n      0.450100\n      0.833333\n      0.8285\n      492.428\n      8.449\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      TinyStories-33M\n      0.00008\n      0.000024\n      2.0\n      0.000024\n      0.1927\n      1.633982e+06\n      0.429838\n      0.889706\n      0.8315\n      490.653\n      8.418\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TinyStories-33M\n      0.00008\n      0.000000\n      3.0\n      0.000000\n      0.0105\n      2.495563e+04\n      0.751890\n      0.879902\n      0.9311\n      438.202\n      7.518\n      35.2349\n      138.868\n      4.342\n      1.090163e+14\n      0.337512\n    \n  \n\n\n\n\n\n\nEvaluating Model Performance on the Test Set\nI’ll see how well the model does on the 10% test set that I’ve set aside.\n\ntest_df = test_ds.to_pandas()[['input', 'labels']]\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      input\n      labels\n    \n  \n  \n    \n      0\n      Indigo and Somoncom serve 377,000 subscribers ...\n      1\n    \n    \n      1\n      The sellers were EOSS Innovationsmanagement an...\n      1\n    \n    \n      2\n      UPM-Kymmene said its has ` not indicated any i...\n      1\n    \n    \n      3\n      These financing arrangements will enable the c...\n      2\n    \n    \n      4\n      Fortum expects its annual capital expenditure ...\n      1\n    \n  \n\n\n\n\n\npreds = trainer.predict(test_ds).predictions.astype(float)\nprobs = F.softmax(torch.tensor(preds), dim=1)\npredicted_classes = torch.argmax(probs, dim=1).numpy()\n\ntest_df['predicted'] = predicted_classes\ntest_df.head()\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      input\n      labels\n      predicted\n    \n  \n  \n    \n      0\n      Indigo and Somoncom serve 377,000 subscribers ...\n      1\n      1\n    \n    \n      1\n      The sellers were EOSS Innovationsmanagement an...\n      1\n      1\n    \n    \n      2\n      UPM-Kymmene said its has ` not indicated any i...\n      1\n      1\n    \n    \n      3\n      These financing arrangements will enable the c...\n      2\n      1\n    \n    \n      4\n      Fortum expects its annual capital expenditure ...\n      1\n      1\n    \n  \n\n\n\n\nThis training run resulted in a TinyStories-33M finetuned model that predicted 85% of the 225 test sentences’ sentiment correctly.\n\ntest_df['match'] = test_df['labels'] == test_df['predicted']\ntest_df['match'].mean()\n\n0.8488888888888889\n\n\nI’ll reuse a confusion matrix helper function I created in a previous notebook:\n\nlabel_map = {i: label_text for i, label_text in enumerate(test_ds.features[\"labels\"].names)}\nlabel_map\n\n{0: 'negative', 1: 'neutral', 2: 'positive'}\n\n\n\ntest_df['label_text'] = test_df['labels'].apply(lambda x: label_map[x])\ntest_df['pred_text'] = test_df['predicted'].apply(lambda x: label_map[x])\n\n\n\nShow function to make confusion matrix\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n    \n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['pred_text'], labels=['negative', 'neutral', 'positive'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive'])\n    \n    fig, ax = plt.subplots(figsize=(4,4))\n    disp.plot(ax=ax,text_kw={'fontsize': 12}, cmap='Blues', colorbar=False);\n    \n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    \n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=14)\n    ax.tick_params(axis='x', labelsize=14)\n\n\nThe model predicted neutral sentences with the highest accuracy (122/125), followed by negative sentences (32/46) and finally positive sentences (37/54).\n\nmake_cm(test_df)\n\n\n\n\n\n\n\n\nAs a final sanity check, I’ll prompt the model with a made-up financial new sentence and see if it correctly classifies it (as positive, negative, or neutral):\n\n\nShow function to generate a prediction\ndef get_prediction(model, text, tokz):\n    # Determine the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Move the model to the appropriate device\n    model = model.to(device)\n\n    # Tokenize the input text\n    inputs = tokz(text, return_tensors=\"pt\", truncation=True, padding=True)\n\n    # Move input tensors to the same device as the model\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Get the model's prediction\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Ensure logits are on CPU for numpy operations\n    logits = outputs.logits.detach().cpu()\n\n    # Get probabilities\n    probs = torch.softmax(logits, dim=-1)\n\n    # Get the predicted class\n    p_class = torch.argmax(probs, dim=-1).item()\n\n    # Get the probability for the predicted class\n    p = probs[0][p_class].item()\n\n    labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n    \n    print(f\"Probability: {p:.2f}\")\n    print(f\"Predicted label: {labels[p_class]}\")\n    return p_class, p\n\n\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(model, text, tokz)\n\nProbability: 0.60\n\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(model, text, tokz)\n\nProbability: 1.00\n\nPredicted label: negative\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(model, text, tokz)\n\nProbability: 0.93\n\nPredicted label: neutral"
  },
  {
    "objectID": "posts/2024-08-13-tinystories-33m-finetune/index.html#learning-rate-sweep",
    "href": "posts/2024-08-13-tinystories-33m-finetune/index.html#learning-rate-sweep",
    "title": "Fine-tuning TinyStories-33M on the financial_phrasebank Dataset",
    "section": "Learning Rate Sweep",
    "text": "Learning Rate Sweep\nLet’s see if I can beat the validation accuracy of 89% and test set accuracy of 85% by using different learning rates. I’ll wrap up some of the training code (there’s so much of it!!) in helper functions so I can loop through my learning rates. I’ve also created a helper function to get the test set accuracy.\n\n\nShow function to prep trainer\ndef get_trainer(lr, bs=16):\n\n    args = TrainingArguments(\n        'outputs',\n        learning_rate=lr,\n        warmup_ratio=0.1,\n        lr_scheduler_type='cosine',\n        fp16=True,\n        eval_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        per_device_train_batch_size=bs,\n        per_device_eval_batch_size=bs*2,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        report_to='none')\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=3) # 3 labels for 3 classes\n    model.resize_token_embeddings(len(tokz))\n    model.config.pad_token_id = model.config.eos_token_id\n    \n    trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=eval_ds, \n                  tokenizer=tokz, compute_metrics=get_acc, callbacks=[metric_callback])\n    \n    return trainer, args\n\n\n\n\nShow function to get test set accuracy\ndef get_test_df(trainer):\n    test_df = test_ds.to_pandas()[['input', 'labels']]\n    \n    preds = trainer.predict(test_ds).predictions.astype(float)\n    probs = F.softmax(torch.tensor(preds), dim=1)\n    predicted_classes = torch.argmax(probs, dim=1).numpy()\n\n    test_df['predicted'] = predicted_classes\n    \n    test_df['match'] = test_df['labels'] == test_df['predicted']\n    acc = test_df['match'].mean()\n    \n    label_map = {i: label_text for i, label_text in enumerate(test_ds.features[\"labels\"].names)}\n    test_df['label_text'] = test_df['labels'].apply(lambda x: label_map[x])\n    test_df['pred_text'] = test_df['predicted'].apply(lambda x: label_map[x])\n    \n    return test_df, acc\n\n\n\n\nShow training loop\nmetrics = []\ntrainers = []\nlearning_rates = [1e-6, 1e-5, 3e-5, 5e-5, 8e-5, 1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 1e-2, 1e-1]\n#learning_rates = [8e-5]\n\nfor lr in learning_rates:\n    print(f\"Learning Rate: {lr}\")\n    \n    metric_callback = MetricCallback()\n    \n    trainer, args = get_trainer(lr, bs=64)\n\n    trainer.train()\n\n    metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nmetrics_df = results_to_dataframe(metrics, model_name=\"TinyStories-33M\")\nmetrics_df = metrics_df.query('current_learning_rate.notna()')\n\n\nmetrics_df.head()\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      1\n      TinyStories-33M\n      0.000001\n      8.455313e-07\n      1.0\n      8.455313e-07\n      1.0189\n      1631760.000\n      0.955949\n      0.588235\n      0.7160\n      569.866\n      2.793\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      TinyStories-33M\n      0.000001\n      3.034875e-07\n      2.0\n      3.034875e-07\n      0.8713\n      1828322.000\n      0.902843\n      0.612745\n      0.7188\n      567.645\n      2.783\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TinyStories-33M\n      0.000001\n      0.000000e+00\n      3.0\n      0.000000e+00\n      0.8171\n      1519178.375\n      0.894423\n      0.620098\n      0.7479\n      545.515\n      2.674\n      25.9696\n      188.413\n      1.502\n      1.090163e+14\n      0.902434\n    \n    \n      5\n      TinyStories-33M\n      0.000010\n      8.455313e-06\n      1.0\n      8.455313e-06\n      1.2371\n      1272109.750\n      0.816271\n      0.617647\n      0.7218\n      565.288\n      2.771\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      TinyStories-33M\n      0.000010\n      3.034875e-06\n      2.0\n      3.034875e-06\n      0.5578\n      1321395.500\n      0.704009\n      0.713235\n      0.7366\n      553.892\n      2.715\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nLearning Rate with Highest Validation Accuracy\nThe learning rate with the highest validation set accuracy (85.5%) is 0.0005 (5e-4).\n\nmetrics_df.query('eval_accuracy == eval_accuracy.max()')\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      31\n      TinyStories-33M\n      0.0005\n      0.0\n      3.0\n      0.0\n      0.2443\n      389968.15625\n      0.406566\n      0.855392\n      0.7971\n      511.863\n      2.509\n      25.9521\n      188.54\n      1.503\n      1.090163e+14\n      0.819404\n    \n  \n\n\n\n\n\nlearning_rates[7]\n\n0.0005\n\n\nThe corresponding model has a 79% accuracy on the test set.\n\ntest_df, acc = get_test_df(trainers[7])\nacc\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n\n0.7911111111111111\n\n\nSimilar to before the model does well at predicting neutral sentences (119/125). This time, the model has a better accuracy with positive sentences (41/54) than negative sentences where it gets less than 50% correct (18/46).\n\nmake_cm(test_df)\n\n\n\n\n\n\n\n\nInterestingly enough, this model gets my made-up “positive” sentence incorrect (while getting the “neutral” and “negative” made-up ones correct):\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(trainers[7].model, text, tokz)\n\nProbability: 0.49\n\nPredicted label: neutral\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(trainers[7].model, text, tokz)\n\nProbability: 0.36\n\nPredicted label: negative\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(trainers[7].model, text, tokz)\n\nProbability: 0.54\n\nPredicted label: neutral\n\n\n\nRelationship between Learning Rate and Validation Accuracy\nThe validation set accuracy starts low, increases to a peak at lr=0.0005 and then decreases again. I’ve used a log-scale on the x-axis to more easily view all of the data.\n\n\nShow plotting code\nfinal_epoch_metrics = metrics_df.query(\"epoch == 3\")\nplt.scatter(final_epoch_metrics['initial_learning_rate'], final_epoch_metrics['eval_accuracy']);\nplt.xscale('log')\nplt.xlabel('Learning Rate (log scale)')\nplt.ylabel('Validation Set Accuracy')\nplt.title('Learning Rate vs. Final Epoch Validation Accuracy');\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Rate with Highest Test Set Accuracy\n\ntest_dfs = []\naccs = []\nfor t in trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\nThe learning rate of 0.0005 also had the highest test set accuracy.\n\naccs\n\n[0.6088888888888889,\n 0.6222222222222222,\n 0.7066666666666667,\n 0.7155555555555555,\n 0.72,\n 0.7511111111111111,\n 0.76,\n 0.7911111111111111,\n 0.6622222222222223,\n 0.6177777777777778,\n 0.5555555555555556,\n 0.5555555555555556]"
  },
  {
    "objectID": "posts/2024-08-13-tinystories-33m-finetune/index.html#training-with-the-best-learning-rate-10-times",
    "href": "posts/2024-08-13-tinystories-33m-finetune/index.html#training-with-the-best-learning-rate-10-times",
    "title": "Fine-tuning TinyStories-33M on the financial_phrasebank Dataset",
    "section": "Training with the Best Learning Rate 10 Times",
    "text": "Training with the Best Learning Rate 10 Times\nI’m curious how consistently this model trains so I’ll train it 10 times with the best-performing learning rate.\n\nbest_metrics = []\nbest_trainers = []\nlr = 0.0005\n\nfor i in range(10):\n    \n    metric_callback = MetricCallback()\n    trainer, args = get_trainer(lr=lr, bs=64)\n    trainer.train()\n\n    best_metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    best_trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\nbest_metrics_df = results_to_dataframe(best_metrics, model_name=\"TinyStories-33M\")\nbest_metrics_df = best_metrics_df.query('current_learning_rate.notna()')\nbest_metrics_df.head(3)\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      1\n      TinyStories-33M\n      0.0005\n      0.000423\n      1.0\n      0.000423\n      2.0251\n      2.925652e+05\n      0.693838\n      0.723039\n      0.7110\n      573.874\n      2.813\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      TinyStories-33M\n      0.0005\n      0.000152\n      2.0\n      0.000152\n      0.6226\n      1.006726e+06\n      0.634036\n      0.678922\n      0.7205\n      566.265\n      2.776\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TinyStories-33M\n      0.0005\n      0.000000\n      3.0\n      0.000000\n      0.3786\n      2.750663e+05\n      0.581288\n      0.769608\n      0.7977\n      511.497\n      2.507\n      26.0778\n      187.631\n      1.496\n      1.090163e+14\n      1.008766\n    \n  \n\n\n\n\nI would say this model’s training is quite consistent! Almost too consistent? Something feels weird about it getting the same accuracy for 9 out of the 10 runs.\n\nfinal_accs = best_metrics_df.query(\"epoch == 3\")['eval_accuracy']\nfinal_accs.describe()\n\ncount    10.000000\nmean      0.846814\nstd       0.027127\nmin       0.769608\n25%       0.855392\n50%       0.855392\n75%       0.855392\nmax       0.855392\nName: eval_accuracy, dtype: float64\n\n\n\nfinal_accs.value_counts()\n\neval_accuracy\n0.855392    9\n0.769608    1\nName: count, dtype: int64\n\n\n\ntest_dfs = []\naccs = []\nfor t in best_trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\nThe model also gets consistently the same test set accuracy.\n\naccs\n\n[0.68,\n 0.7911111111111111,\n 0.7911111111111111,\n 0.7911111111111111,\n 0.7911111111111111,\n 0.7911111111111111,\n 0.7911111111111111,\n 0.7911111111111111,\n 0.7911111111111111,\n 0.7911111111111111]\n\n\n\ntest_df.to_csv(\"TinyStories-33M_test set predictions_LR5e-4.csv\", index=False)"
  },
  {
    "objectID": "posts/2024-08-13-tinystories-33m-finetune/index.html#final-thoughts",
    "href": "posts/2024-08-13-tinystories-33m-finetune/index.html#final-thoughts",
    "title": "Fine-tuning TinyStories-33M on the financial_phrasebank Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI entered this experiment expecting TinyStories-33M to perform poorly on sentiment classification, and am surprised (even shocked?) that it’s achieving 80%+ accuracy consistently. Granted, I have a small test set (and validation set) but these results are promising.\nI also didn’t do an exhaustive hyperparameter search (weight decay, learning rate warmup, number of epochs) so maybe I could have increased the performance of the model. I’ll leave that for a future exercise.\nFor now, in my next notebook/blog post related to this project (that I’m calling TinySentiment), I’ll fine-tune three smaller base models (TinyStories-8M, TinyStories-3M, and TinyStories-1M) on the financial_phrasebank dataset and compare their results with the 33M model.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-03-financialphrasebank-readinglevel/index.html",
    "href": "posts/2024-08-03-financialphrasebank-readinglevel/index.html",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the financial_phrasebank Dataset",
    "section": "",
    "text": "In this notebook I’ll calculate the Flesch-Kincaid reading grade level for the financial_phrasebank dataset. Previously, I found the TinyStories dataset had a median grade level of 2.7. I expect financial_phrasebank to have a higher grade level due to financial jargon. If true, this suggests fine-tuning TinyInstruct-33M on financial_phrasebank may be less effective than using a simplified version (which I’ll create with phi-3 as a future exercise) as I endeavor to build a “TinySentiment” classifier."
  },
  {
    "objectID": "posts/2024-08-03-financialphrasebank-readinglevel/index.html#setup",
    "href": "posts/2024-08-03-financialphrasebank-readinglevel/index.html#setup",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the financial_phrasebank Dataset",
    "section": "Setup",
    "text": "Setup\n\n!pip install textstat -qq\n!pip install datasets -qq\n\n\nfrom datasets import load_dataset\nimport numpy as np\nimport textstat\n\nds = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\",\n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\n\nds\n\nDataset({\n    features: ['sentence', 'label'],\n    num_rows: 2264\n})\n\n\n\nddf = ds.to_pandas() # convert to a DataFrame to apply textstat.flesch_kincaid_grade\nddf.shape\n\n(2264, 2)"
  },
  {
    "objectID": "posts/2024-08-03-financialphrasebank-readinglevel/index.html#calculating-flesch-kincaid-reading-grade-level",
    "href": "posts/2024-08-03-financialphrasebank-readinglevel/index.html#calculating-flesch-kincaid-reading-grade-level",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the financial_phrasebank Dataset",
    "section": "Calculating Flesch-Kincaid Reading Grade Level",
    "text": "Calculating Flesch-Kincaid Reading Grade Level\nI can calculate the reading grade level for the 2264 rows in about 0.5 seconds by using textstat.flesch_kincaid_grade. See my previous blog post for a deeper dive on how textstat calculates reading grade level.\n\nddf['fk_grade'] = ddf['sentence'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n\nThe mean and median reading grade levels for this dataset are 8.6 and 8, respectively, about 6 grade levels higher than the TinyStories dataset.\n\nddf['fk_grade'].describe()\n\n\n\n  \n    \n      \n      fk_grade\n    \n  \n  \n    \n      count\n      2264.000000\n    \n    \n      mean\n      8.616078\n    \n    \n      std\n      4.933004\n    \n    \n      min\n      -3.100000\n    \n    \n      25%\n      4.600000\n    \n    \n      50%\n      8.000000\n    \n    \n      75%\n      12.200000\n    \n    \n      max\n      24.800000\n    \n  \ndtype: float64\n\n\nHere’s an example of a Grade 8 reading level sentence:\n\nddf.query(\"fk_grade == 8\").iloc[0]['sentence']\n\n'Both operating profit and turnover for the three-month period increased , respectively from EUR0 .9 m and EUR8 .3 m , as compared to the corresponding period in 2005 .'\n\n\nThe highest grade level in this dataset is 24.1 (well beyond post-graduate) likely due to its long list of multisyllabic industries:\n\nddf.query(\"fk_grade == 24.8\").iloc[0]['sentence']\n\n'The company serves customers in various industries , including process and resources , industrial machinery , architecture , building , construction , electrical , transportation , electronics , chemical , petrochemical , energy , and information technology , as well as catering and households .'\n\n\nThe lowest reading level is -3.1:\n\nddf.query(\"fk_grade == -3.1\").iloc[0]['sentence']\n\n'R&D Loan ) .'\n\n\nThe histogram illustrates that a significant portion of the dataset (25%) is above Grade 12. Less than 0.0005% of TinyStories was above Grade 12.\n\nddf['fk_grade'].hist();\n\n\n\n\n\nddf.query(\"fk_grade > 12\")['fk_grade'].count() / 2264\n\n0.2539752650176678"
  },
  {
    "objectID": "posts/2024-08-03-financialphrasebank-readinglevel/index.html#final-thoughts",
    "href": "posts/2024-08-03-financialphrasebank-readinglevel/index.html#final-thoughts",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the financial_phrasebank Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAs expected, financial_phrasebank (median reading grade level of 8) uses more complex language than TinyStories (median reading grade level of 2.7). This may make it challenging for TinyInstruct-33M to learn from. To test this, I’ll:\n\nFine-tune TinyInstruct-33M on financial_phrasebank\nCreate a lower reading grade version of the dataset using phi-3\nFine-tune TinyInstruct-33M on the simplified dataset\nCompare performance of both fine-tuned models\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html",
    "href": "posts/2024-02-05-paddy-part-4/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8\nPart 2: Live Coding 9\nPart 3: Live Coding 10\nPart 4: Live Coding 11 (You are here)\nPart 5: Live Coding 12\nPart 6: Live Coding 13\nPart 7: Improving My Large Ensemble, Part 1\nPart 8: Improving My Large Ensemble, Part 2\n\nLink to the Live Coding 11 video"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#setup",
    "href": "posts/2024-02-05-paddy-part-4/index.html#setup",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Setup",
    "text": "Setup\n\n!pip install -qq timm==0.6.13\nimport timm\ntimm.__version__\n\n'0.6.13'\n\n\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai')\n\n\npath.ls()\n\n(#4) [Path('../input/paddy-disease-classification/sample_submission.csv'),Path('../input/paddy-disease-classification/train_images'),Path('../input/paddy-disease-classification/train.csv'),Path('../input/paddy-disease-classification/test_images')]\n\n\n\ntrn_path = path/'train_images'"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#recap-on-paddy-competition",
    "href": "posts/2024-02-05-paddy-part-4/index.html#recap-on-paddy-competition",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Recap on Paddy Competition",
    "text": "Recap on Paddy Competition\nJeremy submitted two more entries to the Paddy competition:\n\nEnsembled the models we had (improved the submission)\nSince the VIT models were better than the rest, doubled their weights (that improved it as well)"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#tips-on-getting-votes-for-kaggle-notebooks",
    "href": "posts/2024-02-05-paddy-part-4/index.html#tips-on-getting-votes-for-kaggle-notebooks",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Tips on getting votes for Kaggle notebooks",
    "text": "Tips on getting votes for Kaggle notebooks\nTo get more Kaggle notebook votes, create them in response to a popular competition. Much harder to get votes for a notebook that’s not part of a competition."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#weights-and-biases-sweep",
    "href": "posts/2024-02-05-paddy-part-4/index.html#weights-and-biases-sweep",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Weights and Biases Sweep",
    "text": "Weights and Biases Sweep\nIn tmux, press shift+D and select a client to disconnect it (in order to removed dotted-background in terminal).\nIn Weights & Biases: you can run a “sweep” which runs lots of copies of your program feeding it different configurations. You can run the wandb client as many times as you want, setting it to a different CUDA device each time. Jeremy turns his model training routine into a python script, taking in arguments and passing them to a training function which initializes wandb using the given configuration, fine-tunes it, and does logging. NVIDIA tells you the GPU power usage (among other metrics). The key thing is the maximum memory use (wandb.summary[\"GPU_mem\"]).\nfastai has a thing called fastgpu which is a “queue service for quickly developing scripts that use all of your GPUs efficiently.” wandb takes this much further.\nNote: The information about your git repo is in .git/config.\nwandb API gives access to the runs logs (list of dictionaries) that we can then chuck into a DataFrame.\nNote: plotly has interactive parallel coordinates plot."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#brute-force-hyperparameter-optimisation-vs-human-approach",
    "href": "posts/2024-02-05-paddy-part-4/index.html#brute-force-hyperparameter-optimisation-vs-human-approach",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Brute force hyperparameter optimisation vs human approach",
    "text": "Brute force hyperparameter optimisation vs human approach\nIn general, Jeremy doesn’t do Bayesian hyperparameter stuff ever. Which is funny because he taught wandb about the method they use for hyperparameter optimization. Used it once specifically for finding a good set of dropouts for AWD_LSTM—coded a random forest that actually tries to predict how accurate something’s going to be and then use that random forest to target better sets of hyperparameters. Jeremy likes to use a much more human-driven approach—what’s the hypothesis I’m trying to test, how can I test that as fast as possible. Most hyperparameters are independent of other hyperparameters. You don’t have to do a huge grid search. For example learning rate of 0.008 is always the best so let’s not try every learning rate for every model for every resize type, etc., let’s just use that learning rate. Same thing for resize method—crop was always better than squish for the few things we tried it on so we don’t have to try every combination. Jeremy also feels that he learns a lot more about deep learning when he asks like what do I want to know about this thing? Or is that thing independent of that other thing? Or are they connected or not? Next time I do another project I can leverage the knowledge of what I’ve learnt rather than do yet another huge hyperparameter sweep. My brain is the thing that’s learning. People at big companies that spend all of their time doing this big hyperparameter optimizations I always feel in talking to them they don’t seem to know much about the practice of deep learning. Like they don’t seem to know like what generally works and what generally doesn’t work because they never bother trying to figure out the answers to those questions, but instead they just chuck in a huge hyperparameter optimization thing into a thousand GPUs.\nHyperparameters generalize across different architectures and models (see this study). Across 90 different models they all had basically the same best learning rate (or close enough). This is true of computer vision, but not necessarily for tabular. All computer vision problems do all look pretty similar, the data for them looks pretty similar, Jeremy suspects that’s also true of object recognition. Nobody seems to be testing this but we should do similar tests for segmentation, bounding boxes, and so forth. Jeremy’s pretty sure we’d find the same thing."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#learning-rate-finder",
    "href": "posts/2024-02-05-paddy-part-4/index.html#learning-rate-finder",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Learning rate finder",
    "text": "Learning rate finder\nJeremy hardly uses learning rate finder, hasn’t mentioned it yet in this course. fastai learning rate default is a bit lower than the optimal just because Jeremy didn’t want to push it, rather it always worked pretty well rather than be pretty much the best."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#debugging-port-issues-with-ps",
    "href": "posts/2024-02-05-paddy-part-4/index.html#debugging-port-issues-with-ps",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Debugging port issues with ps",
    "text": "Debugging port issues with ps\nNormally the Jupyter server uses port 8888, Jeremy only has his SSH server setup to forward to port 8888. The fact that it’s using a different port, 8889, suggests that it’s already running somewhere. To find out where it’s running type ps waux to list all of your processes. To filter them to ones that contain jupyter or notebook: ps waux | grep jupyter."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#background-sessions-in-tmux",
    "href": "posts/2024-02-05-paddy-part-4/index.html#background-sessions-in-tmux",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Background sessions in tmux",
    "text": "Background sessions in tmux\ntmux ls lists all of your tmux sessions. Ctrl+Z to put a job into the background (and also stops the job), fg to bring it into the foreground. If you type bg (and optionally followed the job number, defaults to the last job put into the background) it will run the last job put in the background. It will still print output even if it’s in the background. Jeremy doesn’t so this very much because if he wants to run a job at the same time as the other he chucks it into another tmux pane. If you run something with & at the end, it always runs it in the background. To run processes in parallel put an & after each one. To kill a process type fg to foreground it and then type Ctrl+C. See bash job control for more information. The job number has a % at the start.\nFrom wikipedia: tmux is an open-source terminal multiplexer for Unix-like operating systems. It allows multiple terminal sessions to be accessed simultaneously in a single window. It is useful for running more than one command-line program at the same time."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#strategy-for-iterating-between-notebooks",
    "href": "posts/2024-02-05-paddy-part-4/index.html#strategy-for-iterating-between-notebooks",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Strategy for iterating between notebooks",
    "text": "Strategy for iterating between notebooks\nWhen Jeremy iterates through notebooks, what he tends to do is that once he’s got something vaguely working, he generally duplicates it and then tries to get something else vaguely working (change a parameter, try a different method, etc.), and once that starts vaguely working, he renames it. Then, from time to time, he cleans up the duplicated notebook versions that he didn’t end up using and he can tell which ones those are because he hasn’t renamed them yet (paddy_base_Copy1.ipynb, paddy_base_Copy2.ipynb, etc.).\nIn this case, Jeremy started out with paddy.ipynb, and just experimented (show_batch, lr_find and try to get something running). From that he dupicated it (paddy_Copy1.ipynb). In that he wanted to try different architectures, batch transforms and item transforms so he created a train function which takes those three things, creates a set of ImageDataLoaders with those transforms, use a fixed seed to get the same validation set each time (to compare performance across architectures), train it with an architecture, and then return the tta error rate. He then goes through the notebook trying different small architectures (small so they will run decently quickly), batch and item transforms. From that he gets a sense of which batch and item transforms work well for which architectures. He renames this notebookpaddy-small.ipynb.\nJeremy is looking at two things: error rate at the end of training and tta error rate. The main one he cares about is the tta error rate since that’s the one he’s going to end up using.\nIf you pass an int instead of a tuple to item or batch transforms’ size parameter it will resize to a square.\nMost frameworks (at the time of this video) doesn’t provide a test time augmentation method (except for one unnamed group that copies everything from fastai).\nThe cool kids on Kaggle (at the time of this videoin 2022) use swin2, which has fixed resolution and for larger model sizes has image size options of 192 and 256.\nNone of the crop resize method runs were doing well for vit or swin2, but were doing well for convnext."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#building-an-ensemble---appending-predictions",
    "href": "posts/2024-02-05-paddy-part-4/index.html#building-an-ensemble---appending-predictions",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Building an ensemble - appending predictions",
    "text": "Building an ensemble - appending predictions\nJeremy then duplicated paddy_small.ipynb, picked the models that performed well (and delete the rest), and did a search and replace of small with large. He got rid of the fixed random seed when creating the ImageDataLoaders in the train function, giving a different training set each time (meaning the models will not be comparable, which is fine). Jeremy appends to an empty starting list tta_res the tta predictions from each model.\nFor the very last Kaggle entry: Jeremy took the two best models (in his case the ViTs) and appending their tta predictions to the list tta_res so that they were there twice. A slightly clunky way of doing a weighted average. Then he stacked them all together, took the mean of the predictions, found the argmax (index of the class with the largest prediction) and then submit in the same way as before. This is Jeremy’s process which is not particularly thoughtful, but rather mechanical, which is what he likes about it as it can probably be automated."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#model-stacking",
    "href": "posts/2024-02-05-paddy-part-4/index.html#model-stacking",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Model stacking",
    "text": "Model stacking\nQuestion: how critical is model stacking in Kaggle? Next time we’ll submit just the best ViT predictions and compare it to the stacked predictions. That will give us a sense of how much the ensembling matters.\nFor convnext: the large ensemble model had half the error rate of the small single model."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#keeping-track-of-submission-notebooks",
    "href": "posts/2024-02-05-paddy-part-4/index.html#keeping-track-of-submission-notebooks",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Keeping track of submission notebooks",
    "text": "Keeping track of submission notebooks\nQuestion: how do you keep track of which submissions are tied to which notebook? Jeremy just provides a small description to the submission to remind him. A better approach would be to actually write the notebook name there, which is what Jeremy normally does. Don’t change those notebooks after submission, duplicate them, make changes in the duplicate and rename them to something sensible. This all ends up back in github. Using this approach you will actually become a better deep learning practitioner. There are very few people who actually use this approach and there are very few people Jeremy comes across who are actually good deep learning practitioners. Not many people seem to know what works and what doesn’t.\nThe only way Jeremy can do this approach (small number of models) is because of already running a lot of models prior and logging their performance."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#training-large-models",
    "href": "posts/2024-02-05-paddy-part-4/index.html#training-large-models",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Training Large Models",
    "text": "Training Large Models\nAfter going through walkthrough 11, I’ll now imitate the same approach Jeremy used and create an ensemble of models for my next Kaggle submission.\nI’ll train the large versions of the following three small models (and transforms), which performed the best on TTA error rate:\n\n\n\n\n\n\n\n\n\n\nArchitecture\nitem_tfms\nbatch_tfms\nError Rate (First Run)\nMinutes (per epoch)\n\n\n\n\nswinv2_base_window12_192_22k\nResize(480, method='squish')\naug_transforms(size=192, min_scale=0.75)\n0.0163*\n02:30\n\n\nconvnext_small_in22k\nResize((640,480))\naug_transforms(size=(288,224), min_scale=0.75)\n0.0178*\n01:51\n\n\nvit_small_patch16_224\nResize(480)\naug_transforms(size=224, min_scale=0.75)\n0.0202*\n00:44\n\n\n\nI’ll prepare a helper function which does all the submission csv prep stuff:\n\ndef prep_submission(fn, tta_res):\n    # pull out predictions from tta_res list\n    tta_prs = first(zip(*tta_res))\n    \n    # convert tta_res from list to stacked tensor\n    t_tta = torch.stack(tta_prs)\n    \n    # take mean of each item's predictions\n    avg_pr = t_tta.mean(0)\n    \n    # get the index (class) of the maximum prediction for each item\n    idxs = avg_pr.argmax(dim=1)\n    \n    # create DataLoaders to get its vocab\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(224))\n    \n    # convert indexes to vocab strings\n    mapping = dict(enumerate(dls.vocab))\n    \n    # add vocab strings to sample submission file and export to CSV\n    ss = pd.read_csv(path/'sample_submission.csv')\n    results = pd.Series(idxs.numpy(), name='idxs').map(mapping)\n    ss.label = results\n    ss.to_csv(fn, index=False)\n\nI’ll also copy from the above the train function defined by Jeremy that I’ll use for these models—instead of handling the tta predictions inside the training function, I’m returning the Learner and the DataLoaders so that I can calculate the validation set error rate and prep the test DataLoader with the appropriate Resize based on the model.\n\ntta_res = []\n\n\ndef train(arch, item, batch, accum=False):\n    kwargs = {'bs': 16} if accum else {}\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item, batch_tfms=batch, **kwargs)\n    cbs = GradientAccumulation(2) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    learn.fine_tune(12, 0.01)\n    return learn, dls\n    # tta_res.append(learn.tta(dl=tst_dl))\n    # return error_rate(*learn.tta(dl=dls.valid))\n\nNow I’ll train the three large models:\n\narch = 'swinv2_large_window12_192_22k'\n\n\nlearn, dls = train(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=192, min_scale=0.75), accum=True)\n\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth\" to /root/.cache/torch/hub/checkpoints/swinv2_large_patch4_window12_192_22k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.212364\n      0.643707\n      0.188852\n      03:40\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.476209\n      0.274359\n      0.094666\n      05:01\n    \n    \n      1\n      0.494529\n      0.300078\n      0.098030\n      05:01\n    \n    \n      2\n      0.447107\n      0.328064\n      0.099952\n      05:01\n    \n    \n      3\n      0.429720\n      0.298238\n      0.089861\n      05:02\n    \n    \n      4\n      0.284891\n      0.312586\n      0.082172\n      05:02\n    \n    \n      5\n      0.248839\n      0.156329\n      0.039885\n      05:02\n    \n    \n      6\n      0.132434\n      0.122389\n      0.039404\n      05:01\n    \n    \n      7\n      0.114585\n      0.123687\n      0.031235\n      05:01\n    \n    \n      8\n      0.078405\n      0.103234\n      0.027871\n      05:01\n    \n    \n      9\n      0.061866\n      0.087151\n      0.022105\n      05:01\n    \n    \n      10\n      0.056032\n      0.080488\n      0.017780\n      05:01\n    \n    \n      11\n      0.031765\n      0.082360\n      0.018741\n      05:01\n    \n  \n\n\n\nHere’s the code to prepare the test DataLoader for this swin model that uses 192x192 pixels:\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(192))\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\ntst_dl = dls.test_dl(tst_files)\n\n\ntst_dl.show_batch()\n\n\n\n\n\ntta_res.append(learn.tta(dl=tst_dl))\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tta_res[0][0])\n\n3469\n\n\n\nerror_rate(*learn.tta(dl=dls.valid))\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0159)\n\n\n\narch = 'convnext_large_in22k'\n\n\nlearn, dls = train(arch, item=Resize((640,480)), batch=aug_transforms(size=(288,224), min_scale=0.75), accum=True)\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_large_22k_224.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.213938\n      0.805596\n      0.224892\n      03:14\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.467912\n      0.239441\n      0.077367\n      04:48\n    \n    \n      1\n      0.419594\n      0.247392\n      0.067756\n      04:47\n    \n    \n      2\n      0.377958\n      0.205914\n      0.062470\n      04:45\n    \n    \n      3\n      0.312134\n      0.294184\n      0.071600\n      04:46\n    \n    \n      4\n      0.233862\n      0.241724\n      0.058626\n      04:46\n    \n    \n      5\n      0.149694\n      0.118743\n      0.028832\n      04:45\n    \n    \n      6\n      0.141486\n      0.137468\n      0.030274\n      04:45\n    \n    \n      7\n      0.086328\n      0.117887\n      0.027391\n      04:45\n    \n    \n      8\n      0.069587\n      0.101186\n      0.024027\n      04:45\n    \n    \n      9\n      0.047381\n      0.083770\n      0.017780\n      04:44\n    \n    \n      10\n      0.023768\n      0.087173\n      0.018260\n      04:44\n    \n    \n      11\n      0.023235\n      0.092406\n      0.020663\n      04:45\n    \n  \n\n\n\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(224))\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\ntst_dl = dls.test_dl(tst_files)\ntst_dl.show_batch()\n\n\n\n\n\ntta_res.append(learn.tta(dl=tst_dl))\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0])\n\n(2, 3469, 3469)\n\n\n\nerror_rate(*learn.tta(dl=dls.valid))\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0706)\n\n\n\narch = 'vit_large_patch16_224'\n\n\nlearn, dls = train(arch, item=Resize(480), batch=aug_transforms(size=224, min_scale=0.75), accum=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.303628\n      0.769040\n      0.225853\n      04:53\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.472007\n      0.282628\n      0.092263\n      06:47\n    \n    \n      1\n      0.493476\n      0.508031\n      0.155694\n      06:48\n    \n    \n      2\n      0.510523\n      0.543362\n      0.172994\n      06:46\n    \n    \n      3\n      0.449807\n      0.509493\n      0.131667\n      06:46\n    \n    \n      4\n      0.323358\n      0.229533\n      0.068236\n      06:46\n    \n    \n      5\n      0.239007\n      0.150215\n      0.044210\n      06:46\n    \n    \n      6\n      0.223845\n      0.160197\n      0.047093\n      06:45\n    \n    \n      7\n      0.085320\n      0.112214\n      0.029313\n      06:45\n    \n    \n      8\n      0.078987\n      0.108103\n      0.029313\n      06:46\n    \n    \n      9\n      0.043259\n      0.092681\n      0.024507\n      06:45\n    \n    \n      10\n      0.037170\n      0.075773\n      0.020183\n      06:45\n    \n    \n      11\n      0.028403\n      0.074941\n      0.021144\n      06:44\n    \n  \n\n\n\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(224))\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\ntst_dl = dls.test_dl(tst_files)\ntst_dl.show_batch()\n\n\n\n\n\ntta_res.append(learn.tta(dl=tst_dl))\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0])\n\n(3, 3469, 3469, 3469)\n\n\n\nerror_rate(*learn.tta(dl=dls.valid))\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0135)\n\n\n\nprep_submission('subm_large_ensemble.csv', tta_res)\n\n\n!head subm_large_ensemble.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\n\nImproving the Large Model Ensemble\nHere is a summary of my submissions so far in this competition, with the fifth submission the one I just did with a large model ensemble:\n\n\n\n\n\n\n\n\n\nSubmission\nDescription\nPrivate Score\nPublic Score\n\n\n\n\n1\ninitial submission file after creating a quick small model following Jeremy Howard’s walkthrough video.\n0.13709\n0.12418\n\n\n2\ninitial submission using convnext small 2 epochs fine-tuned sorted file list\n0.94124\n0.92541\n\n\n3\nsquish convnext small 12 epoch ft tta\n0.98156\n0.98308\n\n\n4\nensemble small 12 epoch ft tta\n0.98617\n0.98423\n\n\n5\nswinv2 convnext vit large ensemble 12 epoch ft tta\n0.97811\n0.98039\n\n\n\nThe first four submissions were going great—each successive one improved the private and public score. I was expecting my ensemble with larger models to improve the accuracy of the small model ensemble. Instead, the larger ensemble’s private error rate (1-0.97811) is 58% more than the smaller ensemble (1-0.98617), and the public error rate of the larger ensemble (1-0.98039) is 24% larger than the smaller ensemble (1-0.98423). Why?"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-4/index.html#improving-the-large-model-ensemble-1",
    "href": "posts/2024-02-05-paddy-part-4/index.html#improving-the-large-model-ensemble-1",
    "title": "Paddy Doctor Kaggle Competition - Part 4",
    "section": "Improving the Large Model Ensemble",
    "text": "Improving the Large Model Ensemble\nIn order to try and improve the large model ensemble, I want to share my main observation from the large model training runs: In each of the training runs, the error rate increased for the first 4-5 epochs before decreasing, and was higher after the final epoch than any of the final error rates of the smaller models. I think this means that I should train the models for longer. Granted, the error rates between the larger models and between the larger and smaller models are not comparable (they use different validation sets), so I can’t conclude anything about training performance from that comparison. However, I would generally expect the error rates of the larger models to be lower, even on different validation sets (perhaps that’s not a fair expectation).\nA secondary observation that I made is that the manner in which I calculated the TTA error rate after each training run…\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(224))\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\ntst_dl = dls.test_dl(tst_files)\n\nerror_rate(*learn.tta(dl=dls.valid))\n…is not correct. I should pass dls.valid using the dls used during the training to learn.tta instead of using a new DataLoaders object which has a different training/validation split, and therefore likely is including images from the training when calculating the error rate. I also should use the same item and batch transforms.\nI’ll redefine my train function to include the TTA prediction and error rate calculations, and increase the number of epochs to 24 (this will likely be too large, but I’ll adjust after one training run):\n\n# run this once and re-use for all trainings\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\n\n\ndef train(arch, item, batch, accum=False):\n    kwargs = {'bs': 16} if accum else {}\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item, batch_tfms=batch, **kwargs)\n    cbs = GradientAccumulation(2) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    learn.fine_tune(24, 0.01)\n    \n    # view losses\n    learn.recorder.plot_loss()\n    \n    # TTA predictions using test dataset\n    tst_dl = dls.test_dl(tst_files)\n    tta_res.append(learn.tta(dl=tst_dl))\n    \n    # Return error rate using validation dataset\n    print(error_rate(*learn.tta(dl=dls.valid)))\n    return learn, dls\n\n\nDetermine Ideal Number of Epochs\nI’ll train the swinv2 model to see how many epochs I should use for training—I’ll be keeping an eye on the validation loss. If it starts to increase, I know I’ve trained for too long.\nI’ll train it a few times to see how stable the training is.\n\ntta_res = []\n\n\narch = 'swinv2_large_window12_192_22k'\n\n\ntrain(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=192, min_scale=0.75), accum=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.242866\n      0.766052\n      0.219606\n      03:40\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.439101\n      0.294660\n      0.087938\n      05:01\n    \n    \n      1\n      0.351378\n      0.222908\n      0.068236\n      05:02\n    \n    \n      2\n      0.362053\n      0.194189\n      0.060548\n      05:02\n    \n    \n      3\n      0.352330\n      0.217729\n      0.064873\n      05:02\n    \n    \n      4\n      0.317429\n      0.332206\n      0.088419\n      05:02\n    \n    \n      5\n      0.268157\n      0.211638\n      0.057184\n      05:02\n    \n    \n      6\n      0.281623\n      0.222969\n      0.059587\n      05:02\n    \n    \n      7\n      0.302056\n      0.192562\n      0.053820\n      05:02\n    \n    \n      8\n      0.240952\n      0.229510\n      0.058626\n      05:02\n    \n    \n      9\n      0.121465\n      0.183262\n      0.040365\n      05:02\n    \n    \n      10\n      0.172122\n      0.207367\n      0.056704\n      05:02\n    \n    \n      11\n      0.116110\n      0.218943\n      0.051418\n      05:02\n    \n    \n      12\n      0.110617\n      0.127053\n      0.030274\n      05:02\n    \n    \n      13\n      0.095698\n      0.143608\n      0.035560\n      05:02\n    \n    \n      14\n      0.082635\n      0.119791\n      0.027391\n      05:02\n    \n    \n      15\n      0.086161\n      0.109686\n      0.024507\n      05:02\n    \n    \n      16\n      0.060592\n      0.115672\n      0.026430\n      05:02\n    \n    \n      17\n      0.045066\n      0.116955\n      0.023546\n      05:02\n    \n    \n      18\n      0.027253\n      0.106379\n      0.022105\n      05:02\n    \n    \n      19\n      0.027075\n      0.116326\n      0.022585\n      05:01\n    \n    \n      20\n      0.027959\n      0.106153\n      0.020183\n      05:01\n    \n    \n      21\n      0.018591\n      0.106057\n      0.019222\n      05:01\n    \n    \n      22\n      0.016949\n      0.107893\n      0.017780\n      05:01\n    \n    \n      23\n      0.015706\n      0.106630\n      0.018260\n      05:01\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0187)\n\n\nI’m getting decent results after 24 epochs. Let’s see if that holds when I train it again:\n\nlearn, dls = train(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=192, min_scale=0.75), accum=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.092984\n      0.851812\n      0.221048\n      03:40\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.544550\n      0.254801\n      0.083614\n      05:01\n    \n    \n      1\n      0.348117\n      0.210600\n      0.066795\n      05:02\n    \n    \n      2\n      0.360428\n      0.211086\n      0.065834\n      05:02\n    \n    \n      3\n      0.346224\n      0.313809\n      0.094666\n      05:02\n    \n    \n      4\n      0.368918\n      0.243433\n      0.067756\n      05:02\n    \n    \n      5\n      0.377717\n      0.414140\n      0.119654\n      05:02\n    \n    \n      6\n      0.310699\n      0.225722\n      0.069678\n      05:01\n    \n    \n      7\n      0.245260\n      0.207003\n      0.061028\n      05:01\n    \n    \n      8\n      0.183680\n      0.181822\n      0.049976\n      05:01\n    \n    \n      9\n      0.138067\n      0.211188\n      0.052859\n      05:01\n    \n    \n      10\n      0.192407\n      0.231691\n      0.063912\n      05:01\n    \n    \n      11\n      0.135225\n      0.172450\n      0.041326\n      05:02\n    \n    \n      12\n      0.150147\n      0.115569\n      0.033638\n      05:02\n    \n    \n      13\n      0.090655\n      0.107455\n      0.029313\n      05:02\n    \n    \n      14\n      0.076048\n      0.087698\n      0.023546\n      05:02\n    \n    \n      15\n      0.063137\n      0.082899\n      0.024027\n      05:02\n    \n    \n      16\n      0.055129\n      0.085577\n      0.017299\n      05:02\n    \n    \n      17\n      0.039957\n      0.078003\n      0.018741\n      05:02\n    \n    \n      18\n      0.027110\n      0.094057\n      0.018741\n      05:02\n    \n    \n      19\n      0.036028\n      0.086254\n      0.015858\n      05:02\n    \n    \n      20\n      0.016480\n      0.083638\n      0.014416\n      05:01\n    \n    \n      21\n      0.024642\n      0.079923\n      0.013455\n      05:01\n    \n    \n      22\n      0.017131\n      0.081694\n      0.015377\n      05:01\n    \n    \n      23\n      0.013220\n      0.081680\n      0.015858\n      05:01\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0154)\n\n\nI get a similar final error rate as the first training. However, it’s not so clear from the tables how the validation and training losses change over the course of the training. To do so, I’ve added learn.recorder.plot_loss() to the train function.\nI’ll train it a couple more times to see the loss plots (ignore the NameError for now, I’m not concerned with TTA error rate at this point, I’m only concerned with whether or not the training is stable):\n\nlearn, dls = train(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=192, min_scale=0.75), accum=True)\n\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth\" to /root/.cache/torch/hub/checkpoints/swinv2_large_patch4_window12_192_22k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.294445\n      0.719873\n      0.204229\n      03:42\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.485149\n      0.264520\n      0.084575\n      05:08\n    \n    \n      1\n      0.383336\n      0.238155\n      0.072081\n      05:07\n    \n    \n      2\n      0.322685\n      0.196292\n      0.052379\n      05:06\n    \n    \n      3\n      0.391429\n      0.196975\n      0.056223\n      05:06\n    \n    \n      4\n      0.310772\n      0.311188\n      0.084575\n      05:07\n    \n    \n      5\n      0.370088\n      0.309966\n      0.087458\n      05:07\n    \n    \n      6\n      0.306135\n      0.160217\n      0.045651\n      05:06\n    \n    \n      7\n      0.227858\n      0.303918\n      0.077367\n      05:08\n    \n    \n      8\n      0.237295\n      0.202838\n      0.050937\n      05:08\n    \n    \n      9\n      0.173179\n      0.163243\n      0.046132\n      05:08\n    \n    \n      10\n      0.154630\n      0.141608\n      0.037482\n      05:08\n    \n    \n      11\n      0.123365\n      0.116312\n      0.032677\n      05:06\n    \n    \n      12\n      0.117789\n      0.117131\n      0.031716\n      05:05\n    \n    \n      13\n      0.100615\n      0.150154\n      0.031716\n      05:05\n    \n    \n      14\n      0.069063\n      0.106202\n      0.024507\n      05:04\n    \n    \n      15\n      0.062251\n      0.098203\n      0.022105\n      05:06\n    \n    \n      16\n      0.067058\n      0.103470\n      0.022585\n      05:06\n    \n    \n      17\n      0.041386\n      0.093786\n      0.020183\n      05:05\n    \n    \n      18\n      0.033301\n      0.099226\n      0.017780\n      05:05\n    \n    \n      19\n      0.030562\n      0.086443\n      0.016338\n      05:05\n    \n    \n      20\n      0.045077\n      0.090462\n      0.016819\n      05:05\n    \n    \n      21\n      0.015390\n      0.087115\n      0.017780\n      05:05\n    \n    \n      22\n      0.013893\n      0.084977\n      0.016819\n      05:05\n    \n    \n      23\n      0.021804\n      0.084517\n      0.016819\n      05:06\n    \n  \n\n\n\nNameError: name 'tst_files' is not defined\n\n\n\n\n\n\ntta_res = []\n\n\nlearn, dls = train(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=192, min_scale=0.75), accum=True)\n\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth\" to /root/.cache/torch/hub/checkpoints/swinv2_large_patch4_window12_192_22k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.327175\n      0.800045\n      0.214320\n      03:41\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.428933\n      0.264715\n      0.085536\n      05:10\n    \n    \n      1\n      0.373618\n      0.269784\n      0.081211\n      05:10\n    \n    \n      2\n      0.322864\n      0.236708\n      0.076406\n      05:10\n    \n    \n      3\n      0.318025\n      0.255129\n      0.073522\n      05:11\n    \n    \n      4\n      0.362358\n      0.276745\n      0.073042\n      05:10\n    \n    \n      5\n      0.252548\n      0.238884\n      0.056223\n      05:10\n    \n    \n      6\n      0.269222\n      0.258998\n      0.067275\n      05:11\n    \n    \n      7\n      0.232486\n      0.361521\n      0.095627\n      05:10\n    \n    \n      8\n      0.194597\n      0.210730\n      0.060067\n      05:10\n    \n    \n      9\n      0.195712\n      0.128107\n      0.036040\n      05:10\n    \n    \n      10\n      0.158002\n      0.159793\n      0.040846\n      05:10\n    \n    \n      11\n      0.100048\n      0.135760\n      0.037001\n      05:10\n    \n    \n      12\n      0.081591\n      0.175432\n      0.042768\n      05:11\n    \n    \n      13\n      0.101374\n      0.191513\n      0.041326\n      05:11\n    \n    \n      14\n      0.065971\n      0.126861\n      0.026430\n      05:11\n    \n    \n      15\n      0.059315\n      0.120217\n      0.028352\n      05:09\n    \n    \n      16\n      0.038761\n      0.118691\n      0.026430\n      05:09\n    \n    \n      17\n      0.031474\n      0.106201\n      0.021144\n      05:09\n    \n    \n      18\n      0.029422\n      0.132580\n      0.023546\n      05:09\n    \n    \n      19\n      0.024789\n      0.102459\n      0.021624\n      05:09\n    \n    \n      20\n      0.031319\n      0.110205\n      0.021144\n      05:08\n    \n    \n      21\n      0.021636\n      0.108477\n      0.018741\n      05:08\n    \n    \n      22\n      0.013328\n      0.111174\n      0.019222\n      05:09\n    \n    \n      23\n      0.019866\n      0.108757\n      0.020183\n      05:09\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0183)\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0])\n\n(1, 3469)\n\n\nI ran four training runs of 24 epochs each for this large swinv2 model, including two training runs where I plotted the validation and training loss. I noticed the same patterns in each one:\n\nThe training and validation losses are generally decreasing but tend to rise and fall a bit across the training.\n24 epochs doesn’t seem to be overfitting the model. The validation loss certainly fluctuates a bit at the end of the training (increasing and decreasing in consecutive epochs) but it is relatively flat at the end. Perhaps I’m making a mistake here, and such a flattening is a sign of the start of overfitting, but I’m going to stick to 24 epochs for now.\nThe final error rate during training is still unremarkable at around 0.02. None of the training runs resulted in anything close to the lowest error rate I had with smaller models (0.0163). Again, different validation sets will result in different error rates, but because of this lack of error rate improvement, I’m not expecting to improve my competition score significantly.\n\nNext, I’ll train the other two large models for 24 epochs each:\n\narch = 'convnext_large_in22k'\n\n\nlearn, dls = train(arch, item=Resize((640,480)), batch=aug_transforms(size=(288,224), min_scale=0.75), accum=True)\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_large_22k_224.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.117779\n      0.660938\n      0.174916\n      03:27\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.437924\n      0.200062\n      0.068717\n      05:09\n    \n    \n      1\n      0.315851\n      0.158929\n      0.048054\n      05:10\n    \n    \n      2\n      0.294190\n      0.167008\n      0.049976\n      05:09\n    \n    \n      3\n      0.240635\n      0.133045\n      0.040846\n      05:09\n    \n    \n      4\n      0.296416\n      0.199445\n      0.057184\n      05:10\n    \n    \n      5\n      0.223830\n      0.155541\n      0.044690\n      05:09\n    \n    \n      6\n      0.231965\n      0.144504\n      0.034118\n      05:09\n    \n    \n      7\n      0.176270\n      0.160164\n      0.041807\n      05:09\n    \n    \n      8\n      0.173444\n      0.175411\n      0.039885\n      05:09\n    \n    \n      9\n      0.154608\n      0.150472\n      0.040846\n      05:09\n    \n    \n      10\n      0.128028\n      0.119761\n      0.031235\n      05:09\n    \n    \n      11\n      0.081071\n      0.093472\n      0.020183\n      05:08\n    \n    \n      12\n      0.107737\n      0.099295\n      0.021144\n      05:08\n    \n    \n      13\n      0.096281\n      0.111142\n      0.024507\n      05:08\n    \n    \n      14\n      0.054678\n      0.078692\n      0.019222\n      05:08\n    \n    \n      15\n      0.039529\n      0.092145\n      0.024507\n      05:08\n    \n    \n      16\n      0.049152\n      0.084749\n      0.020183\n      05:07\n    \n    \n      17\n      0.029900\n      0.075587\n      0.014897\n      05:08\n    \n    \n      18\n      0.025885\n      0.088903\n      0.020663\n      05:07\n    \n    \n      19\n      0.016233\n      0.076446\n      0.017299\n      05:07\n    \n    \n      20\n      0.012938\n      0.069057\n      0.015377\n      05:07\n    \n    \n      21\n      0.017901\n      0.072703\n      0.013455\n      05:06\n    \n    \n      22\n      0.015330\n      0.076080\n      0.014416\n      05:06\n    \n    \n      23\n      0.027190\n      0.074604\n      0.014416\n      05:06\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0144)\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0])\n\n(2, 3469, 3469)\n\n\nSimilar to the large swinv2 training run, the training loss starts at a much higher value and ends up at a lower value than the validation loss. Both losses fluctuate, decreasing for the first few epochs, then increasing in epoch 4, then generally decreasing the rest of the way with small bumps.\nIt’s hard not to compare error rates across models, even though I know I shouldn’t because they are on different validation and test sets. That being said:\nAfter the final (24th) epoch, the large convnext model achieved about 30% smaller final training error rate than the large swinv2 models. The TTA error rate for convnext model (0.0144) is 20% lower than the swinv2 models (0.0183).\nLet’s see how the large vit model performs, but I am leaning towards excluding the swinv2 model from the ensemble since it seems to be the worst performing one on both validation and test set error rates. I am now also questioning whether the excellent performance of the small swinv2 model just simply doesn’t translate to the larger model. I also have to question here—am I doing something wrong in how I’m training these large models? Shouldn’t they be performing much better than their smaller counterparts? convnext certainly seems to be doing so, but swinv2 doesn’t.\n\narch = 'vit_large_patch16_224'\n\nI was running into the usual CUDA Out-Of-Memory errors, even after restarting the kernel, so I found the following code online to clear out most of the GPU cache.\n\nimport gc; gc.collect()\ntorch.cuda.empty_cache()\n\n\nlearn, dls = train(arch, item=Resize(480), batch=aug_transforms(size=224, min_scale=0.75), accum=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.293154\n      0.675305\n      0.199423\n      05:01\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.559790\n      0.329836\n      0.110524\n      07:00\n    \n    \n      1\n      0.347774\n      0.223555\n      0.070159\n      07:01\n    \n    \n      2\n      0.375485\n      0.274702\n      0.074003\n      07:01\n    \n    \n      3\n      0.386415\n      0.247032\n      0.070159\n      07:01\n    \n    \n      4\n      0.401717\n      0.207438\n      0.068236\n      07:01\n    \n    \n      5\n      0.367362\n      0.319452\n      0.100432\n      07:01\n    \n    \n      6\n      0.300632\n      0.215864\n      0.066314\n      07:01\n    \n    \n      7\n      0.232857\n      0.152640\n      0.044690\n      07:01\n    \n    \n      8\n      0.268405\n      0.222575\n      0.062470\n      07:01\n    \n    \n      9\n      0.139751\n      0.270562\n      0.072561\n      07:01\n    \n    \n      10\n      0.191241\n      0.155850\n      0.039885\n      07:00\n    \n    \n      11\n      0.116998\n      0.124926\n      0.031235\n      07:00\n    \n    \n      12\n      0.130736\n      0.161409\n      0.043248\n      07:01\n    \n    \n      13\n      0.082500\n      0.174184\n      0.036521\n      07:00\n    \n    \n      14\n      0.061503\n      0.108156\n      0.026430\n      07:00\n    \n    \n      15\n      0.069686\n      0.138058\n      0.034599\n      07:00\n    \n    \n      16\n      0.059782\n      0.116582\n      0.032196\n      07:00\n    \n    \n      17\n      0.054639\n      0.102705\n      0.024507\n      07:00\n    \n    \n      18\n      0.033125\n      0.098741\n      0.025949\n      07:00\n    \n    \n      19\n      0.039223\n      0.099548\n      0.020183\n      06:59\n    \n    \n      20\n      0.021738\n      0.101047\n      0.020183\n      07:00\n    \n    \n      21\n      0.027946\n      0.100101\n      0.021624\n      07:00\n    \n    \n      22\n      0.016446\n      0.095121\n      0.022585\n      06:59\n    \n    \n      23\n      0.021418\n      0.093120\n      0.022105\n      07:00\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0197)\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0])\n\n(3, 3469, 3469, 3469)\n\n\nOnly the larger convnext model seems to be clearly out-performing its smaller counterpart. The larger swinv2 and vit models seems to be performing at around the same error rate as their smaller versions. I’ll make my next submission an ensemble of these three models and see how it performs:\n\nprep_submission('subm_large_ensemble2.csv', tta_res)\n\n\n!head subm_large_ensemble2.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\nThe submission (the sixth submission in the table below) for this ensemble received a lower private score and a higher public score when compared to the best submission (Submission 4: ensemble of small models trained for 12 epochs with final predictions calculated using TTA), and higher private and public score when compared to the same ensemble trained on only 12 epochs.\n\n\n\n\n\n\n\n\n\nSubmission\nDescription\nPrivate Score\nPublic Score\n\n\n\n\n1\ninitial submission file after creating a quick small model following Jeremy Howard’s walkthrough video.\n0.13709\n0.12418\n\n\n2\ninitial submission using convnext small 2 epochs fine-tuned sorted file list\n0.94124\n0.92541\n\n\n3\nsquish convnext small 12 epoch ft tta\n0.98156\n0.98308\n\n\n4\nensemble small 12 epoch ft tta\n0.98617\n0.98423\n\n\n5\nswinv2 convnext vit large ensemble 12 epoch ft tta\n0.97811\n0.98039\n\n\n6\nswinv2 convnext vit large ensemble 24 epoch ft tta\n0.98502\n0.98539\n\n\n\nDuring training, the model with the lowest error rate was convnext_large_in22k. I’ll weigh this model’s predictions more (by duplicating them twice more) so the ratio between this model’s predictions and the other models is 3:1, and see if that performs better.\n\ntta_res += 2 * [tta_res[1]]\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0]), len(tta_res[3][0]), len(tta_res[4][0])\n\n(5, 3469, 3469, 3469, 3469, 3469)\n\n\n\nprep_submission('subm_large_ensemble3.csv', tta_res)\n\n\n!head subm_large_ensemble3.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\nThis submission (#7) actually decreased both the public and private scores:\n\n\n\n\n\n\n\n\n\nSubmission\nDescription\nPrivate Score\nPublic Score\n\n\n\n\n1\ninitial submission file after creating a quick small model following Jeremy Howard’s walkthrough video.\n0.13709\n0.12418\n\n\n2\ninitial submission using convnext small 2 epochs fine-tuned sorted file list\n0.94124\n0.92541\n\n\n3\nsquish convnext small 12 epoch ft tta\n0.98156\n0.98308\n\n\n4\nensemble small 12 epoch ft tta\n0.98617\n0.98423\n\n\n5\nswinv2 convnext vit large ensemble 12 epoch ft tta\n0.97811\n0.98039\n\n\n6\nswinv2 convnext vit large ensemble 24 epoch ft tta\n0.98502\n0.98539\n\n\n7\nswinv2 (3x convnext) vit large ensemble 24 epoch ft tta\n0.98387\n0.98423\n\n\n\nI’ll try 5 more submissions next:\n\n3:1 ratio between swinv2 and the other models\n3:1 ratio between vit and the other models\nlarge swinv2 only\nlarge convnext only\nlarge vit only\n\n\n# triple swinv2\ntta_res = load_pickle('/kaggle/working/tta_res2.pkl')\ntta_res += 2 * [tta_res[0]]\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0]), len(tta_res[3][0]), len(tta_res[4][0])\n\n(5, 3469, 3469, 3469, 3469, 3469)\n\n\n\nprep_submission('subm_large_ensemble4.csv', tta_res)\n!head subm_large_ensemble4.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\n\n# triple vit\ntta_res = load_pickle('/kaggle/working/tta_res2.pkl')\ntta_res += 2 * [tta_res[2]]\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0]), len(tta_res[3][0]), len(tta_res[4][0])\n\n(5, 3469, 3469, 3469, 3469, 3469)\n\n\n\nprep_submission('subm_large_ensemble5.csv', tta_res)\n!head subm_large_ensemble5.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\n\n# large swinv2 only\ntta_res = load_pickle('/kaggle/working/tta_res2.pkl')\ntta_res = [tta_res[0]]\nlen(tta_res), len(tta_res[0][0])\n\n(1, 3469)\n\n\n\nprep_submission('subm_large_swinv2.csv', tta_res)\n!head subm_large_swinv2.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\n\n# large convnext only\ntta_res = load_pickle('/kaggle/working/tta_res2.pkl')\ntta_res = [tta_res[1]]\nlen(tta_res), len(tta_res[0][0])\n\n(1, 3469)\n\n\n\nprep_submission('subm_large_convnext.csv', tta_res)\n!head subm_large_convnext.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\n\n# large vit only\ntta_res = load_pickle('/kaggle/working/tta_res2.pkl')\ntta_res = [tta_res[2]]\nlen(tta_res), len(tta_res[0][0])\n\n(1, 3469)\n\n\n\nprep_submission('subm_large_vit.csv', tta_res)\n!head subm_large_vit.csv\n\nimage_id,label\n\n200001.jpg,hispa\n\n200002.jpg,normal\n\n200003.jpg,blast\n\n200004.jpg,blast\n\n200005.jpg,blast\n\n200006.jpg,brown_spot\n\n200007.jpg,dead_heart\n\n200008.jpg,brown_spot\n\n200009.jpg,hispa\n\n\nHere is the final summary of the public and private scores for each submission:\n\n\n\n\n\n\n\n\n\nSubmission\nDescription\nPrivate Score\nPublic Score\n\n\n\n\n1\ninitial submission file after creating a quick small model following Jeremy Howard’s walkthrough video.\n0.13709\n0.12418\n\n\n2\ninitial submission using convnext small 2 epochs fine-tuned sorted file list\n0.94124\n0.92541\n\n\n3\nsquish convnext small 12 epoch ft tta\n0.98156\n0.98308\n\n\n4\nensemble small 12 epoch ft tta\n0.98617*\n0.98423\n\n\n5\nswinv2 convnext vit large ensemble 12 epoch ft tta\n0.97811\n0.98039\n\n\n6\nswinv2 convnext vit large ensemble 24 epoch ft tta\n0.98502\n0.98539**\n\n\n7\nswinv2 (3x convnext) vit large ensemble 24 epoch ft tta\n0.98387\n0.98423\n\n\n8\n(3x swinv2) convnext vit large ensemble 24 epoch ft tta\n0.98156\n0.985\n\n\n9\nswinv2 convnext (3x vit) large ensemble 24 epoch ft tta\n0.98617*\n0.98462\n\n\n10\nswinv2 large 24 epoch ft tta\n0.98271\n0.98269\n\n\n11\nconvnext large 24 epoch ft tta\n0.98502\n0.98269\n\n\n12\nvit large 24 epoch ft tta\n0.97811\n0.98231\n\n\n\n* largest private score\n** largest public score\nThe two ensembles with the largest private score (0.98617) which I would therefore consider my best submissions:\n\nThree small models (swinv2, convnext, vit) trained for 12 epochs with predictions calculated using TTA.\nThree large models (swinv2, convnext, vit) trained for 24 epochs with predictions calculated using TTA and with a weighted average using 3 times the vit model’s predictions.\n\nI would have expected the larger models trained on more epochs to have a higher private and public score than their smaller versions trained on half the number of epochs.\nIn my next blog post I walk through the discussion and code from Live Coding 12. I continue improving my large ensemble in Part 7 of this blog post series."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-3/index.html",
    "href": "posts/2024-02-05-paddy-part-3/index.html",
    "title": "Paddy Doctor Kaggle Competition - Part 3",
    "section": "",
    "text": "In the fastai course Part 1 Lesson 6 video Jeremy Howard walked through the notebooks First Steps: Road to the Top, Part 1 and Small models: Road to the Top, Part 2 where he builds increasingly accurate solutions to the Paddy Doctor: Paddy Disease Classification Kaggle Competition. In the video, Jeremy referenced a series of walkthrough videos that he made while working through the four-notebook series for this competition. I’m excited to watch these walkthroughs to better understand how to approach a Kaggle competition from the perspective of a former #1 Kaggle grandmaster.\nIn this blog post series, I’ll walk through the code Jeremy shared in each of the 6 Live Coding videos focused on this competition, submitting predictions to Kaggle along the way. My last two blog posts in this series reference Jeremy’s Scaling Up: Road to the Top, Part 3 notebook to improve my large model ensemble predictions. Here are the links to each of the blog posts in this series:\n\nPart 1: Live Coding 8\nPart 2: Live Coding 9\nPart 3: Live Coding 10 (You are here)\nPart 4: Live Coding 11\nPart 5: Live Coding 12\nPart 6: Live Coding 13\nPart 7: Improving My Large Ensemble, Part 1\nPart 8: Improving My Large Ensemble, Part 2\n\nLink to the Live Coding 10 video"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-3/index.html#setup",
    "href": "posts/2024-02-05-paddy-part-3/index.html#setup",
    "title": "Paddy Doctor Kaggle Competition - Part 3",
    "section": "Setup",
    "text": "Setup\n\n!pip install -qq timm==0.6.13\nimport timm\ntimm.__version__\n\n'0.6.13'\n\n\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai')\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\n\npath.ls()\n\n(#4) [Path('../input/paddy-disease-classification/sample_submission.csv'),Path('../input/paddy-disease-classification/train_images'),Path('../input/paddy-disease-classification/train.csv'),Path('../input/paddy-disease-classification/test_images')]\n\n\n\ntrn_path = path/'train_images'"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-3/index.html#the-best-models-for-fine-tuning-image-recognition",
    "href": "posts/2024-02-05-paddy-part-3/index.html#the-best-models-for-fine-tuning-image-recognition",
    "title": "Paddy Doctor Kaggle Competition - Part 3",
    "section": "The best models for fine tuning image recognition",
    "text": "The best models for fine tuning image recognition\nAll deep learning models will return a set of probabilities. That’s what their final layer returns and we decode them using argmax across them. There’s nothing to stop you from using those probabilities directly.\nThe Paddy Classification competition are kind of like the natural images you see in ImageNet, but ImageNet doesn’t have any categories about diseases, they have categories about what’s the main object in this image, such as different types of grass or fields or something. It’s a bit different to ImageNet, which is what most of our pretrained models are trained on. Nearly all of the images are the same shape and size in this competition.\nThere are two key dimensions that really seem to impact how well a model can be fine-tuned: - How similar is your dataset to the dataset used for the pretrained model? - If it’s similar (like PETS to ImageNet), then the critical factor is how well does the fine-tuning of the model maintain the weights that are pretrained. They’re probably not going to change very much. And you can take advantage of really big accurate models that have learned to do almost the exact same thing that you are trying to do. - If it’s not similar (like Planets to ImageNet), a lot of the weights of a pretrained model are going to be useless for fine-tuning this because they’e learned specific features (like what does text look like, what do eyeballs look like, what does fur look like) none of which are going to be useful at all. - How big is your dataset? - On a big dataset, you’ve got time and epochs to take advantage of having lots of parameters in the model to learn to use them effectively. If you don’t have much data you don’t have much ability to do that.\nJeremy and Thomas Capelle analyze which models are the best for fine-tuning and Jeremy published the results in this notebook. They used YAML files for Weights and Biases to define the different models and parameters that they wanted to test. You can use the wandb web GUI to view the training results. This gist has the results.\nYou can export a pandas.DataFrame to a StringIO() object which essentially stores the data as a string.\nfrom io import StringIO\nstrm = StringIO()\ndf.to_csv(strm, index=False)\ntxt = strm.getvalue()\nYou can also create a gist programatically:\nimport ghapi.core as gh\ng = gh.GhApi()\ngist = g.create_gist('name', txt, filename='name.csv', public=True)\n\n# view URL\ngist.html_url\nThe vit family of models is particularly good at rapidly identifying features of data types it hasn’t seen before (like medical imaging or satellite imagery). They also have good error rate with low memory usage. The swin family, also a transformers-based model like vit, was the most accurate for fine-tuning the Planets dataset. For the Planets dataset, the really big slow models don’t necessarily have better error rates. Which makes sense because if they have heaps of parameters but they’re trying to learn something they’ve never seen before it’s unlikely that we will be able to take advantage of those parameters.\nFor some models (like vit_small_patch16_224) you can only use 224x224 image sizes, while with others (like convnext_tiny) you can use any sized images.\nJeremy ran the vision model fine-tuning on 3 RTX GPUs for about 12 hours. They didn’t try all combinations of all parameters. Thomas ran a learning rate sweep to get a sense of what learning rates work well, and then they tried a couple of learning rates, a couple of the best resize methods and a couple of the best pooling types across a few broadly different kinds of models across the two different datasets. In every single case, the same learning rate, resize method and pooling method was the best."
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-3/index.html#applying-learning-on-paddy-notebook-with-small-models",
    "href": "posts/2024-02-05-paddy-part-3/index.html#applying-learning-on-paddy-notebook-with-small-models",
    "title": "Paddy Doctor Kaggle Competition - Part 3",
    "section": "Applying learning on Paddy notebook with small models",
    "text": "Applying learning on Paddy notebook with small models\nLet’s try out some of these models for the paddy classification task to identify which ones’ larger versions we should try training next. We use a fixed validation seed (seed=42) so that the same validation set is created each time we run train. The final batch size in a convnext model is 32x32 you generally you want both sides of the image to be sized in multiples of 32. The correct dimensions for Resize is 640 by 480.\n\ndef train(arch, item, batch, accum=False):\n    kwargs = {'bs': 16} if accum else {}\n    dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch, **kwargs)\n    cbs = GradientAccumulation(4) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    learn.fine_tune(12, 0.01)\n    return error_rate(*learn.tta(dl=dls.valid))\n\n\narch = 'convnext_small_in22k'\n\n\ntrain(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=224, min_scale=0.75))\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small_22k_224.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.069485\n      0.617368\n      0.193657\n      01:13\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.513920\n      0.278679\n      0.093224\n      01:31\n    \n    \n      1\n      0.369457\n      0.236429\n      0.076886\n      01:33\n    \n    \n      2\n      0.344589\n      0.229747\n      0.074003\n      01:30\n    \n    \n      3\n      0.259019\n      0.175089\n      0.050457\n      01:28\n    \n    \n      4\n      0.224322\n      0.149210\n      0.041326\n      01:28\n    \n    \n      5\n      0.176708\n      0.155431\n      0.047573\n      01:28\n    \n    \n      6\n      0.128338\n      0.155574\n      0.040846\n      01:28\n    \n    \n      7\n      0.096755\n      0.103420\n      0.026430\n      01:28\n    \n    \n      8\n      0.083143\n      0.086435\n      0.025469\n      01:28\n    \n    \n      9\n      0.053020\n      0.089935\n      0.021624\n      01:28\n    \n    \n      10\n      0.038454\n      0.082519\n      0.021624\n      01:31\n    \n    \n      11\n      0.041188\n      0.081926\n      0.019222\n      01:32\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0211)\n\n\n\ntrain(arch, item=Resize(480), batch=aug_transforms(size=224, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.092957\n      0.656337\n      0.207593\n      01:12\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.534311\n      0.283374\n      0.097069\n      01:29\n    \n    \n      1\n      0.404589\n      0.271343\n      0.091783\n      01:30\n    \n    \n      2\n      0.366122\n      0.263794\n      0.077367\n      01:28\n    \n    \n      3\n      0.291584\n      0.194437\n      0.056223\n      01:26\n    \n    \n      4\n      0.245451\n      0.202364\n      0.058145\n      01:26\n    \n    \n      5\n      0.176800\n      0.145820\n      0.043248\n      01:27\n    \n    \n      6\n      0.141820\n      0.128727\n      0.038443\n      01:26\n    \n    \n      7\n      0.105305\n      0.103860\n      0.029313\n      01:26\n    \n    \n      8\n      0.082278\n      0.099908\n      0.024988\n      01:26\n    \n    \n      9\n      0.061129\n      0.090908\n      0.020183\n      01:26\n    \n    \n      10\n      0.049765\n      0.085010\n      0.017780\n      01:26\n    \n    \n      11\n      0.042815\n      0.082840\n      0.018260\n      01:26\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0202)\n\n\n\ntrain(arch, item=Resize((640,480)), batch=aug_transforms(size=(288,224), min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.074075\n      0.577121\n      0.189332\n      01:27\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.515035\n      0.284355\n      0.092263\n      01:48\n    \n    \n      1\n      0.400951\n      0.292205\n      0.091783\n      01:48\n    \n    \n      2\n      0.322861\n      0.263579\n      0.079769\n      01:48\n    \n    \n      3\n      0.302507\n      0.182555\n      0.056223\n      01:48\n    \n    \n      4\n      0.240202\n      0.166032\n      0.049015\n      01:48\n    \n    \n      5\n      0.181676\n      0.171471\n      0.046132\n      01:48\n    \n    \n      6\n      0.128153\n      0.124866\n      0.036040\n      01:47\n    \n    \n      7\n      0.105105\n      0.111518\n      0.028352\n      01:48\n    \n    \n      8\n      0.073392\n      0.093408\n      0.024988\n      01:48\n    \n    \n      9\n      0.051107\n      0.083389\n      0.024027\n      01:48\n    \n    \n      10\n      0.042867\n      0.083621\n      0.023066\n      01:48\n    \n    \n      11\n      0.038255\n      0.084581\n      0.022585\n      01:48\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0187)\n\n\n\ntrain(arch, item=Resize((640,480)), batch=aug_transforms(size=(320,240), min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.043271\n      0.641115\n      0.211917\n      01:40\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.481680\n      0.278677\n      0.089380\n      02:02\n    \n    \n      1\n      0.364523\n      0.263106\n      0.082653\n      02:02\n    \n    \n      2\n      0.349608\n      0.226119\n      0.063431\n      02:02\n    \n    \n      3\n      0.297600\n      0.197567\n      0.056223\n      02:01\n    \n    \n      4\n      0.221989\n      0.189447\n      0.058145\n      02:01\n    \n    \n      5\n      0.160790\n      0.156223\n      0.037482\n      02:02\n    \n    \n      6\n      0.120237\n      0.125078\n      0.037963\n      02:02\n    \n    \n      7\n      0.092999\n      0.136008\n      0.035079\n      02:01\n    \n    \n      8\n      0.070052\n      0.101822\n      0.027391\n      02:01\n    \n    \n      9\n      0.051421\n      0.095571\n      0.024507\n      02:01\n    \n    \n      10\n      0.037683\n      0.093875\n      0.023066\n      02:01\n    \n    \n      11\n      0.040058\n      0.093482\n      0.023066\n      02:01\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0226)\n\n\n\narch = 'vit_small_patch16_224'\n\n\ntrain(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=224, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.241114\n      0.609537\n      0.202787\n      01:00\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.603312\n      0.330619\n      0.102355\n      01:05\n    \n    \n      1\n      0.454617\n      0.272407\n      0.090822\n      01:05\n    \n    \n      2\n      0.432220\n      0.399525\n      0.128784\n      01:05\n    \n    \n      3\n      0.343562\n      0.381830\n      0.123018\n      01:05\n    \n    \n      4\n      0.276432\n      0.273114\n      0.068717\n      01:06\n    \n    \n      5\n      0.229089\n      0.318629\n      0.077847\n      01:05\n    \n    \n      6\n      0.167870\n      0.146931\n      0.033157\n      01:05\n    \n    \n      7\n      0.117221\n      0.128760\n      0.037963\n      01:05\n    \n    \n      8\n      0.090773\n      0.112749\n      0.031235\n      01:05\n    \n    \n      9\n      0.073209\n      0.105501\n      0.028352\n      01:05\n    \n    \n      10\n      0.060867\n      0.107474\n      0.027871\n      01:05\n    \n    \n      11\n      0.061845\n      0.104577\n      0.028832\n      01:05\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0245)\n\n\n\ntrain(arch, item=Resize(480), batch=aug_transforms(size=224, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.264427\n      0.745677\n      0.241711\n      00:57\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.636773\n      0.356237\n      0.111485\n      01:03\n    \n    \n      1\n      0.512687\n      0.324432\n      0.112926\n      01:03\n    \n    \n      2\n      0.445590\n      0.373493\n      0.122537\n      01:03\n    \n    \n      3\n      0.386593\n      0.335397\n      0.106679\n      01:03\n    \n    \n      4\n      0.314561\n      0.262394\n      0.074003\n      01:03\n    \n    \n      5\n      0.236516\n      0.197571\n      0.060067\n      01:03\n    \n    \n      6\n      0.197938\n      0.153093\n      0.040846\n      01:03\n    \n    \n      7\n      0.159178\n      0.132239\n      0.038924\n      01:03\n    \n    \n      8\n      0.109954\n      0.117727\n      0.029313\n      01:03\n    \n    \n      9\n      0.084283\n      0.104230\n      0.025469\n      01:03\n    \n    \n      10\n      0.073850\n      0.100741\n      0.024988\n      01:03\n    \n    \n      11\n      0.064490\n      0.098695\n      0.024988\n      01:03\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0250)\n\n\n\ntrain(arch, item=Resize(640, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=224, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.313841\n      0.846934\n      0.269582\n      01:04\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.678171\n      0.413112\n      0.135031\n      01:11\n    \n    \n      1\n      0.497201\n      0.349746\n      0.111004\n      01:10\n    \n    \n      2\n      0.411814\n      0.311638\n      0.098991\n      01:10\n    \n    \n      3\n      0.410544\n      0.440684\n      0.128784\n      01:10\n    \n    \n      4\n      0.309415\n      0.252958\n      0.070159\n      01:10\n    \n    \n      5\n      0.241980\n      0.270128\n      0.073042\n      01:10\n    \n    \n      6\n      0.186923\n      0.202601\n      0.056223\n      01:10\n    \n    \n      7\n      0.130820\n      0.165027\n      0.043729\n      01:10\n    \n    \n      8\n      0.092804\n      0.121890\n      0.030274\n      01:10\n    \n    \n      9\n      0.072829\n      0.123613\n      0.029313\n      01:10\n    \n    \n      10\n      0.069157\n      0.110147\n      0.029793\n      01:10\n    \n    \n      11\n      0.054325\n      0.108744\n      0.026430\n      01:09\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0221)\n\n\n\narch = 'swinv2_base_window12_192_22k'\n\n\ntrain(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=192, min_scale=0.75))\n\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth\" to /root/.cache/torch/hub/checkpoints/swinv2_base_patch4_window12_192_22k.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.036088\n      0.583672\n      0.193176\n      02:14\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.509049\n      0.234983\n      0.078328\n      02:38\n    \n    \n      1\n      0.385443\n      0.205435\n      0.070159\n      02:38\n    \n    \n      2\n      0.334598\n      0.355438\n      0.089380\n      02:38\n    \n    \n      3\n      0.285663\n      0.368389\n      0.106679\n      02:39\n    \n    \n      4\n      0.238095\n      0.159115\n      0.045651\n      02:38\n    \n    \n      5\n      0.183420\n      0.140284\n      0.041326\n      02:38\n    \n    \n      6\n      0.141127\n      0.129525\n      0.036040\n      02:38\n    \n    \n      7\n      0.103826\n      0.111331\n      0.029313\n      02:38\n    \n    \n      8\n      0.077789\n      0.109304\n      0.027391\n      02:38\n    \n    \n      9\n      0.053972\n      0.096646\n      0.022585\n      02:38\n    \n    \n      10\n      0.041229\n      0.088552\n      0.021624\n      02:38\n    \n    \n      11\n      0.034090\n      0.088425\n      0.021144\n      02:38\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0173)\n\n\n\ntrain(arch, item=Resize(480), batch=aug_transforms(size=192, min_scale=0.75), accum=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.345018\n      0.807008\n      0.224892\n      02:29\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.566454\n      0.335172\n      0.117251\n      03:16\n    \n    \n      1\n      0.569964\n      0.336681\n      0.125901\n      03:17\n    \n    \n      2\n      0.562002\n      0.343439\n      0.118212\n      03:17\n    \n    \n      3\n      0.469339\n      0.393603\n      0.124459\n      03:17\n    \n    \n      4\n      0.297434\n      0.332929\n      0.090822\n      03:17\n    \n    \n      5\n      0.269842\n      0.198136\n      0.051898\n      03:17\n    \n    \n      6\n      0.186959\n      0.181704\n      0.054781\n      03:17\n    \n    \n      7\n      0.134943\n      0.134798\n      0.036040\n      03:17\n    \n    \n      8\n      0.113144\n      0.102160\n      0.030274\n      03:17\n    \n    \n      9\n      0.085017\n      0.104802\n      0.025469\n      03:17\n    \n    \n      10\n      0.048129\n      0.101891\n      0.022105\n      03:17\n    \n    \n      11\n      0.057491\n      0.094901\n      0.022585\n      03:17\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0183)\n\n\n\ntrain(arch, item=Resize(640, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=192, min_scale=0.75), accum=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.316884\n      1.035790\n      0.263335\n      02:35\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.617098\n      0.291554\n      0.094666\n      03:22\n    \n    \n      1\n      0.603711\n      0.409637\n      0.126862\n      03:23\n    \n    \n      2\n      0.573029\n      0.425025\n      0.127823\n      03:23\n    \n    \n      3\n      0.401325\n      0.402042\n      0.117732\n      03:23\n    \n    \n      4\n      0.340665\n      0.308467\n      0.089380\n      03:23\n    \n    \n      5\n      0.236972\n      0.177212\n      0.046132\n      03:23\n    \n    \n      6\n      0.212541\n      0.151314\n      0.041807\n      03:23\n    \n    \n      7\n      0.099307\n      0.110350\n      0.026430\n      03:23\n    \n    \n      8\n      0.054712\n      0.108030\n      0.022105\n      03:23\n    \n    \n      9\n      0.051622\n      0.100666\n      0.020183\n      03:22\n    \n    \n      10\n      0.032429\n      0.102271\n      0.022105\n      03:22\n    \n    \n      11\n      0.031421\n      0.097009\n      0.022105\n      03:22\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0192)\n\n\n\narch = 'swin_small_patch4_window7_224'\n\n\ntrain(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=224, min_scale=0.75))\n\nDownloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth\" to /root/.cache/torch/hub/checkpoints/swin_small_patch4_window7_224.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.424551\n      0.834437\n      0.278712\n      01:35\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.659169\n      0.377870\n      0.125420\n      01:48\n    \n    \n      1\n      0.487998\n      0.293272\n      0.092263\n      01:48\n    \n    \n      2\n      0.439836\n      0.344214\n      0.101874\n      01:49\n    \n    \n      3\n      0.337822\n      0.243527\n      0.074964\n      01:48\n    \n    \n      4\n      0.262154\n      0.199788\n      0.065353\n      01:49\n    \n    \n      5\n      0.206655\n      0.129096\n      0.038924\n      01:48\n    \n    \n      6\n      0.179885\n      0.116743\n      0.031716\n      01:48\n    \n    \n      7\n      0.118040\n      0.118282\n      0.035079\n      01:48\n    \n    \n      8\n      0.092112\n      0.114298\n      0.028832\n      01:48\n    \n    \n      9\n      0.078792\n      0.105398\n      0.025949\n      01:48\n    \n    \n      10\n      0.064473\n      0.097622\n      0.024027\n      01:48\n    \n    \n      11\n      0.057387\n      0.097082\n      0.024027\n      01:49\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0207)\n\n\n\ntrain(arch, item=Resize(480), batch=aug_transforms(size=224, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.420280\n      0.869214\n      0.276790\n      01:34\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.727566\n      0.402595\n      0.133109\n      01:47\n    \n    \n      1\n      0.549589\n      0.400400\n      0.129265\n      01:48\n    \n    \n      2\n      0.440090\n      0.304687\n      0.101394\n      01:48\n    \n    \n      3\n      0.397689\n      0.340592\n      0.112926\n      01:48\n    \n    \n      4\n      0.288660\n      0.184638\n      0.057184\n      01:48\n    \n    \n      5\n      0.246669\n      0.180551\n      0.049976\n      01:47\n    \n    \n      6\n      0.189145\n      0.161568\n      0.043729\n      01:48\n    \n    \n      7\n      0.151034\n      0.160868\n      0.039885\n      01:48\n    \n    \n      8\n      0.110399\n      0.115093\n      0.026910\n      01:48\n    \n    \n      9\n      0.084655\n      0.098188\n      0.025469\n      01:48\n    \n    \n      10\n      0.070253\n      0.093308\n      0.023066\n      01:48\n    \n    \n      11\n      0.064076\n      0.095348\n      0.024027\n      01:48\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0231)\n\n\n\ntrain(arch, item=Resize(640, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros), batch=aug_transforms(size=224, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.479291\n      1.005589\n      0.330610\n      01:41\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.758326\n      0.441894\n      0.145123\n      01:55\n    \n    \n      1\n      0.548370\n      0.436102\n      0.139356\n      01:54\n    \n    \n      2\n      0.444455\n      0.361651\n      0.104277\n      01:55\n    \n    \n      3\n      0.370136\n      0.280115\n      0.088419\n      01:55\n    \n    \n      4\n      0.269262\n      0.184901\n      0.059106\n      01:54\n    \n    \n      5\n      0.242950\n      0.177827\n      0.054781\n      01:55\n    \n    \n      6\n      0.171754\n      0.153312\n      0.039404\n      01:55\n    \n    \n      7\n      0.128885\n      0.118345\n      0.030754\n      01:54\n    \n    \n      8\n      0.098144\n      0.103212\n      0.025949\n      01:54\n    \n    \n      9\n      0.078017\n      0.098263\n      0.024988\n      01:54\n    \n    \n      10\n      0.062568\n      0.092275\n      0.021624\n      01:54\n    \n    \n      11\n      0.055316\n      0.091669\n      0.021624\n      01:54\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.0183)\n\n\nI’ll summarize the training run parameters and resulting TTA error rates on the validation set in the following table. I have sorted this table by model name and descening TTA Error Rate (First Run).\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nitem_tfms\nbatch_tfms\nTTA Error Rate (First Run)\nMinutes per epoch (First Run)\nTTA Error Rate (Second Run)\n\n\n\n\nconvnext_small_in22k\nResize((640,480))\naug_transforms(size=(288,224), min_scale=0.75)\n0.0178*\n01:51\n0.0187\n\n\nconvnext_small_in22k\nResize((640,480))\naug_transforms(size=(320,240), min_scale=0.75)\n0.0202\n02:07\n0.0226\n\n\nconvnext_small_in22k\nResize(480, method='squish')\naug_transforms(size=224, min_scale=0.75)\n0.0211\n01:30\n0.0211\n\n\nconvnext_small_in22k\nResize(480)\naug_transforms(size=224, min_scale=0.75)\n0.0216\n01:29\n0.0202\n\n\nvit_small_patch16_224\nResize(480)\naug_transforms(size=224, min_scale=0.75)\n0.0202*\n00:44\n0.0250\n\n\nvit_small_patch16_224\nResize(480, method='squish')\naug_transforms(size=224, min_scale=0.75)\n0.0216\n00:47\n0.0245\n\n\nvit_small_patch16_224\nResize(640, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros)\naug_transforms(size=224, min_scale=0.75)\n0.0226\n00:50\n0.0221\n\n\nswinv2_base_window12_192_22k\nResize(480, method='squish')\naug_transforms(size=192, min_scale=0.75)\n0.0163*\n02:30\n0.0173\n\n\nswinv2_base_window12_192_22k\nResize(640, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros)\naug_transforms(size=192, min_scale=0.75)\n0.0187\n03:27\n0.0192\n\n\nswinv2_base_window12_192_22k\nResize(480)\naug_transforms(size=192, min_scale=0.75)\n0.0197\n03:22\n0.0183\n\n\nswin_small_patch4_window7_224\nResize(480, method='squish')\naug_transforms(size=224, min_scale=0.75)\n0.0202*\n01:48\n0.0207\n\n\nswin_small_patch4_window7_224\nResize(480)\naug_transforms(size=224, min_scale=0.75)\n0.0207\n01:47\n0.0231\n\n\nswin_small_patch4_window7_224\nResize(640, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros)\naug_transforms(size=224, min_scale=0.75)\n0.0221\n01:54\n0.0183\n\n\n\n* = lowest error rate for the architecture\n\nPreparing an Ensemble for Kaggle Submission\nI’ll retrain and create an ensemble of the top 3 models based on TTA Error Rate (First Run):\n\nswinv2_base_window12_192_22k (0.0163)\nconvnext_small_in22k (0.0178)\nvit_small_patch16_224 (0.0202)\n\nThe swin_small_patch4_window7_224 models did not outperform the quicker/smaller vit model so I won’t use them in this submission.\nLater on in the video, Jeremy walks through an example of how he trained large versions of the small models he tested. In this section, he used the following training function, which I’ll use here for these small models, to prepare my submission predictions. Note that Jeremy has removed seed=42 since in the ensemble for submission we want to use different validation sets when training each model (whereas before we wanted to use the same validation set to better compare the performance between models). I’ve also changed a couple of things (I’m not exporting the models, and I’m using a smaller batch size).\n\n# store the tta predictions in a list\ntta_res = []\n\n\n# run this once and re-use for all trainings\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\n\n\ndef train(arch, item, batch, accum=False):\n    kwargs = {'bs': 16} if accum else {}\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item, batch_tfms=batch, **kwargs)\n    cbs = GradientAccumulation(2) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    learn.fine_tune(12, 0.01)\n    \n    # TTA predictions using test dataset\n    tst_dl = dls.test_dl(tst_files)\n    tta_res.append(learn.tta(dl=dls.test_dl(tst_files)))\n\n\narch = 'swinv2_base_window12_192_22k'\n\n\ntrain(arch, item=Resize(480, method='squish'), batch=aug_transforms(size=192, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.093678\n      0.758215\n      0.250360\n      02:12\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.472679\n      0.248962\n      0.077847\n      02:36\n    \n    \n      1\n      0.383199\n      0.263211\n      0.081211\n      02:36\n    \n    \n      2\n      0.360025\n      0.292500\n      0.103316\n      02:36\n    \n    \n      3\n      0.305790\n      0.223976\n      0.066314\n      02:36\n    \n    \n      4\n      0.232600\n      0.209275\n      0.058145\n      02:36\n    \n    \n      5\n      0.185068\n      0.171094\n      0.043729\n      02:36\n    \n    \n      6\n      0.134446\n      0.165977\n      0.039885\n      02:36\n    \n    \n      7\n      0.108682\n      0.135310\n      0.031716\n      02:36\n    \n    \n      8\n      0.074768\n      0.124852\n      0.026430\n      02:36\n    \n    \n      9\n      0.052246\n      0.107549\n      0.024027\n      02:36\n    \n    \n      10\n      0.040028\n      0.102177\n      0.023546\n      02:36\n    \n    \n      11\n      0.038975\n      0.102109\n      0.022585\n      02:36\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0])\n\n(1, 3469)\n\n\n\narch = 'convnext_small_in22k'\n\n\ntrain(arch, item=Resize((640,480)), batch=aug_transforms(size=(288,224), min_scale=0.75))\n\nDownloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small_22k_224.pth\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.088028\n      0.659407\n      0.192696\n      01:26\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.488645\n      0.251234\n      0.082172\n      01:45\n    \n    \n      1\n      0.394844\n      0.260079\n      0.086497\n      01:45\n    \n    \n      2\n      0.341203\n      0.206835\n      0.065834\n      01:46\n    \n    \n      3\n      0.294899\n      0.183829\n      0.057665\n      01:45\n    \n    \n      4\n      0.224933\n      0.172018\n      0.045651\n      01:45\n    \n    \n      5\n      0.179294\n      0.139805\n      0.037482\n      01:46\n    \n    \n      6\n      0.131405\n      0.104101\n      0.027871\n      01:45\n    \n    \n      7\n      0.094273\n      0.112815\n      0.031235\n      01:45\n    \n    \n      8\n      0.064216\n      0.106544\n      0.029313\n      01:46\n    \n    \n      9\n      0.045855\n      0.091775\n      0.021144\n      01:45\n    \n    \n      10\n      0.039155\n      0.086264\n      0.021624\n      01:45\n    \n    \n      11\n      0.027725\n      0.083699\n      0.020183\n      01:45\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0])\n\n(2, 3469, 3469)\n\n\n\narch = 'vit_small_patch16_224'\n\n\ntrain(arch, item=Resize(480), batch=aug_transforms(size=224, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.258543\n      0.658905\n      0.220087\n      00:56\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.630974\n      0.367167\n      0.113407\n      01:02\n    \n    \n      1\n      0.496218\n      0.381497\n      0.124940\n      01:03\n    \n    \n      2\n      0.424657\n      0.341580\n      0.111004\n      01:02\n    \n    \n      3\n      0.381134\n      0.273908\n      0.087458\n      01:02\n    \n    \n      4\n      0.326845\n      0.227150\n      0.072561\n      01:02\n    \n    \n      5\n      0.253998\n      0.209598\n      0.062951\n      01:02\n    \n    \n      6\n      0.179893\n      0.189200\n      0.046612\n      01:02\n    \n    \n      7\n      0.146728\n      0.211501\n      0.045651\n      01:02\n    \n    \n      8\n      0.113472\n      0.159040\n      0.036040\n      01:02\n    \n    \n      9\n      0.076088\n      0.145309\n      0.033157\n      01:02\n    \n    \n      10\n      0.068731\n      0.140491\n      0.031716\n      01:02\n    \n    \n      11\n      0.059864\n      0.140173\n      0.030754\n      01:02\n    \n  \n\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tta_res), len(tta_res[0][0]), len(tta_res[1][0]), len(tta_res[2][0])\n\n(3, 3469, 3469, 3469)\n\n\nBefore I stack the predictions and prepare them for the submission, I’ll save the list of predictions:\n\nsave_pickle('/kaggle/working/tta_res.pkl', tta_res)\n\nNext, I’ll take a quick detour and follow the steps Jeremy shares in Live Coding 11.\nFirst, he takes the first item from each list in tta_res (the predictions) and stores them in a list called tta_prs. The list returned by learn.tta has a second item of None, which represents the targets (which we don’t have in the test set) we need to pick out just the first item (the predictions).\nzipping the items in tta_res creates a list of two tuples: a tuple with the three sets of predictions (the first item in each element of tta_res) and a tuple with three Nones (the second item of each element of tta_res).\nHere’s a toy example to illustrate:\n\nlist(zip(*[[(1), None],[(2), None]]))\n\n[(1, 2), (None, None)]\n\n\nThe first function rerurns the first element of an iterable object.\n\nfirst??\n\n\nSignature: first(x, f=None, negate=False, **kwargs)\nSource:   \ndef first(x, f=None, negate=False, **kwargs):\n    \"First element of `x`, optionally filtered by `f`, or None if missing\"\n    x = iter(x)\n    if f: x = filter_ex(x, f=f, negate=negate, gen=True, **kwargs)\n    return next(x, None)\nFile:      /opt/conda/lib/python3.10/site-packages/fastcore/basics.py\nType:      function\n\n\n\n\nfirst(list(zip(*[[(1), None],[(2), None]])))\n\n(1, 2)\n\n\nThe second element of the zipped tta_res list is a tuple of Nones.\n\nlist(zip(*tta_res))[1]\n\n(None, None, None)\n\n\nI’ll now apply this code to tta_res:\n\ntta_prs = first(zip(*tta_res))\n\n\nlen(tta_prs[0])\n\n3469\n\n\nNext, in order to take the mean value of the predictions, we stack them into a tensor:\n\nt_tta = torch.stack(tta_prs)\n\n\nt_tta.shape\n\ntorch.Size([3, 3469, 10])\n\n\nThen, we take the mean of the three predictions for each of the 10 classes for each image.\n\navg_pr = t_tta.mean(0)\n\n\navg_pr.shape\n\ntorch.Size([3469, 10])\n\n\nWe then get the index of the largest probability out of the 10 classes for each image, which is the “prediction” that the model has made for the image.\n\nidxs = avg_pr.argmax(dim=1)\nidxs.shape\n\ntorch.Size([3469])\n\n\n\nidxs\n\ntensor([7, 8, 3,  ..., 8, 1, 5])\n\n\nFinally, we convert those indexes to strings of disease names using the vocab and prepare the submission file:\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(224))\nmapping = dict(enumerate(dls.vocab))\nss = pd.read_csv(path/'sample_submission.csv')\nresults = pd.Series(idxs.numpy(), name='idxs').map(mapping)\nss.label = results\nss.to_csv('ensemble_subm.csv', index=False)\n\n\n!head ensemble_subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa"
  },
  {
    "objectID": "posts/2024-02-05-paddy-part-3/index.html#gradient-accumulation-to-prevent-out-of-memory",
    "href": "posts/2024-02-05-paddy-part-3/index.html#gradient-accumulation-to-prevent-out-of-memory",
    "title": "Paddy Doctor Kaggle Competition - Part 3",
    "section": "Gradient accumulation to prevent out of memory",
    "text": "Gradient accumulation to prevent out of memory\nIf you run out of memory while training any of these large models, you can use GradientAccumulation to lower the memory usage. In the training loop we get the gradients, we add the gradients times the learning rate to the weights, and then we zero the gradients. What you could do is halve the batch size, so for example from 64 to 32, and then only zero the gradients every two iterations, and only do the update every two iterations. So you calculate in two batches what you calculate in one batch and it will be mathematically identical, That’s called GradientAccumulation, which added to the Learner as a callback, which are things that change the behavior of the training.\nHow batches work: we randomly shuffle the dataset, and grab the next batch size of images, we resize them all to be the same size, and we stack them on top of each other. If it’s black and white images for example, we would have 64 (or whatever the batch size is) 640 x 480 (or whatever image size you want) images so we end up with a 64 x 640 x 480 tensor. Pretty much all of the functionality provided by PyTorch will work fine for a mini batch of things just as it would for a single thing.\nInference is often done on CPU instead of GPU since you only need to process one thing at a time. Or people will queue a few of them up and stick them on a GPU.\nIn my next blog post I walk through the discussion and code from Live Coding 11."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "",
    "text": "Show pip installs\n!pip install transformers -Uqq\n!pip install accelerate -qq\n!pip install torch==2.2.2 -qq\n!pip install datasets~=2.16.1 -qq\n!pip install scikit-learn==1.2 -qq"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#background",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#background",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Background",
    "text": "Background\nIn this notebook I’ll use Qwen2-0.5B-Instruct to classify sentiment in the financial_phrasebank dataset. In previous notebooks I have performed sentiment classification with Qwen2-1.5B-Instruct, phi-2, phi-3, phi-3.5, and the Claude series.\nThis notebook is part of a series of blog posts for a project I’m working called TinySentiment where I’m experimenting with tiny models to improve their ability to classify sentiment in the financial_phrasebank dataset. I was inspired to do so after reading this blog post and this corresponding notebook by Moritz Laurer as part of a fastai study group last year.\nHere are the results from my experiments so far (**the best-performing prompt from this notebook):\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nphi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nphi-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\nQwen2-1.5B\n27-Shot\n86.10%\n90% (264/294)\n96% (1320/1382)\n61% (342/561)\n\n\n**Qwen2-0.5B\n17-Shot\n79.48%\n69% (206/300)\n86% (1180/1380)\n71% (400/567)\n\n\n\nHere are the results from this notebook:\n\n\n\nPrompt\nStrategy\nAccuracy\nNegative\nNeutral\nPositive\n\n\n\n\nA\n0-Shot\n62.41%\n91% (276/303)\n53% (735/1391)\n71% (402/570)\n\n\nB\n0-Shot\n47.84%\n90% (274/303)\n57% (789/1391)\n4% (20/570)\n\n\nC\n0-Shot\n40.46%\n91% (276/303)\n43% (594/1391)\n8% (46/570)\n\n\nD\n0-Shot\n68.29%\n79% (240/303)\n61% (851/1391)\n80% (455/570)\n\n\nE\n0-Shot\n51.19%\n97% (293/303)\n28% (396/1391)\n82% (470/570)\n\n\nF\n0-Shot\n48.19%\n94% (286/303)\n21% (287/1391)\n91% (518/570)\n\n\nG\n0-Shot\n61.09%\n93% (282/303)\n46% (646/1391)\n80% (455/570)\n\n\nH\n0-Shot\n65.42%\n85% (257/303)\n57% (798/1391)\n75% (426/570)\n\n\nI\n0-Shot\n66.12%\n81% (245/303)\n58% (800/1391)\n79% (452/570)\n\n\nJ\n3-Shot\n70.94%\n43% (131/302)\n75% (1042/1390)\n76% (431/569)\n\n\nK\n3-Shot\n74.88%\n67% (201/302)\n75% (1043/1390)\n79% (449/569)\n\n\nL\n3-Shot\n68.11%\n49% (149/302)\n65% (900/1390)\n86% (491/569)\n\n\nM\n3-Shot\n56.97%\n49% (149/302)\n45% (625/1390)\n90% (514/569)\n\n\nN\n3-Shot\n73.95%\n62% (188/302)\n75% (1038/1390)\n78% (446/569)\n\n\nO\n3-Shot\n59.97%\n65% (196/302)\n46% (635/1390)\n92% (525/569)\n\n\nP\n6-Shot\n63.91%\n95% (289/303)\n49% (678/1389)\n84% (476/566)\n\n\nQ\n6-Shot\n65.72%\n69% (207/302)\n55% (765/1389)\n90% (512/567)\n\n\nR\n6-Shot\n64.84%\n94% (285/303)\n49% (686/1387)\n87% (493/568)\n\n\nS\n6-Shot\n62.98%\n96% (292/303)\n47% (656/1387)\n83% (474/568)\n\n\nT\n6-Shot\n68.87%\n51% (155/302)\n70% (966/1387)\n76% (434/569)\n\n\nU\n12-Shot\n65.50%\n53% (159/302)\n59% (820/1386)\n88% (496/564)\n\n\nV\n12-Shot\n73.22%\n70% (209/300)\n80% (1103/1386)\n60% (337/566)\n\n\nW\n12-Shot\n70.43%\n82% (246/301)\n66% (912/1384)\n75% (428/567)\n\n\nX\n12-Shot\n76.60%\n91% (270/298)\n72% (1000/1386)\n80% (455/568)\n\n\nY\n12-Shot\n72.56%\n80% (243/303)\n77% (1069/1381)\n57% (322/568)\n\n\nZ\n18-Shot\n71.33%\n50% (150/301)\n75% (1036/1382)\n74% (416/563)\n\n\nAA\n17-Shot\n79.48%\n69% (206/300)\n86% (1180/1380)\n71% (400/567)\n\n\nAB\n18-Shot\n74.22%\n77% (229/299)\n76% (1054/1381)\n68% (384/566)\n\n\nAC\n18-Shot\n68.57%\n49% (148/302)\n73% (1013/1380)\n67% (379/564)\n\n\nAD\n18-Shot\n74.98%\n89% (271/303)\n76% (1052/1379)\n64% (361/564)\n\n\nAE\n24-Shot\n74.91%\n61% (181/299)\n92% (1267/1375)\n41% (230/566)\n\n\nAF\n24-Shot\n73.08%\n37% (112/302)\n91% (1246/1375)\n50% (279/563)\n\n\nAG\n24-Shot\n75.00%\n58% (173/300)\n92% (1265/1375)\n43% (242/565)\n\n\nAH\n24-Shot\n77.46%\n78% (233/299)\n84% (1153/1375)\n62% (349/566)\n\n\nAI\n23-Shot\n75.37%\n48% (143/301)\n92% (1266/1375)\n50% (280/565)\n\n\nAJ\n30-Shot\n77.39%\n58% (172/298)\n94% (1284/1370)\n48% (273/566)\n\n\nAK\n30-Shot\n67.78%\n63% (187/299)\n61% (844/1375)\n86% (483/560)\n\n\nAL\n30-Shot\n76.54%\n58% (173/299)\n86% (1185/1372)\n63% (352/563)\n\n\nAM\n30-Shot\n74.84%\n82% (242/296)\n72% (984/1376)\n79% (446/562)\n\n\nAN\n30-Shot\n73.81%\n51% (154/300)\n77% (1052/1372)\n79% (443/562)\n\n\nAO\n45-Shot\n74.18%\n54% (159/297)\n76% (1034/1366)\n81% (453/556)\n\n\nAP\n45-Shot\n78.73%\n63% (186/296)\n87% (1192/1365)\n66% (369/558)\n\n\nAQ\n45-Shot\n72.01%\n17% (51/301)\n89% (1210/1359)\n60% (337/559)\n\n\nAR\n45-Shot\n73.86%\n53% (157/297)\n80% (1094/1364)\n70% (388/558)\n\n\nAS\n45-Shot\n74.94%\n42% (125/297)\n89% (1219/1363)\n57% (319/559)\n\n\nAT\n60-Shot\n72.19%\n47% (138/292)\n78% (1055/1356)\n72% (398/556)\n\n\nAU\n60-Shot\n76.86%\n43% (127/296)\n91% (1237/1356)\n60% (330/552)\n\n\nAV\n60-Shot\n75.45%\n26% (79/299)\n89% (1206/1352)\n68% (378/553)\n\n\nAW\n60-Shot\n74.46%\n29% (88/299)\n86% (1157/1349)\n71% (396/556)\n\n\nAX\n60-Shot\n79.63%\n62% (179/290)\n94% (1275/1352)\n54% (301/562)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-a",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-a",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt A",
    "text": "Prompt A\nI’ll start out with a simple instruction.\n\npromptA = \"\"\"Label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\nprint(promptA)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\n\n\n\nformatted_prompt = promptA.format(text=dataset[0]['sentence'])\nprint(formatted_prompt)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ngenerate_response(formatted_prompt)\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n'Negative.'\n\n\nGood—at least it’s responding with a sensible answer, although it’s not formatted how I’d like to be, so I expect to need more data cleaning than Qwen2-1.5B-Instruct’s responses.\nAt ~35ms per prompt it will take about 80 seconds to run inference on the full 2264 item dataset.\n\n%timeit -n 10 generate_response(formatted_prompt)\n\n35.4 ms ± 472 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndf, acc = generate_responses(dataset, promptA)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n0.5B yields messier responses. Note the period at the end of some of the strings. For now I’ll manually check each set of responses and clean them accordingly.\n\ndf['responses'].unique()\n\narray(['neutral.', 'positive', 'neutral', 'negative', 'positive.',\n       'negative.', 'negot', 'negative profit', 'net interest', 'teleste',\n       'neglig'], dtype=object)\n\n\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \n\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n\n0.5B doesn’t do terribly on this simple prompt (62.4% accuracy) but it’s almost 20% less accurate than 1.5B (~82% accuracy).\n\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.6241166077738516\n\n\n0.5B does a great job at classifying negative sentiment, does quite well at positive sentences, and has very few other responses overall.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_A.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-b",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-b",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt B",
    "text": "Prompt B\n\npromptB = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptB)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\n\n\nWith this prompt (where the instruction is repeated after the dataset text) 0.5B responds much more cleanly.\n\ndf['responses'].unique()\n\narray(['negative', 'neutral', 'positive'], dtype=object)\n\n\nHowever, it performs almost 20% worse!\n\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.47835689045936397\n\n\nWhile it’s quite good still with negative sentiment, it performs significantly worse on positive sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_B.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-c",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-c",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt C",
    "text": "Prompt C\nI’ll use the same Prompt C as the 1.5B model: a reword of Prompt A (which performed well for 0.5B).\n\npromptC = \"\"\"Respond with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptC)\n\n\n\n\nRespond with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['negative.', 'negative', 'neutral', 'neutral.', 'positive',\n       'positive.', 'negative loss'], dtype=object)\n\n\nThe change in prompt language significantly deteriorates 0.5B’s accuracy.\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.4045936395759717\n\n\n0.5B still does really well on negative sentiment, but does horribly on positive and underwhelming for neutral sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_C.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-d",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-d",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt D",
    "text": "Prompt D\nI’ll change the order of sentiment listed in Prompt A by putting positive first:\n\npromptD = \"\"\"Label the following TEXT with a single word: positive, negative, or neutral\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptD)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral.', 'positive', 'positive.', 'neutral', 'net income',\n       'the text', 'negative', 'negative.', 'negative net', 'negot',\n       'subscription'], dtype=object)\n\n\nChanging the order of sentiment (putting positive first) increases the overall accuracy by ~6%.\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.6828621908127208\n\n\n0.5B’s performance on negative sentiment dips a bit (36 fewer correct) but that is more than compensated by the increase in correctly classified positive (+53) and neutral (+166) sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_D.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-e",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-e",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt E",
    "text": "Prompt E\nI’ll try another combination:\n\npromptE = \"\"\"Label the following TEXT with a single word: negative, neutral, or positive\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptE)\n\n\n\n\nLabel the following TEXT with a single word: negative, neutral, or positive\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['negative.', 'positive', 'negative', 'positive.', 'neutral.',\n       'neutral', 'negative profit', 'negot', 'teleste'], dtype=object)\n\n\nThis ordering of sentiment worsens the accuracy by 10 points.\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.5119257950530035\n\n\n0.5B is nearly perfect for negative sentiment, and quite good with positive sentences, but abysmal for neutral.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_E.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-f",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-f",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt F",
    "text": "Prompt F\nTrying the next permutation of sentiments:\n\npromptF = \"\"\"Label the following TEXT with a single word: positive, neutral, or negative\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptF)\n\n\n\n\nLabel the following TEXT with a single word: positive, neutral, or negative\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral.', 'positive', 'positive.', 'negative', 'negative.',\n       'neutral', 'positive net', 'negativ', 'negot', 'subscription'],\n      dtype=object)\n\n\nThis ordering of sentiments further worsens the overall accuracy.\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.4818904593639576\n\n\npositive sentences are classified correctly at the highest rate so far, and negative sentiment accuracy is very good, but the model does terribly on neutral sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_F.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-g",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-g",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt G",
    "text": "Prompt G\nThe next ordering of sentiments:\n\npromptG = \"\"\"Label the following TEXT with a single word: neutral, negative, or positive\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptG)\n\n\n\n\nLabel the following TEXT with a single word: neutral, negative, or positive\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral.', 'positive', 'positive.', 'neutral', 'negative',\n       'negative.', 'positive profit', 'negot'], dtype=object)\n\n\nThe accuracy of 61% is worse than the best-performing Prompt D (68%).\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.6108657243816255\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_G.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-h",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-h",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt H",
    "text": "Prompt H\nThe last ordering of sentiment:\n\npromptH = \"\"\"Label the following TEXT with a single word: neutral, positive, or negative\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptH)\n\n\n\n\nLabel the following TEXT with a single word: neutral, positive, or negative\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral.', 'positive.', 'neutral', 'negative',\n       'negative.', 'positive profit', 'negot'], dtype=object)\n\n\nThis yields a 65% accuracy.\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.6541519434628975\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_H.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-i",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-i",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt I",
    "text": "Prompt I\nI’ll make a small change to my best-performing prompt by adding a period at the end of the instruction.\n\npromptI = \"\"\"Label the following TEXT with a single word: positive, negative, or neutral.\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptI)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral.', 'positive', 'positive.', 'neutral', 'net income',\n       'negative', 'negative.', 'negative profit', 'nord', 'negot',\n       'the text', 'negation', 'neglig', 'subscription'], dtype=object)\n\n\nAdding a period to the end of the instruction worsens the accuracy a bit.\n\ndf['responses'] = df['responses'].str.replace('.', '', regex=False) \ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.6612190812720848\n\n\nAdding a period worsens the performance on neutral by 51 sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_I.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-j",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-j",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt J",
    "text": "Prompt J\nI’ll now shift my attention to few-shot prompts, starting with 3-Shot.\n\nexclude_idxs = [0, 1, 292]\n\n\npromptJ_ds = ds_subset(dataset, exclude_idxs)\npromptJ_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\npromptJ = \"\"\"Label the following TEXT with a single word: positive, negative, or neutral\nTEXT: {text}\"\"\"\n\nSince ordering seems to matter, I’ll start with a neutral example, positive example and negative example.\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative')]\n\n\n\ndf = few_shot_responses(promptJ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\n3-Shot prompting resulted in the best accuracy so far! ~71%.\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.709420610349403\n\n\nCompared to my best 0-Shot Prompt D (68%) this prompt results in the model significantly underperforming on negative sentences, (131 < 240), but more than making up for it on neutral sentences (1042 > 851).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_J.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-k",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-k",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt K",
    "text": "Prompt K\nI’ll re-order the examples and use the same Prompt J.\n\nexclude_idxs = [0, 292, 1]\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive')]\n\n\n\ndf = few_shot_responses(promptJ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\nChanging the order of examples to neutral, negative, positive increases the overall accuracy to almost 75%!\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.7487837240159222\n\n\nThe model improves on all three sentiments compared to Prompt J.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_K.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-l",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-l",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt L",
    "text": "Prompt L\nI’ll re-order the examples and use the same Prompt J.\n\nexclude_idxs = [1, 0, 292]\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[(\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative')]\n\n\n\ndf = few_shot_responses(promptJ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\nThis ordering of examples drops the accuracy to 68%.\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.6811145510835913\n\n\nCompared to the best-performing Prompt K, this prompt yields a better accuracy for positive sentences (491 > 449).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_L.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-m",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-m",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt M",
    "text": "Prompt M\nI’ll re-order the examples and use the same Prompt J.\n\nexclude_idxs = [1, 292, 0]\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[(\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative'),\n ('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral')]\n\n\n\ndf = few_shot_responses(promptJ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\nThis ordering of examples worsens the accuracy, dropping it down to 57%.\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.5696594427244582\n\n\nThis prompt yields better results for positive sentiment (514 > 449) than the best overall performing Prompt J.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_M.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-n",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-n",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt N",
    "text": "Prompt N\nTrying the next ordering of sentiments:\n\nexclude_idxs = [292, 0, 1]\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative'),\n ('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive')]\n\n\n\ndf = few_shot_responses(promptJ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\nThis ordering results in the second-highest overall accuracy at 74%.\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.7394957983193278\n\n\nThis prompt performs slightly worse for all three sentiments than the so far best-overall performing Prompt K.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_N.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-o",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-o",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt O",
    "text": "Prompt O\nHere’s the final 3-sentiment ordering:\n\nexclude_idxs = [292, 1, 0]\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral')]\n\n\n\ndf = few_shot_responses(promptJ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\nThis ordering of sentiment does not beat my so far best-performing accuracy.\n\ndf['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\ndf['lm_match'] = df['label_text'] == df['responses']\nacc = df.lm_match.mean()\nacc\n\n0.599734630694383\n\n\nThis prompt yields a much better performance on positive sentiment than my best performing Prompt K (525 > 449).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_O.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-p",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-p",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt P",
    "text": "Prompt P\nNext, I’ll increase the number of examples to 6. Note that I won’t be trying all permutations but a few random ones.\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(6)]\npromptP_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptP_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2258\n})\n\n\nThe random examples I have picked don’t include a negative sentence. I’m curious to see how the model performs on this.\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\n[el[1] for el in examples]\n\n['positive', 'neutral', 'positive', 'neutral', 'positive', 'positive']\n\n\n\ndf = few_shot_responses(promptP_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt results in a worse performance in overall accuracy.\n\nget_acc(df)\n\n0.6390611160318866\n\n\nEven though no negative examples were given, this prompt yields considerably more correct negative sentences (289) than the best-performing Prompt K (201).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_P.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-q",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-q",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt Q",
    "text": "Prompt Q\nI’ll try another random set of 6 examples, this time making sure there’s at least one of each sentiment.\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(6)]\npromptQ_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \npromptQ_ds, [el[1] for el in examples]\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2258\n }),\n ['positive', 'positive', 'positive', 'neutral', 'negative', 'neutral'])\n\n\n\ndf = few_shot_responses(promptQ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative', 'live'], dtype=object)\n\n\nThis set of 6 examples does not improve upon the best-overall accuracy of 75%.\n\nget_acc(df)\n\n0.6572187776793623\n\n\nSomething we haven’t seen in awhile, an other response.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_Q.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-r",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-r",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt R",
    "text": "Prompt R\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(6)]\npromptR_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \npromptR_ds, [el[1] for el in examples]\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2258\n }),\n ['neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral'])\n\n\n\ndf = few_shot_responses(promptR_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nNo improvements on accuracy with this prompt.\n\nget_acc(df)\n\n0.6483613817537643\n\n\nCompared to the best-performing Prompt K, this prompt yields considerably more correct negative (285 > 201) and positive (493 > 449) sentences but underperforms on neutral sentences (686 < 1043).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_R.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-s",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-s",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt S",
    "text": "Prompt S\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(6)]\npromptS_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \npromptS_ds, [el[1] for el in examples]\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2258\n }),\n ['neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral'])\n\n\nThis set of examples has no negative sentences and a majority of neutral sentences.\n\ndf = few_shot_responses(promptS_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n\ndf['responses'].unique()\n\narray(['negative', 'positive', 'neutral'], dtype=object)\n\n\nThis set of examples does not improve on the best-overall accuracy of 75% (Prompt K).\n\nget_acc(df)\n\n0.6297608503100088\n\n\nIt does, however, have a considerably larger number of correctly labeled negative sentences (292 > 201).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_S.csv', index=False)\n\nI’ll try one more 6-shot prompt before I increase the number of examples."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-t",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-t",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt T",
    "text": "Prompt T\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(6)]\npromptT_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \npromptT_ds, [el[1] for el in examples]\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2258\n }),\n ['neutral', 'neutral', 'positive', 'neutral', 'negative', 'neutral'])\n\n\n\ndf = few_shot_responses(promptT_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nSimilar to the other 6-Shot examples, this set of examples does not improve on the best overall accuracy.\n\nget_acc(df)\n\n0.6886625332152347\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_T.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-u",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-u",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt U",
    "text": "Prompt U\nI’ll now increase the number of examples in the prompt to 12, and try out 5 random sets of 12 examples.\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(12)]\npromptU_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \npromptU_ds, [el[1] for el in examples]\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2252\n }),\n ['positive',\n  'negative',\n  'positive',\n  'neutral',\n  'positive',\n  'positive',\n  'neutral',\n  'positive',\n  'neutral',\n  'neutral',\n  'positive',\n  'neutral'])\n\n\n\ndf = few_shot_responses(promptU_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nIncreasing the number of examples to 12, at least the 12 I chose here, doesn’t improve on the best overall accuracy.\n\nget_acc(df)\n\n0.6549733570159858\n\n\nThe number of correct positive sentences is considerably higher than Prompt K (496 > 449).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_U.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-v",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-v",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt V",
    "text": "Prompt V\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(12)]\npromptV_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \npromptV_ds, [el[1] for el in examples]\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2252\n }),\n ['neutral',\n  'positive',\n  'positive',\n  'neutral',\n  'neutral',\n  'negative',\n  'negative',\n  'neutral',\n  'positive',\n  'neutral',\n  'positive',\n  'negative'])\n\n\n\ndf = few_shot_responses(promptV_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\nThis prompt performs well, and competes with but doesn’t improve upon the best overall accuracy of 75%.\n\nget_acc(df)\n\n0.7322380106571936\n\n\nThis prompt performs considerably better on neutral sentences than Prompt K (1103 > 1043).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_V.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-w",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-w",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt W",
    "text": "Prompt W\n\nexclude_idxs = [random.randint(0, 2263) for _ in range(12)]\npromptW_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \npromptW_ds, [el[1] for el in examples]\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2252\n }),\n ['neutral',\n  'negative',\n  'neutral',\n  'positive',\n  'neutral',\n  'neutral',\n  'positive',\n  'neutral',\n  'neutral',\n  'negative',\n  'positive',\n  'neutral'])\n\n\n\ndf = few_shot_responses(promptW_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe accuracy worsens with this set of 12 examples.\n\nget_acc(df)\n\n0.7042628774422736\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_W.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-x",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-x",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt X",
    "text": "Prompt X\n\ndef get_ds(n):\n    exclude_idxs = [random.randint(0, 2263) for _ in range(n)]\n    prompt_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\n    examples = []\n    for idx in exclude_idxs:\n        examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n        \n    print(prompt_ds, [el[1] for el in examples])\n    \n    return prompt_ds, examples\n\n\npromptX_ds, examples = get_ds(12)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2252\n}) ['positive', 'neutral', 'negative', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'negative']\n\n\n\ndf = few_shot_responses(promptX_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nAha! This prompt improves upon the best overall accuracy, reaching about 77%.\n\nget_acc(df)\n\n0.7659857904085258\n\n\nCompared to Prompt K (75%) this prompt performs worse on neutral sentences (1000 < 1043) but more than makes up for it on negative (270 > 201) and positive (455 > 449) sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_X.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-y",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-y",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt Y",
    "text": "Prompt Y\n\npromptY_ds, examples = get_ds(12)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2252\n}) ['neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptY_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['negative', 'positive', 'neutral'], dtype=object)\n\n\nThis prompt does not improve on the best overall accuracy.\n\nget_acc(df)\n\n0.7255772646536413\n\n\nThis prompt performs well on negative and neutral sentences but its worse performance on positive sentences brings down the overall accuracy.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_Y.csv', index=False)\n\nNext, I’ll try 5 prompts with 18 examples."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-z",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-z",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt Z",
    "text": "Prompt Z\n\npromptZ_ds, examples = get_ds(18)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2246\n}) ['neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'negative', 'negative', 'positive', 'neutral']\n\n\n\ndf = few_shot_responses(promptZ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt does not improve upon overall accuracy.\n\nget_acc(df)\n\n0.7132680320569902\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_Z.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aa",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aa",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AA",
    "text": "Prompt AA\n\npromptAA_ds, examples = get_ds(18)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2247\n}) ['neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'neutral']\n\n\n\ndf = few_shot_responses(promptAA_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis set of 18 examples increases the best overall accuraacy to almost 80%!\n\nget_acc(df)\n\n0.7948375611927013\n\n\nCompared to Prompt X, this prompt performs worse on negative (206 < 270) and positive (400 < 455) but more than makes up for it on neutral sentences (1180 > 1000).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AA.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ab",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ab",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AB",
    "text": "Prompt AB\n\npromptAB_ds, examples = get_ds(18)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2246\n}) ['neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'positive', 'positive']\n\n\n\ndf = few_shot_responses(promptAB_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt does not improve upon the best overall accuracy.\n\nget_acc(df)\n\n0.7422083704363313\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AB.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ac",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ac",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AC",
    "text": "Prompt AC\n\npromptAC_ds, examples = get_ds(18)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2246\n}) ['neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral']\n\n\n\ndf = few_shot_responses(promptAC_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt does not improve upon the best overall accuracy.\n\nget_acc(df)\n\n0.6856634016028496\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AC.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ad",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ad",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AD",
    "text": "Prompt AD\n\npromptAD_ds, examples = get_ds(18)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2246\n}) ['neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'positive']\n\n\n\ndf = few_shot_responses(promptAD_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt does not improve upon the best overall accuracy.\n\nget_acc(df)\n\n0.7497773820124666\n\n\nThis prompt yields considerably more correct negative sentences (271 > 206) than the best-performing Prompt AA.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AD.csv', index=False)\n\nNext, I’ll try 5 prompts with 24 examples each."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ae",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ae",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AE",
    "text": "Prompt AE\n\npromptAE_ds, examples = get_ds(24)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2240\n}) ['neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'negative']\n\n\n\ndf = few_shot_responses(promptAE_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nIncreasing the number of examples to 24 (at least for these 24 examples) does not improve upon the overall accuracy.\n\nget_acc(df)\n\n0.7491071428571429\n\n\nCompared to the best performing Prompt AA, this prompt yields considerably more correct neutral sentences (1267 > 1180).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AE.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-af",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-af",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AF",
    "text": "Prompt AF\n\npromptAF_ds, examples = get_ds(24)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2240\n}) ['positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAF_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt doesn’t improve upon the best overall accuracy, and performs better than Prompt AA on neutral sentences.\n\nget_acc(df)\n\n0.7308035714285714\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AF.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ag",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ag",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AG",
    "text": "Prompt AG\n\npromptAG_ds, examples = get_ds(24)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2240\n}) ['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAG_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe same trend continues for this set of 24 examples.\n\nget_acc(df)\n\n0.75\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AG.csv', index=False)\n\nTwo more 24-Shot prompts to go."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ah",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ah",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AH",
    "text": "Prompt AH\n\npromptAH_ds, examples = get_ds(24)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2240\n}) ['positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAH_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt does not improve upon the best overall accuracy (though it comes close).\n\nget_acc(df)\n\n0.7745535714285714\n\n\nThis prompt yields more correct negative sentences than Prompt AA (233 > 206).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AH.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ai",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ai",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AI",
    "text": "Prompt AI\n\npromptAI_ds, examples = get_ds(24)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2241\n}) ['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral']\n\n\n\ndf = few_shot_responses(promptAI_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt does not improve upon the best overall accuracy.\n\nget_acc(df)\n\n0.7536813922356091\n\n\nThis prompt yields considerably more correct neutral sentences than the best performing Prompt AA (1266 > 1180).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AI.csv', index=False)\n\nNext, I’ll try 5 different 30-Shot prompts."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aj",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aj",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AJ",
    "text": "Prompt AJ\n\npromptAJ_ds, examples = get_ds(30)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n}) ['neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAJ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt doesn’t improve the best overall accuracy.\n\nget_acc(df)\n\n0.7739480752014324\n\n\nAs seems to be the trend, this prompt results in more correct neutral responses (1284) than Prompt AA (1180).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AJ.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ak",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ak",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AK",
    "text": "Prompt AK\n\npromptAK_ds, examples = get_ds(30)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n}) ['neutral', 'neutral', 'neutral', 'negative', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'positive']\n\n\n\ndf = few_shot_responses(promptAK_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe model performs considerably worse with these 30 examples.\n\nget_acc(df)\n\n0.6777081468218442\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AK.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-al",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-al",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AL",
    "text": "Prompt AL\n\npromptAL_ds, examples = get_ds(30)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n}) ['positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAL_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe trend continues: the overall accuracy doesn’t improve but the model’s performance on neutral sentences does.\n\nget_acc(df)\n\n0.76544315129812\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AL.csv', index=False)\n\nTwo more 30-Shot prompts to go."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-am",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-am",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AM",
    "text": "Prompt AM\n\npromptAM_ds, examples = get_ds(30)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n}) ['negative', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'neutral']\n\n\n\ndf = few_shot_responses(promptAM_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe overall accuracy doesn’t improve but the model’s performance on negative and positive sentences does.\n\nget_acc(df)\n\n0.7484333034914951\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AM.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-an",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-an",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AN",
    "text": "Prompt AN\n\npromptAN_ds, examples = get_ds(30)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n}) ['positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAN_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe overall accuracy doesn’t improve but the model’s performance on positive sentences does.\n\nget_acc(df)\n\n0.7381378692927484\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AN.csv', index=False)\n\nNext, I’ll increase the number of examples to 45."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ao",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ao",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AO",
    "text": "Prompt AO\n\npromptAO_ds, examples = get_ds(45)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2219\n}) ['neutral', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral']\n\n\n\ndf = few_shot_responses(promptAO_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe overall accuracy doesn’t improve but the model’s performance on positive sentences does.\n\nget_acc(df)\n\n0.7417755745831456\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AO.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ap",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ap",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AP",
    "text": "Prompt AP\n\npromptAP_ds, examples = get_ds(45)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2219\n}) ['neutral', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive']\n\n\n\ndf = few_shot_responses(promptAP_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe overall accuracy doesn’t improve but the model’s performance on neutral sentences does.\n\nget_acc(df)\n\n0.7872915727805317\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AP.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aq",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aq",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AQ",
    "text": "Prompt AQ\n\npromptAQ_ds, examples = get_ds(45)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2219\n}) ['neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAQ_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThe overall accuracy doesn’t improve but the model’s performance on neutral sentences does.\n\nget_acc(df)\n\n0.7201442091031997\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AQ.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ar",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ar",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AR",
    "text": "Prompt AR\n\npromptAR_ds, examples = get_ds(45)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2219\n}) ['neutral', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive']\n\n\n\ndf = few_shot_responses(promptAR_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt performs worse than the best overall Prompt AA.\n\nget_acc(df)\n\n0.7386210004506535\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AR.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-as",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-as",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AS",
    "text": "Prompt AS\n\npromptAS_ds, examples = get_ds(45)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2219\n}) ['neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAS_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n\ndf['responses'].unique()\n\narray(['positive', 'neutral', 'negative'], dtype=object)\n\n\nCompared to Prompt AA, this prompt yields a worse overall accuracy but improves on neutral sentences (1291 > 1180).\n\nget_acc(df)\n\n0.7494366831906264\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AS.csv', index=False)\n\nNext, I’ll move on to the final number of examples: 60."
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-at",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-at",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AT",
    "text": "Prompt AT\n\npromptAT_ds, examples = get_ds(60)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2204\n}) ['positive', 'neutral', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'neutral', 'negative', 'neutral']\n\n\n\ndf = few_shot_responses(promptAT_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nUpping the number of examples to 60 does not improve results.\n\nget_acc(df)\n\n0.7218693284936479\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AT.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-au",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-au",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AU",
    "text": "Prompt AU\n\npromptAU_ds, examples = get_ds(60)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2204\n}) ['neutral', 'positive', 'neutral', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'neutral']\n\n\n\ndf = few_shot_responses(promptAU_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nCompared to Prompt AA, this prompt yields a worse overall accuracy but improves on neutral sentences (1237 > 1180).\n\nget_acc(df)\n\n0.7686025408348457\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AU.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-av",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-av",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AV",
    "text": "Prompt AV\n\npromptAV_ds, examples = get_ds(60)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2204\n}) ['positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'negative', 'neutral']\n\n\n\ndf = few_shot_responses(promptAV_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nCompared to Prompt AA, this prompt yields a worse overall accuracy but improves on neutral sentences (1206 > 1180).\n\nget_acc(df)\n\n0.7545372050816697\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AV.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aw",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-aw",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AW",
    "text": "Prompt AW\n\npromptAW_ds, examples = get_ds(60)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2204\n}) ['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral']\n\n\n\ndf = few_shot_responses(promptAW_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nThis prompt does not improve upon Prompt AA results.\n\nget_acc(df)\n\n0.7445553539019963\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AW.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ax",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#prompt-ax",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Prompt AX",
    "text": "Prompt AX\n\npromptAX_ds, examples = get_ds(60)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2204\n}) ['neutral', 'neutral', 'negative', 'negative', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'negative']\n\n\n\ndf = few_shot_responses(promptAX_ds, promptJ, examples)\n\n\n\n\nLabel the following TEXT with a single word: positive, negative, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\n\ndf['responses'].unique()\n\narray(['neutral', 'positive', 'negative'], dtype=object)\n\n\nAha! We finally improve on the overall accuracy of Prompt AA. This prompt yields a slightly higher accuracy that still rounds off to 80%.\n\nget_acc(df)\n\n0.7962794918330308\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-0.5B-Instruct_AX.csv', index=False)"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#running-inference-10-times-using-the-best-prompt",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#running-inference-10-times-using-the-best-prompt",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Running Inference 10 Times Using the Best Prompt",
    "text": "Running Inference 10 Times Using the Best Prompt\nWhile 60-shot Prompt AX had a slightly higher accuracy (79.63%) I am going to pick the 16-Shot Prompt AA as my best prompt (79.48%) since it has less than a third of the examples, which translates to about a third of the tokens, thus leading to quicker response generation.\n\ndef test_gen(examples):\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": promptJ.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    messages = few_shot_examples + [{\"role\": \"user\", \"content\": promptJ.format(text=dataset[0]['sentence'])}]\n        \n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=2\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n    return response\n\n\npromptAA_ds, examples = get_ds(18)\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2246\n}) ['negative', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive']\n\n\n1 response generation takes 72ms. Running full dataset inference 10 times will take about 30 minutes.\n\n%timeit -n 10 test_gen(examples)\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\n72 ms ± 42.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nShow updated few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    for row in dataset:\n        messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n        generated_ids = model.generate(\n            model_inputs.input_ids,\n            max_new_tokens=2\n        )\n        generated_ids = [\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n        ]\n\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n        responses.append(response)\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    \n    return df, acc\n\n\n\n\nShow updated get_ds function\ndef get_ds(n):\n    exclude_idxs = [random.randint(0, 2263) for _ in range(n)]\n    prompt_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\n\n    examples = []\n    for idx in exclude_idxs:\n        examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n    \n    return prompt_ds, examples\n\n\nI didn’t store the exact 18 examples that I used the first time for Prompt AA, so I had to try different 18-Shot examples until I achieved an accuracy close to 79.48%. It took me about 20 tries, but I finally found a set of examples that broke the 79% threshold.\n\nfor _ in range(20):\n    n = 18\n    ds, examples = get_ds(n)\n    if len(ds) != 2264 - n: pass\n    df, acc = few_shot_responses(ds, promptJ, examples)\n    if round(acc, 2) >= 0.79: break\n\n\nacc\n\n0.815227070347284\n\n\n\nds, len(examples)\n\n(Dataset({\n     features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n     num_rows: 2246\n }),\n 18)\n\n\n\naccs = []\nfor _ in range(10):\n    df, acc = few_shot_responses(ds, promptJ, examples)\n    accs.append(acc)\n\nFor this prompt, the overall accuracy ranges from 80.8% to 82.4%.\n\npd.Series(accs).describe()\n\ncount    10.000000\nmean      0.816830\nstd       0.005397\nmin       0.807658\n25%       0.814003\n50%       0.817453\n75%       0.819791\nmax       0.824577\ndtype: float64"
  },
  {
    "objectID": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#final-thoughts",
    "href": "posts/2024-11-18-tinysentiment-Qwen2-0.5B-SC/index.html#final-thoughts",
    "title": "Sentiment Classification with Qwen2-0.5B-Instruct",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nTakeaways from my Qwen2-0.5B experiments:\n\nExample order matters: Testing 6 prompts with the same 3 examples in different orders yielded accuracies from 57% to 75%.\n\nExample selection matters: Recreating the performance of one of my best prompts (79.48% accuracy) took ~20 attempts, proving not all sets of examples perform equally.\n\nResult variance exists: Running a prompt 10 times produced accuracies ranging from 80.8% to 82.4%.\n\nHere are the results of Qwen2-0.5B in the context of the other models that I have experimented with:\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nphi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nphi-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\nQwen2-1.5B\n27-Shot\n86.10%\n90% (264/294)\n96% (1320/1382)\n61% (342/561)\n\n\n**Qwen2-0.5B\n17-Shot\n79.48%\n69% (206/300)\n86% (1180/1380)\n71% (400/567)\n\n\n\nHere are the results from this notebook:\n\n\n\nPrompt\nStrategy\nAccuracy\nNegative\nNeutral\nPositive\n\n\n\n\nA\n0-Shot\n62.41%\n91% (276/303)\n53% (735/1391)\n71% (402/570)\n\n\nB\n0-Shot\n47.84%\n90% (274/303)\n57% (789/1391)\n4% (20/570)\n\n\nC\n0-Shot\n40.46%\n91% (276/303)\n43% (594/1391)\n8% (46/570)\n\n\nD\n0-Shot\n68.29%\n79% (240/303)\n61% (851/1391)\n80% (455/570)\n\n\nE\n0-Shot\n51.19%\n97% (293/303)\n28% (396/1391)\n82% (470/570)\n\n\nF\n0-Shot\n48.19%\n94% (286/303)\n21% (287/1391)\n91% (518/570)\n\n\nG\n0-Shot\n61.09%\n93% (282/303)\n46% (646/1391)\n80% (455/570)\n\n\nH\n0-Shot\n65.42%\n85% (257/303)\n57% (798/1391)\n75% (426/570)\n\n\nI\n0-Shot\n66.12%\n81% (245/303)\n58% (800/1391)\n79% (452/570)\n\n\nJ\n3-Shot\n70.94%\n43% (131/302)\n75% (1042/1390)\n76% (431/569)\n\n\nK\n3-Shot\n74.88%\n67% (201/302)\n75% (1043/1390)\n79% (449/569)\n\n\nL\n3-Shot\n68.11%\n49% (149/302)\n65% (900/1390)\n86% (491/569)\n\n\nM\n3-Shot\n56.97%\n49% (149/302)\n45% (625/1390)\n90% (514/569)\n\n\nN\n3-Shot\n73.95%\n62% (188/302)\n75% (1038/1390)\n78% (446/569)\n\n\nO\n3-Shot\n59.97%\n65% (196/302)\n46% (635/1390)\n92% (525/569)\n\n\nP\n6-Shot\n63.91%\n95% (289/303)\n49% (678/1389)\n84% (476/566)\n\n\nQ\n6-Shot\n65.72%\n69% (207/302)\n55% (765/1389)\n90% (512/567)\n\n\nR\n6-Shot\n64.84%\n94% (285/303)\n49% (686/1387)\n87% (493/568)\n\n\nS\n6-Shot\n62.98%\n96% (292/303)\n47% (656/1387)\n83% (474/568)\n\n\nT\n6-Shot\n68.87%\n51% (155/302)\n70% (966/1387)\n76% (434/569)\n\n\nU\n12-Shot\n65.50%\n53% (159/302)\n59% (820/1386)\n88% (496/564)\n\n\nV\n12-Shot\n73.22%\n70% (209/300)\n80% (1103/1386)\n60% (337/566)\n\n\nW\n12-Shot\n70.43%\n82% (246/301)\n66% (912/1384)\n75% (428/567)\n\n\nX\n12-Shot\n76.60%\n91% (270/298)\n72% (1000/1386)\n80% (455/568)\n\n\nY\n12-Shot\n72.56%\n80% (243/303)\n77% (1069/1381)\n57% (322/568)\n\n\nZ\n18-Shot\n71.33%\n50% (150/301)\n75% (1036/1382)\n74% (416/563)\n\n\nAA\n17-Shot\n79.48%\n69% (206/300)\n86% (1180/1380)\n71% (400/567)\n\n\nAB\n18-Shot\n74.22%\n77% (229/299)\n76% (1054/1381)\n68% (384/566)\n\n\nAC\n18-Shot\n68.57%\n49% (148/302)\n73% (1013/1380)\n67% (379/564)\n\n\nAD\n18-Shot\n74.98%\n89% (271/303)\n76% (1052/1379)\n64% (361/564)\n\n\nAE\n24-Shot\n74.91%\n61% (181/299)\n92% (1267/1375)\n41% (230/566)\n\n\nAF\n24-Shot\n73.08%\n37% (112/302)\n91% (1246/1375)\n50% (279/563)\n\n\nAG\n24-Shot\n75.00%\n58% (173/300)\n92% (1265/1375)\n43% (242/565)\n\n\nAH\n24-Shot\n77.46%\n78% (233/299)\n84% (1153/1375)\n62% (349/566)\n\n\nAI\n23-Shot\n75.37%\n48% (143/301)\n92% (1266/1375)\n50% (280/565)\n\n\nAJ\n30-Shot\n77.39%\n58% (172/298)\n94% (1284/1370)\n48% (273/566)\n\n\nAK\n30-Shot\n67.78%\n63% (187/299)\n61% (844/1375)\n86% (483/560)\n\n\nAL\n30-Shot\n76.54%\n58% (173/299)\n86% (1185/1372)\n63% (352/563)\n\n\nAM\n30-Shot\n74.84%\n82% (242/296)\n72% (984/1376)\n79% (446/562)\n\n\nAN\n30-Shot\n73.81%\n51% (154/300)\n77% (1052/1372)\n79% (443/562)\n\n\nAO\n45-Shot\n74.18%\n54% (159/297)\n76% (1034/1366)\n81% (453/556)\n\n\nAP\n45-Shot\n78.73%\n63% (186/296)\n87% (1192/1365)\n66% (369/558)\n\n\nAQ\n45-Shot\n72.01%\n17% (51/301)\n89% (1210/1359)\n60% (337/559)\n\n\nAR\n45-Shot\n73.86%\n53% (157/297)\n80% (1094/1364)\n70% (388/558)\n\n\nAS\n45-Shot\n74.94%\n42% (125/297)\n89% (1219/1363)\n57% (319/559)\n\n\nAT\n60-Shot\n72.19%\n47% (138/292)\n78% (1055/1356)\n72% (398/556)\n\n\nAU\n60-Shot\n76.86%\n43% (127/296)\n91% (1237/1356)\n60% (330/552)\n\n\nAV\n60-Shot\n75.45%\n26% (79/299)\n89% (1206/1352)\n68% (378/553)\n\n\nAW\n60-Shot\n74.46%\n29% (88/299)\n86% (1157/1349)\n71% (396/556)\n\n\nAX\n60-Shot\n79.63%\n62% (179/290)\n94% (1275/1352)\n54% (301/562)"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "title": "Transcribing Sherlock into Spanish",
    "section": "",
    "text": "Sherlock Holmes kneeling next to Toby the bloodhound and pointing, likely towards where he thinks Toby should go next\nTranscription Progress (00:06:07 out of 17:40:32 transcribed)"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Background",
    "text": "Background\nI have watched all four seasons of BBC’s Sherlock probably 5 times. I learn something new about it each time.\nI have tried to learn Spanish using Duolingo, stopping and re-starting every year or so, without much success.\nI don’t really recall how the thought came about but I decided to combine my love of the show with my desire to learn Spanish into one project—this one!"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Setup",
    "text": "Setup\nUsing the embedded Google Translate UI and my partner’s translator-level knowledge of the language, I am transcribing every word of the show into Spanish.\n\n\n\nA screenshot of my translation setup: Google Translate embedded underneath the search bar—the result of googling “Google Translate”. I’ve typed “Sherlock Holmes” in the “English” textbox on the left and it has translated to “Sherlock Holmes” in the Spanish output on the right.\n\n\nIn a second tab, I have the show open (with subtitles on).\n\n\n\nA screenshot of Sherlock playing in the Amazon Prime Video player\n\n\nI transcribe in a .txt file titled transcript.txt, documenting the following fields:\n\nseason number\nepisode number\ntimestamp (hours::minutes:seconds)\nwho is the speaker?\nthe english transcription of what they say\nthe spanish translation of that\nnotes which usually documents specific word translations\n\nAs an example, the first bit of dialogue in the series is John Watson’s therapist Ella asking him “How’s your blog going?” which translates to “Cómo va tu blog?” Where va = goes.\nHow goes your blog? I would say quite well heheheh.\nseason,episode,time,speaker,english,spanish,notes\n1,1,00:01:30,ella,how's your blog going?,cómo va tu blog?, va = goes"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "title": "Transcribing Sherlock into Spanish",
    "section": "What I’m Learning",
    "text": "What I’m Learning\nI’ll write in this blog post some examples of the translations and how I’m thinking through the process, as well as what I’m learning from discussions with my partner.\nThree main themes I’m seeing so far about translating from English to Spanish:\n\nwhich words to use depends a lot on context.\nwords that sound the same but mean different things will sometimes have different emphasis.\na word that is technically correct may not be used frequently in conversation.\n\nI’m not quite sure how to best document what I’m learning so I’ll just start writing.\n\nElla: “You haven’t written a word, have you?”\nSomething I enjoy doing is translating the Spanish back into English without changing word positions. The benefit of this exercise of translating and translating back is that it reveals (or focuses my attention on) nuances I wouldn’t otherwise be aware of.\nEnglish: You haven’t written a word, have you?\nSpanish: No has escrito una palabra verdad?\nBack to English: Not you have written a word true?\nI asked my partner how she would translate it and she said: No has escrito ni una palabra, verdad?\nWhich translates to: You haven’t written not even a word, true?\nIt bothers me that I don’t know why in English the question ends in have you? but in Spanish it ends with true?. Of course this may just be how Spanish works or how conversational Spanish works.\nI asked my partner how you would say just have you? in Spanish and it’s lo has?\nGoogle Translate aligns with this when it translates from Spanish to English:\nSpanish: No has escrito ni una palabra lo has?\nEnglish: You haven’t written a word, have you?\nBut recommends ending with verdad? when I translate from English to Spanish.\n\n\nSpeaker: “You can share mine”\nHere are the Google Translate forward and backward translations:\n\nEnglish: You can share mine.\nSpanish: Puedes compartir el mio.\n\nSpanish: Puedes compartir el mio.\nEnglish: Can you share mine.\n\nHowever, if I start the Spanish translation with tu the English translation matches my original prompt:\n\nSpanish: Tu puedes compartir el mio.\nEnglish: You can share mine.\n\nI think this is a good example of how what is technically correct may or may not be what’s used in conversation—saying tu may not be strictly required for conversation and may be implicitly understood because of the form used—puedes (you can).\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npuedes\nyou can\n\n\npuedo\nI can\n\n\npuedemos\nwe can\n\n\npueden\nthey can\n\n\n\n\n\nLestrade: “Well, they all took the same poison.”\nSomething else I’ve enjoyed and learned from is watching how a translation changes as you type the full sentence in Google Translate.\nFor example when translating from Spanish (pues, todos tomaron el mismo veneno) to English (well, they all took the same poison):\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npues\nwell\n\n\npues, todos\nwell, everyone\n\n\npues, todos tomaron\nwell, they all took\n\n\npues, todos tomaron el\nwell, everyone took\n\n\npues, todos tomaron el mismo\nwell, they all took the same\n\n\npues, todos tomaron el mismo veneno\nwell, they all took the same poison\n\n\n\nWhat I’m observing might have less to do with how Spanish works and more to do with how Google Translate works. Although some words seem interchangeable (todos seems to mean everyone or they all)."
  },
  {
    "objectID": "posts/2024-09-05-fastbookRAG-bm25-error-analysis/index.html",
    "href": "posts/2024-09-05-fastbookRAG-bm25-error-analysis/index.html",
    "title": "Conducting a Question-by-Question Error Analysis on Full Text Search Results",
    "section": "",
    "text": "In this notebook I’ll do a deep dive error analysis of my full text search results, where I implemented 6 different keyword-based full text searches to retrieve context sufficient to answer questions from the end-of-chapter Questionnaires in fastbook. Here is the summary of results from those experiments:\n\n\n\n\n\n\n\n\n\n\n\n\nChapter\nBM25_A (Top-1 1p)\nBM25_B (Top-3 1p)\nBM25_C (Top-5 1p)\nBM25_D (Top-1 3p)\nBM25_E (Top-3 3p)\nBM25_F (Top-5 3p)\n\n\n\n\n1\n40% (12/30)\n56.7% (17/30)\n60% (18/30)\n63.3% (19/30)\n83.3% (25/30)\n90% (27/30)\n\n\n2\n38.5% (10/26)\n65.4% (17/26)\n69.2% (18/26)\n46.2% (12.26)\n80.8% (21/26)\n80.8% (21/26)\n\n\n4\n25% (8/32)\n68.8% (22/32)\n71.9% (23/32)\n31.3% (10/32)\n71.9% (23/32)\n75% (24/32)\n\n\n8\n13.8% (4/29)\n34.5% (10/29)\n44.8% (13/29)\n31% (9/29)\n55.2% (16/29)\n65.5% (19/29)\n\n\n9\n13.8% (4/29)\n48.3% (14/29)\n58.6% (17/29)\n34.5% (10/29)\n72.4% (21/29)\n79.3% (23/29)\n\n\n10\n47.6% (12/21)\n42.9% (9/21)\n61.9% (13/21)\n38% (8/21)\n57.1% (12/21)\n61.9% (13/21)\n\n\n13\n37.1% (13/35)\n54.3% (19/35)\n60% (21/35)\n42.9% (15/35)\n68.6% (24/35)\n80% (28/35)\n\n\nAll\n30.2% (61/202)\n53.5% (108/202)\n60.9% (123/202)\n41.1% (83/202)\n70.3% (142/202)\n76.7% (155/202)\n\n\n\nThe granular results are available in this public gist.\nAs a reminder, the two metrics I use for evaluation are Score and Answer Rate\nThe evaluation metric for each question, that I’m simply calling score, is binary: can the retrieved context answer the question (1) or not (0)? The evaluation metric across a set of questions, which I’m calling the Answer Rate, is the mean score for those questions.\nWhile this is a straightforward pair of metrics, they do involve some judgment. After reading the retrieved context, I decide if it’s enough to answer the question.\nThis notebook is a part of series of blog posts for a project I’m calling fastbookRAG where I’m trying to answer questions from the fastbook end-of-chapter Questionnaires using the following pipeline:\n\n\n\nfastbookRAG diagram\n\n\nHere are the number of unanswered questions per chapter:\n\n\n\nChapter\n# of Questions\n\n\n\n\n1\n2\n\n\n2\n4\n\n\n4\n7\n\n\n8\n10\n\n\n9\n5\n\n\n10\n5\n\n\n13\n6\n\n\nTotal\n39"
  },
  {
    "objectID": "posts/2024-09-05-fastbookRAG-bm25-error-analysis/index.html#error-analysis",
    "href": "posts/2024-09-05-fastbookRAG-bm25-error-analysis/index.html#error-analysis",
    "title": "Conducting a Question-by-Question Error Analysis on Full Text Search Results",
    "section": "Error Analysis",
    "text": "Error Analysis\nFor 39 questions, none of the 6 full-text search methods retrieved enough context to provide an answer. I’ll be looking at each of those 39 questions, their “gold standard” answer (obtained from the fastai Forums Questionnaire wikis), and the relevant context from the fastbook chapter.\nI have three objectives for this error analysis:\n\nUnderstand what kinds of questions are difficult to answer using full text search.\nIdentify ambigious questions that need to be removed from the evaluation set (e.g. “How does it solve it?”).\nIdentify unanswerable questions (i.e. “to be done by the reader” type questions) that need to be removed.\n\nThe underlying goal of this analysis: look at your data!\nFor each of the 39 questions, I will write four sections:\n\nRelevant Context: the paragraph(s) from the fastbook text that are sufficient to answer the question.\nAnalysis: my interpretation/explanation for why a keyword-based search did not retrieve the context.\nConclusion: what I think is needed to retrieve the sufficient context (and if I think this question should be removed)\nTags: keywords (no pun intented) that describe the type of error.\n\n\nimport pandas as pd\nurl = 'https://gist.githubusercontent.com/vishalbakshi/4379c92665695b8bd9ab83a1f3ab6b55/raw/97a22f0736b5e179efb8c9d70e5ec80a5b8f4817/fastbookRAG_bm25_all.csv'\ndf = pd.read_csv(url)\nscore_columns = df.filter(regex='_score$').columns\ndf['total_score'] = df[score_columns].sum(axis=1)\nno_answer = df.query(\"total_score == 0\")\nno_answer.shape\n\n(39, 19)\n\n\n\ndef print_data(idx):\n  row = no_answer.iloc[idx]\n  print('Chapter, Question Number:',row['chapter'], row['question_number'])\n  print('Question Text:', row['question_text'])\n  print('Answer:', row['answer'])\n  print('Keywords:', row['keywords'])\n\n\nChapter 1 (2 questions)\n\nQuestion 5\n\nprint_data(0)\n\nChapter, Question Number: 1 5\nQuestion Text: \"\"What were the two theoretical misunderstandings that held back the field of neural networks?\"\"\nAnswer: \"\"In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, \"Perceptrons\", that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\nIn the 1980's, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential.\"\"\nKeywords: \"neural, networks, theoretical, misunderstandings, field\"\n\n\nRelevant Context:\n\nAn MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called Perceptrons (MIT Press), about Rosenblatt’s invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.\n\n\nIn the 1980’s most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their “pattern of connectivity among units,” to use the framework above). And indeed, neural networks were widely used during the ’80s and ’90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful.\n\nAnalysis:\nI think the reason none of the methods retrieved both contexts (they generally only retrieved the second paragraph) is due to the choice of keywords. The second paragraph contains four of the five keywords (neural, networks, theoretical, field). The first paragraph does not contain any of the keywords.\nConclusion: Keep the question in the evaluation set. I expect semantic search to retrieve both paragraphs.\nTags: insufficient keywords, semantic search\n\n\nQuestion 16\n\nprint_data(1)\n\nChapter, Question Number: 1 16\nQuestion Text: \"\"What do you need in order to train a model?\"\"\nAnswer: \"\"You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer).\"\"\nKeywords: \"train, model, training, models, dataset, data\"\n\n\nRelevant Context:\n\nFrom this picture we can now see some fundamental things about training a deep learning model:\n\nA model cannot be created without data.\nA model can only learn to operate on the patterns seen in the input data used to train it.\nThis learning approach only creates predictions, not recommended actions.\nIt’s not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren’t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n\n\n\nMachine learning is a discipline where we define a program not by writing it entirely ourselves, but by learning from data. Deep learning is a specialty within machine learning that uses neural networks with multiple layers. Image classification is a representative example (also known as image recognition). We start with labeled data; that is, a set of images where we have assigned a label to each image indicating what it represents. Our goal is to produce a program, called a model, which, given a new image, will make an accurate prediction regarding what that new image represents.\nEvery model starts with a choice of architecture, a general template for how that kind of model works internally. The process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data. In order to define how well a model does on a single prediction, we need to define a loss function, which determines how we score a prediction as good or bad.\n\nAnalysis:\nThe keywords for this question are not sufficient (they are too general) to find the 3 relevant paragraphs. You can imagine that the words “train”, “model”, “training”, “models”, “dataset”, and “data” are plentiful in an introductory chapter about machine and deep learning.\nConclusion: Keep the question in the evaluation set. This is another question for which the sufficient context that needs to be retrieved is better suited for semantic search.\nTags: insufficient keywords, distracting keywords, semantic search\n\n\n\nChapter 2 (4 questions)\n\nQuestion 1\n\nprint_data(2)\n\nChapter, Question Number: 2 1\nQuestion Text: \"\"Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\"\"\nAnswer: \"\"Working with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\"\"\nKeywords: \"bear, classification, model, production, training, data, structural, style, differences\"\n\n\nRelevant Context:\n\nThis can result in disaster! For instance, let’s say we really were rolling out a bear detection system that will be attached to video cameras around campsites in national parks, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded there would be all kinds of problems in practice, such as:\n\nWorking with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\n\n\nAnalysis:\nOnly two of the keywords are in the relevant context (“model” and “data”).\nConclusion: Keep the question in the evaluation set. This is another question that might perform better using semantic search.\nTags: insufficient keywords, distracting keywords, semantic search\n\n\nQuestion 12\n\nprint_data(3)\n\nChapter, Question Number: 2 12\nQuestion Text: \"\"What does the `splitter` parameter to `DataBlock` do?\"\"\nAnswer: \"\"In fastai DataBlock, you provide the splitter argument a way for fastai to split up the dataset into subsets (usually train and validation set). For example, to randomly split the data, you can use fastai's predefined RandomSplitter class, providing it with the proportion of the data used for validation.\"\"\nKeywords: \"splitter, DataBlock, parameter, function, data\"\n\n\nRelevant Context:\n\nOften, datasets that you download will already have a validation set defined. Sometimes this is done by placing the images for the training and validation sets into different folders. Sometimes it is done by providing a CSV file in which each filename is listed along with which dataset it should be in. There are many ways that this can be done, and fastai provides a very general approach that allows you to use one of its predefined classes for this, or to write your own. In this case, however, we simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed (computers don’t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time—called the seed—then you will get the exact same list each time):\nsplitter=RandomSplitter(valid_pct=0.2, seed=42)\n\nAnalysis:\nThere are 4 occurences of the word “splitter” in Chapter 2 of the fastbook. Two of them, splitter and Splitter, occur in the relevant paragraph. I think what misled the full text search were the other keywords (DataBlock, parameter, function, data). I’m not sure if semantic search will perform better, since the explanatory text doesn’t explicitly say something like “the splitter parameter’s purpose is…” as it focuses more on the randomness of the split (which is more related to RandomSplitter than the splitter parameter itself).\nConclusion:\nThis may just be a question that is unanswerable. I’ll keep it in the evaluation set for now, but might remove it if the semantic search baselines aren’t able to retrieve the relevant context.\nTags: difficult question\n\n\nQuestion 24\n\nprint_data(4)\n\nChapter, Question Number: 2 24\nQuestion Text: \"\"What are three examples of problems that could occur when rolling out a bear warning system in practice?\"\"\nAnswer: \"\"The model we trained will likely perform poorly when:\nHandling night-time images\nDealing with low-resolution images (ex: some smartphone images)\nThe model returns prediction too slowly to be useful\"\"\nKeywords: \"bear, warning, system, problems, rollout, examples\"\n\n\nRelevant Context:\n\nThis can result in disaster! For instance, let’s say we really were rolling out a bear detection system that will be attached to video cameras around campsites in national parks, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded there would be all kinds of problems in practice, such as:\n\nWorking with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\n\n\nAnalysis: This question has the same relevant context as Chapter 2, Question 1. Only 1 of the keywords is in the context (“problems”) and that keyword appears 10 times in the chapter text.\nConclusion: Keep the question in the evaluation set. I would expect semantic search to perform better for this question.\nTags: insufficient keywords, distracting keywords, semantic search\n\n\nQuestion 27\n\nprint_data(5)\n\nChapter, Question Number: 2 27\nQuestion Text: \"\"What are the three steps in the deployment process?\"\"\nAnswer: \"\"Manual process - the model is run in parallel and not directly driving any actions, with humans still checking the model outputs.\nLimited scope deployment - The model's scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised.\nGradual expansion - The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better).\"\"\nKeywords: \"deployment, process, steps\"\n\n\nRelevant Context:\n\nWhere possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense. For instance, with our bear classifier a park ranger could have a screen displaying video feeds from all the cameras, with any possible bear sightings simply highlighted in red. The park ranger would still be expected to be just as alert as before the model was deployed; the model is simply helping to check for problems at this point.\nThe second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time-constrained trial of the model-driven approach. Rather than rolling our bear classifier out in every national park throughout the country, we could pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out.\nThen, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process. For instance, if the number of bear alerts doubles or halves after rollout of the new system in some location, we should be very concerned. Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and ensure that your regular reporting includes that information.\n\nAnalysis: Only one of the keywords, “process”, appears in the relevant context for this question, a word that appears 25 times in Chapter 2.\nConclusion: Keep the question in the evaluation set. Not enough of the keywords in the relevant context are in the keywords used for full text search.\nTags: insufficient keywords, distracting keywords\n\n\n\nChapter 4 (7 questions)\n\nQuestion 1\n\nprint_data(6)\n\nChapter, Question Number: 4 1\nQuestion Text: \"\"How is a grayscale image represented on a computer? How about a color image?\"\"\nAnswer: \"\"Images are represented by arrays with pixel values representing the content of the image. For grayscale images, a 2-dimensional array is used with the pixels representing the grayscale values, with a range of 256 integers. A value of 0 represents black, and a value of 255 represents white, with different shades of gray in between. For color images, three color channels (red, green, blue) are typically used, with a separate 256-range 2D array used for each channel. A pixel value of 0 represents black, with 255 representing solid red, green, or blue. The three 2D arrays form a final 3D array (rank 3 tensor) representing the color image.\"\"\nKeywords: \"grayscale, image, images, color, computer, representation\"\n\n\nRelevant Context:\n\nYou can see that the background white pixels are stored as the number 0, black is the number 255, and shades of gray are between the two. The entire image contains 28 pixels across and 28 pixels down, for a total of 784 pixels. (This is much smaller than an image that you would get from a phone camera, which has millions of pixels, but is a convenient size for our initial learning and experiments. We will build up to bigger, full-color images soon.)\n\nAnalysis: I scanned the Chapter 2 notebook and did not find an explanation about how color images are represented on a computer. Some of the methods did retrieve the relevant context shown above.\nConclusion: Remove this question from the evaluation set as the chapter doesn’t include context to answer it.\nTags: unanswerable\n\n\nQuestion 12\n\nprint_data(7)\n\nChapter, Question Number: 4 12\nQuestion Text: \"\"What is SGD?\"\"\nAnswer: \"\"SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\"\"\nKeywords: \"SGD, stochastic, gradient, descent, optimization, algorithm\"\n\n\nRelevant Context:\n\nTo be exact, we’ll discuss the roles of arrays and tensors and of broadcasting, a powerful technique for using them expressively. We’ll explain stochastic gradient descent (SGD), the mechanism for learning by updating weights automatically. We’ll discuss the choice of a loss function for our basic classification task, and the role of mini-batches. We’ll also describe the math that a basic neural network is actually doing. Finally, we’ll put all these pieces together.\n\n\nTo be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\nStep (that is, change) all the weights based on that calculation.\nGo back to the step 2, and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\n\nAnalysis: The paragraphs that answer this question are relatively far apart in the document so even a 3-paragraph chunk would not capture both. Furthermore, the paragraph that lists the seven steps of SGD only contains one of the keywords, “gradient.”\nConclusion: Keep the question in the evaluation set. I’m not so sure this question will be answerable by semantic search but I don’t see any reason to remove it (other than it’s hard to answer).\nTags: insufficient keywords\n\n\nQuestion 13\n\nprint_data(8)\n\nChapter, Question Number: 4 13\nQuestion Text: \"\"Why does SGD use mini-batches?\"\"\nAnswer: \"\"We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using mini-batches are also more computationally efficient than single items on a GPU.\"\"\nKeywords: \"SGD, mini-batches, minibatches, stochastic, gradient, descent\"\n\n\nRelevant Context:\n\nIn order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you’d be going to the trouble of updating the weights, but taking into account only how that would improve the model’s performance on that single item.\n\n\nSo instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a mini-batch. The number of data items in the mini-batch is called the batch size. A larger batch size means that you will get a more accurate and stable estimate of your dataset’s gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this book.\n\n\nAnother good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it’s helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!\n\nAnalysis: I’m surprised that none of the methods retrieved this context. The term “mini-batch” appears 14 times in the Chapter 4 text and 5 times in the three paragraphs of the relevant context.\nConclusion: Keep the question in the evaluation set. My guess is that the other keywords (“SGD”, “stochastic”, “gradient”, “descent”) distract the full text search. I would expect semantic search to succeed for this question.\nTags: insufficient keywords, semantic search\n\n\nQuestion 14\n\nprint_data(9)\n\nChapter, Question Number: 4 14\nQuestion Text: \"\"What are the seven steps in SGD for machine learning?\"\"\nAnswer: \"\"Initialize the parameters - Random values often work best.\nCalculate the predictions - This is done on the training set, one mini-batch at a time.\nCalculate the loss - The average loss over the minibatch is calculated\nCalculate the gradients - this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights - update the parameters based on the calculated weights\nRepeat the process\nStop - In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\"\"\nKeywords: \"SGD, steps, machine, learning, gradient, descent\"\n\n\nRelevant Context:\n\nTo be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\nStep (that is, change) all the weights based on that calculation.\nGo back to the step 2, and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\n\nAnalysis: Similar to Chapter 4, Question 12, only two of the keywords, “machine” and “learning”, are found in the relevant context. Not only that, but those two keywords are rather distracting as there are 8 occurences of “machine” and 53 occurences of “learning” in the chapter text, distracting the full text search from finding the relevant context.\nConclusion: Keep the question in the evaluation set. I think this a tough question to answer for full text search, but semantic search might perform better.\nTags: insufficient keywords, distracting keywords, difficult question, semantic search\n\n\nQuestion 23\n\nprint_data(10)\n\nChapter, Question Number: 4 23\nQuestion Text: \"\"What is the function to calculate new weights using a learning rate?\"\"\nAnswer: \"\"The optimizer step function\"\"\nKeywords: \"function, calculate, weight, weights, learning, rate\"\n\n\nRelevant Context:\n\nDeciding how to change our parameters based on the values of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything. Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we’ll show you a better approach later in this book, called the learning rate finder). Once you’ve picked a learning rate, you can adjust your parameters using this simple function:\nw -= gradient(w) * lr\nThis is known as stepping your parameters, using an optimizer step. Notice how we subtract the gradient * lr from the parameter to update it. This allows us to adjust the parameter in the direction of the slope by increasing the parameter when the slope is negative and decreasing the parameter when the slope is positive. We want to adjust our parameters in the direction of the slope because our goal in deep learning is to minimize the loss.\n\nAnalysis: Some of the methods (like BM25_C) did retrieve the first paragraph (“Deciding how to change…”) but did not capture the third paragraph that has the answer (“This is known…optimizer step”). The keywords are also distracting, here are the occurences of each keyword in the chapter text:\n\n\n\nkeyword\nOccurences\n\n\n\n\nfunction\n157\n\n\ncalculate\n69\n\n\nweight\n67\n\n\nlearning\n53\n\n\nweights\n43\n\n\nrate\n33\n\n\n\nThat being said, “learning rate” does appear five times in the first paragraph. The problem is that none of the keywords appear in the second paragraph, which has the answer.\nConclusion: Keep the question in the evaluation set. This is an example of where chunking strategy would make a difference. I’m not so sure semantic search will be successful unless the three paragraphs in the relevant context are in one chunk.\nTags: insufficient keywords, chunking strategy\n\n\nQuestion 28\n\nprint_data(11)\n\nChapter, Question Number: 4 28\nQuestion Text: \"\"What are the \"\"bias\"\" parameters in a neural network? Why do we need them?\"\"\nAnswer: \"\"Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.\"\"\nKeywords: \"bias, biases, neural, network, networks, parameters\"\n\n\nRelevant Context:\n\nThe function weights*pixels won’t be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its intercept is 0). You might remember from high school math that the formula for a line is y=w*x+b; we still need the b. We’ll initialize it to a random number too:\nbias = init_params(1)\nIn neural networks, the w in the equation y=w*x+b is called the weights, and the b is called the bias. Together, the weights and bias make up the parameters.\n\nAnalysis: This is another example of how chunking strategy effects the retrieval performance. Multiple approaches retrieved the third paragraph (“In neural networks…”) as it contains 4 of the 6 keywords (“bias”, “neural”, “networks”, and “parameters”). However, if the chunking strategy (1- or 3-paragraph) didn’t capture the two paragraphs before it, the answer was not retrieved (“…it is always equal to 0…”).\nConclusion: This question is answerable, but is sensitive to chunking strategy for full text search. Perhaps semantic search will perform better.\nTags: chunking strategy, semantic search\n\n\nQuestion 31\n\nprint_data(12)\n\nChapter, Question Number: 4 31\nQuestion Text: \"\"Why do we have to zero the gradients?\"\"\nAnswer: \"\"PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value.\"\"\nKeywords: \"zero, zeroing, gradient, gradients, optimization\"\n\n\nRelevant Context:\n\nThe gradients have changed! The reason for this is that loss.backward actually adds the gradients of loss to any gradients that are currently stored. So, we have to set the current gradients to 0 first:\nweights.grad.zero_()\nbias.grad.zero_();\n\nAnalysis: The methods did not retrieve any of the relevant context. My guess is that the key keyword is 0 but the Claude-generated keywords don’t include that number (they only have “zero” and “zeroing”).\nConclusion: Keep the question in the evaluation set. I would imagine semantic search performing better for this question.\nTags insufficient keywords, semantic search\n\n\n\nChapter 8 (10 questions)\n\nQuestion 2\n\nprint_data(13)\n\nChapter, Question Number: 8 2\nQuestion Text: \"\"How does it solve it?\"\"\nAnswer: \"\"The key idea of collaborative filtering is latent factors. The idea is that the model can tell what kind of items you may like (ex: you like sci-fi movies/books) and these kinds of factors are learned (via basic gradient descent) based on what items other users like.\"\"\nKeywords: \"solve, solves, solution, solutions, how\"\n\n\nRelevant Context:\n\nThe key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people’s movie watching decisions.\n\n\nSince we don’t know what the latent factors actually are, and we don’t know how to score them for each user and movie, we should learn them.\n\nAnalysis: This is an amiguous question as it doesn’t provide any context for Claude to generate relevant keywords. Both paragraphs of the relevant context are needed to fully answer this question and they are far apart in the document, so having the right keywords is critical.\nConclusion: Remove this question from the evaluation set because it is too ambiguous for keyword-based or semantic search.\nTags: unanswerable\n\n\nQuestion 3\n\nprint_data(14)\n\nChapter, Question Number: 8 3\nQuestion Text: \"\"Why might a collaborative filtering predictive model fail to be a very useful recommendation system?\"\"\nAnswer: \"\"If there are not many recommendations to learn from, or enough data about the user to provide useful recommendations, then such collaborative filtering systems may not be useful.\"\"\nKeywords: \"collaborative, filtering, predictive, model, recommendation, system, fail\"\n\n\nRelevant Context:\n\nThe biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\n\nAnalysis: The keywords are distracting the full text search. “collaborative” (12 times), “filtering” (12), and “model” (63) appear many other times in the chapter text. A better keyword would have been “problem” which appears twice in the relevant context.\nConclusion: Keep the question in the evaluation set. I think this question will perform better with semantic search.\nTags: insufficient keywords, semantic search\n\n\nQuestion 4\n\nprint_data(15)\n\nChapter, Question Number: 8 4\nQuestion Text: \"\"What does a crosstab representation of collaborative filtering data look like?\"\"\nAnswer: \"\"In the crosstab representation, the users and items are the rows and columns (or vice versa) of a large matrix with the values filled out based on the user's rating of the item.\"\"\nKeywords: \"crosstab, collaborative, filtering, data, representation\"\n\n\nRelevant Context:\n\nWe have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\n\n\nBecause each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle.\n\nAnalysis: The two main pieces of context that answer this question are images which I have not shown above. The text that I have shown doesn’t fully explain the answer to this question.\nConclusion: I will remove this question from the evaluation set because it requires images to answer this question and currently I am not storing images in the database.\nTags: unanswerable, requires image\n\n\nQuestion 6\n\nprint_data(16)\n\nChapter, Question Number: 8 6\nQuestion Text: \"\"What is a latent factor? Why is it \"\"latent\"\"?\"\"\nAnswer: \"\"As described above, a latent factor are factors that are important for the prediction of the recommendations, but are not explicitly given to the model and instead learned (hence \"latent\").\"\"\nKeywords: \"latent, factor, factors, hidden, unobservable, underlying\"\n\n\nRelevant Context:\n\nThe key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people’s movie watching decisions.\n\n\nSince we don’t know what the latent factors actually are, and we don’t know how to score them for each user and movie, we should learn them.\n\nAnalysis: This is similar to Question #2 in this chapter. The word “latent” shows up 17 times in the chapter, and “factors” 61 times.\nConclusion: Keep the question in the evaluation set. It’s likely that full text search, even with better keywords, just might not be able to retrieve this context. I would think semantic search would.\nTags: insufficient keywords, semantic search\n\n\nQuestion 8\n\nprint_data(17)\n\nChapter, Question Number: 8 8\nQuestion Text: \"\"What does pandas.DataFrame.merge do?\"\"\nAnswer: \"\"It allows you to merge DataFrames into one DataFrame.\"\"\nKeywords: \"pandas, dataframe, merge, join, combine\"\n\n\nRelevant Context:\n\nWe can merge this with our ratings table to get the user ratings by title:\nratings = ratings.merge(movies)\nratings.head()\n\nAnalysis: Only those three lines are relevant for this question and they don’t explicitly answer the question (i.e. there isn’t an explicit definition of .merge).\nConclusion: Remove this question from the evaluation set.\nTags: unanswerable\n\n\nQuestion 12\n\nprint_data(18)\n\nChapter, Question Number: 8 12\nQuestion Text: \"\"What does an embedding contain before we start training (assuming we're not using a pretained model)?\"\"\nAnswer: \"\"The embedding is randomly initialized.\"\"\nKeywords: \"embedding, embeddings, training, pretrained, model, models\"\n\n\nRelevant Context:\n\nThis is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\n\n\nSo far, we’ve used Embedding without thinking about how it really works. Let’s re-create DotProductBias without using this class. We’ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall from <> that optimizers require that they can get all the parameters of a module from the module’s parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:\n\nAnalysis: There are over 50 occurences of the words “embedding” and “embeddings”. Perhaps if “initialize” was a keyword the full text search would have retrieved the relevant context.\nConclusion: Keep the question in the evaluation set. Chalk this one up to a lack of appropriate keywords. Perhaps semantic search would do better in this situation.\nTags: insufficient keywords, semantic search\n\n\nQuestion 17\n\nprint_data(19)\n\nChapter, Question Number: 8 17\nQuestion Text: \"\"What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\"\"\nAnswer: \"\"We would need to ensure the model outputs 5 predictions. For example, with a neural network model, we need to change the last linear layer to output 5, not 1, predictions. Then this is passed into the Cross Entropy loss.\"\"\nKeywords: \"cross-entropy, loss, MovieLens, model, change\"\n\n\nAnalysis: There is no relevant context from the chapter text that answers this question.\nConclusion: Remove this question from the evaluation set.\nTags: unanswerable\n\n\nQuestion 23\n\nprint_data(20)\n\nChapter, Question Number: 8 23\nQuestion Text: \"\"What does argsort do in PyTorch?\"\"\nAnswer: \"\"This just gets the indices in the order that the original PyTorch Tensor is sorted.\"\"\nKeywords: \"argsort, pytorch, sorting, indices, tensor, arrays\"\n\n\nRelevant Context:\n\nidxs = movie_bias.argsort()[:5]\n\n\nidxs = movie_bias.argsort(descending=True)[:5]\n\n\nidxs = movie_bias.argsort(descending=True)[:5]\n\nAnalysis: This is another example of the chapter text not having the answer explicit. A\nConclusion: I’ll remove this question from the evaluation set. Although an LLM would likely know what argsort does, so the code examples might be enough.\nTags: unanswerable\n\n\nQuestion 29\n\nprint_data(21)\n\nChapter, Question Number: 8 29\nQuestion Text: \"\"When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\"\"\nAnswer: \"\"In this case, we are not taking the dot product but instead concatenating the embedding matrices, so the number of factors can be different.\"\"\nKeywords: \"neural, networks, collaborative, filtering, factors, movies, users\"\n\n\nRelevant Context:\n\nSince we’ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:\n\nAnalysis: If “different” was one of the keywords, the relevant context would likely have been retrieved by one of the methods as it appears only twice in the document, both times in this paragraph.\nConclusion: Keep the question in the evaluation set. . I expect semantic search to retrieve this context.\nTags: insufficient keywords, semantic search\n\n\nQuestion 30\n\nprint_data(22)\n\nChapter, Question Number: 8 30\nQuestion Text: \"\"Why is there an nn.Sequential in the CollabNN model?\"\"\nAnswer: \"\"This allows us to couple multiple nn.Module layers together to be used. In this case, the two linear layers are coupled together and the embeddings can be directly passed into the linear layers.\"\"\nKeywords: \"sequential, collabnn, model, neural, networks\"\n\n\nRelevant Context:\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\nAnalysis: The explanatory text does not reference nn.Sequential. It’s likely that an LLM would know what that is and does.\nConclusion: Remove question from evaluation set.\nTags: unanswerable\n\n\n\nChapter 9 (5 questions)\n\nQuestion 2\n\nprint_data(23)\n\nChapter, Question Number: 9 2\nQuestion Text: \"\"What is a categorical variable?\"\"\nAnswer: \"\"This refers to variables that can take on discrete levels that correspond to different categories.\"\"\nKeywords: \"categorical, variable, variables, statistics, data\"\n\n\nRelevant Context:\n\nIn tabular data some columns may contain numerical data, like “age,” while others contain string values, like “sex.” The numerical data can be directly fed to the model (with some optional preprocessing), but the other columns need to be converted to numbers. Since the values in those correspond to different categories, we often call this type of variables categorical variables. The first type are called continuous variables.\n\n\njargon: Continuous and Categorical Variables: Continuous variables are numerical data, such as “age,” that can be directly fed to the model, since you can add and multiply them directly. Categorical variables contain a number of discrete levels, such as “movie ID,” for which addition and multiplication don’t have meaning (even if they’re stored as numbers).\n\nAnalysis: There are 50+ occurences of the word “categorical” and 30+ occurences of the word “variable” in this chapter. I think it’s just too common a keyword to give the desired result.\nConclusion: Keep the question in the evaluation set. Semantic search might perform better.\nTags: insufficient keywords, semantic search\n\n\nQuestion 13\n\nprint_data(24)\n\nChapter, Question Number: 9 13\nQuestion Text: \"\"How are mse, samples, and values calculated in the decision tree drawn in this chapter?\"\"\nAnswer: \"\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"\"\nKeywords: \"mse, samples, values, decision, tree, calculated, calculation\"\n\n\nRelevant Context:\n\nThe top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remember that unless you see m_rmse, or a root mean squared error, then the value you are looking at is before taking the square root, so it is just the average of the square of the differences.) We can also see that there are 404,710 auction records in this group—that is the total size of our training set. The final piece of information shown here is the decision criterion for the best split that was found, which is to split based on the coupler_system column.\nMoving down and to the left, this node shows us that there were 360,847 auction records for equipment where coupler_system was less than 0.5. The average value of our dependent variable in this group is 10.21. Moving down and to the right from the initial model takes us to the records where coupler_system was greater than 0.5.\nThe bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node containing records where coupler_system was greater than 0.5. The average value here is 9.21, so we can see the decision tree algorithm did find a single binary decision that separated high-value from low-value auction results. Asking only about coupler_system predicts an average value of 9.21 versus 10.1.\nReturning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether YearMade is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, based on coupler_system and YearMade) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly.\n\nAnalysis: This is one of the harder questions to answer with full text search because it’s not explicitly written out in the text. You have to infer the answer from at a minimum the first two paragraphs.\nConclusion: Keep the question in the evaluation set. While it’s a difficult question to answer given the context, there’s no reason to remove it from the evaluation text.\nTags: insufficient keywords, difficult question\n\n\nQuestion 14\n\nprint_data(25)\n\nChapter, Question Number: 9 14\nQuestion Text: \"\"How do we deal with outliers, before building a decision tree?\"\"\nAnswer: \"\"Finding out of domain data (Outliers)\nSometimes it is hard to even know whether your test set is distributed in the same way as your training data or, if it is different, then what columns reflect that difference. There's actually a nice easy way to figure this out, which is to use a random forest!\nBut in this case we don't use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set.\"\"\nKeywords: \"outliers, outlier, decision, tree, trees\"\n\n\nRelevant Context:\n\nSometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There’s actually an easy way to figure this out, which is to use a random forest!\nBut in this case we don’t use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set. To see this in action, let’s combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance:\n\nAnalysis: This is a tough one for full text search because the keyword in the question text, “outlier”, is not used in the relevant context.\nConclusion: Keep the question in the evaluation set. I would expect semantic search to retrieve the relevant context.\nTags: insufficient keywords, semantic search\n\n\nQuestion 22\n\nprint_data(26)\n\nChapter, Question Number: 9 22\nQuestion Text: \"\"Explain why random forests are well suited to answering each of the following question:\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors?\nHow do predictions vary as we vary these columns?\"\"\nAnswer: \"\"Look at standard deviation between the estimators\nUsing the treeinterpreter package to check how the prediction changes as it goes through the tree, adding up the contributions from each split/feature. Use waterfall plot to visualize.\nLook at feature importance\nLook at partial dependence plots\"\"\nKeywords: \"random forests, predictions, confidence, factors, influence, columns, predictors, variation\"\n\n\nAnalysis: The five questions asked are answered across 10+ paragraphs so I have not listed them here. I’m not sure if I should remove this question because it’s five questions in one, or break them apart into five questions. I’m leaning toward removing it since I don’t want to alter the question text.\nConclusion: Remove the question from the evaluation set.\nTags: unanswerable\n\n\nQuestion 29\n\nprint_data(27)\n\nChapter, Question Number: 9 29\nQuestion Text: \"\"How could we use embeddings with a random forest? Would we expect this to help?\"\"\nAnswer: \"\"Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model.\"\"\nKeywords: \"embeddings, random forest, forests, machine learning, algorithms\"\n\n\nRelevant Context:\n\nThe abstract of the entity embedding paper we mentioned at the start of this chapter states: “the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead”. It includes the very interesting table in <>.\n\n\nThis is showing the mean average percent error (MAPE) compared among four different modeling techniques, three of which we have already seen, along with k-nearest neighbors (KNN), which is a very simple baseline method. The first numeric column contains the results of using the methods on the data provided in the competition; the second column shows what happens if you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. As you see, in every case, the models are dramatically improved by using the embeddings instead of the raw categories.\n\n\nThis is a really important result, because it shows that you can get much of the performance improvement of a neural network without actually having to use a neural network at inference time. You could just use an embedding, which is literally just an array lookup, along with a small decision tree ensemble.\n\nAnalysis: Perhaps the full text search methods would have retrieved the relevant context (or at least parts of it) if the context said “random forest” instead of “decision tree ensemble”.\nConclusion: Keep the question in the evaluation set. I expect semantic search to retrieve the relevant context.\nTags: insufficient keywords, semantic search\n\n\n\nChapter 10 (5 questions)\n\nQuestion 8\n\nprint_data(28)\n\nChapter, Question Number: 10 8\nQuestion Text: \"\"What are the three steps to prepare your data for a language model?\"\"\nAnswer: \"\"Tokenization\nNumericalization\nLanguage model DataLoader\"\"\nKeywords: \"steps, prepare, data, language, model, models\"\n\n\nRelevant Context:\n\nEach of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are:\n\nTokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\nNumericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab\nLanguage model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required\nLanguage model creation:: We need a special kind of model that does something we haven’t seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the <>, but for now, you can think of it as just another deep neural network.\n\n\nAnalysis: The issue here is that while the question says “steps to prepare your data” the relevant context has the phrasing “steps necessary to create a language model”. There are 11 other occurences of the word “steps” in the chapter, and 0 occurences of the word “prepare.”\nConclusion: Keep the question in the evaluation set. Semantic search might perform better.\nTags: insufficient keywords, semantic search\n\n\nQuestion 12\n\nprint_data(29)\n\nChapter, Question Number: 10 12\nQuestion Text: \"\"List four rules that fastai applies to text during tokenization.\"\"\nAnswer: \"\"Here are all the rules:\nfix_html :: replace special HTML characters by a readable version (IMDb reviews have quite a few of them for instance) ;\nreplace_rep :: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it's repeated, then the character ;\nreplace_wrep :: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it's repeated, then the word ;\nspec_add_spaces :: add spaces around / and # ;\nrm_useless_spaces :: remove all repetitions of the space character ;\nreplace_all_caps :: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\nreplace_maj :: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\nlowercase :: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\"\"\nKeywords: \"rules, fastai, text, tokenization, tokens\"\n\n\nRetrieved Context:\n\nThese special tokens don’t come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. In a sense, we are translating the original English language sequence into a simplified tokenized language—a language that is designed to be easy for a model to learn.\n\n\nFor instance, the rules will replace a sequence of four exclamation points with a special repeated character token, followed by the number four, and then a single exclamation point. In this way, the model’s embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization.\n\n\nHere is a brief summary of what each does:\n\nfix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\nreplace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character\nreplace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the word\nspec_add_spaces:: Adds spaces around / and #\nrm_useless_spaces:: Removes all repetitions of the space character\nreplace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\nreplace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\nlowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)\n\n\nAnalysis: The two most important keywords, “four” and “rules”, are not present in the key bulleted list of relevant context that lists all of the fastai rules.\nConclusion: Perhaps a different chunking strategy would have yielded better retrieval using the keyword-based search.\nTags: chunking strategy\n\n\nQuestion 17\n\nprint_data(30)\n\nChapter, Question Number: 10 17\nQuestion Text: \"\"Why do we need padding for text classification? Why don't we need it for language modeling?\"\"\nAnswer: \"\"Since the documents have variable sizes, padding is needed to collate the batch. Other approaches. like cropping or squishing, either to negatively affect training or do not make sense in this context. Therefore, padding is used. It is not required for language modeling since the documents are all concatenated.\"\"\nKeywords: \"padding, text, classification, language, modeling, need\"\n\n\nRelevant Context:\n\nWe will expand the shortest texts to make them all the same size. To do this, we use a special padding token that will be ignored by our model. Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend to be of similar lengths. We won’t pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, but at the time of writing no library provides good support for this yet, and there aren’t any papers covering it. It’s something we’re planning to add to fastai soon, however, so keep an eye on the book’s website; we’ll add information about this as soon as we have it working well.)\n\n\nThe sorting and padding are automatically done by the data block API for us when using a TextBlock, with is_lm=False. (We don’t have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)\n\nAnalysis: There are four occurences of the word “padding” in the chapter, it occurs once in each of the relevant paragraphs. There are 80+ occurences of the keywords “text” and 70+ occurences of “language” in the chapter.\nConclusion: Keep the question in the evaluation set. Semantic search might perform better.\nTags: distracting keywords, semantic search\n\n\nQuestion 20\n\nprint_data(31)\n\nChapter, Question Number: 10 20\nQuestion Text: \"\"Why do we have to pass the vocabulary of the language model to the classifier data block?\"\"\nAnswer: \"\"This is to ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LM fine-tuning.\"\"\nKeywords: \"vocabulary, language, model, classifier, data, block\"\n\n\nRelevant Context:\n\nThe reason that we pass the vocab of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won’t make any sense to this model, and the fine-tuning step won’t be of any use.\n\nAnalysis: Only two of the keywords (“language” and “model”) are in the relevant context. Those two words also have very high occurences in the chapter (70 and 120, respectively). The main keyword, “vocabulary” is not in the relevant context (a different form of it is, “vocab”).\nConclusion: Keep the question in the evaluation set. Semantic search might work better (finding similarity between “vocab” and “vocabulary”).\nTags: insufficient keywords, distracting keywords, semantic search\n\n\nQuestion 22\n\nprint_data(32)\n\nChapter, Question Number: 10 22\nQuestion Text: \"\"Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\"\"\nAnswer: \"\"The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead.\"\"\nKeywords: \"text, generation, automatic, identification, machine-generated, texts\"\n\n\nRelevant Context:\n\nMany people assume or hope that algorithms will come to our defense here—that we will develop classification algorithms that can automatically recognise autogenerated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms.\n\nAnalysis: None of the keywords are present in the relevant context. (“automatically” is, but not “automatic”).\nConclusion: Keep the question in the evaluation set. I expect semantic search to perform better.\nTags: insufficient keywords, semantic search\n\n\n\nChapter 13 (6 questions)\n\nQuestion 4\n\nprint_data(33)\n\nChapter, Question Number: 13 4\nQuestion Text: \"\"What is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\"\"\nAnswer: \"\"A zero matrix.\"\"\nKeywords: \"convolutional, kernel, matrix, zeros, value\"\n\n\nRelevant Context:\n\nNow we’re going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we’ll add them up, like so:\n\n\nim3_t = tensor(im3)\nim3_t[0:3,0:3] * top_edge\nOutput: tensor([[-0., -0., -0.], [0., 0., 0.], [0., 0., 0.]])\n(im3_t[0:3,0:3] * top_edge).sum()\nOutput: tensor(0.)\n\n\nNot very interesting so far—all the pixels in the top-left corner are white.\n\nAnalysis: This is a tough context to retrieve for full text search because it relies on interpreting multiple lines of code.\nConclusion: Keep the question in the evaluation set. Semantic search might perform better.\nTags: insufficient keywords, semantic search\n\n\nQuestion 5\n\nprint_data(34)\n\nChapter, Question Number: 13 5\nQuestion Text: \"\"What is \"\"padding\"\"?\"\"\nAnswer: \"\"Padding is the additional pixels that are added around the outside of the image, allows the kernel to be applied to the edge of the image for a convolution.\"\"\nKeywords: \"padding, paddings, cushion, cushions, fill, filler\"\n\n\nRelevant Context:\n\nIt would be nice to not lose those two pixels on each axis. The way we do that is to add padding, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added.\n\nAnalysis: I was surprised that none of the full text search methods retrieved this relevant context, since it’s usually good with terms/definitions. There are 15 other occurences of “padding” in the chapter.\nConclusion: Keep the question in the evaluation set. Semantic search might perform better.\nTags: insufficient keywords, semantic search\n\n\nQuestion 15\n\nprint_data(35)\n\nChapter, Question Number: 13 15\nQuestion Text: \"\"Why does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications?\"\"\nAnswer: \"\"There are 1168 parameters for that layer, and ignoring the 16 parameters (=number of filters) of the bias, the (1168-16) parameters is applied to the 7x7 grid.\"\"\nKeywords: \"MNIST, CNN, layer, layers, multiplication, multiplications\"\n\n\nRelevant Context:\n\nThere is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let’s ignore the batch axis to keep things simple. So for each of 1414=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that’s 196288=56_448 multiplications at this layer. The next layer will have 77(1168-16)=56_448 multiplications.\n\nAnalysis: I’m not sure an LLM would be able to deduce why the multiplication 77(1168-16) is needed. This might be too difficult a question to answer.\nConclusion: Keep the question in the evaluation set. Even though it’s difficult to answer.\nTags: difficult question\n\n\nQuestion 22\n\nprint_data(36)\n\nChapter, Question Number: 13 22\nQuestion Text: \"\"What method can we use to see that data in DataLoaders?\"\"\nAnswer: \"\"show_batch\"\"\nKeywords: \"method, data, dataloaders, dataloader, view, visualize\"\n\n\nRelevant Context:\n\nRemember, it’s always a good idea to look at your data before you use it:\n\n\ndls.show_batch(max_n=9, figsize=(4,4))\n\nAnalysis: The keyword “view” does not appear in the relevant context, though it does appear in 6 other places in the chapter.\nConclusion: Keep the question in the evaluation set. I expect semantic search to perform better.\nTags: insufficient keywords, semantic search\n\n\nQuestion 24\n\nprint_data(37)\n\nChapter, Question Number: 13 24\nQuestion Text: \"\"Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\"\"\nAnswer: \"\"With the first layer, if the kernel size is 3x3, with four output filters, then nine pixels are being used to produce 8 output numbers so there is not much learning since input and output size are almost the same. Neural networks will only create useful features if they're forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer.\"\"\nKeywords: \"kernel, conv, MNIST, simple_cnn, larger\"\n\n\nRelevant Context:\n\nBut there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3x3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn’t really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they’re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs.\n\n\nTo fix this, we can use a larger kernel in the first layer. If we use a kernel of 5x5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features:\n\nAnalysis: The most important keyword, “first” was not generated by Claude for this question. Two of the other keywords, “conv” and “kernel”, have almost 200 occurences in the chapter.\nConclusion: Keep the question in the evaluation set. I expect semantic search to perform better.\nTags: insufficient keywords, distracting keywords, semantic search\n\n\nQuestion 27\n\nprint_data(38)\n\nChapter, Question Number: 13 27\nQuestion Text: \"\"What are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\"\"\nAnswer: \"\"The mean and standard deviation of the activations, as well as the percentage of activation near zero. The x-axis represents the progress of training (batch number).\"\"\nKeywords: \"plot_layer_stats, statistics, x-axis, plotted, layers\"\n\n\nRelevant Context:\n\nActivationStats includes some handy utilities for plotting the activations during training. plot_layer_stats(idx) plots the mean and standard deviation of the activations of layer number idx, along with the percentage of activations near zero. Here’s the first layer’s plot:\n\nAnalysis: Many of the full text search methods retrieved this paragraph. However, that only answers the first part of the question. An answer to the second part of the question, “What does the x-axis represent?” is not explicitly provided in the chapter text. You would have to infer it from the image (which is not included in the database).\nConclusion: Remove the question from the evaluation set.\nTags: unanswerable, requires image"
  },
  {
    "objectID": "posts/2024-09-05-fastbookRAG-bm25-error-analysis/index.html#final-thoughts",
    "href": "posts/2024-09-05-fastbookRAG-bm25-error-analysis/index.html#final-thoughts",
    "title": "Conducting a Question-by-Question Error Analysis on Full Text Search Results",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere is a summary of the tags I assigned to each question:\n\n\n\nTag\nCount\nPercentage of 39\nPercentage of 202\n\n\n\n\ninsufficient keywords\n25\n64%\n12%\n\n\nsemantic search\n23\n59%\n11%\n\n\nunanswerable\n9\n21%\n4%\n\n\ndistracting keywords\n8\n10%\n4%\n\n\ndifficult questions\n5\n13%\n2%\n\n\nchunking strategy\n3\n8%\n1%\n\n\nrequires image\n2\n5%\n1%\n\n\n\n\nIf my intuition is correct, and 23 of the questions that were not answered by any full text search method will be answered by semantic search, I might see a 11% increase in overall performance (from 76.7% to ~88%).\nThere are 9 unanswerable questions, which is 4% of the overall evaluation set. This would improve the overall Answer Rate from 76.7% to ~81% (which is not bad for a baseline full text search!)\nPursuing a different chunking strategy will only improve performance by 1%. However, if the rate of questions that might be improved by a better chunking strategy is 8% (for these 39 questions) and that rate is applicable to the rest of the 163 questions, I could potentially improve the overall Answer Rate by 8%.\nOf course, this is all speculation at this point and will be determined after I evaluate the semantic search baselines.\n\nHere are the number of unanswered questions per chapter:\n\n\n\nChapter\n# of Questions\n\n\n\n\n1\n2\n\n\n2\n4\n\n\n4\n7\n\n\n8\n10\n\n\n9\n5\n\n\n10\n5\n\n\n13\n6\n\n\nTotal\n39\n\n\n\n\nHere are the tags broken down by chapter:\n\n\n\n\nChapter\nTag\nCount\n\n\n\n\n1\nsemantic search\n2\n\n\n1\ninsufficient keywords\n2\n\n\n1\ndistracting keywords\n1\n\n\n2\ninsufficient keywords\n3\n\n\n2\ndistracting keywords\n3\n\n\n2\nsemantic search\n2\n\n\n2\ndifficult question\n1\n\n\n4\ninsufficient keywords\n5\n\n\n4\nsemantic search\n4\n\n\n4\nchunking strategy\n2\n\n\n4\nunanswerable\n1\n\n\n4\ndifficult question\n1\n\n\n4\ndistracting keywords\n1\n\n\n8\nunanswerable\n6\n\n\n8\nsemantic search\n4\n\n\n8\ninsufficient keywords\n4\n\n\n8\nrequires image\n1\n\n\n9\ninsufficient keywords\n4\n\n\n9\nsemantic search\n3\n\n\n9\nunanswerable\n1\n\n\n9\ndifficult question\n1\n\n\n10\nsemantic search\n4\n\n\n10\ninsufficient keywords\n3\n\n\n10\ndistracting keywords\n2\n\n\n10\nchunking strategy\n1\n\n\n13\nsemantic search\n4\n\n\n13\ninsufficient keywords\n4\n\n\n13\nunanswerable\n1\n\n\n13\ndistracting keywords\n1\n\n\n13\ndifficult question\n1\n\n\n13\nrequires image\n1\n\n\n\n\nI want to highlight that 6 of the 29 Chapter 8 questions are “unanswerable”, so removing those from the evaluation set will greatly increase the Answer Rate for that chapter.\nHere are the updated full text search baseline results with the “unanswerable” questions removed from consideration.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter\nBM25_A (Top-1 1p)\nBM25_B (Top-3 1p)\nBM25_C (Top-5 1p)\nBM25_D (Top-1 3p)\nBM25_E (Top-3 3p)\nBM25_F (Top-5 3p)\n\n\n\n\n1\n40% (12/30)\n56.7% (17/30)\n60% (18/30)\n63.3% (19/30)\n83.3% (25/30)\n90% (27/30)\n\n\n2\n38.5% (10/26)\n65.4% (17/26)\n69.2% (18/26)\n46.2% (12.26)\n80.8% (21/26)\n80.8% (21/26)\n\n\n4\n25.8% (8/31)\n71% (22/31)\n74.2% (23/31)\n32.3% (10/31)\n74.2% (23/31)\n77.4% (24/31)\n\n\n8\n17.4% (4/23)\n43.5% (10/23)\n56.5% (13/23)\n39.1% (9/23)\n69.6% (16/23)\n82.6% (19/23)\n\n\n9\n13.8% (4/28)\n48.3% (14/28)\n58.6% (17/28)\n34.5% (10/28)\n72.4% (21/28)\n79.3% (23/28)\n\n\n10\n47.6% (12/21)\n42.9% (9/21)\n61.9% (13/21)\n38% (8/21)\n57.1% (12/21)\n61.9% (13/21)\n\n\n13\n38.2% (13/34)\n55.9% (19/34)\n61.8% (21/34)\n44.1% (15/34)\n70.6% (24/34)\n82.4% (28/34)\n\n\nAll\n31.6% (61/193)\n56% (108/193)\n63.7% (123/193)\n43% (83/193)\n73.6% (142/193)\n80.3% (155/193)\n\n\n\nWith the “unanswerable” questions removed, the overall performance of each method increases, with the best-performing BM25_D (Top-1 3-paragraph chunks) reaching an 80.3% Answer Rate!\nI have a much better understanding of the errors made by all of the full text search methods, and this will undoubtedly improve my intuition when applying semantic search to this task.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html",
    "href": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html",
    "title": "Calculating the Aspect Ratio of Letters in a Text Image",
    "section": "",
    "text": "In this notebook, I’ll walk through a modified algorithm (suggested by Claude) to calculate the aspect ratio of letters in a text image. The aspect ratio of text in this case corresponds to the ratio of the width to height of a letter. Strictly speaking, in typography, the aspect ratio is defined as the ratio of the letter height to the x-height (lowercase letter height) of the font. That is not the definition I’m using here. Instead, I mean aspect ratio of a rectangle (width:height) that bounds a letter.\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” and “display”). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\n\n\nShow imports\nimport cv2\nimport numpy as np, pandas as pd"
  },
  {
    "objectID": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#loading-the-data",
    "href": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#loading-the-data",
    "title": "Calculating the Aspect Ratio of Letters in a Text Image",
    "section": "Loading the Data",
    "text": "Loading the Data\nThe first image I’ll use is that of a display typeface (the font Bree) with a font-size of 76px.\n\npath = 'display-76px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\nimg\n\n\n      ndarray (512, 512) show dataarray([[255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       ...,\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n\n\nThe next step is to convert the image to binary data (1=white pixels, 0 = black pixels).\n\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#finding-letter-contours",
    "href": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#finding-letter-contours",
    "title": "Calculating the Aspect Ratio of Letters in a Text Image",
    "section": "Finding Letter Contours",
    "text": "Finding Letter Contours\nNext, we get the contours which are defined in the OpenCV docs as:\n\na curve joining all the continuous points (along the boundary), having same color or intensity\n\nIt makes sense why we converted the image to binary data since that makes the letters all have the same color (white pixels or 1). The contours are outlines of the letters:\n\ncontours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncv2.drawContours(img, contours, -1, (0,255,0), 3)\n\n\n      ndarray (512, 512) show dataarray([[255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       ...,\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#calculating-the-aspect-ratio-for-each-letter",
    "href": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#calculating-the-aspect-ratio-for-each-letter",
    "title": "Calculating the Aspect Ratio of Letters in a Text Image",
    "section": "Calculating the Aspect Ratio for Each Letter",
    "text": "Calculating the Aspect Ratio for Each Letter\nFinally, we can calculate the aspect ratio of a letter by creating a bounding box around it. The width and height of the bounding box are used to calculate the aspect ratio \\(\\frac{\\text{width}}{\\text{height}}\\)\n\nx, y, w, h = cv2.boundingRect(contours[0]) # position, width and height of bounding box\ncv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) # bounding box around \"u\" in \"consequat\"\n\n\n      ndarray (512, 512) show dataarray([[255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       ...,\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n\n\n\n# aspect ratio of the \"u\" in \"consequat\"\nw, h, w/h\n\n(33, 40, 0.825)"
  },
  {
    "objectID": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#calculating-the-average-aspect-ratio-for-all-letters",
    "href": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#calculating-the-average-aspect-ratio-for-all-letters",
    "title": "Calculating the Aspect Ratio of Letters in a Text Image",
    "section": "Calculating the Average Aspect Ratio for All Letters",
    "text": "Calculating the Average Aspect Ratio for All Letters\nI can put the above contour-to-bounding-box code in a loop over all contours and calculate the average aspect ratio of the letters in the image:\n\naspect_ratios = []\n\n\nfor contour in contours:\n  x, y, w, h = cv2.boundingRect(contour)\n  if w > 5 and h > 5:  # filter out punctuation or noise\n    aspect_ratios.append(w / h)\n\n\nnp.mean(aspect_ratios), np.median(aspect_ratios)\n\n(0.7164317414293024, 0.7804878048780488)"
  },
  {
    "objectID": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#calculating-aspect-ratio-for-different-images",
    "href": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#calculating-aspect-ratio-for-different-images",
    "title": "Calculating the Aspect Ratio of Letters in a Text Image",
    "section": "Calculating Aspect Ratio for Different Images",
    "text": "Calculating Aspect Ratio for Different Images\nI’ll wrap that code in a function and apply it to different images of the same font and different fonts.\n\n\nShow the aspect ratio function\ndef aspect_ratio(path):\n    # Read the image\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\n    # Threshold the image\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Find contours\n    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    aspect_ratios = []\n\n    for contour in contours:\n        x, y, w, h = cv2.boundingRect(contour)\n        if w > 5 and h > 5:  # Filter out very small contours\n            aspect_ratios.append(w / h)\n\n    averages = np.mean(aspect_ratios), np.median(aspect_ratios) if aspect_ratios else 0\n    return averages\n\n\n\n\nShow aspect ratios calcs\naspect_ratios = []\nfont_szs = [8, 18, 24, 36, 76, 240, 330, 420]\n\nfor typeface in ['display', 'serif']:\n  for sz in font_szs:\n    aspect_ratios.append((typeface, sz, *aspect_ratio(f\"{typeface}-{sz}px.png\")))\n\n\n\n# Create DataFrame\ndf = pd.DataFrame(aspect_ratios, columns=['typeface', 'font_size', 'mean_ratio', 'median_ratio'])\n\nThe median aspect ratio becomes an outlier at very small (8px) and very large (240px) font sizes.\n\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      typeface\n      font_size\n      mean_ratio\n      median_ratio\n    \n  \n  \n    \n      0\n      display\n      8\n      1.715923\n      1.500000\n    \n    \n      1\n      display\n      18\n      0.815149\n      0.800000\n    \n    \n      2\n      display\n      24\n      0.745288\n      0.769231\n    \n    \n      3\n      display\n      36\n      0.711960\n      0.750000\n    \n    \n      4\n      display\n      76\n      0.716432\n      0.780488\n    \n    \n      5\n      display\n      240\n      1.684231\n      1.263158\n    \n    \n      6\n      display\n      330\n      0.685398\n      0.823529\n    \n    \n      7\n      display\n      420\n      0.829781\n      0.829781\n    \n    \n      8\n      serif\n      8\n      2.473861\n      2.401786\n    \n    \n      9\n      serif\n      18\n      1.084338\n      1.000000\n    \n    \n      10\n      serif\n      24\n      0.919979\n      0.909091\n    \n    \n      11\n      serif\n      36\n      0.838929\n      0.833333\n    \n    \n      12\n      serif\n      76\n      0.901583\n      0.861111\n    \n    \n      13\n      serif\n      240\n      0.864006\n      0.850877\n    \n    \n      14\n      serif\n      330\n      0.920587\n      0.759848\n    \n    \n      15\n      serif\n      420\n      2.277634\n      0.909091\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThe median ratio is also more consistent (smaller standard deviation) than the mean ratio (which makes sense as its more robust).\n\ndf.groupby('typeface')[['mean_ratio', 'median_ratio']].describe().T\n\n\n\n  \n    \n\n\n  \n    \n      \n      typeface\n      display\n      serif\n    \n  \n  \n    \n      mean_ratio\n      count\n      8.000000\n      8.000000\n    \n    \n      mean\n      0.988020\n      1.285115\n    \n    \n      std\n      0.442387\n      0.679100\n    \n    \n      min\n      0.685398\n      0.838929\n    \n    \n      25%\n      0.715314\n      0.892189\n    \n    \n      50%\n      0.780219\n      0.920283\n    \n    \n      75%\n      1.043393\n      1.382662\n    \n    \n      max\n      1.715923\n      2.473861\n    \n    \n      median_ratio\n      count\n      8.000000\n      8.000000\n    \n    \n      mean\n      0.939523\n      1.065642\n    \n    \n      std\n      0.281336\n      0.544326\n    \n    \n      min\n      0.750000\n      0.759848\n    \n    \n      25%\n      0.777674\n      0.846491\n    \n    \n      50%\n      0.811765\n      0.885101\n    \n    \n      75%\n      0.938125\n      0.931818\n    \n    \n      max\n      1.500000\n      2.401786\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nA tighter font size range (18px to 76 px) yields a more stable median aspect ratio. Here you can see that the display text consistently has narrower letters than the serif text.\n\ndf.query(\"font_size >= 18 and font_size <= 76\").groupby('typeface')['median_ratio'].describe().T\n\n\n\n  \n    \n\n\n  \n    \n      typeface\n      display\n      serif\n    \n  \n  \n    \n      count\n      4.000000\n      4.000000\n    \n    \n      mean\n      0.774930\n      0.900884\n    \n    \n      std\n      0.020924\n      0.073112\n    \n    \n      min\n      0.750000\n      0.833333\n    \n    \n      25%\n      0.764423\n      0.854167\n    \n    \n      50%\n      0.774859\n      0.885101\n    \n    \n      75%\n      0.785366\n      0.931818\n    \n    \n      max\n      0.800000\n      1.000000"
  },
  {
    "objectID": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#final-thoughts",
    "href": "posts/2024-08-14-typefaceclassifier-aspect-ratio/index.html#final-thoughts",
    "title": "Calculating the Aspect Ratio of Letters in a Text Image",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThe non-ML side of computer vision—an area I’m quite new to—continues to surprise me with algorithms that fit surprisingly well into my niche use case: classifying typefaces from text images. Just like with the x-height to cap-height ratio algorithm, calculating aspect ratios works best within a specific range of font sizes, as very small or large sizes can cause issues like cropped text or blurry binarized images. I still have several more algorithms to explore as I work toward building a (likely multi-pronged) non-ML baseline for this classification task, and I’ll be covering each in future blog posts.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2023-10-10-TabularModel/index.html",
    "href": "posts/2023-10-10-TabularModel/index.html",
    "title": "Understanding the fastai TabularModel Class",
    "section": "",
    "text": "In this notebook, I will work through the last “Further Research” exercise from Chapter 9 of the fastai textbook:\n\nExplain what each line of the source of TabularModel does (with the exception of the BatchNorm1d and Dropout layers).\n\nI’ll start by pasting the source code of TabularModel here:\nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self,\n        emb_szs:list, # Sequence of (num_embeddings, embedding_dim) for each categorical variable\n        n_cont:int, # Number of continuous variables\n        out_sz:int, # Number of outputs for final `LinBnDrop` layer\n        layers:list, # Sequence of ints used to specify the input and output size of each `LinBnDrop` layer\n        ps:float|MutableSequence=None, # Sequence of dropout probabilities for `LinBnDrop`\n        embed_p:float=0., # Dropout probability for `Embedding` layer\n        y_range=None, # Low and high for `SigmoidRange` activation\n        use_bn:bool=True, # Use `BatchNorm1d` in `LinBnDrop` layers\n        bn_final:bool=False, # Use `BatchNorm1d` on final layer\n        bn_cont:bool=True, # Use `BatchNorm1d` on continuous variables\n        act_cls=nn.ReLU(inplace=True), # Activation type for `LinBnDrop` layers\n        lin_first:bool=True # Linear layer is first or last in `LinBnDrop` layers\n    ):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\nIn the sections below, I will walk through each line of code to make sure I understand what it does.\n\nfrom fastai.tabular.all import *"
  },
  {
    "objectID": "posts/2023-10-10-TabularModel/index.html#init__",
    "href": "posts/2023-10-10-TabularModel/index.html#init__",
    "title": "Understanding the fastai TabularModel Class",
    "section": "__init__",
    "text": "__init__\nThe __init__ method’s parameters are well-defined by the comments in the source code so I will not list each of them here. However, in order to run actual code for the rest of the lines, I will assign some test values to each of the parameters. I’ll also define x_cat and x_cont so that we have some fake data to work with. I have set all BatchNorm1d and Dropout related parameters to 0, False, or None.\n\ntorch.manual_seed(42);\n\n# self.init parameters\nemb_szs = [(4,2), (17,8)]\nn_cont = 1\nout_sz = 1\nlayers = [200,100]\nps = None\nembed_p = 0.\ny_range = (0,1)\nuse_bn = False\nbn_final = False\nbn_cont = False\nact_cls=nn.ReLU(inplace=True)\nlin_first = True\n\n# fake data\nx_cat1 = torch.randint(0,4,(10,))\nx_cat2 = torch.randint(0,17,(10,))\nx_cat = torch.column_stack((x_cat1, x_cat2))\nx_cont = torch.randn(10)[:,None]\n\n\nps\nI am not sure what dropout probabilities do exactly, so I won’t explain the code here other than running it as shown in the source code:\n\nps = ifnone(ps, [0]*len(layers))\nif not is_listy(ps): ps = [ps]*len(layers)\n\n\nps\n\n[0, 0]\n\n\n\n\nself.embeds\nself.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\nThis line of code creates Embeddings, one for each tuple defined in emb_szs, which in turn is one tuple defined for each categorical variable. In my example, I have two categorical variables so I will create two Embeddings.\n\nembeds = nn.ModuleList(Embedding(ni, nf) for ni,nf in emb_szs)\nembeds\n\nModuleList(\n  (0): Embedding(4, 2)\n  (1): Embedding(17, 8)\n)\n\n\n\n\nself.emb_drop\nThe following line creates an nn.Dropout object which will be used in the model. According to the PyTorch website:\n\nDuring training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n\nself.emb_drop = nn.Dropout(embed_p)\n\nemb_drop = nn.Dropout(embed_p)\nemb_drop\n\nDropout(p=0.0, inplace=False)\n\n\n\n\nself.bn_cont\nThe following line assigns a BatchNorm1d function to self.bn_cont if the input argument bn_cont is True. Since in this case, I have set bn_cont to False, it would set self.bn_cont to None:\nself.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n\nbn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\nbn_cont\n\n\n\nn_emb\nn_emb = sum(e.embedding_dim for e in self.embeds)\nIn my toy example, I have one Embedding containing 4 tensors of size 2, and one Embedding containing 17 tensors of size 8, so the total size will be 10.\n\nn_emb = sum(e.embedding_dim for e in embeds)\nn_emb\n\n10\n\n\n\n\nself.n_emb,self.n_cont\nself.n_emb,self.n_cont = n_emb,n_cont\nThis line of code is simply storing the total Embedding size and number of continuous variables.\n\n\nsizes\nsizes = [n_emb + n_cont] + layers + [out_sz]\nThis line defines the sizes of the input to the model (which contains Embeddings for categorical variables and n_cont continuous variables), a number of intermediate layers, and a final out_sz output size for the output of the model.\n\nsizes = [n_emb + n_cont] + layers + [out_sz]\nsizes\n\n[11, 200, 100, 1]\n\n\n\n\nactns\nactns = [act_cls for _ in range(len(sizes)-2)] + [None]\nThis line defines the activations for the model for all layers. The final layer does not have an activation function so it is set to None.\n\nactns = [act_cls for _ in range(len(sizes)-2)] + [None]\nactns\n\n[ReLU(inplace=True), ReLU(inplace=True), None]\n\n\n\n\n_layers\n_layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first) for i,(p,a) in enumerate(zip(ps+[0.],actns))]\nI’ll walk through the components of this line, without going into detail about the “why” behind code related to BatchNorm1d and Dropout layers.\nThe following code zips together the list of ps probabilities and activations actns:\n\nlist(zip(ps+[0.], actns))\n\n[(0, ReLU(inplace=True)), (0, ReLU(inplace=True)), (0.0, None)]\n\n\nThe following code determines whether the bn parameter for LinBnDrop is set to True or False. If use_bn is True, and either of i!=len(actns)-1 or bn_final are True then bn will be assigned True, otherwise it will be False.\nLooking at the second condition, (i!=len(actns)-1 or bn_final), in more detail:\nIf i!=len(actns)-1 is True, it means that the final element of the enumeration has not been reached. In other words, the index does not correspond to the final layer of the model. If it’s False, that means we have reached the index corresponding to the final layer in the model. In that case, the or condition can still result in truth if bn_final is True.\nbn=use_bn and (i!=len(actns)-1 or bn_final)\n\nTrue and (True or False)\n\nTrue\n\n\n\nTrue and (False or True)\n\nTrue\n\n\nThe following code generates a LinBnDrop layer for each activation function, setting the inputs and outputs of the layer based on the values in the sizes list:\n\n_layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a,\n                     lin_first=lin_first) for i, (p,a) in enumerate(zip(ps+[0.], actns))]\n\n_layers\n\n[LinBnDrop(\n   (0): Linear(in_features=11, out_features=200, bias=True)\n   (1): ReLU(inplace=True)\n ),\n LinBnDrop(\n   (0): Linear(in_features=200, out_features=100, bias=True)\n   (1): ReLU(inplace=True)\n ),\n LinBnDrop(\n   (0): Linear(in_features=100, out_features=1, bias=True)\n )]\n\n\n\n\ny_range\nif y_range is not None: _layers.append(SigmoidRange(*y_range))\nThis line of code adds on a SigmoidRange function which limits the output values to the values defined in y_range.\nHere is what the function SigmoidRange(0,1) looks like for input values between -10 and 10:\n\ndef plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = torch.linspace(min,max, 100)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)\n\n\nplot_function(SigmoidRange(0,1), min=-10, max=10)\n\n\n\n\n\nif y_range is not None: _layers.append(SigmoidRange(*y_range))\n\n\n_layers\n\n[LinBnDrop(\n   (0): Linear(in_features=11, out_features=200, bias=True)\n   (1): ReLU(inplace=True)\n ),\n LinBnDrop(\n   (0): Linear(in_features=200, out_features=100, bias=True)\n   (1): ReLU(inplace=True)\n ),\n LinBnDrop(\n   (0): Linear(in_features=100, out_features=1, bias=True)\n ),\n fastai.layers.SigmoidRange(low=0, high=1)]\n\n\n\n\nself.layers\nself.layers = nn.Sequential(*_layers)\nThe final piece to handling the layers in the model is to wrap them in a nn.Sequential model, so that inputs are passed sequentially to each layer in the list _layers.\n\nlayers = nn.Sequential(*_layers)\nlayers\n\nSequential(\n  (0): LinBnDrop(\n    (0): Linear(in_features=11, out_features=200, bias=True)\n    (1): ReLU(inplace=True)\n  )\n  (1): LinBnDrop(\n    (0): Linear(in_features=200, out_features=100, bias=True)\n    (1): ReLU(inplace=True)\n  )\n  (2): LinBnDrop(\n    (0): Linear(in_features=100, out_features=1, bias=True)\n  )\n  (3): fastai.layers.SigmoidRange(low=0, high=1)\n)"
  },
  {
    "objectID": "posts/2023-10-10-TabularModel/index.html#forward",
    "href": "posts/2023-10-10-TabularModel/index.html#forward",
    "title": "Understanding the fastai TabularModel Class",
    "section": "forward",
    "text": "forward\n\nif self.n_emb != 0:\nIn this example, n_emb is not equal to 0 so the following code will run.\nx = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\nIn this line of code, the categorical variable columns are passed to the corresponding Embedding and the output tensor is stored in a list.\n\nx = [e(x_cat[:,i]) for i,e in enumerate(embeds)]\nlen(x), len(x[0]), len(x[1])\n\n(2, 10, 10)\n\n\nThere are 10 rows in x_cat. The Embedding corresponding to the first column outputs 2 columns of tensors, and the Embedding corresponding to the second column outputs 8 columns of tensors.\n\nx[0].shape\n\ntorch.Size([10, 2])\n\n\n\nx[1].shape\n\ntorch.Size([10, 8])\n\n\nThe following line takes the list of tensors x (with a 10 x 2 and 10 x 8 tensor) and concatenates them into a single 10 x 10 tensor.\nx = torch.cat(x, 1)\n\nx = torch.cat(x,1)\n\n\nx.shape\n\ntorch.Size([10, 10])\n\n\n\nx\n\ntensor([[ 1.3314e-03,  8.6398e-03,  2.0744e-04,  2.5392e-03,  9.3644e-03,\n          7.1224e-03, -3.1766e-04,  1.0164e-03,  1.3433e-02,  7.1327e-03],\n        [-1.0157e-02, -8.8875e-03, -1.5988e-02, -1.0913e-03,  7.1520e-03,\n          3.9139e-04,  1.3059e-02,  2.4659e-03, -1.9776e-02,  1.7896e-04],\n        [-4.9903e-04,  5.2634e-03,  3.7818e-03,  7.0511e-03, -1.7237e-02,\n         -8.4348e-03,  4.3514e-03,  2.6589e-03, -5.8710e-03,  8.2689e-04],\n        [ 1.3314e-03,  8.6398e-03, -1.5988e-02, -1.0913e-03,  7.1520e-03,\n          3.9139e-04,  1.3059e-02,  2.4659e-03, -1.9776e-02,  1.7896e-04],\n        [ 1.3314e-03,  8.6398e-03, -7.1988e-04, -9.0609e-03, -4.8712e-04,\n         -1.0811e-02,  1.7623e-04,  7.8226e-04,  1.9316e-03,  4.0967e-03],\n        [-1.0157e-02, -8.8875e-03,  1.2554e-02, -7.1496e-03,  8.5392e-03,\n          5.1299e-03,  5.3973e-03,  5.6551e-03,  5.0579e-03,  2.2245e-03],\n        [-4.9903e-04,  5.2634e-03, -6.8548e-03,  5.6356e-03, -1.5072e-02,\n         -1.6107e-02, -1.4790e-02,  4.3227e-03, -1.2503e-03,  7.8212e-03],\n        [-4.9903e-04,  5.2634e-03,  4.0380e-03, -7.1398e-03,  8.3373e-03,\n         -9.5855e-03,  4.5363e-03,  1.2461e-02, -3.0651e-03, -1.2869e-02],\n        [ 1.3314e-03,  8.6398e-03,  1.2554e-02, -7.1496e-03,  8.5392e-03,\n          5.1299e-03,  5.3973e-03,  5.6551e-03,  5.0579e-03,  2.2245e-03],\n        [-8.4988e-05,  7.2906e-03, -6.8548e-03,  5.6356e-03, -1.5072e-02,\n         -1.6107e-02, -1.4790e-02,  4.3227e-03, -1.2503e-03,  7.8212e-03]],\n       grad_fn=<CatBackward0>)\n\n\nThe following line of code passes the x tensor through the nn.Dropout function. If I understand correctly, since I defined embed_p as 0, passing it through the Dropout layer will not affect the tensor x.\nx = self.emb_drop(x)\n\nx = emb_drop(x)\n\n\n\nif self.n_cont != 0\nIn this example, n_cont is not 0 so the following code will run.\nSince bn_cont is None, the code x_cont = self.bn_cont(x_cont) will not run.\nif self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n\nif bn_cont is not None: x_cont = bn_cont(x_cont)\n\nIn the following line, if n_emb is not 0, it will concatenate x (which holds the outputs of the categorical Embeddings) with x_cont (which holds the continuous variable columns) into a single tensor. If n_emb is 0, it will assign x_cont to x.\nIn this example, n_emb is not 0 so it will concatenate x with x_cont.\nx = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n\nx.shape\n\ntorch.Size([10, 10])\n\n\n\nx = torch.cat([x, x_cont], 1) if n_emb != 0 else x_cont\n\nThe concatenation has added a column of tensors (continuous variable) to x:\n\nx.shape\n\ntorch.Size([10, 11])\n\n\n\n\nreturn\nThe final piece of the forward method is to return the outputs of the model. This is done by passing our 11 inputs in x (10 categorical embeddings, 1 continuous variable) to the layers nn.Sequential model defined before.\nself.layers(x)\n\nlayers\n\nSequential(\n  (0): LinBnDrop(\n    (0): Linear(in_features=11, out_features=200, bias=True)\n    (1): ReLU(inplace=True)\n  )\n  (1): LinBnDrop(\n    (0): Linear(in_features=200, out_features=100, bias=True)\n    (1): ReLU(inplace=True)\n  )\n  (2): LinBnDrop(\n    (0): Linear(in_features=100, out_features=1, bias=True)\n  )\n  (3): fastai.layers.SigmoidRange(low=0, high=1)\n)\n\n\nThe output is a tensor with 10 values, 1 for each of the 10 input rows.\n\nlayers(x)\n\ntensor([[0.5217],\n        [0.5210],\n        [0.5216],\n        [0.5214],\n        [0.5149],\n        [0.5192],\n        [0.5377],\n        [0.5174],\n        [0.5223],\n        [0.5221]], grad_fn=<AddBackward0>)"
  },
  {
    "objectID": "posts/2023-10-10-TabularModel/index.html#final-thoughts",
    "href": "posts/2023-10-10-TabularModel/index.html#final-thoughts",
    "title": "Understanding the fastai TabularModel Class",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI really enjoyed this exercise and will definitely apply the same process of running line-by-line code in the future when I am trying to understand fastai (or other) library source code. By the end of this exercise, I was surprised at how simple and straightforward it is to build a TabularModel object. It’s so powerful given its simplicity.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-05-21-label-smoothing-cross-entropy/index.html",
    "href": "posts/2024-05-21-label-smoothing-cross-entropy/index.html",
    "title": "Understanding the Code in fastai’s LabelSmoothingCrossEntropy",
    "section": "",
    "text": "In this blog post I’ll walk through fastai’s LabelSmoothingCrossEntropy function line-by-line and compare it to the helpful Excel example and explanation presented by Aman Arora in his Label Smoothing Explained using Microsoft Excel blog post. This process helped me better visualize how something in Excel (which is visually intuitive for beginners) translates to PyTorch (not always intuitive for beginners)."
  },
  {
    "objectID": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#excel-version",
    "href": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#excel-version",
    "title": "Understanding the Code in fastai’s LabelSmoothingCrossEntropy",
    "section": "Excel Version",
    "text": "Excel Version\nI’ll start be recreating Aman’s Excel example with the following columns:\n\nimage_name: example name of training data\nis_cat: ground truth noisy label\nis_dog: ground truth noisy label\nlogit (cat): model output (activation) for cat class\nlogit (dog): model output (activation) for dog class\nexp (cat): exponential of the cat logit\nexp (dog): exponential of the dog logit\nsum (exp): sum of cat and dog exponential for each image\nprob (cat): exponential of cat divided by sum of exponential o fdog and exponential of cat\nprob (dog): exponential of dog divided by sum of exponential o fdog and exponential of cat\nLS X-entropy: the negative sum of the ground truth noisy label times the natural log of the class probability (for both dog and cat). The screenshot below shows how this value is calculated in Excel."
  },
  {
    "objectID": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#fastais-labelsmoothingcrossentropy",
    "href": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#fastais-labelsmoothingcrossentropy",
    "title": "Understanding the Code in fastai’s LabelSmoothingCrossEntropy",
    "section": "fastai’s LabelSmoothingCrossEntropy",
    "text": "fastai’s LabelSmoothingCrossEntropy\n\nfrom fastai.vision.all import *\n\nHere is the forward method of fastai’s LabelSmoothingCrossEntropy class.\ndef forward(self, output:Tensor, target:Tensor) -> Tensor:\n        \"Apply `F.log_softmax` on output then blend the loss/num_classes(`c`) with the `F.nll_loss`\"\n        c = output.size()[1]\n        log_preds = F.log_softmax(output, dim=1)\n        if self.reduction=='sum': loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=1) #We divide by that size at the return line so sum and not mean\n            if self.reduction=='mean':  loss = loss.mean()\n        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\nI’ll start by defining the output and target tensors. I’ll also define the noisy target defined in the Excel spreadsheet (is_cat and is_dog).\n\n# logits\noutput = torch.tensor([\n    [4.2, -2.4],\n    [1.6, -0.6],\n    [3.6, 1.2],\n    [-0.5, 0.5],\n    [-0.25, 1.7]\n])\n\n# labels\ntarget = torch.tensor([0,1,1,0,0])\n\n# noisy labels\nnoisy_target = torch.tensor([\n    [0.95, 0.05],\n    [0.05, 0.95],\n    [0.05, 0.95],\n    [0.95, 0.05],\n    [0.95, 0.05]])\n\nFirst let’s calculate the loss with fastai to show that it matches the Excel calculations:\n\nLabelSmoothingCrossEntropy(eps=0.1, reduction='none')(output,target)\n\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\n\n\nNote the eps parameter which is \\(\\epsilon\\) in Aman’s blog post. I understand this to be the total “noisiness” divided across the classes. In our case, this value is 0.1.\nNext, I’ll run through the lines of code in LabelSmoothingCrossEntropy’s forward method if reduction='none' (which is the case for our Excel example), and show that it outputs the same values as Excel.\n\neps=0.1\nc = output.size()[1]\nlog_preds = F.log_softmax(output, dim=1)\nloss = -log_preds.sum(dim=1)\nloss*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), reduction='none')\n\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\n\n\nHere, c is the number of classes (2)."
  },
  {
    "objectID": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#recreating-excel-calculation-in-pytorch",
    "href": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#recreating-excel-calculation-in-pytorch",
    "title": "Understanding the Code in fastai’s LabelSmoothingCrossEntropy",
    "section": "Recreating Excel Calculation in PyTorch",
    "text": "Recreating Excel Calculation in PyTorch\nI found it a bit more intuitive to recreate the Excel calculation in PyTorch in a slightly different order of operations.\nIn Excel, we take the softmax of the logits to get the probability of cat and dog (highlighted in the screenshot below).\n\nIn PyTorch, we can recreate those values with F.softmax. dim=-1 tells it to take the softmax across the last dimension (of 2 classes).\n\nF.softmax(output, dim=-1)\n\ntensor([[0.9986, 0.0014],\n        [0.9002, 0.0998],\n        [0.9168, 0.0832],\n        [0.2689, 0.7311],\n        [0.1246, 0.8754]])\n\n\nNext, to calculate cross entropy, we multiply the noisy label with the log probability, sum across classes and multiply by negative 1:\n\nIn PyTorch, we do that by multiplying noisy_targets by the torch.log probabilities (F.softmax), summing across each row (dim=-1) and multiplying by negative 1.\n\n-1 * (noisy_target * torch.log(F.softmax(output, dim=-1))).sum(dim=-1)\n\ntensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])\n\n\nThis gives us the desired result. Although this looks different from the fastai implementation."
  },
  {
    "objectID": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#bringing-it-all-together",
    "href": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#bringing-it-all-together",
    "title": "Understanding the Code in fastai’s LabelSmoothingCrossEntropy",
    "section": "Bringing it All Together",
    "text": "Bringing it All Together\nThe Excel calculation that I recreated in PyTorch and the fastai implementation look different but achieve the same result. I’ll try to connect and reason through the two approaches.\nThe first two lines of interest in LabelSmoothingCrossEntropy are straightforward—they define constants used later on.\n\neps=0.1\nc = output.size()[1]\neps, c\n\n(0.1, 2)\n\n\nIn the next line, log_preds is defined as:\n\nlog_preds = F.log_softmax(output, dim=1)\nlog_preds\n\ntensor([[-1.3595e-03, -6.6014e+00],\n        [-1.0508e-01, -2.3051e+00],\n        [-8.6836e-02, -2.4868e+00],\n        [-1.3133e+00, -3.1326e-01],\n        [-2.0830e+00, -1.3302e-01]])\n\n\nIn Excel, we fold this step into the following formula (multiplying the noisy labels with the log probabilities and summing both classes):\n\nlog_preds is just the LN(I2) and LN(J2) parts in the Excel formula for each image (or row).\nThe next line in LabelSmoothingCrossEntropy sums the log probabilities across each row (or image) and multiplies the sum by negative 1.\nIn Excel, this is would be the same as the part of the formula with the noisy labels removed: =-LN(I2)-LN(J2).\n\n\nloss = -log_preds.sum(dim=1)\nloss\n\ntensor([6.6027, 2.4102, 2.5737, 1.6265, 2.2160])\n\n\nThe last part is where the noisy label magic happens in PyTorch.\nloss*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), reduction='none')\nIn the first term, loss*eps/c, the log probabilities summed across both classes for each image is multiplied by 0.1/2 or 0.05:\n\nloss*eps/c\n\ntensor([0.3301, 0.1205, 0.1287, 0.0813, 0.1108])\n\n\nThe second term, (1-eps) * F.nll_loss(log_preds, target.long(), reduction='none') does a couple of things:\nFirst, it calculates the negative log likelihood loss given the log probabilities (log_preds) and the targets. Note that all nll_loss does is pick out the log_preds items at the target indices for each row:\n\nF.nll_loss(log_preds, target.long(), reduction='none')\n\ntensor([1.3595e-03, 2.3051e+00, 2.4868e+00, 1.3133e+00, 2.0830e+00])\n\n\nSince reduction is 'none', this is the same as just indexing each row with our target tensor and multiplying by -1:\n\n-1 * log_preds[[0, 1, 2, 3, 4], target]\n\ntensor([1.3595e-03, 2.3051e+00, 2.4868e+00, 1.3133e+00, 2.0830e+00])\n\n\n\nlog_preds # reminder of what log_preds looks like\n\ntensor([[-1.3595e-03, -6.6014e+00],\n        [-1.0508e-01, -2.3051e+00],\n        [-8.6836e-02, -2.4868e+00],\n        [-1.3133e+00, -3.1326e-01],\n        [-2.0830e+00, -1.3302e-01]])\n\n\n\ntarget # reminder of what target looks like\n\ntensor([0, 1, 1, 0, 0])\n\n\nSo, basically, nll_loss with reduction='none' takes the 0-th element of the first row (-1.3595e-03), the 1-th element in the second row (-2.3051e+00) and so on. nll_loss picks only the chosen label’s probabilities, whereas loss is the sum of both class’ probabilities.\nThe chosen probabilities are then multiplied by 1-eps or 0.90.\nLet’s visualize what that last line in LabelSmoothCrossEntropy is doing, row by row, given the log_preds values. I’ve rewritten loss as -log_preds.sum(dim=1).\n(-log_preds.sum(dim=1))*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), reduction='none')\n\n\n\n\n\n\n\n\nrow\n-log_preds.sum(dim=1)*eps/c\n(1-eps) * F.nll_loss(log_preds, target.long(), reduction=‘none’)\n\n\n\n\n1\n-(-1.3595e-03 + -6.6014e+00) * 0.05\n0.90 * 1.3595e-03\n\n\n2\n-(-1.0508e-01 + -2.3051e+00) * 0.05\n0.90 * 2.3051e+00\n\n\n3\n-(-8.6836e-02 + -2.4868e+00) * 0.05\n0.90 * 2.4868e+00\n\n\n4\n-(-1.3133e+00, -3.1326e-01) * 0.05\n0.90 * 1.3133e+00\n\n\n5\n-(-2.0830e+00, -1.3302e-01) * 0.05\n0.90 * 2.0830e+00\n\n\n\nIn each row you’ll notice that the target log probability is multiplied first by 0.05 (which is eps/c) and then multiplied by 0.90 (which is 1-eps) and then added together. We can rewrite this as follows (adding together 0.05 and 0.90 to get 0.95 for the target class)\n\n\n\n\n\n\n\nrow\n-log_preds.sum(dim=1)*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), reduction=‘none’)\n\n\n\n\n1\n0.05 * 6.6014e+00 + 0.95 * 1.3595e-03\n\n\n2\n0.05 * 1.0508e-01 + 0.95 * 2.3051e+00\n\n\n3\n0.05 * 8.6836e-02 + 0.95 * 2.4868e+00\n\n\n4\n0.05 * 3.1326e-01 + 0.95 * 1.3133e+00\n\n\n5\n0.05 * 1.3302e-01 + 0.95 * 2.0830e+00\n\n\n\nI’ll expand the Excel version a bit more to match this form so we can see the parallels:\n\nIn this way, the fastai implementation, Aman Arora’s Excel implementation and my PyTorch implementation are visualized and aligned."
  },
  {
    "objectID": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#final-thoughts",
    "href": "posts/2024-05-21-label-smoothing-cross-entropy/index.html#final-thoughts",
    "title": "Understanding the Code in fastai’s LabelSmoothingCrossEntropy",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI often underestimate how much time and thinking it takes to unpack the amount of calculation done in a few lines of code. That’s the beauty and elegance of fastai and PyTorch! But it also emphasizes the time and care needed to walk through each step manually to visualize what is going on.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "",
    "text": "In this blog post, I have written excerpts from the book Visualization Analysis & Design by Tamara Munzner."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#structure-whats-in-this-book",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#structure-whats-in-this-book",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "Structure: What’s in This Book",
    "text": "Structure: What’s in This Book\n\nChapter 1: high-level introduction to an analysis framework of breaking down vis design to what-why-how questions that have data-task-idiom answers\nChapter 2: addresses the what question with answers about data abstraction\nChapter 3: addresses the why question with task abstractions\nChapter 4: extends the analysis framework to two additional levels: the domain situation level on top and the algorithm level on the bottom\nChapter 5: the principles of marks and channels for encoding information\nChapter 6: eight rules of thumb for design\nChapter 7: how to visually encode data by arranging space for tables\nChapter 8: for spatial data\nChapter 9: for networks\nChapter 10: choices for mapping color and other channels in visual encoding\nChapter 11: ways to manipulate and change a view\nChapter 12: ways to facet data between multiple views\nChapter 13: how to reduce the amount of data shown in each view\nChapter 14: embedding information about a focus set within the context of overview data\nChapter 15: six case studies\n\nAccompanying web page"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-human-in-the-loop",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-human-in-the-loop",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.2 Why Have a Human in the Loop?",
    "text": "1.2 Why Have a Human in the Loop?\nVis allows people to analyze data when they don’t know exactly what questions they need to ask in advance.\nIf a fully automatic solution has been deemed to be acceptable, then there is no need for human judgment, and thus no need for you to design a vis tool.\nThe outcome of designing vis tools targeted at specific real-world domain problems is often a much crisper understanding of the user’s task, in addition to the tool itself."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-computer-in-the-loop",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-computer-in-the-loop",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.3 Why Have a Computer in the Loop?",
    "text": "1.3 Why Have a Computer in the Loop?\nBy enlisting computation, you can build tools that allow people to explore or present large datasets that would be completely unfeasible to draw by hand, thus opening up the possibility of seeing how datasets change over time.\nAs a designer, you can think about what aspects of hand-drawn diagrams are important in order to automatically create drawings that retain the hand-drawn spirit."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-an-external-representation",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-an-external-representation",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.4 Why Use an External Representation?",
    "text": "1.4 Why Use an External Representation?\nVis allows people to offload internal cognition and memory usage to the perceptual system, using carefully designed images as a form of external representations, sometimes called external memory."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-depend-on-vision",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-depend-on-vision",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.5 Why Depend on Vision?",
    "text": "1.5 Why Depend on Vision?\nThe visual system provides a very high-bandwidth channel to our brains. A significant amount of visual information processing occurs in parallel at the preconscious level.\nSound is poorly suited for providing overviews of large information spaces compared with vision. We experience the perceptual channel of sound as a sequential stream, rather than as a simultaneous experience where what we hear over a long period of time is automatically merged together."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-show-the-data-in-detail",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-show-the-data-in-detail",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.6 Why Show the Data in Detail?",
    "text": "1.6 Why Show the Data in Detail?\nStatistical characterization of datasets is a very powerful approach but it has the intrinsic limitation of losing information through summarization.\nAnscombe’s Quartet illustrates how datasets that have identical descriptive statistics can have very different structures that are immediately obvious when the dataset is shown graphically."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-interactivity",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-interactivity",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.7 Why Use Interactivity?",
    "text": "1.7 Why Use Interactivity?\nWhen datasets are large enough, the limitations of both people and display preclude just showing everything at once."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-the-vis-idiom-design-space-huge",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-the-vis-idiom-design-space-huge",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.8 Why is the Vis Idiom Design Space Huge?",
    "text": "1.8 Why is the Vis Idiom Design Space Huge?\nidiom: a distinct approach to creating and manipulating visual representations."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-tasks",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-tasks",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.9 Why Focus on Tasks?",
    "text": "1.9 Why Focus on Tasks?\nA tool that serves well for one task can be poorly suited for another, for exactly the same dataset.\nReframing the users’ task from domain-specific form into abstract form allows you to consider the similarities and differences between what people need across many real-world usage contexts."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-effectiveness",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-effectiveness",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.10 Why Focus on Effectiveness?",
    "text": "1.10 Why Focus on Effectiveness?\nThe goals of the designer are not met if the result is beautiful but not effective.\nAny depiction of data is an abstraction where choices are made about which aspects to emphasize."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-most-designs-ineffective",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-most-designs-ineffective",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.11 Why Are Most Designs Ineffective?",
    "text": "1.11 Why Are Most Designs Ineffective?\nThe vast majority of the possibilities in the design space will be ineffective for any specific usage context.\nIn addressing design problems, it’s not a very useful goal to optimize or find the very best choice. A more appropriate goal when you design is to satisfy or find one of the many possible good solutions rather than one of the even larger number of bad ones.\nProgressively smaller search spaces:\n\nSpace of possible solutions\nSpace of solutions known to the designer\nSpace of solutions you actively consider\nSpace of solutions you investigate in detail\nSelected solution\n\nThe problem of a small consideration space is the higher probability of only considering OK or poor solutions and missing a good one.\nOne way to ensure that more than one possibility is considered is to explicitly generate multiple ideas in parallel."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-validation-difficult",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-validation-difficult",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.12 Why Is Validation Difficult?",
    "text": "1.12 Why Is Validation Difficult?\nHow do you know it works? How do you argue that one design is better or worse than another for the intended users? What does better mean? Do users get something done faster? Do they have more fun doing it? Can they work more effectively? What does effectively mean? How do you measure insight or engagement? What is the design better than? Is it better than another vis system? Is it better than doing the same things manually, without visual support? Is it better than doing the same things completely automatically? And what sort of thing does it do better? How do you decide what sort of task the users should do when testing the system? And who is the user? An expert who has done this task for decades, or a novice who needs the task to be explained before they begin? Are they familiar with how the system works from using it for a long time, or are they seeing it for the first time? Are the users limited by the speed of their own thought process, or their ability to move the mouse, or simply the speed of the computer in drawing each picture?\nHow do you decide what sort of benchmark data you should use when testing the system? Can you characterize what classes of data the system is suitable for? How might you measure the quality of an image generated by a vis tool? How well do any of the automatically computed quantitative metrics of quality match up with human judgments? Does the complexity of the algorithm depend on the number of data items to show or the number of pixels to draw? Is there a trade-off between computer speed and computer memory usage?"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-there-resource-limitations",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-there-resource-limitations",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.13 Why Are There Resource Limitations?",
    "text": "1.13 Why Are There Resource Limitations?\nThree different kinds of limitations:\n\nComputational capacity\nHuman perceptual and cognitive capacity\nDisplay capacity\n\nscalability: design systems to handle large amounts of data gracefully.\nDesigning systems that gracefully handle larger datasets that do not fit into core memory requires significantly more complex algorithms.\nHuman memory for things that are not directly visible is notoriously limited.\nchange blindness: when even very large changes are not noticed if we are attending to something else in our view.\ninformation density: a measure of the amount of information encoded versus the amount of unused space.\nThere is a trade-off between the benefits of showing as much as possible at once (to minimize the need for navigation and exploration) and the costs of showing too much at once (where the user is overwhelmed by visual clutter)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.14 Why Analyze?",
    "text": "1.14 Why Analyze?\nAnalyzing existing systems is a good stepping stone to designing new ones.\nHigh-level framework for analyzing vis use according to three questions:\n\nwhat data the user sees (data)\nwhy the user intends to use a vis tool (task)\nhow the visual encoding and interaction idioms are constructed in terms of design choices (idiom)\n\none of these analysis trios is called an instance.\nComplex vis tool usage often requires analysis in terms of a sequence of instances that are chained together. (sort > finding outliers)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.1 The Big Picture",
    "text": "2.1 The Big Picture\n\nWhat?\n\nDatasets\n\nData Types\n\nItems\nAttributes\nLinks\nPositions\nGrids\n\nData and Dataset Types\n\nTables\nNetworks & Trees\nFields\nGeometry\nClusters, Sets, Lists\n\nDataset Availability\n\nStatic\nDynamic\n\n\nAttributes\n\nAttribute Types\n\nCategorical\nOrdered\n\nOrdinal\nQuantitative\n\n\nOrdering Direction\n\nSequential\nDiverging\nCyclic"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-do-data-semantics-and-types-matter",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-do-data-semantics-and-types-matter",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.2 Why Do Data Semantics and Types Matter?",
    "text": "2.2 Why Do Data Semantics and Types Matter?\nMany aspects of vis design are driven by the kind of data that you have at your disposal.\nsemantics: the real-world meaning of the data.\ntype: the structural or mathematical interpretation of the data."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#data-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#data-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.3 Data Types",
    "text": "2.3 Data Types\nFive basic data types discussed in this book:\n\nItems\n\nIndividual entity that is discrete (such as a row in a simple table or a node in a network)\n\nAttributes\n\nSome specific property that can be measured, observed, or logged\n\nLinks\n\nA relationship between items, typically within a network\n\nPositions\n\nspatial data\n\nGrids"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#dataset-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#dataset-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.4 Dataset Types",
    "text": "2.4 Dataset Types\ndataset: any collection of information that is the target of analysis\nDataset types:\n\ntables\n\nitems\nattributes\n\nnetworks\n\nitems (nodes)\nlinks\nattributes\n\nfields\n\ngrids\npositions\nattributes\n\ngeometry\n\nitems\npositions\n\nclusters, sets and lists\n\nitems\n\n\n\n2.4.1 Tables\nflat table: each row represents and item of data, each column is an attribute of the dataset\ncell: fully specific by the combination of a row and a column (item and attribute) and contains a value for that pair.\nmultidimensional table: more complex structure for indexing into a cell, with multiple keys\n\n\n2.4.2 Networks and Trees\nnetworks: well suited for specifying that there is some kind of relationship (link) between two or more items (nodes)\nA synonym for networks is graphs.\nA synonym for node is vertex.\nA synonym for link is edge.\n\n2.4.2.1 Trees\nTrees: networks with hierarchical structure. Each child node has only one parent node pointing to it.\n\n\n\n2.4.3 Fields\nContains attribute values associated with cells. Each cell in a field contains measurements or calculations from a continuous domain.\nsampling: how frequently to take the measurements (of continuous data).\ninterpolation: how to show values in between the sampled points in a way that does not mislead. Interpolating appropriately between the measurements allows you to reconstruct a new view of the data from an arbitrary viewpoint that’s faithful to what you measured.\ndiscrete: data where a finite number of individual items exist where interpolation between them is not a meaningful concept.\nTechnically all data stored within a computer is discrete rather than continuous; however, the interesting question is whether the underlying semantics of the bits that are stored represents samples of a continuous phenomenon or intrinsically discrete data.\n\n2.4.3.1 Spatial Fields\nCell structure of the field is based on sapling at spatial positions.\nA synonym for nonspatial data is abstract data.\nscientific visualization (scivis): concerned with situations where spatial position is given with the dataset. A central concern in scivis is handling continuous data appropriately within the mathematical framework of signal processing.\ninformation visualization (infovis): concerned with situations where the use of space in a visual encoding is chosen by the designer. A central concern of infovis is determining whether the chosen idiom is suitable for the combination of data and task, leading to the use of methods from human-computer interaction and design.\n\n\n2.4.3.2 Grid Types\nWhen a field contains data created by sampling at completely regular intervals, the calls form a uniform grid.\ngrid geometry: location in space.\ngrid topology: how each cell connects with its neighboring cells.\nrectilinear grid: supports nonuniform sampling, allowing efficient storage of information that has high complexity in some areas and low complexity in others, at the cost of storing some information about the geometric location of each row.\nstructured grid: allows curvilinear shapes, where the geometric location of each cell needs to be specified.\nunstructured grid: provides complete flexibility, but the topological information about how cells connect to each other must be stored explicitly in addition to their spatial positions.\n\n\n\n2.4.4 Geometry\nSpecifies information about the shape of items with explicit spatial positions. Geometry datasets do not necessarily have attributes.\nGeometric data is sometimes shown alone, particularly when shape understanding is the primary task. In other cases, it is the backdrop against which additional information is overlaid.\n\n\n2.4.5 Other Combinations\nset: unordered group of items.\nlist: a group of items with a specified ordering.\ncluster: a grouping based on attribute similarity.\npath: an ordered set of segments formed by links connecting nodes.\ncompound network: a network with an associated tree (all the nodes in the network are the leaves of the tree, and interior nodes in the tree provide a hierarchical structure for the nodes that is different from network links between them).\ndata abstraction: describing the what part of an analysis instance that pertains to data.\n\n\n2.4.6 Dataset Availability\nstatic file (offline): the entire dataset is available all at once.\ndynamic streams (online): the dataset information trickles in over the course of the vis session."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#attribute-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#attribute-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.5 Attribute Types",
    "text": "2.5 Attribute Types\nThe major distinction is between categorical versus ordered.\nOrdered type contains further differentiation between ordinal versus quantitative.\nOrdered data might range sequentially from a minimum to a maximum value, or it might diverge in both directions from a zero point in the middle of a range, or the values may wrap around in a cycle.\nAttributes may have a hierarchical structure.\n\n2.5.1 Categorical\nDoes not have implicit ordering, but if often has hierarchical structure.\nA synonym for categorical is nominal.\nAny arbitrary external ordering can be imposed upon categorical data but these orderings are not implicit in the attribute itself.\n\n\n2.5.2 Ordered: Ordinal and Quantitative\nordered data: does have an implicit ordering.\nordinal data: we cannot do full-fledged arithmetic with, but there is a well defined ordering (shirt sizes, rankings).\nquantitative data: a subset of ordered data. A measurement of magnitude that supports arithmetic comparison (height, weight, temperature, stock price, etc). Both integers and real numbers are quantitative data.\n\n2.5.2.1 Sequential versus Diverging\nsequential: a homogeneous range from a minimum to a maximum value.\ndiverging: two sequences pointing in opposite directions that meet at a common zero point.\n\n\n2.5.2.2 Cyclic\ncyclic: where the values wrap around back to a starting point rather than continuing to increase indefinitely.\n\n\n\n2.5.3 Hierarchical Attributes\nThe attribute of time can be aggregated hierarchically from days up to weeks, months and years.\nThe geographic attribute of a postal code can be aggregated up to the level of cities or states or entire countries."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#semantics",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#semantics",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.6 Semantics",
    "text": "2.6 Semantics\nKnowing the type of an attribute does not tell us about its semantics.\n\n2.6.1 Key versus Value Semantics\nkey attribute: acts as an index that is used to look up value attributes.\nA synonym for key attribute is independent attribute or dimension.\nA synonym for value attribute is dependent attribute or measure.\n\n2.6.1.1 Flat Tables\nflat table: has only one key, where each item corresponds to a row in the table and any number of value attributes. Key may be categorical or ordinal attributes but quantitative attributes are typically unsuitable as keys because there is nothing to prevent them from having the same values for multiple items.\n\n\n2.6.1.2 Multidimensional Tables\nwhere multiple keys are required to look up an item. The combination of all keys must be unique for each item, even though an individual key attribute may contain duplicates.\n\n\n2.6.1.3 Fields\nIn spatial fields, spatial position acts as a quantitative key.\nmultivariate structure of fields depends on the number of value attributes.\nmultidimensional structure of fields depends on the number of keys.\na scalar field has one attribute per cell.\na vector field has two or more attributes per cell.\na tensor field has many attributes per cell.\n\n\n2.6.1.4 Scalar Fields\nare univariate, with a single attribute at each point in space.\n\n\n2.6.1.5 Vector Fields\nare multivariate with a list of multiple attributes at each point. The dimensionality of the field determines the number of components in the direction vector.\n\n\n2.6.1.6 Tensor Fields\nhave an array of attributes at each point, representing a more complex multivariate mathematical structure than the list of numbers in a vector. The full information at each point in a tensor field cannot be represented by just an arrow and would require a more complex shape such as an ellipsoid.\n\n\n2.6.1.7 Field Semantics\nCategorization of spatial fields requires knowledge of the attribute semantics and cannot be determined from type information alone.\n\n\n\n2.6.2 Temporal Semantics\ntemporal attribute: any kind of information that relates to time.\nTemporal analysis tasks often involve finding or verifying periodicity either at a predetermined scale or at some scale not known in advance.\nA temporal key attribute is usually considered to have a quantitative type, although it’s possible to consider it as ordinal data if the duration between events is not interesting.\n\n2.6.2.1 Time-Varying Data\nwhen time is one of the key attributes, as opposed to when the temporal attribute is a value rather than a key.\nThe question of whether time has key or value semantics requires external knowledge about the nature of the dataset and cannot be made purely from type information.\ntime-series dataset: an ordered sequence of time-value pairs. A special case of tables where time is the key.\ndynamic can mean a dataset has time-varying semantics or a dataset has stream type."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-1",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-1",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.1 The Big Picture",
    "text": "3.1 The Big Picture\n\nDiscovery may involve generating or verifying a hypothesis\nSearch can be classified according to whether the identity and location of targets are known or not\n\nboth are known with lookup\nthe target is known but its location is not for locate\nthe location is known but the target is not for browse\nneither the target nor the location are known for explore\n\nQueries can have three scopes:\n\nidentify one target\ncompare some targets\nsummarize all targets\n\nTargets for all kinds of data are finding trends and outliers\nFor one attribute, the target can be:\n\none value,\nthe extremes of minimum and maximum values or\nthe distribution of all values across the entire attribute\n\nFor multiple attributes the target can be:\n\ndependencies\ncorrelations or\nsimilarities between them"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze-tasks-abstractly",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze-tasks-abstractly",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.2 Why Analyze Tasks Abstractly?",
    "text": "3.2 Why Analyze Tasks Abstractly?\nTransforming task descriptions from domain-specific language into abstract form allows you to reason about similarities and differences between them.\nIf you don’t to this kind of translation then everything just appears to be different. The apparent difference is misleading: there are lots of similarities in what people want to do once you strip away the surface language differences.\nThe analysis framework has verbs describing actions and nouns describing targets.\nIt is often useful to consider only one of the user’s goals at a time, in order to more easily consider the question of how a particular idiom supports that goal. To describe complex activities, you can specify a chained sequence of tasks, where the output of one becomes the input to the next.\nTask abstraction can and should guide the data abstraction.\n\n3.3 Who: Designer or User\nOn the specific side, tools are narrow: the designer has built many choices into the design of the tool itself in a way that the user cannot override.\nOn the general side, tools are flexible and users have many choices to make.\nSpecialized vis tools are designed for specific contexts with a narrow range of data configurations, especially those created through a problem-driven process.\n\n\n3.4 Actions\nThree levels of actions that define user goals:\n\nhow the vis is being used to analyze (consume or produce data)\nwhat kind of search is involved (whether target and location are known)\nwhat kind query (identify one target, compare targets, or summarize all targets)\n\n\n3.4.1 Analyze\nTwo possible goals of people who want to analyze data: consume or actively produce new information.\n\nConsume information that has already been generated as data stored in a format amenable to computation\n\nDiscover something new\nPresent something that the user already understands\nEnjoy a vis to indulge their casual interests in a topic\n\n\n\n\n3.4.1.1 Discover\nUsing a vis to find new knowledge that was not previously known, by the serendipitous observation of unexpected phenomena or motivated by existing theories, models, hypotheses or hunches.\ngenerate a new hypothesis: finding completely new things\nverify or disconfirm an existing hypothesis.\nThe discover goal is often discussed as the classic motivation for sophisticated interactive idioms, because the vis designer doesn’t know in advance what the user will need to see.\n(discover = explore, present = explain).\nWhy the vis is being used doesn’t dictate how the vis idiom is designed to achieve those goals.\n\n\n3.4.1.2 Present\nThe use of vis for the succinct communication of information, for telling a story with data, or guiding an audience through a series of cognitive operations.\nThe crucial point about the present goal is that vis is being used by somebody to communicate something specific and already understood to an audience. The knowledge communicated is already known to the presenter in advance. The output of a discover session becomes the input to a present session.\nThe decision about why is separable from how the idiom is designed: presentation can be supported through a wide variety of idiom design choices.\n\n\n3.4.1.3 Enjoy\nCasual encounters with vis.\nA vis tool may have been intended by the designer for the goal of discovery with a particular audience, but it might be used for pure enjoyment by a different group of people.\n\n\n\n3.4.2 Produce\nThe intent of the user is to generate new material.\nThere are three kinds of produce goals:\n\nannotate\nrecord\nderive\n\n\n3.4.2.1 Annotate\nthe addition of graphical or textual annotations associated with one or more preexisting visualization elements, typically as a manual action by the user. Annotation for data items could be thought of as a new attribute for them.\n\n\n3.4.2.2 Record\nSaves or captures visualization elements as persistent artifacts (screenshots, lists of bookmarked elements or locations, parameter settings, interaction logs, or annotations). An annotation made by a user can subsequently be recorded.\n\n\n3.4.2.3 Derive\nProduce new data elements based on existing data elements. There is a strong relationship between the form of the data (the attribute and dataset types) and what kinds of vis idioms are effective at displaying it.\nDon’t just draw what you’re given; decide what the right thing to show is, create it with a series of transformations from the original dataset, and draw that.\nA synonym for derive is transform.\nderived attributes extend the dataset beyond the original set of attributes that it contains.\n\n\n\n3.4.3 Search\nThe classification of search into four alternatives is broken down according to whether the identity and location of the search target is already known. The verb find is often used as a synonym in descriptions of search tasks, implying a successful outcome.\n\n3.4.3.1 Lookup\nUsers already know both what they’re looking for and where it is.\n\n\n3.4.3.2 Locate\nTo find a known target at an unknown location.\n\n\n3.4.3.3 Browse\nWhen users don’t know exactly what they’re looking for but they do have a location in mind of where to look for it.\n\n\n3.4.3.4 Explore\nWhen users don’t know what they’re looking for and are not even sure of the location.\n\n\n\n3.4.4 Query\nOnce a target or set of targets for a search has been found, a low-level user goal is to query these targets at one of three scopes: identify (single target), compare (multiple targets) or summarize (all targets).\n\n3.4.4.1 Identify\nIf a search returns known targets either by lookup or locate then identify returns their characteristics.\nIf a search returns targets matching particular characteristics either by browse or explore, then identify returns specific references.\n\n\n3.4.4.2\nComparison tasks are typically more difficult than identify tasks and require more sophisticated idioms to support the user.\n\n\n3.4.4.3 Summarize\nA synonym for summarize is overview: to provide a comprehensive view of everything (verb) and a summary display of everything (noun)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#targets",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#targets",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.5 Targets",
    "text": "3.5 Targets\nTarget: some aspect of the data that is of interest to the user.\nTargets are nouns whereas actions are verbs.\nThree high-level targets are very broadly relevant for all kinds of data:\n\na trend: high-level characterization of a pattern in the data. (A synonym for trend is pattern)\noutliers: data that don’t fit the trend, synonyms for outliers are anomalies, novelties, deviants and surprises.\nfeatures: definition dependent on the task, any particular structures of interest\n\nThe lowest-level target for an attribute is to find an individual value. Another target is to find the extremes (min/max across a range). Another target is the distribution of all values for an attribute.\nSome targets encompass the scope of multiple attributes:\n\ndependency: the values for the first attribute directly depend on those of the second.\ncorrelation: a tendency for the values of the second attribute to be tied to those of the first.\nsimilarity: a quantitative measurement calculated on all values of two attributes, allowing attributes to be ranked with respect to how similar, or different, they are from each other.\n\nNetwork targets:\n\ntopology: the structure of interconnections in a network.\npath: of one or more links that connects two nodes.\nshape: of spatial data."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#how-a-preview",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#how-a-preview",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.6 How: A Preview",
    "text": "3.6 How: A Preview\n\nEncode\n\nArrange\n\nExpress\nSeparate\nOrder\nAlign\nUse (spatial data)\n\nMap\n\nColor\nSize, Angle, Curvature, …\nShape\nMotion\n\n\nManipulate\n\nChange\nSelect\nNavigate\n\nFacet\n\nJuxtapose\nPartition\nSuperimpose\n\nReduce\n\nFilter\nAggregate\nEmbed\n\n\nThe rest of this book defines, describes and discusses these choices in depth.\nThe Strahler number is a measure of node importance. Very central nodes have large Strahler numbers, whereas peripheral nodes have low values."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-2",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-2",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.1 The Big Picture",
    "text": "4.1 The Big Picture\nFour nested levels of design:\n\nDomain situation\n\nTask and data abstraction\n\nVisual encoding and interaction idiom\n\nAlgorithm"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-validate",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-validate",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.2 Why Validate?",
    "text": "4.2 Why Validate?\nThe vis design space is huge, and most designs are ineffective."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#four-levels-of-design",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#four-levels-of-design",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.3 Four Levels of Design",
    "text": "4.3 Four Levels of Design\n\nDomain situation: where you consider the details of a particular application domain for vis\n\nWhy-why abstraction level (Data-task): where you map those domain-specific problems and data into forms that are independent of the domain\n\nHow level (visual encoding/interaction idiom): specify the approach to visual encoding and interaction\n\nAlgorithm level: instantiate idioms computationally\n\n\n\n\nThe four levels are nested, the output from an upstream level above is input to the downstream level below. A block is the outcome of the design process at that level. Choosing the wrong block at an upstream level inevitable cascades to all downstream levels.\nVis design is usually a highly iterative refinement process, where a better understanding of the blocks at one level will feed back and forward into refining the blocks at the other levels.\n\n4.3.1 Domain Situation\ndomain situation: a group of target users, their domain interest, their questions, and their data.\ndomain: a particular field of interest of the target users of a vis tool.\nSituation blocks are identified.\nThe outcome of the design process is an understanding that the designer reaches about the needs of the user. The outcome of identifying a situation block is a detailed set of questions asked about or actions carried out by the target users, about a possible heterogeneous collection of data that’s also understood in detail.\nMethods include: interviews, observations, or careful research about target users within a specific domain.\nWorking closely with a specific target audience to iteratively refine a design is called user-centered design or human-centered design.\nWhat users say they do when reflecting on their past behavior gives you an incomplete picture compared with what they actually do if you observe them.\n\n\n4.3.2 Task and Data Abstraction\nAbstracting into the domain-independent vocabulary allows you to realize how domain situation blocks that are described using very different language might have similar reasons why the user needs the vis tool and what data it shows.\nTask blocks are identified by the designer as being suitable for a particular domain situation block, just as the situation blocks themselves are identified at the level above.\nAbstract data blocks are designed.\nThe data abstraction level requires you to consider whether and how the same dataset provided by a user should be transformed into another form.\nYour goal is to determine which data type would support a visual representation of it that addresses the user’s problem.\nExplicitly considering the choices made in abstracting from domain-specific to generic tasks and data can be very useful in the vis design process.\n\n\n4.3.3 Visual Encoding and Interaction Idiom\nidiom: a distinct way to create and manipulate the visual representation of the abstract data block that you chose at the previous level, guided by the abstract tasks that you also identified at that level.\nthe visual encoding idiom controls exactly what users see.\nthe interaction idiom controls how users change what they see.\nIdiom blocks are designed.\nThe nested model emphasizes identifying task abstractions and deciding on data abstractions in the previous level exactly so that you can use them to rule out many of the options as being a bad match for the goals of the users. You should make decisions about good and bad matches based on understanding human abilities, especially in terms of visual perception and memory.\n\n\n4.3.4 Algorithm\nalgorithm: a detailed procedure that allows a computer to automatically carry out the desired goal.\nAlgorithm blocks are designed."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#angles-of-attack",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#angles-of-attack",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.4 Angles of Attack",
    "text": "4.4 Angles of Attack\nWith problem-driven work, you start at the top domain situation level and work your way down through abstraction, idiom, and algorithm decisions.\nIn technique-driven work, you work at one of the bottom two levels, idiom or algorithm design, where your goal is to invent new idioms that better support existing abstractions, or new algorithms that better support existing idioms."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#threats-the-validity",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#threats-the-validity",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.5 Threats the Validity",
    "text": "4.5 Threats the Validity\nthreats to validity: different fundamental reasons why you might have made the wrong choices.\n\nWrong problem: You (designer) misunderstood their (target users) needs.\nWrong abstraction: You’re showing them the wrong thing.\nWrong idiom: The way you show it doesn’t work.\nWrong algorithm: Your code is too slow."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#validation-approaches",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#validation-approaches",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.6 Validation Approaches",
    "text": "4.6 Validation Approaches\n\n4.6.1 Domain Validation\nThe primary threat is that the problem is mischaracterized; the target users do not in fact have these problems (that the designer asserts would benefit from vis tool support).\nfield study: where the investigator observes how people act in real-world settings, rather than by bringing them into a laboratory setting. Field studies for domain situation assessment often involve gathering qualitative data through semi-structured interviews.\nOne downstream form of validation is adoption rates of the vis tool.\n\n\n4.6.2 Abstraction Validation\nThe threat at this level is that the identified task abstraction blocks and designed data abstraction blocks do not solve the characterized problems of the target audience. The key aspect of validation against this threat is that the system must be tested by target users doing their own work, rather than doing an abstract task specified by the designers of the vis system.\n\n\n4.6.3 Idiom Validation\nThe threat at this level is that the chosen idioms are not effective at communicating the desired abstraction to the person using the system. One immediate validation approach is to carefully justify the design of the idiom with respect to known perceptual and cognitive principles.\nA downstream approach to validate against this threat is to carry out a lab study: a controlled experiment in a laboratory setting.\n\n\n4.6.4 Algorithm Validation\nThe primary threat at this level is that the algorithm is suboptimal in terms of time or memory performance, either to a theoretical minimum or in comparison with previously proposed algorithms.\nAn immediate form of validation is to analyze the computational complexity of the algorithm, using the standard approaches from the computer science literature.\nThe downstream form of validation is to measures the wall-clock time and memory performance of the implemented algorithm.\n\n\n4.6.5 Mismatches\nA common problem in weak vis projects is a mismatch between the level at which the benefit is claimed (for example, visual encoding idiom) and the validation methodologies chosen (for example, wall-clock timings of the algorithm)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-3",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-3",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.1 The Big Picture",
    "text": "5.1 The Big Picture\nMarks are basic geometric elements that depict items or links, and channels control their appearance. Channels that perceptually convey magnitude information are a good match for ordered data, and those that convey identity information are a good match for categorical data.\n\nMagnitude Channels: Ordered Attributes (Most effective to least):\n\nPosition on common scale\nPosition on unaligned scale\nLength (1D size)\nTilt/angle\nArea (2D size)\nDepth (3D position)\nColor luminance\nColor saturation\nCurvature\nVolume (3D size)\n\nIdentity Channels: Categorical Attributes\n\nSpatial Region\nColor hue\nMotion\nShape"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-marks-and-channels",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-marks-and-channels",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.2 Why Marks and Channels?",
    "text": "5.2 Why Marks and Channels?\nThe core of the design space of visual encodings can be described as an orthogonal combination of two aspects: graphical elements called marks and visual channels to control their appearance."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#defining-marks-and-channels",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#defining-marks-and-channels",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.3 Defining Marks and Channels",
    "text": "5.3 Defining Marks and Channels\nmark: a basic graphical element in an image\n\nPoints (0 dimensional)\nLines (1D)\nArea (2D)\nVolume (3D)\n\nchannel: is a way to control the appearance of marks, independent of the dimensionality of the geometric primative.\n\nPosition\n\nHorizontal\nVertical\nBoth\n\nShape\nSize\n\nLength\nArea\nVolume\n\nColor\nTilt (or Angle)\n\nA single quantitative attribute can be encoded with vertical spatial position. Bar charts show this and the horizontal spatial position channel for the categorical attribute.\nScatterplots encode two quantitative attributes using point marks and both vertial and horizontal spatial position. A third categorical attribute is encoded by adding color to the scatterplot. Adding the visual channel of size encodes a fourth quantitative attribute as well.\nHigher-dimensional mark types usually have built-in constraints (on size and shape) that arise from the way that they are defined. An area or line mark cannot be size or shape coded, but a point can.\n\n5.3.1 Channel Types"
  },
  {
    "objectID": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html",
    "href": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html",
    "title": "Iterating on Full Text Search Keywords using claudette",
    "section": "",
    "text": "In a previous blog post I used Claude (with the help of the Answer.AI claudette library) to generate keywords for 220 questions across the 7 Chapter Questionnaires covered in Part 1 of the fastai course. Using those keywords I retrieved context from fastbook that allowed me to answer 33% (10/30 questions) of the Chapter 1 Questionnaire. In another blog post I manually came up with keywords and achieved a 40% answer rate.\nIn this notebook I’ll see if I can improve the quality of information retrieval by improving the keywords that Claude generates.\n\n\n\n\n\n\nImportant\n\n\n\nFull text search uses corpus-wide statistics so I’ve made sure to load the entire corpus (7 chapter notebooks) into the database which matches my final production environment. Otherwise the same keywords with FTS5 may retrieve different top-1 BM25-ranked contexts.\n\n\nThis notebook is part of a series of blog posts for a project I’m working on called fastbookRAG in which I’m building a hybrid search + LLM pipeline to answer questions from the end-of-chapter Questionnaires in the freely available fastai textbook.\nHere are the results from this notebook. Answer Rate is the percentage of Chapter 1 Questionnaire questions answered using context retrieved from SQLite full-text search with Claude-generated keywords.\n\n\n\nPrompt\nAnswer Rate\n\n\n\n\nA\n33%\n\n\nB\n33%\n\n\nC\n36%\n\n\nD\n33%\n\n\nE\n40%"
  },
  {
    "objectID": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#setup",
    "href": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#setup",
    "title": "Iterating on Full Text Search Keywords using claudette",
    "section": "Setup",
    "text": "Setup\nThis section defines helper functions needed to chunk, load and retrieve the Chapter 1 notebook from a sqlite database.\n\n\nShow imports\n!pip install claudette -qq\nimport sqlite3\nimport json\nimport re\nimport os\nimport pandas as pd, numpy as np\nfrom claudette import *\n\n\n\nmodel = models[1]\nmodel\n\n'claude-3-5-sonnet-20240620'\n\n\n\n\nShow chunking code\ndef get_chunks(notebook_path):\n    with open(notebook_path, 'r', encoding='utf-8') as file:\n        notebook = json.load(file)\n\n    chunks = []\n    current_header = \"\"\n\n    def add_chunk(content):\n        if content.strip():\n            chunks.append(f\"{current_header}\\n\\n{content.strip()}\")\n\n    for cell in notebook['cells']:\n        if cell['cell_type'] == 'markdown':\n            content = ''.join(cell['source'])\n            header_match = re.match(r'^(#+\\s+.*?)$', content, re.MULTILINE)\n            if header_match:  # Check if the cell starts with a header\n                current_header = header_match.group(1)\n                # Add any content after the header in the same cell\n                remaining_content = content[len(current_header):].strip()\n                if remaining_content:\n                    paragraphs = re.split(r'\\n\\s*\\n', remaining_content)\n                    for paragraph in paragraphs:\n                        add_chunk(paragraph)\n            else:\n                paragraphs = re.split(r'\\n\\s*\\n', content)\n                for paragraph in paragraphs:\n                    add_chunk(paragraph)\n        elif cell['cell_type'] == 'code':\n          code_content = '```python\\n' + ''.join(cell['source']) + '\\n```'\n\n          # Include the output of the code cell\n          output_content = ''\n          if 'outputs' in cell and cell['outputs']:\n              for output in cell['outputs']:\n                  if 'text' in output:\n                      output_content += ''.join(output['text'])\n                  elif 'data' in output and 'text/plain' in output['data']:\n                      output_content += ''.join(output['data']['text/plain'])\n\n          # Combine code and output in the same chunk\n          combined_content = code_content + '\\n\\nOutput:\\n' + output_content if output_content else code_content\n          add_chunk(combined_content)\n\n    def filter_chunks(chunks, exclude_headers=[\"Questionnaire\", \"Further Research\"]):\n      filtered_chunks = []\n      for chunk in chunks:\n          lines = chunk.split('\\n')\n          # Check if the first line (header) is in the exclude list\n          if not any(header in lines[0] for header in exclude_headers):\n              filtered_chunks.append(chunk)\n      return filtered_chunks\n\n    return filter_chunks(chunks)\n\n\n\n\nShow the load_data function\ndef load_data(chunks, db_path, chapter=1):\n    try:\n        # Create virtual table if database doesn't exist\n        if not os.path.exists(db_path):\n            with sqlite3.connect(db_path) as conn:\n              cur = conn.cursor()\n              cur.execute(\"\"\"\n              CREATE VIRTUAL TABLE fastbook_text\n              USING FTS5(chapter, text);\n              \"\"\")\n              conn.commit()\n\n        # Load in the chunks for each chapter\n        with sqlite3.connect(db_path) as conn:\n            cur = conn.cursor()\n\n            for chunk in chunks:\n                cur.execute(\"INSERT INTO fastbook_text(chapter, text) VALUES (?, ?)\", (chapter, chunk))\n\n            conn.commit()\n            res = cur.execute(\"SELECT * FROM fastbook_text WHERE chapter = ?\", (chapter,)).fetchall()\n\n        if len(res) != len(chunks):\n            raise ValueError(f\"Number of inserted chunks ({len(res)}) doesn't match input chunks ({len(chunks)})\")\n\n        return True\n\n    except sqlite3.Error as e:\n        print(f\"An error occurred: {e}\")\n        return False\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return False\n\n\n\n\nShow the load_data function\ndef load_data(chunks, db_path, chapter=1):\n    try:\n        # Create virtual table if database doesn't exist\n        if not os.path.exists(db_path):\n            with sqlite3.connect(db_path) as conn:\n              cur = conn.cursor()\n              cur.execute(\"\"\"\n              CREATE VIRTUAL TABLE fastbook_text\n              USING FTS5(chapter, text);\n              \"\"\")\n              conn.commit()\n\n        # Load in the chunks for each chapter\n        with sqlite3.connect(db_path) as conn:\n            cur = conn.cursor()\n\n            for chunk in chunks:\n                cur.execute(\"INSERT INTO fastbook_text(chapter, text) VALUES (?, ?)\", (chapter, chunk))\n\n            conn.commit()\n            res = cur.execute(\"SELECT * FROM fastbook_text WHERE chapter = ?\", (chapter,)).fetchall()\n\n        if len(res) != len(chunks):\n            raise ValueError(f\"Number of inserted chunks ({len(res)}) doesn't match input chunks ({len(chunks)})\")\n\n        return True\n\n    except sqlite3.Error as e:\n        print(f\"An error occurred: {e}\")\n        return False\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return False\n\n\n\n\nShow the db_search function\ndef db_search(df, limit=1):\n  results = []\n  with sqlite3.connect('fastbook.db') as conn:\n    cur = conn.cursor()\n\n    for _, row in df.iterrows():\n      keywords = ' OR '.join([f'\"{keyword.strip(\",\")}\"' for keyword in row['keywords'].replace('\"', '').split()])\n\n      q = f\"\"\"\n        SELECT text, rank\n        FROM fastbook_text\n        WHERE fastbook_text MATCH ?\n        AND chapter = ?\n        ORDER BY rank\n        LIMIT ?\n        \"\"\"\n      res = cur.execute(q, (keywords, str(row['chapter']), limit)).fetchall()\n      res = [item[0] for item in res]\n      results.extend(res)\n\n    return results\n\n\nDuring this notebook I learned the implications of full text search using corpus-wide statistics. I wanted to test out Claude keywords for chapter 1 search and only loaded in chapter 1 into the database. This retrieved different contexts for the same keywords than when I used FTS with all 7 chapters loaded in the database. In order to improve the keywords, I need to make sure the corpus is the same as the final “production” environment.\n\n\n\nComparison of retrieved chunks using the same keywords on different databases\n\n\n\n\nShow the dict w/ notebook filenames\nnbs = {\n    '1': '01_intro.ipynb',\n    '2': '02_production.ipynb',\n    '4': '04_mnist_basics.ipynb',\n    '8': '08_collab.ipynb',\n    '9': '09_tabular.ipynb',\n    '10': '10_nlp.ipynb',\n    '13': '13_convolutions.ipynb'\n}\n\n\n\n# chunkify all notebooks\n# chunking each notebook\ndata = {}\n\nfor chapter, nb in nbs.items():\n  data[chapter] = get_chunks(nb)\n\n\nfor chapter, chunks in data.items():\n  print(chapter, len(chunks))\n\n1 307\n2 227\n4 433\n8 157\n9 387\n10 190\n13 266\n\n\n\n# load chunks into the database\nfor chapter, chunks in data.items():\n  print(f\"Chapter {chapter}:\", load_data(chunks, 'fastbook.db', chapter))\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\n\n\n\n# get the questions and keywords\nurl = 'https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/5e41b9eb34f515f00321e55307cc4d5abbd75cb5/fastbookRAG_evals.csv'\nquestions = pd.read_csv(url).query('chapter == 1 and is_answerable == 1')\nquestions.shape\n\n(30, 6)\n\n\n\n# retrieve Top-1 chunk by BM25\nresults = db_search(questions, limit=1)\nlen(results) == 30\n\nTrue\n\n\n\n# export results\nquestions['context'] = results\nquestions.to_csv('bm25_a_keywordsA.csv', index=False)"
  },
  {
    "objectID": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#generating-keywords-with-claude-using-different-prompts",
    "href": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#generating-keywords-with-claude-using-different-prompts",
    "title": "Iterating on Full Text Search Keywords using claudette",
    "section": "Generating Keywords with Claude Using Different Prompts",
    "text": "Generating Keywords with Claude Using Different Prompts\n\nPrompt A\nThe keywords generated using the following prompt retrieved context that allowed me to answer 10/30 (33%) of the Chapter 1 Questionnaire questions:\n\nI am working on a keyword search project and i need to create 3-6 keywords for each question_text that I provide you. Do not generate keywords that stray too far in meaning from the question_text. Only respond with the comma-separated list of keywords surrounded by double quotes.\nNo yapping.\nExamples:\nquestion_text: Name five areas where deep learning is now the best in the world\nkeywords: “deep learning, state of the art, best, world”\nquestion_text: Why is it hard to use a traditional computer program to recognize images in a photo?\nkeywords: “image, recognize, recognition, traditional, computer, program”\nquestion_text: What were the two theoretical misunderstandings that held back the field of neural networks?\nkeywords: “theoretical, misunderstandings, held, back, field, neural network”\nquestion_text: Why is it hard to understand why a deep learning model makes a particular prediction?\nkeywords:\n\n\n\nPrompt B\nI’ll start by creating a prompt that was recommended by Claude:\n\nFor the following question text, please generate 3-6 comma-separated keywords that capture the main concepts and are suitable for use in a SQLite full-text search query. The keywords should be concise, relevant, and help in retrieving appropriate text chunks from a database. Avoid using articles, prepositions, or other common words that don’t add significant meaning. Here’s the question text:\n{question_text}\nPlease provide the keywords in the following format: keywords: “keyword1, keyword2, keyword3”\n\n\npromptB = \"\"\"For the following question text, please generate 3-6 comma-separated keywords that capture the main concepts and are suitable for use in a SQLite full-text search query. The keywords should be concise, relevant, and help in retrieving appropriate text chunks from a database. Avoid using articles, prepositions, or other common words that don't add significant meaning. Here's the question text:\n\n{question_text}\n\nPlease provide the keywords in the following format:\nkeywords: \"keyword1, keyword2, keyword3\" \"\"\"\n\n\nformatted_prompt = promptB.format(question_text=\"Why is it hard to understand why a deep learning model makes a particular prediction?\")\nprint(formatted_prompt)\n\nFor the following question text, please generate 3-6 comma-separated keywords that capture the main concepts and are suitable for use in a SQLite full-text search query. The keywords should be concise, relevant, and help in retrieving appropriate text chunks from a database. Avoid using articles, prepositions, or other common words that don't add significant meaning. Here's the question text:\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\nPlease provide the keywords in the following format:\nkeywords: \"keyword1, keyword2, keyword3\" \n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nchat.use\n\nIn: 0; Out: 0; Total: 0\n\n\n\nr = chat(formatted_prompt)\nr\n\nkeywords: “deep learning, model, prediction, understanding, interpretability”\n\n\nid: msg_01FTbVhR4dwjNqoDmEmBexS1\ncontent: [{'text': 'keywords: \"deep learning, model, prediction, understanding, interpretability\"', 'type': 'text'}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'input_tokens': 141, 'output_tokens': 18, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n\n\n\n\nLooks good! I’ll now use this prompt to generate keywords for all 30 Chapter 1 Questionnaire questions.\n\nkeyword_results = []\ntokens = 0\nfor row in questions['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptB.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results.append(r.content[0].text)\n  tokens += chat.use.total\n\nThat was about 2 cents of tokens used.\n\nlen(keyword_results), tokens\n\n(30, 4736)\n\n\n\nkeyword_results[:5]\n\n['keywords: \"deep learning, math, data, expensive computers, PhD\"',\n 'keywords: \"deep learning, best, areas, applications, achievements\"',\n 'keywords: \"device, artificial neuron, first\"',\n 'keywords: \"parallel distributed processing, PDP, book, requirements\"',\n 'keywords: \"theoretical misunderstandings, neural networks, field setbacks\"']\n\n\nUnfortunately I missed the fact that Claude includes ‘keywords’ in each respone—oops! I’ll have to do some quick cleanup.\n\ncleaned_keywords = [s.replace('keywords: ', '') for s in keyword_results]\ncleaned_keywords[:5]\n\n['\"deep learning, math, data, expensive computers, PhD\"',\n '\"deep learning, best, areas, applications, achievements\"',\n '\"device, artificial neuron, first\"',\n '\"parallel distributed processing, PDP, book, requirements\"',\n '\"theoretical misunderstandings, neural networks, field setbacks\"']\n\n\nI’ll replace the existing keywords and run the full text search again:\n\nquestions['keywords'] = cleaned_keywords\nquestions['keywords'].iloc[:5]\n\n\n\n\n\n  \n    \n      \n      keywords\n    \n  \n  \n    \n      0\n      \"deep learning, math, data, expensive computer...\n    \n    \n      1\n      \"deep learning, best, areas, applications, ach...\n    \n    \n      2\n      \"device, artificial neuron, first\"\n    \n    \n      3\n      \"parallel distributed processing, PDP, book, r...\n    \n    \n      4\n      \"theoretical misunderstandings, neural network...\n    \n  \n\ndtype: object\n\n\n\ncleaned_keywords[-5:]\n\n['\"architecture, design, structure, building, framework\"',\n '\"segmentation, market, division, targeting, customer groups\"',\n '\"y_range, purpose, usage, necessity\"',\n '\"hyperparameters, machine learning, model tuning, optimization\"',\n '\"avoid failures, AI implementation, organization, best practices\"']\n\n\n\nquestions['keywords'].iloc[-5:]\n\n\n\n\n\n  \n    \n      \n      keywords\n    \n  \n  \n    \n      28\n      \"architecture, design, structure, building, fr...\n    \n    \n      29\n      \"segmentation, market, division, targeting, cu...\n    \n    \n      30\n      \"y_range, purpose, usage, necessity\"\n    \n    \n      31\n      \"hyperparameters, machine learning, model tuni...\n    \n    \n      32\n      \"avoid failures, AI implementation, organizati...\n    \n  \n\ndtype: object\n\n\n\n# retrieve Top-1 chunk by BM25\nresults = db_search(questions, limit=1)\nlen(results) == 30\n\nTrue\n\n\n\n# export results\nquestions['context'] = results\nquestions.to_csv('bm25_a_keywordsB.csv', index=False)\n\nThese keywords retrieved chunks that allowed me to answer 10/30 or 33% of the Chapter 1 Questionnaire. This was a different set of 10 questions that Prompt A.\n\n\nPrompt C\nAfter looking at the data, I’ll modify the prompt to provide some more flexibility in the keyword search by requesting Claude to do two things:\n\nuse single-word keywords when possible\ninclude singular and plural versions of nouns when applicable\nremove the keywords: string before the example keywords\n\nHere’s the new prompt:\n\nFor the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Avoid articles, prepositions, and common words. Use this format:\n{question_text}\n“keyword1, keyword2, keyword3”\n\n\npromptC = \"\"\"\"For the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Avoid articles, prepositions, and common words. Use this format:\n\n{question_text}\n\n\"keyword1, keyword2, keyword3\" \"\"\"\n\n\nformatted_prompt = promptC.format(question_text=\"Why is it hard to understand why a deep learning model makes a particular prediction?\")\nprint(formatted_prompt)\n\n\"For the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Avoid articles, prepositions, and common words. Use this format:\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\n\"keyword1, keyword2, keyword3\" \n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nr = chat(formatted_prompt)\nr\n\n“deep learning, model, models, prediction, predictions, understand, understanding”\n\n\nid: msg_01HNHpvbABHGxLzxacXLXuv1\ncontent: [{'text': '\"deep learning, model, models, prediction, predictions, understand, understanding\"', 'type': 'text'}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'input_tokens': 114, 'output_tokens': 19, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n\n\n\n\nNice! It’s following my instructions. I’ll run through the process of generating keywords, running full text search, and evaluating the results:\n\nkeyword_results = []\ntokens = 0\nfor row in questions['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptC.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\nlen(keyword_results), tokens\n\n(30, 3884)\n\n\n\nkeyword_results[:5]\n\n['\"deep learning, math, data, computers, expensive, PhD\"',\n '\"deep learning, areas, best, world, artificial intelligence, neural networks\"',\n '\"neuron, neurons, device, artificial, first\"',\n '\"parallel, distributed, processing, PDP, requirements, book\"',\n '\"neural, networks, theoretical, misunderstandings, field\"']\n\n\n\nquestions['keywords'] = keyword_results\nquestions['keywords'].iloc[:5]\n\n\n\n\n\n  \n    \n      \n      keywords\n    \n  \n  \n    \n      0\n      \"deep learning, math, data, computers, expensi...\n    \n    \n      1\n      \"deep learning, areas, best, world, artificial...\n    \n    \n      2\n      \"neuron, neurons, device, artificial, first\"\n    \n    \n      3\n      \"parallel, distributed, processing, PDP, requi...\n    \n    \n      4\n      \"neural, networks, theoretical, misunderstandi...\n    \n  \n\ndtype: object\n\n\n\n# retrieve Top-1 chunk by BM25\nresults = db_search(questions, limit=1)\nlen(results) == 30\n\nTrue\n\n\n\n# export results\nquestions['context'] = results\nquestions.to_csv('bm25_a_keywordsC.csv', index=False)\n\nThe retrieved context allowed me to answer 11/30 or 36% of the questions. Moving in the right direction! One of the set of keywords had some additional text and that would have likely increased the answer rate to 12/30:\n\nkeyword_results[-2]\n\n'Here are the keywords for the given question:\\n\\n\"hyperparameters, hyperparameter, parameter, parameters, machine learning, tuning\"'\n\n\nI’ll also ask it to include any numbers it finds in the question text as separate keywords, as that might have gotten me another correct answer:\n\nquestions.iloc[14]\n\n\n\n\n\n  \n    \n      \n      17\n    \n  \n  \n    \n      chapter\n      1\n    \n    \n      question_number\n      18\n    \n    \n      question_text\n      \"\"Do we always have to use 224×224-pixel image...\n    \n    \n      answer\n      \"\"No we do not. 224x224 is commonly used for h...\n    \n    \n      is_answerable\n      1\n    \n    \n      keywords\n      \"cat, cats, recognition, model, image, images,...\n    \n    \n      context\n      ### How Our Image Recognizer Works\\n\\nIn the t...\n    \n  \n\ndtype: object\n\n\n\n\nPrompt D\nI’ll update the prompt with the following observations:\n\nremind Claude “no yapping” so it doesn’t include additional explanatory text other than the keywords.\nask Claude to include any numbers in the question as a keyword.\n\nHere’s the new prompt:\n\nFor the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Include any numbers as keywords. Avoid articles, prepositions, and common words. Use this format. No yapping:\n{question_text}\n“keyword1, keyword2, keyword3”\n\n\npromptD = \"\"\"For the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Include any numbers as keywords. Avoid articles, prepositions, and common words. Use this format. No yapping:\n\n{question_text}\n\n\"keyword1, keyword2, keyword3\" \"\"\"\n\n\nformatted_prompt = promptD.format(question_text=\"Why is it hard to understand why a deep learning model makes a particular prediction?\")\nprint(formatted_prompt)\n\nFor the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Include any numbers as keywords. Avoid articles, prepositions, and common words. Use this format. No yapping:\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\n\"keyword1, keyword2, keyword3\" \n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nr = chat(formatted_prompt)\nr\n\n“deep, learning, model, prediction, understand, hard”\n\n\nid: msg_01QtPrjfRGNPmt8eLxDxZcZE\ncontent: [{'text': '\"deep, learning, model, prediction, understand, hard\"', 'type': 'text'}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'input_tokens': 123, 'output_tokens': 16, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n\n\n\n\nIt’s not including plural versions of nouns so I’ll remove the “if relevant” from the instruction and try again:\n\npromptD = \"\"\"For the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords. Include both singular and plural forms for nouns. Include any numbers as keywords. Avoid articles, prepositions, and common words. No yapping:\n\n{question_text}\n\n\"keyword1, keyword2, keyword3\" \"\"\"\n\n\nformatted_prompt = promptD.format(question_text=\"Why is it hard to understand why a deep learning model makes a particular prediction?\")\nprint(formatted_prompt)\n\nFor the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords. Include both singular and plural forms for nouns. Include any numbers as keywords. Avoid articles, prepositions, and common words. No yapping:\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\n\"keyword1, keyword2, keyword3\" \n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nr = chat(formatted_prompt)\nr\n\ndeep, learning, model, models, prediction, predictions, understand, understanding\n\n\nid: msg_01EQqyCJDRCc81cLL78E8sA7\ncontent: [{'text': 'deep, learning, model, models, prediction, predictions, understand, understanding', 'type': 'text'}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'input_tokens': 115, 'output_tokens': 18, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n\n\n\n\nNice! That fixed it at least for one example.\n\nkeyword_results = []\ntokens = 0\nfor row in questions['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptD.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results.append(r.content[0].text)\n  tokens += chat.use.total\n\nI’ll look at the full set of keywords to make sure there are no filler texts included:\n\nkeyword_results\n\n['deep, learning, math, data, computers, phd',\n 'deep, learning, areas, best, world',\n '\"neuron, neurons, device, devices, artificial, first\"',\n 'book, requirements, parallel, distributed, processing, pdp',\n 'theoretical, misunderstandings, held, back, field, neural, networks',\n '\"GPU, GPUs, graphics, processor, processors\"',\n 'notebook, notebooks, execute, cell, cells, 1',\n 'image, images, photo, photos, recognize, recognition, computer, program, traditional',\n '\"Samuel, weight, weights, assignment, assignments\"',\n 'deep, learning, weights, weight, Samuel, term',\n 'deep, learning, model, models, prediction, predictions, understand, understanding',\n 'theorem, neural, networks, mathematical, problem, accuracy',\n 'train, model, models, training, dataset, datasets, data',\n 'feedback, loops, rollout, rollouts, predictive, policing, model, models',\n 'cat, cats, recognition, model, 224, pixel, pixels, image, images',\n 'classification, classifications, regression, regressions, difference, differences',\n 'validation, validations, test, tests, set, sets',\n 'fastai, validation, set, sets',\n 'random, sample, samples, validation, set, sets',\n 'overfitting, overfits, example, examples, model, models',\n 'metric, metrics, loss, losses, differ, difference',\n 'pretrained, models, model, help',\n 'head, heads, model, models',\n 'cnn, cnns, layer, layers, feature, features, early, late',\n 'image, images, model, models, photo, photos',\n 'architecture, architectures',\n '\"segmentation, segment, segments\"',\n '\"y_range, ranges, plotting, visualization, axis, limits\"',\n 'hyperparameters, hyperparameter, machine, learning, model, parameter',\n '\"ai, artificial, intelligence, failures, avoid, organization, organizations\"']\n\n\n\nquestions['keywords'] = keyword_results\nquestions['keywords'].iloc[:5]\n\n\n\n\n\n  \n    \n      \n      keywords\n    \n  \n  \n    \n      0\n      deep, learning, math, data, computers, phd\n    \n    \n      1\n      deep, learning, areas, best, world\n    \n    \n      2\n      \"neuron, neurons, device, devices, artificial,...\n    \n    \n      3\n      book, requirements, parallel, distributed, pro...\n    \n    \n      4\n      theoretical, misunderstandings, held, back, fi...\n    \n  \n\ndtype: object\n\n\n\n# retrieve Top-1 chunk by BM25\nresults = db_search(questions, limit=1)\nlen(results) == 30\n\nTrue\n\n\n\n# export results\nquestions['context'] = results\nquestions.to_csv('bm25_a_keywordsD.csv', index=False)\n\nThis prompt resulted in 10/30 or a 33% Answer Rate and didn’t improve the retrieved contexts in the way I thought it would! I’ll go back to Prompt C and only add “no yapping”\n\n\nPrompt E\n\nFor the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Avoid articles, prepositions, and common words. Use this format. No yapping:\n{question_text}\n“keyword1, keyword2, keyword3”\n\n\npromptE = \"\"\"\"For the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Avoid articles, prepositions, and common words. Use this format. No yapping:\n\n{question_text}\n\n\"keyword1, keyword2, keyword3\" \"\"\"\n\n\nformatted_prompt = promptE.format(question_text=\"Why is it hard to understand why a deep learning model makes a particular prediction?\")\nprint(formatted_prompt)\n\n\"For the given question text, generate 3-6 comma-separated keywords that capture the main concepts for a SQLite full-text search query. Prefer single-word keywords when possible. Include both singular and plural forms for nouns if relevant. Avoid articles, prepositions, and common words. Use this format. No yapping:\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\n\"keyword1, keyword2, keyword3\" \n\n\n\nchat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\nr = chat(formatted_prompt)\nr\n\n“deep learning, model, prediction, understand, predictions”\n\n\nid: msg_01G3w22vDXTpWryUwUpqouXN\ncontent: [{'text': '\"deep learning, model, prediction, understand, predictions\"', 'type': 'text'}]\nmodel: claude-3-5-sonnet-20240620\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'input_tokens': 118, 'output_tokens': 15, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0}\n\n\n\n\n\nkeyword_results = []\ntokens = 0\nfor row in questions['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptE.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\nkeyword_results\n\n['\"deep learning, math, data, computers, PhD\"',\n 'deep learning, areas, best, world',\n '\"neuron, neurons, device, artificial, principle\"',\n '\"parallel, distributed, processing, PDP, requirements, book\"',\n '\"neural, networks, theoretical, misunderstandings, field\"',\n '\"GPU, graphics, processor, processors, computing, hardware\"',\n '\"notebook, execute, cell, calculation, result\"',\n '\"computer, computers, image, images, recognition, photo, photos\"',\n '\"Samuel, weight, weights, assignment, assignments\"',\n 'deep learning, weights, Samuel, term',\n '\"deep learning, model, prediction, understand, predictions\"',\n '\"theorem, neural, network, networks, mathematical, problem, accuracy\"',\n '\"train, model, training, models, dataset, data\"',\n '\"feedback, loop, rollout, predictive, policing, model, models\"',\n '\"cat, cats, recognition, model, image, images, pixel, pixels\"',\n '\"classification, regression, difference, differences, machine learning, models\"',\n '\"validation, test, set, sets, need, purpose\"',\n '\"fastai, validation, set, datasets\"',\n '\"random, sample, samples, validation, set, sets\"',\n '\"overfitting, overfit, example, model, data, machine learning\"',\n '\"metric, metrics, loss, differ, difference, measurement\"',\n '\"pretrained, models, help, pretraining, model\"',\n 'head, model, models',\n '\"CNN, layers, features, early, later\"',\n '\"image, images, model, models, photo, photos\"',\n '\"architecture, architectures, design, structure, building\"',\n '\"segmentation, segment, segments, divide, division, categorize\"',\n '\"y_range, range, purpose, usage, need\"',\n '\"hyperparameters, hyperparameter, machine learning, model, tuning, optimization\"',\n '\"AI, failures, avoid, organization, organizations\"']\n\n\n\nquestions['keywords'] = keyword_results\nquestions['keywords'].iloc[:5]\n\n\n\n\n\n  \n    \n      \n      keywords\n    \n  \n  \n    \n      0\n      \"deep learning, math, data, computers, PhD\"\n    \n    \n      1\n      deep learning, areas, best, world\n    \n    \n      2\n      \"neuron, neurons, device, artificial, principle\"\n    \n    \n      3\n      \"parallel, distributed, processing, PDP, requi...\n    \n    \n      4\n      \"neural, networks, theoretical, misunderstandi...\n    \n  \n\ndtype: object\n\n\n\n# retrieve Top-1 chunk by BM25\nresults = db_search(questions, limit=1)\nlen(results) == 30\n\nTrue\n\n\n\n# export results\nquestions['context'] = results\nquestions.to_csv('bm25_a_keywordsE.csv', index=False)\n\nThe keywords generated by this prompt resulted in retrieved chunks that allowed me to answer 12/30 or 40% of the questions! This is comparable to the 40% achieved with my manually generated keywords.\nI’ll go ahead and Claude-generate keywords for all 202 questions in my dataset."
  },
  {
    "objectID": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#generating-keywords-for-all-questions",
    "href": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#generating-keywords-for-all-questions",
    "title": "Iterating on Full Text Search Keywords using claudette",
    "section": "Generating Keywords for All Questions",
    "text": "Generating Keywords for All Questions\n\n# get the questions\nurl = 'https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/5e41b9eb34f515f00321e55307cc4d5abbd75cb5/fastbookRAG_evals.csv'\nquestions = pd.read_csv(url).query('is_answerable == 1')\nquestions.shape\n\n(202, 6)\n\n\n\nkeyword_results = []\ntokens = 0\nfor row in questions['question_text']:\n  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n  formatted_prompt = promptE.format(question_text=row[2:-2])\n  r = chat(formatted_prompt)\n  keyword_results.append(r.content[0].text)\n  tokens += chat.use.total\n\n\nlen(keyword_results) == 202\n\nTrue\n\n\n\nquestions['keywords'] = keyword_results\n\n\nquestions.to_csv('evals_promptE_keywords.csv', index=False)"
  },
  {
    "objectID": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#final-thoughts",
    "href": "posts/2024-08-27-fastbookRAG-claude-keywords/index.html#final-thoughts",
    "title": "Iterating on Full Text Search Keywords using claudette",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are the results from this notebook. Answer Rate is the percentage of Chapter 1 Questionnaire questions answered using context retrieved from SQLite full-text search with Claude-generated keywords.\n\n\n\nPrompt\nAnswer Rate\n\n\n\n\nA\n33%\n\n\nB\n33%\n\n\nC\n36%\n\n\nD\n33%\n\n\nE\n40%\n\n\n\nI learned a few important lessons in this work:\n\nThe implications of full text search using corpus-wide statistics: sqlite’s FTS5 retrieved different contexts for the same keywords when I used it on a database with 1 versus 7 chapters.\nClaude will tell you how to prompt: I haven’t explored this as much as I should, but definitely benefited from asking Claude what prompt would be effective for generating keywords.\nClaude will make mistakes: It’s sometimes easy to forget that even for relatively simple instructions (“follow this format”) Claude will incorrectly include extraneous text. A good reminder to add “no yapping” to the prompt when brevity is needed.\n\nI’m pretty excited about using these Claude-generated keywords as they resulted in the same answer rate (40%) as my manually generated keywords. That’s a promising start!\nI hope you enjoyed this blog post. Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "",
    "text": "In this notebook, I’ll walk through a modified algorithm (suggested by Claude) to calculate the average stroke width of letters in a text image.\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\n\n\nShow imports\nimport cv2\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#load-the-image-and-binarize-it",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#load-the-image-and-binarize-it",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Load the Image and Binarize it",
    "text": "Load the Image and Binarize it\nAs is the case with all of the algorithm so far (aspect ratio, lowercase-to-uppercase ratio) we start by binarizing the image:\n\npath = 'serif-76px.png'\nimg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#create-a-skeleton",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#create-a-skeleton",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Create a Skeleton",
    "text": "Create a Skeleton\nUsing OpenCV’s ximgproc.thinning algorithm, the binary image is converted to a “skeleton”—this approximates the “centerline” of the stroke.\n\nskeleton = cv2.ximgproc.thinning(binary)\nskeleton\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#calculate-distance-from-white-to-black-pixels",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#calculate-distance-from-white-to-black-pixels",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Calculate Distance from White-to-Black Pixels",
    "text": "Calculate Distance from White-to-Black Pixels\ndistanceTransform uses the Euclidean Distance (cv2.DIST_L2) to calculate the distance between each white pixel to the closes black pixel in the binary.\n\ndist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\ndist_transform\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\nFor each skeleton white pixel, we can find the corresponding distance between that pixel and the closest black pixel, and multiply it by 2 to get the full stroke “width”:\n\n# Initialize stroke width map\nstroke_width_map = np.zeros_like(dist_transform)\n\n# Find non-zero pixels in the skeleton\ny, x = np.where(skeleton > 0)\n\n# For each skeleton pixel, find the corresponding stroke width\nfor i, j in zip(y, x):\n    stroke_width = dist_transform[i, j] * 2  # Diameter is twice the radius\n    stroke_width_map[i, j] = stroke_width\n\nVisualizing stroke_width_map:\n\n# Prepare for visualization\ndef normalize_and_colorize(img):\n    normalized = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX)\n    return cv2.applyColorMap((normalized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\ninitial_color = normalize_and_colorize(stroke_width_map)\nax.imshow(cv2.cvtColor(initial_color, cv2.COLOR_BGR2RGB));\nax.set_title('Initial Stroke Width Map');\nax.axis('off');"
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#dilate-the-skeleton",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#dilate-the-skeleton",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Dilate the Skeleton",
    "text": "Dilate the Skeleton\nWe then dilate the stroke_width_map to fill out the strokes:\n\nkernel = np.ones((3, 3), np.uint8)\nstroke_width_map_dilated = cv2.dilate(stroke_width_map, kernel, iterations=2)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\ndilated = normalize_and_colorize(stroke_width_map_dilated)\nax.imshow(cv2.cvtColor(dilated, cv2.COLOR_BGR2RGB));\nax.set_title('Stroke Width Map - Dilated');\nax.axis('off');"
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#final-stroke-width-map",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#final-stroke-width-map",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Final Stroke Width Map",
    "text": "Final Stroke Width Map\nThe dilated stroke width map is a result of applyig a 3x3 pixel kernel to the skeleton so it doesn’t have the exact shape of the original image. To get a better approximation of the original, we mask the stroke width map with the binary image:\n\n# Mask the stroke width map with the binary image\nstroke_width_map_final = stroke_width_map_dilated * (binary > 0)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\nfinal = normalize_and_colorize(stroke_width_map_final)\nax.imshow(cv2.cvtColor(final, cv2.COLOR_BGR2RGB));\nax.set_title('Stroke Width Map - Final');\nax.axis('off');\n\n\n\n\n\n\n\n\nI’ll wrap the above code into a stroke_width_viz function and apply it to different images.\n\n\nShow stroke_width_viz code\ndef stroke_width_viz(path):\n    # Read the image\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\n    # Threshold the image\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Create a skeleton of the image\n    skeleton = cv2.ximgproc.thinning(binary)\n\n    # Compute distance transform\n    dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\n\n    # Initialize stroke width map\n    stroke_width_map = np.zeros_like(dist_transform)\n\n    # Find non-zero pixels in the skeleton\n    y, x = np.where(skeleton > 0)\n\n    # For each skeleton pixel, find the corresponding stroke width\n    for i, j in zip(y, x):\n        stroke_width = dist_transform[i, j] * 2  # Diameter is twice the radius\n        stroke_width_map[i, j] = stroke_width\n\n    # Dilate the stroke width map to fill the strokes\n    kernel = np.ones((3, 3), np.uint8)\n    stroke_width_map = cv2.dilate(stroke_width_map, kernel, iterations=2)\n\n    # Mask the stroke width map with the binary image\n    stroke_width_map = stroke_width_map * (binary > 0)\n\n    # Normalize the stroke width map for visualization\n    stroke_width_map_norm = cv2.normalize(stroke_width_map, None, 0, 1, cv2.NORM_MINMAX)\n\n    # Apply a color map\n    stroke_width_color = cv2.applyColorMap((stroke_width_map_norm * 255).astype(np.uint8), cv2.COLORMAP_JET)\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Display the original image\n    ax1.imshow(img, cmap='gray')\n    ax1.set_title('Original Image')\n    ax1.axis('off')\n\n    # Display the colorized stroke width map\n    ax2.imshow(cv2.cvtColor(stroke_width_color, cv2.COLOR_BGR2RGB))\n    ax2.set_title('Stroke Width Visualization')\n    ax2.axis('off')\n\n    # Add a colorbar\n    plt.colorbar(ax2.imshow(stroke_width_map_norm), ax=ax2, label='Relative Stroke Width')\n\n    plt.tight_layout()\n    plt.show()\n\n    return stroke_width_map\n\n\n\nstroke_width_viz('serif-76px.png');\n\n\n\n\n\n\n\n\nFor the smaller 18px font size, it’s easier to to see which letters (l, t, N, D) have wider stroke widths. It’s also showing some defects of this algorithm, as two different Ds have different relative stroke widths.\n\nstroke_width_viz('serif-18px.png');\n\n\n\n\n\n\n\n\nIt’s unclear from visual inspection how accurate this algorithm is for tiny font sizes (8px). I’m assuming it’s not.\n\nstroke_width_viz('serif-8px.png');\n\n\n\n\n\n\n\n\nFor a very large font size (420px) the dilation of the skeleton does not seem to be enough:\n\nstroke_width_viz('serif-420px.png');"
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#improving-the-algorithm",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#improving-the-algorithm",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Improving the Algorithm",
    "text": "Improving the Algorithm\nThe algorithm performs well for medium to large font sizes (36px, 76px). However, small font sizes show noisy, clumped areas of high stroke width, while for very large sizes, insufficient dilation leads to underestimating stroke width. I think the issue is the kernel size during dilation. I’ll incorporate (with Claude’s help) a modified kernel size based on the average letter height (calculated with a bounding rectangle for each letter).\n\n\nShow updated stroke_width_viz code\ndef stroke_width_viz(path):\n    # Read the image\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\n    # Threshold the image\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Find contours\n    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Calculate average height of bounding rectangles\n    heights = [cv2.boundingRect(contour)[3] for contour in contours]\n    avg_height = np.median(heights)\n\n    # Determine kernel size based on average height\n    #kernel_size = max(3, int(avg_height / 6))\n    kernel_size = 2 if int(avg_height / 5) == 0 else max(3, int(avg_height / 5))\n    print(kernel_size)\n\n    # Create a skeleton of the image\n    skeleton = cv2.ximgproc.thinning(binary)\n\n    # Compute distance transform\n    dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\n\n    # Initialize stroke width map\n    stroke_width_map = np.zeros_like(dist_transform)\n\n    # Find non-zero pixels in the skeleton\n    y, x = np.where(skeleton > 0)\n\n    # For each skeleton pixel, find the corresponding stroke width\n    for i, j in zip(y, x):\n        stroke_width = dist_transform[i, j] * 2  # Diameter is twice the radius\n        stroke_width_map[i, j] = stroke_width\n\n    # Dilate the stroke width map to fill the strokes\n    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n    stroke_width_map = cv2.dilate(stroke_width_map, kernel, iterations=2)\n\n    # Mask the stroke width map with the binary image\n    stroke_width_map = stroke_width_map * (binary > 0)\n\n    # Normalize the stroke width map for visualization\n    stroke_width_map_norm = cv2.normalize(stroke_width_map, None, 0, 1, cv2.NORM_MINMAX)\n\n    # Apply a color map\n    stroke_width_color = cv2.applyColorMap((stroke_width_map_norm * 255).astype(np.uint8), cv2.COLORMAP_JET)\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Display the original image\n    ax1.imshow(img, cmap='gray')\n    ax1.set_title('Original Image')\n    ax1.axis('off')\n\n    # Display the colorized stroke width map\n    ax2.imshow(cv2.cvtColor(stroke_width_color, cv2.COLOR_BGR2RGB))\n    ax2.set_title('Stroke Width Visualization')\n    ax2.axis('off')\n\n    # Add a colorbar\n    plt.colorbar(ax2.imshow(stroke_width_map_norm), ax=ax2, label='Relative Stroke Width')\n\n    plt.tight_layout()\n    plt.show()\n\n    return stroke_width_map\n\n\n\nstroke_width_viz('serif-420px.png');\n\n19\n\n\n\n\n\n\n\n\n\n\nstroke_width_viz('serif-8px.png');\n\n2\n\n\n\n\n\n\n\n\n\n\nstroke_width_viz('serif-18px.png');\n\n3\n\n\n\n\n\n\n\n\n\n\nstroke_width_viz('serif-36px.png');\n\n3\n\n\n\n\n\n\n\n\n\n\nstroke_width_viz('serif-76px.png');\n\n7\n\n\n\n\n\n\n\n\n\nThe updated algorithm performs better for very large font sizes."
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#calculating-size-invariant-average-stroke-width",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#calculating-size-invariant-average-stroke-width",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Calculating (Size Invariant) Average Stroke Width",
    "text": "Calculating (Size Invariant) Average Stroke Width\nBy visual inspection, the algorithm seems to be accurately calculating the stroke width of the letters. To quantify its performance, I’ll calculate an average stroke width for each image, which should remain consistent across different font sizes of the same font.\n\n\nShow calculate_stroke_width code\ndef calculate_stroke_width(image_path):\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Threshold the image\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Find contours\n    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Calculate average height of bounding rectangles\n    heights = [cv2.boundingRect(contour)[3] for contour in contours]\n    avg_height = np.median(heights)\n\n    # Create a skeleton of the image\n    skeleton = cv2.ximgproc.thinning(binary)\n\n    # Compute distance transform\n    dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\n\n    # Initialize stroke width map\n    stroke_width_map = np.zeros_like(dist_transform)\n\n    # Find non-zero pixels in the skeleton\n    y, x = np.where(skeleton > 0)\n\n    # For each skeleton pixel, find the corresponding stroke width\n    for i, j in zip(y, x):\n        stroke_width = dist_transform[i, j] * 2  # Diameter is twice the radius\n        stroke_width_map[i, j] = stroke_width\n\n    # Determine kernel size based on average height\n    kernel_size = 2 if int(avg_height / 5) == 0 else max(3, int(avg_height / 5))\n\n    # Dilate the stroke width map to fill the strokes\n    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n    stroke_width_map = cv2.dilate(stroke_width_map, kernel, iterations=2)\n\n    # Mask the stroke width map with the binary image\n    stroke_width_map = stroke_width_map * (binary > 0)\n\n    # Calculate the average stroke width for normalized non-zero stroke widths\n    non_zero_stroke_widths = stroke_width_map[stroke_width_map != 0]\n    normalized_stroke_widths = non_zero_stroke_widths / np.max(non_zero_stroke_widths)\n    avg_stroke_width = np.median(normalized_stroke_widths)\n\n    return avg_stroke_width\n\n\nThere still exists variance in average stroke width across font size within the same typeface, but overall, for font sizes between 18 and 76px, the median stroke width for display is larger than serif which intuitively makes sense because the display font visually has thicker strokes.\n\n\nShow stroke width calculation for-loop\nfont_szs = [18, 24, 36, 76]\nsws = []\ntypefaces = [\"display\", \"serif\"]\n\nfor t in typefaces:\n  for sz in font_szs:\n    sws.append(calculate_stroke_width(f\"{t}-{sz}px.png\"))\n\ndf = pd.DataFrame({\n    'typeface': [\"display\"]*len(font_szs) + [\"serif\"]* len(font_szs),\n    'font-sizes': font_szs*2,\n    'median stroke-width': sws})\n\ndf\n\n\n\n\n  \n    \n\n\n  \n    \n      \n      typeface\n      font-sizes\n      median stroke-width\n    \n  \n  \n    \n      0\n      display\n      18\n      0.714286\n    \n    \n      1\n      display\n      24\n      0.666667\n    \n    \n      2\n      display\n      36\n      0.750000\n    \n    \n      3\n      display\n      76\n      0.781629\n    \n    \n      4\n      serif\n      18\n      0.700000\n    \n    \n      5\n      serif\n      24\n      0.637262\n    \n    \n      6\n      serif\n      36\n      0.666667\n    \n    \n      7\n      serif\n      76\n      0.800000\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndf.groupby('typeface')['median stroke-width'].agg(['median', 'mean'])\n\n\n\n  \n    \n\n\n  \n    \n      \n      median\n      mean\n    \n    \n      typeface\n      \n      \n    \n  \n  \n    \n      display\n      0.732143\n      0.728145\n    \n    \n      serif\n      0.683333\n      0.700982\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nstroke_width_viz('display-76px.png');\n\n8\n\n\n\n\n\n\n\n\n\n\nstroke_width_viz('serif-76px.png');\n\n7"
  },
  {
    "objectID": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#final-thoughts",
    "href": "posts/2024-08-19-typefaceclassifier-avg-stroke-width/index.html#final-thoughts",
    "title": "Calculating the Average Stroke Width of Letters in a Text Image",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI enjoyed iterating this algorithm with Claude. In this non-ML baseline development process, I prioritize simpler code over higher accuracy. For average stroke width, I restricted input font sizes to 18-76px, excluding very small (8px) and very large (420px) sizes, to achieve a final metric that distinguishes typefaces in a way that matches my intuition.\nWhile observing the stroke width heat maps, I noticed that each letter’s bounding rectangle has varying amounts of negative (black) and positive (white) space, depending on the typeface. I plan to explore this next!\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html",
    "title": "Sentiment Classification with phi-3",
    "section": "",
    "text": "Show pip installs\n!pip install torch==2.3.1 -qq\n!pip install accelerate==0.31.0 -qq\n!pip install transformers==4.41.2 -qq\n!pip install huggingface_hub -qq\n!pip install datasets~=2.16.1 -qq\n!pip install plotly==5.19.0 -qq\n!pip install scikit-learn==1.2 -qq\n\n\n\n\nShow imports and setup\nimport gc\nimport pandas as pd\nimport numpy as np\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\nimport torch\n\nimport warnings\n#warnings.filterwarnings(\"ignore\")\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport time\n\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom fastcore.all import *\n\n#torch.set_default_device(\"cuda\")\ntorch.cuda.set_device(0)\n\nmodel = AutoModelForCausalLM.from_pretrained( \n    \"microsoft/Phi-3-mini-4k-instruct\",  \n    device_map=\"cuda\",  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n\npipe = pipeline( \n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n) \n\n# load dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\", \n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\n\n\n# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\nlabel_map = {i: label_text for i, label_text in enumerate(dataset.features[\"label\"].names)}\n\ndef add_label_text(example):\n    example[\"label_text\"] = label_map[example[\"label\"]]\n    return example\n\ndataset = dataset.map(add_label_text)\n\nprint(dataset)\n\n\n\nShow add_prompt and generate_responses functions\ndef add_prompt(item, prompt):\n        item['prompt'] = prompt.format(text=item['sentence'])\n        return item\n    \ndef generate_responses(dataset, prompt):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    # check that the prompt is correctly formatted\n    print(dataset[0]['prompt'])\n    print('---------')\n    \n    for row in dataset:\n        messages = [  \n            {\"role\": \"user\", \"content\": row['prompt']},\n        ] \n\n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\n\nShow generate_response function\ndef generate_response(prompt):\n    messages = [  \n        {\"role\": \"user\", \"content\": prompt},\n    ] \n\n    generation_args = { \n        \"max_new_tokens\": 2, \n        \"return_full_text\": False, \n        \"temperature\": 0.1, \n        \"do_sample\": True, \n    } \n\n    output = pipe(messages, **generation_args) \n    return output[0]['generated_text']\n\n\n\n\nShow make_cm function\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n    \n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['responses'], labels=['negative', 'neutral', 'positive', 'other'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive', 'other'])\n    \n    # I chose 8x8 so it fits on one screen but still is large\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp.plot(ax=ax,text_kw={'fontsize': 16}, cmap='Blues', colorbar=False);\n    \n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(18)\n    ax.yaxis.label.set_fontsize(18)\n    \n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=16)\n    ax.tick_params(axis='x', labelsize=16)\n\n\n\n\nShow ds_subset function\ndef ds_subset(dataset, exclude_idxs, columns=[0, 1, 2]):\n    idxs = list(range(len(dataset)))\n    idxs = [x for x in idxs if x not in exclude_idxs]\n    ddf = dataset.to_pandas()\n    new_ds = Dataset.from_pandas(ddf.iloc[idxs, columns])\n    return new_ds"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#background",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#background",
    "title": "Sentiment Classification with phi-3",
    "section": "Background",
    "text": "Background\nIn this notebook I’ll use Phi-3-mini-4k-instruct to classify sentiment in the financial_phrasebank dataset. In previous notebooks I have performed sentiment classification with phi-2 and the Claude series.\nThis notebook is part of a series of blog posts for a project I’m working called TinySentiment where I’m experimenting with tiny models to improve their ability to classify sentiment in the financial_phrasebank dataset. I was inspired to do so after reading this blog post and this corresponding notebook by Moritz Laurer as part of a fastai study group last year.\nHere are the results from my experiments so far (**the best-performing prompt from this notebook):\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nphi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\n**phi-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\n\nHere are the per-prompt results from this notebook (phi-3):\n\n\n\nprompt\nstrategy\naccuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n47.00%\n98% (298/303)\n16% (220/1391)\n96% (546/570)\n\n\nB\n0-Shot\n73.23%\n99% (300/303)\n59% (821/1391)\n94% (537/570)\n\n\nC\n0-Shot\n66.25%\n99% (300/303)\n47% (650/1391)\n96% (550/570)\n\n\nD\n0-Shot\n49.65%\n99% (301/303)\n19% (269/1391)\n97% (554/570)\n\n\nE\n0-Shot\n72.44%\n99% (299/303)\n58% (803/1391)\n94% (538/570)\n\n\nF\n3-Shot\n82.62%\n98% (297/302)\n73% (1009/1390)\n99% (562/569)\n\n\nG\n6-Shot\n76.13%\n98% (297/302)\n62% (865/1387)\n98% (557/569)\n\n\nH\n3-Shot\n83.10%\n98% (296/302)\n73% (1021/1390)\n99% (562/569)\n\n\nI\n3-Shot\n81.16%\n98% (295/302)\n70% (977/1390)\n99% (563/569)\n\n\nJ\n3-Shot\n84.61%\n97% (294/302)\n76% (1055/1390)\n99% (564/569)\n\n\nK\n3-Shot\n83.19%\n98% (295/302)\n74% (1024/1390)\n99% (562/569)\n\n\nL\n3-Shot w/System Prompt\n87.35%\n98% (295/302)\n81% (1120/1390)\n98% (560/569)\n\n\nM\n3-Shot w/System Prompt\n88.41%\n97% (292/302)\n83% (1149/1390)\n98% (558/569)\n\n\nN\n3-Shot w/System Prompt\n88.28%\n97% (293/302)\n82% (1142/1390)\n99% (561/569)\n\n\nO\n3-Shot w/System Prompt\n88.59%\n98% (296/302)\n82% (1145/1390)\n99% (562/569)\n\n\nP\n3-Shot w/System Prompt\n90.00%\n97% (294/302)\n85% (1179/1390)\n99% (562/569)\n\n\nQ\n20-Shot w/System Prompt\n91.71%\n97% (291/299)\n89% (1226/1379)\n96% (541/566)\n\n\nR\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-a",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-a",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt A",
    "text": "Prompt A\nThe HuggingFace model card for Phi-3 Mini-4K-Instruct says:\n\nGiven the nature of the training data, the Phi-3 Mini-4K-Instruct model is best suited for prompts using the chat format\n\nSo, the first prompt I’ll try will be a simple instruction:\n\npromptA = \"\"\"Label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\n\ntext = dataset[1][\"sentence\"]\ntext\n\n\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\"\n\n\n\nformatted_prompt = promptA.format(text=text)\nprint(formatted_prompt)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\n\n\n\ngenerate_response(formatted_prompt)\n\nYou are not running the flash-attention implementation, expect numerical differences.\n\n\n' Negative'\n\n\n\n%time generate_response(formatted_prompt)\n\nCPU times: user 64.3 ms, sys: 87 µs, total: 64.4 ms\nWall time: 63.5 ms\n\n\n' Negative'\n\n\n\n%timeit -n 10 generate_response(formatted_prompt)\n\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\n60.1 ms ± 926 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nGood–at least it works! Although it looks like I’ll have to strip the outputs of whitespace and convert them to lowercase. It takes about 0.06 seconds to generate the response, so it should take about 2-3 minutes to run inference on the whole dataset.\n\ndf, acc = generate_responses(dataset, promptA)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n---------\n\n\nAs expected, the model does not perform well with this prompt. It actually performs worse than phi-2 (58%).\n\nacc\n\n0.46996466431095407\n\n\nThe model actually performs quite well for negative (298/393) and positive (546/570) sentiment, but performs badly for neutral sentiment (220/1391).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_A.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-b",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-b",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt B",
    "text": "Prompt B\nI’ll repeat the instruction after the sentence and see if that improves the performance (as it did for phi-2).\n\npromptB = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptB)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\n---------\n\n\nAs expected, repeating the instruction boosts the accuracy significantly. This is something I learned to do in a fastai study group.\n\nacc\n\n0.7323321554770318\n\n\nThe model improves its performance for negative and neutral sentences, while its performance for positive sentences worsens.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_B.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-c",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-c",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt C",
    "text": "Prompt C\nI’ll add some introductory text to the prompt to see if that improves the model’s performance:\n\npromptC = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptC)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\n---------\n\n\nUnlike phi-2 (although I applied this introductory text to few-shot prompting, not 0-shot) this worsens the model’s overall accuracy.\n\nacc\n\n0.6625441696113075\n\n\nInterestingly, this prompt improves the positive sentiment true positive rate (550/570 > 537/570).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_C.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-d",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-d",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt D",
    "text": "Prompt D\nI’ll try another prompt language adjustment to Prompt B: I’ll replace “label” with “Respond”.\n\npromptD = \"\"\"Instruct: Respond with only one of these words: negative, positive, or neutral\nTEXT: {text}\nRespond with only one of these words: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptD)\n\n\n\n\nInstruct: Respond with only one of these words: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral\n---------\n\n\nYikes! The overall accuracy plummets to 49.6%.\n\nacc\n\n0.49646643109540634\n\n\nInterestingly, the true positive rate for negative and positive sentences increses, but for neutral significantly decreases.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_D.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-e",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-e",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt E",
    "text": "Prompt E\nAnother adjustment that improved phi-2’s performance was to add a period after the instruction. I’ll see if doing so improves phi-3’s performance.\n\npromptE = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral.\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral.\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptE)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral.\n---------\n\n\nYou are not running the flash-attention implementation, expect numerical differences.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\nThe accuracy drops from 73% to 72%.\n\nacc\n\n0.7243816254416962\n\n\nThe true positive rate for negative and neutral sentiment drops a bit while for positive sentiments increases (538/570 > 537/570).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_E.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-f",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-f",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt F",
    "text": "Prompt F\nI’ll now move on to few-shot prompting to see if I can improve on the best overall accuracy so far (73.1%). To do so, I’ll create a new helper function (since the chat template handles few-shot prompt as multiple query-response exchanges between user and assistant).\n\n\nShow few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    count = 0\n    for row in dataset:\n        count += 1\n        messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        if count == 1: print(messages)\n        \n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\nexclude_idxs = [0, 1, 292]\n\n\npromptF_ds = ds_subset(dataset, exclude_idxs)\npromptF_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative')]\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptB, examples)\n\n\n\n\n[{'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral'}]\n\n\nNice! Few-shot prompting improves the overall accuracy by almost 10% (from 73.06% to 82.62%).\n\nacc\n\n0.8261831048208758\n\n\nCompared to prompt B, this prompt yields worse results for negative sentiment (297 < 300), and better results for neutral (1009 > 817) and positive sentiment (562 > 537).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_F.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-g",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-g",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt G",
    "text": "Prompt G\nI’ll now try a 6-Shot prompt using the examples that were best-performing for phi-2.\n\nexclude_idxs=[0, 1, 292, 37, 38, 39]\npromptG_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptG_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2258\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples[0], len(examples)\n\n(('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n 6)\n\n\n\ndf, acc = few_shot_responses(promptG_ds, promptB, examples)\n\n\n\n\n[{'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nlabel the TEXT with a single word: negative, positive, or neutral'}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': 'Instruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral'}]\n\n\nSurprisingly, the overall accuracy drops from 82.6% (3-Shot) to 76.13% (6-Shot).\n\nacc\n\n0.7612931798051373\n\n\nThe model performs the same for negative sentences (297/302), much worse for neutral sentences (865 < 1009), and slightly worse for positive sentences (557 < 562).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_G.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-h",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-h",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt H",
    "text": "Prompt H\nI’ll return to the 3-Shot prompt (82.62%) and see if I can improve it by adjusting the language. First, I’ll add some introductory text to the start of the prompt. Note that this did not improve the 0-Shot performance.\n\npromptH = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\nexclude_idxs = [0, 1, 292]\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative')]\n\n\n\npromptF_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptH, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral\"}]\n\n\nI get a tiny boost (0.48%) with this approach. I’ll take it!\n\nacc\n\n0.8310482087571871\n\n\nCompared to the previous 3-Shot prompt (82.62%), this prompt results in a lower true positive rate for negative sentences (296 < 297), a slightly higher rate for neutral sentences (1021 > 1009), and the same rate for positive sentences (562).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_H.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-i",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-i",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt I",
    "text": "Prompt I\nGiven my success with the language adjustment for 3-Shot prompting, I’ll make another one: adding a period to the end of each instruction sentence.\n\npromptI = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\nInstruct: label the following TEXT with a single word: negative, positive, or neutral.\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral.\"\"\"\n\n\ndf, acc = few_shot_responses(promptF_ds, promptI, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral.\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nlabel the TEXT with a single word: negative, positive, or neutral.\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral.\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nlabel the TEXT with a single word: negative, positive, or neutral.\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral.\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nlabel the TEXT with a single word: negative, positive, or neutral.\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nInstruct: label the following TEXT with a single word: negative, positive, or neutral.\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nlabel the TEXT with a single word: negative, positive, or neutral.\"}]\n\n\nSurprisingly, adding periods at the end of the instructions actually reduces the overall accuracy (81.16%).\n\nacc\n\n0.8115877930119416\n\n\nThe true positive rate drops for negative (295 < 296) and neutral (977 < 1021) but increases for positive (563 > 562) when compared to the best-performing prompt.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_I.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-j",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-j",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt J",
    "text": "Prompt J\nI’ll try another language adjustment for the few-shot prompt: replacing “Instruct: label the TEXT with a single word: negative, positive, or neutral.” with “Respond with a single word: negative, positive, or neutral”.\n\npromptJ = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\nRespond with a single word: negative, positive, or neutral\nTEXT: {text}\nRespond with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = few_shot_responses(promptF_ds, promptJ, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nYes!! I achieve my best result so far with 84.6% overall accuracy.\n\nacc\n\n0.8460858027421495\n\n\nCompared to Prompt H, this prompt yields a worse negative sentiment true positive rate (294 < 296), but a better rate for neutral (1055 > 1021) and positive (564 > 562).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_J.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-k",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-k",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt K",
    "text": "Prompt K\nAnother language adjustment that worked for phi-2: adding the phrase “if you’re not sure, respond with neutral”.\n\npromptK = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\nTEXT: {text}\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\"\"\"\n\n\ndf, acc = few_shot_responses(promptF_ds, promptK, examples)\n\n\n\n\n[{'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral. If you're not sure, respond with neutral.\"}]\n\n\nThis does not improve the overall accuracy.\n\nacc\n\n0.8319327731092437\n\n\nCompared to Prompt J, this prompt results in a higher true positive rate for negative sentiment (295 > 294), but a lower rate for neutral (1024 < 1055) and positive sentiment (562 < 564).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_K.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-l",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-l",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt L",
    "text": "Prompt L\nI’ll see if adding a system prompt improves performance.\n\n\nShow updated few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples, sp=False):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    count = 0\n    for row in dataset:\n        count += 1\n        \n        if sp:\n            messages = [{'role': 'system', 'content': 'You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification.'}\n                       ] + few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        else:\n            messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        if count == 1: print(messages)\n        \n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptJ, examples, sp=True)\n\n\n\n\n[{'role': 'system', 'content': 'You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification.'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nAdding this sytem prompt increases the accuracy from 84.6% to 87.4%!\n\nacc\n\n0.8735072976559045\n\n\nCompared to Prompt J (without system prompt) adding this system prompt results in a higher true positive rate for negative (295 > 294) and neutral (1120 > 1055) sentiment but a lower rate for positive sentiment (560 < 564).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_L.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-m",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-m",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt M",
    "text": "Prompt M\nI’ll add the following phrase to the system prompt:\n\nIf you’re not sure, respond with neutral.\n\n\n\nShow updated few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples, sp=False):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    count = 0\n    for row in dataset:\n        count += 1\n        \n        if sp:\n            messages = [{'role': 'system', 'content': sp}] + few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        else:\n            messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        if count == 1: print(messages)\n        \n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If you're not sure, respond with neutral.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If you're not sure, respond with neutral.\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptJ, examples, sp=sp)\n\n\n\n\n[{'role': 'system', 'content': \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If you're not sure, respond with neutral.\"}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nExcellent—the new system prompt results in the best-so-far overall accuracy (88.4%).\n\nacc\n\n0.8841220698805838\n\n\nCompared to the last system prompt, this one yields a lower true positive rates for negative sentiment (292 < 295) and positive sentiment (558 < 560) but a higher rate for neutral (1149 > 1120) sentiment.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_M.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-n",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-n",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt N",
    "text": "Prompt N\nI’ll iterate on the system prompt. I’ll replace:\n\nIf you’re not sure, respond with neutral.\n\nwith\n\nIf the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptJ, examples, sp=sp)\n\n\n\n\n[{'role': 'system', 'content': 'You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money is not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nThe updated system prompt causes the overall accuracy to drop a tiny bit.\n\nacc\n\n0.8827952233524989\n\n\nThe true positive rate for negative (293 > 292) and positive (561 > 558) sentiment actually increases, but decreases for neutral sentiment (1142 < 1149) when compare to the previous system prompt.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_N.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-o",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-o",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt O",
    "text": "Prompt O\nI’ll iterate on the best-performing (88.4%) Prompt M by adding more detail to how the model should handle neutral sentiment (based on a suggestion by Claude):\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral.\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptJ, examples, sp=sp)\n\n\n\n\n[{'role': 'system', 'content': 'You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral.'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nYou are not running the flash-attention implementation, expect numerical differences.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\nNice! This modification resulted in the best overall accuracy so far, 88.6%.\n\nacc\n\n0.885891198584697\n\n\nThe true positive rate increased for negative and positive sentiments and decreased a bit for neutral sentiment.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_O.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-p",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-p",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt P",
    "text": "Prompt P\nGiven the improvement with a more nuanced system prompt, I’ll continue adding to the prompt.\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\n\n\n\ndf, acc = few_shot_responses(promptF_ds, promptJ, examples, sp=sp)\n\n\n\n\n[{'role': 'system', 'content': \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\"}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nAwesome!! I’ve broken the 90% mark with that system prompt.\n\nacc\n\n0.9000442282176029\n\n\nCompared to the previous best-performing prompt O, this prompt yields fewer correct negative sentences (294 < 296), more correct neutral sentences (1179 > 1145) and thes same number of correct positive sentences (562).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_P.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-q",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-q",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt Q",
    "text": "Prompt Q\nI’ll now increase the number of examples to a significantly larger number (20) and see if that improves performance.\n\nexclude_idxs = [1, 2, 3, 4, 292, 293, 294, 347, 0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283]\n\n\npromptQ_ds = ds_subset(dataset, exclude_idxs=exclude_idxs, columns=[0, 1, 2])\npromptQ_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2244\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples[0], len(examples)\n\n((\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n 20)\n\n\n\ndf, acc = few_shot_responses(promptQ_ds, promptJ, examples, sp=sp)\n\n\n\n\n[{'role': 'system', 'content': \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\"}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: However , the growth margin slowed down due to the financial crisis .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: 2009 3 February 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR46m in the fourth quarter of 2009 from a year-earlier profit of EUR45m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The contract value amounts to about EUR11m , the company added .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nAnother improvement! I am now pretty close to phi-2’s accuracy (91.94%).\n\nacc\n\n0.9171122994652406\n\n\nCompared to Prompt P, this prompt yields a lower true positive rate for negative (291 < 294) and positive (541 < 562) sentiment but more than compensates for that with an increased true positive rate for neutral sentiment (1226 > 1179).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_Q.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-r",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#prompt-r",
    "title": "Sentiment Classification with phi-3",
    "section": "Prompt R",
    "text": "Prompt R\nI’ll increase the number of examples and see if that improves performance.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, # positive\n    292, 293, 294, 347, 348, 349, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283, 284, 285, 286, 287, 288, 289 # neutral\n]\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples[0], len(examples)\n\n((\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n 30)\n\n\n\npromptR_ds = ds_subset(dataset, exclude_idxs=exclude_idxs, columns=[0, 1, 2])\npromptR_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n})\n\n\n\ndf, acc = few_shot_responses(promptR_ds, promptJ, examples, sp=sp)\n\n\n\n\n[{'role': 'system', 'content': \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\"}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'positive'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: However , the growth margin slowed down due to the financial crisis .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: 2009 3 February 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR46m in the fourth quarter of 2009 from a year-earlier profit of EUR45m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: ( ADPnews ) - Feb 3 , 2010 - Finland-based steel maker Rautaruukki Oyj ( HEL : RTRKS ) , or Ruukki , said today it slipped to a larger-than-expected pretax loss of EUR 46 million ( USD 64.5 m ) in the fourth quarter of 2009 from a\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Result before taxes decreased to nearly EUR 14.5 mn , compared to nearly EUR 20mn in the previous accounting period .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'negative'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Finnish Talvivaara Mining Co HEL : TLV1V said Thursday it had picked BofA Merrill Lynch and JPMorgan NYSE : JPM as joint bookrunners of its planned issue of convertible notes worth up to EUR250m USD332m .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The mall is part of the Baltic Pearl development project in the city of St Petersburg , where Baltic Pearl CJSC , a subsidiary of Shanghai Foreign Joint Investment Company , is developing homes for 35,000 people .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Vacon controls a further 5 % of the company via investment fund Power Fund I. EUR 1.0 = USD 1.397\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: 4 ) Complete name of the shareholder : Otto Henrik Bernhard Nyberg 5 ) Further information : The amount of shares now transferred corresponds to 5.68 % of the total number of shares in Aspo Plc. .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: It has some 30 offices worldwide and more than 90 pct of its net sales are generated outside Finland .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The contract value amounts to about EUR11m , the company added .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The business to be divested generates consolidated net sales of EUR 60 million annually and currently has some 640 employees .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The company generates net sales of about 600 mln euro $ 775.5 mln annually and employs 6,000 .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The contract covers the manufacturing , surface-treatment and installation of the steel structures .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The order also includes start-up and commissioning services .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: The phones are targeted at first time users in growth markets .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Tielinja generated net sales of 7.5 mln euro $ 9.6 mln in 2005 .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Tikkurila Powder Coatings has some 50 employees at its four paint plants , which generated revenues of EUR2 .4 m USD3 .3 m in 2010 .\\nRespond with a single word: negative, positive, or neutral\"}, {'role': 'assistant', 'content': 'neutral'}, {'role': 'user', 'content': \"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\\nRespond with a single word: negative, positive, or neutral\\nTEXT: Consolidated net sales increased 16 % to reach EUR74 .8 m , while operating profit amounted to EUR0 .9 m compared to a loss of EUR0 .7 m in the prior year period .\\nRespond with a single word: negative, positive, or neutral\"}]\n\n\nYou are not running the flash-attention implementation, expect numerical differences.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\nAnd with that I have surpassed phi-2!\n\nacc\n\n0.9279319606087735\n\n\nCompared to Prompt P, this prompt yields a lower true positive rate for negative (290 < 294) and positive (499 < 562) sentiment but like Prompt Q, more than compensates for that with an increased true positive rate for neutral sentiment (1284 > 1179).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/phi-3_R.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#running-inference-with-the-best-prompt-multiple-times",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#running-inference-with-the-best-prompt-multiple-times",
    "title": "Sentiment Classification with phi-3",
    "section": "Running Inference with the Best Prompt Multiple Times",
    "text": "Running Inference with the Best Prompt Multiple Times\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, # positive\n    292, 293, 294, 347, 348, 349, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283, 284, 285, 286, 287, 288, 289 # neutral\n]\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n30\n\n\n\npromptR_ds = ds_subset(dataset, exclude_idxs=exclude_idxs, columns=[0, 1, 2])\npromptR_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n})\n\n\n\npromptJ = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\nRespond with a single word: negative, positive, or neutral\nTEXT: {text}\nRespond with a single word: negative, positive, or neutral\"\"\"\n\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\n\n\n\n\nShow test_gen function\ndef test_gen(examples, sp):\n    responses = []\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": promptJ.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n\n\n    messages = [{'role': 'system', 'content': sp}] + few_shot_examples + [{\"role\": \"user\", \"content\": promptJ.format(text=dataset[0]['sentence'])}]\n\n\n    generation_args = { \n        \"max_new_tokens\": 2, \n        \"return_full_text\": False, \n        \"temperature\": 0.1, \n        \"do_sample\": True, \n    } \n\n    response = pipe(messages, **generation_args) \n    responses.append(response[0]['generated_text'].strip().lower())\n    return responses\n\n\nThe model takes about 1.64 seconds to generate a response for a single dataset item, or about 1.01 hours for the 2234 items (on a Paperspace Free-A4000). Given the 6 hour limit, the max I can do is run inference on the dataset 5 times.\n\n%timeit -n 10 test_gen(examples, sp)\n\n1.64 s ± 3.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nShow few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples, sp):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    for row in dataset:\n        messages = [{'role': 'system', 'content': sp}] + few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        generation_args = { \n            \"max_new_tokens\": 2, \n            \"return_full_text\": False, \n            \"temperature\": 0.1, \n            \"do_sample\": True, \n        } \n\n        response = pipe(messages, **generation_args) \n        responses.append(response[0]['generated_text'].strip().lower())\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    return df, acc\n\n\n\naccs = []\nfor _ in range(5):\n    df, acc = few_shot_responses(promptR_ds, promptJ, examples, sp)\n    accs.append(acc)\n\nThe accuracy for this prompt is consistently around 92.7%.\n\npd.Series(accs).describe()\n\ncount    5.000000\nmean     0.928111\nstd      0.000981\nmin      0.927484\n25%      0.927484\n50%      0.927484\n75%      0.928380\nmax      0.929722\ndtype: float64"
  },
  {
    "objectID": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#final-thoughts",
    "href": "posts/2024-09-12-tinysentiment-phi-3-sentiment-classification/index.html#final-thoughts",
    "title": "Sentiment Classification with phi-3",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere is a summary of results including phi-2, phi-3.5, and the Claude family:\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nphi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nphi-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\n\nHere is a summary of results from this notebook:\n\n\n\nprompt\nstrategy\naccuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n47.00%\n98% (298/303)\n16% (220/1391)\n96% (546/570)\n\n\nB\n0-Shot\n73.23%\n99% (300/303)\n59% (821/1391)\n94% (537/570)\n\n\nC\n0-Shot\n66.25%\n99% (300/303)\n47% (650/1391)\n96% (550/570)\n\n\nD\n0-Shot\n49.65%\n99% (301/303)\n19% (269/1391)\n97% (554/570)\n\n\nE\n0-Shot\n72.44%\n99% (299/303)\n58% (803/1391)\n94% (538/570)\n\n\nF\n3-Shot\n82.62%\n98% (297/302)\n73% (1009/1390)\n99% (562/569)\n\n\nG\n6-Shot\n76.13%\n98% (297/302)\n62% (865/1387)\n98% (557/569)\n\n\nH\n3-Shot\n83.10%\n98% (296/302)\n73% (1021/1390)\n99% (562/569)\n\n\nI\n3-Shot\n81.16%\n98% (295/302)\n70% (977/1390)\n99% (563/569)\n\n\nJ\n3-Shot\n84.61%\n97% (294/302)\n76% (1055/1390)\n99% (564/569)\n\n\nK\n3-Shot\n83.19%\n98% (295/302)\n74% (1024/1390)\n99% (562/569)\n\n\nL\n3-Shot w/System Prompt\n87.35%\n98% (295/302)\n81% (1120/1390)\n98% (560/569)\n\n\nM\n3-Shot w/System Prompt\n88.41%\n97% (292/302)\n83% (1149/1390)\n98% (558/569)\n\n\nN\n3-Shot w/System Prompt\n88.28%\n97% (293/302)\n82% (1142/1390)\n99% (561/569)\n\n\nO\n3-Shot w/System Prompt\n88.59%\n98% (296/302)\n82% (1145/1390)\n99% (562/569)\n\n\nP\n3-Shot w/System Prompt\n90.00%\n97% (294/302)\n85% (1179/1390)\n99% (562/569)\n\n\nQ\n20-Shot w/System Prompt\n91.71%\n97% (291/299)\n89% (1226/1379)\n96% (541/566)\n\n\nR\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\n\nI ran inference for phi-3 and phi-3.5 in separate notebooks at the same time, so I have shared final thoughts for both:\n\nFew-shot prompting in a chat format is a different experience: The sentence/label pairs have to be presented as a multi-turn conversation. For a large number of examples, this can lead to running out of GPU memory (as it did for 30-Shot prompting with phi-3.5).\nFew-shot example proportion matters: I used a higher proportion of neutral examples in my 20-shot prompt since the majority of the dataset is made up of neutral sentences. Determining whether the proportion I used is optimal would require further experimentation.\n30-Shot phi-3 surpassed 6-Shot phi-2: Although it took more experimentation than I expected to do so. I was honestly expecting to try a couple of prompts and easily beat phi-2. I did not try a 30-Shot prompt with phi-2, so it may have performed equally well (or better). I’m not convinced that 24 more examples and 50% more parameters to yield ~0.8% more accuracy is worth it.\n30-Shot phi-3 surpassed 3-Shot Claude-3-Haiku: Again, I did not try 30-Shot prompting with Haiku, so it very well could have performed better. Regardless, I was not expecting phi-3 to beat Haiku’s score.\nThe best performing prompt suffers from a low true positive rate for positive sentiments: Although the 30-Shot phi-3 prompt achieved the highest true positive rate (TPR) for neutral sentences (94%), it had the lowest TPR for positive sentences (88%). It’s unclear if this is due to the imbalance between positive and neutral examples, since the TPR for negative sentiment is high (98%) despite having the same number of examples (6) as positive.\nphi-3 performed differently than phi-3.5: phi-3 performed well with a system prompt while phi-3.5 did not. On the other hand, phi-3.5 performed better with 0-Shot prompting than phi-3 (results not shown here). Future work: My next step is to run inference on this dataset using the Qwen2-1.5B model. After that, I’ll analyze the errors, especially for sentences that a majority of models classified incorrectly. With prompt engineering, there is potentially unlimited future work. Before I finish this project, I’ll try 30-Shot prompts for phi-2 and Haiku to see if they can beat phi-3’s 92.79% overall accuracy (and maybe even phi-3.5’s 93.94% accuracy).\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html",
    "title": "How Does Stable Diffusion Work?",
    "section": "",
    "text": "In this blog post, I’ll review the concepts introduced in the second half of the Lesson 9 video from the fastai course (Part 2: Deep Learning Foundations to Stable Diffusion). Note that I will not be covering any of the math of Stable Diffusion in this blog post. As Jeremy says in the video:\n\nthe way Stable Diffusion is normally explained is focused very much on a particular mathematical derivation. We’ve been developing a totally new way of thinking about Stable Diffusion and I’m going to be teaching you that. It’s mathematically equivalent [to other approaches] but it’s actually conceptually much simpler [and it can take you in really innovative directions].\n\nI’ll start with the main takeaway from this lesson, which is this table that shows the three types of models involved in stable diffusion, the inputs they take and the outputs they produce:\n\n\n\nModel\nInputs\nOutputs\n\n\n\n\nU-Net\nSomewhat Noisy Latents\nNoise\n\n\nVAE’s Decoder\nSmall Latents Tensor\nLarge Image\n\n\nCLIP Text Encoder\nText\nEmbedding\n\n\n\n\nThe noise predicted by the U-Net (which receives as input somewhat noisy latents, text embeddings generated by the CLIP Text Encoder and a time step) is (iteratively) scaled and subtracted from the somewhat noisy latents to create denoised latents which are input to the VAE’s Decoder, which reconstructs from them larger images."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#the-magic-api",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#the-magic-api",
    "title": "How Does Stable Diffusion Work?",
    "section": "The Magic API",
    "text": "The Magic API\nWe start by considering some blackbox web API (some “magic API”) that takes as inputs images of handwritten digits and outputs the probability that the inputs are handwritten digits. In other words, this magic API answers the question: what’s the probability that this is an image of a handwritten digit?\nLet’s consider this API to be some function \\(f\\).\n\n\n\nA magic API which predicts the probability that the input image is a handwritten digit"
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#varying-the-inputs-fo-f",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#varying-the-inputs-fo-f",
    "title": "How Does Stable Diffusion Work?",
    "section": "Varying the Inputs fo \\(f\\)",
    "text": "Varying the Inputs fo \\(f\\)\nIn the case of MNIST we have 28x28 = 784 pixels (or variables) in our input. Changing the value of each of these pixels will change the probability of it being a handwritten digit.\nFor example, digits usually don’t have dark pixels near the bottom corners. If we lighten such a pixel (highlighted in red below) and pass it through the function \\(f\\), the probability of it being a handwritten digit will slightly improve (e.g. from 0.7 to 0.707).\n\n\n\nChanging a pixel value to improve the probability that the image is a handwritten digit\n\n\nWe can do this for each pixel: determine whether making it lighter or darker makes it more like a handwritten digit."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#the-gradient-of-the-loss-with-respect-to-the-pixels",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#the-gradient-of-the-loss-with-respect-to-the-pixels",
    "title": "How Does Stable Diffusion Work?",
    "section": "The Gradient of the Loss With Respect to the Pixels",
    "text": "The Gradient of the Loss With Respect to the Pixels\nThere exists a loss function that is a function of the weights of a neural net (in our case, our “magic API” or function \\(f\\)) and the pixel values \\(X\\):\n\\[\\text{loss} = g(w,X)\\]\nThis loss function could be the MSE (Mean Squared Error) between our targets and predictions but for now just assume it’s some function \\(g\\).\nWhat happens to the loss as we change \\(X\\)? Our \\(X\\) consists of 28x28=784 pixels, and our loss function can change with respect to each one of those pixels (also known as partial derivatives):\n\n\\[\\frac{\\partial{\\text{loss}}}{\\partial{X_{(1,1)}}}, \\frac{\\partial{\\text{loss}}}{\\partial{X_{(1,2)}}}, \\frac{\\partial{\\text{loss}}}{\\partial{X_{(1,3)}}}, ..., \\frac{\\partial{\\text{loss}}}{\\partial{X_{(28,28)}}}\\]\n\nWe can rewrite this compactly as:\n\\[\\nabla_X \\text{loss}\\]\nWhich we read as: the gradient of the loss with respect to \\(X\\).\nWe can change the pixel values according to this gradient to get our image looking closer to a handwritten digit. In practice, we subtract the gradient (multiplied by some constant \\(c\\)) from the image pixel data, and do this iteratively (as illustrated below), calculating a new gradient each time:\n\n\n\nIteratively changing pixel values (using the gradient of the loss with respect to pixels) to become more like a handwritten digit\n\n\nIf we have access to our magic function \\(f\\), we can generate images that look like handwritten digits. And assuming that magic API is using python, we don’t even need access to \\(f\\), we just need access to f.backward and X.grad."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#creating-the-magic-function-f-the-u-net",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#creating-the-magic-function-f-the-u-net",
    "title": "How Does Stable Diffusion Work?",
    "section": "Creating the Magic Function \\(f\\) (the U-Net)",
    "text": "Creating the Magic Function \\(f\\) (the U-Net)\n\nGenerally, in this course, when there’s some magic blackbox that we want to exist and it doesn’t exist, we create a Neural Net and we train it. We want to train a Neural Net that tells us which pixels to change to a make an image look more like a handwritten digit.\n\nThe training data (noisy images of digits) and targets (the amount of noise added) for this Neural Net I’ve illustrated as the following:\n\n\n\nA neural net that predicts the noise that eneds to be removed to leave behind something that looks more like a digit\n\n\nThe loss function is the MSE between the predicted noise \\(\\hat{n}\\) and actual noise \\({n}\\) (\\(N\\) is the number of images) which is used then to update the weights of the neural net.\nHow much do we have the change an image (noisy digit) by to make it more digit-like? We have to subtract the noise!\nWe end up with a neural net that can take as an input pure noise and predict the amount of noise that needs to be removed so that what is left behind looks the most like a handwritten digit. To illustrate:\n\n\n\nA neural net that predicts the noise that needs to be removed to leave behind something that looks more like a digit\n\n\nThis process of predicting and subtracting the noise (multiplied by a constant) that needs to be removed occurs multiple times, each time getting closer to leaving behind the pixels for a digit.\nThe neural net that we use for this is the U-Net."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#when-you-dont-have-a-room-full-of-tpus-the-autoencoder",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#when-you-dont-have-a-room-full-of-tpus-the-autoencoder",
    "title": "How Does Stable Diffusion Work?",
    "section": "When You Don’t Have a Room Full of TPUs: The Autoencoder",
    "text": "When You Don’t Have a Room Full of TPUs: The Autoencoder\nIn practice, we want to generate more than just 28x28=784 pixels of handwritten digits. We want to generate 512x512x3=786432 pixels of full color, high resolution images. Training a model on millions of these images will take a lot of time and compute. How do we do this more efficiently?\nWe already know that lossy compression can take place with images, like JPEGs, where the size of the image file (in bytes) is much smaller than the bytes of actual pixels (height pixels x width pixels x number of channels).\nWe can compress large images into small latents using a neural network (with convolutions and ResNet blocks), and then reconstruct the images from these small latents (using inverse convolutions):\n\n\n\nCompressing images into latents (and reconstructing latents back into images) using a neural net with convolutions and ResNet blocks\n\n\nThis neural net compresses 786,432 pixels into 16,384 pixels, a 48x compression!\nDuring training, we input 512x512x3 images and the neural net will initially output 512x512x3 random noise (as the weights are randomly instantiated). The loss function is the MSE between the input images and the output images. As the loss decreases, the output images look closer to the inputs. This model, something that gives back what you give it, is called an autoencoder.\nThe beauty of this model is when you split it in “half” into an encoder (green) and a decoder (red):\n\n\n\nThe encoder (highlighted in green) and the decoder (highlighted in red) of the autoencoder\n\n\nWe can feed full-size images to the encoder and it will output latents that are used as inputs to the U-Net (for training and inference).\nThe final denoised latents (from U-Net) become the inputs to the decoder which outputs full-size images.\nIn this way we can train the U-Net on 48x less data because we are able to recover most of the information with our trained autoencoder’s decoder!\nThe autoencoder that we will use is called a VAE (Variational Autoencoder).\nThe use of latents is entirely optional:\n\ngenerally speaking, we would rather not use more compute than necessary, so, unless you’re trying to sell the world a room full of TPUs, you would probably rather everybody was doing stuff in the thing that’s 48 times smaller. So the VAE is optional but it saves us a whole lot of time and a whole lot of money. So that’s good."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#encoding-a-cute-teddy-clip-contrastive-language-image-pre-training",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#encoding-a-cute-teddy-clip-contrastive-language-image-pre-training",
    "title": "How Does Stable Diffusion Work?",
    "section": "Encoding “A cute teddy”: CLIP (Contrastive Language-Image Pre-Training)",
    "text": "Encoding “A cute teddy”: CLIP (Contrastive Language-Image Pre-Training)\nHow could we modify our pipeline so that we could tell the U-Net that we wanted it to give us the noise to remove and leave behind not just any digit, but a particular digit, like 3?\nWe want to pass into the model (as input) “3” as a one-hot encoded vector so it predicts the noise we need to remove (to leave behind a “3”). There are 10 elements in this vector representing each possible digit in the MNIST dataset, 0-9:\n\n\n\nA neural net that predicts the noise that needs to be removed to leave behind something that looks more like a digit\n\n\nDuring training, in addition to passing in noisy digits, we pass in a one-hot encoded representation of that digit. The model thus learns what noise needs to be removed to leave a particular digit behind:\n\n\n\nA neural net that predicts the noise in an image (given a noisy image and guidance)\n\n\nThat’s a straightforward way to give it guidance. When the guidance gets more complex than single digits, one-hot encoding no longer works:\n\nwe can’t create every possible sentence that’s been uttered in the whole world and then create a one-hot encoded version of every sentence in the world\n\nThe solution? Embeddings!!\nWe can train two neural nets: one that takes in as inputs texts and outputs embeddings (vectors w/ numbers) and one that takes in as inputs images and also outputs embeddings.\n\n\n\nA neural net that takes in input text and outputs embeddings (text encoder) and a neural net that takes input images and outputs embedding (image encoder)\n\n\nFor each pair of text and image, we want the model to output text embeddings that are similar to the corresponding image’s embeddings:\n\n\n\nCosine similarity between image and text embeddings; we want the diagonals to be large and the off-diagonals to be small\n\n\nTo achieve this, we use something called contrastive loss (the “CL” in “CLIP”). Optimizing this loss means increasing the dot product between related image/text pairs (e.g. “a graceful swan” and the image of the swan) and decreasing the dot product between unrelated image/text pairs (e.g. “a graceful swan” and the fast.ai logo).\nThe result is a model where similar texts:\n\n“a graceful swan”\n“a beautiful swan”\n“such a lovely swan”\n\nwill produce similar embeddings as they correspond to similar images.\nThese two models put text and images into the same space; they are a multimodal set of models.\nWe can now embed “a cute teddy” and pass it to a U-Net (that is trained on input images and corresponding text embeddings) and it will return the noise that needs to be removed from the somewhat noisy latent to leave behind something that looks like a cute teddy."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#weird-and-confusing-time-steps-the-inference-process",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#weird-and-confusing-time-steps-the-inference-process",
    "title": "How Does Stable Diffusion Work?",
    "section": "Weird and Confusing “Time Steps”: The Inference Process",
    "text": "Weird and Confusing “Time Steps”: The Inference Process\nWhen we’re training the U-Net, we pick a random amount of noise to add to each input image (or latent). One way to pick it is to select a certain “time step” (an overhang from the mathematical formulation of diffusion) for which there is a corresponding amount of noise. A “noising schedule” will look something like this:\n\n\n\nA noising schedule that is monotonically decreasing as the number of time steps increase. This schedule can be used to determine how much noise to add during training.\n\n\nYou may also see the standard deviation of the noise being used referred to as the Greek letter beta (\\(\\beta\\)).\nAt inference time (generating a picture from pure noise) the model will create some hideous and random thing:\n\n\n\nA hideous random image generating in a few time steps\n\n\nWe multiply the predicted noise by a constant (a la learning rate, but for updating pixels, not weights) and subtract it from the pixels. We have to take these incremental steps toward fully denoising the image because our model didn’t train on the hideous image above, so it doesn’t know how to go in one step (at the time of this video) from hideous-random-thing to a high resolution image of something plausible.\nThe diffusion sampler is used to decide how much noise to add during training and how much noise to subtract during inference.\nIf you squint—diffusion samplers look like optimizers. We have tricks we can use (like momentum, or adaptive learning rate) for optimizers and fastai early research at the time showed that we can use similar ideas for diffusion.\nU-Nets traditionally also take as input the time step t. If a model is trained knowing how much noise is used, the better it will be at removing noise.\nJeremy thinks this premise is incorrect, because neural nets can very easily predict how noisy something is.\nIf you step passing the U-Net the time step t:\n\nthings stop looking like differential equations and they start looking more like optimizers. Early results suggest that when we re-think the whole thing as being about learning rates and optimizers, maybe it actually works better.\n\nIf we stop centering the concepts that are related to the mathematical formulation of diffusion, such as using the mathematically easy Mean Squared Error as loss, we can use something more sophisticated like perceptual loss to evaluate if our outputs resemble our targets (e.g. handwritten digits)."
  },
  {
    "objectID": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#final-thoughts",
    "href": "posts/2024-08-08-how-does-stable-diffusion-work/index.html#final-thoughts",
    "title": "How Does Stable Diffusion Work?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’ll finish this blog post by reiterating what Jeremy emphasized is the main takeaway from this lesson: understanding what the inputs and outputs are of the different models used for diffusion:\n\n\n\nModel\nInputs\nOutputs\n\n\n\n\nU-Net\nSomewhat Noisy Latents\nNoise\n\n\nVAE’s Decoder\nSmall Latents Tensor\nLarge Image\n\n\nCLIP Text Encoder\nText\nEmbedding\n\n\n\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-11-08-punctuation-cosine-similarity/index.html",
    "href": "posts/2024-11-08-punctuation-cosine-similarity/index.html",
    "title": "Comparing Cosine Similarity Between Embeddings of Semantically Similar and Dissimilar Texts with Varying Punctuation",
    "section": "",
    "text": "Show pip install and imports\n!pip install sentence-transformers -Uqq\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer"
  },
  {
    "objectID": "posts/2024-11-08-punctuation-cosine-similarity/index.html#background",
    "href": "posts/2024-11-08-punctuation-cosine-similarity/index.html#background",
    "title": "Comparing Cosine Similarity Between Embeddings of Semantically Similar and Dissimilar Texts with Varying Punctuation",
    "section": "Background",
    "text": "Background\nI was reading the ColBERT paper as part of a fastai study group and it mentions the following:\n\nAfter passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list. This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness.\n\nI’m not going to understand (or test) their hypothesis in full in this notebook but I am doing a tiny experiment to see how punctuation changes translate to embedding changes.\nStarting with a smaller model:\n\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\");\n\nI asked Claude for some examples of sentences where a comma would change its meaning and it gave me the following pair which I’m expanding upon in this notebook:\n\n“The woman said the judge is dishonest”\n“The woman, said the judge, is dishonest”\n\nIn the first sentence, the woman is saying that the judge is dishonest. In the second sentence by adding commas the meaning changes.\nI’ve also added some variants of the sentence using different punctuation.\n\nd1 = \"The woman said the judge is dishonest\"\nd2 = \"The woman, said the judge, is dishonest\"\nd3 = \"The woman said: the judge is dishonest\"\nd4 = 'The woman said: \"the judge is dishonest\"'\nd5 = 'The judge said: \"the woman is dishonest\"'\n\nq = \"Is the woman or the judge dishonest?\"\ns1 = \"The woman is dishonest\"\ns2 = \"The judge is dishonest\"\n\nd1_emb = emb_model.encode(d1, convert_to_tensor=True)\nd2_emb = emb_model.encode(d2, convert_to_tensor=True)\nd3_emb = emb_model.encode(d3, convert_to_tensor=True)\nd4_emb = emb_model.encode(d4, convert_to_tensor=True)\nd5_emb = emb_model.encode(d5, convert_to_tensor=True)\n\nq_emb = emb_model.encode(q, convert_to_tensor=True)\ns1_emb = emb_model.encode(s1, convert_to_tensor=True)\ns2_emb = emb_model.encode(s2, convert_to_tensor=True)\n\nThe most similar text to the question “Is the woman or the judge dishonest?”, by cosine similarity, is “The woman, said the judge, is dishonest”. The least similar is ‘The woman said: “the judge is dishonest”’. My guess is that the additional punctuation (: and \") causes this dissimilarity.\nq = \"Is the woman or the judge dishonest?\"\n\n(\n    F.cosine_similarity(q_emb, d2_emb, dim=0), # \"The woman, said the judge, is dishonest\"\n    F.cosine_similarity(q_emb, d1_emb, dim=0), # \"The woman said the judge is dishonest\"\n    F.cosine_similarity(q_emb, d3_emb, dim=0), # \"The woman said: the judge is dishonest\"\n    F.cosine_similarity(q_emb, d5_emb, dim=0), # 'The judge said: \"the woman is dishonest\"'\n    F.cosine_similarity(q_emb, d4_emb, dim=0), # 'The woman said: \"the judge is dishonest\"'\n)\n\n(tensor(0.9355),\n tensor(0.9292),\n tensor(0.9170),\n tensor(0.9149),\n tensor(0.8996))\n\n\nThe text\n\nThe woman is dishonest\n\nis most similar by cosine similarity to the text:\n\nThe woman, said the judge, is dishonest\n\nThat makes sense. However, “The woman is dishonest” has a lower cosine similarity with the semantically similar ‘The judge said: “the woman is dishonest”’ (0.8561) than the semantically dissimilar “The woman said the judge is dishonest” (0.8631).\ns1 = \"The woman is dishonest\"\n\nres = torch.tensor(\n        [\n            F.cosine_similarity(s1_emb, d2_emb, dim=0), # \"The woman, said the judge, is dishonest\"\n            F.cosine_similarity(s1_emb, d1_emb, dim=0), # \"The woman said the judge is dishonest\"\n            F.cosine_similarity(s1_emb, d5_emb, dim=0), # 'The judge said: \"the woman is dishonest\"'\n            F.cosine_similarity(s1_emb, d3_emb, dim=0), # \"The woman said: the judge is dishonest\"\n            F.cosine_similarity(s1_emb, d4_emb, dim=0), # 'The woman said: \"the judge is dishonest\"'\n        ]\n    )\n\nres\n\ntensor([0.8812, 0.8631, 0.8561, 0.8502, 0.8383])\n\n\n\ntorch.median(res)\n\ntensor(0.8561)\n\n\nFor the following text:\ns2 = \"The judge is dishonest\"\nThe most similar, by cosine similarity, is “The woman, said the judge, is dishonest” which is semantically dissimilar.\n\nres = torch.tensor(\n        [\n          F.cosine_similarity(s2_emb, d2_emb, dim=0), # \"The woman, said the judge, is dishonest\"\n          F.cosine_similarity(s2_emb, d1_emb, dim=0), # \"The woman said the judge is dishonest\"\n          F.cosine_similarity(s2_emb, d3_emb, dim=0), # \"The woman said: the judge is dishonest\"\n          F.cosine_similarity(s2_emb, d5_emb, dim=0),  # 'The judge said: \"the woman is dishonest\"'\n          F.cosine_similarity(s2_emb, d4_emb, dim=0), # 'The woman said: \"the judge is dishonest\"'\n        ]\n    )\nres\n\ntensor([0.9208, 0.9194, 0.9102, 0.8969, 0.8907])\n\n\n\ntorch.median(res)\n\ntensor(0.9102)\n\n\nTrying a bigger model that ranks higher on the MTEB leaderboard:\n\nemb_model = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\");\n\n\nd1_emb = emb_model.encode(d1, convert_to_tensor=True)\nd2_emb = emb_model.encode(d2, convert_to_tensor=True)\nd3_emb = emb_model.encode(d3, convert_to_tensor=True)\nd4_emb = emb_model.encode(d4, convert_to_tensor=True)\nd5_emb = emb_model.encode(d5, convert_to_tensor=True)\n\nq_emb = emb_model.encode(q, convert_to_tensor=True)\ns1_emb = emb_model.encode(s1, convert_to_tensor=True)\ns2_emb = emb_model.encode(s2, convert_to_tensor=True)\n\nFor this model, for this text:\nq = \"Is the woman or the judge dishonest?\"\nthe closest text by cosine similarity is “The woman said: the judge is dishonest”.\n\n(\n    F.cosine_similarity(q_emb, d3_emb, dim=0), # \"The woman said: the judge is dishonest\"\n    F.cosine_similarity(q_emb, d1_emb, dim=0), # \"The woman said the judge is dishonest\"\n    F.cosine_similarity(q_emb, d2_emb, dim=0), # \"The woman, said the judge, is dishonest\"\n    F.cosine_similarity(q_emb, d4_emb, dim=0), # 'The woman said: \"the judge is dishonest\"'\n    F.cosine_similarity(q_emb, d5_emb, dim=0),  # 'The judge said: \"the woman is dishonest\"'\n)\n\n(tensor(0.8180),\n tensor(0.8175),\n tensor(0.7875),\n tensor(0.7849),\n tensor(0.7731))\n\n\nFor the following text:\ns1 = \"The woman is dishonest\"\nthe most similar text, by cosine similarity, is “The woman said the judge is dishonest” which is semantically dissimilar.\n\nres = torch.tensor(\n        [\n            F.cosine_similarity(s1_emb, d1_emb, dim=0), # \"The woman said the judge is dishonest\"\n            F.cosine_similarity(s1_emb, d3_emb, dim=0), # \"The woman said: the judge is dishonest\"\n            F.cosine_similarity(s1_emb, d2_emb, dim=0), # \"The woman, said the judge, is dishonest\"\n            F.cosine_similarity(s1_emb, d4_emb, dim=0), # 'The woman said: \"the judge is dishonest\"'\n            F.cosine_similarity(s1_emb, d5_emb, dim=0) # 'The judge said: \"the woman is dishonest\"'\n        ]\n    )\n\nres\n\ntensor([0.9738, 0.9461, 0.9042, 0.8714, 0.8577])\n\n\n\ntorch.median(res)\n\ntensor(0.9042)\n\n\nFinally, for the following text:\ns2 = \"The judge is dishonest\"\nthe most similar text, by cosine similarity, is “The woman said the judge is dishonest” which is semantically similar. The second-most similar by cosine similarity text “The woman said: the judge is dishonest” is also semantically similar. However, the semantically similar ‘The woman said: “the judge is dishonest”’ has a lower cosine similarity than the semantically dissimilar “The woman, said the judge, is dishonest”. Whew!\n\nres = torch.tensor(\n        [\n          F.cosine_similarity(s2_emb, d1_emb, dim=0), # \"The woman said the judge is dishonest\"\n          F.cosine_similarity(s2_emb, d3_emb, dim=0), # \"The woman said: the judge is dishonest\"\n          F.cosine_similarity(s2_emb, d2_emb, dim=0), # \"The woman, said the judge, is dishonest\"\n          F.cosine_similarity(s2_emb, d4_emb, dim=0), # 'The woman said: \"the judge is dishonest\"'\n          F.cosine_similarity(s2_emb, d5_emb, dim=0),  # 'The judge said: \"the woman is dishonest\"'\n        ]\n    )\nres\n\ntensor([0.9763, 0.9507, 0.9107, 0.8791, 0.8642])\n\n\n\ntorch.median(res)\n\ntensor(0.9107)"
  },
  {
    "objectID": "posts/2024-11-08-punctuation-cosine-similarity/index.html#final-thoughts",
    "href": "posts/2024-11-08-punctuation-cosine-similarity/index.html#final-thoughts",
    "title": "Comparing Cosine Similarity Between Embeddings of Semantically Similar and Dissimilar Texts with Varying Punctuation",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’m not going to make any conclusions about the relationship between punctuation, embeddings and cosine similarity, but I’ll say that this tiny experiment has left me with more questions than answers."
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "",
    "text": "Side view of Jalen Hurts walking on the Eagles sideline with Kansas City Chiefs-colored confetti falling around him"
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "Background and Goals",
    "text": "Background and Goals\nIt’s been 5 years since I last explored NFL’s play-by-play data. It’s also been 5 years since my Eagles won the Super Bowl, which will be played in less than 24 hours from now. Go Birds.\nIt’s been so long since I’ve blogged that fastpages, the blogging library I use, has been deprecated.\nI have thoroughly enjoyed some of the statistical analyses put forth by fans of the NFL this year. My favorite analyst is Deniz Selman, a fellow Eagles fan who makes these beautiful data presentations.\nI also appreciate Deniz’ critique of analysis-without-context that often negates the brilliance of Jalen Hurts:\n\n\nAs I’ve been trying to say all year, EPA/dropback is not nearly as valuable a metric when the offense lets the QB decide whether it’s a “dropback” or not during the play by reading the defense, and that QB is the absolute best at making that decision. #FlyEaglesFly\n\n— Deniz Selman (@denizselman33) February 11, 2023\n\n\nMy second favorite analyst is Ben Baldwin, AKA Computer Cowboy especially his 4th down analysis realtime during games.\nThere has been an onslaught of statistical advances in the NFL since I last explored play-by-play data and I’m excited to learn as much as I can. In particular, I’d like to get a hang of the metrics EPA (Expected Points Added) and DVOA (Defense-adjusted Value Over Average), which may not necessarily intersect with my play-by-play analysis (I believe Football Outsiders is the proprietor of that formula).\nI’d also like to use this project to practice more advanced SQL queries than I’m used to. Given the complexity of the play-by-play dataset (by team, down, field position, etc.) I’m hoping I can get those reps in.\nLastly, I’d like to explore data presentation with these statistics using R, python, Adobe Illustrator and Photoshop. I’ve been inspired by simple, elegant graphics like those made by Peter Gorman in Barely Maps and bold, picturesque statistics posted by PFF on twitter:\n\n\nThe most clutch pass rushers face off in the Super Bowl pic.twitter.com/o50lV9Bkgk\n\n— PFF (@PFF) February 12, 2023\n\n\nI’ll work on this project in this post throughout this year–and maybe beyond if it fuels me with enough material–or it’ll fork off into something entirely new or different.\nI’ll start off by next exploring the schema of the play-by-play dataset."
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#documenting-the-nfl-play-by-play-dataset-fields",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#documenting-the-nfl-play-by-play-dataset-fields",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "Documenting the NFL Play-by-Play Dataset Fields",
    "text": "Documenting the NFL Play-by-Play Dataset Fields\nIn this section, I describe the fields in the 2022 NFL Play-by-Play Dataset. Not all of the fields are intuitive or immediately useful, so not all 372 column descriptions will be listed.\n\nimport pandas as pd\nimport numpy as np\n\n\n# load the data\nfpath = \"../../../nfl_pbp_data/play_by_play_2022.csv\"\npbp_2022 = pd.read_csv(fpath, low_memory=False)\n\npbp_2022.head()\n\n\n\n\n\n  \n    \n      \n      play_id\n      game_id\n      old_game_id\n      home_team\n      away_team\n      season_type\n      week\n      posteam\n      posteam_type\n      defteam\n      ...\n      out_of_bounds\n      home_opening_kickoff\n      qb_epa\n      xyac_epa\n      xyac_mean_yardage\n      xyac_median_yardage\n      xyac_success\n      xyac_fd\n      xpass\n      pass_oe\n    \n  \n  \n    \n      0\n      1\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NaN\n      NaN\n      NaN\n      ...\n      0\n      1\n      0.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      43\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.443521\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      68\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      1.468819\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.440373\n      -44.037291\n    \n    \n      3\n      89\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.492192\n      0.727261\n      6.988125\n      6.0\n      0.60693\n      0.227598\n      0.389904\n      61.009598\n    \n    \n      4\n      115\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.325931\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.443575\n      -44.357494\n    \n  \n\n5 rows × 372 columns\n\n\n\nThe 2022 NFL Play-by-Play dataset has 50147 rows (plays) and 372 columns.\n\npbp_2022.shape\n\n(50147, 372)\n\n\nplay_id is an identifier for each play in each game. It not a unique identifier as there are many duplicates. There are 4597 unique play_id values in this dataset.\n\nlen(pbp_2022.play_id.unique())\n\n4597\n\n\ngame_id is an identifier for each game in the dataset in the format of {year}_{week}_{away_team}_{home_team}. There are 284 unique games in this dataset.\n\nlen(pbp_2022.game_id.unique()), pbp_2022.game_id[1]\n\n(284, '2022_01_BAL_NYJ')\n\n\nThere are 32 unique home_teams and away_teams.\n\nlen(pbp_2022.home_team.unique()), len(pbp_2022.away_team.unique())\n\n(32, 32)\n\n\nThere are two season_type values: 'REG' for regular season and 'POST' for postseason.\n\npbp_2022.season_type.unique()\n\narray(['REG', 'POST'], dtype=object)\n\n\nThere are 22 week values: - 18 regular season weeks (17 games + 1 bye) - 4 postseason weeks - Wild Card Weekend - Divisional Playoffs - Conference Championships - Super Bowl\n\npbp_2022.week.unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22])\n\n\nI believe posteam stands for the team that has possession of the ball. There are 32 unique teams that can have possession of the ball in a game, and in some cases the posteam is nan.\n\nlen(pbp_2022.posteam.unique()), pbp_2022.posteam.unique()\n\n(33,\n array([nan, 'NYJ', 'BAL', 'BUF', 'LA', 'CAR', 'CLE', 'SEA', 'DEN', 'MIN',\n        'GB', 'IND', 'HOU', 'JAX', 'WAS', 'KC', 'ARI', 'LAC', 'LV', 'NE',\n        'MIA', 'ATL', 'NO', 'NYG', 'TEN', 'DET', 'PHI', 'PIT', 'CIN',\n        'CHI', 'SF', 'DAL', 'TB'], dtype=object))\n\n\nposteam_type has values 'home', 'away' and nan.\n\nlen(pbp_2022.posteam_type.unique()), pbp_2022.posteam_type.unique()\n\n(3, array([nan, 'home', 'away'], dtype=object))\n\n\ndefteam lists any of the 32 teams on defense on a given play. It can also have the value nan.\n\nlen(pbp_2022.defteam.unique()), pbp_2022.defteam.unique()\n\n(33,\n array([nan, 'BAL', 'NYJ', 'LA', 'BUF', 'CLE', 'CAR', 'DEN', 'SEA', 'GB',\n        'MIN', 'HOU', 'IND', 'WAS', 'JAX', 'ARI', 'KC', 'LV', 'LAC', 'MIA',\n        'NE', 'NO', 'ATL', 'TEN', 'NYG', 'PHI', 'DET', 'CIN', 'PIT', 'SF',\n        'CHI', 'TB', 'DAL'], dtype=object))\n\n\nside_of_field can be nan, any of the 32 team abbreviations, or 50 (midfield).\n\nlen(pbp_2022.side_of_field.unique()), pbp_2022.side_of_field.unique()\n\n(34,\n array([nan, 'BAL', 'NYJ', 'LA', 'BUF', '50', 'CLE', 'CAR', 'DEN', 'SEA',\n        'GB', 'MIN', 'HOU', 'IND', 'WAS', 'JAX', 'ARI', 'KC', 'LV', 'LAC',\n        'MIA', 'NE', 'NO', 'ATL', 'TEN', 'NYG', 'PHI', 'DET', 'CIN', 'PIT',\n        'SF', 'CHI', 'TB', 'DAL'], dtype=object))\n\n\nyardline_100 can be nan or between 1 and 99.\n\nlen(pbp_2022.yardline_100.unique()), np.nanmin(pbp_2022.yardline_100), np.nanmax(pbp_2022.yardline_100)\n\n(100, 1.0, 99.0)\n\n\nThere are 61 game_date values.\n\nlen(pbp_2022.game_date.unique()), pbp_2022.game_date[0]\n\n(61, '2022-09-11')\n\n\nquarter_seconds_remaining is between 0 and 900 (15 minutes).\n\npbp_2022.quarter_seconds_remaining.min(), pbp_2022.quarter_seconds_remaining.max()\n\n(0, 900)\n\n\nhalf_seconds_remaining is between 0 and 1800 (30 minutes).\n\npbp_2022.half_seconds_remaining.min(), pbp_2022.half_seconds_remaining.max()\n\n(0, 1800)\n\n\ngame_seconds_remaining is between 0 and 3600 (60 minutes).\n\npbp_2022.game_seconds_remaining.min(), pbp_2022.game_seconds_remaining.max()\n\n(0, 3600)\n\n\ngame_half is either Half1 (first half), Half2 (second half), or Overtime.\n\npbp_2022.game_half.unique()\n\narray(['Half1', 'Half2', 'Overtime'], dtype=object)\n\n\nquarter_end is either 1 (True) or 0 (False).\n\npbp_2022.quarter_end.unique(), pbp_2022.query('quarter_end == 1').desc[41]\n\n(array([0, 1]), 'END QUARTER 1')\n\n\ndrive is the current number of drives in the game (including both teams) as well as nan values.\n\npbp_2022.drive.unique()\n\narray([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n       26., 27., 28., 29., 30., 31., 32., 33., 34., 35.])\n\n\nsp teams seems to indicate whether the play involves the Special Teams unit, either 1 (True) or 0 (False).\n\npbp_2022.sp.unique(), pbp_2022.query('sp == 1').desc[32]\n\n(array([0, 1]),\n '(3:19) 9-J.Tucker 24 yard field goal is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\nquarter indicates the current quarter of the play. quarter == 5 represents Overtime.\n\npbp_2022.qtr.unique()\n\narray([1, 2, 3, 4, 5])\n\n\ndown represents the current down of the play (nan, 1st, 2nd, 3rd or 4th).\n\npbp_2022.down.unique()\n\narray([nan,  1.,  2.,  3.,  4.])\n\n\ngoal_to_go indicates whether this play is 1st & Goal, 2nd & Goal, 3rd & Goal or 4th & Goal, either 1 (True) or 0 (False).\n\npbp_2022.goal_to_go.unique()\n\narray([0, 1])\n\n\ntime is the minutes:seconds formatted time left in the current quarter.\n\npbp_2022.head().time.unique()\n\narray(['15:00', '14:56', '14:29', '14:25'], dtype=object)\n\n\nyrdln is a formatted string of team abbreviation and yard number.\n\npbp_2022.yrdln.unique()\n\narray(['BAL 35', 'NYJ 22', 'NYJ 41', ..., 'NYJ 3', 'CIN 6', 'MIN 12'],\n      dtype=object)\n\n\nydstogo is the number of yards before the next first down.\n\npbp_2022.ydstogo.unique()\n\narray([ 0, 10,  5, 15,  6,  2,  1, 12,  9, 19, 11,  3,  8,  4, 16, 17,  7,\n       20, 14, 18, 13, 22, 26, 24, 21, 25, 23, 28, 30, 27, 31, 38, 36, 29,\n       34, 35, 32, 33])\n\n\nydsnet is the net yards (yards gained - yards lost) of the current drive.\n\npbp_2022.ydsnet.unique()\n\narray([ nan,  14.,  21.,   7.,   1.,  15.,   9.,  16.,  44.,  18.,  62.,\n        48.,   3.,  11.,   4.,  88.,  75.,  23.,  43.,  -2.,  38.,   0.,\n        45.,  60.,  13.,   6.,  -1.,  58.,  25.,  89.,  59.,  19.,  66.,\n        29.,  -4.,  24.,   2.,  12.,  42.,  78.,  52.,  57.,  64.,  35.,\n        -3.,  70.,  77.,  72.,  50.,  37.,  31.,  -6.,  32.,  -5.,  20.,\n        79.,  74.,  34.,  65.,   8.,  47.,   5.,  69.,  53.,  33.,  76.,\n        80., -16.,  71.,  68.,  55.,  27.,  90.,  86.,  17.,  30.,  67.,\n        63.,  73.,  61., -13.,  92.,  40.,  22.,  -7.,  39.,  41.,  28.,\n        82.,  49.,  10.,  36.,  46.,  84.,  54., -23., -11.,  83.,  26.,\n        94.,  87., -10.,  85.,  51., -14.,  56.,  -8.,  81.,  -9.,  93.,\n       -12., -15., -17.,  91.,  99.,  98., -19.,  96.,  95.,  97., -20.,\n       -25.])\n\n\ndesc is a narrative description of the current play.\n\npbp_2022.head().desc[1]\n\n'9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).'\n\n\nplay_type is either nan or one of 9 different play types, including no_play.\n\nlen(pbp_2022.play_type.unique()), pbp_2022.play_type.unique()\n\n(10,\n array([nan, 'kickoff', 'run', 'pass', 'punt', 'no_play', 'field_goal',\n        'extra_point', 'qb_kneel', 'qb_spike'], dtype=object))\n\n\nyards_gained is the number of yards gained (positive) or lost (negative) on the current play. It does not capture yards gained or lost due to a penalty.\n\npbp_2022.head().yards_gained, pbp_2022.yards_gained.min()\n\n(0     NaN\n 1     0.0\n 2    19.0\n 3     0.0\n 4     5.0\n Name: yards_gained, dtype: float64,\n -26.0)\n\n\nshotgun indicates whether the quarterback was in shotgun position, either 1 (True) or 0 (False).\n\npbp_2022.shotgun.unique(), pbp_2022.query('shotgun == 1').desc[3]\n\n(array([0, 1]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nno_huddle indicates whether the team huddled before the snap, either 1 (True) or 0 (False).\n\npbp_2022.no_huddle.unique(), pbp_2022.query('no_huddle == 1').desc[3]\n\n(array([0, 1]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nqb_dropback indicates whether the quarterback drops back on the play, either 1 (True), 0 (False) or nan.\n\npbp_2022.qb_dropback.unique(), pbp_2022.query('qb_dropback == 1').desc[3]\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nqb_kneel indicates whether the quarterback kneels on the play, either 1 (True) or 0 (False).\n\npbp_2022.qb_kneel.unique(), pbp_2022.query('qb_kneel == 1').desc[176]\n\n(array([0, 1]), '(:59) 8-L.Jackson kneels to NYJ 43 for -1 yards.')\n\n\nqb_spike indicates whether the quarterback spikes the ball on the play, either 1 (True) or 0 (False).\n\npbp_2022.qb_spike.unique(), pbp_2022.query('qb_spike == 1').desc[520]\n\n(array([0, 1]),\n '(:29) (No Huddle) 7-J.Brissett spiked the ball to stop the clock.')\n\n\nqb_scramble indicates whether the quarterback scrambles on the play, either 1 (True) or 0 (False). It looks like a scramble is not the same as a designed quarterback run, so I’ll dig deeper into this before using this field in analyses.\n\npbp_2022.qb_scramble.unique()\n\narray([0, 1])\n\n\npass_length is either nan, 'short' or 'deep'. I’ll first understand what distance (in yards) corresponds to these designations before I use this field in analyses.\n\npbp_2022.pass_length.unique()\n\narray([nan, 'short', 'deep'], dtype=object)\n\n\npass_location is either nan, 'left', 'right', or 'middle'.\n\npbp_2022.pass_location.unique()\n\narray([nan, 'left', 'right', 'middle'], dtype=object)\n\n\nair_yards is the number of yards a quarterback’s pass traveled in the air. It can be positive, zero or negative.\n\npbp_2022.air_yards.unique()\n\narray([ nan,   0.,  -4.,   3.,   2.,  16.,  11.,   5.,  21.,  14.,  -1.,\n         1.,   7.,   6.,  15.,  -3.,   8.,  10.,  50.,  27.,  25.,  -5.,\n        31.,  -6.,  17.,  51.,  13.,   4.,  12.,  36.,   9.,  32.,  18.,\n        22.,  -2.,  23.,  45.,  40.,  52.,  -7.,  26.,  29.,  20.,  47.,\n        24.,  30.,  28.,  37.,  39.,  -8.,  19.,  41.,  38., -12.,  42.,\n       -10.,  46.,  35.,  33.,  -9.,  34.,  44.,  43.,  53.,  57.,  48.,\n        49.,  54.,  58.,  56.,  59.,  55.,  61., -18., -54., -13.,  62.,\n        65., -20., -16.])\n\n\nyards_after_catch is the number of yards the receiver gains or loses after catching the ball.\n\npbp_2022.yards_after_catch.unique()\n\narray([ nan,   8.,   1.,   6.,   0.,   3.,   5.,   4.,  12.,   9.,  10.,\n        -4.,  18.,   7.,  15.,   2.,  11.,  13.,  -1.,  29.,  30.,  27.,\n        28.,  16.,  26.,  24.,  25.,  -5.,  41.,  14.,  22.,  19.,  17.,\n        21.,  32.,  20.,  -2.,  35.,  -3.,  51.,  66.,  38.,  46.,  23.,\n        31.,  37.,  68.,  -6.,  33.,  52.,  75.,  34.,  71.,  44.,  61.,\n        60.,  58.,  48.,  50.,  53.,  39.,  62.,  47.,  -7.,  42.,  40.,\n        36.,  49.,  70.,  45.,  65.,  43.,  74., -10.,  -9.])\n\n\nrun_location is either nan, 'left', 'right', or 'middle'.\n\npbp_2022.run_location.unique()\n\narray([nan, 'left', 'right', 'middle'], dtype=object)\n\n\nrun_gap represents which offensive line gap the runner ran through. It is either nan, 'end', 'tackle' or 'guard'. I’ll have to dig a bit deeper (look at some video corresponding to the run plays) to understand if 'guard' represents the A (gap between center and guard) or B gap (gap between guard and tackle), if 'tackle' represents the B or C gap (gap between tackle and end), and if 'end' represents the C or D (gap outside the end) gap.\n\npbp_2022.run_gap.unique()\n\narray([nan, 'end', 'tackle', 'guard'], dtype=object)\n\n\nfield_goal_result is either nan, 'made', 'missed', or 'blocked'.\n\npbp_2022.field_goal_result.unique()\n\narray([nan, 'made', 'missed', 'blocked'], dtype=object)\n\n\nkick_distance is the distance of the kick in yards for the following play_type values: 'punt', 'field_goal', 'extra_point', and 'kickoff'. Looking through the data, not all 'kickoff's have a kick_distance value.\n\npbp_2022.kick_distance.unique(), pbp_2022.query('kick_distance.notnull()').play_type.unique()\n\n(array([nan, 45., 40., 48., 24., 50., 56., 41., 33., 20., 49., 43.,  7.,\n        36., 57., 25., 39., 60., 62., 61., 44., 46., 58., 26., 34., 64.,\n        30., 47., 54., 28., 53., 38., 29., 70., 37., 27., 52., 42., 63.,\n        51., 23., 55., 59., 69., 66., 14., 32., 35.,  0., 31., 67., 74.,\n        19., 10., 22., 12.,  8.,  5., -1., 73., 65.,  3., 21.,  9., 16.,\n        15., 13., 18., 17.,  6., 77., 68., 11., 71., 79.]),\n array(['punt', 'field_goal', 'extra_point', 'kickoff'], dtype=object))\n\n\nextra_point_result is either nan, 'good', 'failed' or 'blocked'.\n\npbp_2022.extra_point_result.unique()\n\narray([nan, 'good', 'failed', 'blocked'], dtype=object)\n\n\ntwo_point_conv_result, the result of a two-point conversion is either nan, 'failure' or 'success'.\n\npbp_2022.two_point_conv_result.unique()\n\narray([nan, 'failure', 'success'], dtype=object)\n\n\nhome_timeouts_remaining is the number of timeouts the home team has left. It is either 3, 2, 1, or 0.\n\npbp_2022.home_timeouts_remaining.unique()\n\narray([3, 2, 1, 0])\n\n\naway_timeouts_remaining is the number of timeouts the away team has left. It is either 3, 2, 1, or 0.\n\npbp_2022.away_timeouts_remaining.unique()\n\narray([3, 2, 1, 0])\n\n\ntimeout indicates if a team calls a timeout, either 1 (True) or 0 (False).\n\npbp_2022.timeout.unique(), pbp_2022.query('timeout == 1').desc[13]\n\n(array([nan,  0.,  1.]), 'Timeout #1 by BAL at 09:56.')\n\n\ntimeout_team indicates which team called the timeout, and has 33 unique values—1 nan and 32 team abbreviations.\n\n(pbp_2022.timeout_team.unique(), \npbp_2022.query('timeout == 1').desc[13], \npbp_2022.query('timeout == 1').timeout_team[13])\n\n(array([nan, 'BAL', 'NYJ', 'LA', 'BUF', 'CLE', 'CAR', 'DEN', 'SEA', 'GB',\n        'MIN', 'IND', 'HOU', 'WAS', 'JAX', 'KC', 'ARI', 'LAC', 'LV', 'NE',\n        'MIA', 'ATL', 'NO', 'NYG', 'TEN', 'PHI', 'DET', 'PIT', 'CIN', 'SF',\n        'CHI', 'DAL', 'TB'], dtype=object),\n 'Timeout #1 by BAL at 09:56.',\n 'BAL')\n\n\ntd_team indicates which team scored the touchdown. It is nan or one of 32 team abbreviations.\n\n(pbp_2022.td_team.unique(),\npbp_2022.query('td_team.notnull()').td_team[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, 'BAL', 'NYJ', 'BUF', 'LA', 'CLE', 'CAR', 'SEA', 'DEN', 'MIN',\n        'GB', 'HOU', 'IND', 'WAS', 'JAX', 'KC', 'ARI', 'LAC', 'LV', 'MIA',\n        'NE', 'NO', 'ATL', 'TEN', 'NYG', 'DET', 'PHI', 'PIT', 'CIN', 'SF',\n        'CHI', 'TB', 'DAL'], dtype=object),\n 'BAL',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\ntd_player_name indicates which player scored the touchdown. It is nan or one of 416 players who scored a touchdown in the 2022 season.\n\n(pbp_2022.td_player_name.unique()[:5],\nlen(pbp_2022.td_player_name.unique()),\npbp_2022.query('td_team.notnull()').td_player_name[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, 'D.Duvernay', 'R.Bateman', 'T.Conklin', 'G.Davis'],\n       dtype=object),\n 417,\n 'D.Duvernay',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\ntd_player_id indicates the id of the player who scored the touchdown. There are 422 unique player IDs. Later on, I’ll look into why there are 5 fewer player IDs than player names.\n\n(pbp_2022.td_player_id.unique()[:5],\nlen(pbp_2022.td_player_id.unique()),\npbp_2022.query('td_team.notnull()').td_player_name[68],\n pbp_2022.query('td_team.notnull()').td_player_id[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, '00-0036331', '00-0036550', '00-0034270', '00-0036196'],\n       dtype=object),\n 423,\n 'D.Duvernay',\n '00-0036331',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\nposteam_timeouts_remaining is the number of timeouts remaining for the team with ball possession. It can be nan, 3, 2, 1, or 0.\n\npbp_2022.posteam_timeouts_remaining.unique()\n\narray([nan,  3.,  2.,  0.,  1.])\n\n\ndefteam_timeouts_remaining is the number of timeouts remaining for the team on defense. It can be nan, 3, 2, 1, or 0.\n\npbp_2022.defteam_timeouts_remaining.unique()\n\narray([nan,  3.,  2.,  1.,  0.])\n\n\ntotal_home_score is the total number of points scored by the home team.\n\npbp_2022.total_home_score.unique()[:5]\n\narray([0, 3, 9, 6, 7])\n\n\ntotal_away_score is the total number of points scored by the away team.\n\npbp_2022.total_away_score.unique()[:5]\n\narray([ 0,  3,  9, 10, 16])\n\n\nposteam_score is the total number of points scored by the team with ball possession on the current play.\n\npbp_2022.posteam_score.unique()[:5]\n\narray([nan,  0.,  3.,  9., 10.])\n\n\ndefteam_score is the total number of points scored by the team on defense on the current play.\n\npbp_2022.defteam_score.unique()[:5]\n\narray([nan,  0.,  3., 10., 17.])\n\n\nscore_differential is the difference between posteam_score and defteam_score.\n\npbp_2022.score_differential.unique()[:5]\n\narray([nan,  0., -3.,  3.,  9.])\n\n\npunt_blocked indicates if the punt was blocked. It is either nan, 1 (True) or 0 (False).\n\npbp_2022.punt_blocked.unique(),pbp_2022.query('punt_blocked == 1').desc[3236]\n\n(array([nan,  0.,  1.]),\n '(5:06) 11-R.Dixon punt is BLOCKED by 44-T.Andersen, Center-42-M.Orzech, RECOVERED by ATL-9-L.Carter at LA 26. 9-L.Carter for 26 yards, TOUCHDOWN.')\n\n\nfirst_down_rush indicates whether a first down was achieved by a rushing play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_rush.unique(), \n pbp_2022.query('first_down_rush == 1').desc[2],\n pbp_2022.query('first_down_rush == 1').play_type[2])\n\n(array([nan,  0.,  1.]),\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).',\n 'run',\n nan)\n\n\nfirst_down_pass indicates whether a first down was achieved by a passing play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_pass.unique(), \n pbp_2022.query('first_down_pass == 1').desc[26],\n pbp_2022.query('first_down_pass == 1').play_type[26])\n\n(array([nan,  0.,  1.]),\n '(6:01) 19-J.Flacco pass deep left to 8-E.Moore to NYJ 41 for 24 yards (32-M.Williams).',\n 'pass')\n\n\nfirst_down_penalty indicates whether a first down was achieved by a penalty. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_penalty.unique(), \n pbp_2022.query('first_down_penalty == 1').desc[17],\n pbp_2022.query('first_down_penalty == 1').play_type[17])\n\n(array([nan,  0.,  1.]),\n '(8:31) (Shotgun) 19-J.Flacco pass incomplete deep left to 8-E.Moore. PENALTY on BAL-44-M.Humphrey, Illegal Contact, 5 yards, enforced at NYJ 12 - No Play.',\n 'no_play')\n\n\nthird_down_converted indicates if the team with ball possession on third down got a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.third_down_converted.unique(), \n pbp_2022.query('third_down_converted == 1').down[9],\n pbp_2022.query('third_down_converted == 1').ydstogo[9],\n pbp_2022.query('third_down_converted == 1').desc[9],\npbp_2022.query('third_down_converted == 1').yards_gained[9])\n\n(array([nan,  0.,  1.]),\n 3.0,\n 2,\n '(12:41) (Shotgun) 8-L.Jackson right tackle to BAL 40 for 4 yards (57-C.Mosley, 3-J.Whitehead).',\n 4.0)\n\n\nthird_down_failed indicates if the team with ball possession on third down did not get a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.third_down_failed.unique(), \n pbp_2022.query('third_down_failed == 1').down[5],\n pbp_2022.query('third_down_failed == 1').ydstogo[5],\n pbp_2022.query('third_down_failed == 1').desc[5],\npbp_2022.query('third_down_failed == 1').yards_gained[5])\n\n(array([nan,  0.,  1.]),\n 3.0,\n 5,\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.',\n 0.0)\n\n\nfourth_down_converted indicates if the team with ball possession on fourth down got a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.fourth_down_converted.unique(), \n pbp_2022.query('fourth_down_converted == 1').down[145],\n pbp_2022.query('fourth_down_converted == 1').ydstogo[145],\n pbp_2022.query('fourth_down_converted == 1').desc[145],\npbp_2022.query('fourth_down_converted == 1').yards_gained[145])\n\n(array([nan,  0.,  1.]),\n 4.0,\n 1,\n '(7:32) 19-J.Flacco pass short right to 84-C.Davis to BAL 21 for 7 yards (23-K.Fuller).',\n 7.0)\n\n\nfourth_down_failed indicates if the team with ball possession on fourth down did not get a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.fourth_down_failed.unique(), \n pbp_2022.query('fourth_down_failed == 1').down[154],\n pbp_2022.query('fourth_down_failed == 1').ydstogo[154],\n pbp_2022.query('fourth_down_failed == 1').desc[154],\npbp_2022.query('fourth_down_failed == 1').yards_gained[154])\n\n(array([nan,  0.,  1.]),\n 4.0,\n 6,\n '(4:22) (Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.',\n 0.0)\n\n\nincomplete_pass indicates if the pass was incomplete. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.incomplete_pass.unique(),\n pbp_2022.query('incomplete_pass == 1').desc[3])\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\ntouchback indicates if the kickoff or punt either went past the back of the endzone or was fair-caught in the end zone.\n\n(pbp_2022.touchback.unique(),\n pbp_2022.query('touchback == 1').desc[33])\n\n(array([0, 1]),\n '9-J.Tucker kicks 65 yards from BAL 35 to end zone, Touchback.')\n\n\ninterception indicates if the quarterback’s pass was intercepted by a defender. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.interception.unique(),\n pbp_2022.query('interception == 1').desc[28])\n\n(array([nan,  0.,  1.]),\n '(5:07) (Shotgun) 19-J.Flacco pass short middle intended for 81-L.Cager INTERCEPTED by 32-M.Williams at NYJ 46. 32-M.Williams to NYJ 13 for 33 yards (19-J.Flacco).')\n\n\nfumble_forced indicates if a fumble was forced on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_forced.unique(),\n pbp_2022.query('fumble_forced == 1').desc[80])\n\n(array([nan,  0.,  1.]),\n '(1:16) (Shotgun) 19-J.Flacco pass short right to 83-T.Conklin to BAL 21 for 6 yards (32-M.Williams, 58-M.Pierce). FUMBLES (58-M.Pierce), touched at BAL 25, recovered by NYJ-17-G.Wilson at BAL 27. 17-G.Wilson to BAL 27 for no gain (14-K.Hamilton).')\n\n\nfumble_not_forced indicates if a fumble occurred on the play but was not forced by another player. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_not_forced.unique(),\n pbp_2022.query('fumble_not_forced == 1').desc[264])\n\n(array([nan,  0.,  1.]),\n '(13:46) (Shotgun) 9-M.Stafford to LA 11 for -6 yards. FUMBLES, and recovers at LA 11. 9-M.Stafford sacked at LA 10 for -7 yards (50-G.Rousseau).')\n\n\nfumble_out_of_bounds indicates if a fumbled ball went out of bounds. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_out_of_bounds.unique(),\n pbp_2022.query('fumble_out_of_bounds == 1').desc[1160])\n\n(array([nan,  0.,  1.]),\n '(:32) (Shotgun) 16-T.Lawrence pass short right to 1-T.Etienne to WAS 11 for 3 yards (22-D.Forrest). FUMBLES (22-D.Forrest), ball out of bounds at WAS 19. The Replay Official reviewed the pass completion ruling, and the play was Upheld. The ruling on the field stands.')\n\n\nsolo_tackle indicates if a player made a solo tackle on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.solo_tackle.unique(),\n pbp_2022.query('solo_tackle == 1').desc[1])\n\n(array([nan,  1.,  0.]),\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\nsafety indicates if a defensive player scored a safety on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.safety.unique(),\n pbp_2022.query('safety == 1').desc[3255])\n\n(array([nan,  0.,  1.]),\n '(:13) (Run formation) 19-B.Powell right end ran ob in End Zone for -26 yards, SAFETY (37-D.Alford).')\n\n\npenalty indicates if there was a penalty on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.penalty.unique(),\n pbp_2022.query('penalty == 1').desc[5])\n\n(array([nan,  0.,  1.]),\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\ntackled_for_loss indicates if a player was tackled for a loss of yards. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.tackled_for_loss.unique(),\n pbp_2022.query('tackled_for_loss == 1').desc[15])\n\n(array([nan,  0.,  1.]),\n '(9:49) 20-Br.Hall right end to NYJ 9 for -2 yards (92-J.Madubuike).')\n\n\nfumble_lost indicates if a player lost a fumble to the other team. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_lost.unique(),\n pbp_2022.query('fumble_lost == 1').desc[129])\n\n(array([nan,  0.,  1.]),\n '(14:13) (No Huddle, Shotgun) 19-J.Flacco pass short middle to 20-Br.Hall to BAL 16 for 6 yards (36-C.Clark). FUMBLES (36-C.Clark), RECOVERED by BAL-44-M.Humphrey at BAL 15.')\n\n\nqb_hit indicates if the quarterback was hit on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.qb_hit.unique(),\n pbp_2022.query('qb_hit == 1').desc[5])\n\n(array([nan,  0.,  1.]),\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\nrush_attempt indicates if the play was a rushing play. It is either nan, 1 (True), or 0 (False). A QB scramble is considered a rush attempt.\n\n(pbp_2022.rush_attempt.unique(),\n pbp_2022.query('rush_attempt == 1').desc[9],\n pbp_2022.query('rush_attempt == 1 and qb_scramble == 1').desc[89])\n\n(array([nan,  0.,  1.]),\n '(12:41) (Shotgun) 8-L.Jackson right tackle to BAL 40 for 4 yards (57-C.Mosley, 3-J.Whitehead).',\n '(14:15) (Shotgun) 8-L.Jackson scrambles left end ran ob at BAL 35 for 8 yards (3-J.Whitehead).')\n\n\npass_attempt indicates if the play was a passing play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.pass_attempt.unique(),\n pbp_2022.query('pass_attempt == 1').desc[3])\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nsack indicates if the quarterback was sacked on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.sack.unique(),\n pbp_2022.query('sack == 1').desc[54])\n\n(array([nan,  0.,  1.]),\n '(9:43) (Shotgun) 8-L.Jackson sacked ob at NYJ 49 for 0 yards (56-Qu.Williams).')\n\n\ntouchdown indicates if a player scored a touchdown on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.touchdown.unique(),\n pbp_2022.query('touchdown == 1').desc[68])\n\n(array([nan,  0.,  1.]),\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\npass_touchdown, rush_touchdown, and return_touchdown indicate if the touchdown was a result of a pass, rush or kickoff/punt/fumble/interception return play, respectively. Their value is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.pass_touchdown.unique(),\n pbp_2022.query('pass_touchdown == 1').desc[68])\n\n(array([nan,  0.,  1.]),\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\n\n(pbp_2022.rush_touchdown.unique(),\n pbp_2022.query('rush_touchdown == 1').desc[298])\n\n(array([nan,  0.,  1.]),\n '(13:34) (Shotgun) 17-J.Allen scrambles right end for 4 yards, TOUCHDOWN.')\n\n\n\n(pbp_2022.return_touchdown.unique(),\n pbp_2022.query('return_touchdown == 1').desc[1651],\n pbp_2022.query('return_touchdown == 1').desc[2197],\n pbp_2022.query('return_touchdown == 1').desc[47094])\n\n(array([nan,  0.,  1.]),\n '(7:40) (Shotgun) 10-M.Jones sacked at NE 6 for -9 yards (29-Br.Jones). FUMBLES (29-Br.Jones) [29-Br.Jones], RECOVERED by MIA-6-M.Ingram at NE 2. 6-M.Ingram for 2 yards, TOUCHDOWN.',\n '(6:36) (Shotgun) 16-J.Goff pass short left intended for 88-T.Hockenson INTERCEPTED by 24-J.Bradberry (43-K.White) [95-M.Tuipulotu] at DET 27. 24-J.Bradberry for 27 yards, TOUCHDOWN.',\n '6-N.Folk kicks 66 yards from NE 35 to BUF -1. 20-N.Hines for 101 yards, TOUCHDOWN.')\n\n\nThe following fields indicate if the play involved an attempt at an Extra Point, Two Point Conversion, Field Goal, Kickoff, or Punt, respectively:\n\nextra_point_attempt\ntwo_point_attempt\nfield_goal_attempt\nkickoff_attempt\npunt_attempt\n\nTheir value is either nan, 1 (True), or 0 (False).empt\n\n(pbp_2022.extra_point_attempt.unique(),\n pbp_2022.query('extra_point_attempt == 1').desc[69])\n\n(array([nan,  0.,  1.]),\n '9-J.Tucker extra point is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\n\n(pbp_2022.two_point_attempt.unique(),\n pbp_2022.query('two_point_attempt == 1').desc[1179])\n\n(array([nan,  0.,  1.]),\n 'TWO-POINT CONVERSION ATTEMPT. 16-T.Lawrence pass to 17-E.Engram is incomplete. ATTEMPT FAILS.')\n\n\n\n(pbp_2022.field_goal_attempt.unique(),\n pbp_2022.query('field_goal_attempt == 1').desc[32])\n\n(array([nan,  0.,  1.]),\n '(3:19) 9-J.Tucker 24 yard field goal is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\n\n(pbp_2022.kickoff_attempt.unique(),\n pbp_2022.query('kickoff_attempt == 1').desc[1])\n\n(array([nan,  1.,  0.]),\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.punt_attempt.unique(),\n pbp_2022.query('punt_attempt == 1').desc[6])\n\n(array([nan,  0.,  1.]),\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\nfumble indicates if a player fumbled the ball on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble.unique(),\n pbp_2022.query('fumble == 1').desc[80])\n\n(array([nan,  0.,  1.]),\n '(1:16) (Shotgun) 19-J.Flacco pass short right to 83-T.Conklin to BAL 21 for 6 yards (32-M.Williams, 58-M.Pierce). FUMBLES (58-M.Pierce), touched at BAL 25, recovered by NYJ-17-G.Wilson at BAL 27. 17-G.Wilson to BAL 27 for no gain (14-K.Hamilton).')\n\n\ncomplete_pass indicates if a player completed a pass on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.complete_pass.unique(),\n pbp_2022.query('complete_pass == 1').desc[7])\n\n(array([nan,  0.,  1.]),\n '(13:42) 8-L.Jackson pass short right to 7-R.Bateman pushed ob at BAL 32 for 4 yards (3-J.Whitehead).')\n\n\nassist_tackle indicates if a player assisted on the tackle on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.assist_tackle.unique(),\n pbp_2022.query('assist_tackle == 1').desc[2])\n\n(array([nan,  0.,  1.]),\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThe following fields provide the player_id (string), player_name (string) and yards gained (integer) for the passer, receiver or rusher on the play, respectively.\n\npasser_player_id\npasser_player_name\npassing_yards\nreceiver_player_id\nreceiver_player_name\nreceiving_yards\nrusher_player_id\nrusher_player_name\nrushing_yards\n\n\n(pbp_2022.passer_player_id[3],\n pbp_2022.passer_player_name[3],\n pbp_2022.passing_yards[3],\n pbp_2022.desc[3])\n\n('00-0026158',\n 'J.Flacco',\n nan,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\n(pbp_2022.receiver_player_id[3],\n pbp_2022.receiver_player_name[3],\n pbp_2022.receiving_yards[3],\n pbp_2022.desc[3])\n\n('00-0036924',\n 'Mi.Carter',\n nan,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\n(pbp_2022.rusher_player_id[2],\n pbp_2022.rusher_player_name[2],\n pbp_2022.rushing_yards[2],\n pbp_2022.desc[2])\n\n('00-0036924',\n 'Mi.Carter',\n 19.0,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThe following fields provide the player_id (string) and player_name (string) for players who intercepted the ball, returned a punt, returned a kickoff, punted the ball, kicked off the ball, recovered their own kickoff, or blocked the kick, respectively:\n\ninterception_player_id\ninterception_player_name\npunt_returner_player_id\npunt_returner_player_name\nkickoff_returner_player_name\nkickoff_returner_player_id\npunter_player_id\npunter_player_name\nkicker_player_name\nkicker_player_id\nown_kickoff_recovery_player_id\nown_kickoff_recovery_player_name\nblocked_player_id\nblocked_player_name\n\n\n(pbp_2022.interception_player_id[28],\n pbp_2022.interception_player_name[28],\n pbp_2022.desc[28])\n\n('00-0033894',\n 'M.Williams',\n '(5:07) (Shotgun) 19-J.Flacco pass short middle intended for 81-L.Cager INTERCEPTED by 32-M.Williams at NYJ 46. 32-M.Williams to NYJ 13 for 33 yards (19-J.Flacco).')\n\n\n\n(pbp_2022.punt_returner_player_id[6],\n pbp_2022.punt_returner_player_name[6],\n pbp_2022.desc[6])\n\n('00-0036331',\n 'D.Duvernay',\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\n\n(pbp_2022.kickoff_returner_player_id[1],\n pbp_2022.kickoff_returner_player_name[1],\n pbp_2022.desc[1])\n\n('00-0034419',\n 'B.Berrios',\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.punter_player_id[6],\n pbp_2022.punter_player_name[6],\n pbp_2022.desc[6])\n\n('00-0036313',\n 'B.Mann',\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\n\n(pbp_2022.kicker_player_id[1],\n pbp_2022.kicker_player_name[1],\n pbp_2022.desc[1])\n\n('00-0029597',\n 'J.Tucker',\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.own_kickoff_recovery_player_id[4964],\n pbp_2022.own_kickoff_recovery_player_name[4964],\n pbp_2022.desc[4964])\n\n('00-0033770',\n 'J.Hardee',\n '7-B.Mann kicks onside 12 yards from NYJ 35 to NYJ 47. RECOVERED by NYJ-34-J.Hardee.')\n\n\n\n(pbp_2022.blocked_player_id[1947],\n pbp_2022.blocked_player_name[1947],\n pbp_2022.desc[1947])\n\n('00-0036926',\n 'P.Turner',\n '(:02) 7-Y.Koo 63 yard field goal is BLOCKED (98-P.Turner), Center-48-L.McCullough, Holder-13-B.Pinion, recovered by ATL-13-B.Pinion at ATL 49. 13-B.Pinion to 50 for 1 yard (53-Z.Baun, 48-J.Gray).')\n\n\nThe following fields show player_id (string), player_name (string) or team (string) for a variety of defensive plays such as tackle for loss, quarterback hit, solo tackle, assist tackle and so on.\n\ntackle_for_loss_1_player_id\ntackle_for_loss_1_player_name\ntackle_for_loss_2_player_id\ntackle_for_loss_2_player_name\nqb_hit_1_player_id\nqb_hit_1_player_name\nqb_hit_2_player_id\nqb_hit_2_player_name\nsolo_tackle_1_team\nsolo_tackle_2_team\nsolo_tackle_1_player_id\nsolo_tackle_2_player_id\nsolo_tackle_1_player_name\nsolo_tackle_2_player_name\nassist_tackle_1_player_id\nassist_tackle_1_player_name\nassist_tackle_1_team\nassist_tackle_2_player_id\nassist_tackle_2_player_name\nassist_tackle_2_team\nassist_tackle_3_player_id\nassist_tackle_3_player_name\nassist_tackle_3_team\nassist_tackle_4_player_id\nassist_tackle_4_player_name\nassist_tackle_4_team\ntackle_with_assist\ntackle_with_assist_1_player_id\ntackle_with_assist_1_player_name\ntackle_with_assist_1_team\ntackle_with_assist_2_player_id\ntackle_with_assist_2_player_name\ntackle_with_assist_2_team\npass_defense_1_player_id\npass_defense_1_player_name\npass_defense_2_player_id\npass_defense_2_player_name\nsack_player_id\nsack_player_name\nhalf_sack_1_player_id\nhalf_sack_1_player_name\nhalf_sack_2_player_id\nhalf_sack_2_player_name\n\n\n(pbp_2022.tackled_for_loss[15],\n pbp_2022.tackle_for_loss_1_player_id[15],\n pbp_2022.tackle_for_loss_1_player_name[15],\n pbp_2022.desc[15])\n\n(1.0,\n '00-0036130',\n 'J.Madubuike',\n '(9:49) 20-Br.Hall right end to NYJ 9 for -2 yards (92-J.Madubuike).')\n\n\nThere are no plays where tackle_for_loss_2_player_id has a value.\n\npbp_2022.tackle_for_loss_2_player_id.unique()\n\narray([nan])\n\n\n\n(pbp_2022.qb_hit[5],\n pbp_2022.qb_hit_1_player_id[5],\n pbp_2022.qb_hit_1_player_name[5],\n pbp_2022.desc[5])\n\n(1.0,\n '00-0026190',\n 'C.Campbell',\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\n\n(pbp_2022.qb_hit[55],\n pbp_2022.qb_hit_1_player_id[55],\n pbp_2022.qb_hit_1_player_name[55],\n pbp_2022.qb_hit_2_player_id[55],\n pbp_2022.qb_hit_2_player_name[55],\n pbp_2022.desc[55])\n\n(1.0,\n '00-0034163',\n 'J.Johnson',\n '00-0034163',\n 'J.Martin',\n '(8:59) (Shotgun) 8-L.Jackson sacked at BAL 49 for -2 yards (sack split by 52-J.Johnson and 54-J.Martin).')\n\n\n\n(pbp_2022.solo_tackle[777],\n pbp_2022.solo_tackle_1_team[777],\n pbp_2022.solo_tackle_1_player_id[777],\n pbp_2022.solo_tackle_1_player_name[777],\n pbp_2022.solo_tackle_2_team[777],\n pbp_2022.solo_tackle_2_player_id[777],\n pbp_2022.solo_tackle_2_player_name[777],\n pbp_2022.desc[777])\n\n(1.0,\n 'MIN',\n '00-0032129',\n 'J.Hicks',\n 'GB',\n '00-0036631',\n 'R.Newman',\n '(12:21) 12-A.Rodgers sacked at GB 35 for -9 yards (58-J.Hicks). FUMBLES (58-J.Hicks) [58-J.Hicks], RECOVERED by MIN-94-D.Tomlinson at GB 33. 94-D.Tomlinson to GB 33 for no gain (70-R.Newman).')\n\n\n\n(pbp_2022.assist_tackle[2],\n pbp_2022.assist_tackle_1_team[2],\n pbp_2022.assist_tackle_1_player_id[2],\n pbp_2022.assist_tackle_1_player_name[2],\n pbp_2022.assist_tackle_2_team[2],\n pbp_2022.assist_tackle_2_player_id[2],\n pbp_2022.assist_tackle_2_player_name[2],\n pbp_2022.desc[2])\n\n(1.0,\n 'BAL',\n '00-0033894',\n 'M.Williams',\n 'BAL',\n '00-0033294',\n 'C.Clark',\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThere are no plays where assist_tackle_3_player_id or assist_tackle_4_player_id have a value.\n\npbp_2022.assist_tackle_3_player_id.unique(), pbp_2022.assist_tackle_4_player_id.unique()\n\n(array([nan]), array([nan]))\n\n\ntackle_with_assist is not the same as assist_tackle.\n\n(pbp_2022.tackle_with_assist[2],\n pbp_2022.tackle_with_assist_1_team[2],\n pbp_2022.tackle_with_assist_1_player_id[2],\n pbp_2022.tackle_with_assist_1_player_name[2],\n pbp_2022.tackle_with_assist_2_team[2],\n pbp_2022.tackle_with_assist_2_player_id[2],\n pbp_2022.tackle_with_assist_2_player_name[2],\n pbp_2022.desc[2])\n\n(0.0,\n nan,\n nan,\n nan,\n nan,\n nan,\n nan,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\n(pbp_2022.tackle_with_assist[22659],\n pbp_2022.tackle_with_assist_1_team[22659],\n pbp_2022.tackle_with_assist_1_player_id[22659],\n pbp_2022.tackle_with_assist_1_player_name[22659],\n pbp_2022.tackle_with_assist_2_team[22659],\n pbp_2022.tackle_with_assist_2_player_id[22659],\n pbp_2022.tackle_with_assist_2_player_name[22659],\n pbp_2022.desc[22659])\n\n(1.0,\n 'LAC',\n '00-0031040',\n 'K.Mack',\n 'ATL',\n '00-0035208',\n 'O.Zaccheaus',\n '(9:31) (No Huddle, Shotgun) 1-M.Mariota pass short left to 5-D.London to LAC 6 for 5 yards (52-K.Mack, 43-M.Davis). FUMBLES (52-K.Mack), RECOVERED by LAC-52-K.Mack at LAC 6. 52-K.Mack pushed ob at 50 for 44 yards (17-O.Zaccheaus, 5-D.London).')\n\n\nI’ll explore this more later before using these fields in analyses, but it seems like the assist_tackle fields provide information on players who assisted with the tackle, while tackle_with_assist lists information of the “main” player who was assisted on the tackle.\n\n(pbp_2022.assist_tackle[22659],\n pbp_2022.assist_tackle_1_team[22659],\n pbp_2022.assist_tackle_1_player_id[22659],\n pbp_2022.assist_tackle_1_player_name[22659],\n pbp_2022.assist_tackle_2_team[22659],\n pbp_2022.assist_tackle_2_player_id[22659],\n pbp_2022.assist_tackle_2_player_name[22659],\n pbp_2022.desc[22659])\n\n(1.0,\n 'LAC',\n '00-0033697',\n 'M.Davis',\n 'ATL',\n '00-0037238',\n 'D.London',\n '(9:31) (No Huddle, Shotgun) 1-M.Mariota pass short left to 5-D.London to LAC 6 for 5 yards (52-K.Mack, 43-M.Davis). FUMBLES (52-K.Mack), RECOVERED by LAC-52-K.Mack at LAC 6. 52-K.Mack pushed ob at 50 for 44 yards (17-O.Zaccheaus, 5-D.London).')\n\n\n\n(pbp_2022.pass_defense_1_player_id[1613],\n pbp_2022.pass_defense_1_player_name[1613],\n pbp_2022.pass_defense_2_player_id[1613],\n pbp_2022.pass_defense_2_player_name[1613],\n pbp_2022.desc[1613])\n\n('00-0033050',\n 'X.Howard',\n '00-0036998',\n 'J.Holland',\n '(10:05) (Shotgun) 10-M.Jones pass deep right intended for 1-D.Parker INTERCEPTED by 8-J.Holland (25-X.Howard) at MIA -3. 8-J.Holland to MIA 28 for 31 yards (76-I.Wynn).')\n\n\nThe following fields show player_id (string), player_name (string) or team (string) for a variety of fumble-related plays:\n\nforced_fumble_player_1_team\nforced_fumble_player_1_player_id\nforced_fumble_player_1_player_name\nforced_fumble_player_2_team\nforced_fumble_player_2_player_id\nforced_fumble_player_2_player_name\nfumbled_1_team\nfumbled_1_player_id\nfumbled_1_player_name\nfumbled_2_player_id\nfumbled_2_player_name\nfumbled_2_team\nfumble_recovery_1_team\nfumble_recovery_1_yards\nfumble_recovery_1_player_id\nfumble_recovery_1_player_name\nfumble_recovery_2_team\nfumble_recovery_2_yards\nfumble_recovery_2_player_id\nfumble_recovery_2_player_name\n\n\n(pbp_2022.fumble_forced[9041],\n pbp_2022.forced_fumble_player_1_team[9041],\n pbp_2022.forced_fumble_player_1_player_id[9041],\n pbp_2022.forced_fumble_player_1_player_name[9041],\n pbp_2022.forced_fumble_player_2_team[9041],\n pbp_2022.forced_fumble_player_2_player_id[9041],\n pbp_2022.forced_fumble_player_2_player_name[9041],\n pbp_2022.desc[9041])\n\n(1.0,\n 'NYG',\n '00-0033046',\n 'J.Ward',\n 'NYG',\n '00-0036167',\n 'T.Crowder',\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.fumbled_1_team[9041],\n pbp_2022.fumbled_1_player_id[9041],\n pbp_2022.fumbled_1_player_name[9041],\n pbp_2022.fumbled_2_team[9041],\n pbp_2022.fumbled_2_player_id[9041],\n pbp_2022.fumbled_2_player_name[9041],\n pbp_2022.desc[9041])\n\n('CHI',\n '00-0034279',\n 'E.St. Brown',\n 'CHI',\n '00-0036953',\n 'T.Ebner',\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.fumble_recovery_1_team[9041],\n pbp_2022.fumble_recovery_1_player_id[9041],\n pbp_2022.fumble_recovery_1_player_name[9041],\n pbp_2022.fumble_recovery_1_yards[9041],\n pbp_2022.fumble_recovery_2_team[9041],\n pbp_2022.fumble_recovery_2_player_id[9041],\n pbp_2022.fumble_recovery_2_player_name[9041],\n pbp_2022.fumble_recovery_2_yards[9041],\n pbp_2022.desc[9041])\n\n('CHI',\n '00-0036945',\n 'J.Fields',\n -3.0,\n 'CHI',\n '00-0033082',\n 'L.Patrick',\n -6.0,\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.sack[54],\n pbp_2022.sack_player_name[54],\n pbp_2022.sack_player_id[54],\n pbp_2022.desc[54])\n\n(1.0,\n 'Qu.Williams',\n '00-0035680',\n '(9:43) (Shotgun) 8-L.Jackson sacked ob at NYJ 49 for 0 yards (56-Qu.Williams).')\n\n\nWhen a sack is split, sack == 1 but sack_player_name and id are nan.\n\n(pbp_2022.sack[55],\n pbp_2022.sack_player_name[55],\n pbp_2022.sack_player_id[55],\n pbp_2022.half_sack_1_player_id[55],\n pbp_2022.half_sack_1_player_name[55],\n pbp_2022.half_sack_2_player_id[55],\n pbp_2022.half_sack_2_player_name[55],\n pbp_2022.desc[55])\n\n(1.0,\n nan,\n nan,\n '00-0034163',\n 'J.Johnson',\n '00-0034163',\n 'J.Martin',\n '(8:59) (Shotgun) 8-L.Jackson sacked at BAL 49 for -2 yards (sack split by 52-J.Johnson and 54-J.Martin).')\n\n\nreturn_team (string) and return_yards (integer) are the abbreviation and yardage of the team that returned the kickoff or punt. I’ll look into if fumble returns are included before I use this field for analyses.\n\n(pbp_2022.return_team[1], \n pbp_2022.return_yards[1],\n pbp_2022.desc[1])\n\n('NYJ',\n 25.0,\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\nThe following fields hold information about penalties.\n\npenalty_team (string)\npenalty_player_id (string)\npenalty_player_name (string)\npenalty_yards (integer)\npenalty_type (string)\n\n\n(pbp_2022.penalty[5],\n pbp_2022.penalty_team[5],\n pbp_2022.penalty_player_id[5],\n pbp_2022.penalty_player_name[5],\n pbp_2022.penalty_yards[5],\n pbp_2022.penalty_type[5],\n pbp_2022.desc[5])\n\n(1.0,\n 'NYJ',\n '00-0026158',\n 'J.Flacco',\n 10.0,\n 'Intentional Grounding',\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\n\npbp_2022.penalty_type.unique()\n\narray([nan, 'Intentional Grounding', 'Illegal Contact',\n       'Offensive Holding', 'Defensive Pass Interference',\n       'Defensive Holding', 'Offensive Pass Interference', 'False Start',\n       'Horse Collar Tackle', 'Defensive Too Many Men on Field',\n       'Taunting', 'Delay of Game', 'Roughing the Passer',\n       'Unsportsmanlike Conduct', 'Low Block', 'Illegal Formation',\n       'Ineligible Downfield Pass', 'Unnecessary Roughness',\n       'Neutral Zone Infraction', 'Running Into the Kicker',\n       'Illegal Shift', 'Defensive Offside', 'Illegal Use of Hands',\n       'Illegal Block Above the Waist', 'Offensive Too Many Men on Field',\n       'Encroachment', 'Disqualification', 'Ineligible Downfield Kick',\n       'Face Mask', 'Player Out of Bounds on Kick',\n       'Illegal Forward Pass', 'Chop Block', 'Delay of Kickoff',\n       'Tripping', 'Illegal Substitution', 'Offensive Offside',\n       'Illegal Blindside Block', 'Illegal Touch Pass',\n       'Offside on Free Kick', 'Roughing the Kicker',\n       'Fair Catch Interference', 'Leverage', 'Illegal Motion',\n       'Defensive Delay of Game', 'Illegal Bat', 'Illegal Touch Kick',\n       'Illegal Double-Team Block', 'Invalid Fair Catch Signal',\n       'Illegal Crackback', 'Illegal Kick/Kicking Loose Ball'],\n      dtype=object)\n\n\nreplay_or_challenge (1 for True and 0 for False) and replay_or_challenge_result (nan, 'upheld', or 'reversed') show information about whether a replay or challenge occurred on the play.\n\n(pbp_2022.replay_or_challenge[621],\n pbp_2022.replay_or_challenge_result[621],\n pbp_2022.desc[621])\n\n(1,\n 'upheld',\n '(7:42) (Shotgun) 25-M.Gordon right tackle to SEA 1 for no gain (6-Q.Diggs, 10-U.Nwosu). FUMBLES (6-Q.Diggs), RECOVERED by SEA-30-M.Jackson at SEA 2. 30-M.Jackson to SEA 10 for 8 yards (14-C.Sutton). The Replay Official reviewed the fumble ruling, and the play was Upheld. The ruling on the field stands.')\n\n\nsafety_player_name and safety_player_id have information about the player who caused the safety.\n\n(pbp_2022.safety[3255],\n pbp_2022.safety_player_name[3255],\n pbp_2022.safety_player_id[3255],\n pbp_2022.desc[3255])\n\n(1.0,\n 'D.Alford',\n '00-0037034',\n '(:13) (Run formation) 19-B.Powell right end ran ob in End Zone for -26 yards, SAFETY (37-D.Alford).')\n\n\nseries_result is the result of the offensive series.\n\npbp_2022.series_result.unique()\n\narray(['First down', 'Punt', 'Turnover', 'Field goal',\n       'Missed field goal', 'Touchdown', 'End of half',\n       'Turnover on downs', 'QB kneel', 'Opp touchdown', 'Safety', nan],\n      dtype=object)\n\n\nplay_type_nfl shows slightly different play type categories.\n\npbp_2022.play_type_nfl.unique()\n\narray(['GAME_START', 'KICK_OFF', 'RUSH', 'PASS', 'PUNT', 'TIMEOUT',\n       'PENALTY', 'FIELD_GOAL', 'END_QUARTER', 'SACK', 'XP_KICK',\n       'END_GAME', 'PAT2', nan, 'FREE_KICK'], dtype=object)\n\n\ndrive_play_count shows how many plays the drive had. I’ll look into it more before using it for analyses. It doesn’t always match the number of plays on the drive, or at least seems not to, so I need to understand how they calculate this value.\n\npbp_2022.drive_play_count.unique()\n\narray([nan,  4.,  6.,  5.,  3.,  8.,  1.,  9., 16., 11.,  2., 13.,  7.,\n       14., 10., 15., 12.,  0., 18., 19., 20., 17., 21.])\n\n\ndrive_time_of_possession is a formatted string of minutes:seconds the drive took.\n\npbp_2022.drive_time_of_possession.unique()[:5]\n\narray([nan, '1:18', '3:53', '2:44', '1:04'], dtype=object)\n\n\ndrive_first_downs is the number of first downs achieved on the drive.\n\npbp_2022.drive_first_downs.unique()[:5]\n\narray([nan,  1.,  0.,  3.,  2.])\n\n\ndrive_inside20 is either nan, 1 (True) or 0 (False) and indicates if a drive ended inside of the red zone (20 yards from the end zone).\n\npbp_2022.drive_inside20.unique()\n\narray([nan,  0.,  1.])\n\n\ndrive_ended_with_score indicates if a drive ended with the offensive team scoring. It is either nan, 1 (True) or 0 (False).\n\npbp_2022.drive_ended_with_score.unique()\n\narray([nan,  0.,  1.])\n\n\nI’ll have to look into it more before using it for analyses, but I believe drive_yards_penalized is the total number of offensive penalty yards on the drive.\n\npbp_2022.drive_yards_penalized.unique()[:5]\n\narray([ nan, -10.,   0.,   5.,  32.])\n\n\ndrive_play_id_started and drive_play_id_ended indicate the start and end play_id of the drive. Note that play_id are not consecutive and doesn’t start at 1.\n\n(pbp_2022.drive_play_id_started[1],\npbp_2022.drive_play_id_ended[1])\n\n(43.0, 172.0)\n\n\naway_score and home_score are the final scores of the away team and home team.\n\n(pbp_2022.away_team[1],\n pbp_2022.away_score[1],\n pbp_2022.home_team[1],\n pbp_2022.home_score[1])\n\n('BAL', 24, 'NYJ', 9)\n\n\nresult is the difference between the home and the away team (I think—will look into it more).\n\npbp_2022.result[1]\n\n-15\n\n\ntotal is the total number of points scored by both teams.\n\npbp_2022.total[1]\n\n33\n\n\ndiv_game indicates if the game is between teams in the same division. It is either 1 (True) or 0 (False).\n\npbp_2022.div_game.unique(), pbp_2022.div_game[1]\n\n(array([0, 1]), 0)\n\n\naway_coach and home_coach are the names of the away team and home team coaches, respectively.\n\npbp_2022.away_coach[1], pbp_2022.home_coach[1]\n\n('John Harbaugh', 'Robert Saleh')\n\n\nThe following fields give the name and jersey number of the passer, rusher or receiver on the play:\n\npasser\npasser_id\npasser_jersey_number\nrusher\nrusher_id\nrusher_jersey_number\nreceiver\nreceiver_id\nreceiver_jersey_number\n\n\n(pbp_2022.passer[3], \n pbp_2022.passer_id[3],\n pbp_2022.passer_jersey_number[3])\n\n('J.Flacco', '00-0026158', 19.0)\n\n\n\n(pbp_2022.rusher[2], \n pbp_2022.rusher_id[2],\n pbp_2022.rusher_jersey_number[2])\n\n('Mi.Carter', '00-0036924', 32.0)\n\n\n\n(pbp_2022.receiver[3], \n pbp_2022.receiver_id[3],\n pbp_2022.receiver_jersey_number[3])\n\n('Mi.Carter', '00-0036924', 32.0)\n\n\nThe following fields indicate if the play is a pass, rush, first down, or special teams, respectively. Their value is nan, 1 (True) or 0 (False):\n\npass\nrush\nfirst_down\nspecial\n\n\npbp_2022['pass'][3], pbp_2022.desc[3]\n\n(1,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\npbp_2022.rush[2], pbp_2022.desc[2]\n\n(1,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\npbp_2022.first_down[2], pbp_2022.desc[2]\n\n(1.0,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\npbp_2022.special[1], pbp_2022.desc[1]\n\n(1,\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')"
  },
  {
    "objectID": "posts/2024-05-15-nfl-schedule-release/index.html",
    "href": "posts/2024-05-15-nfl-schedule-release/index.html",
    "title": "Initial Reaction: Eagles 2024 NFL Schedule",
    "section": "",
    "text": "The NFL schedule got released today! My mood has instantly improved. Fall is my favorite season in general, and watching football is one of my purest joys (and purest stressors). In this blog post I’ll write down my thoughts and predictions after a few minutes of looking at the 2024 Philadelphia Eagles’ schedule."
  },
  {
    "objectID": "posts/2024-05-15-nfl-schedule-release/index.html#initial-reaction-i-really-like-it.",
    "href": "posts/2024-05-15-nfl-schedule-release/index.html#initial-reaction-i-really-like-it.",
    "title": "Initial Reaction: Eagles 2024 NFL Schedule",
    "section": "Initial Reaction: I really like it.",
    "text": "Initial Reaction: I really like it.\nAt first glance, I think this is a great schedule. It’s tied for the 10th-easiest schedule in the league. The longest stretch against teams with 2023 winning records is three weeks (at Cincy, Jax, at Dallas). The only other such stretches are two weeks long (at NO, at TB—revenge match; at LAR, at BAL)."
  },
  {
    "objectID": "posts/2024-05-15-nfl-schedule-release/index.html#prediction-12-5",
    "href": "posts/2024-05-15-nfl-schedule-release/index.html#prediction-12-5",
    "title": "Initial Reaction: Eagles 2024 NFL Schedule",
    "section": "Prediction: 12-5",
    "text": "Prediction: 12-5\n\n\n\nMy knee-jerk predictions for the 2024 Philadelphia Eagles season\n\n\nThe toughest matches outside the division are against the AFC North. I would be happy if we went 2-2 against them. I think Eagles beat Cincy and Cleveland and lose to the Ravens and Steelers.\nDivision matchups are always tricky but with Kellen Moore’s offensive scheme (which hopefully takes advantage of checkdowns, quick throws and the middle of the field) I think the Eagles won’t stumble against the Giants. Washington is always a weird matchup, they always seem to play really well against the Eagles (at least the last few years) so I think we split against them. Same for Dallas. International games are always weird with jet lag playing a factor, so I think Eagles lose to Green Bay in Brazil. That means the Eagles end up 4-4 against 2023 playoff teams."
  },
  {
    "objectID": "posts/2024-05-15-nfl-schedule-release/index.html#success-at-least-two-playoff-wins",
    "href": "posts/2024-05-15-nfl-schedule-release/index.html#success-at-least-two-playoff-wins",
    "title": "Initial Reaction: Eagles 2024 NFL Schedule",
    "section": "Success = At Least Two Playoff Wins",
    "text": "Success = At Least Two Playoff Wins\nAs has been the case the last two seasons, I feel that a successful season would result in at least two playoff wins. If we get the 1 seed, that means making it to the Super Bowl, otherwise at least making it to the NFCCG.\nGo Birds!"
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "",
    "text": "In this blog post I’ll work through the third “Further Research” exercise from Chapter 9 of the fastai textbook:\n\nUse the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw.\n\nI’ll train a neural net on the dataset, grab its embeddings for categorical variables, replace the existing categorical variables in the dataset with them, and then train a random forest on that updated dataset.\nThe neural net in the chapter was trained on only a subset of columns (after removing columns that were redundant and low importance). I’ll first go through the process of reducing the number of used columns with a random forest as done in the chapter text before training the neural net.\nIn addition to the textbook, I also heavily reference the code provided in this medium post by Adam Mehdi a similar implementation of which was shared in this fastai forum post (sign-in required).\n\nHere is a summary of error values for models fit on the original and embedding-filled datasets in this exercise:\n\n\n\nModel\nValidation MSE\nReduction in MSE\n\n\n\n\nRF (no embedding columns)\n0.247074\n–\n\n\nRF (329 embedding columns)\n0.238825\n3.33%\n\n\nRF (32 embedding columns)\n0.243094\n1.6%"
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#load-the-data",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#load-the-data",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Load the Data",
    "text": "Load the Data\n\n!pip install dtreeviz\n\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\n\nimport zipfile,kaggle\n\npath = Path('bluebook-for-bulldozers')\nif not path.exists():\n  kaggle.api.competition_download_cli(str(path))\n  zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading bluebook-for-bulldozers.zip to /content\n\n\n100%|██████████| 48.4M/48.4M [00:00<00:00, 65.2MB/s]\n\n\n\n\n\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\n\n\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      SalesID\n      SalePrice\n      MachineID\n      ModelID\n      ...\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n    \n  \n  \n    \n      0\n      1139246\n      66000.0\n      999089\n      3157\n      ...\n      NaN\n      NaN\n      Standard\n      Conventional\n    \n    \n      1\n      1139248\n      57000.0\n      117657\n      77\n      ...\n      NaN\n      NaN\n      Standard\n      Conventional\n    \n    \n      2\n      1139249\n      10000.0\n      434808\n      7009\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      1139251\n      38500.0\n      1026470\n      332\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      1139253\n      11000.0\n      1057373\n      17311\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 53 columns"
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#clean-the-data",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#clean-the-data",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Clean the Data",
    "text": "Clean the Data\nCategorize the ProductSize variable:\n\nsizes = 'Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\nTake the log of the dependent variable, SalePrice:\n\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\nAdd date-related additional columns—I’ll deviate from the textbook here and save the original saledate column so I can more accurately create the training and validation sets. I’ll likely remove it later on when I remove redundant features.\n\nsaledate = df.saledate\ndf = add_datepart(df, 'saledate')\ndf['saledate'] = saledate\n\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed saledate'"
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#create-training-and-validation-sets",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#create-training-and-validation-sets",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Create Training and Validation Sets",
    "text": "Create Training and Validation Sets\n\nprocs = [Categorify, FillMissing]\n\nI’ll define a validation set consisting of data from after November 2011:\n\ndf['saledate'] = pd.to_datetime(df['saledate'])\n\n\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      SalesID\n      SalePrice\n      MachineID\n      ModelID\n      ...\n      saleIs_year_end\n      saleIs_year_start\n      saleElapsed\n      saledate\n    \n  \n  \n    \n      0\n      1139246\n      11.097410\n      999089\n      3157\n      ...\n      False\n      False\n      1.163635e+09\n      2006-11-16\n    \n    \n      1\n      1139248\n      10.950807\n      117657\n      77\n      ...\n      False\n      False\n      1.080259e+09\n      2004-03-26\n    \n    \n      2\n      1139249\n      9.210340\n      434808\n      7009\n      ...\n      False\n      False\n      1.077754e+09\n      2004-02-26\n    \n    \n      3\n      1139251\n      10.558414\n      1026470\n      332\n      ...\n      False\n      False\n      1.305763e+09\n      2011-05-19\n    \n    \n      4\n      1139253\n      9.305651\n      1057373\n      17311\n      ...\n      False\n      False\n      1.248307e+09\n      2009-07-23\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      412693\n      6333344\n      9.210340\n      1919201\n      21435\n      ...\n      False\n      False\n      1.331078e+09\n      2012-03-07\n    \n    \n      412694\n      6333345\n      9.259131\n      1882122\n      21436\n      ...\n      False\n      False\n      1.327709e+09\n      2012-01-28\n    \n    \n      412695\n      6333347\n      9.433484\n      1944213\n      21435\n      ...\n      False\n      False\n      1.327709e+09\n      2012-01-28\n    \n    \n      412696\n      6333348\n      9.210340\n      1794518\n      21435\n      ...\n      False\n      False\n      1.331078e+09\n      2012-03-07\n    \n    \n      412697\n      6333349\n      9.472705\n      1944743\n      21436\n      ...\n      False\n      False\n      1.327709e+09\n      2012-01-28\n    \n  \n\n412698 rows × 66 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf['saledate'] < '2011-11-01'\n\n0          True\n1          True\n2          True\n3          True\n4          True\n          ...  \n412693    False\n412694    False\n412695    False\n412696    False\n412697    False\nName: saledate, Length: 412698, dtype: bool\n\n\n\ncond = df['saledate'] < '2011-11-01'\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx), list(valid_idx))\n\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\nlen(to.train), len(to.valid)\n\n(395371, 17327)\n\n\n\nxs,y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y"
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#removing-features",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#removing-features",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Removing Features",
    "text": "Removing Features\nTo simplify our model, and in some cases improve the accuracy, I will follow the procedure in the textbook to reduce the number of features we use for training.\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs):\n  return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs,y)\n\n\ndef rf_feat_importance(m, df):\n  return pd.DataFrame({\n      'cols': df.columns,\n      'imp': m.feature_importances_}\n                      ).sort_values('imp', ascending=False)\n\nI’ll start by fitting a random forest to the data and establishing a baseline MSE on the validation set to use in comparison at each step of reducing features.\n\nm = rf(xs, y);\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.169314, 0.29212)\n\n\n\nLow-Importance Features\nNext, I’ll take a look at the most important features and remove low-importance features from the dataset.\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      58\n      YearMade\n      0.182072\n    \n    \n      6\n      ProductSize\n      0.114709\n    \n    \n      30\n      Coupler_System\n      0.104344\n    \n    \n      7\n      fiProductClassDesc\n      0.064090\n    \n    \n      55\n      ModelID\n      0.057407\n    \n    \n      31\n      Grouser_Tracks\n      0.047770\n    \n    \n      3\n      fiSecondaryDesc\n      0.042480\n    \n    \n      10\n      ProductGroupDesc\n      0.038196\n    \n    \n      32\n      Hydraulics_Flow\n      0.034735\n    \n    \n      50\n      saledate\n      0.033807\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndef plot_fi(fi):\n  return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\nto_keep = fi[fi.imp>0.005].cols\nlen(to_keep)\n\n23\n\n\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\nI’ll check the accuracy on the validation set with the low-importance features removed:\n\nm = rf(xs_imp, y);\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n\n(0.178435, 0.284805)\n\n\nOur model is much simpler (fewer features to analyze) with a similar error as before.\n\nlen(xs.columns), len(xs_imp.columns)\n\n(67, 23)\n\n\n\n\nRedundant Features\nNext, I’ll take a look at which features are redundant and try to remove some of them (as long as they don’t diminish the model’s OOB score):\n\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n\n\ncluster_columns(xs_imp)\n\n\n\n\n\ndef get_oob(df):\n  m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n                            max_samples=50_000, max_features=0.5, n_jobs=-1, oob_score=True)\n  m.fit(df, y)\n  return m.oob_score_\n\nHere’s the baseline OOB score for the dataset with only high-importance variables:\n\nget_oob(xs_imp)\n\n0.8775680670271139\n\n\nHere are the OOB scores if we drop redundant features from the dataset—the higher the OOB score the better:\n\n{c: get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'Hydraulics_Flow', 'Grouser_Tracks', 'Coupler_System',\n    'saleElapsed', 'saledate', 'saleYear',\n    'ProductGroup', 'ProductGroupDesc',\n    'fiBaseModel', 'fiModelDesc')}\n\n{'Hydraulics_Flow': 0.8784714680954607,\n 'Grouser_Tracks': 0.8781347745872132,\n 'Coupler_System': 0.8782757080902821,\n 'saleElapsed': 0.8780747364566069,\n 'saledate': 0.8775743905455275,\n 'saleYear': 0.8777160634533703,\n 'ProductGroup': 0.8777345710442639,\n 'ProductGroupDesc': 0.8785070588073342,\n 'fiBaseModel': 0.8775911401001298,\n 'fiModelDesc': 0.8764381706728157}\n\n\nI’ll select the variable from each redundant group, the removal of which increased the OOB score (or kept it the same) and remove it from the dataset:\n\nto_drop = ['Hydraulics_Flow', 'saleElapsed', 'ProductGroupDesc', 'fiBaseModel']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8759470650709008\n\n\n\n0.8759470650709008/0.8775680670271139\n\n0.9981528476056514\n\n\nThe OOB score slightly decreased, but now we have fewer redundant features in the dataset.\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\n# check error\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n\n(0.181023, 0.37027)\n\n\nThe validation error is more than twice the training error and has considerably increased from previous datasets. I’ll see if removing out-of-domain features improves the error.\n\n\nOut-of-Domain Features\nI’ll remove columns from the training set that are out-of-domain (i.e., significantly different in values from) in the validation set. I’ll identify these columns by fitting a random forest to predict whether a row is in the training or validation set and then observing the most important features in this prediction:\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      7\n      saledate\n      0.704401\n    \n    \n      15\n      saleYear\n      0.215157\n    \n    \n      10\n      SalesID\n      0.075075\n    \n    \n      13\n      MachineID\n      0.003997\n    \n    \n      0\n      YearMade\n      0.000683\n    \n    \n      4\n      ModelID\n      0.000228\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe most important feature in predicting whether a row is in the training or validation set is the saledate feature. This makes sense because I explicitly define the training and validation sets based on saledate. The next two features of high importance are saleYear and SalesID which also are related to when the sale occured.\n\n# baseline\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('saledate', 'saleYear', 'SalesID'):\n  m = rf(xs_final.drop(c, axis=1), y)\n  print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.365206\nsaledate 0.246061\nsaleYear 0.437766\nSalesID 0.402223\n\n\nRemoving saledate reduces the error by a third!\n\nxs_final_time = xs_final.drop('saledate', axis=1)\nvalid_xs_time = valid_xs_final.drop('saledate', axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n\nm_rmse(m, xs_final_time, y), m_rmse(m, valid_xs_time, valid_y)\n\n(0.189063, 0.247074)\n\n\n\nlen(xs_final_time.columns)\n\n18\n\n\nThe validation error is still larger than the training error but removing saledate has reduced the validation error considerably, even when compared to the original dataset and the dataset after only low-importance features were removed. I consider this process of feature removal successful."
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#train-a-neural-net",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#train-a-neural-net",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Train a Neural Net",
    "text": "Train a Neural Net\nI’ll re-load the data and apply the same feature engineering as before:\n\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\n\nlen(df_nn_final.columns)\n\n19\n\n\nI’ll split the columns into continuous and categorical groups:\n\ncont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\nI’ll look at the cardinality of the categorical variables and see if there are any variables that have similarly large cardinality (as those would be candidates for removal). I also want to look at cardinality so I can later on identify which embeddings belong to which categorical variables.\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nModelID               5281\nGrouser_Tracks           2\nfiSecondaryDesc        177\nfiModelDesc           5059\nProductGroup             6\nEnclosure                6\nfiModelDescriptor      140\nHydraulics              12\nsaleYear                24\nDrive_System             4\nTire_Size               17\nPad_Type                 4\ndtype: int64\n\n\nThe two features with similar high cardinality are ModelID and fiModelDesc. I’ll remove and see which one improves the model more.\n\nxs_final_time2 = xs_final_time.drop('ModelID', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('ModelID', axis=1)\nm2 = rf(xs_final_time2, y)\nm_rmse(m2, xs_final_time2, y), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.190922, 0.250589)\n\n\n\nxs_final_time2 = xs_final_time.drop('fiModelDesc', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDesc', axis=1)\nm2 = rf(xs_final_time2, y)\nm_rmse(m2, xs_final_time2, y), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.192338, 0.251594)\n\n\nRemoving ModelID gives a smaller error than removing fiModelDesc. Also, we want to predict future auction sales so I’ll move saleYear to cont_nn (as categorical values cannot be extrapolated beyond their existing levels).\n\ncat_nn.remove('ModelID')\ncat_nn.remove('saleYear')\ncont_nn.append('saleYear')\n\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nGrouser_Tracks           2\nfiSecondaryDesc        177\nfiModelDesc           5059\nProductGroup             6\nEnclosure                6\nfiModelDescriptor      140\nHydraulics              12\nDrive_System             4\nTire_Size               17\nPad_Type                 4\ndtype: int64\n\n\n\ncont_nn\n\n['SalesID', 'MachineID', 'saleYear']\n\n\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)\n\n\n# save for later\ndata = (xs_final_time, y, valid_xs_time, valid_y)\nsave_pickle(\"to_nn.pkl\", to_nn)\nsave_pickle(\"data.pkl\", data)\n\n\n# load objects\nto_nn = load_pickle(\"to_nn.pkl\")\nxs_final_time, y, valid_xs_time, valid_y = load_pickle(\"data.pkl\")\n\n\ndls = to_nn.dataloaders(1024)\n\n\n# set y-range\ny = to_nn.train.y\ny.min(), y.max()\n\n(8.465899, 11.863583)\n\n\n\nfrom fastai.tabular.all import *\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00013182566908653826)\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.066889\n      0.173385\n      00:30\n    \n    \n      1\n      0.055220\n      0.071100\n      00:27\n    \n    \n      2\n      0.050529\n      0.059935\n      00:30\n    \n    \n      3\n      0.045348\n      0.058667\n      00:27\n    \n    \n      4\n      0.041716\n      0.058603\n      00:27\n    \n  \n\n\n\n\npreds, targs = learn.get_preds()\nr_mse(preds, targs)\n\n\n\n\n\n\n\n\n0.242081"
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#neural-net-embeddings",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#neural-net-embeddings",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Neural Net Embeddings",
    "text": "Neural Net Embeddings\nNow that the model is trained, let’s take a look at the embeddings that it created. The first dimension of each embedding roughly corresponds to the number of unique values in the corresponding categorical column:\n\nlearn.model.embeds\n\nModuleList(\n  (0): Embedding(72, 18)\n  (1): Embedding(7, 5)\n  (2): Embedding(3, 3)\n  (3): Embedding(75, 18)\n  (4): Embedding(3, 3)\n  (5): Embedding(178, 29)\n  (6): Embedding(5060, 190)\n  (7-8): 2 x Embedding(7, 5)\n  (9): Embedding(141, 26)\n  (10): Embedding(13, 7)\n  (11): Embedding(5, 4)\n  (12): Embedding(18, 8)\n  (13): Embedding(5, 4)\n)\n\n\n\nto_nn.train.xs[to_nn.cat_names].nunique()\n\nYearMade                71\nProductSize              7\nCoupler_System           3\nfiProductClassDesc      74\nGrouser_Tracks           3\nfiSecondaryDesc        176\nfiModelDesc           4965\nProductGroup             6\nEnclosure                7\nfiModelDescriptor      140\nHydraulics              13\nDrive_System             5\nTire_Size               18\nPad_Type                 5\ndtype: int64\n\n\nIn this medium blog post Adam Mehdi uses the following code to replace categorical columns in the training set with embedding matrices:\ndef embed_features(learner, xs):\n  \"\"\"\n  learner: fastai Learner used to train the neural net\n  xs: DataFrame containing input variables. Categorical values are defined by their rank.\n ::return:: copy of `xs` with embeddings replacing each categorical variable\n  \"\"\"\n  xs = xs.copy()\n  for i,col in enumerate(learn.dls.cat_names):\n    \n    # get matrix containing each row's embedding vector\n    emb = learn.model.embeds[i]\n    emb_data = emb(tensor(xs[col], dtype=torch.int64))\n    emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n    \n    # join the embedded category and drop the old feature column\n    feat_df = pd.DataFrame(data=emb_data, index=xs.index,               \n                           columns=emb_names)\n    xs = xs.drop(col, axis=1)\n    xs = xs.join(feat_df)\n  return xs\nI’ll work through the code line-by-line for one of the categorical columns, ProductSize. First we grab the column’s corresponding Embedding from the model:\n\nemb = learn.model.embeds[1]\nemb\n\nEmbedding(7, 5)\n\n\nI then pass all ProductSize values to that Embedding as an integer tensor. The output is a matrix with one row for each training observation, and 5 columns (chosen by the model for ProductSize):\n\nemb_data = emb(tensor(xs_final_time['ProductSize'], dtype=torch.int64))\nemb_data.shape\n\ntorch.Size([395371, 5])\n\n\n\nlen(xs_final_time)\n\n395371\n\n\nTo appropriately name the columns in the updated dataset, I label the embedding layer names as done in the medium post:\n\nemb_names = [f'ProductSize_{j}' for j in range(emb_data.shape[1])]\nemb_names\n\n['ProductSize_0',\n 'ProductSize_1',\n 'ProductSize_2',\n 'ProductSize_3',\n 'ProductSize_4']\n\n\nI then convert the 2-D tensor to a DataFrame:\n\nfeat_df = pd.DataFrame(\n    data=emb_data,\n    index=xs_final_time.index,\n    columns=emb_names)\n\nfeat_df.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      ProductSize_0\n      ProductSize_1\n      ProductSize_2\n      ProductSize_3\n      ProductSize_4\n    \n  \n  \n    \n      0\n      -0.003886\n      0.204614\n      -0.099826\n      0.152207\n      0.215685\n    \n    \n      1\n      -0.114329\n      -0.100198\n      -0.071906\n      -0.128801\n      -0.039516\n    \n    \n      2\n      -0.003886\n      0.204614\n      -0.099826\n      0.152207\n      0.215685\n    \n    \n      3\n      -0.005051\n      0.042276\n      -0.102471\n      0.016768\n      0.001030\n    \n    \n      4\n      -0.003886\n      0.204614\n      -0.099826\n      0.152207\n      0.215685\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nFinally, I drop the ProductSize column from the dataset and replace it with my embedding DataFrame:\n\nxs_temp = xs_final_time.drop('ProductSize', axis=1)\nxs_temp = xs_temp.join(feat_df)\nxs_temp.columns\n\nIndex(['YearMade', 'Coupler_System', 'fiProductClassDesc', 'ModelID',\n       'Grouser_Tracks', 'fiSecondaryDesc', 'fiModelDesc', 'ProductGroup',\n       'SalesID', 'Enclosure', 'fiModelDescriptor', 'MachineID', 'Hydraulics',\n       'saleYear', 'Drive_System', 'Tire_Size', 'Pad_Type', 'ProductSize_0',\n       'ProductSize_1', 'ProductSize_2', 'ProductSize_3', 'ProductSize_4'],\n      dtype='object')\n\n\nWith a single iteration done successfully, now I can run the whole loop and get the updated dataset with embedding matrices:\n\ndef embed_features(learn, xs):\n  xs = xs.copy()\n\n  for i, col in enumerate(learn.dls.cat_names):\n    emb = learn.model.embeds[i]\n    emb_data = emb(tensor(xs[col], dtype=torch.int64))\n    emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n\n    feat_df = pd.DataFrame(\n        data=emb_data,\n        index=xs.index,\n        columns=emb_names\n    )\n\n    xs = xs.drop(col, axis=1)\n    xs = xs.join(feat_df)\n  return xs\n\nOne bit of preprocessing I’ll have to do is swap the categorical columns in xs_final_time with the ones in to_nn.train.xs since the latter has been “categorified” by the Categorify fastai processor into 0 to n integer values. To illustrate: the first value of the YearMade column in xs_final_time is 2004 whereas it’s converted to the integer 63 in the corresponding to_nn.train.xs column. The embedding corresponding to this column has a dimension of 72 so it can take a maximum value of 71 as the input, which is the maximum value in to_nn.train.xs['YearMade'].\n\nxs_final_time['YearMade'][0]\n\n2004\n\n\n\nto_nn.train.xs['YearMade'][0]\n\n63\n\n\n\nlearn.model.embeds[0]\n\nEmbedding(72, 18)\n\n\n\nto_nn.train.xs['YearMade'].max()\n\n71\n\n\nHere’s a bit of code to swap the columns:\n\ndef prep_xs(to_nn, xs):\n  xs = xs.copy()\n  for col in to_nn.train.xs.columns:\n    xs[col] = to_nn[col]\n  return xs\n\n\nxs_with_embs = prep_xs(to_nn, xs_final_time)\nxs_with_embs['YearMade'].unique()\n\narray([63, 55, 60, 66, 52, 67,  1, 57, 58, 62, 50, 59, 64, 54, 65, 61, 43,\n       47, 39, 51, 46, 56, 30, 37, 48, 44, 38, 35, 53, 41, 49, 33, 27, 25,\n       42, 45, 40, 29, 36, 34, 32, 24, 26, 68, 69, 28, 31, 23, 16, 17, 22,\n        2,  3,  9,  7, 11,  5, 15, 13, 12, 14, 18, 19, 20, 21, 10,  4,  8,\n        6, 71, 70], dtype=int8)\n\n\nNow that I have categorified the categorical columns the embeddings are trained on, I can process to swap them with embedding matrices:\n\nxs_with_embs = embed_features(learn, xs_with_embs)\nxs_with_embs.columns\n\nIndex(['ModelID', 'SalesID', 'MachineID', 'saleYear', 'YearMade_0',\n       'YearMade_1', 'YearMade_2', 'YearMade_3', 'YearMade_4', 'YearMade_5',\n       ...\n       'Tire_Size_2', 'Tire_Size_3', 'Tire_Size_4', 'Tire_Size_5',\n       'Tire_Size_6', 'Tire_Size_7', 'Pad_Type_0', 'Pad_Type_1', 'Pad_Type_2',\n       'Pad_Type_3'],\n      dtype='object', length=329)\n\n\nI’ll check to see that the number of columns corresponding to the embedding matches the number of columns in the second dimension of the embedding matrix:\n\nlearn.model.embeds[0]\n\nEmbedding(72, 18)\n\n\n\n' '.join(o for o in xs_with_embs.columns if o.startswith('YearMade'))\n\n'YearMade_0 YearMade_1 YearMade_2 YearMade_3 YearMade_4 YearMade_5 YearMade_6 YearMade_7 YearMade_8 YearMade_9 YearMade_10 YearMade_11 YearMade_12 YearMade_13 YearMade_14 YearMade_15 YearMade_16 YearMade_17'\n\n\n\nlearn.model.embeds[1]\n\nEmbedding(7, 5)\n\n\n\n' '.join(o for o in xs_with_embs.columns if o.startswith('ProductSize'))\n\n'ProductSize_0 ProductSize_1 ProductSize_2 ProductSize_3 ProductSize_4'\n\n\nI’ll make the same changes to the validation set:\n\nvalid_xs_with_embs = prep_xs(to_nn, valid_xs_time)\nvalid_xs_with_embs['YearMade'].unique()\n\narray([ 1, 63, 61, 66, 68, 64, 60, 57, 58, 65, 67, 59, 48, 62, 56, 40, 44,\n       49, 55, 34, 35, 39, 54, 42, 41, 23, 45, 38, 43, 52, 47, 51, 53, 50,\n       33, 37, 36, 46, 27, 30, 28, 32, 25, 31, 24, 15, 22, 29, 21, 26,  0,\n       70, 69, 17, 16, 12], dtype=int8)\n\n\n\nvalid_xs_with_embs = embed_features(learn, valid_xs_with_embs)\nvalid_xs_with_embs.columns\n\nIndex(['ModelID', 'SalesID', 'MachineID', 'saleYear', 'YearMade_0',\n       'YearMade_1', 'YearMade_2', 'YearMade_3', 'YearMade_4', 'YearMade_5',\n       ...\n       'Tire_Size_2', 'Tire_Size_3', 'Tire_Size_4', 'Tire_Size_5',\n       'Tire_Size_6', 'Tire_Size_7', 'Pad_Type_0', 'Pad_Type_1', 'Pad_Type_2',\n       'Pad_Type_3'],\n      dtype='object', length=329)\n\n\n\n' '.join(o for o in valid_xs_with_embs.columns if o.startswith('YearMade'))\n\n'YearMade_0 YearMade_1 YearMade_2 YearMade_3 YearMade_4 YearMade_5 YearMade_6 YearMade_7 YearMade_8 YearMade_9 YearMade_10 YearMade_11 YearMade_12 YearMade_13 YearMade_14 YearMade_15 YearMade_16 YearMade_17'\n\n\n\n' '.join(o for o in valid_xs_with_embs.columns if o.startswith('ProductSize'))\n\n'ProductSize_0 ProductSize_1 ProductSize_2 ProductSize_3 ProductSize_4'\n\n\nLooks good! Now I can go on to fit a random forest on this embedding-filled dataset."
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#fitting-a-random-forest",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#fitting-a-random-forest",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Fitting a Random Forest",
    "text": "Fitting a Random Forest\n\nm = rf(xs_with_embs, y);\nm_rmse(m, xs_with_embs, y), m_rmse(m, valid_xs_with_embs, valid_y)\n\n(0.178098, 0.238825)\n\n\nThe previous random forest, with low-importance, redundant and out-of-domain features removed had training and validation errors of 0.189063 and 0.247074, respectively. This model has lower errors for both.\n\nRemoving Features\nNext, I’ll look into removing features from this updated dataset that has embedding matrices in it. Currently my training dataset has 300+ columns in it.\n\nxs_with_embs.shape\n\n(395371, 329)\n\n\n\nLow-Importance Features\nHere are the top-10 features in my model—all except one of the top 10 features are outputs from the categorical embeddings.\n\nfi = rf_feat_importance(m, xs_with_embs)\nfi[:10]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      37\n      fiProductClassDesc_7\n      0.117214\n    \n    \n      263\n      fiModelDesc_183\n      0.099233\n    \n    \n      3\n      saleYear\n      0.063399\n    \n    \n      36\n      fiProductClassDesc_6\n      0.062105\n    \n    \n      209\n      fiModelDesc_129\n      0.050837\n    \n    \n      208\n      fiModelDesc_128\n      0.037791\n    \n    \n      43\n      fiProductClassDesc_13\n      0.033951\n    \n    \n      181\n      fiModelDesc_101\n      0.027102\n    \n    \n      152\n      fiModelDesc_72\n      0.017599\n    \n    \n      100\n      fiModelDesc_20\n      0.017077\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nAnd here’s a plot of the top 30 most important features:\n\ndef plot_fi(fi):\n  return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\nLike last time, I’ll only keep features that have an importance of 0.005 or greater—which results in keeping only about 10% of the columns:\n\nto_keep = fi[fi.imp>0.005].cols\nlen(to_keep)\n\n36\n\n\n\nxs_imp_embs = xs_with_embs[to_keep]\nvalid_xs_imp_embs = valid_xs_with_embs[to_keep]\n\n\nxs_imp_embs.shape, valid_xs_imp_embs.shape\n\n((395371, 36), (17327, 36))\n\n\n\nm = rf(xs_imp_embs, y);\nm_rmse(m, xs_imp_embs, y), m_rmse(m, valid_xs_imp_embs, valid_y)\n\n(0.184552, 0.242728)\n\n\nThe errors are higher than when all columns were included, but lower than when the original categorical columns were included.\n\n\nRedundant Features\nSimilar to before, I’ll see if I can remove any redundant features from the dataset to continue simplifying the model.\n\ncluster_columns(xs_imp_embs)\n\n\n\n\nI was expecting more columns to be redundant, since the columns in a categorical embedding matrix are related to the same column. However, it’s interesting to note that different embedding matrix columns for a categorical variable represent different aspects of that feature.\nAs before, I’ll calculate a baseline OOB score, and then use it to compare with scores after redundant features are removed:\n\nget_oob(xs_imp_embs)\n\n0.8892042378420308\n\n\nHere are the OOB scores if I drop each individual redundant feature from the dataset:\n\n{c: get_oob(xs_imp_embs.drop(c, axis=1)) for c in (\n    'ProductSize_1', 'ProductSize_4',\n    'YearMade_8', 'YearMade_5',\n    'ProductGroup_4', 'ProductGroup_3')}\n\n{'ProductSize_1': 0.8885392743641873,\n 'ProductSize_4': 0.8890680891755572,\n 'YearMade_8': 0.8885895030231491,\n 'YearMade_5': 0.8884153739148248,\n 'ProductGroup_4': 0.8890909768453596,\n 'ProductGroup_3': 0.8890208894539288}\n\n\nI’ll remove the three columns, the removal of which keeps the first three decimal places of the OOB score the same.\n\nto_drop = ['ProductSize_4', 'ProductGroup_4', 'ProductGroup_3']\nget_oob(xs_imp_embs.drop(to_drop, axis=1))\n\n0.8890859448391993\n\n\n\nxs_embs_final = xs_imp_embs.drop(to_drop, axis=1)\nvalid_xs_embs_final = valid_xs_imp_embs.drop(to_drop, axis=1)\n\n\nm = rf(xs_embs_final, y)\nm_rmse(m, xs_embs_final, y), m_rmse(m, valid_xs_embs_final, valid_y)\n\n(0.184845, 0.244098)\n\n\nThe errors have slightly increased but are still lower than the model fitted to the original dataset.\n\n\nOut-of-Domain Features\nThe final set of features I’ll look to remove are those that have significantly different values between the training and validation sets. As I did before, I’ll fit a random forest to predict whether a row is from the training or validation set, and then look at the most important features for this distinction:\n\ndf_dom = pd.concat([xs_embs_final, valid_xs_embs_final])\nis_valid = np.array([0]*len(xs_embs_final) + [1]*len(valid_xs_embs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      2\n      saleYear\n      0.567392\n    \n    \n      12\n      SalesID\n      0.266997\n    \n    \n      22\n      MachineID\n      0.072159\n    \n    \n      26\n      YearMade_12\n      0.006785\n    \n    \n      15\n      YearMade_5\n      0.004990\n    \n    \n      24\n      YearMade_8\n      0.004247\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe three most important features in distinguishing a training set row from a validation set row are saleYear, SalesID, and MachineID. This is similar to what we experienced before—these are all time-influenced features and the training and validation set is split based on the sale date. I’ll remove each one and see if it decreases the error of the model:\n\n# baseline\nm = rf(xs_embs_final, y)\nprint('orig', m_rmse(m, valid_xs_embs_final, valid_y))\n\nfor c in ('saleYear', 'SalesID', 'MachineID'):\n  m = rf(xs_embs_final.drop(c, axis=1), y)\n  print(c, m_rmse(m, valid_xs_embs_final.drop(c,axis=1), valid_y))\n\norig 0.243168\nsaleYear 0.248063\nSalesID 0.243861\nMachineID 0.243165\n\n\nIt looks like I can remove MachineID with only a tiny increase in the error.\n\ntime_vars = ['MachineID']\nxs_embs_final_time = xs_embs_final.drop(time_vars, axis=1)\nvalid_xs_embs_time = valid_xs_embs_final.drop(time_vars, axis=1)\n\nm = rf(xs_embs_final_time, y)\n\n\nm_rmse(m, xs_embs_final_time, y), m_rmse(m, valid_xs_embs_time, valid_y)\n\n(0.189369, 0.243094)\n\n\n\n# save for later\nembs_xs = xs_embs_final_time, valid_xs_embs_time\nsave_pickle('embs_xs.pkl', embs_xs)\n\nAfter removing features, the errors on the training and validation sets are a tiny bit smaller than the errors on the original datasets."
  },
  {
    "objectID": "posts/2023-10-09-nn-rf-embeddings/index.html#final-thoughts",
    "href": "posts/2023-10-09-nn-rf-embeddings/index.html#final-thoughts",
    "title": "Using Neural Net Embeddings to Improve a Random Forest",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere is a summary of error values for models fit on the original and embedding-filled datasets:\n\n\n\nModel\nValidation Error\nReduction in Error\n\n\n\n\nRF (no embedding columns)\n0.247074\n–\n\n\nRF (329 embedding columns)\n0.238825\n3.33%\n\n\nRF (30 embedding columns)\n0.243094\n1.6%\n\n\n\nTo be honest I’m surprised that swapping categorical variables with neural net embeddings only reduced the validation MSE by 3.3%. I’m not sure what explains this small reduction. Maybe there’s something about this dataset that doesn’t benefit from converting categorical variables to embeddings? There’s always a chance that my implementation is incorrect, or not the best way to achieve benefits in the model. As I practice working with more tabular datasets, I’ll continue to implement neural net embeddings and observe if it makes a difference in how the model performs.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-09-03-random-numbers/index.html",
    "href": "posts/2024-09-03-random-numbers/index.html",
    "title": "Comparing ~100k Random Numbers Generated with Different Methods",
    "section": "",
    "text": "In this notebook I’ll run some experiments to compare random numbers generated with the following approaches:\n\nThe Australian National University Quantum Numbers API\nrandom (Python Standard Library)\nnumpy.random.randint\ntorch.randint\nThe rand function from Lesson 10 of the fastai course (Part 2)"
  },
  {
    "objectID": "posts/2024-09-03-random-numbers/index.html#getting-100k-random-numbers-from-the-anu-api",
    "href": "posts/2024-09-03-random-numbers/index.html#getting-100k-random-numbers-from-the-anu-api",
    "title": "Comparing ~100k Random Numbers Generated with Different Methods",
    "section": "Getting ~100k Random Numbers from the ANU API",
    "text": "Getting ~100k Random Numbers from the ANU API\nI refactored code from the ANU documentation into the following function:\n\nimport json, requests, random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom scipy import stats\n\n\ndef get_anu(qrn_key, dtype, length, blocksize=1):\n  QRN_URL = \"https://api.quantumnumbers.anu.edu.au/\"\n  params = {\"length\": length, \"type\": dtype, \"size\": blocksize}\n  headers = {\"x-api-key\": qrn_key}\n\n  response = requests.get(QRN_URL, headers=headers, params=params)\n\n  if response.status_code == 200:\n    js = response.json()\n    if js[\"success\"] == True:\n        print(js[\"data\"])\n    else:\n        print(js[\"message\"])\n  else:\n    print(f\"Got an unexpected status-code: {response.status_code}\")\n    print(response.text)\n\nSo that I could run through a loop for multiple API calls. I ended up using 3 calls for testing so only generated 99328 total random numbers (1024 per 97 API calls).\n\nresults = []\n\nfor i in range(100):\n  res = get_anu(qrn_key=QRN_KEY, dtype=\"uint8\", length=1024, blocksize=1)\n  results.extend(res)\n\n\nlen(results)\n\n99328\n\n\nI have uploaded the list of random numbers to this gist. You can use the following helper function to load the txt file into a python list:\n# Loading the list from a file\ndef load_list(file_name):\n    with open(file_name, 'r') as file:\n        return json.load(file)\n        \n# Load the list\nresults = load_list(\"anu_random_numbers.txt\")"
  },
  {
    "objectID": "posts/2024-09-03-random-numbers/index.html#viewing-the-random-numbers",
    "href": "posts/2024-09-03-random-numbers/index.html#viewing-the-random-numbers",
    "title": "Comparing ~100k Random Numbers Generated with Different Methods",
    "section": "Viewing the Random Numbers",
    "text": "Viewing the Random Numbers\nLet’s take a moment and appreciate the beauty of truly random numbers! It looks like static on an old TV screen:\n\nresults = pd.Series(results)\nplt.scatter(results.index, results.values, s=1, alpha=0.3);"
  },
  {
    "objectID": "posts/2024-09-03-random-numbers/index.html#generating-random-numbers-using-other-approaches",
    "href": "posts/2024-09-03-random-numbers/index.html#generating-random-numbers-using-other-approaches",
    "title": "Comparing ~100k Random Numbers Generated with Different Methods",
    "section": "Generating Random Numbers Using Other Approaches",
    "text": "Generating Random Numbers Using Other Approaches\nI’ll now generate 99,328 random numbers using the following methods:\n\nrandom (Python Standard Library)\nnumpy.random.randint\ntorch.randint\nThe rand function from Lesson 10 of the fastai course (Part 2)\n\nNote that the ANU random numbers are between 0-255.\n\nthe data type must be 'uint8' (returns integers between 0-255)\n\n\nresults.min(), results.max()\n\n(0, 255)\n\n\n\nrandom (Python Standard Library)\nFrom the Python docs:\n\nrandom.randint(a, b)\nReturn a random integer N such that a <= N <= b. Alias for randrange(a, b+1).\n\n\nrandom.randint(0,255)\n\n134\n\n\n\npy_results = pd.Series([random.randint(0,255) for _ in range(99328)])\npy_results.min(), py_results.max()\n\n(0, 255)\n\n\nVisibly, there’s not much difference between this scatter plot and the ANU random numbers.\n\nplt.scatter(py_results.index, py_results.values, s=1, alpha=0.3);\n\n\n\n\n\n\n\n\n\n\nnumpy.random.randint\nThe NumPy functionality is a bit different, it excludes the upper bound that you provide:\n\nReturn random integers from low (inclusive) to high (exclusive).\n\n\nnp.random.randint(0, 256)\n\n236\n\n\n\nnp_results = pd.Series([np.random.randint(0, 256) for _ in range(99328)])\nnp_results.min(), np_results.max()\n\n(0, 255)\n\n\n\nplt.scatter(np_results.index, np_results.values, s=1, alpha=0.3);\n\n\n\n\n\n\n\n\n\n\ntorch.randint\nPyTorch does it similar to NumPy:\n\nReturns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).\n\nBut also asks the shape of the tensor you want as a parameter.\n\npt_results = pd.Series(torch.randint(0, 256, (99328,)))\npt_results.shape, pt_results.min(), pt_results.max()\n\n((99328,), 0, 255)\n\n\n\nplt.scatter(pt_results.index, pt_results.values, s=1, alpha=0.3);\n\n\n\n\n\n\n\n\n\n\nCustom rand Implementation\nIn Lesson 10 of the fastai course (Part 2), Jeremy implements the following from-scratch random number generator:\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand()\n\n0.7645251082582081\n\n\nSince this implementation generates floats between 0 and 1, I’ll have to handle it a bit differently:\n\nrand_results = pd.Series([int(rand()*256) for _ in range(99328)])\nrand_results.min(), rand_results.max()\n\n(0, 255)\n\n\n\nplt.scatter(rand_results.index, rand_results.values, s=1, alpha=0.3);"
  },
  {
    "objectID": "posts/2024-09-03-random-numbers/index.html#comparing-random-numbers-generated-with-different-methods",
    "href": "posts/2024-09-03-random-numbers/index.html#comparing-random-numbers-generated-with-different-methods",
    "title": "Comparing ~100k Random Numbers Generated with Different Methods",
    "section": "Comparing Random Numbers Generated with Different Methods",
    "text": "Comparing Random Numbers Generated with Different Methods\nUpon visual inspection, the distributions of 99328 random numbers generated by the different methods look similar.\nWith the help of Claude, I’ll apply a few statistical tests and compare the results across all five sets of (differently generated) random integers:\n\ndef random_analysis(data_sets, names):\n    results = []\n\n    # Create a 2x3 grid of subplots\n    fig, axs = plt.subplots(2, 3, figsize=(10, 5))\n    axs = axs.flatten()\n\n    for i, (data, name) in enumerate(zip(data_sets, names)):\n        # Basic statistical measures\n        mean = np.mean(data)\n        variance = np.var(data)\n        std_dev = np.std(data)\n\n        # Chi-square test for uniformity\n        observed_freq, _ = np.histogram(data, bins=256, range=(0, 255))\n        expected_freq = len(data) / 256  # Assuming uniform distribution\n        chi2_stat, chi2_p = stats.chisquare(observed_freq, f_exp=[expected_freq]*256)\n\n        # Kolmogorov-Smirnov test\n        ks_stat, ks_p = stats.kstest(data, 'uniform', args=(0, 256))\n\n        results.append({\n            'Name': name,\n            'Mean': mean,\n            'Variance': variance,\n            'Std Dev': std_dev,\n            'Chi-square Statistic': chi2_stat,\n            'Chi-square p-value': chi2_p,\n            'KS Statistic': ks_stat,\n            'KS p-value': ks_p\n        })\n\n        # Plot histogram in the corresponding subplot\n        axs[i].hist(data, bins=256, range=(0, 255), density=True)\n        axs[i].set_title(f'Histogram for {name}')\n        axs[i].set_xlabel('Value')\n        axs[i].set_ylabel('Frequency')\n\n    # Remove any unused subplots\n    for j in range(i+1, len(axs)):\n        fig.delaxes(axs[j])\n\n    plt.tight_layout()\n    plt.show()\n\n    return pd.DataFrame(results)\n\nAgain, by visual inspection, the histograms of the differently generated random integers look similar, if not the same. It’s interesting to note that there are dips in the distribution where certain integers have significantly lower occurences than others.\n\ndata_sets = [results, py_results, np_results, pt_results, rand_results]\nnames = ['Quantum', 'Python Random', 'NumPy Random', 'Torch Random', 'Custom Random']\n\nres = random_analysis(data_sets, names)\n\n\n\n\n\n\n\n\nNext, I’ll interpret the various statistics calculated.\nFor a uniform distribution (between \\(a=0\\) and \\(b=255\\)), the expected value for the mean, variance and standard deviation are as follows:\n\\[\\text{Mean: }  \\mu = \\frac{a + b}{2} = \\frac{0 + 255}{2} = 127.5\\]\n\n\\[\\text{Var: }  \\sigma^2 = \\frac{(b - a + 1)^2 - 1}{12} =  \\frac{(255 - 0 + 1)^2 - 1}{12} = \\frac{256^2 - 1}{12} \\approx 5461.25\\]\n\n\\[\\text{Std: } \\sigma = \\sqrt{5461.25} \\approx 73.91\\]\nThe method with the mean closest to the expected value is numpy.random.randint (127.502). The closest variance to the expected value is the ANU Quantum method (5459.429).The closes standard deviation is also the ANU Quantum method (73.888).\n\nres\n\n\n\n  \n    \n\n\n  \n    \n      \n      Name\n      Mean\n      Variance\n      Std Dev\n      Chi-square Statistic\n      Chi-square p-value\n      KS Statistic\n      KS p-value\n    \n  \n  \n    \n      0\n      Quantum\n      127.584518\n      5459.429128\n      73.887950\n      216.123711\n      0.963092\n      0.004792\n      0.020812\n    \n    \n      1\n      Python Random\n      127.677402\n      5451.449199\n      73.833930\n      246.185567\n      0.642552\n      0.004651\n      0.027113\n    \n    \n      2\n      NumPy Random\n      127.502638\n      5482.749449\n      74.045590\n      277.453608\n      0.159672\n      0.006041\n      0.001416\n    \n    \n      3\n      Torch Random\n      127.531039\n      5477.553241\n      74.010494\n      237.572165\n      0.776473\n      0.005477\n      0.005147\n    \n    \n      4\n      Custom Random\n      127.894380\n      5469.084638\n      73.953260\n      289.896907\n      0.065635\n      0.004671\n      0.026121\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nNext, I’ll look at the chi-square statistic, where lower values are better. The Quantum method has the lowest chi-square statistic with the highest p-value, meaning it’s the closest to a uniform distribution.\nNext, looking at the KS statistic, from the SciPy docs, it measures:\n\nthe distance between the empirical distribution function and the hypothesized cumulative distribution function\n\nAgain, lower values are better. However, the p-values are all lower than 0.05, indicating that the null hypothesis (that the data comes from a uniform distribution) should be rejected. Claude provided the following insight:\n\nThe KS test is very sensitive, especially with large sample sizes. With 99,328 numbers, even small deviations from perfect uniformity can lead to statistically significant results.\n\nMy interpretation of this is that it’s not a perfectly uniform distribution."
  },
  {
    "objectID": "posts/2024-09-03-random-numbers/index.html#reducing-the-number-of-samples",
    "href": "posts/2024-09-03-random-numbers/index.html#reducing-the-number-of-samples",
    "title": "Comparing ~100k Random Numbers Generated with Different Methods",
    "section": "Reducing the Number of Samples",
    "text": "Reducing the Number of Samples\nI’ll run these tests with a much lower number of samples and see if that changes any of the statistics significantly.\n\ndata_sets = [results[:10000], py_results[:10000], np_results[:10000], pt_results[:10000], rand_results[:10000]]\nnames = ['Quantum', 'Python Random', 'NumPy Random', 'Torch Random', 'Custom Random']\n\nres = random_analysis(data_sets, names)\n\n\n\n\n\n\n\n\nUsing only 10,000 samples each, the statistics have changed:\n\nThe Quantum method no longer has the variance closest to the expected value of 5461.25. That claim belongs to Python’s random module (which has the closest value, 73.9 to the expected standard deviation of 73.91).\nNumPy again has the closet mean to the expected value of 127.5.\nThe Chi-square statistics have changed, with Python leading the way (although all of the methods still have a high p-value).\nThe KS statistic has also significantly changed: all methods now have a p-value greater than 0.05, indicating that the null hypothesis (that they belong to a uniform distribution) cannot be rejected.\n\n\nres\n\n\n\n  \n    \n\n\n  \n    \n      \n      Name\n      Mean\n      Variance\n      Std Dev\n      Chi-square Statistic\n      Chi-square p-value\n      KS Statistic\n      KS p-value\n    \n  \n  \n    \n      0\n      Quantum\n      127.6421\n      5556.331208\n      74.540802\n      236.6720\n      0.788774\n      0.008306\n      0.492614\n    \n    \n      1\n      Python Random\n      128.3820\n      5461.679276\n      73.903175\n      234.7264\n      0.814062\n      0.007994\n      0.542459\n    \n    \n      2\n      NumPy Random\n      127.5575\n      5459.439094\n      73.888017\n      236.2112\n      0.794925\n      0.009550\n      0.319373\n    \n    \n      3\n      Torch Random\n      127.9056\n      5668.028889\n      75.286313\n      265.1392\n      0.318255\n      0.015931\n      0.012354\n    \n    \n      4\n      Custom Random\n      127.8054\n      5392.456331\n      73.433346\n      276.6592\n      0.167893\n      0.008838\n      0.413184"
  },
  {
    "objectID": "posts/2024-09-03-random-numbers/index.html#final-thoughts",
    "href": "posts/2024-09-03-random-numbers/index.html#final-thoughts",
    "title": "Comparing ~100k Random Numbers Generated with Different Methods",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI was surprised that the random numbers generated by alternative approaches to the ANU Quantum generated were comparable to it! I was also surprised that even the Quantum-generated random numbers were not perfect—they still deviated from the expected values of a uniform distribution. I was also not expecting the statistics to change so dramatically depending on the number of samples analyzed.\nThis is my first true foray into the world of random number generation (outside of setting seeds during training) and I have probably only scratched the surface. I look forward to being more mindful about random numbers in the future.\nThanks for reading this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-22-tinystories-3m-finetune/index.html",
    "href": "posts/2024-08-22-tinystories-3m-finetune/index.html",
    "title": "Fine-tuning TinyStories-3M on the financial_phrasebank Dataset",
    "section": "",
    "text": "In a previous blog post I finetuned the TinyStories-33M and the TinyStories-8M model on the financial_phrasebank dataset and achieved the following results:\n\n\n\nArch\nFine-tuning Learning Rate\nBest Val Acc\nBest Test Acc\n\n\n\n\nTinyStories-33M\n5e-04\n86%\n79%\n\n\nTinyStories-8M\n8e-05\n85%\n86%\n\n\nTinyStories-8M\n5e-04\n79%\n86%\n\n\n\nIn this notebook, I’ll finetune the smaller TinyStories-3M model and see how it performs. I also suspect these smaller models might perform better on a (synthetically generated) simpler version of this dataset, which I plan to explore in a future notebook.\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ trusted=‘true’}\n\nShow imports and setup\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, TrainerCallback\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nimport gc\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n#model_nm = \"roneneldan/TinyStories-33M\"\n#model_nm = \"roneneldan/TinyStories-1M\"\nmodel_nm = \"roneneldan/TinyStories-3M\"\n#model_nm = \"roneneldan/TinyStories-8M\"\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\ndef tok_func(x): return tokz(x[\"input\"], padding=True, truncation=True)\n\n:::"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-3m-finetune/index.html#preparing-datasets",
    "href": "posts/2024-08-22-tinystories-3m-finetune/index.html#preparing-datasets",
    "title": "Fine-tuning TinyStories-3M on the financial_phrasebank Dataset",
    "section": "Preparing Datasets",
    "text": "Preparing Datasets\nMuch of the code in this section is boilerplate, tokenizing the dataset and splitting it into training, validation and test sets.\n\n\nShow load_dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\",\n    split=\"train\"  # note that the dataset does not have a default test split\n)\n\ndataset = dataset.rename_columns({'label':'labels', 'sentence': 'input'})\n\n\n\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ntokz.padding_side = \"left\" # https://github.com/huggingface/transformers/issues/16595 and https://www.kaggle.com/code/baekseungyun/gpt-2-with-huggingface-pytorch\ntok_ds = dataset.map(tok_func, batched=True)\n\n\ntok_ds[0]['input']\n\n'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'\n\n\n\ntok_ds[0]['input_ids'][100:110] # first 100 elements are 50257 ('[PAD]')\n\n[50257, 50257, 50257, 50257, 50257, 50257, 4821, 284, 17113, 837]\n\n\n\ntokz.decode(50257), tokz.decode(4821), tokz.decode(284), tokz.decode(17113)\n\n('[PAD]', 'According', ' to', ' Gran')\n\n\n\ntok_ds[0]['labels']\n\n1\n\n\n\nsplit_dataset = tok_ds.train_test_split(test_size=225/2264, seed=42)\n\ntraining_split = split_dataset['train'].train_test_split(test_size=0.2, seed=42)\n\ntrain_ds = training_split['train']\neval_ds = training_split['test']\ntest_ds = split_dataset['test']\n\ntrain_ds, eval_ds, test_ds\n\n(Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 1631\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 408\n }),\n Dataset({\n     features: ['input', 'labels', 'input_ids', 'attention_mask'],\n     num_rows: 225\n }))\n\n\n\ntrain_ds[0]['input']\n\n'The result will also be burdened by increased fixed costs associated with operations in China , and restructuring costs in Japan .'\n\n\n\ntrain_ds[0]['labels']\n\n0\n\n\nThe dataset distributions show a predominance of neutral (1) sentences:\n\ntrain_ds.to_pandas()['labels'].value_counts() / len(train_ds)\n\nlabels\n1    0.622318\n2    0.251993\n0    0.125690\nName: count, dtype: float64\n\n\n\neval_ds.to_pandas()['labels'].value_counts() / len(eval_ds)\n\nlabels\n1    0.615196\n2    0.257353\n0    0.127451\nName: count, dtype: float64\n\n\n\ntest_ds.to_pandas()['labels'].value_counts() / len(test_ds)\n\nlabels\n1    0.555556\n2    0.240000\n0    0.204444\nName: count, dtype: float64"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-3m-finetune/index.html#prepare-for-training",
    "href": "posts/2024-08-22-tinystories-3m-finetune/index.html#prepare-for-training",
    "title": "Fine-tuning TinyStories-3M on the financial_phrasebank Dataset",
    "section": "Prepare for Training",
    "text": "Prepare for Training\nMuch of the code in this section is either helper functions (like get_acc, MetricCallback, or results_to_dataframe) or boilerplate code to prepare a HuggingFace trainer:\n\n\nShow get_acc function\ndef get_acc(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": (predictions == labels).astype(np.float32).mean().item()}\n\n\n\n\nShow MetricCallback function\n# thanks Claude\n\nclass MetricCallback(TrainerCallback):\n    def __init__(self):\n        self.metrics = []\n        self.current_epoch_metrics = {}\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            self.current_epoch_metrics.update(logs)\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        if hasattr(state, 'log_history') and state.log_history:\n            # Get the last logged learning rate\n            last_lr = state.log_history[-1].get('learning_rate', None)\n        else:\n            last_lr = None\n\n        self.metrics.append({\n            \"epoch\": state.epoch,\n            \"learning_rate\": last_lr,\n            **self.current_epoch_metrics\n        })\n        self.current_epoch_metrics = {}  # Reset for next epoch\n\n    def on_train_end(self, args, state, control, **kwargs):\n        # Capture final metrics after the last epoch\n        if self.current_epoch_metrics:\n            self.metrics.append({\n                \"epoch\": state.num_train_epochs,\n                \"learning_rate\": self.metrics[-1].get('learning_rate') if self.metrics else None,\n                **self.current_epoch_metrics\n            })\n\n\n\n\nShow results_to_dataframe function\ndef results_to_dataframe(results, model_name):\n    rows = []\n    for result in results:\n        initial_lr = result['learning_rate']\n        for metric in result['metrics']:\n            row = {\n                'model_name': model_name,\n                'initial_learning_rate': initial_lr,\n                'current_learning_rate': metric.get('learning_rate'),\n            }\n            row.update(metric)\n            rows.append(row)\n    \n    df = pd.DataFrame(rows)\n    \n    # Ensure specific columns are at the beginning\n    first_columns = ['model_name', 'initial_learning_rate', 'current_learning_rate', 'epoch']\n    other_columns = [col for col in df.columns if col not in first_columns]\n    df = df[first_columns + other_columns]\n    \n    return df\n\n\n\n\nShow make_cm function\ndef make_cm(df):\n    \"\"\"Create confusion matrix for true vs predicted sentiment classes\"\"\"\n    \n    cm = confusion_matrix(y_true=df['label_text'], y_pred=df['pred_text'], labels=['negative', 'neutral', 'positive'])\n    disp = ConfusionMatrixDisplay(cm, display_labels=['negative', 'neutral', 'positive'])\n    \n    fig, ax = plt.subplots(figsize=(4,4))\n    disp.plot(ax=ax,text_kw={'fontsize': 12}, cmap='Blues', colorbar=False);\n    \n    # change label font size without changing label text\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    \n    # make tick labels larger\n    ax.tick_params(axis='y', labelsize=14)\n    ax.tick_params(axis='x', labelsize=14)\n\n\n\n\nShow get_prediction function\ndef get_prediction(model, text, tokz):\n    # Determine the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Move the model to the appropriate device\n    model = model.to(device)\n\n    # Tokenize the input text\n    inputs = tokz(text, return_tensors=\"pt\", truncation=True, padding=True)\n\n    # Move input tensors to the same device as the model\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Get the model's prediction\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Ensure logits are on CPU for numpy operations\n    logits = outputs.logits.detach().cpu()\n\n    # Get probabilities\n    probs = torch.softmax(logits, dim=-1)\n\n    # Get the predicted class\n    p_class = torch.argmax(probs, dim=-1).item()\n\n    # Get the probability for the predicted class\n    p = probs[0][p_class].item()\n\n    labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n    \n    print(f\"Probability: {p:.2f}\")\n    print(f\"Predicted label: {labels[p_class]}\")\n    return p_class, p\n\n\n\n\nShow get_trainer function\ndef get_trainer(lr, bs=16):\n\n    args = TrainingArguments(\n        'outputs',\n        learning_rate=lr,\n        warmup_ratio=0.1,\n        lr_scheduler_type='cosine',\n        fp16=True,\n        eval_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        per_device_train_batch_size=bs,\n        per_device_eval_batch_size=bs*2,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        report_to='none')\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=3) # 3 labels for 3 classes\n    model.resize_token_embeddings(len(tokz))\n    model.config.pad_token_id = model.config.eos_token_id\n    \n    trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=eval_ds, \n                  tokenizer=tokz, compute_metrics=get_acc, callbacks=[metric_callback])\n    \n    return trainer, args\n\n\n\n\nShow get_test_df function\ndef get_test_df(trainer):\n    test_df = test_ds.to_pandas()[['input', 'labels']]\n    \n    preds = trainer.predict(test_ds).predictions.astype(float)\n    probs = F.softmax(torch.tensor(preds), dim=1)\n    predicted_classes = torch.argmax(probs, dim=1).numpy()\n\n    test_df['predicted'] = predicted_classes\n    \n    test_df['match'] = test_df['labels'] == test_df['predicted']\n    acc = test_df['match'].mean()\n    \n    label_map = {i: label_text for i, label_text in enumerate(test_ds.features[\"labels\"].names)}\n    test_df['label_text'] = test_df['labels'].apply(lambda x: label_map[x])\n    test_df['pred_text'] = test_df['predicted'].apply(lambda x: label_map[x])\n    \n    return test_df, acc"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-3m-finetune/index.html#training-learning-rate-sweep",
    "href": "posts/2024-08-22-tinystories-3m-finetune/index.html#training-learning-rate-sweep",
    "title": "Fine-tuning TinyStories-3M on the financial_phrasebank Dataset",
    "section": "Training: Learning Rate Sweep",
    "text": "Training: Learning Rate Sweep\nWhile there are other hyperparameters to tune (epochs, warmup_ratio, weight_decay) I’ll focus this notebook on fine-tuning with different learning rates. I’ll start with the same learning rates that I used for the 33M and 8M models:\n\n\nShow training loop\nmetrics = []\ntrainers = []\nlearning_rates = [1e-6, 1e-5, 3e-5, 5e-5, 8e-5, 1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 1e-2, 1e-1]\n\nfor lr in learning_rates:\n    print(f\"Learning Rate: {lr}\")\n    \n    metric_callback = MetricCallback()\n    \n    trainer, args = get_trainer(lr, bs=64)\n\n    trainer.train()\n\n    metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nmetrics_df = results_to_dataframe(metrics, model_name=\"TinyStories-3M\")\nmetrics_df = metrics_df.query('current_learning_rate.notna()')"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-3m-finetune/index.html#results",
    "href": "posts/2024-08-22-tinystories-3m-finetune/index.html#results",
    "title": "Fine-tuning TinyStories-3M on the financial_phrasebank Dataset",
    "section": "Results",
    "text": "Results\n\nHighest Validation Set Accuracy\nThe highest validation set accuracy (73%) was obtained with a learning rate of 8e-5.\n\nmetrics_df.query('eval_accuracy == eval_accuracy.max()')\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      19\n      TinyStories-3M\n      0.00008\n      0.0\n      3.0\n      0.0\n      0.5798\n      620476.375\n      0.653758\n      0.732843\n      0.3241\n      1259.023\n      6.172\n      8.6789\n      563.784\n      4.494\n      6.090918e+12\n      0.757333\n    \n  \n\n\n\n\n\nlearning_rates[4] == 0.00008\n\nTrue\n\n\nThis model achieved a 67% test set accuracy.\n\ntest_df, acc = get_test_df(trainers[4])\nacc\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n\n0.6666666666666666\n\n\nThis 3M parameter finetuned model predicts neutral sentences the best (116/125) followed by positive sentences (25/54) and lastly, negative sentences (22/46). I’ll reiterate that the dataset contains a majority of neutral sentences, followed by positive sentences and the least represented sentiment is negative.\n\nmake_cm(test_df)\n\n\n\n\n\n\n\n\nAs the learning rate increases (starting at 1e-6) the validation set accuracy increases until it reaches a peak at a learning rate of 8e-5. It reaches a bit of a second peak at 1e-3.\n\n\nShow plotting code\nfinal_epoch_metrics = metrics_df.query(\"epoch == 3\")\nplt.scatter(final_epoch_metrics['initial_learning_rate'], final_epoch_metrics['eval_accuracy']);\nplt.xscale('log')\nplt.xlabel('Learning Rate (log scale)')\nplt.ylabel('Validation Set Accuracy')\nplt.title('Learning Rate vs. Final Epoch Validation Accuracy');\n\n\n\n\n\n\n\n\n\nI’ll test the model (run a “sanity check”) on three made-up sentences. I don’t want to weigh these results too much as they are cherry-picked sentences, but this model gets 2/3 right.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(trainers[4].model, text, tokz)\n\nProbability: 0.55\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(trainers[4].model, text, tokz)\n\nProbability: 0.53\nPredicted label: positive\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(trainers[4].model, text, tokz)\n\nProbability: 0.74\nPredicted label: neutral\n\n\n\n\nHighest Test Set Accuracy\n\ntest_dfs = []\naccs = []\nfor t in trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\n8e-5 is also the learning rate with the highest test set accuracy (67%).\n\naccs\n\n[0.52,\n 0.6177777777777778,\n 0.6355555555555555,\n 0.6488888888888888,\n 0.6666666666666666,\n 0.6266666666666667,\n 0.6311111111111111,\n 0.6133333333333333,\n 0.6133333333333333,\n 0.6577777777777778,\n 0.5555555555555556,\n 0.5555555555555556]"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-3m-finetune/index.html#training-with-the-best-learning-rates-10-times",
    "href": "posts/2024-08-22-tinystories-3m-finetune/index.html#training-with-the-best-learning-rates-10-times",
    "title": "Fine-tuning TinyStories-3M on the financial_phrasebank Dataset",
    "section": "Training with the Best Learning Rates 10 Times",
    "text": "Training with the Best Learning Rates 10 Times\nI’ll train 10 models for the best-performing learning rate (8e-5) to see if the results are consistent.\n\nLR = 8e-5 (Highest Validation and Test Set Accuracy)\n\nlearning_rates[4]\n\n8e-05\n\n\nTo prevent (all but the first) models from getting the same loss and accuracy per epoch, I’ll reset the random seed each iteration.\n\n\nShow set_seed function\nimport random\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\n\n\nShow training loop\nbest_metrics = []\nbest_trainers = []\nlr = learning_rates[4]\n\nfor i in range(10):\n    set_seed(42 + i)  # Use a different seed for each run\n    metric_callback = MetricCallback()\n    trainer, args = get_trainer(lr=lr, bs=64)\n    trainer.train()\n\n    best_metrics.append({\n        \"learning_rate\": lr,\n        \"metrics\": metric_callback.metrics\n        })\n    \n    best_trainers.append(trainer) \n    \n    # clean up\n    report_gpu()\n    !rm -r /kaggle/working/outputs\n\n\n\nbest_metrics_df = results_to_dataframe(best_metrics, model_name=\"TinyStories-3M\")\nbest_metrics_df = best_metrics_df.query('current_learning_rate.notna()')\nbest_metrics_df.head(3)\n\n\n\n\n\n  \n    \n      \n      model_name\n      initial_learning_rate\n      current_learning_rate\n      epoch\n      learning_rate\n      loss\n      grad_norm\n      eval_loss\n      eval_accuracy\n      eval_runtime\n      eval_samples_per_second\n      eval_steps_per_second\n      train_runtime\n      train_samples_per_second\n      train_steps_per_second\n      total_flos\n      train_loss\n    \n  \n  \n    \n      1\n      TinyStories-3M\n      0.00008\n      0.000068\n      1.0\n      0.000068\n      0.9378\n      679908.0000\n      0.809436\n      0.656863\n      0.3223\n      1265.836\n      6.205\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      TinyStories-3M\n      0.00008\n      0.000024\n      2.0\n      0.000024\n      0.7057\n      660703.8750\n      0.647278\n      0.715686\n      0.3267\n      1248.882\n      6.122\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      TinyStories-3M\n      0.00008\n      0.000000\n      3.0\n      0.000000\n      0.5673\n      626431.0625\n      0.636278\n      0.713235\n      0.3271\n      1247.404\n      6.115\n      8.6262\n      567.227\n      4.521\n      6.090918e+12\n      0.736961\n    \n  \n\n\n\n\nThe minimum accuracy achieved by this learning rate was about 71% and the maximum accuracy was 78%, which is a considerably wide range.\n\nfinal_accs = best_metrics_df.query(\"epoch == 3\")['eval_accuracy']\nfinal_accs.describe()\n\ncount    10.000000\nmean      0.743627\nstd       0.018421\nmin       0.713235\n25%       0.737132\n50%       0.745098\n75%       0.747549\nmax       0.784314\nName: eval_accuracy, dtype: float64\n\n\n\nfinal_accs.value_counts()\n\neval_accuracy\n0.747549    2\n0.745098    2\n0.742647    1\n0.713235    1\n0.725490    1\n0.784314    1\n0.735294    1\n0.750000    1\nName: count, dtype: int64\n\n\n\ntest_dfs = []\naccs = []\nfor t in best_trainers:\n    test_df, acc = get_test_df(t)\n    test_dfs.append(test_df)\n    accs.append(acc)\n\nThe minimum test set accuracy is 64% and the maximum is about 74% for this learning rate (8e-5).\n\naccs = pd.Series(accs)\naccs.value_counts()\n\n0.684444    2\n0.720000    2\n0.693333    1\n0.640000    1\n0.715556    1\n0.671111    1\n0.737778    1\n0.702222    1\nName: count, dtype: int64\n\n\n\naccs.describe()\n\ncount    10.000000\nmean      0.696889\nstd       0.028558\nmin       0.640000\n25%       0.684444\n50%       0.697778\n75%       0.718889\nmax       0.737778\ndtype: float64\n\n\n\naccs\n\n0    0.640000\n1    0.693333\n2    0.715556\n3    0.684444\n4    0.671111\n5    0.737778\n6    0.720000\n7    0.720000\n8    0.702222\n9    0.684444\ndtype: float64\n\n\nThe model with the best test set accuracy (74%) also gets 2/3 of my sanity check sentiments correct.\n\ntext = \"The net sales went up from USD $3.4M to USD $5.6M since the same quarter last year\"\n\n_ = get_prediction(best_trainers[5].model, text, tokz)\n\nProbability: 0.74\nPredicted label: positive\n\n\n\ntext = \"The net sales went down from USD $8.9M to USD $1.2M since the same quarter last year\"\n\n_ = get_prediction(best_trainers[5].model, text, tokz)\n\nProbability: 0.73\nPredicted label: positive\n\n\n\ntext = \"The net sales stayed the as the same quarter last year\"\n\n_ = get_prediction(best_trainers[5].model, text, tokz)\n\nProbability: 0.47\nPredicted label: neutral"
  },
  {
    "objectID": "posts/2024-08-22-tinystories-3m-finetune/index.html#final-thoughts",
    "href": "posts/2024-08-22-tinystories-3m-finetune/index.html#final-thoughts",
    "title": "Fine-tuning TinyStories-3M on the financial_phrasebank Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’ll summarize my results so far for the 33M, 8M and now 3M TinyStories fine-tuned models:\n\n\n\nArch\nFine-tuning Learning Rate\nBest Val Acc\nBest Test Acc\n\n\n\n\nTinyStories-33M\n5e-04\n86%\n79%\n\n\nTinyStories-8M\n8e-05\n85%\n86%\n\n\nTinyStories-8M\n5e-04\n79%\n86%\n\n\nTinyStories-3M\n8e-05\n78%\n74%\n\n\n\nI’ll do the final TinyStories architecture, 1M parameters, in the next notebook in this series.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-11-13-textual-inversion/index.html",
    "href": "posts/2024-11-13-textual-inversion/index.html",
    "title": "Training Textual Inversion Embeddings on Some Samurai Jack Drawings",
    "section": "",
    "text": "In Lesson 9 of the fastai course (Part 2) we are introduced to the concept of textual inversion, where you train an embedding on a new set of images the model hasn’t seen before, and then use that embedding during inference to have the model adapt its style (or object) in the generated image. To get some experience with training and inference, I decided to train a textual inversion embeddings on six pencil/pen drawings I made of one of my favorite childhood cartoons: Samurai Jack.\n\n\n\nSamurai Jack pencil/pen drawings\n\n\nI have uploaded the trained embeddings to Huggingface: sd-concepts-library/samurai-jack. I have created this minimal Colab demo for inference."
  },
  {
    "objectID": "posts/2024-11-13-textual-inversion/index.html#version-1-initial-training",
    "href": "posts/2024-11-13-textual-inversion/index.html#version-1-initial-training",
    "title": "Training Textual Inversion Embeddings on Some Samurai Jack Drawings",
    "section": "Version 1: Initial Training",
    "text": "Version 1: Initial Training\nI used the Huggingface-provided notebook to train my textual inversion embeddings.\nI used the default hyperparameters for the first version of the embeddings I trained, which took about 45 minutes to train with a Free-A4000 on Paperspace:\nhyperparameters = {\n    \"learning_rate\": 5e-04,\n    \"scale_lr\": True,\n    \"max_train_steps\": 2000,\n    \"save_steps\": 250,\n    \"train_batch_size\": 2,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpointing\": True,\n    \"mixed_precision\": \"fp16\",\n    \"seed\": 42,\n    \"output_dir\": \"sd-concept-output\"\n}\nHere are some images I generated using the trained embeddings with the prompts displayed in the caption:\n\n30 Inference Steps\nThe prompt corresponding to the image is listed below it.\n\n\n\nprompt: “a man in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a woman in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a person in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a cat in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a mouse in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “<samurai-jack>”\n\n\n\n\n50 Inference Steps\nThe prompt corresponding to the image is listed below it.\n\n\n\nprompt: “a man in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a woman in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a person in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a cat in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a mouse in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “<samurai-jack>”\n\n\n\n\nReflecting on Version 1\nThe trained embeddings (with the associated token \\<samurai-jack\\>) have clearly learned features related to my original drawings. The generated images have similar clothing and weapons. However, at both 30 and 50 inference steps, the style of the generated images doesn’t really resemble the style I drew the source drawings with (pencil/pen sketch). Additionally, the generated images have color, whereas my drawings were grayscale. I do like the generated images for the cat and mouse prompts. Finally, there’s something stereotypical about the generated images which bothers me—it’s almost like the model has detected that the trained embeddings represent japanese art and it has drawn upon whatever training data aligns with that."
  },
  {
    "objectID": "posts/2024-11-13-textual-inversion/index.html#version-2-longer-training",
    "href": "posts/2024-11-13-textual-inversion/index.html#version-2-longer-training",
    "title": "Training Textual Inversion Embeddings on Some Samurai Jack Drawings",
    "section": "Version 2: Longer Training",
    "text": "Version 2: Longer Training\nI provided my code and 5 generated images to Claude, asking it for feedback on what hyperparameters I could try to improve my embeddings. It suggested to increase the number of training steps, batch size and gradient accumulation steps, and lower the learning rate to yield a training that learned more details from my input images. While I couldn’t increase the batch size without getting an OOM error, I applied the rest of its suggestions in my training script:\nhyperparameters = {\n    \"learning_rate\": 1e-04,\n    \"scale_lr\": True,\n    \"max_train_steps\": 4000,\n    \"save_steps\": 2000,\n    \"train_batch_size\": 2,\n    \"gradient_accumulation_steps\": 4,\n    \"gradient_checkpointing\": True,\n    \"mixed_precision\": \"fp16\",\n    \"seed\": 42,\n    \"output_dir\": \"sd-concept-output-2\"\n}\nThe resulting training took about 6 hours to run (with seconds to spare before Paperspace’s auto-shutdown!).\nHere are some images I generated using the trained embeddings with the prompts displayed in the caption. Note that I only used 50 inference steps as I like the resulting generations more than the 30-step ones.\n\n50 Inference Steps\nThe prompt corresponding to the image is listed below it.\n\n\n\nprompt: “a man in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a woman in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a person in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a cat in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a mouse in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “<samurai-jack>”\n\n\n\n\nReflecting on Version 2\nWhile the image generations using this embeddings are more abstract, I find them better quality and more similar to the original style of my drawings. The following prompts generated more pencil/pen-sketch styled generations:\n\n“a man in the style of <samurai-jack>”\n“a woman in the style of <samurai-jack>”\n“a person in the style of <samurai-jack>”\n“in the style of <samurai-jack>”\n\nWhile the generated image for the prompt \"\\<samurai-jack\\>\" looks nothing like my original drawings, I do find them very beautiful.\nThere were still some “stereotypical” features in some of the generated outputs. For example, the following images contained a red spot (I’m not an art historian but I recall seeing similar red spots/marks/seals in japanese paintings)\n\n\n\nprompt: “a cat in the style of <samurai-jack>”\n\n\nThese red spots were more abstract in some of the generations:\n\n\n\nprompt: “a mouse in the style of <samurai-jack>”\n\n\nAdditionally, some of the generations contained unsolicited language-like characters:\n\n\n\nprompt: “a cartoon man in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “in the style of <samurai-jack>”"
  },
  {
    "objectID": "posts/2024-11-13-textual-inversion/index.html#final-thoughts",
    "href": "posts/2024-11-13-textual-inversion/index.html#final-thoughts",
    "title": "Training Textual Inversion Embeddings on Some Samurai Jack Drawings",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThere is much I haven’t explored in this experiment, for example, using my input drawings to train an “object” to see how that fares during inference, and of course, trying different hyperparameters. That being said, I’m happy that what I attempted at least worked! I found many of the generated images pleasant to look at, as the pencil+pen-sketch style was captured quite well by my version 2 embedddings.\nI’ll end this post with some of the other prompt/generation pairs that I found interesting/beautiful. The prompt corresponding to the image is listed below it:\n\n\n\nprompt: “an african gray swordfighting in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a mouse in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a mouse in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a mouse in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a man fighting in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a drawing in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a dog in the style of <samurai-jack>”\n\n\n\n\n\nprompt: “a dog in the style of <samurai-jack>”"
  },
  {
    "objectID": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html",
    "href": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html",
    "title": "Using TinyInstruct-33M for financial_phrasebank Sentiment Classification",
    "section": "",
    "text": "Show pip installs\n!pip install transformers~=4.37.2 -qq\n!pip install huggingface_hub~=0.20.3 -qq\n!pip install datasets~=2.16.1 -qq\n!pip install accelerate -qq\n\n\n\n\nShow imports\nimport torch\nfrom datasets import load_dataset\nfrom transformers import pipeline, logging\n\nlogging.set_verbosity_error()\n\ntorch.cuda.set_device(0)\n\nmodel_name = \"roneneldan/TinyStories-Instruct-33M\"\n\n# create pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model_name,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# load dataset\ndataset = load_dataset(\n    \"financial_phrasebank\", \"sentences_allagree\", \n    split=\"train\"  # note that the dataset does not have a default test split\n)"
  },
  {
    "objectID": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#background",
    "href": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#background",
    "title": "Using TinyInstruct-33M for financial_phrasebank Sentiment Classification",
    "section": "Background",
    "text": "Background\nIn this notebook I’ll see how accurately the TinyInstruct-33M model can classify sentiment in the financial_phrasebank dataset out-of-the-box, without any fine-tuning. I expect that the model will not perform well for two reasons:\n\nit’s trained specifically to generate stories given a set of prompts.\nit’s vocabulary is at a much lower grade level than the financial_phrasebank dataset."
  },
  {
    "objectID": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#initial-warmup-prompts",
    "href": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#initial-warmup-prompts",
    "title": "Using TinyInstruct-33M for financial_phrasebank Sentiment Classification",
    "section": "Initial Warmup Prompts",
    "text": "Initial Warmup Prompts\nI’ll start by prompting TinyInstruct-33M with a prompt format it’s trained on:\n\nprompt = \"\"\"\nSummary: Lily and Timmy build a sandcastle together and learn to compromise, but it gets knocked over by a gust of wind.\nThey find beauty in the broken sandcastle and play happily with a butterfly.\nFeatures: Dialogue, Foreshadowing, Twist\nSentence: One day, she went to the park and saw a beautiful butterfly.\nWords: disagree, network, beautiful\nStory: \"\"\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=200,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n\n\nOnce upon a time, there were two best friends, Lily and Timmy. Every day they would play together and have lots of fun. \n\nOne day, they decided to build a sandcastle. Lily wanted to make it really tall, but Timmy wanted to make it straight. They disagreed and argued for a long time. \n\nFinally, Lily said, \"Let's make it really tall, Timmy!\" \n\nTimmy agreed and they started to build. They worked together and soon they had built the most beautiful sandcastle ever. \n\nBut then something unexpected happened. A big gust of wind blew through the park and knocked over their sandcastle! \n\nLily and Timmy were sad, but they decided to find something else to do. They found a beautiful butterfly and decided to make it even better. \n\nThey were so happy and they spent the rest of the day playing with the butterfly. \n\nThe end\n\n\nGreat! The model responds appropriately to the type of prompt it’s trained on. Next, I’ll try a simple prompt that contains language the model should be able to understand (given that it understands words like 'beautiful', 'Foreshadowing', and 'network').\n\nprompt = \"What color is an apple?\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=200,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n”\n\nThe little girl smiled and said, “It’s a red apple.”\n\nThe man smiled and said, “That’s right! Apples are healthy and delicious.”\n\nThe little girl smiled and said, “I like apples!”\n\nThe man and the little girl both laughed and enjoyed the apple together. They had a wonderful time in the park.\n\n\n\nI suppose it indirectly answers the question by including \"red apple\" in the context of the story. Does it respond to an instruction?\n\nprompt = \"What color is a banana? Respond with one word\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=2,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n, like\n\n\n\nprompt = \"What color is an orange? Respond with one word\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=2,\n    do_sample=True, \n    temperature=0.9,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n?\"\n\n\n\n\nprompt = \"What color is a crow? Respond with one word\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=2,\n    do_sample=True, \n    temperature=0.6,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n: C\n\n\nNope! Trying different simple prompts (with different temperature levels) yields unsatisfactory results. The model is not following the given instruction."
  },
  {
    "objectID": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#prompting-tinyinstruct-33m-with-financial_phrasebank-data",
    "href": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#prompting-tinyinstruct-33m-with-financial_phrasebank-data",
    "title": "Using TinyInstruct-33M for financial_phrasebank Sentiment Classification",
    "section": "Prompting TinyInstruct-33M with financial_phrasebank Data",
    "text": "Prompting TinyInstruct-33M with financial_phrasebank Data\nGiven that TinyInstruct-33M can’t follow simple instructions that differ from its training data, I am expecting it won’t follow sentiment classification for the financial_phrasebank datset.\nI’ll start by giving it my best-performing phi-2 prompt:\n\npromptM = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nExamples:\n\nInstruct: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: positive\n\nInstruct: Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: negative\n\nInstruct: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput: neutral\n\nInstruct: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\"\"\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=5,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n. It is black and\n\n\nNope! That doesn’t seem to work. I’ll give it a simpler prompt:\n\nprompt = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\n\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\n\nAccording to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with only one of these words: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nOutput:\"\"\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=5,\n    do_sample=True, \n    temperature=0.3,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n\nSummary: Two countries\n\n\nTinyInstruct-33M does not seem aligned to this type of instruction following. As a final party trick I’ll see if setting up the financial_phrasebank data in the model’s training format nudges it in the right direction.\n\nprompt = \"\"\"Summary: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier,\nwhile it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nFeatures: positive\nSentence: positive\nWords: positive\nStory: \"\"\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=100,\n    do_sample=True, \n    temperature=0.2,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n\n\nOnce upon a time there was a little girl called Athena. She was three years old and loved to play with her friends. One day, Athena's friends asked her to come to the park with them. When she arrived, Athena noticed that everyone was wearing the same type of clothing as her. She was confused and asked her friends what they were doing.\n\nAthenaces were announced that they were called distinguishedIN Neck�a, and she was the official mascot of the local Park\n\n\n\nprompt = \"\"\"Summary: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier,\nwhile it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nFeatures: respond with one word (negative, positive, neutral)\nSentence: respond with one word (negative, positive, neutral)\nWords: respond with one word (negative, positive, neutral)\nStory: \"\"\"\n\n\n\nShow pipeline\noutput = pipe(\n    prompt,\n    max_new_tokens=100,\n    do_sample=True, \n    temperature=0.1,\n    return_full_text=False)\n\nprint(output[0][\"generated_text\"])\n\n\n\n\nOnce upon a time there was a quarter yearrissa. She was a quarterstruck shopper and loved to move around. Every year, she would go to a different town and meet new people.\n\nOne year, she was asked to come to a special place. It was called \"Adopt A Receive\". She was excited to go and meet new people.\n\nWhen she arrived at the place, she saw a big sign that said \"Adwin Opener\".\n\n\nInteresting that at a low temperature (0.1) TinyInstruct has snuck in the word “quarter” from the financial_phrasebank sentence. However, it still does not classify the sentiment of the sentence."
  },
  {
    "objectID": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#final-thoughts",
    "href": "posts/2024-08-05-tinyinstruct-sentiment-classification/index.html#final-thoughts",
    "title": "Using TinyInstruct-33M for financial_phrasebank Sentiment Classification",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis was probably the least exciting LLM exercise I’ve ever done, but I felt it was necessary to at least give TinyInstruct-33M a fair shot at classifying financial_phrasebank sentiment without fine-tuning it.\nIn a separate notebook, I’ll fine-tune TinyInstruct-33M on a portion of the financial_phrasebank dataset and see how it performs on a held out test set.\nI hope you enjoyed this (short) blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-05-15-multi-target/index.html",
    "href": "posts/2024-05-15-multi-target/index.html",
    "title": "Improving Kaggle Private Score with Multi-Target Classification",
    "section": "",
    "text": "In this notebook, I’ll use the code from Jeremy’s Road to the Top, Part 4 notebook to train a model that classifies both the disease and the variety of the rice paddy. In the fastai course Part 1 Lesson 7 video, Jeremy encourages viewers/students to see how this model scores and to explore the inputs and outputs in order to understand how the model behaves. I’ll do just that in this notebook.\n::: {.cell _kg_hide-input=‘true’ _kg_hide-output=‘true’ execution=‘{“iopub.execute_input”:“2024-05-16T00:04:59.838201Z”,“iopub.status.busy”:“2024-05-16T00:04:59.837710Z”,“iopub.status.idle”:“2024-05-16T00:05:32.536710Z”,“shell.execute_reply”:“2024-05-16T00:05:32.535138Z”,“shell.execute_reply.started”:“2024-05-16T00:04:59.838095Z”}’ trusted=‘true’ execution_count=1}\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastcore fastai timm\n\nimport timm\n\nfrom fastai.vision.widgets import *\nfrom fastai.vision.all import *\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uqq fastkaggle\n\nfrom fastkaggle import *\nfrom fastcore.all import *\nfrom fastdownload import download_url\n:::"
  },
  {
    "objectID": "posts/2024-05-15-multi-target/index.html#multi-output-dataloader",
    "href": "posts/2024-05-15-multi-target/index.html#multi-output-dataloader",
    "title": "Improving Kaggle Private Score with Multi-Target Classification",
    "section": "Multi-output DataLoader",
    "text": "Multi-output DataLoader\n::: {.cell _kg_hide-output=‘true’ execution=‘{“iopub.execute_input”:“2024-05-16T00:05:32.540740Z”,“iopub.status.busy”:“2024-05-16T00:05:32.539503Z”,“iopub.status.idle”:“2024-05-16T00:05:42.792866Z”,“shell.execute_reply”:“2024-05-16T00:05:42.791792Z”,“shell.execute_reply.started”:“2024-05-16T00:05:32.540695Z”}’ trusted=‘true’ execution_count=2}\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\nfrom fastai.vision.all import *\nset_seed(42)\n\nfrom fastcore.parallel import *\ntrn_path = path/'train_images'\n:::\n\ndf = pd.read_csv(path/'train.csv', index_col='image_id')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      label\n      variety\n      age\n    \n    \n      image_id\n      \n      \n      \n    \n  \n  \n    \n      100330.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100365.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100382.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100632.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      101918.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n  \n\n\n\n\nThere are 10 unique labels (including normal) and 10 unique variety values. This means the model will have to predict 10 + 10 = 20 different probabilities.\n\ndf['label'].unique().shape, df['variety'].unique().shape\n\n((10,), (10,))\n\n\nJeremy creates a get_variety helper function which returns the variety column value for a given image path. Note that when he created df, he passes index_col='image_id' in order to make the index of that DataFrame the image path for easier lookup.\n\ndef get_variety(p): return df.loc[p.name, 'variety']\n\n\nget_variety(Path('100330.jpg')) == 'ADT45'\n\nTrue\n\n\nJeremy’s DataBlock consists of three blocks—one ImageBlock that processes the inputs and two CategoryBlocks, one per target (label and variety). Because there are three blocks we have to specify that the number of inputs, n_inp is 1. Note that we can specify a list of get_y getters, one for each target.\n\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock,CategoryBlock),\n    n_inp=1,\n    get_items=get_image_files,\n    get_y = [parent_label,get_variety],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(192, method='squish'),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n).dataloaders(trn_path)\n\n\ndls.show_batch(max_n=6)\n\n\n\n\nAs done in his notebook, I’ll first test this approach by training a single-target classifier for disease label. Since there are three blocks, the loss function and metrics will receive three things: the predictions (inp), the disease labels and the variety labels."
  },
  {
    "objectID": "posts/2024-05-15-multi-target/index.html#single-target-model-with-multi-output-dataloaders",
    "href": "posts/2024-05-15-multi-target/index.html#single-target-model-with-multi-output-dataloaders",
    "title": "Improving Kaggle Private Score with Multi-Target Classification",
    "section": "Single-target Model with Multi-output DataLoaders",
    "text": "Single-target Model with Multi-output DataLoaders\n\nerror_rate??\n\n\nSignature: error_rate(inp, targ, axis=-1)\nSource:   \ndef error_rate(inp, targ, axis=-1):\n    \"1 - `accuracy`\"\n    return 1 - accuracy(inp, targ, axis=axis)\nFile:      /opt/conda/lib/python3.7/site-packages/fastai/metrics.py\nType:      function\n\n\n\n\n\ndef disease_err(inp,disease,variety): return error_rate(inp,disease)\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp,disease)\n\n\nlearn = vision_learner(dls, resnet34, loss_func=disease_loss, metrics=disease_err, n_out=10).to_fp16()\nlr = 0.01\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlearn.fine_tune(12, lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      time\n    \n  \n  \n    \n      0\n      1.950858\n      1.315877\n      0.420471\n      01:15\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      time\n    \n  \n  \n    \n      0\n      0.804355\n      0.466699\n      0.148967\n      01:06\n    \n    \n      1\n      0.602656\n      0.520798\n      0.152811\n      01:07\n    \n    \n      2\n      0.535991\n      0.533135\n      0.146084\n      01:06\n    \n    \n      3\n      0.499837\n      0.413230\n      0.125420\n      01:06\n    \n    \n      4\n      0.374249\n      0.522707\n      0.145123\n      01:07\n    \n    \n      5\n      0.303674\n      0.249570\n      0.074003\n      01:07\n    \n    \n      6\n      0.233556\n      0.222586\n      0.061989\n      01:07\n    \n    \n      7\n      0.162041\n      0.166682\n      0.044690\n      01:06\n    \n    \n      8\n      0.123177\n      0.137297\n      0.036521\n      01:06\n    \n    \n      9\n      0.080774\n      0.139264\n      0.034599\n      01:06\n    \n    \n      10\n      0.051097\n      0.124689\n      0.031235\n      01:06\n    \n    \n      11\n      0.054974\n      0.123993\n      0.032196\n      01:07\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nThe outputs of this model consist of 10 predictions for each image:\n\nprobs = learn.tta(dl=learn.dls.valid)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nprobs[0].shape\n\ntorch.Size([2081, 10])\n\n\nHere’s what the activations of the models look like:\n\nprobs[0][:5]\n\ntensor([[ 2.8325, -7.3865, -3.3020,  8.6670, -3.5552, -5.3201,  0.5134,  0.8277,\n         -1.1257,  1.3131],\n        [-0.3330, -4.0044, -4.1958,  0.3577, -2.9964, -3.6226, -0.7052,  3.2369,\n          9.9019, -0.7284],\n        [-1.3333, -3.0190, -4.2167, -2.3115, -2.1287, -2.2457, -1.8324, -2.1084,\n         -1.1698, 13.2637],\n        [ 4.0613, 16.8584, -3.1682, -1.8026, -0.4133, -5.2385,  0.7230, -3.3894,\n         -7.2209, -2.8204],\n        [-1.5410, -0.0458,  1.3879, -0.5194, -2.3740, 13.3403, -1.4106, -4.4908,\n         -1.3759, -1.7310]])\n\n\nMost of the output activations are between -5 and +5:\n\nplt.hist(probs[0].flatten().detach().numpy());\n\n\n\n\nThe second object returned by tta is a tuple where the first tensor is the target label for disease and the second tensor is the target variety.\n\nprobs[0].argmax(dim=1)\n\ntensor([3, 8, 9,  ..., 9, 5, 1])\n\n\nThe error rate for the TTA predictions on the validation set is 0.025, similar to what Jeremy had.\n\n1 - (probs[0].argmax(dim=1) == probs[1][0]).float().mean()\n\ntensor(0.0250)\n\n\nI’ll now create the test DataLoaders using the test set (making sure to sort the files so they are in the same order as the sample submission CSV):\n\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\ntst_files[:5]\n\n(#5) [Path('../input/paddy-disease-classification/test_images/200001.jpg'),Path('../input/paddy-disease-classification/test_images/200002.jpg'),Path('../input/paddy-disease-classification/test_images/200003.jpg'),Path('../input/paddy-disease-classification/test_images/200004.jpg'),Path('../input/paddy-disease-classification/test_images/200005.jpg')]\n\n\nThen I’ll calculate the TTA predictions on the test set:\n\ntst_dl = dls.test_dl(tst_files)\nprobs = learn.tta(dl=tst_dl)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nlen(tst_files), probs[0].shape\n\n(3469, torch.Size([3469, 10]))\n\n\nMost of the activations are between -10 and +10.\n\nplt.hist(probs[0].flatten().detach().numpy());\n\n\n\n\nHere is the distribution of classes (index of maximum activation per image):\n\nplt.hist(probs[0].argmax(dim=1).flatten().detach().numpy());\n\n\n\n\nI’ll export this as a CSV so I can submit it to Kaggle for scoring.\n\n# get the index (class) of the maximum prediction for each item\nidxs = probs[0].argmax(dim=1)\nidxs\n\ntensor([7, 8, 3,  ..., 8, 1, 5])\n\n\nThe vocab contains two sets of labels—one for disease and one for variety. I only want to map the disease labels for now.\n\ndls.vocab[0]\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\n\n# convert indexes to vocab strings\nmapping = dict(enumerate(dls.vocab[0]))\nmapping\n\n{0: 'bacterial_leaf_blight',\n 1: 'bacterial_leaf_streak',\n 2: 'bacterial_panicle_blight',\n 3: 'blast',\n 4: 'brown_spot',\n 5: 'dead_heart',\n 6: 'downy_mildew',\n 7: 'hispa',\n 8: 'normal',\n 9: 'tungro'}\n\n\n\n# add vocab strings to sample submission file and export to CSV\nss = pd.read_csv(path/'sample_submission.csv')\nresults = pd.Series(idxs.numpy(), name='idxs').map(mapping)\nss.label = results\nss.to_csv('subm1.csv', index=False)\n\n\n!head subm1.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis submission resulted in a Private score of 0.97580."
  },
  {
    "objectID": "posts/2024-05-15-multi-target/index.html#single-target-model-with-single-output-dataloaders",
    "href": "posts/2024-05-15-multi-target/index.html#single-target-model-with-single-output-dataloaders",
    "title": "Improving Kaggle Private Score with Multi-Target Classification",
    "section": "Single-target Model with Single-output DataLoaders",
    "text": "Single-target Model with Single-output DataLoaders\nAs another comparison/baseline, I’ll train a single-target disease classifier using a single-output DataLoaders object.\n\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock),\n    get_items=get_image_files,\n    get_y = parent_label,\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(192, method='squish'),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n).dataloaders(trn_path)\n\n\ndls.show_batch(max_n=6)\n\n\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate).to_fp16()\nlearn.fine_tune(12, 0.01)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.941936\n      1.325037\n      0.419990\n      01:16\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.808069\n      0.461713\n      0.158097\n      01:06\n    \n    \n      1\n      0.592128\n      0.562304\n      0.172033\n      01:06\n    \n    \n      2\n      0.553166\n      0.526510\n      0.142239\n      01:06\n    \n    \n      3\n      0.499870\n      0.468296\n      0.137914\n      01:06\n    \n    \n      4\n      0.365323\n      0.344052\n      0.095627\n      00:58\n    \n    \n      5\n      0.315392\n      0.372406\n      0.105718\n      00:51\n    \n    \n      6\n      0.245668\n      0.210443\n      0.062470\n      00:51\n    \n    \n      7\n      0.165351\n      0.178430\n      0.047093\n      00:51\n    \n    \n      8\n      0.112827\n      0.153295\n      0.038924\n      00:50\n    \n    \n      9\n      0.079190\n      0.144607\n      0.033638\n      00:51\n    \n    \n      10\n      0.048934\n      0.128548\n      0.029313\n      00:51\n    \n    \n      11\n      0.046523\n      0.129133\n      0.027871\n      00:50\n    \n  \n\n\n\n\nprobs = learn.tta(dl=learn.dls.valid)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n\nprobs[0].shape, probs[1].shape\n\n(torch.Size([2081, 10]), torch.Size([2081]))\n\n\nThis model has a slightly better TTA error rate on the validation set.\n\n1 - (probs[0].argmax(dim=1) == probs[1]).float().mean()\n\ntensor(0.0240)\n\n\nThere is only one set of vocab since there is only 1 target:\n\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\n\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\n\ntst_dl = dls.test_dl(tst_files)\nprobs = learn.tta(dl=tst_dl)\n\n# get the index (class) of the maximum prediction for each item\nidxs = probs[0].argmax(dim=1)\n\n# convert indexes to vocab strings\nmapping = dict(enumerate(dls.vocab))\n\n# add vocab strings to sample submission file and export to CSV\nss = pd.read_csv(path/'sample_submission.csv')\nresults = pd.Series(idxs.numpy(), name='idxs').map(mapping)\nss.label = results\nss.to_csv('subm2.csv', index=False)\n!head subm2.csv\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis submission got the same Private score as the single-target model trained on the multi-output DataLoaders: 0.97580"
  },
  {
    "objectID": "posts/2024-05-15-multi-target/index.html#multi-target-model-on-multi-output-dataloaders",
    "href": "posts/2024-05-15-multi-target/index.html#multi-target-model-on-multi-output-dataloaders",
    "title": "Improving Kaggle Private Score with Multi-Target Classification",
    "section": "Multi-target model on Multi-output DataLoaders",
    "text": "Multi-target model on Multi-output DataLoaders\n\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock,CategoryBlock),\n    n_inp=1,\n    get_items=get_image_files,\n    get_y = [parent_label,get_variety],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(192, method='squish'),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n).dataloaders(trn_path)\n\n\ndls.show_batch()\n\n\n\n\nJeremy picks the first ten activations of the model as the disease classes and the second ten as the variety classes. The disease_loss and variety_loss are defined accordingly:\n\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp[:,:10],disease)\n\n\ndef variety_loss(inp,disease,variety): return F.cross_entropy(inp[:,10:],variety)\n\nThe combined loss we are trying to minimize is the sum of the two target’s loss:\n\ndef combine_loss(inp,disease,variety): return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety)\n\nThe error rates are defined the same way (first 10 predictions for disease, second 10 for variety):\n\ndef disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease)\ndef variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety)\n\nerr_metrics = (disease_err,variety_err)\n\nJeremy also chooses to view the disease loss and variety loss separately.\n\nall_metrics = err_metrics+(disease_loss,variety_loss)\n\nn_out is set to 20 since we have two pairs of 10 classes that the model is trying to predict.\n\nlearn = vision_learner(dls, resnet34, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16()\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\nJeremy mentioned that we might have to train this model for longer before it performs as well as the single-target model since we are asking to do more (predicts twice the number of targets). I’ll train and submit with 12 epochs first as a baseline.\n\nlearn.fine_tune(12, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      3.191633\n      1.969826\n      0.419990\n      0.219125\n      1.272959\n      0.696868\n      01:19\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      1.266333\n      0.694686\n      0.154733\n      0.059106\n      0.489899\n      0.204788\n      01:06\n    \n    \n      1\n      0.887806\n      0.605122\n      0.146564\n      0.053340\n      0.440036\n      0.165086\n      01:06\n    \n    \n      2\n      0.834215\n      1.014124\n      0.188371\n      0.090341\n      0.617206\n      0.396918\n      01:06\n    \n    \n      3\n      0.709637\n      0.634970\n      0.117732\n      0.058145\n      0.439491\n      0.195479\n      01:06\n    \n    \n      4\n      0.587873\n      0.580158\n      0.120135\n      0.045651\n      0.420883\n      0.159275\n      01:06\n    \n    \n      5\n      0.453230\n      0.404975\n      0.084575\n      0.031716\n      0.295974\n      0.109001\n      01:06\n    \n    \n      6\n      0.332355\n      0.315852\n      0.069678\n      0.017780\n      0.252630\n      0.063222\n      01:07\n    \n    \n      7\n      0.240017\n      0.276671\n      0.054781\n      0.025469\n      0.197499\n      0.079172\n      01:07\n    \n    \n      8\n      0.166121\n      0.182990\n      0.039885\n      0.012494\n      0.140265\n      0.042726\n      01:06\n    \n    \n      9\n      0.112039\n      0.182566\n      0.036040\n      0.011533\n      0.138646\n      0.043920\n      01:06\n    \n    \n      10\n      0.081297\n      0.177871\n      0.034599\n      0.008650\n      0.136665\n      0.041206\n      01:06\n    \n    \n      11\n      0.074365\n      0.173486\n      0.031716\n      0.008650\n      0.133155\n      0.040331\n      01:06\n    \n  \n\n\n\n\nprobs = learn.tta(dl=learn.dls.valid)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nThere are 20 predictions for each image.\n\nprobs[0].shape\n\ntorch.Size([2081, 20])\n\n\nThe first 10 predictions are for the disease label.\n\n1 - (probs[0][:,:10].argmax(dim=1) == probs[1][0]).float().mean()\n\ntensor(0.0279)\n\n\nThe TTA error rate on the validation set is slightly lower than the single-target models.\nJust out of curiosity, I’ll also calculate the TTA error rate for variety:\n\n1 - (probs[0][:,10:].argmax(dim=1) == probs[1][1]).float().mean()\n\ntensor(0.0077)\n\n\nThe model is much more accurate at predicting the variety of rice.\nI’ll submit TTA predictions on the test set:\n\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\n\ntst_dl = dls.test_dl(tst_files)\nprobs = learn.tta(dl=tst_dl)\n\n# get the index (class) of the maximum prediction for each item\nidxs = probs[0][:,:10].argmax(dim=1)\n\n# convert indexes to vocab strings\nmapping = dict(enumerate(dls.vocab[0]))\n\n# add vocab strings to sample submission file and export to CSV\nss = pd.read_csv(path/'sample_submission.csv')\nresults = pd.Series(idxs.numpy(), name='idxs').map(mapping)\nss.label = results\nss.to_csv('subm3.csv', index=False)\n!head subm3.csv\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThis model gave me the same Private score: 0.97580.\nFinally, I’ll train the model for a few more epochs and see if that improves the score.\n\nlearn = vision_learner(dls, resnet34, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16()\nlearn.fine_tune(16, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      3.186875\n      2.025326\n      0.409419\n      0.214320\n      1.350042\n      0.675284\n      01:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      1.275486\n      0.722841\n      0.168188\n      0.068236\n      0.495478\n      0.227363\n      01:06\n    \n    \n      1\n      0.812811\n      0.582493\n      0.128784\n      0.053340\n      0.401935\n      0.180558\n      01:08\n    \n    \n      2\n      0.736832\n      0.655212\n      0.145603\n      0.064873\n      0.447887\n      0.207325\n      01:08\n    \n    \n      3\n      0.777283\n      1.089296\n      0.193176\n      0.119654\n      0.611161\n      0.478135\n      01:08\n    \n    \n      4\n      0.656918\n      0.736651\n      0.132148\n      0.063912\n      0.465955\n      0.270696\n      01:07\n    \n    \n      5\n      0.611457\n      0.523899\n      0.104277\n      0.044690\n      0.359421\n      0.164478\n      01:06\n    \n    \n      6\n      0.497228\n      0.408523\n      0.076886\n      0.039885\n      0.276221\n      0.132302\n      01:11\n    \n    \n      7\n      0.418236\n      0.349095\n      0.065834\n      0.026430\n      0.262863\n      0.086232\n      01:10\n    \n    \n      8\n      0.292306\n      0.334778\n      0.070639\n      0.022105\n      0.253223\n      0.081556\n      01:07\n    \n    \n      9\n      0.223652\n      0.276235\n      0.051418\n      0.014897\n      0.214205\n      0.062030\n      01:06\n    \n    \n      10\n      0.172232\n      0.222825\n      0.046612\n      0.013936\n      0.166589\n      0.056236\n      01:07\n    \n    \n      11\n      0.117083\n      0.198665\n      0.038443\n      0.008650\n      0.154245\n      0.044420\n      01:06\n    \n    \n      12\n      0.086082\n      0.196476\n      0.040365\n      0.010091\n      0.159040\n      0.037436\n      01:08\n    \n    \n      13\n      0.074184\n      0.185352\n      0.039404\n      0.006728\n      0.152728\n      0.032625\n      01:07\n    \n    \n      14\n      0.056641\n      0.174692\n      0.033638\n      0.006728\n      0.144675\n      0.030018\n      01:07\n    \n    \n      15\n      0.043815\n      0.177776\n      0.034118\n      0.007208\n      0.144719\n      0.033058\n      01:06\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nprobs = learn.tta(dl=learn.dls.valid)\n1 - (probs[0][:,:10].argmax(dim=1) == probs[1][0]).float().mean()\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\ntensor(0.0245)\n\n\nThat’s a slightly better TTA validation error rate.\n\ntst_files = get_image_files(path/'test_images')\ntst_files.sort()\n\ntst_dl = dls.test_dl(tst_files)\nprobs = learn.tta(dl=tst_dl)\n\n# get the index (class) of the maximum prediction for each item\nidxs = probs[0][:,:10].argmax(dim=1)\n\n# convert indexes to vocab strings\nmapping = dict(enumerate(dls.vocab[0]))\n\n# add vocab strings to sample submission file and export to CSV\nss = pd.read_csv(path/'sample_submission.csv')\nresults = pd.Series(idxs.numpy(), name='idxs').map(mapping)\nss.label = results\nss.to_csv('subm4.csv', index=False)\n!head subm4.csv\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\nThe Private scored improved to 0.97811! That’s not insignificant. Training on multi-target, at least for a resnet34 on the Resize(192, method='squish') item transform and aug_transforms(size=128, min_scale=0.75) batch transform. Here is the summary of the four submissions from this notebook:"
  },
  {
    "objectID": "posts/2024-05-15-multi-target/index.html#final-thoughts",
    "href": "posts/2024-05-15-multi-target/index.html#final-thoughts",
    "title": "Improving Kaggle Private Score with Multi-Target Classification",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI am so glad that I ran this experiment since I am currently involved in a Kaggle competition where I was considering multi-target classification. There’s no certainty that it’ll improve my Private score in that situation, but it’s promising to see it improve the Paddy Disease Classification Private score here. Many thanks to Jeremy for introducing us to these engaging concepts.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-07-02-bn-placement/index.html",
    "href": "posts/2024-07-02-bn-placement/index.html",
    "title": "Comparing CNN Performance by Varying Batch Normalization Placement",
    "section": "",
    "text": "In this notebook I’ll work through the following prompt given in the “Further Research” section of Chapter 13 (Convolutional Neural Networks):\n\nTry moving the activation function after the batch normalization layer in conv. Does it make a difference? See what you can find out about what order is recommended and why.\n\n\nfrom fastai.vision.all import *\nfrom fastai.callback.hook import *\nmatplotlib.rc('image', cmap='Greys')\n\n\npath = untar_data(URLs.MNIST)\npath.ls()\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\n(#2) [Path('/root/.fastai/data/mnist_png/testing'),Path('/root/.fastai/data/mnist_png/training')]"
  },
  {
    "objectID": "posts/2024-07-02-bn-placement/index.html#activation-function-after-batch-normalization",
    "href": "posts/2024-07-02-bn-placement/index.html#activation-function-after-batch-normalization",
    "title": "Comparing CNN Performance by Varying Batch Normalization Placement",
    "section": "Activation Function After Batch Normalization",
    "text": "Activation Function After Batch Normalization\nI’ll first train a model with the same architecture as the textbook—the activation function (in this case nn.ReLU) is placed after nn.BatchNorm2d in nn.Sequential.\n\ndef conv(ni, nf, ks=3, act=True):\n  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n  layers.append(nn.BatchNorm2d(nf))\n  if act: layers.append(nn.ReLU())\n  return nn.Sequential(*layers)\n\n\nconv(1,8) # activation function is after batch normalization\n\nSequential(\n  (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU()\n)\n\n\nI’ll re-use the simple_cnn architecture as well as the get_dls and fit helper functions defined in the textbook:\n\ndef simple_cnn():\n  return sequential(\n      conv(1, 8, ks=5),         # 14x14\n      conv(8, 16),              # 7x7\n      conv(16, 32),             # 4x4\n      conv(32, 64),             # 2x2\n      conv(64, 10, act=False),  # 1x1\n      Flatten()\n  )\n\n\n# create a function to change dls params\ndef get_dls(bs=64):\n  return DataBlock(\n      blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n      get_items=get_image_files,\n      splitter=GrandparentSplitter('training', 'testing'),\n      get_y=parent_label,\n      batch_tfms=Normalize()\n  ).dataloaders(path, bs=bs)\n\n\ndls = get_dls(512)\ndls.show_batch(max_n=9, figsize=(4,4))\n\n\n\n\n\ndef fit(epochs=1, lr=0.06):\n  learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))\n  learn.fit_one_cycle(epochs, lr)\n  return learn\n\n\nlearn = fit()\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.135710\n      0.056765\n      0.986600\n      01:10\n    \n  \n\n\n\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nI’ll take a look at the activation distribution across batches in a couple of ways:\n\ncolor_dim which will display an image where each column of pixels is a histogram of one batch of activations where lighter colors are close to zero and darker colors are non-zero.\nplot_layer_stats which displays the mean, standard deviation and %-near-zero activations across batches during training.\n\nThe output of color_dim looks as expected—a smooth transition from zero to non-zero activations.\n\nlearn.activation_stats.color_dim(-4)\n\n\n\n\nThe % near zero chart looks better than the counterexamples in the text but it still looks pretty high (~65% of the activations are near zero after batch #25).\n\nlearn.activation_stats.plot_layer_stats(-2)"
  },
  {
    "objectID": "posts/2024-07-02-bn-placement/index.html#activation-function-before-batch-normalization",
    "href": "posts/2024-07-02-bn-placement/index.html#activation-function-before-batch-normalization",
    "title": "Comparing CNN Performance by Varying Batch Normalization Placement",
    "section": "Activation Function Before Batch Normalization",
    "text": "Activation Function Before Batch Normalization\nNext, I’ll reverse the order of nn.ReLU and nn.BatchNorm2d, placing the activation function before the batch normalization:\n\ndef conv(ni, nf, ks=3, act=True):\n  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n  if act: layers.append(nn.ReLU()) # activation function before batch norm\n  layers.append(nn.BatchNorm2d(nf))\n  return nn.Sequential(*layers)\n\n\nconv(1,8)\n\nSequential(\n  (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n  (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n\n\n\nsimple_cnn()\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (1): ReLU()\n    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (1): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (4): Sequential(\n    (0): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (5): fastai.layers.Flatten(full=False)\n)\n\n\nThe accuracy is slightly higher than when the activation function was placed after the batch norm (0.986600):\n\nlearn = fit()\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.130553\n      0.055105\n      0.987000\n      01:09\n    \n  \n\n\n\nThe activations look pretty good and it seems like there are greater non-zero activations to start with in this model, which is an improvement.\n\nlearn.activation_stats.color_dim(-4)\n\n\n\n\nThe standard deviation of the activations are smoother than before. However, there are more near-zero activations (and % near zero increases during training) which is a downside to this architecture.\n\nlearn.activation_stats.plot_layer_stats(-2)"
  },
  {
    "objectID": "posts/2024-07-02-bn-placement/index.html#running-more-experiments",
    "href": "posts/2024-07-02-bn-placement/index.html#running-more-experiments",
    "title": "Comparing CNN Performance by Varying Batch Normalization Placement",
    "section": "Running More Experiments",
    "text": "Running More Experiments\nIt’s tough to arrive at a conclusion with just one model for each architecture trained for just 1 epoch. To get a better sense of how performance is affected by the position of the activation function, I’ll train 10 models for 5 epochs for each architecture, storing the activation stats along the way.\nTo store the activation stats, I’ll grab the layer_stats for the penultimate layer (idx = -2) for each training run.\nI’ll store the layer_stats object in an L object so I can use the itemgot method to get the mean, standard deviation and %-near-zero values across trainings.\nI’ll also store each training run’s histograms to view the output of color_dim manually (using idx = -4 as done in the textbook):\n\nh = learn.activation_stats.hist(-4)\n\n\nax = subplots(figsize=(10,5))[1][0]\nax.imshow(h, origin='lower');\nax.axis('off');\n\n\n\n\n\nActivation Function After Batch Normalization\nI’ll update fit so that it doesn’t display the progress bar and logging during training (so that I don’t get a screen full of logs):\n\ndef fit(epochs=5, lr=0.06):\n  learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True))\n  with learn.no_logging(), learn.no_bar(): learn.fit_one_cycle(epochs, lr)\n  return learn\n\nI’ll train 10 models, each for 5 epochs, and save the layer_stats and hist for each model. I’ll also make sure to define conv such that the activation function is after the batch normalization.\nThese training runs should take about 50 minutes in total.\n\nlayer_stats_after = L()\nhist_after = L()\n\ndef conv(ni, nf, ks=3, act=True):\n  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n  layers.append(nn.BatchNorm2d(nf))\n  if act: layers.append(nn.ReLU()) # activation function after batch norm\n  return nn.Sequential(*layers)\n\nfor _ in range(10):\n  learn = fit()\n  layer_stats_after.append(learn.activation_stats.layer_stats(-2))\n  hist_after.append(learn.activation_stats.hist(-4))\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n\nActivation Function Before Batch Normalization\nI’ll run a similar set of trainings for the architecture where the activation function is placed before batch normalization.\n\nlayer_stats_before = L()\nhist_before = L()\n\ndef conv(ni, nf, ks=3, act=True):\n  layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n  if act: layers.append(nn.ReLU()) # activation function before batch norm\n  layers.append(nn.BatchNorm2d(nf))\n  return nn.Sequential(*layers)\n\nfor _ in range(10):\n  learn = fit()\n  layer_stats_before.append(learn.activation_stats.layer_stats(-2))\n  hist_before.append(learn.activation_stats.hist(-4))\n\n/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\nAnalysis of Training Results\nI’ll start by plotting the mean, standard deviation and %-near-zero activations for both architectures (activation function before/after batch norm).\nI’ll create this helper function to plot the spread of mean, standard deviation and %-near-zero activations:\n\ndef plot_stats_spread(layer_stats, titles, super_title):\n  fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n  fig.suptitle(super_title, fontsize=16)\n\n  for i, title in enumerate(titles):\n    y = layer_stats.itemgot(i)\n    x = range(len(y[0]))\n\n    y_min = np.minimum.reduce(y)\n    y_max = np.maximum.reduce(y)\n\n    axs[i].fill_between(x, y_min, y_max, alpha=0.5)\n    axs[i].set_title(f\"Spread of {title} Activations Across 10 Trainings\")\n    axs[i].set_xlabel(\"Batch\")\n    axs[i].set_ylabel(f\"{title}\")\n\n  plt.tight_layout()\n  plt.show()\n\n\ntitles = ['Mean', 'Std', '%-near-zero']\n\nIn the following six charts, I plot the spread or range of activation stats over 5 epochs across the 10 trainings. I chose to plot the spread (instead of 10 line plots) so that you can more easily see the range of values for each statistic.\nWhen the activation function is placed after the batch norm, the standard deviations and the %-age of activations near zero are lower than when the activation is placed before the batch norm. This shows the benefit of placing the activation after the batch norm.\n\nplot_stats_spread(layer_stats_after, titles, \"Activation Function After Batch Norm\")\n\n\n\n\n\nplot_stats_spread(layer_stats_before, titles, \"Activation Function Before Batch Norm\")\n\n\n\n\nI’ll create a helper function to plot the mean and median values for each statistic:\n\ndef plot_stats_avg(layer_stats, titles, super_title):\n  fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n  fig.suptitle(super_title, fontsize=16)\n\n  for i, title in enumerate(titles):\n    y = layer_stats.itemgot(i)\n    x = range(len(y[0]))\n\n    y_mean = np.mean(np.stack(y), axis=0)\n    y_median = np.median(np.stack(y), axis=0)\n\n    axs[i].plot(x, y_mean, color='red', alpha=0.5, linewidth=1)\n    axs[i].plot(x, y_median, color='blue', alpha=0.5, linewidth=1)\n    axs[i].set_title(f\"Average {title} Activations Across 10 Trainings\")\n    axs[i].set_xlabel(\"Batch\")\n    axs[i].set_ylabel(f\"{title}\")\n\n  plt.tight_layout()\n  plt.show()\n\nThe mean (red) and median (blue) values across the 10 training runs for each architecture show the same trends: lower standard deviation and lower %-near-zero activations when the activation function is placed after batch norm. Additionally, the average mean activations for the first 100 or so batches are less than 0 when the activation function is placed after the batch norm.\nOverall, these plots reiterate the benefit of placing the activation function after the batch norm.\n\nplot_stats_avg(layer_stats_after, titles, \"Activation Function After Batch Norm\")\n\n\n\n\n\nplot_stats_avg(layer_stats_before, titles, \"Activation Function Before Batch Norm\")\n\n\n\n\nNext, I’ll look at histograms of mean and median activation values for both architectures:\n\ndef plot_hist_avg(hist, super_title):\n  fig, axs = plt.subplots(2, 1, figsize=(20, 5))\n  fig.suptitle(super_title, fontsize=16)\n  h_mean = torch.stack(list(hist)).mean(0)\n  h_median = torch.stack(list(hist)).median(0)[0]\n\n  axs[0].imshow(h_mean, origin='lower');\n  axs[0].set_title(f\"Mean Activations Across 10 Trainings\")\n  axs[0].axis('off');\n\n  axs[1].imshow(h_median, origin='lower');\n  axs[1].set_title(f\"Median Activations Across 10 Trainings\")\n  axs[1].axis('off');\n\nIt’s tough to visually distinguish between the different shades of gray but the clear difference is that the non-zero activations when the activation function is placed after batch norm increase much slower than when it’s placed before.\n\nplot_hist_avg(hist_after, \"Activation Function After Batch Norm\")\n\n\n\n\n\nplot_hist_avg(hist_before, \"Activation Function Before Batch Norm\")"
  },
  {
    "objectID": "posts/2024-07-02-bn-placement/index.html#references-to-batch-norm-placement",
    "href": "posts/2024-07-02-bn-placement/index.html#references-to-batch-norm-placement",
    "title": "Comparing CNN Performance by Varying Batch Normalization Placement",
    "section": "References to Batch Norm Placement",
    "text": "References to Batch Norm Placement\nMy quick experiments show that placing the activation function after the batch normalization is preferred because it decreases the %-age of activations that are near zero, and decreases the standard deviation of the activations, both which are signs of “stable” training.\nHowever, these are just quick, rough experiments. What do the experts say? I googled around and found the following:\nThis fastai forums thread which led me to more resources.\nIn the original Batch Normalization paper the authors state (emphasis mine):\n\nWe add the BN transform immediately before the nonlinearity, by normalizing \\(x = Wu+ b\\)\n\nA quick note that BN before nonlinearity = nonlinearity after BN (which is the terminology I have been using: “activation function before BN” and “activation function after BN”).\nThey compare placing BN before and after the nonlinearity—my experiments matches their explanation, that the activations are sparser (i.e. there are more near-zero activations) when BN is placed after the nonlinearity (emphasis mine):\n\nThe goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution. On the contrary, (Gulcehre & Bengio, 2013) apply the standardization layer to the output of the nonlinearity, which results in sparser activations. In our large-scale image classification experiments, we have not observed the nonlinearity inputs to be sparse, neither with nor without Batch Normalization.\n\nI didn’t explore it here, but they also compare placing BN to the layer inputs before the weights are applied (which can be seen as another way of placing it after the nonlinearity; emphasis mine):\n\nWe could have also normalized the layer inputs \\(u\\), but since \\(u\\) is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would not eliminate the covariate shift. In contrast, \\(Wu + b\\) is more likely to have a symmetric, non-sparse distribution, that is “more Gaussian” (Hyvarinen & Oja, 2000); normalizing it is likely to produce activations with a stable distribution.\n\nIn a 2016 Keras GitHub comment Francois Chollet said (emphasis mine):\n\nI haven’t gone back to check what they are suggesting in their original paper, but I can guarantee that recent code written by Christian applies relu before BN. It is still occasionally a topic of debate, though."
  },
  {
    "objectID": "posts/2024-07-02-bn-placement/index.html#final-thoughts",
    "href": "posts/2024-07-02-bn-placement/index.html#final-thoughts",
    "title": "Comparing CNN Performance by Varying Batch Normalization Placement",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis portion of the Chapter got me excited because I felt I was finally peaking under the hood to see the dynamics of a neural net training run. The activation_stats callback is a simple but powerful tool to understand how different architectural changes are affecting training dynamics. I look forward to digging deeper under the hood in part 2 of the course!\nI hope you enjoyed this blog post. Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-04-05-dpo/index.html",
    "href": "posts/2024-04-05-dpo/index.html",
    "title": "Paper Math: DPO (Direct Preference Optimization)",
    "section": "",
    "text": "In this blog post I’ll walk through some of the math involved in the research paper Direct Preference Optimization: Your Language Model is Secretly a Reward Model.\nThe abstract:\n\nWhile large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n\nDPO involves preference data, a reference model and the parameterized model (i.e. the model being trained).\nI found this section of the paper provide the most intuition about why DPO works:\n\nAs the model gets more likely to pick the preferred response, the gradient increases. Conversely, as the implicit reward (log probability ratio of parameterized model and reference model) for rejected responses increases, the gradient increases. I think of these two terms contrasting each other, keeping the gradient from vanishing or exploding.\nIn the next few sections of this blog post, I’ll walk through parts of the paper where I found that math either challenging (and used Claude or ChatGPT to help me figure it out) and/or particulary helpful for my understanding of the concepts. In the final section, I do my best to connect the code implementation of DPO loss with the math formulas provided in the paper. Overall I found this process very rewarding and built my confidence to take on understanding the mathy parts of the paper moving forward."
  },
  {
    "objectID": "posts/2024-04-05-dpo/index.html#section-3-preliminaries",
    "href": "posts/2024-04-05-dpo/index.html#section-3-preliminaries",
    "title": "Paper Math: DPO (Direct Preference Optimization)",
    "section": "Section 3: Preliminaries",
    "text": "Section 3: Preliminaries\n\nReward Modelling Phase\nBradley-Terry model stipulates that the human preference distribution \\(p^*\\) can be written as:\nEquation (1): \\[ p^*(y_1 \\succ y_2|x) = \\frac{\\exp(r^*(x, y_1))}{\\exp(r^*(x, y_1)) + \\exp(r^*(x, y_2))}\\]\nWhere \\(r^*(y, x)\\) is a latent reward model we don’t have access to.\n(I don’t know if it’s mathematically correct to say this but \\(p^*\\) function looks like softmax).\n\\(\\succ\\) = “succeeds” symbol. A couple of definitions I found online, seems pretty straightforward:\n\nThe term successor and predecessor in Math means that they come directly after or before the particular number respectively. Both successor and predecessor basically applied only to the whole numbers. The successor is termed as after numbers whereas the predecessor is termed as before numbers.\nSuccessor in Math refers to the after the term of a particular term while the predecessor in Math refers to them before the term of a particular term. We can find the successor of a whole number by adding one to the particularly given number whereas to find the predecessor of a whole number we will subtract one from the particularly given number. In this article, we will study what are successor and predecessor, successor meaning, predecessor meaning, how to find the successor of a given number, how to find the predecessor of a given number, etc. Source.\n\n\n\nThe succeeds operator (≻) is a generalization of idea behind some numbers being bigger than others.\nThe succeeds operator (≻) is like 10>4 , except that it applies to objects other than numbers. (Source)\n\n\n\nA preference relation expresses the consumer’s feelings between pairs of objects in X . We denote the preference relation by \\(\\succeq\\) and imagine that for every pair x and y from X , the consumer is willing to say that either x \\(\\succeq\\) y , meaning x is at least as good as y, or not. Source\n\nStatic dataset of comparisons sampled from \\(p^*\\):\n\\[\\mathcal{D}\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\}^N\\]\nAssuming we have access to \\(\\mathcal{D}\\) we can parameterize a reward model \\(r_{\\phi}(x,y)\\) and estimate parameters via maximum likelihood. Framed as a binary classification problem we have the negative log-likelihood loss:\nEquation (2):\n\n\\[\\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\log\\sigma(r_{\\phi}(x, y_w) -  r_{\\phi}(x, y_l)\\big]\\]\n\nIn the loss function above, as \\(r_{\\phi}(x, y_w) - r_{\\phi}(x, y_l)\\) increases (the model assigns higher reward to the preferred response), sigmoid (\\(\\sigma\\), blue function below) goes to 1 and \\(-\\log\\sigma\\) (red function below) goes to 0.\n\nThe network \\(r_{\\phi}\\) is often initialized from the SFT model \\(\\pi^{SFT}(y|x)\\) with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value.\n\n\nRL Fine-Tuning Phase\nOptimization problem formulated as:\nEquation (3):\n\\[\\max_{\\pi_{\\theta}}\\mathbb{E}_{x\\sim\\mathcal{D}, y\\sim\\pi_{\\theta}(y|x)}[r_\\phi(x,y)]-\\beta\\mathbb{D}_{KL}[\\pi_\\theta(y|x) \\;||\\; \\pi_{ref}(y|x)]\\]\n\\(\\beta\\) is a parameter controlling the deviation from the base reference policy \\(\\pi_{ref}\\) (the initial SFT model \\(\\pi^{SFT}\\)).\nThe language model policy \\(\\pi_\\theta\\) is also initialized to \\(\\pi^{SFT}\\).\nThis objective is not differentiable because language generation is discrete, so the standard approach is to construct the reward function as:\n\\[ r(x,y) = r_\\phi(x,y)-\\beta(\\log\\pi_\\theta(y|x) - \\log\\pi_{ref}(y|x)\\]\nAnd maximize it using PPO."
  },
  {
    "objectID": "posts/2024-04-05-dpo/index.html#appendix-a.1-deriving-the-optimum-of-the-kl-constrained-reward-maximization-objective",
    "href": "posts/2024-04-05-dpo/index.html#appendix-a.1-deriving-the-optimum-of-the-kl-constrained-reward-maximization-objective",
    "title": "Paper Math: DPO (Direct Preference Optimization)",
    "section": "Appendix A.1: Deriving the Optimum of the KL-Constrained Reward Maximization Objective",
    "text": "Appendix A.1: Deriving the Optimum of the KL-Constrained Reward Maximization Objective\nStart with a modified version of Equation (3), where now we have any reward function \\(r(x,y)\\) and a general non-parametric policy class \\(\\pi\\):\nStep 1: \\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}, y\\sim\\pi}[r(x,y)]-\\beta\\mathbb{D}_{KL}[\\pi(y|x) \\;||\\; \\pi_{ref}(y|x)]\\]\nStep 2:\n\\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[r(x,y)]-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\big]\\]\n\nChatGPT explanation on how to go from Step 1 to Step 2:\nPrompt:\nExplain in detail how you go from:\nStep 1: \\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}, y\\sim\\pi}[r(x,y)]-\\beta\\mathbb{D}_{KL}[\\pi(y|x) \\;||\\; \\pi_{ref}(y|x)]\\]\nto Step 2: \\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[r(x,y)]-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\big]\\]\nResponse:\nTo go from Step 1 to Step 2, we will first break down the components of the expectation terms and then manipulate them accordingly.\nIn step 1:\n\\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}, y\\sim\\pi}[r(x,y)]-\\beta\\mathbb{D}_{KL}[\\pi(y|x) \\;||\\; \\pi_{ref}(y|x)]\\]\nThe first term represents the expected reward under the joint distribution of \\(x\\) and \\(y\\) drawn from the distribution \\(\\mathcal{D}\\) and \\(\\pi\\) respectively. The second term is the Kullback-Leibler (KL) divergence between the conditional distribution \\(\\pi(y|x)\\) and a reference conditional distribution \\(\\pi_{ref}(y|x)\\).\nNow, to transform it into Step 2:\n\\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[r(x,y)]-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\big]\\]\nWe will separate the expectation over \\(x\\) and \\(y\\), and then express the KL divergence term differently.\n\nSeparation of expectations: We start by separating the joint expectation into two nested expectations:\n\n\\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[ \\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)\\right]\\right] - \\beta\\mathbb{D}_{KL}[\\pi(y|x) \\;||\\; \\pi_{ref}(y|x)]\\]\n\nTransforming the KL divergence term:\n\nNext, we express the KL divergence term differently using its definition:\n\\[\\mathbb{D}_{KL}[\\pi(y|x) \\;||\\; \\pi_{ref}(y|x)] = \\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\right]\\]\nHere, we are effectively taking the expectation over \\(y\\) for the logarithm of the ratio of the two distributions.\nAfter these transformations, we have successfully rewritten Step 1 into the desired form of Step 2. The key steps involve separating the expectations over \\(x\\) and \\(y\\), and expressing the KL divergence term using its definition.\n\nBased on what ChatGPT provided, I’m adding one more detail that it didn’t write out, which is plugging in the KL divergence term using its definition into the main equation before simplifying it:\n\\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[ \\mathbb{E}_{y\\sim\\pi(y|x)}\\left[r(x,y)\\right]\\right] - \\beta\\mathbb{E}_{y\\sim\\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\right]\\]\nI think is then simplified to:\n\\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[r(x,y) - \\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\big]\\]\n\nNext in the paper, is Step 3:\n\\[\\min_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)} - \\frac{1}{\\beta}r(x,y)\\big]\\]\n(Summarized) ChatGPT explanation on how to go from Step 2 to Step 3:\nPrompt:\nExplain in detail how you go from:\nStep 2: \\[\\max_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\big]\\]\nto Step 3:\n\\[\\min_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)} - \\frac{1}{\\beta}r(x,y)\\big]\\]\nResponse:\nTo go from Step 2 to Step 3, we need to transform the maximization problem into a minimization problem, and rearrange the terms accordingly. Let’s break down the steps:\n\nNegate the Objective Function:\n\nTo transform a maximization problem into a minimization problem, we can simply negate the objective function. Since maximizing \\(\\mathbb{E}[f(x)]\\) is equivalent to minimizing \\(−\\mathbb{E}[f(x)]\\), we negate the entire objective function:\n\\[-\\big(r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\big) = \\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)} - r(x,y) \\]\n\nReorder the terms\n\nNext, we rearrange the terms in the objective function to match the desired form.\n\\[\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)} - \\frac{1}{\\beta}r(x,y)\\]\nBy dividing the second term by \\(\\beta\\), we transform the maximization problem into a minimization problem with a positive weight on the reward term.\n\nI think what is implied in the above explanation is that you are allowed to divide the optimization objective by a constant (in this case, \\(\\beta\\)).\n\nNext in the paper is Step 4:\n\\[\\min_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\mathbb{E}_{y\\sim\\pi(y|x)}\\big[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)\\big]\\]\nWhere:\n\\[Z(x) = \\sum_{y}\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))\\]\nis the “partition function”.\nChatGPT didn’t have a very helpful explanation (even after multiple prompts) except for the following explanation of why \\(Z(x)\\) is needed:\n\nWe introduce a normalization constant \\(Z(x)\\) inside the logarithm. This constant ensures that the expression inside the logarithm integrates to 1, effectively making it a probability distribution.\n\nI’ll try to connect the dots myself.\nThis:\n\\[\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)} - \\frac{1}{\\beta}r(x,y)\\]\n\nShould be shown to be equal to this:\n\\[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)\\]\nI’ll start with the “normalized” version:\n\\[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)\\]\n\nrewrite the denominator:\n\\[\\log\\frac{\\pi(y|x)}{\\frac{\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}{Z(x)}} - \\log Z(x)\\]\n\nApply the property of logarithm \\(\\log(\\frac{a}{b}) = \\log(a) - \\log(b)\\) to the first term:\n\n\\[\\log\\pi(y|x) - \\log\\frac{\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}{Z(x)} - \\log Z(x)\\]\n\nApply that property to the second term (putting it inside brackets to maintain the minus sign before it):\n\\[\\log\\pi(y|x) - \\big[\\log\\big( \\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))\\big) - \\log Z(x) \\big] - \\log Z(x)\\]\n\nMinus a negative becomes a plus:\n\\[\\log\\pi(y|x) - \\log \\big(\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))\\big) + \\log Z(x) - \\log Z(x)\\]\n\nThe \\(\\log Z(x)\\)’s cancel out:\n\\[\\log\\pi(y|x) - \\log \\big(\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))\\big)\\]\n\nApplying the property of logarithms \\(\\log(ab) = \\log(a) + \\log(b)\\):\n\\[\\log\\pi(y|x) - \\big[\\log \\pi_{ref}(y|x) + \\log\\exp(\\frac{1}{\\beta}r(x,y))\\big]\\]\n\nMinus a positive stays a minus:\n\\[\\log\\pi(y|x) - \\log \\pi_{ref}(y|x) - \\log\\exp(\\frac{1}{\\beta}r(x,y))\\]\n\nGiven that \\(\\log(\\exp(x)) = x\\):\n\\[\\log\\pi(y|x) - \\log \\pi_{ref}(y|x) - \\frac{1}{\\beta}r(x,y)\\]\n\nApplying the property that \\(\\log(a) - \\log(b) = \\log(\\frac{a}{b})\\):\n\\[\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)} - \\frac{1}{\\beta}r(x,y) \\]\nWhich is the original expression.\nNext in the paper they define a “valid probability distribution” as:\n\\[\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)\\]\n\nThey say it’s valid because \\(\\pi^*(y|x) \\ge 0\\) for all \\(y\\) and \\(\\sum_{y}\\pi^*(y|x) = 1\\). They don’t provide much detail. I’ll rewrite that expression with the full form of \\(Z(x)\\):\n\\[\\pi^*(y|x) = \\frac{\\pi_{ref}(y|x)\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)}{Z(x)}\\]\n\n\\[\\pi^*(y|x) = \\frac{\\pi_{ref}(y|x)\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)}{\\sum_{y}\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}\\]\n\nThat looks like the softmax function, which is a valid probability distribution so I’m going with that understanding for now.\nNext in the paper they rewrite the optimization objective with this \\(\\pi^*\\) expression:\n\\[\\min_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\big[\\mathbb{E}_{y\\sim\\pi(y|x)} \\big[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\big] -\\log Z(x) \\big]\\]\n\nthe first term inside the first set of brackets is KL-divergence:\n\\[\\min_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\big[\\mathbb{D}_{KL} \\big(\\pi(y|x) \\; || \\; \\pi^*(y|x)\\big) -\\log Z(x) \\big]\\]\n\n\\(Z(x)\\) does not depend on \\(y\\) so minimizing the objective is dependent only on minimizing the KL-divergence term, which is minimized to 0 when the two probability distributions are identical:\n\\[\\pi(y|x) = \\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)\\]\n\nI’ll rewrite that with the full form of \\(Z(x)\\):\n\\[\\pi(y|x) = \\pi^*(y|x) = \\frac{\\pi_{ref}(y|x)\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)}{\\sum_{y}\\pi_{ref}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}\\]"
  },
  {
    "objectID": "posts/2024-04-05-dpo/index.html#appendix-a.2-deriving-the-dpo-objective-under-the-bradley-terry-model",
    "href": "posts/2024-04-05-dpo/index.html#appendix-a.2-deriving-the-dpo-objective-under-the-bradley-terry-model",
    "title": "Paper Math: DPO (Direct Preference Optimization)",
    "section": "Appendix A.2 Deriving the DPO Objective Under the Bradley-Terry Model",
    "text": "Appendix A.2 Deriving the DPO Objective Under the Bradley-Terry Model\nBradley-Terry preference model:\n\\[p^*(y_1 \\gt y_2) = \\frac{\\exp(r^*(x,y_1))}{\\exp(r^*(x,y_1)) + \\exp(r^*(x,y_2))}\\]\n\nThe unavailable ground-truth reward expressed through its optimal policy:\n\\[r^*(x,y)=\\beta\\log\\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta\\log Z(x)\\]\n\nIn section 4 they say that they derive this expression of reward from the following:\n\\[\\pi_r(y|x) = \\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)\\]\n\nBy taking the logarithm of each side:\n\\[\\log\\pi_r(y|x) = \\log\\big(\\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)\\big)\\]\n\nApplying the property \\(\\log(ab) = \\log(a) + \\log(b)\\) to the right hand side:\n\\[\\log\\pi_r(y|x) = \\log\\big(\\frac{1}{Z(x)}\\pi_{ref}(y|x)\\big) + \\log\\big(\\exp\\big(\\frac{1}{\\beta}r(x,y)\\big)\\big)\\]\n\nApplying the property \\(\\log(\\exp(x)) = x\\) to the second term on the right hand side:\n\\[\\log\\pi_r(y|x) = \\log\\big(\\frac{1}{Z(x)}\\pi_{ref}(y|x)\\big) + \\frac{1}{\\beta}r(x,y)\\]\n\nApplying the property \\(\\log(\\frac{a}{b}) = \\log(a) - \\log(b)\\) to the first term on the right hand side:\n\\[\\log\\pi_r(y|x) = \\log\\pi_{ref}(y|x) - \\log Z(x) + \\frac{1}{\\beta}r(x,y)\\]\n\nIsolating \\(r(x,y)\\):\n\\[\\log\\pi_r(y|x) - \\log\\pi_{ref}(y|x) + \\log Z(x) = \\frac{1}{\\beta}r(x,y)\\]\n\nRewriting the left hand side using logarithm property:\n\\[\\log\\frac{\\pi_r(y|x)}{\\pi_{ref}(y|x)} + \\log Z(x) = \\frac{1}{\\beta}r(x,y)\\]\n\nMultiplying both sides by \\(\\beta\\):\n\\[\\beta\\log\\frac{\\pi_r(y|x)}{\\pi_{ref}(y|x)} + \\beta\\log Z(x) = r(x,y)\\]\nWhich is the final result.\n\nIn the paper they then substitute that for \\(r^*(x,y)\\) in the Bradley-Terry preference model expression:\n\\[p^*(y_1 \\gt y_2|x) = \\frac{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)} + \\beta\\log Z(x)\\big)}{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)} + \\beta\\log Z(x)\\big) + \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)} + \\beta\\log Z(x)\\big)}\\]\nThey arrive at the following form without intermediate steps:\n\\[\\frac{1}{1 + \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)} - \\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)}\\]\nUsing ChatGPT (its response was not fully clear so I’m writing it out here with my two cents):\n\nDistribute the exponentials in the numerator and denominator using the property \\(\\exp(a + b) = \\exp(a)\\exp(b)\\)\n\n\\[\\frac{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)} \\big)\\exp \\big(\\beta\\log Z(x)\\big)}{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)\\exp\\big(\\beta\\log Z(x)\\big) + \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)}\\big)\\big(\\beta\\log Z(x)\\big)}\\]\n\n\nCancel out the common term \\(\\exp(\\beta\\log Z(x))\\)\n\n\\[\\frac{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)} \\big)}{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big) + \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)}\\big)}\\]\nThis resembles the softmax function.\n\nDivide numerator and denominator by \\(\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)\\)\n\n\\[\\frac{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)} \\big) \\div \\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)}{\\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big) \\div \\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big) + \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)}\\big) \\div \\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)}\\]\n\n\nNumerator and first term in denominator cancel out to equal 1\n\n\\[\\frac{1}{1+ \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)}\\big) \\div \\exp\\big(\\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)}\\]\n\n\nUse the property \\(\\exp(a) \\div \\exp(b) = \\exp(a - b)\\)\n\n\\[\\frac{1}{1+ \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)} - \\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)}\\]\n\nThis is in the form of sigmoid: \\(\\sigma(a) = \\frac{1}{1 + e^{-a}}\\)\nWhere \\(a\\) in this case is the full term inside \\(\\exp()\\)\n\n\nRewrite it as sigmoid function\n\n\\[\\frac{1}{1+ \\exp\\big(\\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)} - \\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)}=\\sigma \\big( \\beta\\log\\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)} - \\beta\\log\\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)}\\big)\\]\n\nThis is the per-instance loss in the following loss function (Equation 7 in the paper):\n\n\\[\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\log\\sigma\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\big]\\]"
  },
  {
    "objectID": "posts/2024-04-05-dpo/index.html#appendix-a.4-deriving-the-gradient-of-the-dpo-objective",
    "href": "posts/2024-04-05-dpo/index.html#appendix-a.4-deriving-the-gradient-of-the-dpo-objective",
    "title": "Paper Math: DPO (Direct Preference Optimization)",
    "section": "Appendix A.4 Deriving the Gradient of the DPO Objective",
    "text": "Appendix A.4 Deriving the Gradient of the DPO Objective\nFor the gradient of the DPO objective—I’m unclear why in this section it’s written with the \\(y_l\\) term as the first term, minus the \\(y_w\\) term whereas in Equation 7 it’s written as the \\(y_w\\) term first, minus the \\(y_l\\) term—I’m going to go with what they have written in the Equation 7. Proceed with caution as this deviates from the appendix.\n\\[\\nabla_\\theta\\mathcal{L}_{DPO}(\\pi_\\theta;\\pi_{ref}) = -\\nabla_\\theta\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\log\\sigma\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\big]\\]\n\nThey rewrite this as:\n\\[\\nabla_\\theta\\mathcal{L}_{DPO}(\\pi_\\theta;\\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[ \\frac{\\sigma'(u)}{\\sigma(u)}\\nabla_\\theta(u)\\big]\\]\nWhere:\n\\[u = \\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\]\nNote: their \\(u\\) is flipped but I’m following Equation 7.\nUsing ChatGPT (simplifying its response), the following\n\\[\\log\\sigma\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\]\nis rewritten as:\n\\[\\log \\sigma(u)\\]\n\\(u\\) is a function of \\(\\theta\\) so taking the derivative of \\(\\log \\sigma(u)\\) with respect to \\(\\theta\\) requires the chain rule:\n\n\\[\\frac{d}{d\\theta}\\log(\\sigma(u)) = \\frac{du}{d\\theta} \\times \\frac{d\\sigma}{du} \\times \\frac{d}{d\\sigma}\\log(\\sigma(u))\\]\nWorking right to left:\n\\[\\frac{d}{d\\sigma(u)}\\log\\sigma(u) = \\frac{1}{\\sigma(u)}\\]\n\n\\[\\frac{d\\sigma}{du} = \\sigma(u) \\times (1 - \\sigma(u))\\]\n\nMultiplying those two together, the \\(\\sigma(u)\\) terms cancel out:\n\\[\\frac{1}{\\sigma(u)} \\times \\sigma(u) \\times (1 - \\sigma(u)) = 1 - \\sigma(u)\\]\n\nUsing the property of sigmoid that \\(\\sigma(-u) = 1 - \\sigma(u)\\) I’ll rewrite the gradient using that:\n\n\\[\\nabla_\\theta\\mathcal{L}_{DPO}(\\pi_\\theta;\\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[ \\frac{\\sigma'(u)}{\\sigma(u)}\\nabla_\\theta(u)\\big] = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[ (1-\\sigma(u))\\nabla_\\theta(u)\\big] = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[ \\sigma(-u)\\nabla_\\theta(u)\\big]\\]\n\nPlugging Equation 7 version of the full form of \\(u\\) back in:\n\n\\[-\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[ \\sigma\\big( \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} - \\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} \\big)\\nabla_\\theta\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\big]\\]\n\nI’ll look at just the \\(\\nabla_\\theta\\) term:\n\\[\\nabla_\\theta\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\]\n\nI’ll use logarithm properties to expand the logarithms:\n\n\\[\\beta\\nabla_\\theta\\big(\\log\\pi_\\theta(y_w|x) - \\log\\pi_{ref}(y_w|x) - \\log\\pi_\\theta(y_l|x) + \\log\\pi_{ref}(y_l|x)\\big)\\]\n\nI’m pretty certain \\(\\pi_{ref}\\) is not a function of \\(\\theta\\) (i.e. it’s not being trained) so the gradient of that with respect to \\(\\theta\\) is 0, so we’re left with:\n\n\\[\\beta\\nabla_\\theta\\big(\\log\\pi_\\theta(y_w|x) - \\log\\pi_\\theta(y_l|x)\\big)\\]\n\nWhich gives the final form of the gradient:\n\\[-\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[ \\beta\\sigma\\big( \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} - \\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} \\big)\\big[\\nabla_\\theta\\log\\pi_\\theta(y_w|x) - \\nabla_\\theta\\log\\pi_\\theta(y_l|x)\\big)\\big]\\big]\\]\n\nI’m not sure why in their final form of the gradient, instead of \\(\\pi_\\theta\\) they have just \\(\\pi\\).\n\nSubstituting \\(\\hat{r}(x,y) = \\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\) and pulling out the \\(\\beta\\)\n\n\\[\\nabla_\\theta\\mathcal{L}_{DPO}(\\pi_\\theta;\\pi_{ref}) = -\\beta\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\sigma\\big( \\hat{r}(x,y_l) - \\hat{r}(x,y_w) \\big)\\big[\\nabla_\\theta\\log\\pi_\\theta(y_w|x) - \\nabla_\\theta\\log\\pi_\\theta(y_l|x)\\big)\\big]\\big]\\]\n\nWhich is the same as the equation in section 4, page 5 “What does the DPO update do?”."
  },
  {
    "objectID": "posts/2024-04-05-dpo/index.html#appendix-b-dpo-implementation-details-and-hyperparameters",
    "href": "posts/2024-04-05-dpo/index.html#appendix-b-dpo-implementation-details-and-hyperparameters",
    "title": "Paper Math: DPO (Direct Preference Optimization)",
    "section": "Appendix B: DPO Implementation Details and Hyperparameters",
    "text": "Appendix B: DPO Implementation Details and Hyperparameters\nHere is the formula (Equation 7):\n\\[\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\log\\sigma\\big(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)\\big]\\]\n\nIf I expand the log terms inside the sigmoid function:\n\\[\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\log\\sigma\\big(\\beta\\log\\pi_\\theta(y_w|x)- \\beta\\log\\pi_{ref}(y_w|x) - \\beta\\log\\pi_\\theta(y_l|x) + \\beta\\log\\pi_{ref}(y_l|x)\\big]\\]\n\nRearranging terms to combine \\(\\pi_\\theta\\) and \\(\\pi_{ref}\\) values:\n\\[\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}}\\big[\\log\\sigma\\big(\\beta\\log\\pi_\\theta(y_w|x)- \\beta\\log\\pi_\\theta(y_l|x) + \\beta\\log\\pi_{ref}(y_l|x) - \\beta\\log\\pi_{ref}(y_w|x)\\big]\\]\n\nNote that for policy model (\\(\\pi_\\theta\\)) the log probabilities of the rejected responses are subtracted from the chosen responses (i.e. chosen - rejected):\n\\[\\beta\\log\\pi_\\theta(y_w|x)- \\beta\\log\\pi_\\theta(y_l|x)\\]\n\nBut for the reference model (\\(\\pi_{ref}\\)) the log probabilities of the chosen responses are subtracted from the reject responses (i.e. rejected - chosen):\n\\[\\beta\\log\\pi_{ref}(y_l|x) - \\beta\\log\\pi_{ref}(y_w|x)\\]\n\nHere is the implementation code\ndef dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):\n  \"\"\"\n  pi_logps: policy logprobs, shape (B,)\n  ref_logps: reference model logprobs, shape (B,)\n  yw_idxs: preferred completion indices in [0, B-1], shape (T,)\n  yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)\n  beta: temperature controlling strength of KL penalty\n  Each pair of (yw_idxs[i], yl_idxs[i]) represents the\n  indices of a single preference pair.\n  \"\"\"\n  pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]\n  ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]\n\n  pi_logratios = pi_yw_logps - pi_yl_logps\n  ref_logratios = ref_yw_logps - ref_yl_logps\n\n  losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))\n  rewards = beta * (pi_logps - ref_logps).detach()\n  return losses, rewards\npi_logratios is defined as the rejected responses subracted from the chosen responses (i.e. chosen - rejected):\npi_logratios = pi_yw_logps - pi_yl_logps\n\nref_logratios is also defined the same way (chosen - rejected):\nref_logratios = ref_yw_logps - ref_yl_logps\nThen when calculating losses, ref_logratios is subtracted from pi_logratios, so the signs match:\nlosses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))\nis the same as\nlosses = -F.logsigmoid(beta * ((pi_yw_logps - pi_yl_logps) - (ref_yw_logps - ref_yl_logps))\nwhich is simplified to\nlosses = -F.logsigmoid(beta * (pi_yw_logps - pi_yl_logps - ref_yw_logps + ref_yl_logps))\nrearranging the terms to match the paper:\nlosses = -F.logsigmoid(beta * (pi_yw_logps - pi_yl_logps + ref_yl_logps - ref_yw_logps))\nThe TRL library has the same implementation:\npi_logratios = policy_chosen_logps - policy_rejected_logps\nref_logratios = reference_chosen_logps - reference_rejected_logps\nlogits = pi_logratios - ref_logratios"
  },
  {
    "objectID": "posts/2024-06-24-cv-algos/index.html",
    "href": "posts/2024-06-24-cv-algos/index.html",
    "title": "Exploring Different Feature Detection Algorithms in Computer Vision",
    "section": "",
    "text": "In this notebook, I’ll work through the first “Further Research” prompt from Chapter 13 (Convolutional Neural Networks) in the fastai book.\n\nWhat features other than edge detectors have been used in computer vision (especially before deep learning became popular)?\n\nTo start this exploration, I prompted ChatGPT and got a list of feature detection algorithms. I’ll explore 7 of the algorithms that I could get to work and/or ran relatively quickly and/or ran without crashing my Google Colab kernel.\n\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\nfrom google.colab.patches import cv2_imshow\n\nfrom math import sqrt\n\nfrom skimage import data, exposure\nfrom skimage.feature import blob_dog, blob_log, blob_doh, hog, local_binary_pattern\nfrom skimage.color import rgb2gray, label2rgb\nfrom skimage.transform import rotate\n\nfrom sklearn import cluster, decomposition"
  },
  {
    "objectID": "posts/2024-06-24-cv-algos/index.html#corner-detectors",
    "href": "posts/2024-06-24-cv-algos/index.html#corner-detectors",
    "title": "Exploring Different Feature Detection Algorithms in Computer Vision",
    "section": "Corner Detectors",
    "text": "Corner Detectors\n\nHarris Corner Detection\nThe Harris Corner Detection algorithm was developed in 1988 by Chris Harris & Mike Stephens in their paper “A Combined Corner and Edge Detector”. I’ll use the example code given in the OpenCV documentation on this algorithm. I found the following image to help understand the concept of this algorithm:\n\n\n\n\n\nI created an image with simple shapes in Google Slides with varying rounded-ness of corners. I’ll wrap the OpenCV documentation code in a function so I can call it with different parameters to experiment.\n\ndef do_corner_harris(fname='/content/shapes1.png', blockSize=2, ksize=3, k=0.04):\n  img = cv.imread(fname)\n  gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\n  gray = np.float32(gray)\n  dst = cv.cornerHarris(gray,blockSize,ksize,k)\n\n  #result is dilated for marking the corners, not important\n  dst = cv.dilate(dst,None)\n\n  # Threshold for an optimal value, it may vary depending on the image.\n  img[dst>0.01*dst.max()]=[0,0,255]\n\n  cv2_imshow(img)\n\nUsing the default settings, the perfect square’s corners are detected. I asked the new Claude 3.5-Sonnet model how k affects the algorithm and it said that:\n\nThe value of k influences the trade-off between detecting true corners and rejecting edge-like features:\nIf k is too small: The algorithm becomes more sensitive to edges.\nIf k is too large: The algorithm may miss some corners.\n\n\ndo_corner_harris()\n\n\n\n\n\n\n\n\nIf I reduce the value of k to 0.005, it starts detecting corners in the rounded squares (which is fine) but also on the circle (which seems like noise).\n\ndo_corner_harris(k=0.005)\n\n\n\n\n\n\n\n\nI’ll go back to k=0.04 and change blockSize to see how it changes corner detection:\nA larger blockSize (20 instead of 2) starts to detect the rounded corners of the rounded square.\n\ndo_corner_harris(blockSize=20)\n\n\n\n\n\n\n\n\nDecreasing ksize (1 instead of the default 3) seems to increase the sensitivity of the corner detection, as the rounded squares’ corners are detected as well as points on the circle.\n\ndo_corner_harris(ksize=1)\n\n\n\n\n\n\n\n\n\n\nShi-Tomasi Corner Detector\nNext, I’ll experiment with the Shi-Tomasi Corner Detector, which is a modification of the Harris Corner Detection algorithm.\n\ndef do_shi_tomasi(maxCorners=25, qualityLevel=0.01, minDistance=10):\n  img = cv.imread('/content/shapes1.png')\n  gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\n  corners = cv.goodFeaturesToTrack(gray,maxCorners,qualityLevel,minDistance)\n  corners = np.int0(corners)\n\n  for i in corners:\n    x,y = i.ravel()\n    cv.circle(img,(x,y),3,255,-1)\n\n  plt.imshow(img),plt.show()\n\nThe documentation example settings in this case are more sensitive to corner-like regions, even detecting 8 points on the circle as corners.\n\ndo_shi_tomasi()\n\nDeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n  corners = np.int0(corners)\n\n\n\n\n\n\n\n\n\nThat might be because the number of corners it’s looking for is 25. Let’s see what happens if I reduce maxCorners to 5:\n\ndo_shi_tomasi(maxCorners=5)\n\nDeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n  corners = np.int0(corners)\n\n\n\n\n\n\n\n\n\nInterestingly, reducing maxCorners doesn’t avoid corner detection on the circle. I’ll increase the qualityLevel and see if that avoids detection of corners on the circle:\n\ndo_shi_tomasi(maxCorners=100, qualityLevel=0.04)\n\nDeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n  corners = np.int0(corners)\n\n\n\n\n\n\n\n\n\nAfter quadrupuling the qualityLevel, even if maxCorners is 100, the corners on the rounded squares are not detected. Let’s see how changing minDistance (from 10 to 100) affects corner detection:\n\ndo_shi_tomasi(minDistance=100)\n\nDeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n  corners = np.int0(corners)\n\n\n\n\n\n\n\n\n\nFewer corners are detected because of the larger minimum distance required, but the quality of corners detected doesn’t improve."
  },
  {
    "objectID": "posts/2024-06-24-cv-algos/index.html#blob-detectors",
    "href": "posts/2024-06-24-cv-algos/index.html#blob-detectors",
    "title": "Exploring Different Feature Detection Algorithms in Computer Vision",
    "section": "Blob Detectors",
    "text": "Blob Detectors\nI’ll be running the same code as the scikit-image documentation with different images, going from slowest and most accurate (Laplacian of Gaussian) to fastest (Determinant of Hessian).\n\nLaplacian of Gaussian\n\ndef do_blob_log(fname='/content/shapes1.png', max_sigma=30, num_sigma=10, threshold=0.1):\n  image = cv.imread(fname)\n  image_gray = rgb2gray(image)\n\n  blobs_log = blob_log(image_gray, max_sigma=max_sigma, num_sigma=num_sigma, threshold=threshold)\n\n  # Compute radii in the 3rd column.\n  blobs_log[:, 2] = blobs_log[:, 2] * sqrt(2)\n  fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n\n  ax.set_title(f'Laplacian of Gaussian: {len(blobs_log)} blobs')\n  ax.imshow(image)\n\n  for blob in blobs_log:\n    y, x, r = blob\n    c = plt.Circle((x, y), r, color='red', linewidth=2, fill=False)\n    ax.add_patch(c)\n\n  ax.set_axis_off()\n\n  plt.tight_layout()\n  plt.show()\n\n\ndo_blob_log()\n\n\n\n\n\n\n\n\nThat took about 10 seconds to run and did not result in what I was expecting—it seems to be identifying the white space as blobs? I’m not sure how to interpret this result. I’ll change the max_sigma value and see what that does.\nA very small max_sigma results in many smaller blobs around the edges of the shapes. A large max_sigma results in fewer, much larger blobs.\n\ndo_blob_log(max_sigma=1)\n\n\n\n\n\n\n\n\n\ndo_blob_log(max_sigma=100)\n\n\n\n\n\n\n\n\nChanging num_sigma next (from 10 to 1 and 100):\nA smaller num_sigma has a similar effect to a small max_sigma (many small blobs).\n\ndo_blob_log(num_sigma=1)\n\n\n\n\n\n\n\n\nA larger num_sigma results in fewer blobs but of greatly varying sizes.\n\ndo_blob_log(num_sigma=100)\n\n\n\n\n\n\n\n\n\ndo_blob_log(threshold=0.01)\n\n\n\n\n\n\n\n\nAs threshold gets larger, the number of blobs decreases significantly.\n\ndo_blob_log(threshold=0.4)\n\n\n\n\n\n\n\n\n\n\nDifference of Gaussian\n\ndef do_blob_dog(fname='/content/shapes1.png', max_sigma=30, threshold=0.1):\n  image = cv.imread(fname)\n  image_gray = rgb2gray(image)\n\n  blobs_dog = blob_dog(image_gray, max_sigma=max_sigma, threshold=threshold)\n  blobs_dog[:, 2] = blobs_dog[:, 2] * sqrt(2)\n\n  fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n\n  ax.set_title(f'Difference of Gaussian: {len(blobs_dog)} blobs')\n  ax.imshow(image)\n\n  for blob in blobs_dog:\n    y, x, r = blob\n    c = plt.Circle((x, y), r, color='red', linewidth=2, fill=False)\n    ax.add_patch(c)\n\n  ax.set_axis_off()\n\n  plt.tight_layout()\n  plt.show()\n\nI still don’t understand why it’s focused on detecting the white space and not the black objects.\n\ndo_blob_dog()\n\n\n\n\n\n\n\n\nI wonder if I reverse the colors in the image (manually, in Google Slides) would that change the results?\n\ndo_blob_dog(fname='/content/shapes3.png')\n\n\n\n\n\n\n\n\nYup—it’s now focused on the bright (white) objects compared to the dark (black) background. It’s still doing a terrible job at detecting the shapes as it has way too many blobs.\nA larger max_sigma (100 instead of 30) doesn’t improve the results (we get only marginally closer to 4 total blobs).\n\ndo_blob_dog(fname='/content/shapes3.png', max_sigma=100)\n\n\n\n\n\n\n\n\nA larger threshold value (0.32 instead of 0.1) captures one blog per small shape but doesn’t detect the largest square as a single blob (instead, it captures each corner as its own blob).\n\ndo_blob_dog(fname='/content/shapes3.png', threshold=0.32)\n\n\n\n\n\n\n\n\nIncreasing max_sigma with a larger threshold gets the desired result—all four shapes are detected with one blob each.\n\ndo_blob_dog(fname='/content/shapes3.png', max_sigma=100, threshold=0.35)\n\n\n\n\n\n\n\n\nI’ll retry Laplacian of Gaussian on this new image with white shapes on a black background.\nLaplacian of Gaussian requires a slightly larger threshold of 0.45 to get the same results. Increasing max_sigma (with num_sigma at 10 or 3 and threshold=0.35) picks up additional noise (four corners of the square).\n\ndo_blob_log(fname='/content/shapes3.png', max_sigma=100, threshold=0.45)\n\n\n\n\n\n\n\n\n\ndo_blob_log(fname='/content/shapes3.png', max_sigma=300, threshold=0.35)\n\n\n\n\n\n\n\n\n\ndo_blob_log(fname='/content/shapes3.png', max_sigma=100, num_sigma=3, threshold=0.35)\n\n\n\n\n\n\n\n\n\n\nDeterminant of Hessian\n\ndef do_blob_doh(fname='/content/shapes3.png', max_sigma=30, threshold=0.01):\n  image = cv.imread(fname)\n  image_gray = rgb2gray(image)\n\n  blobs_doh = blob_doh(image_gray, max_sigma=max_sigma, threshold=threshold)\n\n  fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n\n  ax.set_title(f'Determinant of Hessian: {len(blobs_doh)} blobs')\n  ax.imshow(image)\n\n  for blob in blobs_doh:\n    y, x, r = blob\n    c = plt.Circle((x, y), r, color='red', linewidth=2, fill=False)\n    ax.add_patch(c)\n\n  ax.set_axis_off()\n\n  plt.tight_layout()\n  plt.show()\n\nThe documentation values of max_sigma=30 and threshold=0.01 result in 21 blobs for the Determinant of Hessian approach.\n\ndo_blob_doh()\n\n\n\n\n\n\n\n\nIncreasing max_sigma significantly gets the correct number of blobs (4) but they are erratically positioned and don’t match the target shapes.\n\ndo_blob_doh(max_sigma=200)\n\n\n\n\n\n\n\n\nI can’t quite find the right combination of parameters with the Determimant of Hessian approach. The best I can do (with the limited manual combinations I tried) is to get the right number of blobs (4) with the wrong placement.\n\ndo_blob_doh(threshold=0.023)\n\n\n\n\n\n\n\n\n\ndo_blob_doh(max_sigma=200, threshold=0.01)\n\n\n\n\n\n\n\n\n\ndo_blob_doh(max_sigma=175, threshold=0.025)"
  },
  {
    "objectID": "posts/2024-06-24-cv-algos/index.html#scale-invariant-feature-transform-sift",
    "href": "posts/2024-06-24-cv-algos/index.html#scale-invariant-feature-transform-sift",
    "title": "Exploring Different Feature Detection Algorithms in Computer Vision",
    "section": "Scale-Invariant Feature Transform (SIFT)",
    "text": "Scale-Invariant Feature Transform (SIFT)\nFrom the OpenCV docs:\n\nA corner may not be a corner if the image is scaled.\n\nIn the image below, I created a small rounded square at the top left. I copied that square and scaled it up a few times to get the “zoomed-in” corner on the bottom right.\nWhen using the Harris Corner detection algorithm with an increased blockSize the rounded corners in the small version and large version of the square are detected, however I’m not sure if such large blocks are recommended or ever used. There also seems to be some overlap between the bottom right corner of the small square and the top right corner of the large square.\n\ndo_corner_harris(fname='/content/shapes4.png', blockSize=80)\n\n\n\n\n\n\n\n\nIf I use SIFT with a larger edgeThreshold than the default, it seems to identify key points along the rounded corner of the large square, and the entire smaller square. I’m not entirely sure how to interpret these keypoints so I can’t say if it’s better or worse than the Harris Corner Detector.\n\ndef do_sift(fname='/content/shapes4.png', edgeThreshold=10):\n  img = cv.imread(fname)\n  gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\n  sift = cv.SIFT_create(edgeThreshold=edgeThreshold)\n  kp = sift.detect(gray, mask=None)\n\n  img=cv.drawKeypoints(gray,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\n  cv2_imshow(img)\n\n\ndo_sift()\n\n\n\n\n\n\n\n\n\ndo_sift(fname='/content/shapes3.png', edgeThreshold=50)\n\n\n\n\n\n\n\n\nWith a larger edgeThreshold than the default, SIFT is placing key points on all corners of the shapes in the image, including points on the circle."
  },
  {
    "objectID": "posts/2024-06-24-cv-algos/index.html#histogram-of-oriented-gradients-hog",
    "href": "posts/2024-06-24-cv-algos/index.html#histogram-of-oriented-gradients-hog",
    "title": "Exploring Different Feature Detection Algorithms in Computer Vision",
    "section": "Histogram of Oriented Gradients (HoG)",
    "text": "Histogram of Oriented Gradients (HoG)\n\ndef do_hog(fname='/content/shapes3.png'):\n  image = cv.imread(fname)\n\n  fd, hog_image = hog(\n      image,\n      orientations=8,\n      pixels_per_cell=(16, 16),\n      cells_per_block=(1, 1),\n      visualize=True,\n      channel_axis=-1,\n  )\n\n  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n\n  ax1.axis('off')\n  ax1.imshow(image, cmap=plt.cm.gray)\n  ax1.set_title('Input image')\n\n  # Rescale histogram for better display\n  hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n\n  ax2.axis('off')\n  ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n  ax2.set_title('Histogram of Oriented Gradients')\n  plt.show()\n\nThe Histogram of Gradients algorithm generally seems to find the gradient at each point of the shape’s boundary.\n\ndo_hog()\n\n\n\n\n\n\n\n\nI’ll give it a much simpler image (straight lines) to see how it transforms that:\n\ndo_hog(fname='/content/shapes5.png')\n\n\n\n\n\n\n\n\nIt’s interesting to note that on the diagonal lines it has “thicker” gradients than the vertical and horizontal lines."
  },
  {
    "objectID": "posts/2024-06-24-cv-algos/index.html#final-thoughts",
    "href": "posts/2024-06-24-cv-algos/index.html#final-thoughts",
    "title": "Exploring Different Feature Detection Algorithms in Computer Vision",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nComputer Vision is obviously a vast field with a significant history of research and applications. I’m not very familiar with this field outside of what I’ve learned and applied from the fastai course and Kaggle competitions about image recognition, so this was a helpful exercise to slightly expand my understanding of the CV universe.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "",
    "text": "Show pip installs\n!pip install transformers -Uqq\n!pip install accelerate -qq\n!pip install torch==2.2.2 -qq\n!pip install datasets~=2.16.1 -qq\n!pip install scikit-learn==1.2 -qq"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#background",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#background",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Background",
    "text": "Background\nIn this notebook I’ll use Qwen2-1.5B-Instruct to classify sentiment in the financial_phrasebank dataset. In previous notebooks I have performed sentiment classification with phi-2, phi-3, phi-3.5, and the Claude series.\nThis notebook is part of a series of blog posts for a project I’m working called TinySentiment where I’m experimenting with tiny models to improve their ability to classify sentiment in the financial_phrasebank dataset. I was inspired to do so after reading this blog post and this corresponding notebook by Moritz Laurer as part of a fastai study group last year.\nHere are the results from my experiments so far (**the best-performing prompt from this notebook):\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nphi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nphi-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\n**Qwen2-1.5B\n27-Shot\n86.10%\n90% (264/294)\n95.5% (1320/1382)\n61% (342/561)\n\n\n\nHere are the results from this notebook. The best-performing prompt was a randomly shuffled 27-Shot prompt (Prompt AD), yielding an overall accuracy of 86.10%.\n\n\n\nprompt\nstrategy\naccuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n81.76%\n97% (293/303)\n85% (1185/1391)\n65% (373/570)\n\n\nB\n0-Shot\n51.86%\n99% (300/303)\n61% (846/1391)\n5% (28/570)\n\n\nC\n0-Shot\n81.40%\n93% (283/303)\n96% (1330/1391)\n40% (230/570)\n\n\nD\n0-Shot\n78.53%\n92% (279/303)\n92% (1281/1391)\n38% (218/570)\n\n\nE\n0-Shot\n66.21%\n100% (302/303)\n82% (1145/1391)\n9% (52/570)\n\n\nF\n0-Shot\n78.05%\n88% (267/303)\n97% (1355/1391)\n25% (145/570)\n\n\nG\n0-Shot\n66.70%\n94% (285/303)\n80% (1107/1391)\n21% (118/570)\n\n\nH\n0-Shot\n70.89%\n85% (259/303)\n90% (1247/1391)\n17% (99/570)\n\n\nI\n0-Shot\n69.17%\n58% (176/303)\n86% (1201/1391)\n33% (189/570)\n\n\nJ\n0-Shot\n57.38%\n47% (142/303)\n78% (1086/1391)\n12% (71/570)\n\n\nK\n0-Shot\n41.87%\n34% (102/303)\n52% (728/1391)\n21% (118/570)\n\n\nL\n0-Shot\n42.84%\n66% (200/303)\n45% (629/1391)\n25% (141/570)\n\n\nM\n0-Shot\n51.46%\n26% (79/303)\n77% (1078/1391)\n1% (8/570)\n\n\nN\n0-Shot\n29.77%\n11% (33/303)\n44% (608/1391)\n6% (33/570)\n\n\nO\n0-Shot\n61.00%\n37% (113/303)\n90% (1257/1391)\n2% (11/570)\n\n\nP\n3-Shot\n78.20%\n91% (275/302)\n91% (1266/1390)\n40% (227/569)\n\n\nQ\n6-Shot\n76.93%\n96% (289/302)\n73% (1010/1387)\n77% (438/569)\n\n\nR\n20-Shot\n81.42%\n92% (274/299)\n94% (1301/1379)\n45% (252/566)\n\n\nS\n30-Shot\n81.51%\n87% (255/294)\n98% (1345/1379)\n39% (221/561)\n\n\nT\n27-Shot\n83.73%\n93% (272/294)\n94.3% (1303/1382)\n53% (298/561)\n\n\nU\n25-Shot\n82.94%\n90% (266/294)\n96.2% (1331/1384)\n46% (260/561)\n\n\nV\n21-Shot\n83.28%\n92% (273/296)\n94.9% (1314/1384)\n50% (281/563)\n\n\nW\n15-Shot\n81.55%\n94% (279/298)\n87.7% (1215/1386)\n60% (340/565)\n\n\nX\n30-Shot\n81.74%\n89% (261/293)\n96.7% (1336/1381)\n41% (229/560)\n\n\nY\n60-Shot\n75.82%\n66% (186/283)\n99.8% (1368/1371)\n21% (117/550)\n\n\nZ\n27-Shot\n81.36%\n80% (236/294)\n99.4% (1374/1382)\n37% (210/561)\n\n\nAA\n23-Shot\n82.95%\n93% (276/296)\n94.9% (1314/1384)\n48% (269/561)\n\n\nAB\n25-Shot\n83.70%\n92% (270/294)\n95.3% (1317/1382)\n51% (287/563)\n\n\nAC\n23-Shot\n83.40%\n95% (278/294)\n93.8% (1296/1382)\n52% (295/565)\n\n\nAD\n27-Shot\n86.10%\n90% (264/294)\n95.5% (1320/1382)\n61% (342/561)\n\n\nAE\n60-Shot\n83.71%\n83% (234/283)\n97.8% (1341/1371)\n49% (270/550)\n\n\nAF\n15-Shot\n82.00%\n91% (272/298)\n88.8% (1231/1386)\n60% (341/565)\n\n\nAG\n27-Shot w/System Prompt\n84.40%\n84% (248/294)\n97.8% (1351/1382)\n52% (289/561)\n\n\nAH\n27-Shot w/System Prompt\n84.67%\n87% (256/294)\n97.1% (1342/1382)\n53% (296/561)\n\n\nAI\n27-Shot w/System Prompt\n84.99%\n88% (260/294)\n97.3% (1345/1382)\n50% (283/561)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-a",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-a",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt A",
    "text": "Prompt A\nI’ll start out with a simple instruction.\n\npromptA = \"\"\"Label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\nprint(promptA)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\n\n\n\nformatted_prompt = promptA.format(text=dataset[0]['sentence'])\nprint(formatted_prompt)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nLooks good! I’m able to generate a response.\n\ngenerate_response(formatted_prompt)\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\n'neutral'\n\n\nAt ~50ms per prompt, it would take about 2 minutes to run inference on the full 2264 item dataset.\n\n%timeit -n 10 generate_response(formatted_prompt)\n\n45.4 ms ± 829 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndf, acc = generate_responses(dataset, promptA)\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nRight off the bat, Qwen2-1.5B-Instruct has decent performance on sentiment classification. 81.85% is not bad at all! Let’s see if I can improve on that.\n\nacc\n\n0.8175795053003534\n\n\nInteresting to note: Qwen2-1.5B-Instruct does not classify any other values than negative, neutral and positive.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_A.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-b",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-b",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt B",
    "text": "Prompt B\nI’ll repeat the instruction after providing the sentence, as this has usually improved performance.\n\npromptB = \"\"\"Instruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\nlabel the TEXT with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptB)\n\n\n\n\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nlabel the TEXT with a single word: negative, positive, or neutral\n\n\nInteresting! Repeating the instruction usually increases the accuracy. Here, it drops by about 30%!\n\nacc\n\n0.5185512367491166\n\n\nThe model actually gets better at negative classification but significantly plummets for the other two.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_B.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-c",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-c",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt C",
    "text": "Prompt C\nI’ll return to Prompt A and change the format a bit.\n\npromptC = \"\"\"Respond with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptC)\n\n\n\n\nRespond with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThis yields a similar accuracy to Prompt A.\n\nacc\n\n0.8140459363957597\n\n\nThe model does quite well with neutral and negative sentences but terribly with positive sentences. In fact, Qwen2-1.5B-Instruct (96%) beats the best-performing phi-3 prompt (94%) in neutral True Positive Rate (TPR).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_C.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-d",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-d",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt D",
    "text": "Prompt D\nI’ll add a period after the instruction.\n\npromptD = \"\"\"Respond with a single word: negative, positive, or neutral.\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptD)\n\n\n\n\nRespond with a single word: negative, positive, or neutral.\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nNope! Period == bad.\n\nacc\n\n0.7853356890459364\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_D.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-e",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-e",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt E",
    "text": "Prompt E\nI’ll repeat the instruction to see if it improves performance.\n\npromptE = \"\"\"Respond with a single word: negative, positive, or neutral\nTEXT: {text}\nRespond with a single word: negative, positive, or neutral\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptE)\n\n\n\n\nRespond with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nRespond with a single word: negative, positive, or neutral\n\n\nNope! The repeated instruction decreases the accuracy.\n\nacc\n\n0.6621024734982333\n\n\nNote that the negative sentiment TPR is near perfect.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_E.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-f",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-f",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt F",
    "text": "Prompt F\nI realized I didn’t have “Instruct:” at the start of the prompt so I’ll add that to Prompt C.\n\npromptF = \"\"\"Instruct: Respond with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptF)\n\n\n\n\nInstruct: Respond with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nThat decreases the accuracy by 3%.\n\nacc\n\n0.7804770318021201\n\n\nNote that this prompt yields the best neutral TPR so far (1355/1391 = 97.4%) for this model.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_F.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-g",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-g",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt G",
    "text": "Prompt G\nI’ll now add some additional instructions.\n\npromptG = \"\"\"Your task is to analyze the sentiment (from an investor's perspective) of the text below.\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: {text}\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptG)\n\n\n\n\nYour task is to analyze the sentiment (from an investor's perspective) of the text below.\nInstruct: label the following TEXT with a single word: negative, positive, or neutral\nTEXT: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n\n\nAdditional instruction worsens the model’s performance by ~15%.\n\nacc\n\n0.6669611307420494\n\n\nNothing immediately notable about the results.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_G.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-h",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-h",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt H",
    "text": "Prompt H\nI’ll now try a series of prompts suggested by Claude (based on the previous prompts and accuracies achieved). These prompts are simple and concise.\n\npromptH = \"\"\"Sentiment: {text}\nOutput: [negative/positive/neutral]\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptH)\n\n\n\n\nSentiment: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nOutput: [negative/positive/neutral]\n\n\nI’m surprised that such a simple prompt performs well, though it’s 11% worse than my best prompts so far.\n\nacc\n\n0.708922261484099\n\n\nWhat’s notable about this prompt’s results is the incredibly bad performance on positive sentiment (18%). Also noteworthy is that with this prompt there are now other responses than negative, neutral or positive.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_H.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-i",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-i",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt I",
    "text": "Prompt I\nAnother prompt suggested by Claude.\n\npromptI = \"\"\"Classify sentiment: {text}\nAnswer: [negative/positive/neutral]\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptI)\n\n\n\n\nClassify sentiment: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nAnswer: [negative/positive/neutral]\n\n\nA similar performance to Prompt H.\n\nacc\n\n0.691696113074205\n\n\nThis prompt performs worse for all three sentiments, but especially negative and positive.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_I.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-j",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-j",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt J",
    "text": "Prompt J\n\npromptJ = \"\"\"Text: {text}\nSentiment classification: [negative/positive/neutral]\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptJ)\n\n\n\n\nText: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nSentiment classification: [negative/positive/neutral]\n\n\nThis prompt yields a significantly worse performance.\n\nacc\n\n0.5737632508833922\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_J.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-k",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-k",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt K",
    "text": "Prompt K\nAnother Claude suggested prompt:\n\npromptK = \"\"\"Analyze the sentiment of this text:\n{text}\nClassification: [negative/positive/neutral]\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptK)\n\n\n\n\nAnalyze the sentiment of this text:\nAccording to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nClassification: [negative/positive/neutral]\n\n\nThis prompt yields worse results.\n\nacc\n\n0.41872791519434627\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_K.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-l",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-l",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt L",
    "text": "Prompt L\nAnother Claude suggested prompt (so far, has not yielded good results!).\n\npromptL = \"\"\"Categorize the following text as negative, positive, or neutral:\n{text}\nCategory:\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptL)\n\n\n\n\nCategorize the following text as negative, positive, or neutral:\nAccording to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nCategory:\n\n\nAnother subpar result.\n\nacc\n\n0.4284452296819788\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_L.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-m",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-m",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt M",
    "text": "Prompt M\nAnother Claude suggested prompt (3 more left to try out!)\n\npromptM = \"\"\"Sentiment analysis task:\nInput: {text}\nOutput: [negative/positive/neutral]\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptM)\n\n\n\n\nSentiment analysis task:\nInput: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nOutput: [negative/positive/neutral]\n\n\nNope! No success with this one either (well, 51% success).\n\nacc\n\n0.5145759717314488\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_M.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-n",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-n",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt N",
    "text": "Prompt N\n\npromptN = \"\"\"determine the sentiment:\n{text}\nsentiment: [negative/positive/neutral]\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptN)\n\n\n\n\ndetermine the sentiment:\nAccording to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\nsentiment: [negative/positive/neutral]\n\n\nThis one was particularly bad.\n\nacc\n\n0.29770318021201414\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_N.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-o",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-o",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt O",
    "text": "Prompt O\n\npromptO = \"\"\"Sentiment classification task:\n'{text}'\nLabel: [negative/positive/neutral]\"\"\"\n\n\ndf, acc = generate_responses(dataset, promptO)\n\n\n\n\nSentiment classification task:\n'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'\nLabel: [negative/positive/neutral]\n\n\nLooks like I’ll stick with Prompt A for now!\n\nacc\n\n0.609982332155477\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_O.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-p",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-p",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt P",
    "text": "Prompt P\nI’ll revisit Prompt A and give it a few examples.\n\n\nShow updated few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    print(dataset[0]['prompt'])\n    \n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    for row in dataset:\n        messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n        generated_ids = model.generate(\n            model_inputs.input_ids,\n            max_new_tokens=2\n        )\n        generated_ids = [\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n        ]\n\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n        responses.append(response)\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    \n    return df, acc\n\n\n\nexclude_idxs = [0, 1, 292]\n\n\npromptP_ds = ds_subset(dataset, exclude_idxs)\npromptP_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2261\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nexamples\n\n[('According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .',\n  'neutral'),\n (\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n  'positive'),\n ('Jan. 6 -- Ford is struggling in the face of slowing truck and SUV sales and a surfeit of up-to-date , gotta-have cars .',\n  'negative')]\n\n\n\ndf, acc = few_shot_responses(promptP_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n3-Shot prompting does not improve performance.\n\nacc\n\n0.7819548872180451\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_P.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-q",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-q",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt Q",
    "text": "Prompt Q\nI’ll try 6-Shot prompting next.\n\nexclude_idxs=[0, 1, 292, 37, 38, 39]\npromptQ_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptQ_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2258\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n6\n\n\n\ndf, acc = few_shot_responses(promptQ_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n\n\n6-Shot doesn’t fare much better.\n\nacc\n\n0.7692648361381754\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_Q.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-r",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-r",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt R",
    "text": "Prompt R\nI’ll now bump it up to 20 examples.\n\nexclude_idxs = [\n    1, 2, 3, 4, # positive\n    292, 293, 294, 347, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283 # neutral\n]\n\n\npromptR_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptR_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2244\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n20\n\n\n\ndf, acc = few_shot_responses(promptR_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\n\n\nInterestingly, the 20-Shot prompt yields a worse accuracy than the 0-Shot prompt.\n\nacc\n\n0.8141711229946524\n\n\nCompared to 0-Shot Prompt A (85%), the 20-Shot prompt has a significantly higher TPR for neutral sentiment (94%) whereas negative and positive perform worse.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_R.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-s",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-s",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt S",
    "text": "Prompt S\nI’ll increase the number of examples to 30, but will only add negative and positive examples as the model’s performance was lacking for those sentiments.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 274, 283 # neutral\n]\n\n\npromptS_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptS_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n30\n\n\n\ndf, acc = few_shot_responses(promptS_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nIncreasing the number of examples and changing the proportions has not improved the overall accuracy.\n\nacc\n\n0.815129811996419\n\n\nInterestingly, the number of correctly predicted neutral sentences has increased with this prompt while the number of correctly predicted negative and positive sentences decreased.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_S.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-t",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-t",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt T",
    "text": "Prompt T\nAs the number of examples have increased, the model is classifying more and more negative and positive sentences as neutral. After chatting with Claude: I’ll start removing neutral sentences from the example to see if that reverses this trend.\nI’ll start by giving the model 9 examples per sentiment.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266 # neutral\n]\n\n\npromptT_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptT_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2237\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n27\n\n\n\ndf, acc = few_shot_responses(promptT_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nHooray! This strategy yielded results. The accuracy increases by a couple percent.\n\nacc\n\n0.8372820742065266\n\n\nThe neutral TPR has decreased, as expected, but negative and positive are now making a comeback.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_T.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-u",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-u",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt U",
    "text": "Prompt U\nI’ll continue to decrease the number of examples, this time by decreasing the number of neutral examples.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, # negative\n    0, 37, 38, 39, 40, 263, 264 # neutral\n]\n\n\npromptU_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptU_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2239\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n25\n\n\n\ndf, acc = few_shot_responses(promptU_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nDecreasing the number of neutral examples decreases the accuracy.\n\nacc\n\n0.829388119696293\n\n\nThe model is actually worse at predicting negative and positive sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_U.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-v",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-v",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt V",
    "text": "Prompt V\nI’ll continue decreasing examples, with equal amounts from each sentiment for a 21-Shot prompt.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, # positive\n    292, 293, 294, 347, 348, 349, 350,  # negative\n    0, 37, 38, 39, 40, 263, 264 # neutral\n]\n\n\npromptV_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptV_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2243\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n21\n\n\n\ndf, acc = few_shot_responses(promptV_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Foundries division reports its sales increased by 9.7 % to EUR 63.1 mn from EUR 57.5 mn in the corresponding period in 2006 , and sales of the Machine Shop division increased by 16.4 % to EUR 41.2 mn from EUR 35.4 mn in the corresponding period in 2006 .\n\n\nThe accuracy improves, but is still second-best so far.\n\nacc\n\n0.8328131966116807\n\n\nCompared to my best-performing prompting strategy, this prompt yields one more correct negative sentence, 11 more correct neutrals and 17 fewer correct positives.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_V.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-w",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-w",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt W",
    "text": "Prompt W\nI’ll try one more reduction in examples to 15 before increasing them past 27.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5,  # positive\n    292, 293, 294, 347, 348,   # negative\n    0, 37, 38, 39, 40 # neutral\n]\n\n\npromptW_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptW_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2249\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n15\n\n\n\ndf, acc = few_shot_responses(promptW_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\n\n\nDecreasing the number of examples to 15 has worsened the accuracy.\n\nacc\n\n0.8154735437972432\n\n\nInterestingly, here the negative and positive sentiments perform better but neutral performs worse (compared to the best- performing prompt).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_W.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-x",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-x",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt X",
    "text": "Prompt X\nI’ll now go in the other direction and increase the number of prompts from 27 to 30 (10 per sentiment).\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, 353, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270 # neutral\n]\n\n\npromptX_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptX_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2234\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n30\n\n\n\ndf, acc = few_shot_responses(promptX_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: MegaFon 's subscriber base increased 16.1 % in 2009 to 50.5 million users as of December 31 , while its market share by the number of customers amounted to 24 % as of late 2009 , up from 23 % as of late 2008 , according to TeliaSonera estimates .\n\n\nNope! Increasing the prompts to 30 doesn’t yield better results.\n\nacc\n\n0.8173679498657117\n\n\nA similar trend as before is appearing: the model gets a lot better at predicting neutral sentences at the cost of negative and positive sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_X.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-y",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-y",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt Y",
    "text": "Prompt Y\nBefore I return to the best-performing 21-Shot prompt, I’ll try and give the model a significantly higher number of examples (60) to see if that improves performance.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,# positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 370, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280 # neutral\n]\n\n\npromptY_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptY_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2204\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n60\n\n\n\ndf, acc = few_shot_responses(promptY_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: The fair value of the property portfolio doubled as a result of the Kapiteeli acquisition and totalled EUR 2,686.2 1,259.7 million .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\nNope! 60 examples does not improve performance.\n\nacc\n\n0.7581669691470054\n\n\nInterestingly this prompt causes the model to perform ridiculously well on neutral sentences (1368/1371 = 99.8%), but abysmally on negative and especially positive sentences (117/550 = 21%).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_Y.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-z",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-z",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt Z",
    "text": "Prompt Z\nI’ll return to the best-performing prompt: 27-Shot Prompt A. I’ll see if adding instructions helps.\n\npromptZ = \"\"\"Label the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nTEXT: {text}\"\"\"\n\n\nprint(promptZ)\n\nLabel the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nTEXT: {text}\n\n\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266 # neutral\n]\n\n\npromptZ_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptZ_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2237\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n27\n\n\n\ndf, acc = few_shot_responses(promptZ_ds, promptZ, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral. If the amount of money is not explicitly increasing or decreasing, respond with neutral.\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nThe modified prompt resulted in a ~2% drop in accuracy.\n\nacc\n\n0.813589628967367\n\n\nThis prompt performs better than the best-performing one for neutral sentences (1374 > 1300) at the cost of negative and positive sentences.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_Z.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-aa",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-aa",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AA",
    "text": "Prompt AA\nSince positive sentiment is performing the worst, I’ll see if reducing neutral and negative examples improves its performance. I’ll stick with the original Prompt A.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9,  # positive\n    292, 293, 294, 347, 348, 349, 350,  # negative\n    0, 37, 38, 39, 40, 263, 264 # neutral\n]\n\n\npromptAA_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptAA_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2241\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n23\n\n\n\ndf, acc = few_shot_responses(promptAA_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nThe overall accuracy drops a bit.\n\nacc\n\n0.8295403837572513\n\n\nThe positive performance deteriorates while negative and neutral get better.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AA.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ab",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ab",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AB",
    "text": "Prompt AB\nI’ll now reduce the positive examples while keeping the other two the same as my best-performing prompt.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266 # neutral\n]\n\n\npromptAB_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptAB_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2239\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n25\n\n\n\ndf, acc = few_shot_responses(promptAB_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Foundries division reports its sales increased by 9.7 % to EUR 63.1 mn from EUR 57.5 mn in the corresponding period in 2006 , and sales of the Machine Shop division increased by 16.4 % to EUR 41.2 mn from EUR 35.4 mn in the corresponding period in 2006 .\n\n\nWhile still a bit lower than my best-performing accuracy, this approach might be worth expanding on.\n\nacc\n\n0.8369807949977669\n\n\nThe number of correctly predicted neutral sentences increases, while the other two decrease.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AB.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ac",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ac",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AC",
    "text": "Prompt AC\nI’ll continue reducing positive examples.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266 # neutral\n]\n\n\npromptAC_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptAC_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2241\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n23\n\n\n\ndf, acc = few_shot_responses(promptAC_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\n\n\nNope! That doesn’t improve accuracy.\n\nacc\n\n0.8340026773761714\n\n\nneutral and positive sentences are correctly predicted at a worse rate, negative sentences predicted a bit better.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AC.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ad",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ad",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AD",
    "text": "Prompt AD\nAs I was staring at exclude_idxs I realized that they are sorted by sentiment with positive first, then negative and then neutral. Perhaps this order affects the generations? I’ll try randomizing the order of the 27 examples that yielded the best accuracy.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9,  # positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266 # neutral\n]\n\n\nrandom.shuffle(exclude_idxs)\nexclude_idxs\n\n[293,\n 266,\n 264,\n 6,\n 347,\n 7,\n 263,\n 9,\n 38,\n 37,\n 0,\n 352,\n 5,\n 4,\n 351,\n 350,\n 40,\n 294,\n 1,\n 8,\n 349,\n 2,\n 348,\n 292,\n 3,\n 39,\n 265]\n\n\n\npromptAD_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptAD_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2237\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n27\n\n\n\ndf, acc = few_shot_responses(promptAD_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n\nWow, that actually improved my accuracy! It’s making me question all previous few-shot prompt results!\n\nacc\n\n0.8609745194456861\n\n\nnegative TPR decreases, neutral and positive increase quite a bit!\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AD.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ae",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ae",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AE",
    "text": "Prompt AE\nI’ll return to my 60-Shot prompt and shuffle the examples to see if that improves the accuracy.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,# positive\n    292, 293, 294, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 370, # negative\n    0, 37, 38, 39, 40, 263, 264, 265, 266, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280 # neutral\n]\n\n\nrandom.shuffle(exclude_idxs)\nexclude_idxs[:5]\n\n[15, 37, 358, 16, 0]\n\n\n\npromptAE_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptAE_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2204\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n60\n\n\n\ndf, acc = few_shot_responses(promptAE_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: The fair value of the property portfolio doubled as a result of the Kapiteeli acquisition and totalled EUR 2,686.2 1,259.7 million .\n\n\nThe shuffled 60-Shot prompt does not yield better results.\n\nacc\n\n0.8371143375680581\n\n\nInterestingly it gets the same number of neutral sentences right (1341) but get lower counts for the other two.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AE.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-af",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-af",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AF",
    "text": "Prompt AF\nI’ll now try a shuffled 15-Shot prompt.\n\nexclude_idxs = [\n    1, 2, 3, 4, 5,  # positive\n    292, 293, 294, 347, 348,   # negative\n    0, 37, 38, 39, 40 # neutral\n]\n\n\nrandom.shuffle(exclude_idxs)\nexclude_idxs[:5]\n\n[292, 38, 1, 0, 4]\n\n\n\npromptAF_ds = ds_subset(dataset, exclude_idxs=exclude_idxs)\npromptAF_ds\n\nDataset({\n    features: ['sentence', 'label', 'label_text', '__index_level_0__'],\n    num_rows: 2249\n})\n\n\n\nexamples = []\nfor idx in exclude_idxs:\n    examples.append((dataset[idx]['sentence'], dataset[idx]['label_text']))\n\nlen(examples)\n\n15\n\n\n\ndf, acc = few_shot_responses(promptAF_ds, promptA, examples)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\n\n\nShuffling the 15-Shot prompt doesn’t beat the 27-Shot prompt.\n\nacc\n\n0.8199199644286349\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AF.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ag",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ag",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AG",
    "text": "Prompt AG\nThe last thing I’ll experiment with is adding a system prompt.\n\n\nShow updated few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples, sp):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    print(dataset[0]['prompt'])\n    \n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    for row in dataset:\n        messages = [{\"role\": \"system\", \"content\": sp}] + few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n        generated_ids = model.generate(\n            model_inputs.input_ids,\n            max_new_tokens=2\n        )\n        generated_ids = [\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n        ]\n\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n        responses.append(response)\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    \n    return df, acc\n\n\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification. If the amount of money, market share, or key performance indicators are not explicitly increasing or decreasing, respond with neutral. Consider terms like 'growth', 'decline', 'improvement', or 'deterioration' as indicators of change.\n\n\n\ndf, acc = few_shot_responses(promptAD_ds, promptA, examples, sp)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nThis particular system prompt doesn’t improve the accuracy.\n\nacc\n\n0.8439874832364774\n\n\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AG.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ah",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ah",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AH",
    "text": "Prompt AH\nI’ll simplify the system prompt.\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral. Consider the overall impact and implications of the statement when making your classification.\n\n\n\ndf, acc = few_shot_responses(promptAD_ds, promptA, examples, sp)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nUsing a simpler system prompt lowers the accuracy.\n\nacc\n\n0.8466696468484578\n\n\nThe model gets 22 more neutral sentences correct but does worse on negative and positive sentences (when compared to the best-performing Prompt AD).\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AH.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ai",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#prompt-ai",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Prompt AI",
    "text": "Prompt AI\nI’ll further simplify the system prompt.\n\nsp = \"You are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral.\"\n\n\nprint(sp)\n\nYou are an expert in financial sentiment analysis. Your task is to accurately classify the sentiment of financial statements as negative, positive, or neutral.\n\n\n\ndf, acc = few_shot_responses(promptAD_ds, promptA, examples, sp)\n\n\n\n\nLabel the following TEXT with a single word: negative, positive, or neutral\nTEXT: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n\n\nI still can’t recover (or improve upon) the best-performing accuracy of 86%.\n\nacc\n\n0.8439874832364774\n\n\nThe neutral performance continues to increase while negative and positive sentences suffer.\n\nmake_cm(df)\n\n\n\n\n\n\n\n\n\ndf.to_csv('/notebooks/Qwen2-1.5B-Instruct_AI.csv', index=False)"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#running-inference-10-times-using-the-best-prompt",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#running-inference-10-times-using-the-best-prompt",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Running Inference 10 Times Using the Best Prompt",
    "text": "Running Inference 10 Times Using the Best Prompt\n\ndef test_gen(examples):\n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": promptA.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    messages = few_shot_examples + [{\"role\": \"user\", \"content\": promptA.format(text=dataset[0]['sentence'])}]\n        \n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=2\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n    return response\n\n\n%timeit -n 10 test_gen(examples)\n\n208 ms ± 1.93 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nShow updated few_shot_responses function\ndef few_shot_responses(dataset, prompt, examples):\n    responses = []\n    dataset = dataset.map(add_prompt, fn_kwargs={\"prompt\": prompt})\n    \n    few_shot_examples = []\n    \n    for example in examples:\n        few_shot_examples.append({\"role\": \"user\", \"content\": prompt.format(text=example[0])})\n        few_shot_examples.append({\"role\": \"assistant\", \"content\": example[1]})\n    \n    for row in dataset:\n        messages = few_shot_examples + [{\"role\": \"user\", \"content\": row['prompt']}]\n        \n        text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n        generated_ids = model.generate(\n            model_inputs.input_ids,\n            max_new_tokens=2\n        )\n        generated_ids = [\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n        ]\n\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip().lower()\n        responses.append(response)\n        \n    # calculate accuracy\n    df = dataset.to_pandas()\n    df['responses'] = pd.Series(responses)\n    df['responses'] = df['responses'].apply(lambda x: x if x in ['negative', 'positive', 'neutral'] else \"other\")\n    df['lm_match'] = df['label_text'] == df['responses']\n    acc = df.lm_match.mean()\n    \n    return df, acc\n\n\n\nlen(examples)\n\n27\n\n\n\nexclude_idxs[:10]\n\n[293, 266, 264, 6, 347, 7, 263, 9, 38, 37]\n\n\n\naccs = []\nfor _ in range(10):\n    df, acc = few_shot_responses(promptAD_ds, promptA, examples)\n    accs.append(acc)\n\nThis prompt results in a pretty consistent overall accuracy, around 86%.\n\npd.Series(accs).describe()\n\ncount    10.000000\nmean      0.859321\nstd       0.004617\nmin       0.847564\n25%       0.858739\n50%       0.860304\n75%       0.861310\nmax       0.865445\ndtype: float64"
  },
  {
    "objectID": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#final-thoughts",
    "href": "posts/2024-09-23-tinysentiment-Qwen2-1.5B-sentiment-classification/index.html#final-thoughts",
    "title": "Sentiment Classification with Qwen2-1.5B-Instruct",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are the results from my experiments so far (**the best-performing prompt from this notebook):\n\n\n\n\n\n\n\n\n\n\n\nModel\nPrompting Strategy\nOverall Accuracy\nnegative\nneutral\npositive\n\n\n\n\nclaude-3-5-sonnet-20240620\n3-Shot\n94.78%\n98% (297/303)\n94% (1302/1391)\n95% (544/570)\n\n\nclaude-3-opus-20240229\n0-Shot\n94.13%\n98% (297/303)\n96% (1333/1391)\n88% (501/570)\n\n\nphi-3.5\n20-Shot\n93.94%\n96% (286/299)\n98% (1355/1379)\n83% (467/566)\n\n\nphi-3\n30-Shot w/System Prompt\n92.79%\n98% (290/297)\n94% (1284/1373)\n88% (499/564)\n\n\nclaude-3-haiku-20240307\n3-Shot\n92.39%\n90% (272/303)\n91% (1267/1391)\n96% (550/570)\n\n\nphi-2\n6-Shot\n91.94%\n88% (267/302)\n94% (1299/1387)\n90% (510/569)\n\n\n**Qwen2-1.5B\n27-Shot\n86.10%\n90% (264/294)\n95.5% (1320/1382)\n61% (342/561)\n\n\n\nHere are the results from this notebook. The best-performing prompt was a randomly shuffled 27-Shot prompt (Prompt AD), yielding an overall accuracy of 86.10%.\n\n\n\nprompt\nstrategy\naccuracy\nnegative\nneutral\npositive\n\n\n\n\nA\n0-Shot\n81.76%\n97% (293/303)\n85% (1185/1391)\n65% (373/570)\n\n\nB\n0-Shot\n51.86%\n99% (300/303)\n61% (846/1391)\n5% (28/570)\n\n\nC\n0-Shot\n81.40%\n93% (283/303)\n96% (1330/1391)\n40% (230/570)\n\n\nD\n0-Shot\n78.53%\n92% (279/303)\n92% (1281/1391)\n38% (218/570)\n\n\nE\n0-Shot\n66.21%\n100% (302/303)\n82% (1145/1391)\n9% (52/570)\n\n\nF\n0-Shot\n78.05%\n88% (267/303)\n97% (1355/1391)\n25% (145/570)\n\n\nG\n0-Shot\n66.70%\n94% (285/303)\n80% (1107/1391)\n21% (118/570)\n\n\nH\n0-Shot\n70.89%\n85% (259/303)\n90% (1247/1391)\n17% (99/570)\n\n\nI\n0-Shot\n69.17%\n58% (176/303)\n86% (1201/1391)\n33% (189/570)\n\n\nJ\n0-Shot\n57.38%\n47% (142/303)\n78% (1086/1391)\n12% (71/570)\n\n\nK\n0-Shot\n41.87%\n34% (102/303)\n52% (728/1391)\n21% (118/570)\n\n\nL\n0-Shot\n42.84%\n66% (200/303)\n45% (629/1391)\n25% (141/570)\n\n\nM\n0-Shot\n51.46%\n26% (79/303)\n77% (1078/1391)\n1% (8/570)\n\n\nN\n0-Shot\n29.77%\n11% (33/303)\n44% (608/1391)\n6% (33/570)\n\n\nO\n0-Shot\n61.00%\n37% (113/303)\n90% (1257/1391)\n2% (11/570)\n\n\nP\n3-Shot\n78.20%\n91% (275/302)\n91% (1266/1390)\n40% (227/569)\n\n\nQ\n6-Shot\n76.93%\n96% (289/302)\n73% (1010/1387)\n77% (438/569)\n\n\nR\n20-Shot\n81.42%\n92% (274/299)\n94% (1301/1379)\n45% (252/566)\n\n\nS\n30-Shot\n81.51%\n87% (255/294)\n98% (1345/1379)\n39% (221/561)\n\n\nT\n27-Shot\n83.73%\n93% (272/294)\n94.3% (1303/1382)\n53% (298/561)\n\n\nU\n25-Shot\n82.94%\n90% (266/294)\n96.2% (1331/1384)\n46% (260/561)\n\n\nV\n21-Shot\n83.28%\n92% (273/296)\n94.9% (1314/1384)\n50% (281/563)\n\n\nW\n15-Shot\n81.55%\n94% (279/298)\n87.7% (1215/1386)\n60% (340/565)\n\n\nX\n30-Shot\n81.74%\n89% (261/293)\n96.7% (1336/1381)\n41% (229/560)\n\n\nY\n60-Shot\n75.82%\n66% (186/283)\n99.8% (1368/1371)\n21% (117/550)\n\n\nZ\n27-Shot\n81.36%\n80% (236/294)\n99.4% (1374/1382)\n37% (210/561)\n\n\nAA\n23-Shot\n82.95%\n93% (276/296)\n94.9% (1314/1384)\n48% (269/561)\n\n\nAB\n25-Shot\n83.70%\n92% (270/294)\n95.3% (1317/1382)\n51% (287/563)\n\n\nAC\n23-Shot\n83.40%\n95% (278/294)\n93.8% (1296/1382)\n52% (295/565)\n\n\nAD\n27-Shot\n86.10%\n90% (264/294)\n95.5% (1320/1382)\n61% (342/561)\n\n\nAE\n60-Shot\n83.71%\n83% (234/283)\n97.8% (1341/1371)\n49% (270/550)\n\n\nAF\n15-Shot\n82.00%\n91% (272/298)\n88.8% (1231/1386)\n60% (341/565)\n\n\nAG\n27-Shot w/System Prompt\n84.40%\n84% (248/294)\n97.8% (1351/1382)\n52% (289/561)\n\n\nAH\n27-Shot w/System Prompt\n84.67%\n87% (256/294)\n97.1% (1342/1382)\n53% (296/561)\n\n\nAI\n27-Shot w/System Prompt\n84.99%\n88% (260/294)\n97.3% (1345/1382)\n50% (283/561)\n\n\n\nHere are my takeaways from working with Qwen2-1.5B-Instruct:\n\nIt was easy to get a decent overall accuracy (on my first try!) but quite difficult to improve upon it. Although I tested out 35 prompts, there’s still much more room for experimentation.\nThe positive sentiment True Positive Rate was considerably worse than neutral or negative sentiments. The most accurate positive sentiment classification was 77% (Prompt Q) compared to 99.8% for negative (Prompt Y) and 100% for neutral (Prompt E).\nRandomizing the order of the examples improved my 27-Shot prompt’s overall accuracy. I had not tried this before and is something I’ll make sure to use again in the future.\nThe best-performing prompt contained an equal number of examples from each sentiment (9 per sentiment). Decreasing the representation of each sentiment worsened the overall accuracy.\nThe best performing prompt for each sentiment is not the best overall prompt. This is important to note because there may be situations where one type of sentiment is more critical than the other. For example, it may be “safer” to have a high TPR for negative or positive sentiment if people are making decisions based on the predicted sentiment.\n\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html",
    "href": "posts/2024-04-19-orpo/index.html",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "",
    "text": "In this blog post I’ll provide a summary (and an exploration of some of the math) for the research paper ORPO: Monolithic Preference Optimization without Reference Model.\nHere’s the abstract:\n\nWhile recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on AlpacaEval2.0 (Figure 1), 66.19% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-\\(\\alpha\\) (7B) and Mistral-ORPO-\\(\\beta\\) (7B)."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#main-takeaways",
    "href": "posts/2024-04-19-orpo/index.html#main-takeaways",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Main Takeaways",
    "text": "Main Takeaways\nI took away 5 main points from this paper:\n\nORPO does not use a reference model like DPO (in the KL term) or a reward model and initial SFT snapshot like RLHF.\nInstead, ORPO directly trains a preference-aligned Supervised Fine-Tuned (SFT) model.\nThe ORPO loss includes a penalty (added to the normal causal LM Negative Log Likelihood loss) which maximizes the likelihood of generating a favored reponse.\nORPO consistently is preferred by a reward model against SFT and RLHF.\nThe ORPOR win rate vs. DPO increases as model size increases."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#quick-review-dpo-and-rlhf",
    "href": "posts/2024-04-19-orpo/index.html#quick-review-dpo-and-rlhf",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Quick Review: DPO and RLHF",
    "text": "Quick Review: DPO and RLHF\nIn order to better visualize how ORPO differs from DPO and RLHF I’ll provide a couple of visuals to highlight those points.\nFirst, from Chip Huyen’s blog post on RLHF, I’ve highlighted where the SFT model and reward model are used.\n\nNext I’ll show the DPO loss function, where the reference model is used in the log probability ratios:\n\nFinally, from the ORPO paper, a graphic that compares RLHF, DPO and ORPO. The odds ratio is shown on the right (it strongly adapts to chosen reponses and has a weak penalty for rejected responses)."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#quick-review-sft-without-orpo",
    "href": "posts/2024-04-19-orpo/index.html#quick-review-sft-without-orpo",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Quick Review: SFT without ORPO",
    "text": "Quick Review: SFT without ORPO\nI’ll do a quick review of what the loss function looks like for SFT without ORPO. The loss function is cross entropy loss, where the log probabilities \\(\\log(p_i^{(k)})\\) of the label tokens (when \\(y_i\\) is True) are average across the input sequence of length \\(m\\):\n\\[\\mathcal{L} = -\\frac{1}{m}\\sum^m_{k=1}\\sum^{|V|}_{i=1}y_i^{(k)}\\cdot\\log(p_i^{(k)})\\]\nThis loss function is effective for domain adaptation (for next token prediction) but doesn’t have a mechanism to penalize rejected responses.\nIn the ORPO paper they studies the log probabilities of chosen and rejected responses during SFT (without ORPO) and found that both increase over the course of training. In other words, the model does not discriminate between desired and undesired tokens."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#orpo-loss",
    "href": "posts/2024-04-19-orpo/index.html#orpo-loss",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "ORPO Loss",
    "text": "ORPO Loss\nThe ORPO loss function is an enhancement or augmentation of SFT loss:\n\\[\\mathcal{L} = \\mathbb{E}_{(x, y_w, y_l)}\\big[\\mathcal{L}_{SFT} + \\lambda \\cdot\\mathcal{L}_{OR}\\big]\\]\nIn this loss function, \\(\\mathcal{L}_{SFT}\\) helps the model adapt to the specified subset of the domain. In other words, it does what good ol’ SFT does—fine-tune a model toward a given downstream task (chat, QA, reasoning, etc.).\nThe \\(\\mathcal{L}_{OR}\\) term (called relative ratio loss) helps the mode disfavor generations in the rejected responses set.\nThe term \\(\\lambda\\) weighs the relative ratio loss and affects how much the model disfavors rejected responses. An ablation study on this term is done in the appendix which I’ll talk about in a bit.\nThe relative ratio loss is defined as:\n\\[\\mathcal{L}_{OR} = -\\log\\sigma\\big(\\log\\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)}\\big)\\]\nThis loss contains the odds ratio which I’ll talk about next."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#odds-ratio",
    "href": "posts/2024-04-19-orpo/index.html#odds-ratio",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Odds Ratio",
    "text": "Odds Ratio\nThe odds ratio is a ratio of:\n\nthe odds of generating the desired output sequence \\(y_w\\) given an input sequence \\(x\\)\nand the odds of generating the undesired output sequence \\(y_l\\) given an input sequence \\(x\\).\n\nOdds are defined as follows:\n\\[\\textbf{odds}_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}\\]\nWhere \\(P_\\theta(y|x)\\) is the likelihood that the model \\(\\theta\\) will generate \\(y\\) given input \\(x\\).\n\nIntuitively \\(\\textbf{odds}_\\theta(y|x)=k\\) implies that it is \\(k\\) times more likely for the model \\(\\theta\\) to generate the output sequence \\(y\\) than not generating it \\(y\\).\n\nNote that as the likelihood \\(P\\) increases, \\(1-P\\) decreases and the odds increase.\nIn the following visual I show how we can plot odds as \\(\\frac{x}{1-x}\\) in Desmos and see how quickly the odds increase as likelihood increases.\n\nI also plot the relative ratio loss as a function of the odds ratio in desmos. You can see that as the odds ratio (in the graph denoted by \\(x\\)) increases the loss function decreases. When the odds ratio increases, the numerator increases relative to the denominator. This means that the odds that the model will generated desired responses increases relative to the odds that it will generated undesired responses. In other words, as the model is trained and learns to minimize the loss, it also learns to maximize the likelihood of generating desired responses.\n\nIn the ORPO paper, they study how the log odds ratio and log probabilities of chosen and rejected responses vary over the course of training. They find that as the model is trained longer, the log odds ratio increases and after awhile, the log probabilities of rejected responses decreases (while chosen log probs continue to increase). This show the discriminating behavior of the relative ratio loss as compared to SFT loss."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#gradient-of-orpo-loss",
    "href": "posts/2024-04-19-orpo/index.html#gradient-of-orpo-loss",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Gradient of ORPO Loss",
    "text": "Gradient of ORPO Loss\nI’ll go through some of the intuition provided in the paper around the gradient of the ORPO loss function, which contains two factors: \\(\\delta(d)\\) and \\(h(d)\\) which are functions of the dataset \\(d\\) which contains inputs \\(x\\), desired responses \\(y_w\\) and undesired responses \\(y_l\\).\n\n\n\\(\\delta(d)\\)\nI’ll start by looking at \\(\\delta(d)\\) first: when the odds of the desired responses are relatively higher than the undesired responses, this term will converge to 0.\nIn the desmos graph below, \\(x\\) represents the odds ratio and so \\(\\delta(d)\\) takes the form of \\(\\frac{1}{1+x}\\):\n\nFor the following concept:\n\n\\(\\delta(d)\\) will play the role of a penalty term. accelerating the parameter updates if the model is more likely to generate the rejected responses.\n\nI found the following rearranging of \\(\\delta(d)\\) terms more intuitive:\n\n\\[\\delta(d) = \\big[1 + \\frac{\\textbf{odds}_\\theta P(y_w|x)}{\\textbf{odds}_\\theta P(y_l|x)}\\big]^{-1}\\]\nI’ll replace \\(1\\) with an equivalent fraction that has the same denominator as the odds ratio:\n\n\\[\\delta(d) = \\big[\\frac{\\textbf{odds}_\\theta P(y_l|x)}{\\textbf{odds}_\\theta P(y_l|x)} + \\frac{\\textbf{odds}_\\theta P(y_w|x)}{\\textbf{odds}_\\theta P(y_l|x)}\\big]^{-1}\\]\nNow I can easily add together the fractions inside the brackets because they have the same denominator:\n\n\\[\\delta(d) = \\big[\\frac{\\textbf{odds}_\\theta P(y_l|x) + \\textbf{odds}_\\theta P(y_w|x)}{\\textbf{odds}_\\theta P(y_l|x)}\\big]^{-1}\\]\nRaising a fraction to the \\(-1\\) power is the same as flipping it:\n\n\\[\\delta(d) = \\big[\\frac{\\textbf{odds}_\\theta P(y_l|x)}{\\textbf{odds}_\\theta P(y_l|x) + \\textbf{odds}_\\theta P(y_w|x)}\\big]\\]\n\nI find this form of \\(\\delta(d)\\) more intuitive. Here you can directly see that as the odds of generating rejected responses (the numerator) increases, the overall fraction increases and thus the gradient increases, accelerating the parameter update.\n\n\n\\(h(d)\\)\nThe second term of the ORPO loss gradient is \\(h(d)\\) which has the form:\n\\[h(d) = \\frac{\\nabla_\\theta\\log P_\\theta(y_w|x)}{1-P_\\theta(y_w|x)} - \\frac{\\nabla_\\theta\\log P_\\theta(y_l|x)}{1-P_\\theta(y_l|x)}\\]\n\nFor the chosen responses, this accelerates the model’s adaptation toward the distribution of chosen responses as the likelihood increases.\n\nAs the likelihood of chosen reponses (\\(P_\\theta(y_w|x)\\)) increases, the denominator (\\(1-P\\)) decreases and the first fraction increases, increasing the gradient and accelerating the model’s adaptation toward predicting chosen responses.\nOn the other hand, as the likelihood of the rejected response increases the denominator decreases and the second fraction increases, decreasing the gradient and slowing down the model’s adapation toward the rejected response distribution.\nI’ve summarized the behavior of the two terms below:\n\nI see these two terms, \\(\\delta(d)\\) and \\(h(d)\\), contrasting each other and keeping the gradients from vanishing or exploding."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#training",
    "href": "posts/2024-04-19-orpo/index.html#training",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Training",
    "text": "Training\nIn this section I’ll recap the training details provided in the paper.\n\nModels\nThe authors trained the following models for SFT, PPO, DPO and ORPO to compare results across each other:\n\nOPT (125M to 1.3B).\nPhi-2 (2.7B).\nLlama 2 (7B).\nMistral (7B).\n\nThey used the following training techniques and hyperparameters:\n\nFlash Attention 2.\nDeep Speed Zero 2 (for OPT series and Phi-2).\nFSDP (Fully Sharded Data Parallel) for Llama-2 and Mistral (both 7B).\nAdamW and paged Adam optimizers.\nLinear Warmup with Cosine Decay.\nInput length of 1024 (HH dataset) and 2048 (UltraFeedback dataset).\nSFT: Max LR of 1e-5, trained for 1 epoch.\nDPO: \\(\\beta\\)=0.1, LR = 5e-6, 3 epochs.\nORPO: LR=8e-6, 10 epochs.\n\n\n\n\nDatasets\n\nAnthropic’s HH-RLHF.\nBinarized UltraFeedback.\n\n\n\nReward Model\nThe reward model is used to score the responses from the different models. They trained OPT 250M and 1.3B on each dataset for a single epoch."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#results",
    "href": "posts/2024-04-19-orpo/index.html#results",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Results",
    "text": "Results\nTheir ORPO model performed better than their SFT, DPO and PPO models for each model family for AlpacaEval 1.0 and 2.0 (corrected for length-bias in models).\n\nOn the MT-Bench dataset they found that:\n\nIn comparison to \\(\\lambda\\) = 0.1, Mistral+ORPO (7B) with \\(\\lambda\\) = 1.0 performs worse in extraction, math and reasoning=, which are the categories that generally require deterministic answers. On the other hand, it performs better in STEM, humanities, and roleplay, which ask the generations without hard answers.\n\nI will note that STEM contains science, engineering and math which can contain deterministic answers so I’m not sure this distinction between the two \\(\\lambda\\) values holds.\nOn the HH-RLHF dataset, ORPO had high win-rates for 125M and 350M models against SFT and SFT+PPO, and high win-rate for 1.3B against SFT+DPO.\n\nThe trend was the same for the UltraFeedback dataset:\n\n\nComputation Efficiency\nThe authors found the following results as relating to computational efficiency:\n\nORPO does not require a reference model which in RLHF and DPO is the model trained with SFT used during training to keep the parameterized model from degenerating.\nDPO and RLHF require two SFT models: a frozen reference model (KL term) and the model being trained.\nORPO only has one model: the model being trained with SFT. This requires half the forward passes of DPO or RLHF."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#limitations",
    "href": "posts/2024-04-19-orpo/index.html#limitations",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Limitations",
    "text": "Limitations\nThe authors highlighted that their work lacks comparisons with alignment algorithms other than PPO and DPO (such KTO or IPO). They also only trained models that are up to 7B parameters in size."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#future-work",
    "href": "posts/2024-04-19-orpo/index.html#future-work",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Future Work",
    "text": "Future Work\nThe authors highlighted three areas of future work:\n\nEvaluate ORPO performance on models larger than 7B parameters.\nEvaluate the impact of ORPO on pretrained models.\nExpand to consecutive preference alignment algorithms.\n\n\nIn the following sections I walk through some of the math in the paper."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#section-4-odds-ratio-preference-optimization",
    "href": "posts/2024-04-19-orpo/index.html#section-4-odds-ratio-preference-optimization",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Section 4: Odds Ratio Preference Optimization",
    "text": "Section 4: Odds Ratio Preference Optimization\n\n4.1 Preliminaries\n\nGiven an input sequence \\(x\\), the average log-likelihood of generating the output sequence \\(y\\), of length \\(m\\) tokens, is computed as:\n\n\\[\\log P_\\theta(y|x) = \\frac{1}{m}\\sum^m_{t=1} \\log P_\\theta(y_t|x,y_{<t})\\]\nExcellently worded explanations from ChatGPT (emphasis mine):\n\n\\(\\frac{1}{m}\\)​: This term represents the reciprocal of the length of the output sequence \\(y\\). It’s the inverse of the number of tokens in the output sequence. This term normalizes the sum of log probabilities over the length of the sequence, ensuring that longer sequences don’t disproportionately influence the average log-likelihood.\n\n\n\\(\\log P_\\theta​(y_t​∣x,y_{<t}​)\\): This term represents the logarithm of the probability of generating the current token \\(y_t\\)​ given the input sequence \\(x\\) and the preceding tokens \\(y_{<t}\\). It measures how likely the model thinks the current token \\(y_t\\)​ is, given the input \\(x\\) and the previously generated tokens \\(y_{<t}\\)​.\n\nHere’s what \\(log(x)\\) looks like:\n\nThe odds of generating the output sequence \\(y\\) given an input sequence \\(x\\):\n\\[\\textbf{odds}_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}\\]\n\nIntuitively \\(\\textbf{odds}_\\theta(y|x)=k\\) implies that it is \\(k\\) times more likely for the model \\(\\theta\\) to generate the output sequence \\(y\\) than not generating it.\n\nWriting that out:\n\\[\\textbf{odds}_\\theta(y|x)= \\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)} = k\\]\n\n\\[\\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)} = k\\]\n\n\\[P_\\theta(y|x) = k[1-P_\\theta(y|x)]\\]\n\nit is \\(k\\) times more likely to generate \\(y\\) (i.e. \\(P_\\theta)\\)) than not generating it (\\(1-P_\\theta\\)).\n\n\nThe odds ratio of the chosen response \\(y_w\\) over the rejected response:\n\n\\[\\textbf{OR}_\\theta(y_w,y_l) = \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)}\\]\n\nindicates how much more likely it is for the model \\(\\theta\\) to generate \\(y_w\\) than \\(y_l\\) given input \\(x\\).\n\nIf the odds ratio is some value \\(r\\):\n\\[\\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)} = r\\]\n\n\\[\\textbf{odds}_\\theta(y_w|x) = r \\cdot \\textbf{odds}_\\theta(y_l|x)\\]\nThe odds of the model generating a chosen response is \\(r\\) times the odds of it generating a rejected response.\n\n\n4.2 Objective Function of ORPO\n\\[\\mathcal{L}_{ORPO}=\\mathbb{E}_{(x,y_w,y_l)}\\big[ \\mathcal{L}_{SFT} + \\lambda \\cdot \\mathcal{L}_{OR}\\big]\\]\nWhere:\n\\[\\mathcal{L}_{SFT} \\text{ is the supervised fine-tuning loss (maximizes the likelihood of generating the reference tokens)}\\]\n\n\\[\\mathcal{L}_{OR} \\text{ is the relative ratio loss (maximizes the odds ratio between the likelihood of generating the favored response } y_w\\text{ and the disfavored response } y_l)\\]\n\nRelative Ratio Loss \\(\\mathcal{L}_{OR}\\)\n\\[\\mathcal{L}_{OR} = - \\log \\sigma\\big(\\log \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)} \\big)\\]\nThe term inside log-sigmoid, \\(\\log \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)}\\), increases as the odds of generating chosen responses increases (and therefore increases as the likelihood of the model generating chosen responses increases).\n\\(\\mathcal{L}_{OR}\\), represented below by \\(-\\log\\sigma(x)\\), decreases as x increases, meaning the loss decreases as the likelihood and the odds of the model generating chosen responses increases.\n\nMinimizing \\(\\mathcal{L}_{OR}\\) means maximizing \\(\\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)}\\)\n\n\n\n4.3 Gradient of ORPO\n\\[\\nabla_\\theta\\mathcal{L}_{OR}=\\delta(d) \\cdot h(d)\\]\nWhere:\n\\[\\delta(d) \\text{ penalizes the wrong predictions}\\] \\[h(d) \\text{ contrasts between chosen and rejected responses}\\] \\[d = (x, y_w, y_l) \\sim D\\]\nFull form of \\(\\delta(d)\\):\n\\[\\delta(d) = \\big[1 + \\frac{\\textbf{odds}_\\theta P(y_w|x)}{\\textbf{odds}_\\theta P(y_l|x)} \\big]^{-1}\\]\n\nVisualizing this as \\(\\frac{1}{1+x}\\), as the odds ratio increases (odds of generating favored responses increases) \\(delta(d)\\) decreases.\n\nRewriting that by expanding out the fraction in multiple steps. First, replace \\(1\\) with an equivalent (rejected odds divided by rejected odds):\n\\[\\delta(d) = \\big[\\frac{\\textbf{odds}_\\theta P(y_l|x)}{\\textbf{odds}_\\theta P(y_l|x)}  + \\frac{\\textbf{odds}_\\theta P(y_w|x)}{\\textbf{odds}_\\theta P(y_l|x)} \\big]^{-1}\\]\nNow that they have the same denominator, add the fractions:\n\n\\[\\delta(d) = \\big[\\frac{\\textbf{odds}_\\theta P(y_l|x) +\\textbf{odds}_\\theta P(y_w|x)}{\\textbf{odds}_\\theta P(y_l|x)} \\big]^{-1}\\]\nTaking the inverse (exponent of \\(-1\\)), flips the fraction:\n\n\\[\\delta(d) = \\big[\\frac{\\textbf{odds}_\\theta P(y_l|x)}{\\textbf{odds}_\\theta P(y_l|x) +\\textbf{odds}_\\theta P(y_w|x)} \\big]\\]\nI find this version more intuitive as it’s easier to see that as the odds of generating rejected responses increases, \\(\\delta(d)\\) increases. As the odds of generating favored responses increases, \\(\\delta(d)\\) decreases.\n\nWhen the odds of the favored responses are relatively higher than the disfavored responses, \\(\\delta(d)\\) will converge to 0.\n\n\nThis indicates that \\(\\delta(d)\\) will play the role of a penalty term, accelerating the parameter updates\nif the model is more likely to generate the rejected responses.\n\nFull form of \\(h(d)\\):\n\\[h(d) = \\frac{\\nabla_\\theta\\log P_\\theta(y_w|x)}{1-P_\\theta(y_w|x)} - \\frac{\\nabla_\\theta\\log P_\\theta(y_l|x)}{1-P_\\theta(y_l|x)}\\]\n\n\\(h(d)\\) implies a weighted contrast of the two gradients from the chosen and rejected responses. Specifically, \\(1-P(y|x)\\) in the denominators amplifies the gradients when the corresponding side of the likelihood \\(P(y|x)\\) is low. For the chosen responses, this accelerates the model’s adaptation toward the distribution of chosen responses as the likelihood increases.\n\nThe last sentence (that I bolded) clarifies the concept (that the previous sentence muddied for me). As \\(P_\\theta(y_w|x)\\) increases, \\(1 - P_\\theta(y_w|x)\\) decreases (towards 0) and the fraction \\(\\frac{\\nabla_\\theta\\log P_\\theta(y_w|x)}{1-P_\\theta(y_w|x)}\\) increases (i.e. “this accelerates the model’s adaptation toward the distribution of chosen responses as the likelihood increases.”).\nConversely, as \\(P_\\theta(y_l|x)\\) increases (the likelihood of the model generating rejected responses), \\(1 - P_\\theta(y_l|x)\\) decreases (towards 0) and the fraction \\(\\frac{\\nabla_\\theta\\log P_\\theta(y_l|x)}{1-P_\\theta(y_l|x)}\\) increases. \\(h(d)\\) gets smaller, slowing the parameter updates since the model is more likely to generate the rejected responses.\nMy takeaway from the explanation of \\(\\delta(d)\\) and \\(h(d)\\) is that they are opposing forces that keep each other in check. If the model is more likely to generate rejected responses, \\(\\delta(d)\\) accelerates the parameter updates (i.e. increases the gradient) and \\(h(d)\\) slows down the parameter updates (decreases the gradient). If the model is more likely to generate favored responses, \\(\\delta(d)\\) slows down the parameter update (decreases the gradient) and \\(h(d)\\) accelerates the parameter update (increases the gradient). The intuition (I think) is that if either term gets too large or too small, the other term counters it so you don’t have vanishing or exploding gradients."
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#section-7.1-comparison-to-probability-ratio",
    "href": "posts/2024-04-19-orpo/index.html#section-7.1-comparison-to-probability-ratio",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "Section 7.1: Comparison to Probability Ratio",
    "text": "Section 7.1: Comparison to Probability Ratio\nThe following equations are slightly modified equations from the paper, defining their experiment comparing probability ratio (PR) to odds ratio (OR):\n\\(X_1\\) and \\(X_2\\) are sampled from a uniform probability distribution.\n\\[X_1, X_2 \\sim \\text{Unif}(0,1)\\]\n\\(Y_{PR}\\) is the log probability ratio:\n\\[Y_{PR} \\sim \\beta(\\log\\frac{X_1}{X_2}) = \\beta(\\log X_1 - \\log X_2)\\]\n\\(Y_{OR}\\) is the log odds ratio:\n\\[Y_{OR} = \\log(\\frac{\\textbf{odds}(X_1)}{\\textbf{odds}(X_2)}) = \\log\\big(\\frac{X_1}{1-X_1}\\big) - \\log\\big(\\frac{X_2}{1-X_2}\\big)\\]\nI used Claude 3’s help in explaining the above equations and translating them to code so I could re-run the experiment in the paper:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample x1 and x2 from Uniform(0, 1)\nx1 = np.random.uniform(0, 1, 50000)\nx2 = np.random.uniform(0, 1, 50000)\n\n# Probability Ratio\nbeta = 1.0  # Set the value of the proportionality constant\ny_pr = beta * (np.log(x1) - np.log(x2))\n\n# Odds Ratio\ny_or = np.log(x1 / (1 - x1)) - np.log(x2 / (1 - x2))\n\n# histogram bins\nbins = np.linspace(-10, 10, 100)\n\nplt.hist(y_pr, bins, alpha=0.5, label='PR')\nplt.hist(y_or, bins, alpha=0.5, label='OR')\nplt.legend(loc='upper right')\nplt.show()"
  },
  {
    "objectID": "posts/2024-04-19-orpo/index.html#trl-library-implementation",
    "href": "posts/2024-04-19-orpo/index.html#trl-library-implementation",
    "title": "Paper Math and Summary: ORPO (Odds Ratio Preference Optimization)",
    "section": "TRL Library Implementation",
    "text": "TRL Library Implementation\n\\[\\mathcal{L}_{OR} = - \\log \\sigma\\big(\\log \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)} \\big)\\]\nGiven that:\n\\[\\textbf{odds}_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}\\]\nPlugging in the odds function:\n\\[\\mathcal{L}_{OR} = - \\log \\sigma\\big(\\log \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)} \\big) = -\\log\\sigma(\\log(\\frac{\\frac{P_\\theta(y_w|x)}{1-P_\\theta(y_w|x)}}{\\frac{P_\\theta(y_l|x)}{1-P_\\theta(y_l|x)}}))\\]\nUsing log property of division:\n\\[-\\log\\sigma(\\log(\\frac{\\frac{P_\\theta(y_w|x)}{1-P_\\theta(y_w|x)}}{\\frac{P_\\theta(y_l|x)}{1-P_\\theta(y_l|x)}})) = -\\log\\sigma(\\log(\\frac{P_\\theta(y_w|x)}{1-P_\\theta(y_w|x)}) - \\log({\\frac{P_\\theta(y_l|x)}{1-P_\\theta(y_l|x)}}))\\]\nUsing log property of division again:\n\n\\[= -\\log\\sigma(\\log P_\\theta(y_w|x)- \\log(1-P_\\theta(y_w|x)) - \\log P_\\theta(y_l|x) + \\log (1-P_\\theta(y_l|x)))\\]\nRewriting so it’s cleaner:\n\n\\[-\\log\\sigma\\big(\\log P_\\theta(y_w|x)- \\log P_\\theta(y_l|x) - \\big[\\log(1-P_\\theta(y_w|x))  - \\log(1-P_\\theta(y_l|x))\\big]\\big)\\]\nIn the code below the components of this line:\nlog_odds = (policy_chosen_logps - policy_rejected_logps) - (\n            torch.log1p(-torch.exp(policy_chosen_logps)) - torch.log1p(-torch.exp(policy_rejected_logps))\n        )\ncorrespond to the following math as follows:\npolicy_chosen_logps - policy_rejected_logps\n\\[\\log P_\\theta(y_w|x)- \\log P_\\theta(y_l|x)\\]\n \ntorch.log1p(-torch.exp(policy_chosen_logps)) - torch.log1p(-torch.exp(policy_rejected_logps))\n\\[\\big[\\log(1-P_\\theta(y_w|x)) - \\log(1-P_\\theta(y_l|x))\\big]\\]\ndef odds_ratio_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        \"\"\"Compute ORPO's odds ratio (OR) loss for a batch of policy and reference model log probabilities.\n\n        Args:\n            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n\n        Returns:\n            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n            The losses tensor contains the ORPO loss for each example in the batch.\n            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.\n            The log odds ratio of the chosen responses over the rejected responses ratio for logging purposes.\n            The `log(sigmoid(log_odds_chosen))` for logging purposes.\n        \"\"\"\n\n        # Derived from Eqs. (4) and (7) from https://arxiv.org/abs/2403.07691 by using log identities and exp(log(P(y|x)) = P(y|x)\n        log_odds = (policy_chosen_logps - policy_rejected_logps) - (\n            torch.log1p(-torch.exp(policy_chosen_logps)) - torch.log1p(-torch.exp(policy_rejected_logps))\n        )\n        sig_ratio = F.sigmoid(log_odds)\n        ratio = torch.log(sig_ratio)\n        losses = self.beta * ratio\n\n        chosen_rewards = self.beta * (policy_chosen_logps.to(self.accelerator.device)).detach()\n        rejected_rewards = self.beta * (policy_rejected_logps.to(self.accelerator.device)).detach()\n\n        return losses, chosen_rewards, rejected_rewards, torch.mean(ratio).item(), torch.mean(log_odds).item()"
  },
  {
    "objectID": "posts/2024-02-29-first-live-competition/index.html",
    "href": "posts/2024-02-29-first-live-competition/index.html",
    "title": "Recap: My First Live Kaggle Competition",
    "section": "",
    "text": "After submitting predictions for a couple of closed Kaggle competitions as part of the fastai course, I participated in my first live Kaggle competition, Multi-Class Prediction of Obesity Risk. I ended up in the bottom 17% of the Private Leaderboard, ranking 2960 out of 3587. My Private ranking was 281 spots lower than my Public ranking (yikes!).\nYou can see my live competition notebook here and my post-competition notebook here.\nIn this blog post, I’ll recap my experience in this competition, and what I took away from it.\nMy main goal right now is learning, learning, learning. That being said, getting 10+ upvotes and 200+ views (probably 50 of them were mine) on my first bronze notebook felt AWESOME. I am already excited to try to get another bronze notebook. I also would like to get into the top 50% on the final Private Leaderboard for any live competition (including the playground series) this year."
  },
  {
    "objectID": "posts/2024-02-29-first-live-competition/index.html#live-competition-approach",
    "href": "posts/2024-02-29-first-live-competition/index.html#live-competition-approach",
    "title": "Recap: My First Live Kaggle Competition",
    "section": "Live Competition Approach",
    "text": "Live Competition Approach\nI decided to strictly follow the fastai textbook’s Chapter 9 approach to tabular data prediction using a Random Forest, Neural Net, and ensemble of both. I wanted to understand how each step affected the Public score.\nIn the end, I did 10 versions or iterations of my notebook that are summarized in the table below:\n\n\n\nVersion\nDescription\nPrivate Score\nPublic Score\n\n\n\n\n1\nquick and dirty RF\n0.89378\n0.89585\n\n\n2\nordered ordinal columns\n0.89495\n0.89559\n\n\n3\nhigh importance cols only\n0.89342\n0.8945\n\n\n4*\n–\n–\n–\n\n\n5\nrf with id col removed\n0.89116\n0.88728\n\n\n6\nneural net\n0.8675\n0.86452\n\n\n7^^\nrf nn ensemble\n0.8861\n0.88656\n\n\n8**\nincrease number of trees\n–\n–\n\n\n9^^\nembedding-rf nn ensemble\n0.88538\n0.89053\n\n\n\n*In version 4, I was planning to remove redundant features, but none of them were redundant so I didn’t re-train my Random Forest and didn’t submit predictions.\n**In version 8, I was planning on increasing the number of trees in my Random Forest but that didn’t improve the validation set accuracy so I didn’t submit any predictions.\n^^Selected for final leaderboard score (0.88610)"
  },
  {
    "objectID": "posts/2024-02-29-first-live-competition/index.html#why-did-i-score-so-low",
    "href": "posts/2024-02-29-first-live-competition/index.html#why-did-i-score-so-low",
    "title": "Recap: My First Live Kaggle Competition",
    "section": "Why Did I Score So Low?",
    "text": "Why Did I Score So Low?\nIt’s interesting to note that the highest Private scores were my Random Forest with ordinal columns set (0.89495) and my quick and dirty Random Forest (0.89378).\nI think the reason that the chapter’s strategies did not improve my score is because those strategies were not solely meant to improve accuracy—they were also meant to simplify the model and dataset for better interpretability. However, this dataset was small to begin with (17 independent variables) and removing low importance variables and the id column brought that down to 12 columns. Contrast that with the textbook where we went from about 70 columns to 20.\nJeremy has also mentioned throughout the course that getting that final 1% or 2% of accuracy for a Kaggle score is generally when you have to start fussing with the details. Simplifying and understanding a model (how different columns and rows of the dataset are used/affected by it) is a different problem to solve than getting 2% more to win a Kaggle competition.\nI was surprised that the neural nets performed so poorly, even in the public score. I was banking on the ensemble being more flexible than the Random Forest, and expected it to result in a higher final Private score.\nI haven’t looked at anyone else’s notebooks yet, I plan on doing that next, but I did see quite a few XGBoost-related notebook titles, and my understanding is that model performs better than Random Forests. Something I’ll practice modeling in the next tabular competition I join."
  },
  {
    "objectID": "posts/2024-02-29-first-live-competition/index.html#post-competition-analysis",
    "href": "posts/2024-02-29-first-live-competition/index.html#post-competition-analysis",
    "title": "Recap: My First Live Kaggle Competition",
    "section": "Post-Competition Analysis",
    "text": "Post-Competition Analysis\nAfter the competition was over, I decided to dig deeper into Random Forests, exploring differences in validation accuracy due to changes in parameters like n_jobs and n_estimators.\nI ended up modeling and analyzing 960 Random Forests. You can see my whole process detailed in this notebook.\nI chose 15 models out of 960 to submit to Kaggle post-competition in order to see their Private score, answering the question—should I have focused on tuning Random Forests instead of tuning an ensemble with a neural net?\nThe following table lists out my results. Here is a definition of the parameters I experimented with:\n\nn_jobs = the number of processors used (None = 1 and -1 = All)\nn_estimators = the number of trees included in the Random Forest\nmax_samples = the number of randomly selected rows included for a tree\nmax_features = the number of randomly selected columns included for a tree\nmin_samples_leaf = the minimum samples allowed on a tree node\noob_score = whether or not to use OOB score to evaluate a tree\n\n\n\n\n\n\n\n\n\n\n\n\n\nn_jobs\nn_estimators\nmax_samples\nmax_features\nmin_samples_leaf\noob_score\nPrivate score\n\n\n\n\nNone\n100\n10000\n0.5\n2\nFalse\n0.89875*\n\n\nNone\n100\n15000\n0.5\n2\nTrue\n0.89839\n\n\nNone\n60\n10000\n0.5\n2\nTrue\n0.89803\n\n\nNone\n80\n15000\n0.5\n2\nTrue\n0.89748\n\n\nNone\n100\n15000\n0.5\n5\nFalse\n0.8973\n\n\nNone\n60\n5000\nNone\n5\nTrue\n0.89297\n\n\n-1\n40\n10000\n0.5\n10\nFalse\n0.89207\n\n\n-1\n80\n10000\n0.5\n10\nFalse\n0.89197\n\n\nNone\n20\n15000\nNone\n10\nTrue\n0.89143\n\n\nNone\n80\n10000\nNone\n10\nFalse\n0.89143\n\n\nNone\n60\n1000\nNone\n10\nFalse\n0.87165\n\n\nNone\n20\n1000\nNone\n10\nFalse\n0.87039\n\n\n-1\n40\n1000\nNone\n10\nFalse\n0.86994\n\n\n-1\n60\n1000\nNone\n10\nTrue\n0.86768\n\n\nNone\n20\n1000\nNone\n10\nTrue\n0.86109\n\n\n\n*Top 65% in final leaderboard\n\nThe best post-competition result with a single Random Forest I was able to get was a Private score of 0.89875 which would have landed me in the top 65% of the final leaderboard. Not the top 50% result I’m looking for this year, but significantly better than the top 83% result I got.\nIt’s tough to tell which parameters contributed to better Private scores, I would need a larger sample to work with, but it’s interesting to note that a max_samples value of 1000 did not crack the top 10 of the 15 models listed here. Similarly, an n_estimators value of 20 or a min_samples_leaf value of 10 did not get into the top 5. I had expected that setting max_samples “too high” or setting min_samples_leaf “too low” would overfit the Random Forest. But it seems like that is not the case. At least not for this competition with this test set.\nI’ll also note that all of my submissions with a Private score of 0.89 or greater (live and post-competition) were single Random Forests, and all of them also had a Public score greater than 0.89.\nI certainly don’t feel like I can make any solid claims with this analysis about Random Forests in general, but I can say that tuning Random Forest parameters is worth exploring in a Kaggle tabular competition."
  },
  {
    "objectID": "posts/2024-02-29-first-live-competition/index.html#final-thoughts",
    "href": "posts/2024-02-29-first-live-competition/index.html#final-thoughts",
    "title": "Recap: My First Live Kaggle Competition",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAlmost every time I code something, I keep at the forefront of my mind the saying “make it work, make it right, make it fast” by Kent Beck. After this competition, I feel I landed somewhere between making it work and making it right. What is “right” when it comes to a competition? Well, at this stage of my machine learning journey, I would like to rank in the top 50% in the final Leaderboard. I was in the top 87% during the live competition, and in the top 65% post-competition, so I’m moving in the right direction.\nI also want to take all of my learnings with a grain of salt. This was one (relatively small) dataset that certainly had its own problems (as I detailed in my live competition notebook) with one (relatively small) test set. Just because my neural net didn’t perform very well doesn’t indict all neural nets. Similarly, just because my single Random Forests performed well, doesn’t mean they always will. Also, just because the textbook’s Chapter 9 approach to tabular prediction didn’t result in a top Kaggle competition Private score, doesn’t mean it’s not immensibly valuable to data science in production.\nAt the end of this year, after hopefully competing in at least a couple more live competitions, I will look back at this experience as a necessary but insufficient step towards having good intuition about machine learning.\nAs always, I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html",
    "href": "posts/2024-02-19-common-crawl/index.html",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "",
    "text": "In this blog post I’ll summarize what I learned from the paper Training Data for the Price of a Sandwich: Common Crawl’s Impact on Generative AI by Stefan Baack and Mozilla Insights. This blog post originally started as a presentation I gave to the cluster-of-stars fastai study group–I have reformatted it to a more narrative style. I have also added more detail and context, as well as my reactions and ponderings.\nThis blog post is split up into seven sections, closely following the paper’s structure:\nA couple of terms that may need to be defined for some readers:"
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html#what-is-common-crawl-cc",
    "href": "posts/2024-02-19-common-crawl/index.html#what-is-common-crawl-cc",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "What is Common Crawl (CC)?",
    "text": "What is Common Crawl (CC)?\nCC is a small (3-ish employees) nonprofit organization providing 9.5+ petabytes of freely available archive of web crawl data dating back to 2008 (250 billion web pages), with 3 to 5 billions pages added each month.\nThis data is not a single dataset. Instead, it’s offered as individual crawls of varying sizes. The data is extremely popular, cited in over 10,000 papers.\nA key point that will come up again throughout this paper is that CC data is not a representative sample of the internet. More on that later."
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html#cc-use-in-generative-ai",
    "href": "posts/2024-02-19-common-crawl/index.html#cc-use-in-generative-ai",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "CC use in Generative AI",
    "text": "CC use in Generative AI\nOne of the most impactful concepts in this paper was that of infrastructure as contrasted with data.\nMedia studies scholar Luke Munn says:\n\nOne of the things that make infrastructures so powerful is that they model their own ideals. They privilege certain logics and then operationalize them. And in this sense… they both register wider societal values and establish blueprints for how they should be carried out.\n\nCC has an infrastructural role within generative AI R&D: it provides a basis from which AI builders create training datasets. Its data is never used directly, instead, AI builders filter CC data before using it in training.\nCC data is primarily used for pre-training a model, meaning when an architecture is fed data in order to predict the next token in a sequence of tokens. During this phase, we expect the model to store patterns or associations between words in a language (like English) in the sense that given an English word or subword, it can predict the next word or subword that in a grammatically sensible way. For example, if the model is prompted “the bird is” after being pre-trained on sensible data it will likely predict the next word to be “red” or “hungry” or something sensible. This is contrasted with fine-tuning where a model that can generally predict the next token sensibly is then trained on domain-specific data to predict the next token of a particular domain (such as ornithology, the study of birds). So, if a model fine-tuned on ornithological data is prompted “a group of ravens is called” it will hopefully predict the next word as “unkindness.”\n82% of the GPT-3 tokens are from CC. More accurately speaking (see below) 60% of the training data seen by GPT-3 is from CC.\nFrom Brown et al. 2020:\n\nMore boradly speaking, of the 47 LLM papers between 2019-2023 reviewed by Stefan and Mozilla Insights, 64% used filtered CC data. The top 5-most used filtered datasets were:\n\nPile-CC (EletheurAI)\nC4 (Alphabet)\nCustom CC (meaning the AI builders filtered the data themselves)\nCCNet (Facebook)\nRealNews (UW)"
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html#ccs-mission",
    "href": "posts/2024-02-19-common-crawl/index.html#ccs-mission",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "CC’s Mission",
    "text": "CC’s Mission\nCC’s stated mission is to provide:\n\nhigh quality crawl data that was previously only available to large search engine corporations [to] small startups or even individuals\n\nFounder Gil Elbaz said in an interview (emphasis mine):\n\nI felt like a world where many companies are bringing innovation forth, across the world…is ultimately the world that I want to live in. I started to think about creating a neutral data company…that wants to democratize access to information to provide data to other companies\n\nIts guiding principle is that less curation of the provided data enables more research and innovation by downstream users.\nThe authors revisit this mission later on when discussing the relationship between CC and trustworthy AI."
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html#ccs-data",
    "href": "posts/2024-02-19-common-crawl/index.html#ccs-data",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "CC’s Data",
    "text": "CC’s Data\n\nOverview\nCC aims to support “machine scale analysis” which means automated, large-scale analysis of web data across web domains, as opposed to human scale analysis where a person (or many people) ingests information with their senses and then processes and analyzes it with their brain.\nHow does CC pick which parts of the internet to crawl? CC data consists of samples of URLs from web domains sampled from the CrawlDB, which stores 25+ billion URLs (as well as a score for each URL, when it was last fetches, whether it was successfully crawled and other fields).\nCC contains three types of data (see this site for examples) - WARC (WebARChive) files which store the raw crawl data (HTML code) - WAT (Web Archive Transformation) files which store computed metadata for the data stored in the WARC - WET (WARC Encapsulated Text) files which store extracted plaintext from the data stored in the WARC\nThe CC crawling process is designed to automatically find (a pre-defined maximum number of) new URLs considered good quality CC thinks of “quality” in terms of how CC’s data represents the web as a whole as well as the quality of the URLs included in the crawls.\nThe uncertainty of CC about how their data reflects the web as a whole is due to not knowing the size of the web as a whole. As one of CC’s staff put it:\n\nthe web is practically infinite.\n\nEarlier they mentioned that 3 to 5 billion URLs are added each month. Why not more? Because there is a tradeoff between the size of a crawl and the quality of the crawl. To expand the size of the crawl they have to include lower quality URLs, and many lower quality URLs are spam. Crawlers can get stuck in “crawler traps” which are these pockets of the internet where spam URLs are directed to one another. If a crawler gets stuck in there, potentially a majority of the crawled URLs can be spam, and the crawl data contains spammy content.\nHere’s a screenshot of information of a CC main crawl (1 TiB = 2^40 bytes, around 1100 GB):\nHere is a list of all their main crawls.\n\n\n\nCrawlDB\nURLs are added to CrawlDB during main crawls, discovery crawls (crawls with the sole purpose of fetching more URLs), and sitemap analyses.\n\na Sitemap is an XML file that lists URLs for a site along with additional metadata about each URL (when it was last updated, how often it usually changes, and how important it is, relative to other URLs in the site)\n\n\n\nHarmonic Centrality Score\nHow are URLs scored? and thereby sampled to include in the next crawl? With the Harmonic Centrality Score.\nThe Harmonic Centrality Score measures the importance of a node in a network based on its distance all other nodes. - Shorter distance = higher score. - More direct and indirect links to a domain = higher score. - Captures how accessible a domain is to other web pages.\n“High quality” implies a higher Harmonic Centrality Score. The score for a URL is increased if the URL has never been crawled before or hasn’t been crawled in awhile."
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html#filtering-cc-for-ai",
    "href": "posts/2024-02-19-common-crawl/index.html#filtering-cc-for-ai",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "Filtering CC for AI",
    "text": "Filtering CC for AI\nAs mentioned earlier, AI builders filter CC data before using it to pre-train their models. This section goes into more detail around filtering.\n\nTypes of filtering\n\nBy language (most of CC data is in English)\nKeywords or simple heuristics (only keep lines that end in a punctuation mark, or remove documents with certain keywords)\n\nThe List of Dirty, Naughty, Obscene, and Otherwise Bad Words used for C4 dataset is problematic because the words included in that list are not inherently “bad” or “harmful”. It depends on context. For example, here are some words included in this list of “bad words” that are not “bad” given a particular context:\n\n“domination” (filtering out webpages that have the word “domination” will exclude pages with a discussion about domination of one group or system on another)\n“sexuality” and similar terms are of course normal and healthy words to use in many contexts.\nanatomical words (“penis”, “vagina”, “clitoris”, “vulva”) are all perfectly “good” words in many contexts. Furthermore, the censorship of female sexuality is perpetuated by the inclusion of those words in this “bad word” list.\nslurs reclaimed by racial and gender/sex minorities are used in non-derogatory ways in their communities and cultures—exluding these words excludes their representation in the data.\n\n\nAI classifiers (only keeps documents statistically similar to reference dataset)\n\nPile-CC (EletheurAI) uses an AI classifier trained on OpenWebText2 (deduplicated Reddit comments with 3+ upvotes). Most Reddit users are male and white so this is not a representative dataset of the global population. Reddit has also struggled moderating toxicity.\nGPT-3 is pre-trained on CC filtered by using a classifier trained on WebText as a proxy for “high-quality” documents. Documents that are similar to WebText are deemed “low quality”.\n\nDeduplication (remove one document if it is exactly he same or similar to another—“similar” in a statistical sense)\n\nGPT-3 is pre-trained on CC data that was filtered to remove documents with high overlap with other documents (fuzzy deduplication).\n\n\n\n\n(In)adequacy of Filtering Methods\nThere is a fundamental unresolved conflict or dilemma: the amount of data desired is too large for manual curation but automated filtering for toxicity and bias are significantly limited. The authors discuss solutions to this later on."
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html#cc-and-trustworthy-ai",
    "href": "posts/2024-02-19-common-crawl/index.html#cc-and-trustworthy-ai",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "CC and Trustworthy AI",
    "text": "CC and Trustworthy AI\nMozilla defines Trustworthy AI is AI that is:\n\ndemonstrably worthy of trust, tech that considers accountability, agency, and individual and collective well-being…[trustworthy] AI-driven products and services are designed with human agency and accountability from the beginning\n\nWhat stood out to me from this definition were the terms accountability, agency and well-being.\n\nUpside of CC\nThe filtered CC versions used in LLM training are inherently more auditable than any proprietary training datasets because CC data is freely accessible online.\nLLMs open and transparent about their data typically come from outside of Big Tech (e.g., Bloom), achieving CC’s mission of making this data accessible to small startups and individuals.\n\n\nDownsides of CC\nWhile filtered CC data is more auditable than proprietary datasets, AI builders don’t necessarily take the opportunity to be transparent about their CC use. In other words, what use is this auditability if how this freely accessible data is filtered is not disclosed?\nThe size and diversity of CC makes it hard to understand what an LLM is trained on. This is reinforced by the (false) assumption among some AI builders that CC represents the “entire internet” and somehow is a proxy for representing “all human knowledge”. CC staff explicitly state:\n\nOften it is claimed that Common Crawl contains the entire web, but that’s absolutely not true. Based on what I know about how many URLs exist, it’s very, very small.\n\nTraining of generative AI on massive amounts of copyrighted material could trend towards making the internet less open and collaborative (“data revolts”: when content platforms block crawlers to protect their data). Note that CC stays within the bounds of US fair use policy for copyrighted materials. It only copies HTML code, no images or media or full copies of domains.\n\nHere’s an example of how content platforms block crawlers, from NY Times’ robot.txt:\nUser-agent: CCBot\nDisallow: /\n\n\nRecommendations for using CC to train AI\n\nPut more effort into filtering CC. Filter more types of problematic content (e.g., content that is racist or mysoginist).\nProblematic content should be annotated (if it’s not filtered out). There are some models who are trained on problematic content in order to better detect it. These models will need to be trained on problematic data.\nConsistently provide proper dataset documentation. (See “Dataset Audit Card” example on page 7 of Large Datasets: A Pyrrhic Win for Computer Vision?)\n\n\n\nRecommendations for LLM-based end-user products\n\nBetter industry standards and government regulation for evaluating filtered CC versions and downstream model effects.\nMore nuanced, culturally contextual tools to evaluate profanity, racism, discrimination, etc. found in the datasets.\nA descriptive demographic overview of the dataset content (e.g., what region and culture does this data represent?)\nEvaluations by human moderators under fair, safe conditions (‘It’s destroyed me completely’: Kenyan moderators decry toll of training of AI models).\nEvaluating the effects of individual datasets on model behavior (like EletheurAI’s Language Model Evaluation Harness).\nTrustworthy intermediaries who filter CC for various purposes (e.g., subject matter experts or cultural experts who can curate data to match their subject or culture appropriately)."
  },
  {
    "objectID": "posts/2024-02-19-common-crawl/index.html#the-future-of-cc",
    "href": "posts/2024-02-19-common-crawl/index.html#the-future-of-cc",
    "title": "Paper Summary: Training Data for the Price of a Sandwich",
    "section": "The Future of CC",
    "text": "The Future of CC\n\nCC’s Shortcomings\n\nCC is not a “neutral data” organization as its samples are not representative of the web and because the web is not representative of all people (about 40% or 3 billion people in the world do not have internet access). I would go further and say that there is no such thing as neutral data, even raw data is not neutral because data collection, and the environment within which data is collected is not neutral.\nCC’s lack of transparency (around its data governance) is at odds with its self-image as a public resource. For a long time, there was almost no public communication from CC outside of its mailing list (which mostly dealt with technical questions) and its blog (mostly dedicated to announcing new crawl data).\n\n\n\nRecommendations for CC\n\nAdd a Terms of Use to the data. If AI builders want to use your data, they should have to document their filtering methodology, and take approaches to better filter (or annotate) their data for problematic, biased and harmful content.\nCC should conduct more curated, values-oriented crawls so that digitally marginalized communities are more included. Since a URL’s quality is determined by its Harmonic Centrality Score, and since that score is determined by how accessible the URL is to other URLs, URLs from communities without socioeconomic power and/or resources will not be deemed “accessible” as such and will be scored low. Additionally, many communities will post popular links to Facebook, but because it doesn’t allow crawlers, CC won’t get to see that URL.\nAdd a community-driven approach to identify relevant content for crawls. Let the people themselves tell you directly what content matters to them and represents their interests and cultures.\nProvide quality and toxicity evaluations, or language labeling.\nCreate a formal way to make requests about crawls.\nProvide educational resources about the limitations of CC data.\nFoster discussions of filtering and data analysis tools.\nIncrease the number and diversity of high-quality datasets curated by humans equitably. In other words, it’s okay if these datasets are small if they are high quality and there are a lot of them.\n\n\n\nFinal Thoughts\nI really enjoyed this paper. I came away from it inspired and empowered. If we can put our heads together and expand the filtered CC data space to include more intentional and representative data about cultures, topics and ideologies that are either ignored or filtered out in the most popular datasets today, we can reshape how LLMs predict the next token.\nAs always, I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-07-13-movielens25m/index.html",
    "href": "posts/2024-07-13-movielens25m/index.html",
    "title": "Training Models on the MovieLens 25M Dataset",
    "section": "",
    "text": "In this notebook I’ll work through the following prompt from the “Further Research” section of Chapter 8 (Collaborative Filtering) from the fastai textbook:\n\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forums for ideas. Note that there are more columns in the full dataset–see if you can use those too (the next chapter might give you ideas).\n\nHere’s a summary of my results in this notebook:\n\n\n\nArch\nMetric\nMetric Value\n\n\n\n\nDotProductBias\nMSE\n0.654875\n\n\nDotProductBiasCE\nAccuracy\n35%\n\n\nRandom Forest (baseline)\nAccuracy\n29%\n\n\nRandom Forest (additional columns)\nAccuracy\n30%\n\n\nNeural Net\nAccuracy\n38%"
  },
  {
    "objectID": "posts/2024-07-13-movielens25m/index.html#load-the-data",
    "href": "posts/2024-07-13-movielens25m/index.html#load-the-data",
    "title": "Training Models on the MovieLens 25M Dataset",
    "section": "Load the Data",
    "text": "Load the Data\nThe data is formatted slightly differently than the 100k subset (main difference is the columns are labeled differently).\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\npath = Path('/content/drive/MyDrive/movielens25m')\npath.ls()\n\n(#12) [Path('/content/drive/MyDrive/movielens25m/tags.csv'),Path('/content/drive/MyDrive/movielens25m/ratings.csv'),Path('/content/drive/MyDrive/movielens25m/movies.csv'),Path('/content/drive/MyDrive/movielens25m/genome-tags.csv'),Path('/content/drive/MyDrive/movielens25m/genome-scores.csv'),Path('/content/drive/MyDrive/movielens25m/links.csv'),Path('/content/drive/MyDrive/movielens25m/README.txt'),Path('/content/drive/MyDrive/movielens25m/rf_baseline_vars.pkl'),Path('/content/drive/MyDrive/movielens25m/rf_additional_vars.pkl'),Path('/content/drive/MyDrive/movielens25m/to_nn.pkl')...]\n\n\n\nratings = pd.read_csv(path/'ratings.csv')\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      1\n      296\n      5.0\n      1147880044\n    \n    \n      1\n      1\n      306\n      3.5\n      1147868817\n    \n    \n      2\n      1\n      307\n      5.0\n      1147868828\n    \n    \n      3\n      1\n      665\n      5.0\n      1147878820\n    \n    \n      4\n      1\n      899\n      3.5\n      1147868510\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nratings['movieId'].unique().shape, ratings['userId'].unique().shape\n\n((59047,), (162541,))\n\n\n\nmovies = pd.read_csv(path/'movies.csv')\nmovies.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      movieId\n      title\n      genres\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n      Adventure|Animation|Children|Comedy|Fantasy\n    \n    \n      1\n      2\n      Jumanji (1995)\n      Adventure|Children|Fantasy\n    \n    \n      2\n      3\n      Grumpier Old Men (1995)\n      Comedy|Romance\n    \n    \n      3\n      4\n      Waiting to Exhale (1995)\n      Comedy|Drama|Romance\n    \n    \n      4\n      5\n      Father of the Bride Part II (1995)\n      Comedy\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nmovies['movieId'].unique().shape\n\n(62423,)\n\n\n\nratings = ratings.merge(movies[['movieId', 'title']])\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      1\n      296\n      5.0\n      1147880044\n      Pulp Fiction (1994)\n    \n    \n      1\n      3\n      296\n      5.0\n      1439474476\n      Pulp Fiction (1994)\n    \n    \n      2\n      4\n      296\n      4.0\n      1573938898\n      Pulp Fiction (1994)\n    \n    \n      3\n      5\n      296\n      4.0\n      830786155\n      Pulp Fiction (1994)\n    \n    \n      4\n      7\n      296\n      4.0\n      835444730\n      Pulp Fiction (1994)\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=1024)\ndls.show_batch()\n\n\n\n  \n    \n      \n      userId\n      title\n      rating\n    \n  \n  \n    \n      0\n      18382\n      Goldfinger (1964)\n      3.0\n    \n    \n      1\n      47473\n      Eyes Wide Shut (1999)\n      0.5\n    \n    \n      2\n      132661\n      Garden State (2004)\n      3.0\n    \n    \n      3\n      68944\n      X-Men Origins: Wolverine (2009)\n      0.5\n    \n    \n      4\n      126422\n      Animal Kingdom (2010)\n      3.5\n    \n    \n      5\n      122810\n      Hotel Rwanda (2004)\n      3.5\n    \n    \n      6\n      8458\n      Sherlock Holmes (2009)\n      4.0\n    \n    \n      7\n      21172\n      Indiana Jones and the Temple of Doom (1984)\n      4.0\n    \n    \n      8\n      94712\n      Dark Knight, The (2008)\n      3.5\n    \n    \n      9\n      88335\n      Chicken Run (2000)\n      2.0\n    \n  \n\n\n\n\ndls.classes.keys()\n\ndict_keys(['userId', 'title'])\n\n\n\nn_users = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\n\nn_users, n_movies\n\n(162542, 58959)"
  },
  {
    "objectID": "posts/2024-07-13-movielens25m/index.html#training-using-different-approaches",
    "href": "posts/2024-07-13-movielens25m/index.html#training-using-different-approaches",
    "title": "Training Models on the MovieLens 25M Dataset",
    "section": "Training Using Different Approaches",
    "text": "Training Using Different Approaches\n\nDotProductBias with Embeddings\nThe first architecture I’ll use is the DotProductBias with Embeddings:\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n\n        return sigmoid_range(res, *self.y_range)\n\nI’ll use the same number of epochs, learning rate and weight decay as the textbook training example:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.718568\n      0.750686\n      1:31:29\n    \n    \n      1\n      0.721771\n      0.743393\n      1:50:58\n    \n    \n      2\n      0.675583\n      0.713021\n      1:51:44\n    \n    \n      3\n      0.627697\n      0.671975\n      1:48:30\n    \n    \n      4\n      0.608647\n      0.654875\n      1:41:17\n    \n  \n\n\n\nWhen using the 100k subset the lowest validation MSE I got was about 0.836. A validation MSE of 0.654875 is about a 22% reduction.\nAfter rounding the predictions to the nearest 0.5, the model has a validation accuracy of about 30%. Yikes! That’s terrible.\n\npreds, targs = learn.get_preds(dl=dls.valid)\n\n\n\n\n\n\n\n\n\npreds\n\ntensor([[3.1825],\n        [3.1959],\n        [3.6061],\n        ...,\n        [1.6408],\n        [3.3054],\n        [3.4723]])\n\n\n\nrounded_preds = (preds / 0.5).round() * 0.5\nrounded_preds\n\ntensor([[3.0000],\n        [3.0000],\n        [3.5000],\n        ...,\n        [1.5000],\n        [3.5000],\n        [3.5000]])\n\n\n\ntargs\n\ntensor([[3.0000],\n        [5.0000],\n        [3.5000],\n        ...,\n        [1.0000],\n        [2.0000],\n        [3.5000]])\n\n\n\n(rounded_preds == targs).float().mean()\n\ntensor(0.2931)\n\n\nIf I round to the nearest integer, the validation accuracy increases to about 36%. Still not great.\n\n(preds.round(decimals=0) == targs).float().mean()\n\ntensor(0.3581)\n\n\nPlotting predictions versus the targets shows the weak relationship between the two:\n\ndef plot_preds_v_targs(preds, targs):\n  plt.figure(figsize=(10, 6))\n  plt.scatter(targs.detach().numpy().squeeze(), preds.detach().numpy().squeeze(), alpha=0.5)\n  plt.xlabel('Targets')\n  plt.ylabel('Predictions')\n  plt.title('Predictions vs Targets')\n  plt.show()\n\n\nplot_preds_v_targs(preds, targs)\n\n\n\n\nHere’s the distribution of the ratings targets for the ~5M validation records:\n\nplt.hist(targs.detach().numpy().squeeze());\n\n\n\n\nThere are considerably fewer predictions less than 3 and greater than 4:\n\nplt.hist(preds.detach().numpy().squeeze());\n\n\n\n\nLet’s hope for better luck with other architectures!\n\n\nDotProductBiasCE (for Cross Entropy Loss)\nI’ll use the same architecture that I created for another Further Research prompt, with the slight modification that instead of projecting the dot product to 5 ratings I’ll project them to 10 ratings (as there are ten 0.5-increment ratings in the dataset: 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0).\n\nclass DotProductBiasCE(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.linear = nn.Linear(1, 10)\n\n  def forward(self, x_cat, x_cont):\n    x = x_cat\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return self.linear(res)\n\nI’ll use the same training setup as I did with the 100k subset, but with a larger batch size (otherwise it takes much longer to train). Note that using the same learning rate for a batch size of 1024 as a batch size of 64 will likely not result in optimal training.\n\ndls = TabularDataLoaders.from_df(\n    ratings[['userId', 'title', 'rating']],\n    procs=[Categorify],\n    cat_names=['userId','title'],\n    y_names=['rating'],\n    y_block=CategoryBlock,\n    bs=1024)\n\n\nb = dls.one_batch()\nlen(b), b[0].shape, b[1].shape, b[2].shape\n\n(3, torch.Size([1024, 2]), torch.Size([1024, 0]), torch.Size([1024, 1]))\n\n\n\ndls.vocab\n\n[0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      userId\n      title\n      rating\n    \n  \n  \n    \n      0\n      64415\n      Jumpin' Jack Flash (1986)\n      2.0\n    \n    \n      1\n      10508\n      Lord of the Rings: The Return of the King, The (2003)\n      4.5\n    \n    \n      2\n      126649\n      Frances Ha (2012)\n      4.0\n    \n    \n      3\n      119566\n      Elizabeth (1998)\n      3.0\n    \n    \n      4\n      77160\n      Snake Eyes (1998)\n      5.0\n    \n    \n      5\n      99259\n      Untouchables, The (1987)\n      3.5\n    \n    \n      6\n      3726\n      Myth of Fingerprints, The (1997)\n      2.0\n    \n    \n      7\n      100959\n      Meet the Parents (2000)\n      3.5\n    \n    \n      8\n      134993\n      Nightmare on Elm Street, A (1984)\n      1.0\n    \n    \n      9\n      117798\n      Doubt (2008)\n      4.0\n    \n  \n\n\n\n\nn_users = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\n\nn_users, n_movies\n\n(162542, 58959)\n\n\nTraining with Cross Entropy Loss on the 25M dataset resulted in a model with about 35% validation accuracy, about 6% less than the 41% achieved on the 100k subset. The model is not showing signs of overfitting so I could have trained it for more epochs and potentially gained more accuracy.\n\nmodel = DotProductBiasCE(n_users, n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 0.1, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.919933\n      1.924016\n      0.288326\n      1:04:57\n    \n    \n      1\n      1.914961\n      1.927970\n      0.284413\n      1:33:05\n    \n    \n      2\n      1.900328\n      1.901067\n      0.294077\n      1:19:56\n    \n    \n      3\n      1.837524\n      1.847121\n      0.313432\n      1:40:26\n    \n    \n      4\n      1.704779\n      1.740781\n      0.354360\n      1:18:08\n    \n  \n\n\n\n3’s and 4’s were the most correctly predicted ratings by this model, with the model performing quite badly for other ratings—in particular, the model did not predict any 0.5, 1.5, 2.5, 3.5, or 4.5 ratings.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6, 6))\n\n\n\n\n\n\n\n\n\n\n\n\npreds, targs = learn.get_preds(dl=learn.dls.valid)\n\n\n\n\n\n\n\n\nBy far the most common predicted rating was 4.0 (the 7th category in the vocab). Again, I’ll note the gaps between the bars where the 0.5-increment ratings are absent from the model’s predictions.\n\nplt.hist(preds.argmax(dim=1));\n\n\n\n\nWhile the most common target is also 4.0, note that it’s frequency is about half that in the prediction distribution.\n\nplt.hist(targs.squeeze());\n\n\n\n\n\n\nRandom Forest (baseline)\nAs an additional exercise I’ll train a random forest on the userId, movieId and rating fields. In the following section, I’ll add some of the additional fields available and see if that improves the forest’s performance. I’ll follow the approach given in Chapter 9 of the fastai textbook.\n\nSetup\nI’ll start by creating a TabularPandas object with a random split:\n\nsplits = RandomSplitter(seed=42)(range_of(ratings))\n\n\nlen(splits), len(splits[0]), len(splits[1])\n\n(2, 20000076, 5000019)\n\n\n\nto = TabularPandas(\n    ratings[['userId', 'title', 'rating']],\n    procs=[Categorify, FillMissing],\n    cat_names=['userId', 'title'],\n    cont_names=None,\n    y_names='rating',\n    y_block=CategoryBlock,\n    splits=splits)\n\n\nlen(to.train), len(to.valid)\n\n(20000076, 5000019)\n\n\n\nto.show(3)\n\n\n\n  \n    \n      \n      userId\n      title\n      rating\n    \n  \n  \n    \n      8613915\n      11056\n      Divergent (2014)\n      2.0\n    \n    \n      20221395\n      128803\n      Town, The (2010)\n      4.0\n    \n    \n      21140474\n      56442\n      Jack Ryan: Shadow Recruit (2014)\n      3.0\n    \n  \n\n\n\n\nto.items.head(3) # coded values\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      title\n      rating\n    \n  \n  \n    \n      8613915\n      11056\n      13670\n      3\n    \n    \n      20221395\n      128803\n      53868\n      7\n    \n    \n      21140474\n      56442\n      24434\n      5\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nto.vocab # 10 possible ratings\n\n[0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\n\n# defining variables\nxs,y = to.train.xs, to.train.y\nvalid_xs,valid_y = to.valid.xs, to.valid.y\n\nxs.shape, y.shape, valid_xs.shape, valid_y.shape\n\n((20000076, 2), (20000076,), (5000019, 2), (5000019,))\n\n\n\n#save_pickle(path/'rf_baseline_vars.pkl', (xs, y, valid_xs, valid_y))\n\nI’ll create helper functions to calculate accuracy of the model:\n\ndef acc(pred,y): return (pred == y).mean()\ndef m_acc(m, xs, y): return acc(m.predict(xs), y)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef rf(xs, y, n_estimators=4, max_samples=10_000, max_features=0.5,\n       min_samples_leaf=5, **kwargs):\n  return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples,\n              max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\n\n\nTraining Results\nSince training on the full data will likely take awhile, I’ll first fit a random forest with 4 trees and a max of ten thousand samples for each tree, which takes about a minute to train and results in a 22% validation accuracy which is not bad!\n\nm = rf(xs, y, n_estimators=4, max_samples=10_000)\n\n\nm_acc(m, xs, y), m_acc(m, valid_xs, valid_y)\n\n(0.22450199689241182, 0.22447194700660136)\n\n\nDoubling the number of trees (to 8) increases the validation accuracy to 24% (+2%).\n\nm = rf(xs, y, n_estimators=8, max_samples=10_000)\nm_acc(m, xs, y), m_acc(m, valid_xs, valid_y)\n\n(0.24051508604267305, 0.24045988625243225)\n\n\nTripling the number of trees (to 12) gives a smaller boost (1%) to the validation accuracy (25%).\n\nm = rf(xs, y, n_estimators=12, max_samples=10_000)\nm_acc(m, xs, y), m_acc(m, valid_xs, valid_y)\n\n(0.25057739780588834, 0.25026804898141386)\n\n\nAs the number of samples increases by 10_000 (while keeping n_estimators=4) the validation accuracy increases by about 0.2-0.5% each time.\n\nfor samples in [20_000, 30_000, 40_000]:\n  m = rf(xs, y, n_estimators=4, max_samples=samples)\n  print(f'samples: {samples}; train acc: {m_acc(m, xs, y)}; valid acc: {m_acc(m, valid_xs, valid_y)}')\n\nsamples: 20000; train acc: 0.23020222523154413; valid acc: 0.22950612787671407\nsamples: 30000; train acc: 0.23159146995241417; valid acc: 0.2313785207616211\nsamples: 40000; train acc: 0.23347261280407133; valid acc: 0.23300171459348454\n\n\nNext, I’ll train a random forest using the parameters given in Chapter 9 of the text (40 trees, 200_000 samples), keeping in mind that was for a 400k row dataset, so not optimized for 25M rows of data. I’ll then double n_estimators and max_samples to see which combination works best. I’m not using a for-loop like above since my Colab instance kept crashing so I’ll fit the different random forests in individual cells.\n40 trees and 200_000 samples results in a validation accuracy of 28% (+3% from the previous best achieved by 12 trees and 10_000 samples).\n\ntrees = 40\nsamples = 200_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 200000; trees: 40; train acc: 0.29; valid acc: 0.28\n\n\nDoubling the number of trees from 40 to 80 results in a 29% validation accuracy (+1%). It took about 35 minutes to train and predict.\n\ntrees = 80\nsamples = 200_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 200000; trees: 80; train acc: 0.29; valid acc: 0.29\n\n\nDoubling the number of samples used to 400_000 while keeping the number of trees at 40 achieves the same validation accuracy (29%) and took about 21 minutes for training and inference.\n\ntrees = 40\nsamples = 400_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 400000; trees: 40; train acc: 0.30; valid acc: 0.29\n\n\nDoubling the number of trees to 80 with 400_000 samples achieves the same accuracy of 29% (while taking 42 minutes for training and inference).\n\ntrees = 80\nsamples = 400_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 400000; trees: 80; train acc: 0.30; valid acc: 0.29\n\n\nI’ll increase the number of samples significantly to 2_000_000. I’ll keep the number of trees at 40 for now since increasing that number doesn’t seem to improve validation accuracy significantly.\nThis results in a validation accuracy of 29%.\nIt doesn’t seem like increasing the number of trees or samples will significantly change the validation accuracy.\n\ntrees = 40\nsamples = 2_000_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 2000000; trees: 40; train acc: 0.33; valid acc: 0.29\n\n\n\ndef rf_feat_importance(m, df):\n  return pd.DataFrame({'cols': df.columns, 'imp': m.feature_importances_}\n                      ).sort_values('imp', ascending=False)\n\nWith this data and model, userId is almost twice as important as the movie title.\n\nrf_feat_importance(m, xs)\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      0\n      userId\n      0.640407\n    \n    \n      1\n      title\n      0.359593\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n\nRandom Forest (with additional data)\nThere a few additional columns available that may improve the performance of the random forest:\n\nratings.csv has a timestamp column.\n\nThis is easy to incorporate as there is one value per rating.\n\nmovies.csv has a genres column.\n\nThere are multiple pipe-separated genres per movie, I’ll pick one genre per movie.\n\ntags.csv has tags associated with each movie by users.\n\nThere are multiple tags for each movie/user pair, I’ll pick one tag per user/movie pair.\n\ngenome-scores.csv has genome-tags associated with each movie.\n\nThere are multiple genome-tags per movie, so I’ll pick the genome-tag with the highest score for each movie.\n\n\n\n# do str.split on `movies` before merging with `ratings` since it has fewer rows\nmovies['genres'] = movies['genres'].str.split('|', n=1).str[0]\n\n\nratings = ratings.merge(movies)\n\n\nratings.head() # peep the new `genres` column with a single genre\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      timestamp\n      title\n      genres\n    \n  \n  \n    \n      0\n      1\n      296\n      5.0\n      1147880044\n      Pulp Fiction (1994)\n      Comedy\n    \n    \n      1\n      3\n      296\n      5.0\n      1439474476\n      Pulp Fiction (1994)\n      Comedy\n    \n    \n      2\n      4\n      296\n      4.0\n      1573938898\n      Pulp Fiction (1994)\n      Comedy\n    \n    \n      3\n      5\n      296\n      4.0\n      830786155\n      Pulp Fiction (1994)\n      Comedy\n    \n    \n      4\n      7\n      296\n      4.0\n      835444730\n      Pulp Fiction (1994)\n      Comedy\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nratings.shape\n\n(25000095, 6)\n\n\nOnly a fraction (~10%) of all of the ratings.csv userIds are captured in tags.csv, whereas about 75% of the movieIds are captured in tags.csv. I’ll pick the most frequent tag for each movie and merge with the ratings data.\n\ntags = pd.read_csv(path/'tags.csv')\n\n\nratings['userId'].unique().shape, tags['userId'].unique().shape\n\n((162541,), (14592,))\n\n\n\nratings['movieId'].unique().shape, tags['movieId'].unique().shape\n\n((59047,), (45251,))\n\n\nThere’s about 8k tags that are different only because of capitalization—I’ll set all tags to lower case:\n\ntags['tag'].unique().shape, tags['tag'].str.lower().unique().shape\n\n((73051,), (65465,))\n\n\n\ntags['tag'] = tags['tag'].str.lower()\n\n\n# thanks Claude 3.5 Sonnet\n# this was MUCH faster than using groupby + agg\nmost_common_tags = (\n    tags.groupby(['movieId', 'tag'])\n    .size()\n    .reset_index(name='count')\n    .sort_values(['movieId', 'count'], ascending=[True, False])\n    .drop_duplicates('movieId')\n    .drop('count', axis=1)\n)\n\nmost_common_tags.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      movieId\n      tag\n    \n  \n  \n    \n      78\n      1\n      pixar\n    \n    \n      155\n      2\n      robin williams\n    \n    \n      168\n      3\n      fishing\n    \n    \n      185\n      4\n      chick flick\n    \n    \n      207\n      5\n      steve martin\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nratings.shape\n\n(25000095, 6)\n\n\n\nratings = ratings.merge(most_common_tags, on=['movieId'], how='left')\nprint(ratings.shape)\nratings.head()\n\n(25000095, 7)\n\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      timestamp\n      title\n      genres\n      tag\n    \n  \n  \n    \n      0\n      1\n      296\n      5.0\n      1147880044\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n    \n    \n      1\n      3\n      296\n      5.0\n      1439474476\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n    \n    \n      2\n      4\n      296\n      4.0\n      1573938898\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n    \n    \n      3\n      5\n      296\n      4.0\n      830786155\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n    \n    \n      4\n      7\n      296\n      4.0\n      835444730\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nOnly a very small fraction of ratings are without a tag:\n\nratings['tag'].isna().sum()\n\n83417\n\n\n\ngenome_scores = pd.read_csv(path/'genome-scores.csv')\n\n\ngenome_tags = pd.read_csv(path/'genome-tags.csv')\n\n\ngenome_scores.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      movieId\n      tagId\n      relevance\n    \n  \n  \n    \n      0\n      1\n      1\n      0.02875\n    \n    \n      1\n      1\n      2\n      0.02375\n    \n    \n      2\n      1\n      3\n      0.06250\n    \n    \n      3\n      1\n      4\n      0.07575\n    \n    \n      4\n      1\n      5\n      0.14075\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ngenome_tags.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      tagId\n      tag\n    \n  \n  \n    \n      0\n      1\n      007\n    \n    \n      1\n      2\n      007 (series)\n    \n    \n      2\n      3\n      18th century\n    \n    \n      3\n      4\n      1920s\n    \n    \n      4\n      5\n      1930s\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nI’ll use the most relevant genome tag for each movie:\n\nmost_common_genomes = (\n    genome_scores.sort_values(['movieId', 'relevance'], ascending=[True, False])\n    .drop_duplicates('movieId')\n)\n\nmost_common_genomes.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      movieId\n      tagId\n      relevance\n    \n  \n  \n    \n      1035\n      1\n      1036\n      0.99925\n    \n    \n      1156\n      2\n      29\n      0.97600\n    \n    \n      3156\n      3\n      901\n      0.97525\n    \n    \n      4499\n      4\n      1116\n      0.97525\n    \n    \n      5412\n      5\n      901\n      0.96025\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ngenome_scores['movieId'].unique().shape, most_common_genomes.shape\n\n((13816,), (13816, 3))\n\n\nWith that sorted, I’ll get the actual tag text:\n\nmost_common_genomes = most_common_genomes.merge(genome_tags)\nmost_common_genomes.shape\n\n(13816, 4)\n\n\n\nmost_common_genomes = most_common_genomes.rename(columns={'tag': 'genome_tag'})\nmost_common_genomes.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      movieId\n      tagId\n      relevance\n      genome_tag\n    \n  \n  \n    \n      0\n      1\n      1036\n      0.99925\n      toys\n    \n    \n      1\n      1920\n      1036\n      0.99575\n      toys\n    \n    \n      2\n      3114\n      1036\n      0.99850\n      toys\n    \n    \n      3\n      78499\n      1036\n      0.99875\n      toys\n    \n    \n      4\n      81981\n      1036\n      0.84225\n      toys\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nmost_common_genomes['movieId'].unique().shape\n\n(13816,)\n\n\nand add it to the main DataFrame:\n\nratings.shape\n\n(25000095, 7)\n\n\n\nratings = ratings.merge(most_common_genomes[['movieId', 'genome_tag']], how='left')\nratings.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      timestamp\n      title\n      genres\n      tag\n      genome_tag\n    \n  \n  \n    \n      0\n      1\n      296\n      5.0\n      1147880044\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      1\n      3\n      296\n      5.0\n      1439474476\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      2\n      4\n      296\n      4.0\n      1573938898\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      3\n      5\n      296\n      4.0\n      830786155\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      4\n      7\n      296\n      4.0\n      835444730\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWith the DataFrame established, I’ll create a TabularPandas object using the same seed (42) as before for splits.\n\nsplits = RandomSplitter(seed=42)(range_of(ratings))\nlen(splits), len(splits[0]), len(splits[1])\n\n(2, 20000076, 5000019)\n\n\n\nto = TabularPandas(\n    ratings[['userId', 'title', 'timestamp', 'genres', 'tag', 'genome_tag', 'rating']],\n    procs=[Categorify, FillMissing],\n    cat_names=['userId', 'title', 'genres', 'tag', 'genome_tag'],\n    cont_names=['timestamp'],\n    y_names='rating',\n    y_block=CategoryBlock,\n    splits=splits)\n\n\nlen(to.train), len(to.valid)\n\n(20000076, 5000019)\n\n\n\nto.show(3)\n\n\n\n  \n    \n      \n      userId\n      title\n      genres\n      tag\n      genome_tag\n      timestamp\n      rating\n    \n  \n  \n    \n      8613915\n      11056\n      Divergent (2014)\n      Adventure\n      dystopia\n      vampire human love\n      1422282990\n      2.0\n    \n    \n      20221395\n      128803\n      Town, The (2010)\n      Crime\n      ben affleck\n      crime\n      1312046717\n      4.0\n    \n    \n      21140474\n      56442\n      Jack Ryan: Shadow Recruit (2014)\n      Action\n      cia\n      tom clancy\n      1491438306\n      3.0\n    \n  \n\n\n\n\nto.items.head(3) # coded values\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      title\n      timestamp\n      genres\n      tag\n      genome_tag\n      rating\n    \n  \n  \n    \n      8613915\n      11056\n      13670\n      1422282990\n      3\n      4536\n      791\n      3\n    \n    \n      20221395\n      128803\n      53868\n      1312046717\n      7\n      1983\n      183\n      7\n    \n    \n      21140474\n      56442\n      24434\n      1491438306\n      2\n      3241\n      769\n      5\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nxs,y = to.train.xs, to.train.y\nvalid_xs,valid_y = to.valid.xs, to.valid.y\n\nxs.shape, y.shape, valid_xs.shape, valid_y.shape\n\n((20000076, 6), (20000076,), (5000019, 6), (5000019,))\n\n\nUsing the additional columns has increased the validation accuracy from 22% (when using only movieId and userId) to 23.7% (when using the additional columns timestamp, tag, genome_tag and genres).\n\nm = rf(xs, y, n_estimators=4, max_samples=10_000)\nm_acc(m, xs, y), m_acc(m, valid_xs, valid_y)\n\n(0.23697224950545187, 0.23664510074861717)\n\n\nDoubling the number of trees to 8 yields a validation accuracy of 25% (a ~1% increase from before).\n\nm = rf(xs, y, n_estimators=8, max_samples=10_000)\nm_acc(m, xs, y), m_acc(m, valid_xs, valid_y)\n\n(0.2500907496551513, 0.24987865046112825)\n\n\nTripling the number of trees to 12 yields a validation accuracy of 26% (a ~1% increase from before).\n\nm = rf(xs, y, n_estimators=12, max_samples=10_000)\nm_acc(m, xs, y), m_acc(m, valid_xs, valid_y)\n\n(0.25923971488908343, 0.2590440156327406)\n\n\nFor every 10_000 sample increase (with 4 trees) the validation accuracy improves between 0.02-0.6%.\n\nfor samples in [20_000, 30_000, 40_000]:\n  m = rf(xs, y, n_estimators=4, max_samples=samples)\n  print(f'samples: {samples}; train acc: {m_acc(m, xs, y)}; valid acc: {m_acc(m, valid_xs, valid_y)}')\n\nsamples: 20000; train acc: 0.24337092519048428; valid acc: 0.24275807751930542\nsamples: 30000; train acc: 0.2467036625260824; valid acc: 0.24597866528107193\nsamples: 40000; train acc: 0.24714531084781877; valid acc: 0.24619486445951505\n\n\nIncreasing the number of trees to 40 and the number of samples to 200_000 results in a validation accuracy of 30% (the baseline random forest validation accuracy was 28%).\n\ntrees = 40\nsamples = 200_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 200000; trees: 40; train acc: 0.30; valid acc: 0.30\n\n\nDoubling the number of trees results in a validation accuracy of 30%. (baseline was 29%).\n\ntrees = 80\nsamples = 200_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 200000; trees: 80; train acc: 0.31; valid acc: 0.30\n\n\nDoubling the number of samples to 400_000 with 40 trees gets a validation accuracy of 30% (baseline was 29%).\n\ntrees = 40\nsamples = 400_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 400000; trees: 40; train acc: 0.31; valid acc: 0.30\n\n\nDoubling the number of trees to 80 with 400_000 samples does not improve the validation accuracy of 30%.\n\ntrees = 80\nsamples = 400_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 400000; trees: 80; train acc: 0.32; valid acc: 0.30\n\n\nEven after increasing the number of samples to 2M, the validation accuracy stays at 30%.\n\ntrees = 40\nsamples = 2_000_000\nm = rf(xs, y, n_estimators=trees, max_samples=samples)\nprint(f'samples: {samples}; trees: {trees}; train acc: {m_acc(m, xs, y):.2f}; valid acc: {m_acc(m, valid_xs, valid_y):.2f}')\n\nsamples: 2000000; trees: 40; train acc: 0.36; valid acc: 0.30\n\n\nIt’s interesting (and a bit concerning) to note that timestamp is the most important feature for this model—it’s 6-7 times as important as the movie title.\n\nrf_feat_importance(m, xs)\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      5\n      timestamp\n      0.443988\n    \n    \n      0\n      userId\n      0.346306\n    \n    \n      1\n      title\n      0.073165\n    \n    \n      3\n      tag\n      0.060231\n    \n    \n      4\n      genome_tag\n      0.056996\n    \n    \n      2\n      genres\n      0.019314\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nI’ll follow fastai’s Chapter 9 approach to determining which columns’ values differ the most between the training and validation set.\n\ndf_dom = pd.concat([xs, valid_xs])\nis_valid = np.array([0]*len(xs) + [1]*len(valid_xs))\n\n\nm = rf(df_dom, is_valid, n_estimators=40, max_samples=400_000)\n\ntimestamp is the most important feature when distinguishing between the training and validation set. I’ll remove it to see if it improves training.\n\nrf_feat_importance(m, df_dom)\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      5\n      timestamp\n      0.313090\n    \n    \n      0\n      userId\n      0.311026\n    \n    \n      1\n      title\n      0.131196\n    \n    \n      3\n      tag\n      0.106029\n    \n    \n      4\n      genome_tag\n      0.099509\n    \n    \n      2\n      genres\n      0.039150\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nm = rf(xs.drop('timestamp', axis=1), y, n_estimators=40, max_samples=400_000)\n\nAfter removing timestamp, userId surges to the top, being 6 times as important as the 2nd-most important feature (title). The validation accuracy stays at 30%.\n\nrf_feat_importance(m, xs.drop('timestamp', axis=1))\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      0\n      userId\n      0.674746\n    \n    \n      1\n      title\n      0.113734\n    \n    \n      3\n      tag\n      0.096440\n    \n    \n      4\n      genome_tag\n      0.088569\n    \n    \n      2\n      genres\n      0.026510\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nm_acc(m, valid_xs.drop('timestamp', axis=1), valid_y)\n\n0.2971844706990113\n\n\n\n\nNeural Net (tabular_learner)\nNext, I’ll train a neural net on this data and see how it performs. I’ll also use the embeddings from the neural net later on to train a new random forest.\n\ndf_nn = ratings.drop(['timestamp', 'movieId'], axis=1)\n\n\ndf_nn.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      userId\n      rating\n      title\n      genres\n      tag\n      genome_tag\n    \n  \n  \n    \n      0\n      1\n      5.0\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      1\n      3\n      5.0\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      2\n      4\n      4.0\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      3\n      5\n      4.0\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n    \n      4\n      7\n      4.0\n      Pulp Fiction (1994)\n      Comedy\n      quentin tarantino\n      hit men\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nprocs_nn = [Categorify, FillMissing]\ncat_nn = ['userId', 'genres', 'tag', 'genome_tag']\ncont_nn = None\nsplits = RandomSplitter(seed=42)(range_of(df_nn))\nlen(splits), len(splits[0]), len(splits[1])\n\n(2, 20000076, 5000019)\n\n\n\nto_nn = TabularPandas(\n    df_nn,\n    procs_nn,\n    cat_names=cat_nn,\n    cont_names=None,\n    splits=splits,\n    y_names='rating',\n    y_block=CategoryBlock)\n\n\ndls = to_nn.dataloaders(1024)\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      userId\n      genres\n      tag\n      genome_tag\n      rating\n    \n  \n  \n    \n      0\n      149149\n      Documentary\n      boxing\n      documentary\n      4.0\n    \n    \n      1\n      5588\n      Action\n      conspiracy\n      conspiracy\n      5.0\n    \n    \n      2\n      67949\n      Adventure\n      animation\n      computer animation\n      3.0\n    \n    \n      3\n      61069\n      Action\n      satire\n      satire\n      0.5\n    \n    \n      4\n      43620\n      Comedy\n      1930s\n      1930s\n      3.0\n    \n    \n      5\n      161519\n      Action\n      natalie portman\n      hit men\n      4.0\n    \n    \n      6\n      155206\n      Crime\n      tom hanks\n      oscar (best directing)\n      4.0\n    \n    \n      7\n      91508\n      Drama\n      england\n      skinhead\n      4.0\n    \n    \n      8\n      76347\n      Drama\n      clint eastwood\n      western\n      5.0\n    \n    \n      9\n      30987\n      Crime\n      serial killer\n      oscar (best directing)\n      5.0\n    \n  \n\n\n\n\nlearn = tabular_learner(dls, layers=[1000, 500], loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0003311311302240938)\n\n\n\n\n\nThe GPU RAM stayed at around 3.5/15.0 GB during training so if I wanted to train again I’d use a larger batch size. The neural net achieved a 38% validation accuracy, better than the random forest (30%) and the DotProductBiasCE model (35%).\n\nlearn.fit_one_cycle(5, 2e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.576747\n      1.615499\n      0.359292\n      19:06\n    \n    \n      1\n      1.557086\n      1.605259\n      0.362625\n      18:48\n    \n    \n      2\n      1.514029\n      1.582966\n      0.369193\n      19:03\n    \n    \n      3\n      1.422227\n      1.559873\n      0.377868\n      18:50\n    \n    \n      4\n      1.328115\n      1.570342\n      0.382583\n      18:48\n    \n  \n\n\n\nThe neural net is much better at predicting a diverse range of ratings (whereas DotProductBiasCE did not predict any ratings ending with .5). Like the DotProductBiasCE model, the neural net’s most accurate prediction was 4. Unlike DotProductBiasCE the diagonal in the confusion matrix is somewhat darker than the non-diagonal, showing that the model is doing a better job of correctly predicting ratings (as exhibited by the higher overall accuracy).\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6, 6))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npreds, targs = learn.get_preds(dl=learn.dls.valid)\n\n\n\n\n\n\n\n\nThe distribution of predictions has no gaps—the model predicts all possible ratings.\n\nplt.figure(figsize=(10, 6))\nplt.hist(preds.argmax(dim=1).squeeze(), alpha=0.7, color='blue', label='Predictions');\nplt.hist(targs.squeeze(), alpha=0.5, color='red', label='Targets');\nplt.legend()\nplt.xlabel('Rating Index')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\nRandom Forest with Neural Net Embeddings\nThe final approach I was hoping to implement was using the embeddings from the neural net to create additional columns and using them to train a random forest, as done in this Medium post.\nHowever, I ran into some RAM and disk space issues. To illustrate, I’ll show the four Embeddings that the model has learned, which totals an output of 955 values.\n\nlearn.model.embeds\n\nModuleList(\n  (0): Embedding(162542, 600)\n  (1): Embedding(21, 9)\n  (2): Embedding(9856, 276)\n  (3): Embedding(841, 70)\n)\n\n\nThe smallest data type (without receiving an error) I was able to use when passing a column of xs through an Embedding was int32 which takes up 4 bytes per element. With 20M rows in the training set, outputting 955 values each 4 bytes large would take up about 76 GB of storage.\n\n20e6*4*955/1e9\n\n76.4\n\n\nAnother 5M rows of the validation set (each with 955 Embedding outputs) would bump that up to over 100GB. Kaggle provides 73GB of disk space and 30GB of RAM. Google Colab provides 107GB of disk space and 13GB of RAM.\nHandling these sorts of RAM/disk space issues is something I want to learn about and experiment with in the future, after which I can return to this scale of a dataset in an attempt to train it."
  },
  {
    "objectID": "posts/2024-07-13-movielens25m/index.html#final-thoughts",
    "href": "posts/2024-07-13-movielens25m/index.html#final-thoughts",
    "title": "Training Models on the MovieLens 25M Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nTraining models on the full MovieLens 25M rating dataset using Kaggle and/or Google Colab was tough. Training a single model took up to 9 hours (in the case of DotProductBias), so just getting through five different architectures required patience (on top of a lot of runtime crashes due to RAM maxing out). However, it was still fun to see how these different architectures behaved and performed during training. Here’s a summary of my results:\n\n\n\nArch\nMetric\nMetric Value\n\n\n\n\nDotProductBias\nMSE\n0.654875\n\n\nDotProductBiasCE\nAccuracy\n35%\n\n\nRandom Forest (baseline)\nAccuracy\n29%\n\n\nRandom Forest (additional columns)\nAccuracy\n30%\n\n\nNeural Net\nAccuracy\n38%\n\n\n\nIn the fastai textbook, the MSE of DotProductBias on the 100k rating subset was about 0.8 and in my experiments with DotProductBiasCE (Cross-Entropy Loss) the Accuracy for DotProductBiasCE was about 40%. The 25M DotProductBias beat the 100k subset’s MSE and the 25M neural net was competitive with the 100k subset’s accuracy.\nThere’s still performance gains that I didn’t pursue (for example, training the neural net for more epochs or trying a different number of hidden layer sizes), but I’m satisfied with the breadth of my experiments and learning experience.\nI hope you enjoyed this blog post! Following me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-02-tinystories-readinglevel/index.html",
    "href": "posts/2024-08-02-tinystories-readinglevel/index.html",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the TinyStories Dataset",
    "section": "",
    "text": "In this notebook I’ll calculate the Flesch Kincaid reading grade level for the TinyStories dataset.\nThis is part of a larger project I’m working on where my goal is to train the TinyInstruct-33M model on a simplified, lower reading level version of the financial_phrasebank dataset. I’m experimenting with using phi-3 to simplify the sentences in the original financial_phrasebank dataset using language appropriate for low reading grade levels. To establish a baseline of what reading level of text TinyInstruct-33M is capable of understanding, I’ll measure the reading level across the TinyStories dataset in this notebook using the textstat library."
  },
  {
    "objectID": "posts/2024-08-02-tinystories-readinglevel/index.html#reading-grade-level-of-a-sample-story",
    "href": "posts/2024-08-02-tinystories-readinglevel/index.html#reading-grade-level-of-a-sample-story",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the TinyStories Dataset",
    "section": "Reading Grade Level of a Sample Story",
    "text": "Reading Grade Level of a Sample Story\n\n!pip install textstat -qq\n!pip install datasets -qq\n\n\nfrom datasets import load_dataset\nimport numpy as np\nimport textstat\n\nds = load_dataset(\"roneneldan/TinyStories\")\n\nAccording to the textstat library, the following story has a reading grade level of 3.8.\n\ntext = ds['train'][0]['text']\ntext\n\n'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'\n\n\n\ntextstat.flesch_kincaid_grade(text)\n\n3.8\n\n\nI checked a few online Flesch-Kincaid calculators and got a wide variety of results:\n\ngoodcalculators.com: 4.5\nserpninja.io: 5.49\ncharactercalculator.com: 6.76\nonline-utility.org: 5.4\n\nWhy so much variability? I’ll take a closer look at the Flesch-Kincaid reading grade level formula from the Wiki page:\n\\[0.39\\big(\\frac{\\text{total words}}{\\text{total sentences}}\\big) + 11.8\\big(\\frac{\\text{total syllables}}{\\text{total words}}\\big) - 15.59\\]\nCounting by hand, there are a 134 total words, 9 total sentences and 169 syllables. Plugging this into the formula, you get a reading level of 5.09.\n\ndef fk_reading_level(w, s, sy):\n  return 0.39*(w/s) + 11.8*(sy/w) - 15.59\n\n\nfk_reading_level(134, 9, 169)\n\n5.098756218905475\n\n\nI’m guessing that one source of variability is how you count sentences. For example, if I choose to count the full dialogue as part of a sentence (in sentence # 4), the number of sentences is 9:\n\n\nOne day, a little girl named Lily found a needle in her room.\nShe knew it was difficult to play with it because it was sharp.\nLily wanted to share the needle with her mom, so she could sew a button on her shirt.\nLily went to her mom and said, “Mom, I found this needle. Can you share it with me and sew my shirt?”\nHer mom smiled and said, “Yes, Lily, we can share the needle and fix your shirt.”\nTogether, they shared the needle and sewed the button on Lily’s shirt.\nIt was not difficult for them because they were sharing and helping each other.\nAfter they finished, Lily thanked her mom for sharing the needle and fixing her shirt.\nThey both felt happy because they had shared and worked together.\n\n\nOn the other hand, if I split the multiple-sentence dialogue into multiple sentences, the number of sentences increases to 10 and the reading level drops to 4.5.\n\nfk_reading_level(134, 10, 169)\n\n4.518089552238806\n\n\n\n\nOne day, a little girl named Lily found a needle in her room.\nShe knew it was difficult to play with it because it was sharp.\nLily wanted to share the needle with her mom, so she could sew a button on her shirt.\nLily went to her mom and said, “Mom, I found this needle.\nCan you share it with me and sew my shirt?”\nHer mom smiled and said, “Yes, Lily, we can share the needle and fix your shirt.”\nTogether, they shared the needle and sewed the button on Lily’s shirt.\nIt was not difficult for them because they were sharing and helping each other.\nAfter they finished, Lily thanked her mom for sharing the needle and fixing her shirt.\nThey both felt happy because they had shared and worked together."
  },
  {
    "objectID": "posts/2024-08-02-tinystories-readinglevel/index.html#deeper-dive-into-textstat",
    "href": "posts/2024-08-02-tinystories-readinglevel/index.html#deeper-dive-into-textstat",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the TinyStories Dataset",
    "section": "Deeper Dive into textstat",
    "text": "Deeper Dive into textstat\nHere’s the Flesch-Kincaid grade level calculation code in textstat:\nsentence_length = self.avg_sentence_length(text)\nsyllables_per_word = self.avg_syllables_per_word(text)\nflesch = (\n        float(0.39 * sentence_length)\n        + float(11.8 * syllables_per_word)\n        - 15.59)\nreturn self._legacy_round(flesch, 1)\nFor the given text I’m using, here are the stats using textstat methods, compared to my manual calculations:\n\n\n\nStatistics\ntextstat\nManual Calcs\n\n\n\n\nTotal Words\n134\n134\n\n\nTotal Sentences\n10\n10\n\n\nTotal Syllables\n163\n169\n\n\n\n\ntextstat.lexicon_count(text), \\\ntextstat.sentence_count(text), \\\ntextstat.syllable_count(text)\n\n(134, 10, 163)\n\n\nlexicon_count optionally removes punctuation from the text and and splits the string by empty spaces:\nif removepunct:\n  text = self.remove_punctuation(text)\ncount = len(text.split())\nreturn count\nsentence_count which calculates the number of sentences (that have more than 2 words) by matching groups of text in between sentence-ending punctation (.?!):\nsentences = re.findall(r'\\b[^.!?]+[.!?]*', text, re.UNICODE)\n        for sentence in sentences:\n            if self.lexicon_count(sentence) <= 2:\n                ignore_count += 1\n        return max(1, len(sentences) - ignore_count)\nsyllable_count uses the library pyphen:\nfor word in text.split():\n    count += len(self.pyphen.positions(word)) + 1\nreturn count\nThe difference between my manually calculated reading grade level and textstat’s is a matter of 6 syllables. I looked at the output of len(self(pyphen.positions(word)) + 1) and my manual counts for each word and found that for all six cases of the word \"Lily\", I counted two syllables (Li-ly) while pyphen returned 1 syllable.\nIt’s amazing that a difference of 6 syllables can lead to almost a full grade level difference!\ntextstat, and pyphen indirectly, seems to undercount syllables:\n\nfor word in ['vishal', 'pakistan', 'afghanistan', 'legolas', 'synchronicity', 'persiphone', 'camera', 'reward', 'appalachian']:\n  print(f\"{word}: {textstat.syllable_count(word)}\")\n\nvishal: 1\npakistan: 2\nafghanistan: 1\nlegolas: 2\nsynchronicity: 4\npersiphone: 3\ncamera: 2\nreward: 2\nappalachian: 3\n\n\nKeeping in mind that the reading grade level calculated by textstat maybe be consistently too low, I’ll move forward with using it to establish the baseline reading level for the TinyStories dataset."
  },
  {
    "objectID": "posts/2024-08-02-tinystories-readinglevel/index.html#reading-level-for-the-tinystories-training-set",
    "href": "posts/2024-08-02-tinystories-readinglevel/index.html#reading-level-for-the-tinystories-training-set",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the TinyStories Dataset",
    "section": "Reading Level for the TinyStories Training Set",
    "text": "Reading Level for the TinyStories Training Set\nI’ll apply textstat.flesch_kincaid_grade to each row of the training data set for TinyStories:\n\nddf = ds['train'].to_pandas()\n\n\nddf.shape\n\n(2119719, 1)\n\n\n\nddf.iloc[0]\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      text\n      One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n    \n  \ndtype: object\n\n\nThis took about 14.5 minutes to run:\n\nddf['fk_grade'] = ddf['text'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n\nThe mean and median Flesch-Kincaid reading level of the TinyStories dataset (using textstat.flesch_kincaid_grade) is about 2.6 and 2.7, respectively. Keeping in mind that textstat may be underestimating the reading level, perhaps by 1 grade level (or more). The minimum grade level is somehow negative (-15.7) and the maximum is somehow 74.\n\nddf['fk_grade'].describe()\n\n\n\n  \n    \n      \n      fk_grade\n    \n  \n  \n    \n      count\n      2.119719e+06\n    \n    \n      mean\n      2.553505e+00\n    \n    \n      std\n      1.485081e+00\n    \n    \n      min\n      -1.570000e+01\n    \n    \n      25%\n      1.600000e+00\n    \n    \n      50%\n      2.700000e+00\n    \n    \n      75%\n      3.600000e+00\n    \n    \n      max\n      7.410000e+01\n    \n  \ndtype: float64\n\n\nViewing the data between kindergarten (Grade 0) and Grade 12, about a third of the texts are between grades 2 through 4.\n\nddf['fk_grade'].hist(range=(0,12));\n\n\n\n\n85% of the stories are at or below a Grade 4 reading level. 99% of them are at or below a Grade 6 level.\n\nddf.query('fk_grade <= 4')['text'].count() / len(ddf)\n\n0.8508873110067892\n\n\n\nddf.query('fk_grade <= 6')['text'].count() / len(ddf)\n\n0.9937878558431565\n\n\nThe story with the grade level of 74 has 750 words with 1033 syllables across only 4 sentences. The ratio of words per sentence is a staggering 187.5, which is greatly inflating the reading level.\n\nhard_text = ddf.query('fk_grade > 74').iloc[0]['text']\nhard_text\n\n\"Lily and Tom were playing in the kitchen. They liked to pretend they were chefs and make yummy food. They had a big pot and some spoons and bowls. They put water and salt and sugar and flour and carrots and apples and cheese and cookies and milk and eggs and bananas and pasta and bread and jam and butter and ketchup and mustard and chocolate and candy and nuts and beans and corn and peas and rice and meat and fish and cheese and yogurt and cereal and juice and tea and coffee and soda and water and ice and lemon and lime and orange and grape and cherry and strawberry and blueberry and raspberry and blackberry and cranberry and pineapple and mango and kiwi and coconut and melon and peach and plum and pear and apple and apricot and nectarine and grapefruit and pomegranate and fig and date and olive and avocado and tomato and cucumber and lettuce and spinach and kale and cabbage and broccoli and cauliflower and celery and carrot and onion and garlic and ginger and turmeric and parsley and basil and mint and rosemary and thyme and oregano and sage and dill and cilantro and chives and scallions and leeks and mushrooms and peppers and zucchini and squash and pumpkin and eggplant and potato and sweet potato and yam and cassava and taro and plantain and breadfruit and jackfruit and durian and lychee and rambutan and dragon fruit and star fruit and passion fruit and guava and papaya and persimmon and loquat and kumquat and tangerine and clementine and mandarin and satsuma and pomelo and ugli and quince and medlar and mulberry and elderberry and gooseberry and currant and cranberry and lingonberry and loganberry and boysenberry and huckleberry and bilberry and cloudberry and salmonberry and dewberry and blackcurrant and redcurrant and whitecurrant and jostaberry and worcesterberry and gooseberry and aronia and chokeberry and serviceberry and saskatoon and juneberry and honeyberry and blue honeysuckle and sea buckthorn and wolfberry and goji and acai and maqui and camu camu and lucuma and baobab and moringa and spirulina and chlorella and wheatgrass and barley grass and oat grass and rye grass and alfalfa and clover and fenugreek and flax and hemp and chia and sesame and sunflower and pumpkin and poppy and nigella and fennel and anise and caraway and cumin and coriander and cardamom and nutmeg and mace and allspice and clove and cinnamon and cassia and saffron and vanilla and licorice and star anise and bay and laurel and curry and garam masala and ras el hanout and baharat and za'atar and dukkah and harissa and berbere and sumac and tamarind and lemon grass and kaffir lime and galangal and makrut and pandan and curry leaf and neem and asafoetida and fenugreek and mustard and horseradish and wasabi and ginger and turmeric and paprika and cayenne and chili and pepper and salt and sugar and honey and maple and molasses and agave and stevia and xylitol and erythritol and monk fruit and coconut and palm and date and brown and white and raw and refined and powdered and granulated and crystal and rock and sea and kosher and himalayan and celtic and flake and smoked and iodized and non-iodized and organic and non-organic and gluten-free and vegan and vegetarian and paleo and keto and low-carb and high-carb and low-fat and high-fat and low-sugar and high-sugar and low-salt and high-salt and low-calorie and high-calorie and healthy and unhealthy and delicious and disgusting and sweet and sour and bitter and salty and savory and umami and spicy and mild and hot and cold and warm and cool and frozen and thawed and cooked and raw and baked and boiled and fried and grilled and roasted and toasted and broiled and poached and steamed and microwaved and pressure-cooked and slow-cooked and sous-vide and smoked and cured and pickled and fermented and dried and canned and jarred and bottled and vacuum-sealed and wrapped and bagged and boxed and cartoned and packaged and labeled and branded and generic and fresh and stale and rotten and spoiled and moldy and rancid and expired and best before and use by and sell by and enjoy by and made by and produced by and distributed by and imported by and exported by and sold by and bought by and owned by and used by and eaten by and loved by and hated by and shared by and hoarded by and wasted by and\"\n\n\n\ntextstat.lexicon_count(hard_text), \\\ntextstat.sentence_count(hard_text), \\\ntextstat.syllable_count(hard_text)\n\n(750, 4, 1033)\n\n\n\ntextstat.lexicon_count(hard_text) / textstat.sentence_count(hard_text), \\\ntextstat.syllable_count(hard_text) / textstat.lexicon_count(hard_text)\n\n(187.5, 1.3773333333333333)\n\n\nA more reasonable story (although past K-12 grade levels) has 627 words across 20 sentences, resulting in a grade level of 15.5 (post-grad):\n\nddf.query('fk_grade > 12 and fk_grade < 16').iloc[0]['fk_grade']\n\n15.5\n\n\n\nhard_text = ddf.query('fk_grade > 12 and fk_grade < 16').iloc[0]['text']\nhard_text\n\n'Sam and Tom are friends. They like to play in the park. One day, they find a big sack near a tree. They wonder what is inside.\\n\\n\"Let\\'s open it and see!\" Sam says.\\n\\n\"OK, but we have to share what we find,\" Tom says.\\n\\nThey pull the sack to a quiet spot. They open it and see many shiny things. They are coins! Sam and Tom are very happy. They have never seen so many coins before.\\n\\n\"Wow, we are so lucky!\" Sam says.\\n\\n\"Yes, we are! How many coins do you think there are?\" Tom says.\\n\\nThey start to count the coins. But they have a problem. They do not know how to count very well. They only know how to count to ten. After ten, they get confused.\\n\\n\"Ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty-one, twenty-two, twenty-three, twenty-four, twenty-five, twenty-six, twenty-seven, twenty-eight, twenty-nine, thirty, thirty-one, thirty-two, thirty-three, thirty-four, thirty-five, thirty-six, thirty-seven, thirty-eight, thirty-nine, forty, forty-one, forty-two, forty-three, forty-four, forty-five, forty-six, forty-seven, forty-eight, forty-nine, fifty, fifty-one, fifty-two, fifty-three, fifty-four, fifty-five, fifty-six, fifty-seven, fifty-eight, fifty-nine, sixty, sixty-one, sixty-two, sixty-three, sixty-four, sixty-five, sixty-six, sixty-seven, sixty-eight, sixty-nine, seventy, seventy-one, seventy-two, seventy-three, seventy-four, seventy-five, seventy-six, seventy-seven, seventy-eight, seventy-nine, eighty, eighty-one, eighty-two, eighty-three, eighty-four, eighty-five, eighty-six, eighty-seven, eighty-eight, eighty-nine, ninety, ninety-one, ninety-two, ninety-three, ninety-four, ninety-five, ninety-six, ninety-seven, ninety-eight, ninety-nine, one hundred, one hundred and one, one hundred and two, one hundred and three, one hundred and four, one hundred and five, one hundred and six, one hundred and seven, one hundred and eight, one hundred and nine, one hundred and ten, one hundred and eleven, one hundred and twelve, one hundred and thirteen, one hundred and fourteen, one hundred and fifteen, one hundred and sixteen, one hundred and seventeen, one hundred and eighteen, one hundred and nineteen, one hundred and twenty, one hundred and twenty-one, one hundred and twenty-two, one hundred and twenty-three, one hundred and twenty-four, one hundred and twenty-five, one hundred and twenty-six, one hundred and twenty-seven, one hundred and twenty-eight, one hundred and twenty-nine, one hundred and thirty, one hundred and thirty-one, one hundred and thirty-two, one hundred and thirty-three, one hundred and thirty-four, one hundred and thirty-five, one hundred and thirty-six, one hundred and thirty-seven, one hundred and thirty-eight, one hundred and thirty-nine, one hundred and forty, one hundred and forty-one, one hundred and forty-two, one hundred and forty-three, one hundred and forty-four, one hundred and forty-five, one hundred and forty-six, one hundred and forty-seven, one hundred and forty-eight, one hundred and forty-nine, one hundred and fifty, one hundred and fifty-one, one hundred and fifty-two, one hundred and fifty-three, one hundred and fifty-four, one hundred and fifty-five, one hundred and fifty-six, one hundred and fifty-seven, one hundred and fifty-eight, one hundred and fifty-nine, one hundred and sixty, one hundred and sixty-one, one hundred and sixty-two, one hundred and sixty-three, one hundred and sixty-four, one hundred and sixty-five, one hundred and sixty-six, one hundred and sixty-seven, one hundred and sixty-eight, one hundred and sixty-nine, one hundred and seventy, one hundred and seventy-one, one hundred and seventy-two, one hundred and seventy-three, one hundred and seventy-four, one hundred and seventy-five, one hundred and seventy-six, one hundred and seventy-seven, one hundred and seventy-eight, one hundred and seventy-nine, one hundred and eighty, one hundred and eighty-one, one hundred and eighty-two, one hundred and eighty-three, one hundred and eighty-four, one hundred and eighty-five, one hundred and eighty-six, one hundred and eighty-seven, one hundred and eighty-eight, one hundred and eighty-nine, one hundred and ninety, one hundred and ninety-one, one hundred and ninety-two, one hundred and ninety-three, one hundred and ninety-four, one hundred and ninety-five, one hundred and ninety-six, one hundred and ninety-seven, one hundred and ninety-eight, one hundred and ninety-nine, two hundred, two hundred and one, two hundred'\n\n\n\ntextstat.lexicon_count(hard_text), \\\ntextstat.sentence_count(hard_text), \\\ntextstat.syllable_count(hard_text)\n\n(627, 20, 989)\n\n\n\ntextstat.lexicon_count(hard_text) / textstat.sentence_count(hard_text), \\\ntextstat.syllable_count(hard_text) / textstat.lexicon_count(hard_text)\n\n(31.35, 1.5773524720893142)\n\n\n11 stories have a grade level above 12.\n\nddf.query('fk_grade > 12')['text'].count()\n\n11"
  },
  {
    "objectID": "posts/2024-08-02-tinystories-readinglevel/index.html#final-thoughts",
    "href": "posts/2024-08-02-tinystories-readinglevel/index.html#final-thoughts",
    "title": "Calculating the Flesch Kincaid Reading Grade Level for the TinyStories Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI am elated. I thought I was going to have to calculate reading levels by manually copy pasting text into one of the online calculators. For the financial_phrasebank dataset, that would be 2264 such operations. Instead, I can now use textstat as a baseline when comparing reading levels between original and LLM-generated simplified texts.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html",
    "href": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html",
    "title": "Calculating the Letter Area Ratio in a Text Image",
    "section": "",
    "text": "In this notebook, I’ll walk through a modified algorithm (suggested by Claude) to calculate the percentage of positive space in the letters of an image. I’ll define this percentage as:\n\n\\[\\frac{\\text{Area of Letter}}{\\text{Area of Bounding Box Around Letter}}\\]\n\nThis algorithm is part of my exploration of non-ML baselines to classify text images into various typeface categories (e.g., “humanist sans,” “grotesque sans,” “script,” “display,” etc.). Once the non-ML baseline is established, I’ll train a neural network for this task. This is one of many notebooks in my TypefaceClassifier project series.\n\n\nShow imports\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#find-the-contours-of-a-binarized-image",
    "href": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#find-the-contours-of-a-binarized-image",
    "title": "Calculating the Letter Area Ratio in a Text Image",
    "section": "Find the Contours of a Binarized Image",
    "text": "Find the Contours of a Binarized Image\nAs we do with all of these algorithms (thus far), we start by loading the image of text and binarizing it.\n\nimage_path = 'serif-76px.png'\n\n\nimg = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n_, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nbinary\n\n\n      ndarray (512, 512) show dataarray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n\n\nThen we findContours in the image. In order to find both the inner and outer contours (for letters with “holes” in them like o, d, e p q and so on) we use the cv2.RETR_TREE parameter.\nThis returns a hierarchy which has the shape 1, N, 4 where N is the number of contours. The 4 elements per contour are [Next, Previous, First_Child, Parent]. If a contour doesn’t have one of those, the value of hierarchy is -1.\n\ncontours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n\nhierarchy.shape\n\n(1, 75, 4)\n\n\nVisualizing the contours—note how the inside shapes (holes) of letters like d are also identified as contours.\n\n\nShow plot_contour definition\ndef plot_contour(img, contour):\n  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  cv2.drawContours(img_rgb, contour, -1, (250, 0, 92), 2)\n  fig, ax = plt.subplots(1, 1 , figsize=(5,5))\n  fig.suptitle(f'Binarized Contours', fontsize=16)\n  ax.imshow(img_rgb);\n\n\n\nplot_contour(img, contours)"
  },
  {
    "objectID": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#calculating-bounding-box-and-contour-areas",
    "href": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#calculating-bounding-box-and-contour-areas",
    "title": "Calculating the Letter Area Ratio in a Text Image",
    "section": "Calculating Bounding Box and Contour Areas",
    "text": "Calculating Bounding Box and Contour Areas\nAs an example, I’ll calculate the area inside the contours of the letter d (it has an outer contour, 67 and an inner contour 68):\n\nplot_contour(img, contours[67]) # outside contour\n\n\n\n\n\n\n\n\n\nplot_contour(img, contours[68]) # inside contour\n\n\n\n\n\n\n\n\nThe bounding box (rectangle) around the contour is calculated with boundingRect:\n\nx, y, w, h = cv2.boundingRect(contours[67]) # outer contour bounding box\n\nThe rectangle’s area is:\n\nrect_area = w*h\nrect_area\n\n1998\n\n\nThe area of the contour is a bit more involved as it’s the difference between the outer area of the d and the inner (hole) area of the d:\n\nouter_area = cv2.contourArea(contours[67])\nouter_area\n\n1049.5\n\n\nThe hierarchy of this d’s outer contour tells us that is has no parent (-1) and it has a child 68.\n\nhierarchy[0][67]\n\narray([69, 66, 68, -1], dtype=int32)\n\n\nIterating through the children (it has 1 child) we calculate the inner contour’s area:\n\nchild = hierarchy[0][67][2]  # First child\ninner_area = 0\n\nwhile child != -1 and child < len(contours):\n  inner_area += cv2.contourArea(contours[child])\n  child = hierarchy[0][child][2]\n\n\ninner_area\n\n425.5\n\n\nIn this case, since there is only one child, we could calculate the contour directly:\n\ncv2.contourArea(contours[68])\n\n425.5\n\n\nThe area of the letter is the difference between inner and outer areas:\n\nd_area = outer_area - inner_area\nd_area\n\n624.0\n\n\nThe percentage of positive space of the letter d, in pixels, is the area of the letter divided by the area of the bounding box:\n\nd_area / rect_area\n\n0.3123123123123123\n\n\nIf we had ignored the inner area of the d and used the outer contour area, this percentage would be significantly larger:\n\nouter_area / rect_area\n\n0.5252752752752753"
  },
  {
    "objectID": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#calculating-average-letter-area-ratio-of-different-images-of-text",
    "href": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#calculating-average-letter-area-ratio-of-different-images-of-text",
    "title": "Calculating the Letter Area Ratio in a Text Image",
    "section": "Calculating Average Letter Area Ratio of Different Images of Text",
    "text": "Calculating Average Letter Area Ratio of Different Images of Text\nI’ll wrap all of this functionality into a set of functions (well, Claude did that for me to begin with) and then test it out on different images of text. I’m also calculating the outer contour area so I can illustrate the difference of areas for an image.\n\n\nShow letter_area_ratio definition\ndef get_letter_area(contour, hierarchy, contours, idx):\n    outer_area = cv2.contourArea(contour)\n    inner_area = 0\n\n    # Check if hierarchy is valid\n    if hierarchy is None or len(hierarchy) < 3:\n        return outer_area  # Return outer area if hierarchy is invalid\n\n    child = hierarchy[idx][2]  # First child\n\n    while child != -1 and child < len(contours):\n        inner_area += cv2.contourArea(contours[child])\n        # Safely get next child\n        #if child < len(hierarchy) and len(hierarchy[child]) > 0:\n        child = hierarchy[child][2]\n        # else:\n        #     break  # Exit loop if we can't get next child safely\n\n    return outer_area - inner_area\n\ndef letter_area_ratio(image_path):\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    area_ratios = []\n    total_outer_area = 0\n    total_box_area = 0\n    total_letter_area = 0\n\n    for i, contour in enumerate(contours):\n        if hierarchy[0][i][3] == -1:  # This is an outer contour\n            x, y, w, h = cv2.boundingRect(contour)\n            rect_area = w * h\n            outer_area = cv2.contourArea(contour)\n            letter_area = get_letter_area(contour, hierarchy[0], contours, i)\n\n            total_box_area += rect_area\n            total_outer_area += outer_area\n            total_letter_area += letter_area\n\n            if rect_area > 0:\n                ratio = letter_area / rect_area\n                area_ratios.append(ratio)\n\n    avg_ratio = np.median(area_ratios) if area_ratios else 0\n    return avg_ratio, total_box_area, total_outer_area, total_letter_area\n\n\nI also asked Claude for a function to visualize the original image, the bounding boxes, the outer contours, and the refined (outer - inner) contours.\n\n\nShow visualize_analysis definition\ndef visualize_analysis(image_path):\n    img = cv2.imread(image_path)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Threshold the image\n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n    # Find contours with hierarchy\n    contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    avg_ratio, total_box_area, total_outer_area, total_letter_area = letter_area_ratio(image_path)\n\n    img_boxes = img_rgb.copy()\n    img_contours = img_rgb.copy()\n    img_refined = img_rgb.copy()\n\n    # Draw bounding boxes\n    for contour in contours:\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(img_boxes, (x, y), (x+w, y+h), (250, 0, 92), 2)\n\n    # Draw all contours, including inner ones\n    cv2.drawContours(img_refined, contours, -1, (250, 0, 92), 2)\n\n    # Draw only outer contours\n    for i, contour in enumerate(contours):\n        if hierarchy[0][i][3] == -1:  # This is an outer contour\n            cv2.drawContours(img_contours, [contour], 0, (250, 0, 92), 2)\n\n    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n    fig.suptitle(f'Letter Analysis (Avg Area Ratio: {avg_ratio:.2f})', fontsize=16)\n\n    axs[0, 0].imshow(img_rgb)\n    axs[0, 0].set_title('Original Image')\n    axs[0, 0].axis('off')\n\n    axs[0, 1].imshow(img_boxes)\n    axs[0, 1].set_title(f'Bounding Boxes\\nTotal Area: {total_box_area:.0f} pixels')\n    axs[0, 1].axis('off')\n\n    axs[1, 0].imshow(img_contours)\n    axs[1, 0].set_title(f'Outer Contours\\nTotal Area: {total_outer_area:.0f} pixels')\n    axs[1, 0].axis('off')\n\n    axs[1, 1].imshow(img_refined)\n    axs[1, 1].set_title(f'Outer - Inner Contours\\nTotal Area: {total_letter_area:.0f} pixels')\n    axs[1, 1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\nAcross the entire image, the refined (outer - inner) contour area is about 10% less than the outer area.\n\nvisualize_analysis('serif-76px.png')\n\n\n\n\n\n\n\n\nFor the same typeface (serif) the average percentage of positive space is (kind of?) consistent.\n\nfor sz in [18, 24, 36, 76, 330]:\n  avg_ratio, _, _, _ = letter_area_ratio(f'serif-{sz}px.png')\n  print('serif', sz, avg_ratio)\n\nserif 18 0.25\nserif 24 0.30213903743315507\nserif 36 0.32516339869281047\nserif 76 0.37477598566308246\nserif 330 0.425178283873936\n\n\nFor a different typeface (display), the average ratio (of letter area to bounding box area) is considerably larger:\n\nfor sz in [18, 24, 36, 76, 330]:\n  avg_ratio, _, _, _ = letter_area_ratio(f'display-{sz}px.png')\n  print('display', sz, avg_ratio)\n\ndisplay 18 0.4\ndisplay 24 0.4444444444444444\ndisplay 36 0.5015625\ndisplay 76 0.5613636363636364\ndisplay 330 0.579352195423624"
  },
  {
    "objectID": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#final-thoughts",
    "href": "posts/2024-08-21-typefaceclassifier-letter-area-ratio/index.html#final-thoughts",
    "title": "Calculating the Letter Area Ratio in a Text Image",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nGiven that the median letter area ratio for display is larger than serif texts, I think this algorithm is a good candidate for distinguishing between different typefaces.\nI’m continually impressed by the functionality offered in the OpenCV library. I’m also impressed by Claude’s ability to provide me simple, usable and understandable code for OpenCV.\nI hope you enjoyed this blog post! Follow me on Twitter @vishal_learner."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "vishal bakshi",
    "section": "",
    "text": "welcome to my blog.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nTinyScale Lab: Bridging Training Dynamics and Model Capabilities\n\n\n\n\n\n\n\nLLM\n\n\ndeep learning\n\n\nTinyScale-Lab\n\n\n\n\nIn TinyScale Lab, I’m exploring the connection between training dynamics and model capabilities using tiny language models (3M-120M parameters) as research proxies. This project bridges insights from the TinyStories and Small-scale proxies papers to understand how training stability affects emergent capabilities like grammar, consistency, and reasoning. By demonstrating that meaningful ML research is possible with modest computational resources, I hope to make AI research more accessible and democratized for resource-constrained researchers worldwide.\n\n\n\n\n\n\nSaturday, April 26, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nLossInspector: A Deep Dive Into LLM-Foundry’s Next-Token Prediction with a Custom Composer Callback\n\n\n\n\n\n\n\npython\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nI’m working on a research project where we’re fine-tuning small models with various techniques and datasets using LLM-Foundry. As part of our infrastructure setup, I wanted to thoroughly understand how a batch of data is prepared, and how the outputs of a model, along with the labels, are passed to the loss function. Enter the custom Composer callback LossInspector!\n\n\n\n\n\n\nTuesday, April 22, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nOptimizing Matrix Multiplication Using Numba and Broadcasting\n\n\n\n\n\n\n\npython\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nFollowing the fastai course part 2 Lesson 11 video, I optimize the naive Python nested for-loop matrix multiplication using PyTorch, NumPy and Numba to achieve a 12000x speedup!\n\n\n\n\n\n\nMonday, April 21, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry\n\n\n\n\n\n\n\npython\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nI write a custom Composer callback (with lots of Claude’s help!) to log data types of different entities during mixed precision LoRA fine-tuning. When the model is in fp32, all entities except activations are in fp32 (activations are in bf16).\n\n\n\n\n\n\nWednesday, April 2, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Python Descriptors\n\n\n\n\n\n\n\npython\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nWhile vibe coding with Claude, it introduced me to unfamiliar behavior: using the __get__(obj) method to convert a function to a bound method of the given object. This was necessary to monkey patch the self attention module’s forward pass to log input data types as register_forward_hook only works on positional arguments (which LLaMA’s self attention module doesn’t have, it only has keyword arguments). This led me to do a deep dive into understanding descriptors with the helpful Descriptor Guide in the Python docs, which I walkthrough in this blog post.\n\n\n\n\n\n\nTuesday, April 1, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: Creating a Custom Composer Callback\n\n\n\n\n\n\n\nLLM\n\n\ndeep learning\n\n\n\n\nA walkthrough of my first custom Composer callback where I log weight, activation, gradient and loss data types during the training loop.\n\n\n\n\n\n\nSunday, March 30, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRAGatouille/ColBERT Indexing Deep Dive\n\n\n\n\n\n\n\npython\n\n\ninformation retrieval\n\n\nmachine learning\n\n\ndeep learning\n\n\n\n\nIn this technical deep dive, I recreate the entire RAG indexing pipeline by directly using the internal methods of RAGatouille and ColBERT libraries. I methodically build all indexing artifacts from scratch - from processing document collections and sampling embeddings to calculating centroids with k-means clustering and compressing document vectors with residual encoding. If you’re curious about what actually happens when you call RAG.index(), this video breaks down the full process step by step.\n\n\n\n\n\n\nWednesday, March 12, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: PeftModel Base Model Behavior\n\n\n\n\n\n\n\npython\n\n\ndeep learning\n\n\nmachine learning\n\n\nLLM\n\n\n\n\nTIL that the base model gets altered (merged) after you call PeftModel.pretrained to load LoRA adapter weights. I compare weight matrices and analyze memory usage.\n\n\n\n\n\n\nMonday, March 10, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nMemory Profiling raw ColBERT and RAGatouille\n\n\n\n\n\n\n\npython\n\n\ninformation retrieval\n\n\ndeep learning\n\n\n\n\nI use the memory-profiler library to log memory using for different indexing functions for raw ColBERT and RAGatouille indexing operations for 100k, 250k, 500k, 1M and 2M collection sizes. In general, RAGatouille uses more memory than raw ColBERT.\n\n\n\n\n\n\nMonday, February 17, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nEstimating Storage and CPU RAM Requirements for Indexing 12.6M Documents\n\n\n\n\n\n\n\npython\n\n\ninformation retrieval\n\n\ndeep learning\n\n\n\n\nI index 100k, 250k, 500k, 1M and 2M documents using T4 and RTX6000Ada instances and estimate the storage and CPU RAM requirements for a 12.6M document collection.\n\n\n\n\n\n\nWednesday, February 12, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the DAPR ConditionalQA Dataset with RAGatouille\n\n\n\n\n\n\n\npython\n\n\ninformation retrieval\n\n\ndeep learning\n\n\n\n\nI calculate the Recall@10 metric for answerai-colbert-small-v1 retrieval (via RAGatouille) on the ConditionalQA dataset (via UKPLab/DAPR dataset) using the pytrec and ranx libraries.\n\n\n\n\n\n\nSaturday, February 8, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoRA’s Magnitude Vector\n\n\n\n\n\n\n\npython\n\n\ndeep learning\n\n\nmachine learning\n\n\nLLM\n\n\n\n\nIn this blog post I highlight a key difference I saw between Raschka’s and peft’s implementation of DoRA.\n\n\n\n\n\n\nSaturday, February 1, 2025\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nRecreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille\n\n\n\n\n\n\n\npython\n\n\ninformation retrieval\n\n\nmachine learning\n\n\ndeep learning\n\n\n\n\nIn this blog post, I walk through the colbert research codebase (via AnswerAI’s RAGatouille) and work my way line-by-line through the 4-stage PLAID scoring pipeline to recreate RAGatouille results for a toy example of 1 query and 3 documents.\n\n\n\n\n\n\nTuesday, December 24, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post, I run retrieval on three differently preprocessed datasets using four retrieval methods from chunk sizes 100 to 2000 tokens, using my fastbook-benchmark dataset to auto-score the results. Surprisingly, full text search yields the best MRR@10 (0.67) and Recall@10 (0.95) for a chunk size of 2000 tokens.\n\n\n\n\n\n\nFriday, November 29, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nImplementing Image-to-Image Generation for Stable Diffusion\n\n\n\n\n\n\n\npython\n\n\nstable diffusion\n\n\nfastai\n\n\ndeep learning\n\n\nmachine learning\n\n\ngenerative AI\n\n\n\n\nIn this blog post I successfully implement image-to-image generation in the diffusion loop provided in Lesson 10 of the fastai course.\n\n\n\n\n\n\nWednesday, November 27, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset\n\n\n\n\n\n\n\npython\n\n\nfastbookRAG\n\n\ninformation retrieval\n\n\n\n\nIn this blog post, I perform retrieval on the fastbook chapter documents using 24 different retrieval method-chunking strategy combinations, auto-scoring using my fastbook-benchmark dataset.\n\n\n\n\n\n\nTuesday, November 26, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nImplementing Negative Prompting for Stable Diffusion\n\n\n\n\n\n\n\npython\n\n\nstable diffusion\n\n\nfastai\n\n\ndeep learning\n\n\nmachine learning\n\n\ngenerative AI\n\n\n\n\nIn this blog post I successfully implement negative prompting in the diffusion loop provided in Lesson 10 of the fastai course. I also explore some other relatively unsuccessful implementations that were interesting and informative nontheless.\n\n\n\n\n\n\nWednesday, November 20, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Cosine Similarity in Stable Diffusion’s Latent Space\n\n\n\n\n\n\n\nstable diffusion\n\n\ngenerative AI\n\n\npython\n\n\n\n\nIn this blog post I explore cosine similarity between conditioned and unconditioned UNet predictions and the sensitivity of UNet predictions in latent space!\n\n\n\n\n\n\nTuesday, November 19, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Classification with Qwen2-0.5B-Instruct\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I use Qwen2-0.5B-Instruct to classify sentiment in the financial_phrasebank dataset with 79.5% accuracy.\n\n\n\n\n\n\nMonday, November 18, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nWebGPU Puzzles: Walk through of Official Solutions\n\n\n\n\n\n\n\nAnswerAI\n\n\nWebGPU\n\n\n\n\nThis blog post contains my walkthrough of the official AnswerAI WebGPU Puzzle solutions that I found challenging to understand and/or critical in helping me understand core concepts of GPU programming.\n\n\n\n\n\n\nSunday, November 17, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nTraining Textual Inversion Embeddings on Some Samurai Jack Drawings\n\n\n\n\n\n\n\npython\n\n\nstable diffusion\n\n\ndeep learning\n\n\nmachine learning\n\n\n\n\nIn this blog post, I recap my experience (and results) with textual inversion embeddings trained on 6 sketches I created of Samurai Jack.\n\n\n\n\n\n\nWednesday, November 13, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Cosine Similarity Between Embeddings of Semantically Similar and Dissimilar Texts with Varying Punctuation\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\n\n\nIn this blog post, I calculate the cosine similarity between different embeddings for texts that have varying types of punctuation and semantic similarity\n\n\n\n\n\n\nFriday, November 8, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nEstablishing a Semantic Search (Embedding Cosine Similarity) Baseline for My fastbookRAG Project\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post, I experiment with 6 chunking/retrieval strategies to retrieve context from an array of text embeddings sufficient to answer 80.31% of the 193 fastbook end-of-chapter questions from Part 1 of the course.\n\n\n\n\n\n\nTuesday, October 22, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nConducting a Question-by-Question Error Analysis on Semantic Search Results\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post, I conduct a detailed error analysis of 29 questions (from a set of 193), where none of the 6 semantic search methods retrieved sufficient context to answer them. I examine each question, categorize the errors, and discuss potential improvements and implications for future work.\n\n\n\n\n\n\nTuesday, October 22, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with os.fork\n\n\n\n\n\n\n\npython\n\n\nfastai\n\n\n\n\nIn this blog post I work through four examples provided by Claude to understand some key concepts related to os.fork, and observe different behaviors when using os.fork in a notebook environment or the shell.\n\n\n\n\n\n\nMonday, September 30, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nGenerating a GIF Animation Using Stable Diffusion\n\n\n\n\n\n\n\npython\n\n\nstable diffusion\n\n\nfastai\n\n\ndeep learning\n\n\nmachine learning\n\n\ngenerative AI\n\n\n\n\nIn this blog post I repurpose the code provided in Lesson 9/10 of the fastai Part 2 course to generate an animation GIF transitioning from a picture of a skunk to a picture of a puppy.\n\n\n\n\n\n\nThursday, September 26, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Classification with Qwen2-1.5B-Instruct\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I use Qwen2-1.5B-Instruct to classify sentiment in the financial_phrasebank dataset with 86.1% accuracy.\n\n\n\n\n\n\nMonday, September 23, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Classification with phi-3.5\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I use phi-3.5 to classify sentiment in the financial_phrasebank dataset with 93.94% accuracy.\n\n\n\n\n\n\nThursday, September 12, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Classification with phi-3\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I use phi-3 to classify sentiment in the financial_phrasebank dataset with 92.79% accuracy.\n\n\n\n\n\n\nThursday, September 12, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Ratio of Gradients in an Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post I use the OpenCV library to calculate the ratio of the sum of non-zero x- and y-gradients to the sum of non-zero original pixels of an image. The serif font has consistently larger ratios than the sans serif font.\n\n\n\n\n\n\nMonday, September 9, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Ratio of Corners in an Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post I use the OpenCV library to calculate the ratio of the sum of non-zero corner pixels to the sum of non-zero original pixels of an image. The serif font has consistently larger ratios than the sans serif font.\n\n\n\n\n\n\nMonday, September 9, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Ratio of 2D FFT Magnitude and Phase of a Text Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post I use NumPy to calculate the ratio of the mean of 2D FFT magnitude to the count of non-zero binarized pixels (“FFT Magnitude Ratio”) and the ratio of the sum of the absolute value of 2D FFT phase to the sum of binarized pixels (“FFT Phase Ratio”). Both ratios are consistently larger for images with serif text.\n\n\n\n\n\n\nMonday, September 9, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Ratio of Letter Perimeter to Area\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post I use the OpenCV library to calculate the ratio of letter (contour) perimeter to area. The serif font has consistently larger ratios than the sans serif font.\n\n\n\n\n\n\nFriday, September 6, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nConducting a Question-by-Question Error Analysis on Full Text Search Results\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post, I conduct a detailed error analysis of 39 questions from a set of 202, where none of the 6 full text search methods retrieved sufficient context to answer them. I examine each question, categorize the errors, and discuss potential improvements and implications for future work.\n\n\n\n\n\n\nThursday, September 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nEstablishing a Full Text Search (BM25) Baseline for My fastbookRAG Project\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post, I experiment with 6 chunking/retrieval strategies to retrieve context from a sqlite database sufficient to answer 76.7% of the 202 fastbook end-of-chapter questions from Part 1 of the course.\n\n\n\n\n\n\nTuesday, September 3, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing ~100k Random Numbers Generated with Different Methods\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\ndeep learning\n\n\n\n\nIn this blog post, I generate close to 100k random numbers using 5 different methods: ANU Quantum numbers, Python’s random module, NumPy, PyTorch and a custom implementation from Lesson 10 of the fastai course (Part 2). I am surprised by the results!\n\n\n\n\n\n\nTuesday, September 3, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Matrix Class in Python\n\n\n\n\n\n\n\npython\n\n\ndeep learning\n\n\nmachine learning\n\n\n\n\nIn this blog post I implement a Matrix class following Lesson 10 of the fastai course (Part 2) and add some additional functionality.\n\n\n\n\n\n\nMonday, September 2, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Classification with phi-2\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I use phi-2 to classify sentiment in the financial_phrasebank dataset with 92% accuracy.\n\n\n\n\n\n\nSaturday, August 31, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Squareness of Letters in an Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post I use the OpenCV library to calculate the “squareness” of letters (minimum dimension to maximum dimension of bounded rectangle around letter) in a text image.\n\n\n\n\n\n\nSaturday, August 31, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Classification with Claude Using claudette\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I use Sonnet (94% accuracy), Opus (94%) and Haiku (92%) to classify sentiment in the financial_phrasebank dataset.\n\n\n\n\n\n\nThursday, August 29, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Ratio of Horizontal to Vertical Details in a Text Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post I use the PyWavelets library to calculate the ratio of horizontal to vertical details in text image.\n\n\n\n\n\n\nWednesday, August 28, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nIterating on Full Text Search Keywords using claudette\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nLLM\n\n\nfastbookRAG\n\n\n\n\nIn this blog post, I use Answer.AI’s claudette library to iteratively improve keywords generated for sqlite’s full text search.\n\n\n\n\n\n\nTuesday, August 27, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nIs it a Digit?\n\n\n\n\n\n\n\npython\n\n\nfastai\n\n\ndeep learning\n\n\ncomputer vision\n\n\n\n\nIn this notebook, I train a model on noisy MNIST images and deploy it in a HuggingFace Space. The model is then used to predict the probability that a user-drawn figure on a canvas is a digit.\n\n\n\n\n\n\nMonday, August 26, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Full Text Search Keywords using claudette\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nLLM\n\n\nfastbookRAG\n\n\n\n\nIn this blog post, I use Answer.AI’s claudette library to interface with the Claude-3.5 Sonnet API.\n\n\n\n\n\n\nSunday, August 25, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning TinyStories-3M on the financial_phrasebank Dataset\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I fine-tune the TinyStories-3M model on the financial_phrasebank dataset and achieve 74%+ accuracy on the validation and test set.\n\n\n\n\n\n\nThursday, August 22, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning TinyStories-1M on the financial_phrasebank Dataset\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I fine-tune the TinyStories-1M model on the financial_phrasebank dataset and achieve 68%+ accuracy on the validation and test set.\n\n\n\n\n\n\nThursday, August 22, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Letter Area Ratio in a Text Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post, as I develop a non-ML baseline for image typeface classification, I use the OpenCV library to calculate the average letter area ratio in a text image.\n\n\n\n\n\n\nWednesday, August 21, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning TinyStories-8M on the financial_phrasebank Dataset\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I fine-tune the smaller TinyStories-8M model on the financial_phrasebank dataset and achieve 86% accuracy on the test set and 85% accuracy on the validation set.\n\n\n\n\n\n\nMonday, August 19, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Average Stroke Width of Letters in a Text Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post, as I develop a non-ML baseline for image typeface classification, I use the OpenCV library to calculate the average stroke width of the letters in a text image.\n\n\n\n\n\n\nMonday, August 19, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Aspect Ratio of Letters in a Text Image\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post, as I develop a non-ML baseline for image typeface classification, I use the OpenCV library to calculate the aspect ratio (width/height) of each letter in a text image.\n\n\n\n\n\n\nWednesday, August 14, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning TinyStories-33M on the financial_phrasebank Dataset\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I fine-tune the TinyStories-33M model on the financial_phrasebank dataset and achieve 79%+ accuracy on the validation and test set.\n\n\n\n\n\n\nTuesday, August 13, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nDetermining the Ratio of Lowercase to Uppercase Letter Heights Using Signal Analysis\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\nTypefaceClassifier\n\n\n\n\nIn this blog post I apply concepts from signal analysis to determine the ratio of lowercase-to-uppercase letter heights based on an image of text.\n\n\n\n\n\n\nMonday, August 12, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Hybrid Search to Answer fastai the Chapter 1 Questionnaire\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post I use different approaches to combine FTS5 (keyword search) and Cosine Similarity (semantic search) to retrieve context necessary to answer questions about Chapter 1 of the fastai textbook.\n\n\n\n\n\n\nSunday, August 11, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nHow Does Stable Diffusion Work?\n\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\nfastai\n\n\nstable diffusion\n\n\ngenerative AI\n\n\n\n\nIn this blog post I review the material taught in Lesson 9 of the fastai course (Part 2: Deep Learning Foundations to Stable Diffusion).\n\n\n\n\n\n\nThursday, August 8, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Cosine Similarity to Retrieve Context to Answer fastbook Questionnaire\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post I am able to answer 76% of the fastbook Chapter 1 Questionnaire using cosine similarity between the question and the chunked chapter text.\n\n\n\n\n\n\nTuesday, August 6, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing TinyInstruct-33M for financial_phrasebank Sentiment Classification\n\n\n\n\n\n\n\npython\n\n\nLLM\n\n\nTinySentiment\n\n\n\n\nIn this blog post I find that TinyInstruct-33M does not follow instructions that deviate from its training data.\n\n\n\n\n\n\nMonday, August 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Full Text Search to Answer the fastbook Chapter 1 Questionnaire\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post I’ll walk through my experiments of using sqlite full text search to retrieve context relevant to answering chapter review questions. This is part of a larger fastbookRAG proejct I’m work on.\n\n\n\n\n\n\nSunday, August 4, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Flesch Kincaid Reading Grade Level for the financial_phrasebank Dataset\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\ndeep learning\n\n\n\n\nIn this blog post I calculate the Flesch Kincaid reading grade level for the financial_phrasebank dataset and find that it’s much higher than the average TinyStories reading level.\n\n\n\n\n\n\nSaturday, August 3, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Flesch Kincaid Reading Grade Level for the TinyStories Dataset\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\ndeep learning\n\n\nTinySentiment\n\n\n\n\nIn this blog post I explore the Flesch Kincaid reading grade level formula and implementation in the textstat library to calculate it for the TinyStories dataset.\n\n\n\n\n\n\nFriday, August 2, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBM25 and Cosine Similarity Demo\n\n\n\n\n\n\n\npython\n\n\nRAG\n\n\ninformation retrieval\n\n\nfastbookRAG\n\n\n\n\nIn this blog post I walk through a couple of toy examples using full text search and cosine similarity to answer queries using a toy dataset of documents.\n\n\n\n\n\n\nWednesday, July 31, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nPaper Math: rsLoRA\n\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\npaper math\n\n\nLLM\n\n\n\n\nIn this blog post I think out loud as I attempt to understand pieces of the math presented in the rsLoRA paper.\n\n\n\n\n\n\nThursday, July 25, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learnings For Coders - Part 1 Notes and Examples\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nmachine learning\n\n\npython\n\n\n\n\nThis notebook contains all of my notes on the videos, notebooks and book chapters covered in Part 1 of the Practical Deep Learning for Coders fastai course.\n\n\n\n\n\n\nSunday, July 14, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Models on the MovieLens 25M Dataset\n\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I train models using 5 different architectures on the 25 million rating MovieLens dataset and compare performance and results.\n\n\n\n\n\n\nSaturday, July 13, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing CNN Performance by Varying Activation Normalization Layers\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I train CNNs using different activation normalization layers and compare performance and results.\n\n\n\n\n\n\nSunday, July 7, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing CNN Performance by Varying Batch Normalization Placement\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I place the activation function before and after the batch normalization layer in a CNN and compare the model performance and results.\n\n\n\n\n\n\nTuesday, July 2, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nTraining a Collaborative Filtering Model Using Cross Entropy Loss\n\n\n\n\n\n\n\nmachine learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I create a collaborative filtering (classifier) architecture suited to use with cross-entropy loss.\n\n\n\n\n\n\nMonday, July 1, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Collaborative Filtering Applications\n\n\n\n\n\n\n\nmachine learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I explore 3-4 areas where collaborative filtering is used, citing examples from research and commercial publications.\n\n\n\n\n\n\nWednesday, June 26, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nComparing PyTorch Embeddings with Custom Embeddings\n\n\n\n\n\n\n\nmachine learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I compare the code required to build a collaborative filtering model using PyTorch Embeddings and custom embeddings.\n\n\n\n\n\n\nTuesday, June 25, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nExploring Different Feature Detection Algorithms in Computer Vision\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\n\n\nIn this notebook I run code examples for different non-ML Computer Vision feature detection algorithms.\n\n\n\n\n\n\nMonday, June 24, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nTraining Collaborative Filtering Models on MovieLens 100k with Different Weight Decay Values\n\n\n\n\n\n\n\nmachine learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I explore the question—how does the wd (weight decay) parameter affect model performance and weight distributions? I use the MovieLens 100k subset as the dataset.\n\n\n\n\n\n\nMonday, June 3, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Collaborative Filtering Models on MovieLens 100k with Different y_range Values\n\n\n\n\n\n\n\nmachine learning\n\n\nfastai\n\n\npython\n\n\n\n\nIn this notebook I explore the question—how does the y_range parameter affect model performance and prediction distributions? I use the MovieLens 100k subset as the dataset.\n\n\n\n\n\n\nThursday, May 23, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Code in fastai’s LabelSmoothingCrossEntropy\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\npython\n\n\n\n\nInspired by Aman Arora’s blog post, I walk through code of the fastai function LabelSmoothingCrossEntropy.\n\n\n\n\n\n\nTuesday, May 21, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nInitial Reaction: Eagles 2024 NFL Schedule\n\n\n\n\n\n\n\nnfl\n\n\neagles\n\n\n\n\nA knee-jerk reaction to the 2024 NFL schedule release.\n\n\n\n\n\n\nWednesday, May 15, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nImproving Kaggle Private Score with Multi-Target Classification\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I apply Jeremy Howard’s approach to multi-target classification in fastai to improve a Kaggle submission score.\n\n\n\n\n\n\nWednesday, May 15, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nPaper Summary: RewardBench\n\n\n\n\n\n\n\npaper summary\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nA summary of research benchmarking reward models.\n\n\n\n\n\n\nFriday, April 26, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Math and Summary: ORPO (Odds Ratio Preference Optimization)\n\n\n\n\n\n\n\npaper math\n\n\npaper summary\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nSummarizing the research from the Odds Ratio Preference Optimization paper and exploring its math to better understand it.\n\n\n\n\n\n\nFriday, April 19, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecap: HMS HBAC Kaggle Competition\n\n\n\n\n\n\n\nfastai\n\n\nkaggle competition\n\n\ndeep learning\n\n\n\n\nA recap of what and how I did on the Harvard Medical Harmful Brain Activity Classification Kaggle Competition.\n\n\n\n\n\n\nTuesday, April 16, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Math: KTO (Kahneman Tversky Optimization)\n\n\n\n\n\n\n\npaper math\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nExploring the math from the Kahneman Tversky Optimization paper to better understand it.\n\n\n\n\n\n\nFriday, April 12, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Math: DPO (Direct Preference Optimization)\n\n\n\n\n\n\n\npaper math\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nExploring the math from the Direct Preference Optimization paper to better understand it.\n\n\n\n\n\n\nFriday, April 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Summary: Attention is All You Need\n\n\n\n\n\n\n\npaper summary\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nA summary of research introducing the Transformer architecture and a code walkthrough for the Encoder and Decoder.\n\n\n\n\n\n\nSaturday, March 30, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Summary: Constitutional AI\n\n\n\n\n\n\n\npaper summary\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nA summary of research on Constitutional AI by Anthropic, in which they train a non-evasive harmless AI assistant using human-generated helpfulness preference data and AI-generated harmlessness preference data.\n\n\n\n\n\n\nFriday, March 22, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecap: My First Live Kaggle Competition\n\n\n\n\n\n\n\nfastai\n\n\nkaggle competition\n\n\nmachine learning\n\n\n\n\nA recap of what and how I did on the Multi-Class Prediction of Obesity Risk Kaggle Competition.\n\n\n\n\n\n\nThursday, February 29, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Summary: Training Data for the Price of a Sandwich\n\n\n\n\n\n\n\nTrustworthy AI\n\n\nLLM\n\n\n\n\nIn this blog post I summarize the discussion in the paper ‘Training Data for the Price of a Sandwich: Common Crawl’s Impact on Generative AI’ by Stefan Baack and Mozilla Insights.\n\n\n\n\n\n\nMonday, February 19, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nPaper Summary: Textbooks are All You Need I & II\n\n\n\n\n\n\n\npaper summary\n\n\ndeep learning\n\n\nLLM\n\n\n\n\nA summary of research on the phi-1, phi-1.5 and phi-2 from the Textbook Are All You Need I and II series of publications by Microsoft Research.\n\n\n\n\n\n\nMonday, February 19, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 8\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I apply to my large ensemble Jeremy Howard’s approach in the “Scaling Up - Road to the Top, Part 3” notebook.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 7\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I run the code from Jeremy Howard’s “Scaling Up - Road to the Top, Part 3” notebook.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 6\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I work through Jeremy Howard’s Live Coding 13 video in which he finishes working on the Paddy Doctor Disease Classification Kaggle Competition.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 5\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I work through Jeremy Howard’s Live Coding 12 video in which he continues working on the Paddy Doctor Disease Classification Kaggle Competition.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 4\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I work through Jeremy Howard’s Live Coding 11 video in which he continues working on the Paddy Doctor Disease Classification Kaggle Competition.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 3\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I work through Jeremy Howard’s Live Coding 10 video in which he continues working on the Paddy Doctor Disease Classification Kaggle Competition.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 2\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I work through Jeremy Howard’s Live Coding 9 video in which he continues working on the Paddy Doctor Disease Classification Kaggle Competition.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaddy Doctor Kaggle Competition - Part 1\n\n\n\n\n\n\n\ndeep learning\n\n\nfastai\n\n\nkaggle competition\n\n\npaddy doctor\n\n\npython\n\n\n\n\nIn this notebook I work through Jeremy Howard’s Live Coding 8 video in which he starts working on the Paddy Doctor Disease Classification Kaggle Competition.\n\n\n\n\n\n\nMonday, February 5, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nRegardless, Go Birds\n\n\n\n\n\n\n\nnfl\n\n\neagles\n\n\n\n\nA reflection on the 2023 Philadelphia Eagles season.\n\n\n\n\n\n\nWednesday, January 17, 2024\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nPrompting LLMs Using Different Prompting Styles\n\n\n\n\n\n\n\ndeep learning\n\n\nLLM\n\n\npython\n\n\n\n\nIn this notebook I use 20 math reasoning dataset questions to prompt three 7B-parameter LLMs using 3 different prompting styles.\n\n\n\n\n\n\nThursday, November 2, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nPaper Summary: Plan-and-Solve Prompting\n\n\n\n\n\n\n\ndeep learning\n\n\nLLM\n\n\npython\n\n\n\n\nIn this notebook I summarize the findings from the paper “Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models” by Lei Wang, et al.\n\n\n\n\n\n\nFriday, October 20, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the fastai TabularModel Class\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I walk through line-by-line the source code of the fastai TabularModel class.\n\n\n\n\n\n\nTuesday, October 10, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Neural Net Embeddings to Improve a Random Forest\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I replace categorical features in a dataset with corresponding neural net embedding outputs to improve the performance of a random forest.\n\n\n\n\n\n\nMonday, October 9, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nImplementing a Decision Tree Algorithm from Scratch\n\n\n\n\n\n\n\nmachine learning\n\n\npython\n\n\n\n\nIn this notebook I implement a decision tree algorithm from scratch and compare it to the algorithm from Lesson 6 of the fastai course.\n\n\n\n\n\n\nThursday, September 28, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Random Forest for the Kaggle Zillow Competition Dataset\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I follow the techniques presented in the fastai textbook to train a leaderboard-beating random forest.\n\n\n\n\n\n\nThursday, September 14, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning a Language Model Using LoRA\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I want to compare fine-tuning a pretrained model with and without using LoRA.\n\n\n\n\n\n\nFriday, September 1, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing HuggingFace Transformers for Tabular Titanic Data\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I run a fun experiment to see how well a NLP model can predict tabular data.\n\n\n\n\n\n\nWednesday, August 23, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining a Multi-Target Regression Deep Learning Model with fastai\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I train single and multi-target regression tabular deep learning models using fastai, and compare the results.\n\n\n\n\n\n\nSaturday, August 19, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a From-Scratch Deep Learning Model to Predict Two Variables\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I build a deep learning model from scratch which can predict two dependent variables from a tabular dataset.\n\n\n\n\n\n\nThursday, August 17, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Losses and Accuracy for 100 Deep Neural Net Training Runs\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I modify existing code to record and plot training values for a simple deep neural net model.\n\n\n\n\n\n\nTuesday, August 15, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tuning a Language Model as a Text Classifier\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I work through the chapter exercise presented in Chapter 10 of the fastai textbook.\n\n\n\n\n\n\nSaturday, August 5, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing an MNIST Classifier From Scratch\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this blog post, I write a MNISTLearner class which trains a neural net to classify the full MNIST dataset of 10 handwritten digits.\n\n\n\n\n\n\nMonday, July 24, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a fastai Learner from Scratch\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this blog post, I write a BasicLearner class which trains a neural net to classify handwritten digits.\n\n\n\n\n\n\nSunday, July 23, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Pandas - Notes and Exercises\n\n\n\n\n\n\n\ndata analysis\n\n\npython\n\n\n\n\nAn update on my progress in the book “Effective Pandas” by Matt Harrison.\n\n\n\n\n\n\nThursday, July 6, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization Analysis & Design - Excerpts\n\n\n\n\n\n\n\n\n\n\n\n\nSaturday, May 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTranscribing Sherlock into Spanish\n\n\n\n\n\n\n\nspanish\n\n\n\n\nAn update on my goal to transcribe BBC’s Sherlock into Spanish.\n\n\n\n\n\n\nMonday, February 20, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nMaking Chicken Wings\n\n\n\n\n\n\n\nfood\n\n\n\n\nAn update on my goal to make 10/10 chicken wings by the 2023 NFL season.\n\n\n\n\n\n\nSunday, February 19, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nExploring NFL Play-by-Play Data with SQL and Python\n\n\n\n\n\n\n\ndata analysis\n\n\nSQL\n\n\npython\n\n\n\n\nAn update on my analysis and visualization of NFL play-by-play data.\n\n\n\n\n\n\nSaturday, February 11, 2023\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nVisualize U.S. Census Data with ArcGIS\n\n\n\n\n\n\n\nArcGIS\n\n\ndata analysis\n\n\n\n\nA tutorial to create a geodatabase, maps and layouts to visualize U.S. Census Data in ArcGIS Pro\n\n\n\n\n\n\nSunday, September 26, 2021\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nR Shiny Census App\n\n\n\n\n\n\n\nR\n\n\ndata analysis\n\n\nSQL\n\n\n\n\nAn explanation of my development process for a census data shiny app\n\n\n\n\n\n\nTuesday, September 21, 2021\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 8: Collaborative Filter Deep Dive\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nFriday, June 4, 2021\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 7:Test Time Augmentation\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSaturday, May 29, 2021\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Bear Classifier\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSunday, April 25, 2021\n\n\nVishal Bakshi\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Classification Models\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMonday, April 12, 2021\n\n\nVishal Bakshi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "my name is vishal. i’m a data analyst."
  },
  {
    "objectID": "about.html#things-i-like-consistently",
    "href": "about.html#things-i-like-consistently",
    "title": "About",
    "section": "things i like consistently",
    "text": "things i like consistently\ncoding, writing, hip hop, shooting hoops, reading, cold weather, philadelphia eagles."
  },
  {
    "objectID": "about.html#what-im-working-on-consistently",
    "href": "about.html#what-im-working-on-consistently",
    "title": "About",
    "section": "what i’m working on consistently",
    "text": "what i’m working on consistently\n\nmachine learning.\nunderstanding my thoughts, emotions, behaviors, speech and actions.\nlearning how and why our society is structured the way it is (and could be)."
  },
  {
    "objectID": "posts/2024-11-19-diffusion-cosine-similarity/index.html",
    "href": "posts/2024-11-19-diffusion-cosine-similarity/index.html",
    "title": "Exploring Cosine Similarity in Stable Diffusion’s Latent Space",
    "section": "",
    "text": "Show setup code\n!pip install -qq diffusers transformers==4.46.2\n!pip install -qq pillow==11.0.0\n\nfrom diffusers import LMSDiscreteScheduler, AutoencoderKL, UNet2DConditionModel, StableDiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch, math\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nbeta_start,beta_end = 0.00085,0.012\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1"
  },
  {
    "objectID": "posts/2024-11-19-diffusion-cosine-similarity/index.html#background",
    "href": "posts/2024-11-19-diffusion-cosine-similarity/index.html#background",
    "title": "Exploring Cosine Similarity in Stable Diffusion’s Latent Space",
    "section": "Background",
    "text": "Background\nIn this notebook, I’ll explore the question: how different are UNet predictions (in latent space) for the conditioned and unconditioned (empty) prompts? The code used in this notebook comes from the “Stable Diffusion with Diffusers” from part 2 of the fastai course."
  },
  {
    "objectID": "posts/2024-11-19-diffusion-cosine-similarity/index.html#generating-images",
    "href": "posts/2024-11-19-diffusion-cosine-similarity/index.html#generating-images",
    "title": "Exploring Cosine Similarity in Stable Diffusion’s Latent Space",
    "section": "Generating Images",
    "text": "Generating Images\nI’ll start by running the code below to generate an image using stable diffusion:\n\ntext_enc tokenizes the given prompt and converts it to text embeddings using the text_encoder(“openai/clip-vit-large-patch14”).\nmk_img converts a Tensor to a PIL Image.\nmk_samples takes the prompt, guidance scale, seed, and number of inference steps to run the diffusion loop and generate a image using the scheduler (LMSDiscreteScheduler), UNet (\"CompVis/stable-diffusion-v1-4\") and VAE (\"stabilityai/sd-vae-ft-ema\").\n\n\ndef text_enc(prompts, maxlen=None):\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\ndef mk_img(t):\n    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\n\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        pred = u + g*(t-u)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nprompts = [\n    'a photograph of an astronaut riding a horse',\n    'an oil painting of an astronaut riding a horse in the style of grant wood'\n]\n\nimages = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/2926544816.py:8: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two images are generated as expected!"
  },
  {
    "objectID": "posts/2024-11-19-diffusion-cosine-similarity/index.html#classifier-free-guidance",
    "href": "posts/2024-11-19-diffusion-cosine-similarity/index.html#classifier-free-guidance",
    "title": "Exploring Cosine Similarity in Stable Diffusion’s Latent Space",
    "section": "Classifier-Free Guidance",
    "text": "Classifier-Free Guidance\nThe lines of code I’m most interested in for this notebook are:\nwith torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\npred = u + g*(t-u)\nIn the first line, the unet takes the noisy input latents inp, timestep ts and text embeddings emb to predict the noise in the noisy latent. In the second line, the “final” prediction is taken as u + g*(t-u) where t and u are both predictions form the same UNet:\n\nu is the predicted noise corresponding to the unconditioned prompt (\"\").\nt is the predicted noise corresponding to the conditioned prompt (e.g., 'a photograph of an astronaut riding a horse'),\ng is the guidance scale which is the amount we want to weight the prediction towards t and away from u.\n\nExpanding this equation we get: pred = u + gt - gu = gt - (g-1)u. We amplify t by g and then subtract (g-1) times u. Conceptually, we are moving away from the empty prompt and toward the desired prompt. My understanding of why we need u is that we need a baseline or reference point to which we compare t so that we can guide the generation process to learn specific features of t in comparison to general noisy features of of u. This “comparison” is done with the t-u term."
  },
  {
    "objectID": "posts/2024-11-19-diffusion-cosine-similarity/index.html#cosine-similarity-between-u-and-t",
    "href": "posts/2024-11-19-diffusion-cosine-similarity/index.html#cosine-similarity-between-u-and-t",
    "title": "Exploring Cosine Similarity in Stable Diffusion’s Latent Space",
    "section": "Cosine Similarity Between u and t",
    "text": "Cosine Similarity Between u and t\nI’m going to modify mk_samples so that at each timestep, it calculates and stores the cosine similarity betwern t and u:\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    cs = []\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n       \n        cs.append(torch.nn.functional.cosine_similarity(t, u).mean().item())\n        \n        pred = u + g*(t-u)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): \n        return vae.decode(1 / 0.18215 * latents).sample, cs\n\n\n\nprompts = [\n    'a photograph of an astronaut riding a horse'\n]\n\nimages, cs = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/1126956742.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\nI was shocked to find that the cosine similarity between the unconditioned and the conditioned UNet predictions is so high! Essentially 1.0!!\n\npd.Series(cs).describe()\n\ncount    70.000000\nmean      0.999616\nstd       0.000261\nmin       0.998535\n25%       0.999512\n50%       0.999512\n75%       0.999878\nmax       1.000000\ndtype: float64\n\n\nThe two predictions start out basically exactly the same (cosine similarity = 1) and diverge as the diffusion process goes on, reaching the lowest cosine similarity at the 70 inference steps.\n\nplt.scatter(range(len(cs)),cs);\n\n\n\n\n\n\n\n\nThis trend of high mean similarity between t and u overall and decreasing cosine similarity over time holds for other prompts as well:\n\nprompts = [\n    'a painting of a dog in the style of Picasso'\n]\n\nimages, cs = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/1126956742.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\npd.Series(cs).describe()\n\ncount    70.000000\nmean      0.999923\nstd       0.000197\nmin       0.999023\n25%       1.000000\n50%       1.000000\n75%       1.000000\nmax       1.000000\ndtype: float64\n\n\n\nplt.scatter(range(len(cs)),cs);\n\n\n\n\n\n\n\n\n\nprompts = [\n    'a drawing of a woman sitting on a park bench'\n]\n\nimages, cs = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/1126956742.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\npd.Series(cs).describe()\n\ncount    70.000000\nmean      0.999679\nstd       0.000233\nmin       0.999512\n25%       0.999512\n50%       0.999512\n75%       1.000000\nmax       1.000000\ndtype: float64\n\n\n\nplt.scatter(range(len(cs)),cs);"
  },
  {
    "objectID": "posts/2024-11-19-diffusion-cosine-similarity/index.html#perturbations-in-pred",
    "href": "posts/2024-11-19-diffusion-cosine-similarity/index.html#perturbations-in-pred",
    "title": "Exploring Cosine Similarity in Stable Diffusion’s Latent Space",
    "section": "Perturbations in pred",
    "text": "Perturbations in pred\nMy takeaway from the high cosine similarity between conditioned and unconditioned predictions is that the diffusion process is sensitive to small perturbations. However, I found that this wasn’t always the case. To illustrate, I’ll first add a large amount of noise to pred at each timestep and see how that impacts the resulting image.\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    cs = []\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n       \n        cs.append(torch.nn.functional.cosine_similarity(t, u).mean().item())\n        \n        r = 0.1*torch.randn_like(t)\n        orig_pred = u + g*(t-u)\n        pred = u + g*(t-u) + r\n        \n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): \n        return vae.decode(1 / 0.18215 * latents).sample, cs, orig_pred, r\n\n\n\nprompts = [\n    'a photograph of an astronaut riding a horse'\n]\n\nimages, cs, orig_pred, r = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/2326634756.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\norig_pred.norm(), r.norm()\n\n(tensor(27.3281, device='cuda:0', dtype=torch.float16),\n tensor(12.6328, device='cuda:0', dtype=torch.float16))\n\n\nAdding a noise tensor with about half of the magnitude as the full noise prediction did not prevent the diffusion loop from generating a high quality image! At what point does random noise impact generation?\n\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    cs = []\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n       \n        cs.append(torch.nn.functional.cosine_similarity(t, u).mean().item())\n        \n        r = 0.2*torch.randn_like(t)\n        orig_pred = u + g*(t-u)\n        pred = u + g*(t-u) + r\n        \n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): \n        return vae.decode(1 / 0.18215 * latents).sample, cs, orig_pred, r\n\n\nprompts = [\n    'a photograph of an astronaut riding a horse'\n]\n\nimages, cs, orig_pred, r = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/2882368180.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\norig_pred.norm(), r.norm()\n\n(tensor(26.2188, device='cuda:0', dtype=torch.float16),\n tensor(25.2656, device='cuda:0', dtype=torch.float16))\n\n\nAdding random noise equal in magnitude to the guided prediction does visibly affect the quality of the generated image, though it still maintains its main structural components (earth, astronaut, horse).\nWhat happens if instead of random noise, I add a UNet prediction based on some other unrelated prompt?\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    cs = []\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    other = text_enc([\"a toad riding a bicycle\"] * bs, text.shape[1])\n    \n    emb = torch.cat([uncond, text, other])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        with torch.no_grad(): u,t,o= unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n       \n        cs.append(torch.nn.functional.cosine_similarity(t, u).mean().item())\n        \n        orig_pred = u + g*(t-u)\n        pred = u + g*(t-u) + 0.2*o\n        \n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): \n        return vae.decode(1 / 0.18215 * latents).sample, cs, orig_pred, o\n\n\n\nprompts = [\n    'a photograph of an astronaut riding a horse'\n]\n\nimages, cs, orig_pred, o = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/70226676.py:11: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\norig_pred.norm(), (0.2*o).norm()\n\n(tensor(98.5625, device='cuda:0', dtype=torch.float16),\n tensor(19.9688, device='cuda:0', dtype=torch.float16))\n\n\nNow that I’ve introduced text embeddings related to a real prompt with a similar magnitude as the random noise, it’s significantly impacting the image generation result! Which by the way looks super cool.\n\nprompts = [\n    'a drawing of a woman sitting on a park bench'\n]\n\nimages, cs, orig_pred, o = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_34/70226676.py:11: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\norig_pred.norm(), (0.2*o).norm()\n\n(tensor(110.1250, device='cuda:0', dtype=torch.float16),\n tensor(22.0625, device='cuda:0', dtype=torch.float16))\n\n\nI get a similar result using a different prompt."
  },
  {
    "objectID": "posts/2024-11-19-diffusion-cosine-similarity/index.html#final-thoughts",
    "href": "posts/2024-11-19-diffusion-cosine-similarity/index.html#final-thoughts",
    "title": "Exploring Cosine Similarity in Stable Diffusion’s Latent Space",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI hesitate to make any strong conclusions about the diffusion process as I’ve spent only a couple dozen hours experimenting with stable diffusion, but do want to summarize my observations from these experiments:\n\nUnconditioned and conditioned prompt UNet predictions are similar: this was seen by the (very) high cosine similarity between the two at each time step, with cosine similarity decreasing as the time step increased.\nSmall amounts of random noise doesn’t impact image generation: while the term “small” is relative, adding random noise that was 50% of the original guided prediction’s norm did not visibly alter the image. Even after adding random noise with the same magnitude as the original guided prediction, the generated image still maintained its core composition and structure.\nAdding another prompt’s UNet predictions drastically changes the generated image: while random noise of the same magnitude did not impact the output image, adding UNet predictions for an unrelated prompt completely changes the color, features and structure of the image (while still maintaining some thematic elements).\n\nThese observations make me wonder the following questions, that I’ll keep in mind throughout part 2 of the fastai course:\n\nIs the small difference in direction between t and u the reason we need relatively large guidance scale values? Is it also the reason why if the guidance scale is too large, we start losing structural features of the desired prompt/image?\nDoes the fact that structured UNet predictions (as opposed to random noise) impact image generation significantly mean that UNet’s are not sensitive to random noise but are generally susceptible to structured data in the latent space? Why is that?\n\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html",
    "href": "posts/2024-11-20-negative-prompting/index.html",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "",
    "text": "In Lesson 10 of the fastai course (Part 2) Jeremy assigns us the following homework assignment:\n\ntry picking one of the extra tricks we learned about like image-to-image, or negative prompts; see if you can implement negative prompt in your version of this; or try doing image-to-image; try adding callbacks\n\nIn this blog post I’ll implement negative prompting using the diffusion loop code provided in the course’s Stable Diffusion with Diffusers notebook.\nI’ll start by copy/pasting all of the boilerplate code provided in that notebook, and running it to make sure we get the desired images.\n\n\nShow setup\n!pip install -qq diffusers transformers==4.46.2\n!pip install -qq pillow==11.0.0\n\nfrom diffusers import LMSDiscreteScheduler, AutoencoderKL, UNet2DConditionModel, StableDiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom IPython.display import display\nimport torch, math\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nbeta_start,beta_end = 0.00085,0.012\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\n\n\n\nShow stable diffusion implementation functions\ndef text_enc(prompts, maxlen=None):\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\ndef mk_img(t):\n    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        pred = u + g*(t-u)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\n\nprompts = [\n    'a photograph of an astronaut riding a horse',\n    'an oil painting of an astronaut riding a horse in the style of grant wood'\n]\n\n\nimages = mk_samples(prompts)\nfor img in images: display(mk_img(img))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooks good! These are the same two images generated in the course notebook."
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html#negative-prompting",
    "href": "posts/2024-11-20-negative-prompting/index.html#negative-prompting",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "Negative Prompting",
    "text": "Negative Prompting\nWhat is negative prompting? I’ll illustrate an example using a stable diffusion pipeline:\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n\nHere is the generated image for the prompt “Labrador in the style of Vermeer”.\n\ntorch.manual_seed(1000)\nprompt = \"Labrador in the style of Vermeer\"\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\nAnd here’s the result after passing a negative prompt: “blue”\n\ntorch.manual_seed(1000)\npipe(prompt, negative_prompt=\"blue\").images[0]\n\n\n\n\n\n\n\n\n\n\n\nLooking at these images side-by-side, without the negative prompt (left) and with the negative prompt (right) we see that with negative prompting, the blue hat and scarf are replaced with black ones. Additionally, the labrador’s eyes, snout, nose, ears and other features have also slightly changed.\nIt’s important to note that not all seeds return such desired results. For example, here is another lab in the style of Vermeer, notice the blue head scarf.\n\ntorch.manual_seed(100)\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\nThe negative prompt result does remove the blue from the image, but it also considerably changes other features of the image.\n\ntorch.manual_seed(100)\npipe(prompt, negative_prompt=\"blue\").images[0]\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal Image\nI’ll choose the following generated image as the baseline image, using the same prompt and number of steps as the stable diffusion pipe generation, but now using the code from Lesson 10:\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, seed=18, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_75/2343166050.py:8: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))"
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html#working-implementation-replace-u-with-n",
    "href": "posts/2024-11-20-negative-prompting/index.html#working-implementation-replace-u-with-n",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "Working Implementation: Replace u with n",
    "text": "Working Implementation: Replace u with n\nI tried out four different implementations of negative prompting. Three of them did not succeed but I did learn a lot from implementing them. I’ll go into detail into those approaches later on in this post. The working solution I came up with was relatively simpler than the first two implementations I attempted.\nIn the diffusion loop, the following line of code is key:\npred = u + g*(t-u)\nu is the “starting point” or “reference point” for our predicted noise. It represents some general noisy-image features. t is our desired direction, and g is the guidance scale, amplifying the difference between t and u. Overall, we are moving away from u and towards t.\nIn this case, u is the UNet noise prediction given the unconditioned (empty string) prompt, and t is the UNet noise prediction given the desired prompt. Moving away from one prompt and moving towards another prompt sounded to me exactly like the goal of negative prompting. We want to move away from our negative prompt and towards our desired prompt. To implement this, I simply added a negative_prompt that defaults to an empty string (our unconditioned prompt scenario), passed this string to text_enc in the following line:\nuncond = text_enc([negative_prompt] * bs, text.shape[1])\nAnd otherwise kept the mk_samples code unchanged.\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, negative_prompt = \"\", g=7.5, seed=100, steps=70):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([negative_prompt] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        pred = u + g*(t-u)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nHere is the output of mk_samples with no negative_prompt specific, this is essentially our normal classifier-free guidance implementation:\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, seed=18, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/3351458932.py:8: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\nAnd here is the output with a negative_prompt provided:\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=18, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/3351458932.py:8: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\nThis results in a (somewhat) desired image: ✅ the blue clothing has been removed (and changed to black) but ❌ the image structure and composition has considerably changed.\nI found a seed (20) which performs better. Here is the original unconditioned prompt result:\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, seed=20, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/3351458932.py:8: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\nAnd here is the result after replacing the unconditioned prompt with the negative prompt \"blue\":\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=20, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/3351458932.py:8: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s much better! The dog does have a different pose (though a similar perspective), but the blue has been removed from the image.\nAs a sanity check, I wanted to see how Huggingface implements negative prompting in the diffusers library. I was thrilled (and relieved) to see that they implement it similarly! In the StableDiffusionPipeline.encode_prompt method they simply assign the negative prompt tokens to the uncond_tokens variable:\nelif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\nthen they embed these tokens:\n\nnegative_prompt_embeds = self.text_encoder(\n                uncond_input.input_ids.to(device),\n                attention_mask=attention_mask,\n            )\nAnd return them along with the desired prompt embeddings:\nreturn prompt_embeds, negative_prompt_embeds\nThey then concatenate the desired and negative prompt embeddings:\nprompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\nAnd in the diffusion loop they get the UNet predictions:\nnoise_pred = self.unet(\n    latent_model_input,\n    t,\n    encoder_hidden_states=prompt_embeds,\n    timestep_cond=timestep_cond,\n    cross_attention_kwargs=self.cross_attention_kwargs,\n    added_cond_kwargs=added_cond_kwargs,\n    return_dict=False,\n)[0]\nand perform classifier-free guidance (where noise_pred_uncond are the UNet predictions for the negative prompt):\nif self.do_classifier_free_guidance:\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)"
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html#implementation-1-pred-u-gt-u-n",
    "href": "posts/2024-11-20-negative-prompting/index.html#implementation-1-pred-u-gt-u-n",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "Implementation 1: pred = u + g*(t-u-n)",
    "text": "Implementation 1: pred = u + g*(t-u-n)\nThe first approach I tried was to modify the classifier-free guidance code from this:\npred = u + g*(t-u)\nto this:\npred = u + g*(t-u-n)\nMy motivation for this was that this subtraction was analogous to “moving away from n”. In hindsight, this was a good first attempt and eventually led to (somewhat accidentally) implementing the correct solution as I was throwing things at the wall to see if they stuck.\nI modified mk_samples with the following changes:\n\nCreate a separate text embeddings (n_embs) for the negative prompt.\nConcatenate n_embs to uncond and text.\nMultiply [latents] by 3 when passing it to the scheduler as we now have three embeddings.\nchunk the UNet predictions into 3 instead of 2.\nReplace pred = u + g*(t-u-n) with pred = u + g*(t-u-n).\n\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, negative_prompt = \"\", g=7.5, seed=100, steps=70):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    n_embs = text_enc([negative_prompt] * bs, text.shape[1])\n    emb = torch.cat([uncond, text, n_embs])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        with torch.no_grad(): u,t,n = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n        pred = u + g*(t-u-n)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nUnfortunately, this resulted in garbled noise!\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=20, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/3564900662.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))"
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html#implementation-2-pred-u-g2t-u-n",
    "href": "posts/2024-11-20-negative-prompting/index.html#implementation-2-pred-u-g2t-u-n",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "Implementation 2: pred = u + g*(2*t-u-n)",
    "text": "Implementation 2: pred = u + g*(2*t-u-n)\nAs I was experimenting with the above mk_samples implementation, I found some success multiplying t by a scalar factor.\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, negative_prompt = \"\", g=7.5, seed=100, steps=70):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    n_embs = text_enc([negative_prompt] * bs, text.shape[1])\n    emb = torch.cat([uncond, text, n_embs])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        with torch.no_grad(): u,t,n = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n        pred = u + g*(2*t-u-n)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nThe resulting image, for both seeds 20 and 18, are not bad! For seed=20 the generated image looks coherent and similar to the structure and composition of the original image though the background still contains blue-ish tones,\nThe result for seed=18 in my opinion is actually better than the pred = n + g*(t-n) implementation! Although the dog’s pose has changed, the dog is no longer wearing a blue sweater (instead it’s black) and more importantly, this approach has not generated a woman standing next to him.\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=20, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/4112522131.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=18, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/4112522131.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\nI don’t understand the intuition behind why, but it seems like 2 is the magical factor for this implementation (in terms of generating somewhat stable images). Here’s a slightly different prompt:\n\nprompts = [\"Cat in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=18, steps=70)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/4112522131.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\nI was curious to see how the result changed given different values of the factor multiplying t. I tried 100 different factors, incrementing from 1 to 3, for seed=18:\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, negative_prompt = \"\", g=7.5, seed=100, steps=70, t_factor=2):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    n_embs = text_enc([negative_prompt] * bs, text.shape[1])\n    emb = torch.cat([uncond, text, n_embs])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        with torch.no_grad(): u,t,n = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n        pred = u + g*(t_factor*t-u-n)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\n\nimages = []\nprompts = [\"Labrador in the style of Vermeer\"]\nfor i in range(100):\n  img = mk_samples(prompts, negative_prompt=\"blue\", seed=18, t_factor=1 + (i+1)/50)\n  images.append(img)\n\n\nfrom PIL import Image, ImageDraw, ImageFont\nimgs = [mk_img(im.squeeze()) for im in images]\n\nfor i, image in enumerate(imgs):\n  font = ImageFont.load_default().font_variant(size=24)\n  draw = ImageDraw.Draw(image)\n  text = f\"t_factor = {round(1 + (i+1)/50, 2)}\"\n  bbox = draw.textbbox((10, 10), text, font=font)  # Get text boundaries\n  draw.rectangle(bbox, fill='black')  # Draw black background\n  ImageDraw.Draw(image).text((10, 10), text, font=font, fill=\"white\")\n    \nimgs[0].save(f't_factor_18.gif', save_all=True, append_images=imgs[1:], duration=100, loop=0)\n\n\n\n\nt_factor GIF\n\n\nWe can see that for a only a very small range of t_factor values (around 2.00) do we get a coherent image.\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nIn fact for only three t_factor values out 100 (2.00, 2.02, 2.04) does the diffusion loop generate coherent images, and only in one of those (2.00) does the composition and style match the desired prompt (“in the style of Vermeer”).\n\nimage_grid([imgs[i] for i in [48, 49, 50, 51, 52, 53]], 2, 3)"
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html#implementation-2-pred-u-gt-u-n_factorn",
    "href": "posts/2024-11-20-negative-prompting/index.html#implementation-2-pred-u-gt-u-n_factorn",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "Implementation 2: pred = u + g*(t-u-n_factor*n)",
    "text": "Implementation 2: pred = u + g*(t-u-n_factor*n)\nWith some success achieved with that implementation, I decided to try a variant: scaling n.\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, negative_prompt = \"\", g=7.5, seed=100, steps=70, n_factor=0.5):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    n_embs = text_enc([negative_prompt] * bs, text.shape[1])\n    emb = torch.cat([uncond, text, n_embs])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        with torch.no_grad(): u,t,n = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n        pred = u + g*(t-u-n_factor*n)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nHowever, with this approach, I found that only negligible values of n_factor produced coherent images, and n’s impact was so small that it did not remove any “blue” elements from the image.\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=18, n_factor=0.005)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/989121822.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n\n\n\n\n\n\n\n\n\n\n\n\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=18, n_factor=0.05)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/989121822.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))"
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html#implementation-3-using-rejection-of-t-on-n",
    "href": "posts/2024-11-20-negative-prompting/index.html#implementation-3-using-rejection-of-t-on-n",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "Implementation 3: Using Rejection of t on n",
    "text": "Implementation 3: Using Rejection of t on n\nMy final implementation was the result of a lengthy conversation with Claude where I explicitly asked it not to provide me with fully fleshed solutions to my prompts but to respond with probing questions based on first principles. This led me to a very engaging and interesting conversation where we explored and developed my intuition of vector geometry.\nThe implementation I came up with was based on the concept of rejection. In the image from Wikipedia below, \\(a_2\\) is the rejection of \\(a\\) onto \\(b\\). In other words, it’s the component of vector \\(a\\) that is perpendicular to vector \\(b\\). We want to move our noise predictions away from n—adding the component of t that is perpendicular to n seemed an intuitive implementation.\n\n\n\nVector rejection\n\n\nIn my sketch below, I visualize in a simplified 2-D space how adding to t a vector p perpendicular to n moves it away from it.\n\n\n\nMoving t away from n with p\n\n\nThe implementation of rejection between two vectors t and n (thanks Claude) is:\np = t - (t * n).sum(dim=1, keepdim=True) * n\nHere’s a trivial example showing that the full vector a is the perpendicular component to the orthogonal b:\n\na = torch.tensor([[0, 0, 0]])\nb = torch.tensor([[1, 1, 1]])\na - (a * b).sum(dim=0, keepdim=True) * b\n\ntensor([[0, 0, 0]])\n\n\nHere’s the modifications I made to mk_samples:\n\nCalculate the rejection of t onto n: p = t - (t * n).sum(dim=1, keepdim=True) * n.\nAdd this vector (scaled) to the noise prediction: pred = u + g*(t-u) + g2*p.\n\n\n\nShow modified mk_samples function\ndef mk_samples(prompts, negative_prompt = \"\", g=7.5, seed=100, steps=70, g2=1):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    n_embs = text_enc([negative_prompt] * bs, text.shape[1])\n    emb = torch.cat([uncond, text, n_embs])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)\n        with torch.no_grad(): u,t,n = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)\n        \n        p = t - (t * n).sum(dim=1, keepdim=True) * n\n        \n        pred = u + g*(t-u) + g2*p\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nOn paper this seemed like an interesting approach but I found that only small values of g2 allowed for coherent image generation, and even then, it did not remove any “blue” components.\n\nprompts = [\"Labrador in the style of Vermeer\"]\nimages = mk_samples(prompts, negative_prompt=\"blue\", seed=18, g2=0.001)\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_52/3545698243.py:9: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))"
  },
  {
    "objectID": "posts/2024-11-20-negative-prompting/index.html#final-thoughts",
    "href": "posts/2024-11-20-negative-prompting/index.html#final-thoughts",
    "title": "Implementing Negative Prompting for Stable Diffusion",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWorking on these small, relatively contained stable diffusion experiments has been really fun and informative. On one hand, it’s broken some of the “mystique” around diffusion, as I’m able to change and somewhat control image generation with a few lines of code. On the other hand, it’s given a deeper insight into how mercurial and sensitive, and sometimes counterintuitive, the diffusion loop can be.\nFor negative prompting, the simplest implementation was the most stable one. Swapping the unconditioned prompt with the negative prompt allows for seemingly guaranteed coherent image generation (although I’m sure there will be prompts that would break it!). That being said, I found that pred = u+g*(2*t-u-n) yielded a better image generation for seed=18 at the cost of a high level of difficulty (or chance in my case) of finding the right factor to multiply t with.\nIn the coming blog posts, I’ll be implementing other diffusion tricks, such as image-to-image generation and callbacks."
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "",
    "text": "In this notebook I re-run my full text search and semantic search retrieval methods for fastbook questions and use my newly curated fastbook-benchmark dataset to calculate Answer Component MRR@10 and Answer Component Recall@10 retrieval metrics. Due to how my benchmark dataset is structured, I had to modify (with Claude’s help) the classic MRR and Recall functions:\n\nAnswer Component MRR@10: Returns the rank of the n-th passage needed to satisfy all answer_components for the question. So, if a question has 4 answer_components and their relevant contexts were contained across the first 5 retrieved passages, MRR would be 1/5 = 0.2.\nAnswer Component Recall@10: Measures the proportion of answer components for which at least one supporting context was retrieved. Using the same example, if the top-10 passages only contain contexts relevant to 2 answer_components, Recall would be 2/4 = 0.5\n\nSee the section below to see how my benchmark dataset is structured.\nThere are four retrieval methods I implement in this notebook:\n\nFull text search (using sqlite and Claude-generated keywords)\nSingle-vector cosine similarity (using BAAI/bge-small-en-v1.5)\nColBERTv2\nanswerai-colbert-small-v1\n\nThere are six chunking strategies I implement:\n\n\n\nChunking Strategy Name\nDescription\n\n\n\n\nA\n1-paragraph (w/headers)\n\n\nB\n3-paragraph (w/headers)\n\n\nC\n1-paragraph (w/o headers)\n\n\nD\n3-paragraph (w/o headers)\n\n\nE\n3-paragraph (w/headers, w/o HTML tags)\n\n\nF\n3-paragraph (w/headers, w/o HTML tags, w/o punctuation)\n\n\n\nHere are the results from this notebook:\nAnswer Component MRR@10\n\n\n\nRetrieval Method\nA\nB\nC\nD\nE\nF\n\n\n\n\nFull text search\n0.30\n0.46\n0.29\n0.44\n0.46\n0.46\n\n\nSingle-vector cosine similiarity\n0.38\n0.50\n0.35\n0.46\n0.50\n0.49\n\n\nColBERTv2\n0.46\n0.49\n0.41\n0.50\n0.49\n0.44\n\n\nanswerai-colbert-small-v1\n0.48\n0.52\n0.45\n0.52\n0.52\n0.45\n\n\n\n\nAnswer Component Recall@10\n\n\n\nRetrieval Method\nA\nB\nC\nD\nE\nF\n\n\n\n\nFull text search\n65%\n83%\n65%\n82%\n83%\n83%\n\n\nSingle-vector cosine similiarity\n71%\n85%\n72%\n82%\n87%\n86%\n\n\nColBERTv2\n80%\n80%\n74%\n80%\n81%\n71%\n\n\nanswerai-colbert-small-v1\n82%\n84%\n77%\n82%\n84%\n73%\n\n\n\nThe best-performing retrieval method and chunking strategies:\n\n\n\nMetric Name\nRetrieval Method\nChunking Strategies\nMetric Value\n\n\n\n\nAnswer Component MRR@10\nanswerai-colbert-small-v1\nB, D, E\n0.52\n\n\nAnswer Component Recall@10\nSingle-vector cosine similiarty\nE\n87%"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#the-fastbook-benchmark-dataset",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#the-fastbook-benchmark-dataset",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "The fastbook-benchmark Dataset",
    "text": "The fastbook-benchmark Dataset\nThe fastbook-benchmark dataset contains a list of items (questions). Each item looks something like this:\n{\n            \"chapter\": 1,\n            \"question_number\": 1,\n            \"question_text\": \"Do you need these for deep learning?\\n\\n- Lots of math...\"\",\n            \"answer_context\": [\n                {\n                    \"answer_component\": \"\\\"Lots of math...\"\",\n                    \"scoring_type\": \"simple\",\n                    \"context\": [\n                        \"...Lots of math...\"\n                    ],\n                    \"explicit_context\": \"true\",\n                    \"extraneous_answer\": \"false\"\n                }\n            ],\n            \"question_context\": []\n        },\nI have broken down each gold_standard_answer into separate answer_components, each of which has associated with it one or more contexts from the chapter text that address that answer_component. Here’s an example of a question with two answer_components:\n{\n            \"chapter\": 1,\n            \"question_number\": 5,\n            \"question_text\": \"What were the two theoretical misunderstandings that held back the field of neural networks?\",\n            \"gold_standard_answer\": \"\\\"In 1969...\"\",\n            \"answer_context\": [\n                {\n                    \"answer_component\": \"\\\"In 1969...\"\",\n                    \"scoring_type\": \"simple\",\n                    \"context\": [\n                        \"An MIT professor named...\"\n                    ],\n                    \"explicit_context\": \"true\",\n                    \"extraneous_answer\": \"false\"\n                },\n                {\n                    \"answer_component\": \"\\\"\\n\\nIn the 1980's...\"\",\n                    \"scoring_type\": \"simple\",\n                    \"context\": [\n                        \"In the 1980's...\"\n                    ],\n                    \"explicit_context\": \"true\",\n                    \"extraneous_answer\": \"false\"\n                }\n            ],\n            \"question_context\": []\n        },\nAny one of the contexts is sufficient to address the associated answer_component. For example, for Chapter 1 Question 12:\n{\n  \"answer_component\": \"We instead use the term parameters.\",\n  \"scoring_type\": \"simple\",\n  \"context\": [\n      \"By the way, what Samuel called \\\"weights\\\" are most generally referred to as model *parameters* these days\",\n\n      \"The *weights* are called *parameters*\"\n  ],\n  \"explicit_context\": \"true\",\n  \"extraneous_answer\": \"false\"\n}\nFor some questions’ gold_standard_answer I found that some answer_components were extraneous to the goal of the question. These have been marked with the flag \"extraneous_answer\": \"false\".\nFor some answer_components I found the corresponding context implicitly addressing it. These have been marked with the flag \"explicit_context\": \"false\".\nFinally, for some answer_components I did not find relevant context in the given chapter so the context field is assigned an empty list [].\nAll of the design decisions for this fastbook-benchmark dataset have largely been driven by one goal: don’t change the gold_standard_answer. I have been using the fastai Forums’ Wiki solutions page for each chapter as the gold standard answer set (example)."
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#setup",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#setup",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Setup",
    "text": "Setup\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ trusted=‘true’}\n!pip install sentence-transformers -Uqq\n!pip install -qq RAGatouille\n!pip install ftfy -qq\n:::\n\n\nShow imports\nimport sqlite3\nimport json\nimport re\nimport os\nimport pandas as pd, numpy as np\nimport requests\nimport torch\nimport torch.nn.functional as F\nfrom ftfy import fix_text\nfrom sentence_transformers import SentenceTransformer\nfrom ragatouille import RAGPretrainedModel\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n\n\n\n\nShow chunking code\ndef get_chunks(notebook_path):\n    with open(notebook_path, 'r', encoding='utf-8') as file:\n        notebook = json.load(file)\n\n    chunks = []\n    current_header = \"\"\n\n    def add_chunk(content):\n        if content.strip():\n            chunks.append(f\"{current_header}\\n\\n{content.strip()}\")\n\n    for cell in notebook['cells']:\n        if cell['cell_type'] == 'markdown':\n            content = ''.join(cell['source'])\n            # see if the cell starts with a markdown header\n            header_match = re.match(r'^(#+\\s+.*?)$', content, re.MULTILINE)\n            if header_match:\n                # grab the header\n                current_header = header_match.group(1)\n                # add any content after the header in the same cell\n                remaining_content = content[len(current_header):].strip()\n                if remaining_content:\n                    # split content into paragraphs\n                    paragraphs = re.split(r'\\n\\s*\\n', remaining_content)\n                    # append the paragraph to the list of chunks\n                    for paragraph in paragraphs:\n                        add_chunk(paragraph)\n            else:\n                # split content into paragraphs\n                paragraphs = re.split(r'\\n\\s*\\n', content)\n                # append the paragraph to the list of chunks\n                for paragraph in paragraphs:\n                    add_chunk(paragraph)\n        elif cell['cell_type'] == 'code':\n          code_content = '```python\\n' + ''.join(cell['source']) + '\\n```'\n\n          # include the output of the code cell\n          output_content = ''\n          if 'outputs' in cell and cell['outputs']:\n              for output in cell['outputs']:\n                  if 'text' in output:\n                      output_content += ''.join(output['text'])\n                  elif 'data' in output and 'text/plain' in output['data']:\n                      output_content += ''.join(output['data']['text/plain'])\n\n          # combine code and output in the same chunk\n          combined_content = code_content + '\\n\\nOutput:\\n' + output_content if output_content else code_content\n          add_chunk(combined_content)\n\n    def filter_chunks(chunks, exclude_headers=[\"Questionnaire\", \"Further Research\"]):\n      filtered_chunks = []\n      for chunk in chunks:\n          lines = chunk.split('\\n')\n          # check if the first line (header) is in the exclude list\n          if not any(header in lines[0] for header in exclude_headers):\n              filtered_chunks.append(chunk)\n      return filtered_chunks\n\n    return filter_chunks(chunks)\n\n\n\n\nShow chunking code\ndef combine_chunks(chunks, num_p=3):\n    combined_chunks = []\n    current_header = None\n    current_group = []\n\n    for chunk in chunks:\n        # Extract header from chunk\n        header = chunk.split('\\n\\n')[0]\n\n        if header != current_header:\n            if len(current_group) > 1:  # Only add if group has content besides header\n                # Add current group to combined chunks if header changes\n                combined_chunks.append('\\n\\n'.join(current_group))\n            # Update current header\n            current_header = header\n            # Start new group with header and content of current chunk\n            current_group = [header, chunk.split('\\n\\n', 1)[1] if len(chunk.split('\\n\\n')) > 1 else '']\n        else:\n            if len(current_group) < num_p + 1:  # +1 to account for header\n                # Add chunk content (without header) to current group\n                current_group.append(chunk.split('\\n\\n', 1)[1] if len(chunk.split('\\n\\n')) > 1 else '')\n\n            if len(current_group) == num_p + 1:  # +1 to account for header\n                # Add full group to combined chunks\n                combined_chunks.append('\\n\\n'.join(current_group))\n                # Reset current group, keeping the header\n                current_group = [current_header]\n\n    if len(current_group) > 1:  # Only add if group has content besides header\n        # Add any remaining group to combined chunks\n        combined_chunks.append('\\n\\n'.join(current_group))\n\n    return combined_chunks\n\n\n\n\nShow the load_data function used for full text search\ndef load_data(chunks, db_path, chapter=1):\n    try:\n        # create virtual table if database doesn't exist\n        if not os.path.exists(db_path):\n            with sqlite3.connect(db_path) as conn:\n              cur = conn.cursor()\n              cur.execute(\"\"\"\n              CREATE VIRTUAL TABLE fastbook_text\n              USING FTS5(chapter, text);\n              \"\"\")\n              conn.commit()\n\n        # load in the chunks for each chapter\n        with sqlite3.connect(db_path) as conn:\n            cur = conn.cursor()\n\n            for chunk in chunks:\n                cur.execute(\"INSERT INTO fastbook_text(chapter, text) VALUES (?, ?)\", (chapter, chunk))\n\n            conn.commit()\n            res = cur.execute(\"SELECT * FROM fastbook_text WHERE chapter = ?\", (chapter,)).fetchall()\n        # make sure all the data was loaded into the database\n        if len(res) != len(chunks):\n            raise ValueError(f\"Number of inserted chunks ({len(res)}) doesn't match input chunks ({len(chunks)})\")\n\n        return True\n\n    except sqlite3.Error as e:\n        print(f\"An error occurred: {e}\")\n        return False\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return False\n\n\n\n\nShow the db_search function used for full text search\ndef db_search(df, limit=1):\n  results = []\n  with sqlite3.connect('fastbook.db') as conn:\n    cur = conn.cursor()\n    # concatenate the keywords into a string \"keyword1 OR keyword 2 OR keyword3 ...\"\n    for _, row in df.iterrows():\n      keywords = ' OR '.join([f'\"{keyword.strip(\",\")}\"' for keyword in row['keywords'].replace('\"', '').split()])\n\n      q = f\"\"\"\n        SELECT text, rank\n        FROM fastbook_text\n        WHERE fastbook_text MATCH ?\n        AND chapter = ?\n        ORDER BY rank\n        LIMIT ?\n        \"\"\"\n      res = cur.execute(q, (keywords, str(row['chapter']), limit)).fetchall()\n      # grab the retrieved chunk from the query results\n      res = [item[0] for item in res]\n\n      # if there are multiple chunks retrieved, combine them into a single string\n      results.append(res)\n\n    return results\n\n\n\n\nDownload chapter ipynb files\nurls = {\n    '01_intro.ipynb': 'https://drive.google.com/uc?export=view&id=1mmBjFH_plndPBC4iRZHChfMazgBxKK4_',\n    '02_production.ipynb': 'https://drive.google.com/uc?export=view&id=1Cf5QHthHy1z13H0iu3qrzAWgquCfqVHk',\n    '04_mnist_basics.ipynb': 'https://drive.google.com/uc?export=view&id=113909_BNulzyLIKUNJHdya0Hhoqie30I',\n    '08_collab.ipynb': 'https://drive.google.com/uc?export=view&id=1BtvStgFjUtvtqbSZNrL7Y2N-ey3seNZU',\n    '09_tabular.ipynb': 'https://drive.google.com/uc?export=view&id=1rHFvwl_l-AJLg_auPjBpNrOgG9HDnfqg',\n    '10_nlp.ipynb': 'https://drive.google.com/uc?export=view&id=1pg1pH7jMMElzrXS0kBBz14aAuDsi2DEP',\n    '13_convolutions.ipynb': 'https://drive.google.com/uc?export=view&id=19P-eEHpAO3WrOvdxgXckyhHhfv_R-hnS'\n}\n\ndef download_file(url, filename):\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Open the file in write-binary mode\n        with open(filename, 'wb') as file:\n            # Write the content of the response to the file\n            file.write(response.content)\n        print(f\"File downloaded successfully: {filename}\")\n    else:\n        print(f\"Failed to download file. Status code: {response.status_code}\")\n\nfor fname, url in urls.items():\n  download_file(url, fname)\n\n\nFile downloaded successfully: 01_intro.ipynb\nFile downloaded successfully: 02_production.ipynb\nFile downloaded successfully: 04_mnist_basics.ipynb\nFile downloaded successfully: 08_collab.ipynb\nFile downloaded successfully: 09_tabular.ipynb\nFile downloaded successfully: 10_nlp.ipynb\nFile downloaded successfully: 13_convolutions.ipynb\n\n\n\n\nShow the dict w/ notebook filenames\nnbs = {\n    '1': '01_intro.ipynb',\n    '2': '02_production.ipynb',\n    '4': '04_mnist_basics.ipynb',\n    '8': '08_collab.ipynb',\n    '9': '09_tabular.ipynb',\n    '10': '10_nlp.ipynb',\n    '13': '13_convolutions.ipynb'\n}\n\n\n\n# load the question texts\nurl = 'https://gist.githubusercontent.com/vishalbakshi/2c22ca69ac7bc4bc845052c1b9d949c8/raw/d498259f2fc75d27c485ddc73933f145987feef3/cs_bm25_baselines.csv'\nquestions = pd.read_csv(url).query(\"is_answerable == 1\")[[\"chapter\", \"question_number\", \"question_text\", \"answer\", \"keywords\"]]\n\n# remove double quotations from the question text\n# as these affect embeddings/cosine similarity: https://vishalbakshi.github.io/blog/posts/2024-11-08-punctuation-cosine-similarity/\nquestions['question_text'] = questions['question_text'].str.strip('\"\\'')\nquestions.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      chapter\n      question_number\n      question_text\n      answer\n      keywords\n    \n  \n  \n    \n      0\n      1\n      1\n      Do you need these for deep learning?\\n\\n- Lots...\n      \"Lots of math - False\\nLots of data - False\\nL...\n      \"deep learning, math, data, computers, PhD\"\n    \n    \n      1\n      1\n      2\n      Name five areas where deep learning is now the...\n      \"Any five of the following:\\nNatural Language ...\n      deep learning, areas, best, world\n    \n    \n      2\n      1\n      3\n      What was the name of the first device that was...\n      \"Mark I perceptron built by Frank Rosenblatt\"\n      \"neuron, neurons, device, artificial, principle\"\n    \n    \n      3\n      1\n      4\n      Based on the book of the same name, what are t...\n      \"A set of processing units\\nA state of activat...\n      \"parallel, distributed, processing, PDP, requi...\n    \n    \n      4\n      1\n      5\n      What were the two theoretical misunderstanding...\n      \"In 1969, Marvin Minsky and Seymour Papert dem...\n      \"neural, networks, theoretical, misunderstandi...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nassert questions.shape == (191,5)\n\n\n# download fastbook-benchmark\ndownload_file(\n    \"https://gist.githubusercontent.com/vishalbakshi/a507b6e9e893475e93a4141e96b8947d/raw/e32835ba1dbf94384943ed5a65404112e1c89df2/fastbook-benchmark.json\",\n    \"fastbook-benchmark.json\"\n    )\n\n# Load the benchmark data\nwith open('fastbook-benchmark.json', 'r') as f:\n    benchmark = json.load(f)\n\nFile downloaded successfully: fastbook-benchmark.json\n\n\n\nassert len(benchmark['questions']) == 191\n\n\n\nShow calculate_mrr function\ndef calculate_mrr(question, retrieved_passages, cutoff=10):\n    retrieved_passages = retrieved_passages[:cutoff]\n    highest_rank = 0\n\n    for ans_comp in question[\"answer_context\"]:\n        contexts = ans_comp.get(\"context\", [])\n        component_found = False\n\n        for rank, passage in enumerate(retrieved_passages, start=1):\n            if any(fix_text(context) in fix_text(passage) for context in contexts):\n                highest_rank = max(highest_rank, rank)\n                component_found = True\n                break\n\n        if not component_found:\n            return 0.0\n\n    return 1.0/highest_rank if highest_rank > 0 else 0.0\n\n\n\n\nShow calculate_recall function\ndef calculate_recall(question, retrieved_passages, cutoff=10):\n    retrieved_passages = retrieved_passages[:cutoff]\n\n    # Track if we've found at least one context for each answer component\n    ans_comp_found = []\n\n    for ans_comp in question[\"answer_context\"]:\n        contexts = ans_comp.get(\"context\", [])\n        found = False\n\n        # Check if any context for this answer component appears in retrieved passages\n        for passage in retrieved_passages:\n            if any(fix_text(context) in fix_text(passage) for context in contexts):\n                found = True\n                break\n\n        ans_comp_found.append(found)\n\n    # Recall is ratio of answer components with at least one found context\n    return sum(ans_comp_found) / len(ans_comp_found)\n\n\n\n\nShow fts_retrieval function\ndef fts_retrieval(data, questions):\n    if os.path.exists(\"fastbook.db\"):\n        os.remove(\"fastbook.db\")\n\n    for chapter, chunks in data.items():\n      print(f\"Chapter {chapter}:\", load_data(chunks, 'fastbook.db', chapter))\n\n    print(\"Retrieving passages...\")\n    results = db_search(questions, limit=10)\n\n    assert len(results) == 191\n    for res in results:\n        assert len(res) <= 10\n\n    print(\"Retrieval complete.\")\n    return results\n\n\n\n\nShow single_vector_retrieval function\ndef single_vector_retrieval(data, benchmark):\n    # Group questions by chapter\n    questions = {}\n    for q in benchmark[\"questions\"]:\n        chapter = str(q[\"chapter\"])\n        if chapter not in questions:\n            questions[chapter] = []\n        questions[chapter].append(q['question_text'].strip('\"\\''))\n\n    q_embs = {}\n    print(\"Encoding Questions...\")\n    for chapter, _ in data.items():\n        qs = questions[chapter]\n        q_embs[chapter] = emb_model.encode(qs, convert_to_tensor=True)\n\n    data_embs = {}\n    print(\"Encoding Data...\")\n    for chapter, chunks in data.items():\n        data_embs[chapter] = emb_model.encode(chunks, convert_to_tensor=True)\n\n    results = []\n    print(\"Retrieving passages...\")\n    for chapter in ['1', '2', '4', '8', '9', '10', '13']:\n        # Compute cosine similarity and get top 10 indices for each row\n        idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_embs[chapter].unsqueeze(0), dim=2).sort(descending=True)[1]\n        top_10_idxs = idxs[:, :10]  # Get the top 10 indices for each row\n\n        # Extract top 10 chunks for each row\n        top_10_chunks = [\n            [data[chapter][idx.item()] for idx in row_idxs]\n            for row_idxs in top_10_idxs\n        ]\n        results.extend(top_10_chunks)\n\n    assert len(results) == 191\n\n    for res in results:\n        assert len(res) <= 10\n\n    print(\"Retrieval complete.\")\n    return results\n\n\n\n\nShow ragetouille_retrieval function\ndef ragatouille_retrieval(data, benchmark, model_nm=\"colbert-ir/colbertv2.0\"):\n    # Group questions by chapter\n    questions_by_chapter = {}\n    for q in benchmark[\"questions\"]:\n        chapter = str(q[\"chapter\"])\n        if chapter not in questions_by_chapter:\n            questions_by_chapter[chapter] = []\n        questions_by_chapter[chapter].append(q)\n\n    # Dictionary to store results per chapter\n    chapter_results = {}\n    chapter_metrics = {}\n\n    # Initialize ColBERTv2\n    RAG = RAGPretrainedModel.from_pretrained(model_nm)\n\n    # Process each chapter separately\n    for chapter in nbs.keys():\n        print(f\"\\nProcessing Chapter {chapter}\")\n\n        # Create chapter-specific index\n        index_path = RAG.index(\n            index_name=f\"chapter_{chapter}_index\",\n            collection=data[chapter],\n            document_ids=[f\"{chapter}_{i}\" for i in range(len(data[chapter]))]\n        )\n\n        # Get questions for this chapter\n        chapter_questions = questions_by_chapter[chapter]\n\n        # Perform retrieval for each question in this chapter\n        results = []\n        for q in chapter_questions:\n            retrieved = RAG.search(q[\"question_text\"].strip('\"\\''), k=10)\n            results.append(retrieved)\n\n        # Store results\n        chapter_results[chapter] = results\n\n    results = []\n    for chapter, res in chapter_results.items():\n        results.extend(res)\n\n    assert len(results) == 191\n\n    final_results = []\n    for res in results:\n        assert len(res) <= 10\n        intermediate_results = [r['content'] for r in res]\n        final_results.append(intermediate_results)\n\n    print(\"Retrieval complete.\")\n    return final_results\n\n\n\n\nShow do_retrieval function\ndef do_retrieval(method, chunking_strategy, data, benchmark, benchmark_results, questions=None):\n  if method == \"bm25\": results = fts_retrieval(data, questions)\n  if method == \"single_vector\": results = single_vector_retrieval(data, benchmark)\n  if method == \"colbertv2\": results = ragatouille_retrieval(data, benchmark, model_nm=\"colbert-ir/colbertv2.0\")\n  if method == \"answerai_colbert\": results = ragatouille_retrieval(data, benchmark, model_nm=\"answerdotai/answerai-colbert-small-v1\")\n\n  name = f\"{method}_{chunking_strategy}\"\n  q_mrr, q_recall = score_retrieval(results, benchmark)\n  benchmark_results = save_results(results, benchmark_results, q_mrr, q_recall, name=name)\n\n  return benchmark_results\n\n\n\n\nShow score_retrieval function\ndef score_retrieval(results, benchmark):\n    q_mrr = []\n    q_recall = []\n\n    for i, question in enumerate(benchmark[\"questions\"]):\n        mrr = calculate_mrr(question, results[i], cutoff=10)\n        recall = calculate_recall(question, results[i], cutoff=10)\n        q_mrr.append(mrr)\n        q_recall.append(recall)\n\n    assert len(q_mrr) == 191\n    assert len(q_recall) == 191\n\n    return q_mrr, q_recall\n\n\n\n\nShow save_results function\ndef save_results(results, df, q_mrr, q_recall, name):\n    flat_results = []\n    for res in results:\n        flat_results.append(\"\\n\\n\".join(res))\n\n    assert len(flat_results) == 191\n\n    df[f'{name}_retrieval'] = flat_results\n    df[f'{name}_mrr10'] = q_mrr\n    df[f'{name}_recall10'] = q_recall\n\n    return df"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-a-1-paragraph-with-headers",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-a-1-paragraph-with-headers",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Chunking Strategy A: 1-Paragraph (with headers)",
    "text": "Chunking Strategy A: 1-Paragraph (with headers)\n\n# chunking each notebook\ndata = {}\n\nfor chapter, nb in nbs.items():\n  data[chapter] = get_chunks(nb)\n\ntotal_chunks = 0\nfor chapter, chunks in data.items():\n  print(chapter, len(chunks))\n  total_chunks += len(chunks)\n\nassert total_chunks == 1967 # 1-paragraph chunks\n\n1 307\n2 227\n4 433\n8 157\n9 387\n10 190\n13 266\n\n\n\nRetrieval Method: Full Text Search\n\nbenchmark_results = do_retrieval(\n    method=\"bm25\",\n    chunking_strategy=\"A\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=questions.copy(),\n    questions=questions)\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['bm25_A_mrr10'].mean(),2), round(benchmark_results['bm25_A_recall10'].mean(),2)\n\n(0.3, 0.65)\n\n\n\n\nRetrieval Method: Single-Vector Cosine Similarity\n\nbenchmark_results = do_retrieval(\n    method=\"single_vector\",\n    chunking_strategy=\"A\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\nEncoding Questions...\nEncoding Data...\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['single_vector_A_mrr10'].mean(),2), round(benchmark_results['single_vector_A_recall10'].mean(),2)\n\n(0.38, 0.71)\n\n\n\n\nRetrieval Method: ColBERTv2\n\nbenchmark_results = do_retrieval(\n    method=\"colbertv2\",\n    chunking_strategy=\"A\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['colbertv2_A_mrr10'].mean(),2), round(benchmark_results['colbertv2_A_recall10'].mean(),2)\n\n(0.46, 0.8)\n\n\n\n\nRetrieval Method: answerai-colbert-small-v1\n\nbenchmark_results = do_retrieval(\n    method=\"answerai_colbert\",\n    chunking_strategy=\"A\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['answerai_colbert_A_mrr10'].mean(),2), round(benchmark_results['answerai_colbert_A_recall10'].mean(),2)\n\n(0.48, 0.82)"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-b-3-paragraph-with-headers",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-b-3-paragraph-with-headers",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Chunking Strategy B: 3-Paragraph (with headers)",
    "text": "Chunking Strategy B: 3-Paragraph (with headers)\nNext, I’ll expand the chunks to include 3-paragraphs at a time. I’m still keeping the headers.\n\nfor chapter, chunks in data.items():\n  data[chapter] = combine_chunks(chunks, num_p=3)\n\ntotal_chunks = 0\n\nfor chapter, chunks in data.items():\n  print(chapter, len(chunks))\n  total_chunks += len(chunks)\n\nassert total_chunks == 713\n\n1 112\n2 84\n4 152\n8 58\n9 141\n10 70\n13 96\n\n\n\nRetrieval Method: Full Text Search\n\nbenchmark_results = do_retrieval(\n    method=\"bm25\",\n    chunking_strategy=\"B\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results,\n    questions=questions)\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['bm25_B_mrr10'].mean(),2), round(benchmark_results['bm25_B_recall10'].mean(),2)\n\n(0.46, 0.83)\n\n\n\n\nRetrieval Method: Single-Vector Cosine Similarity\n\nbenchmark_results = do_retrieval(\n    method=\"single_vector\",\n    chunking_strategy=\"B\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\nEncoding Questions...\nEncoding Data...\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['single_vector_B_mrr10'].mean(),2), round(benchmark_results['single_vector_B_recall10'].mean(),2)\n\n(0.5, 0.85)\n\n\n\n\nRetrieval Method: ColBERTv2\n\nbenchmark_results = do_retrieval(\n    method=\"colbertv2\",\n    chunking_strategy=\"B\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['colbertv2_B_mrr10'].mean(),2), round(benchmark_results['colbertv2_B_recall10'].mean(),2)\n\n(0.49, 0.8)\n\n\n\n\nRetrieval Method: answerai-colbert-small-v1\n\nbenchmark_results = do_retrieval(\n    method=\"answerai_colbert\",\n    chunking_strategy=\"B\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['answerai_colbert_B_mrr10'].mean(),2), round(benchmark_results['answerai_colbert_B_recall10'].mean(),2)\n\n(0.52, 0.84)"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-c-1-paragraph-wo-headers",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-c-1-paragraph-wo-headers",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Chunking Strategy C: 1-Paragraph (w/o headers)",
    "text": "Chunking Strategy C: 1-Paragraph (w/o headers)\nNext, I’ll remove markdown headers from each chunk.\n\n# chunking each notebook\ndata = {}\n\nfor chapter, nb in nbs.items():\n    data[chapter] = get_chunks(nb)\n\nfor chapter, chunks in data.items():\n    data[chapter] = [re.sub(r'^#+\\s+[^\\n]+\\n*', '', c) for c in data[chapter]]\n\ntotal_chunks = 0\nfor chapter, chunks in data.items():\n    print(chapter, len(chunks))\n    total_chunks += len(chunks)\n\nassert total_chunks == 1967 # 1-paragraph chunks\n\n1 307\n2 227\n4 433\n8 157\n9 387\n10 190\n13 266\n\n\n\nRetrieval Method: Full Text Search\n\nbenchmark_results = do_retrieval(\n    method=\"bm25\",\n    chunking_strategy=\"C\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results,\n    questions=questions)\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['bm25_C_mrr10'].mean(),2), round(benchmark_results['bm25_C_recall10'].mean(),2)\n\n(0.29, 0.65)\n\n\n\n\nRetrieval Method: Single-Vector Cosine Similarity\n\nbenchmark_results = do_retrieval(\n    method=\"single_vector\",\n    chunking_strategy=\"C\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\nEncoding Questions...\nEncoding Data...\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['single_vector_C_mrr10'].mean(),2), round(benchmark_results['single_vector_C_recall10'].mean(),2)\n\n(0.35, 0.72)\n\n\n\n\nRetrieval Method: ColBERTv2\n\nbenchmark_results = do_retrieval(\n    method=\"colbertv2\",\n    chunking_strategy=\"C\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['colbertv2_C_mrr10'].mean(),2), round(benchmark_results['colbertv2_C_recall10'].mean(),2)\n\n(0.41, 0.74)\n\n\n\n\nRetrieval Method: answerai-colbert-small-v1\n\nbenchmark_results = do_retrieval(\n    method=\"answerai_colbert\",\n    chunking_strategy=\"C\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['answerai_colbert_C_mrr10'].mean(),2), round(benchmark_results['answerai_colbert_C_recall10'].mean(),2)\n\n(0.45, 0.77)"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-d-3-paragraph-wo-headers",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-d-3-paragraph-wo-headers",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Chunking Strategy D: 3-Paragraph (w/o headers)",
    "text": "Chunking Strategy D: 3-Paragraph (w/o headers)\nExpanding header-less chunks to 3-paragraphs.\n\n\nShow modified combine_chunks function\ndef combine_chunks2(chunks, num_p=3):\n    \"\"\"\n    Combines text chunks into groups of specified size (num_p).\n    If chunks have no headers, treats them as standalone content.\n    \"\"\"\n    combined_chunks = []\n    current_group = []\n\n    for chunk in chunks:\n        if len(current_group) < num_p:\n            current_group.append(chunk)\n\n        if len(current_group) == num_p:\n            combined_chunks.append('\\n\\n'.join(current_group))\n            current_group = []\n\n    # Add any remaining chunks\n    if current_group:\n        combined_chunks.append('\\n\\n'.join(current_group))\n\n    return combined_chunks\n\n\n\nfor chapter, chunks in data.items():\n  data[chapter] = combine_chunks2(chunks, num_p=3)\n\ntotal_chunks = 0\n\nfor chapter, chunks in data.items():\n  print(chapter, len(chunks))\n  total_chunks += len(chunks)\n\nassert total_chunks == 659\n\n1 103\n2 76\n4 145\n8 53\n9 129\n10 64\n13 89\n\n\n\nRetrieval Method: Full Text Search\n\nbenchmark_results = do_retrieval(\n    method=\"bm25\",\n    chunking_strategy=\"D\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results,\n    questions=questions)\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['bm25_D_mrr10'].mean(),2), round(benchmark_results['bm25_D_recall10'].mean(),2)\n\n(0.44, 0.82)\n\n\n\n\nRetrieval Method: Single-Vector Cosine Similarity\n\nbenchmark_results = do_retrieval(\n    method=\"single_vector\",\n    chunking_strategy=\"D\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\nEncoding Questions...\nEncoding Data...\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['single_vector_D_mrr10'].mean(),2), round(benchmark_results['single_vector_D_recall10'].mean(),2)\n\n(0.46, 0.82)\n\n\n\n\nRetrieval Method: ColBERTv2\n\nbenchmark_results = do_retrieval(\n    method=\"colbertv2\",\n    chunking_strategy=\"D\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['colbertv2_D_mrr10'].mean(),2), round(benchmark_results['colbertv2_D_recall10'].mean(),2)\n\n(0.5, 0.8)\n\n\n\n\nRetrieval Method: answerai-colbert-small-v1\n\nbenchmark_results = do_retrieval(\n    method=\"answerai_colbert\",\n    chunking_strategy=\"D\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['answerai_colbert_D_mrr10'].mean(),2), round(benchmark_results['answerai_colbert_D_recall10'].mean(),2)\n\n(0.52, 0.82)"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-e-3-paragraph-wheaders-wo-html-tags",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-e-3-paragraph-wheaders-wo-html-tags",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Chunking Strategy E: (3-paragraph w/headers, w/o HTML tags)",
    "text": "Chunking Strategy E: (3-paragraph w/headers, w/o HTML tags)\nI’ll add headers back, but will remove HTML tags.\n\n# chunking each notebook\ndata = {}\n\nfor chapter, nb in nbs.items():\n  data[chapter] = get_chunks(nb)\n\ntotal_chunks = 0\nfor chapter, chunks in data.items():\n  total_chunks += len(chunks)\n\nassert total_chunks == 1967 # 1-paragraph chunks\n\nfor chapter, chunks in data.items():\n  data[chapter] = combine_chunks(chunks, num_p=3)\n\ntotal_chunks = 0\n\nfor chapter, chunks in data.items():\n  total_chunks += len(chunks)\n\nassert total_chunks == 713\n\n\nchunks[3]\n\n'## The Magic of Convolutions\\n\\nIt turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a *convolution*. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book!\\n\\nA convolution applies a *kernel* across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of <<basic_conv>>.\\n\\n<img src=\"images/chapter9_conv_basic.png\" id=\"basic_conv\" caption=\"Applying a kernel to one location\" alt=\"Applying a kernel to one location\" width=\"700\">'\n\n\n\ndef clean_html(text):\n    # Step 1: Temporarily replace double-bracketed content with a placeholder\n    import uuid\n    placeholder = f\"PLACEHOLDER_{uuid.uuid4()}\"\n    double_bracketed = re.findall(r'<<[^>]*>>', text)\n    step1 = re.sub(r'<<[^>]*>>', placeholder, text)\n\n    # Step 2: Remove HTML tags\n    step2 = re.sub(r'<[/]?[a-zA-Z][^>]*>', '', step1)\n\n    # Step 3: Restore double-bracketed content\n    if double_bracketed:\n        step3 = step2.replace(placeholder, double_bracketed[0])\n        return step3\n    return step2\n\nclean_html('The <a href=\"#\">text</a> is <<untouched>>.')\n\n'The text is <<untouched>>.'\n\n\n\nfor chapter, chunks in data.items():\n  data[chapter] = [clean_html(chunk) for chunk in chunks]\n\ntotal_chunks = 0\n\nfor chapter, chunks in data.items():\n  total_chunks += len(chunks)\n\nassert total_chunks == 713\n\n\nchunks[3]\n\n'## The Magic of Convolutions\\n\\nIt turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a *convolution*. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book!\\n\\nA convolution applies a *kernel* across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of <<basic_conv>>.\\n\\n'\n\n\n\nRetrieval Method: Full Text Search\n\nbenchmark_results = do_retrieval(\n    method=\"bm25\",\n    chunking_strategy=\"E\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results,\n    questions=questions)\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['bm25_E_mrr10'].mean(),2), round(benchmark_results['bm25_E_recall10'].mean(),2)\n\n(0.46, 0.83)\n\n\n\n\nRetrieval Method: Single-Vector Cosine Similarity\n\nbenchmark_results = do_retrieval(\n    method=\"single_vector\",\n    chunking_strategy=\"E\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\nEncoding Questions...\nEncoding Data...\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['single_vector_E_mrr10'].mean(),2), round(benchmark_results['single_vector_E_recall10'].mean(),2)\n\n(0.5, 0.87)\n\n\n\n\nRetrieval Method: ColBERTv2\n\nbenchmark_results = do_retrieval(\n    method=\"colbertv2\",\n    chunking_strategy=\"E\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['colbertv2_E_mrr10'].mean(),2), round(benchmark_results['colbertv2_E_recall10'].mean(),2)\n\n(0.49, 0.81)\n\n\n\n\nRetrieval Method: answerai-colbert-small-v1\n\nbenchmark_results = do_retrieval(\n    method=\"answerai_colbert\",\n    chunking_strategy=\"E\",\n    data=data,\n    benchmark=benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['answerai_colbert_E_mrr10'].mean(),2), round(benchmark_results['answerai_colbert_E_recall10'].mean(),2)\n\n(0.52, 0.84)"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-f-wheaders-wo-html-tags-wo-punctuation",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#chunking-strategy-f-wheaders-wo-html-tags-wo-punctuation",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Chunking Strategy F: (w/headers, w/o HTML tags, w/o punctuation)",
    "text": "Chunking Strategy F: (w/headers, w/o HTML tags, w/o punctuation)\nFinally, I’ll keep headers, remove HTML tags and remove all punctuation.\n\nchunks[3]\n\n'## The Magic of Convolutions\\n\\nIt turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a *convolution*. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book!\\n\\nA convolution applies a *kernel* across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of <<basic_conv>>.\\n\\n'\n\n\n\ndef remove_punctuation(text):\n  import string\n  return ''.join(char if char.isalnum() or char == '#' else ' ' if char in string.punctuation else char for char in text)\n\nremove_punctuation(chunks[3])\n\n'## The Magic of Convolutions\\n\\nIt turns out that finding the edges in an image is a very common task in computer vision  and is surprisingly straightforward  To do it  we use something called a  convolution   A convolution requires nothing more than multiplication  and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book \\n\\nA convolution applies a  kernel  across an image  A kernel is a little matrix  such as the 3×3 matrix in the top right of   basic conv   \\n\\n'\n\n\n\nfor chapter, chunks in data.items():\n  data[chapter] = [remove_punctuation(chunk) for chunk in chunks]\n\ntotal_chunks = 0\n\nfor chapter, chunks in data.items():\n  total_chunks += len(chunks)\n\nassert total_chunks == 713\n\n\nchunks[3]\n\n'## The Magic of Convolutions\\n\\nIt turns out that finding the edges in an image is a very common task in computer vision  and is surprisingly straightforward  To do it  we use something called a  convolution   A convolution requires nothing more than multiplication  and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book \\n\\nA convolution applies a  kernel  across an image  A kernel is a little matrix  such as the 3×3 matrix in the top right of   basic conv   \\n\\n'\n\n\nSince I’m removing punctuation from the contexts, I need to do the same for the benchmark dataset. I think a better solution would be to modify the scoring functions by removing the punctuation there, but I’m saving some time and space by just copying the benchmark dataset and removing punctuation from each context string in it:\n\ndef process_contexts(data):\n    # Process questions\n    for question in data['questions']:\n        # Process only answer_context\n        if 'answer_context' in question:\n            for context_item in question['answer_context']:\n                if 'context' in context_item:\n                    if isinstance(context_item['context'], list):\n                        # If context is a list, process each string in the list\n                        context_item['context'] = [\n                            remove_punctuation(text) if text else text\n                            for text in context_item['context']\n                        ]\n                    elif isinstance(context_item['context'], str):\n                        # If context is a single string, process it directly\n                        context_item['context'] = remove_punctuation(context_item['context'])\n\n    return data\n\nmodified_benchmark = process_contexts(benchmark)\n\n\nmodified_benchmark['questions'][4]['answer_context'][0]['context']\n\n['An MIT professor named Marvin Minsky  who was a grade behind Rosenblatt at the same high school    along with Seymour Papert  wrote a book called  Perceptrons   MIT Press   about Rosenblatt s invention  They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions  such as XOR   In the same book  they also showed that using multiple layers of the devices would allow these limitations to be addressed  Unfortunately  only the first of these insights was widely recognized  As a result  the global academic community nearly entirely gave up on neural networks for the next two decades ']\n\n\n\nRetrieval Method: Full Text Search\n\nbenchmark_results = do_retrieval(\n    method=\"bm25\",\n    chunking_strategy=\"F\",\n    data=data,\n    benchmark=modified_benchmark,\n    benchmark_results=benchmark_results,\n    questions=questions)\n\nChapter 1: True\nChapter 2: True\nChapter 4: True\nChapter 8: True\nChapter 9: True\nChapter 10: True\nChapter 13: True\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['bm25_F_mrr10'].mean(),2), round(benchmark_results['bm25_F_recall10'].mean(),2)\n\n(0.46, 0.83)\n\n\n\n\nRetrieval Method: Single-Vector Cosine Similarity\n\nbenchmark_results = do_retrieval(\n    method=\"single_vector\",\n    chunking_strategy=\"F\",\n    data=data,\n    benchmark=modified_benchmark,\n    benchmark_results=benchmark_results)\n\nEncoding Questions...\nEncoding Data...\nRetrieving passages...\nRetrieval complete.\n\n\n\nround(benchmark_results['single_vector_F_mrr10'].mean(),2), round(benchmark_results['single_vector_F_recall10'].mean(),2)\n\n(0.49, 0.86)\n\n\n\n\nRetrieval Method: ColBERTv2\n\nbenchmark_results = do_retrieval(\n    method=\"colbertv2\",\n    chunking_strategy=\"F\",\n    data=data,\n    benchmark=modified_benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['colbertv2_F_mrr10'].mean(),2), round(benchmark_results['colbertv2_F_recall10'].mean(),2)\n\n(0.44, 0.71)\n\n\n\n\nRetrieval Method: answerai-colbert-small-v1\n\nbenchmark_results = do_retrieval(\n    method=\"answerai_colbert\",\n    chunking_strategy=\"F\",\n    data=data,\n    benchmark=modified_benchmark,\n    benchmark_results=benchmark_results)\n\n\nround(benchmark_results['answerai_colbert_F_mrr10'].mean(),2), round(benchmark_results['answerai_colbert_F_recall10'].mean(),2)\n\n(0.45, 0.73)"
  },
  {
    "objectID": "posts/2024-11-26-fastbook-benchmark-results/index.html#final-thoughts",
    "href": "posts/2024-11-26-fastbook-benchmark-results/index.html#final-thoughts",
    "title": "Evaluating 4 Retrieval Methods with 6 Chunking Strategies on my fastbook-benchmark Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are the definitions of the metrics, retrieval methods and chunking strategies that I am using in this benchmark evaluation:\nMetrics\n\nAnswer Component MRR@10: Returns the rank of the n-th passage needed to satisfy all answer_components for the question. So, if a question has 4 answer_components and their relevant contexts were contained across the first 5 retrieved passages, MRR would be 1/5 = 0.2.\nAnswer Component Recall@10: Measures the proportion of answer components for which at least one supporting context was retrieved. Using the same example, if the top-10 passages only contain contexts relevant to 2 answer_components, Recall would be 2/4 = 0.5\n\nRetrieval Methods\n\nFull text search (using sqlite and Claude-generated keywords)\nSingle-vector cosine similarity (using BAAI/bge-small-en-v1.5)\nColBERTv2\nanswerai-colbert-small-v1\n\nChunking Strategies\n\n\n\nChunking Strategy Name\nDescription\n\n\n\n\nA\n1-paragraph (w/headers)\n\n\nB\n3-paragraph (w/headers)\n\n\nC\n1-paragraph (w/o headers)\n\n\nD\n3-paragraph (w/o headers)\n\n\nE\n3-paragraph (w/headers, w/o HTML tags)\n\n\nF\n3-paragraph (w/headers, w/o HTML tags, w/o punctuation)\n\n\n\nHere are the results from this notebook:\nAnswer Component MRR@10\n\n\n\nRetrieval Method\nA\nB\nC\nD\nE\nF\n\n\n\n\nFull text search\n0.30\n0.46\n0.29\n0.44\n0.46\n0.46\n\n\nSingle-vector cosine similiarity\n0.38\n0.50\n0.35\n0.46\n0.50\n0.49\n\n\nColBERTv2\n0.46\n0.49\n0.41\n0.50\n0.49\n0.44\n\n\nanswerai-colbert-small-v1\n0.48\n0.52\n0.45\n0.52\n0.52\n0.45\n\n\n\nAnswer Component Recall@10\n\n\n\nRetrieval Method\nA\nB\nC\nD\nE\nF\n\n\n\n\nFull text search\n65%\n83%\n65%\n82%\n83%\n83%\n\n\nSingle-vector cosine similiarity\n71%\n85%\n72%\n82%\n87%\n86%\n\n\nColBERTv2\n80%\n80%\n74%\n80%\n81%\n71%\n\n\nanswerai-colbert-small-v1\n82%\n84%\n77%\n82%\n84%\n73%\n\n\n\nThe best-performing retrieval method and chunking strategies:\n\n\n\nMetric Name\nRetrieval Method\nChunking Strategies\nMetric Value\n\n\n\n\nAnswer Component MRR@10\nanswerai-colbert-small-v1\nB, D, E\n0.52\n\n\nAnswer Component Recall@10\nSingle-vector cosine similiarty\nE\n87%\n\n\n\nI was quite surprised that single-vector cosine similarity yielded the best Recall. I was less surprised that answerai-colbert-small-v1 had the best MRR@10 since it was better than the other retrieval methods for 5 out of 6 chunking strategies. Other noteworthy observations:\n\nColBERTv2 and answerai-colbert-small-v1 both experienced a considerable performance drop when punctuation was removed from the documents.\nFull text search was very competitive after the chunk size was increased to 3-paragraphs (B, D, E, F). It yielded the second-highest MRR@10 for Chunking Strategy F (3-paragraph, w/headers, w/o HTMl tags, w/o punctuation).\nRemoving HTML tags (Chunking Strategy E) improved the performance of all four retrieval methods than when they were included (Chunking Strategy D). The biggest beneficiary of removing them was single-vector cosine similarity (82% –> 87%).\n\nA couple of notes about my process:\n\nHaving a benchmark dataset saved me about 15-20 hours of manual evaluation.\nRefactoring the code (into a do_retrieval function) made it easier for me to iterate quickly different chunking strategies.\n\nBefore I move on to experimenting with hybrid approaches (full text search + semantic search) I want to research and apply chunking strategies that are particularly suited to ColBERTv2 and answerai-colbert-small-v1 to see if I can improve on the overall-best Recall@10 of 87% and MRR@10 of 0.52."
  },
  {
    "objectID": "posts/2024-11-27-image-to-image-diffusion/index.html",
    "href": "posts/2024-11-27-image-to-image-diffusion/index.html",
    "title": "Implementing Image-to-Image Generation for Stable Diffusion",
    "section": "",
    "text": "Show setup\n!pip install -qq diffusers transformers==4.46.2\n!pip install -qq pillow==11.0.0\n\nfrom diffusers import LMSDiscreteScheduler, AutoencoderKL, UNet2DConditionModel, StableDiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom IPython.display import display\nimport torch, math\nimport numpy as np\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nbeta_start,beta_end = 0.00085,0.012\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1"
  },
  {
    "objectID": "posts/2024-11-27-image-to-image-diffusion/index.html#background",
    "href": "posts/2024-11-27-image-to-image-diffusion/index.html#background",
    "title": "Implementing Image-to-Image Generation for Stable Diffusion",
    "section": "Background",
    "text": "Background\nIn Lesson 10 of the fastai course (Part 2) Jeremy assigns us the following homework assignment:\n\ntry picking one of the extra tricks we learned about like image-to-image, or negative prompts; see if you can implement negative prompt in your version of this; or try doing image-to-image; try adding callbacks\n\nIn this blog post I’ll implement negative prompting using the diffusion loop code provided in the course’s Stable Diffusion with Diffusers notebook.\nI’ll start by copy/pasting all of the boilerplate code provided in that notebook, and running it to make sure we get the desired images.\n\ndef text_enc(prompts, maxlen=None):\n    if maxlen is None: maxlen = tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n\ndef mk_img(t):\n    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\ndef mk_samples(prompts, g=7.5, seed=100, steps=70):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n    scheduler.set_timesteps(steps)\n    #latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n    #latents = latents.to(\"cuda\").half()\n    latents = latents.to(\"cuda\").half() * 15\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        pred = u + g*(t-u)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n\n\nimages = mk_samples(prompts=['A dancer wearing a colorful dress'])\nfor img in images: display(mk_img(img))\n\n/tmp/ipykernel_33/2843554848.py:17: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((bs, unet.in_channels, height//8, width//8))"
  },
  {
    "objectID": "posts/2024-11-27-image-to-image-diffusion/index.html#implementing-image-to-image-generation",
    "href": "posts/2024-11-27-image-to-image-diffusion/index.html#implementing-image-to-image-generation",
    "title": "Implementing Image-to-Image Generation for Stable Diffusion",
    "section": "Implementing Image-to-Image Generation",
    "text": "Implementing Image-to-Image Generation\nIn the default implementation of the stable diffusion loop, we start out with a set of random latents:\nlatents = torch.randn((bs, unet.in_channels, height//8, width//8))\nlatents = latents.to(\"cuda\").half() * 15\nWhat we want for image-to-image generation is to start with noisy latents for some initial image. This will involve:\n\nLoading an image as a tensor.\nEncoding the image into latents.\nAdding some noise to the latents.\n\n\nLoading an image as a tensor\nI’ll use this Macaw from Lesson 10 as the initial image.\n\ninit_image_path = \"macaw.jpg\"\nim = Image.open(init_image_path)\nim\n\n\n\n\n\n\n\n\nThe original image contains values between 0 and 255:\n\nnp.array(im).min(), np.array(im).max()\n\n(0, 255)\n\n\nThe following lines load the image, resize it to the desired size (512x512) and convert it to a tensor using torchvision.transforms.ToTensor. I also make sure the image is on the GPU and is using half-precision (which is used by the VAE):\n\ntransform = ToTensor()\ninit_image = transform(Image.open(init_image_path).convert('RGB').resize((512, 512))).to(\"cuda\").half()\n\nThe transformed image (to tensor) contains values between 0 and 1:\n\ninit_image.min(), init_image.max()\n\n(tensor(0., device='cuda:0', dtype=torch.float16),\n tensor(1., device='cuda:0', dtype=torch.float16))\n\n\n\n\nEncoding the image into latents\nI reference the following line from mk_img:\nimage = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\nand the following line from mk_samples:\nwith torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\nto encode the initial image into latents with the following line:\n\nlatents = vae.encode(init_image.unsqueeze(0)*2 - 1).latent_dist.sample() * 0.18215\n\n\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nNote that the VAE encoder expects values between -1 and 1 so we have to transform the image tensor accordingly:\n\nshifted_im = init_image.unsqueeze(0)*2 - 1\nshifted_im.min(), shifted_im.max()\n\n(tensor(-1., device='cuda:0', dtype=torch.float16),\n tensor(1., device='cuda:0', dtype=torch.float16))\n\n\n\n\nAdding some noise to the latents\nWe don’t want to start the diffusion loop with the original image’s latents because the UNet is trained to predict noise on noisy latents. In order to give the UNet it’s expected input (noisy latents) we need to add noise to our initial image’s latents!\nWe don’t want to literally add (with +) noise to the latents. Instead, we want to simulating the diffusion process as if it were starting from pure random noise. To do this, we need to prep the scheduler with the total number of steps (so it can calculate noise appropriately), pick some initial step for our noise, and add it to our latents with add_noise:\n\n# set timesteps\nsteps = 70\nscheduler.set_timesteps(steps)\n\n# get start timestep\ninit_strength = 0.15 # can be anything 0-1\ninit_step = int(init_strength * steps)\nts_start = torch.tensor([scheduler.timesteps[init_step]])\n\n# create noise\nbs = 1\nnoise = torch.randn((bs, unet.in_channels, height//8, width//8)).to(\"cuda\")\n\n# add noise\nlatents = scheduler.add_noise(latents, noise, ts_start).half()\n\n/tmp/ipykernel_33/3549084226.py:12: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  noise = torch.randn((bs, unet.in_channels, height//8, width//8)).to(\"cuda\")\n\n\n\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nNote that init_step and ts_start are two different values.\n\ninit_step, ts_start\n\n(10, tensor([854.2174]))\n\n\n\n\nRunning the diffusion loop\nI’ll define some of the related inputs so I can run the diffusion loop with our initial image’s noisy latents. Note that we are not starting the diffusion loop with the first ts but rather starting at the init_step we calculated above:\n\nprompts=['A dancer wearing a colorful dress']\ng = 7.5\nseed = 100\n\nbs = len(prompts)\ntext = text_enc(prompts)\nuncond = text_enc([\"\"] * bs, text.shape[1])\nemb = torch.cat([uncond, text])\nif seed: torch.manual_seed(seed)\n\n\nfor i,ts in enumerate(tqdm(scheduler.timesteps[init_step:])):\n    inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n\n    with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n    pred = u + g*(t-u)\n    latents = scheduler.step(pred, ts, latents).prev_sample\n\nwith torch.no_grad(): \n    final_image = vae.decode(1 / 0.18215 * latents).sample\n\n\n\n\n\ndisplay(mk_img(final_image[0]))\n\n\n\n\n\n\n\n\nAs a reminder, here is the initial image, we can see the similarities in color structure (note the transitions from red –> blue –> green –> yellow).\n\nImage.open(init_image_path)"
  },
  {
    "objectID": "posts/2024-11-27-image-to-image-diffusion/index.html#varying-init_strength",
    "href": "posts/2024-11-27-image-to-image-diffusion/index.html#varying-init_strength",
    "title": "Implementing Image-to-Image Generation for Stable Diffusion",
    "section": "Varying init_strength",
    "text": "Varying init_strength\nWith the core functionality of image-to-image generation working properly, I’ll wrap it all into a function so I can loop through different init_strength values to see how it affects the generated image.\n\ndef mk_samples(prompts, init_image_path, g=7.5, seed=100, steps=70, init_strength=0.15):\n    bs = len(prompts)\n    text = text_enc(prompts)\n    uncond = text_enc([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed: torch.manual_seed(seed)\n\n    # load image as tensor\n    transform = ToTensor()\n    init_image = transform(Image.open(init_image_path).convert('RGB').resize((512, 512))).to(\"cuda\").half()\n\n    # encode image into latents\n    latents = vae.encode(init_image.unsqueeze(0)*2 - 1).latent_dist.sample() * 0.18215\n\n    # set timesteps\n    scheduler.set_timesteps(steps)\n\n    # get start timestep\n    init_step = int(init_strength * steps)\n    ts_start = torch.tensor([scheduler.timesteps[init_step]])\n\n    # create noise\n    noise = torch.randn((bs, unet.in_channels, height//8, width//8)).to(\"cuda\")\n\n    # add noise\n    latents = scheduler.add_noise(latents, noise, ts_start).half()\n\n    for i,ts in enumerate(tqdm(scheduler.timesteps[init_step:])):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n\n        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n        pred = u + g*(t-u)\n        latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample\n    \n\n\nimgs = []\nfor i in range(100):\n  images = mk_samples(prompts=[\"A dancer wearing a colorful dress\"], init_image_path=\"macaw.jpg\", init_strength=i*0.01)\n  imgs.append(images[0])\n\n\nimgs = [mk_img(img.squeeze()) for img in imgs]\n\n\nimgs[0].save(f'init_strength.gif', save_all=True, append_images=imgs[1:], duration=100, loop=0)\n\n\n\n\ninit_strength goes from 0 to 1.0\n\n\nAs init_strength goes from 0.0 (totally random initial noise) to 0.99 (very lightly noised initial image) we can see how the prompt conforms to the color and structure of the initial image."
  },
  {
    "objectID": "posts/2024-11-27-image-to-image-diffusion/index.html#final-thoughts",
    "href": "posts/2024-11-27-image-to-image-diffusion/index.html#final-thoughts",
    "title": "Implementing Image-to-Image Generation for Stable Diffusion",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWorking through this implementation solidified my understanding of the diffusion loop. A few small but key points that I paid more attention to this time around:\n\nThe VAE encoder expects latent values between -1 and 1 so we have to transform our image tensor accordingly.\nAdding noise to our initial image’s latents requires:\n\nPicking a total number of inference steps.\nPicking an initial step (at which we will apply the noise) and the corresponding scheduler timestep.\nUsing scheduler.add_noise.\n\nThe text encoding process remains untouched.\nThe only change to the diffusion loop is starting at scheduler.timesteps[init_step] instead of the first timestep.\n\nThe last implementation for this HW assignment will be to implement callbacks, which I’ll do in a future blog post! Thanks for reading!"
  },
  {
    "objectID": "posts/2024-11-29-fastbook-benchmark-results/index.html",
    "href": "posts/2024-11-29-fastbook-benchmark-results/index.html",
    "title": "Scoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens",
    "section": "",
    "text": "pip installs and imports\n!pip install sentence-transformers -Uqq\n!pip install -qq RAGatouille\n!pip install ftfy -qq\n\nimport sqlite3\nimport json\nimport re\nimport os\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nimport torch\nimport torch.nn.functional as F\nfrom ftfy import fix_text\nfrom sentence_transformers import SentenceTransformer\nfrom ragatouille import RAGPretrainedModel\nfrom ragatouille.data import CorpusProcessor\n\ncorpus_processor = CorpusProcessor()\nemb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n\n\n\n\nDownload chapter ipynb files\nurls = {\n    '01_intro.ipynb': 'https://drive.google.com/uc?export=view&id=1mmBjFH_plndPBC4iRZHChfMazgBxKK4_',\n    '02_production.ipynb': 'https://drive.google.com/uc?export=view&id=1Cf5QHthHy1z13H0iu3qrzAWgquCfqVHk',\n    '04_mnist_basics.ipynb': 'https://drive.google.com/uc?export=view&id=113909_BNulzyLIKUNJHdya0Hhoqie30I',\n    '08_collab.ipynb': 'https://drive.google.com/uc?export=view&id=1BtvStgFjUtvtqbSZNrL7Y2N-ey3seNZU',\n    '09_tabular.ipynb': 'https://drive.google.com/uc?export=view&id=1rHFvwl_l-AJLg_auPjBpNrOgG9HDnfqg',\n    '10_nlp.ipynb': 'https://drive.google.com/uc?export=view&id=1pg1pH7jMMElzrXS0kBBz14aAuDsi2DEP',\n    '13_convolutions.ipynb': 'https://drive.google.com/uc?export=view&id=19P-eEHpAO3WrOvdxgXckyhHhfv_R-hnS'\n}\n\ndef download_file(url, filename):\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Open the file in write-binary mode\n        with open(filename, 'wb') as file:\n            # Write the content of the response to the file\n            file.write(response.content)\n        print(f\"File downloaded successfully: {filename}\")\n    else:\n        print(f\"Failed to download file. Status code: {response.status_code}\")\n\nfor fname, url in urls.items():\n  download_file(url, fname)\n\n\nFile downloaded successfully: 01_intro.ipynb\nFile downloaded successfully: 02_production.ipynb\nFile downloaded successfully: 04_mnist_basics.ipynb\nFile downloaded successfully: 08_collab.ipynb\nFile downloaded successfully: 09_tabular.ipynb\nFile downloaded successfully: 10_nlp.ipynb\nFile downloaded successfully: 13_convolutions.ipynb\n\n\n\n\nnbs dict\nnbs = {\n    '1': '01_intro.ipynb',\n    '2': '02_production.ipynb',\n    '4': '04_mnist_basics.ipynb',\n    '8': '08_collab.ipynb',\n    '9': '09_tabular.ipynb',\n    '10': '10_nlp.ipynb',\n    '13': '13_convolutions.ipynb'\n}\n\n\n\n\nload questions\nurl = 'https://gist.githubusercontent.com/vishalbakshi/2c22ca69ac7bc4bc845052c1b9d949c8/raw/d498259f2fc75d27c485ddc73933f145987feef3/cs_bm25_baselines.csv'\nquestions = pd.read_csv(url).query(\"is_answerable == 1\")[[\"chapter\", \"question_number\", \"question_text\", \"answer\", \"keywords\"]]\n\n# remove double quotations from the question text\n# as these affect embeddings/cosine similarity: https://vishalbakshi.github.io/blog/posts/2024-11-08-punctuation-cosine-similarity/\nquestions['question_text'] = questions['question_text'].str.strip('\"\\'')\nassert questions.shape == (191,5)\n\n\n\n\ndownload fastbook-benchmark\ndownload_file(\n    \"https://gist.githubusercontent.com/vishalbakshi/a507b6e9e893475e93a4141e96b8947d/raw/e32835ba1dbf94384943ed5a65404112e1c89df2/fastbook-benchmark.json\",\n    \"fastbook-benchmark.json\"\n    )\n\ndef load_benchmark():\n    # Load the benchmark data\n    with open('fastbook-benchmark.json', 'r') as f:\n        benchmark = json.load(f)\n    return benchmark\n\nbenchmark = load_benchmark()\nassert len(benchmark['questions']) == 191\n\n\nFile downloaded successfully: fastbook-benchmark.json\n\n\n\n\ncalculate_mrr function\ndef calculate_mrr(question, retrieved_passages, cutoff=10):\n    retrieved_passages = retrieved_passages[:cutoff]\n    highest_rank = 0\n\n    for ans_comp in question[\"answer_context\"]:\n        contexts = ans_comp.get(\"context\", [])\n        component_found = False\n\n        for rank, passage in enumerate(retrieved_passages, start=1):\n            if any(fix_text(context) in fix_text(passage) for context in contexts):\n                highest_rank = max(highest_rank, rank)\n                component_found = True\n                break\n\n        if not component_found:\n            return 0.0\n\n    return 1.0/highest_rank if highest_rank > 0 else 0.0\n\n\n\n\ncalculate_recall function\ndef calculate_recall(question, retrieved_passages, cutoff=10):\n    retrieved_passages = retrieved_passages[:cutoff]\n\n    # Track if we've found at least one context for each answer component\n    ans_comp_found = []\n\n    for ans_comp in question[\"answer_context\"]:\n        contexts = ans_comp.get(\"context\", [])\n        found = False\n\n        # Check if any context for this answer component appears in retrieved passages\n        for passage in retrieved_passages:\n            if any(fix_text(context) in fix_text(passage) for context in contexts):\n                found = True\n                break\n\n        ans_comp_found.append(found)\n\n    # Recall is ratio of answer components with at least one found context\n    return sum(ans_comp_found) / len(ans_comp_found)\n\n\n\n\nload_data function\ndef load_data(chunks, db_path, chapter=1):\n    try:\n        # create virtual table if database doesn't exist\n        if not os.path.exists(db_path):\n            with sqlite3.connect(db_path) as conn:\n              cur = conn.cursor()\n              cur.execute(\"\"\"\n              CREATE VIRTUAL TABLE fastbook_text\n              USING FTS5(chapter, text);\n              \"\"\")\n              conn.commit()\n\n        # load in the chunks for each chapter\n        with sqlite3.connect(db_path) as conn:\n            cur = conn.cursor()\n\n            for chunk in chunks:\n                cur.execute(\"INSERT INTO fastbook_text(chapter, text) VALUES (?, ?)\", (chapter, chunk))\n\n            conn.commit()\n            res = cur.execute(\"SELECT * FROM fastbook_text WHERE chapter = ?\", (chapter,)).fetchall()\n        # make sure all the data was loaded into the database\n        if len(res) != len(chunks):\n            raise ValueError(f\"Number of inserted chunks ({len(res)}) doesn't match input chunks ({len(chunks)})\")\n\n        return True\n\n    except sqlite3.Error as e:\n        print(f\"An error occurred: {e}\")\n        return False\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return False\n\n\n\n\ndb_search function\ndef db_search(df, limit=1):\n  results = []\n  with sqlite3.connect('fastbook.db') as conn:\n    cur = conn.cursor()\n    # concatenate the keywords into a string \"keyword1 OR keyword 2 OR keyword3 ...\"\n    for _, row in df.iterrows():\n      keywords = ' OR '.join([f'\"{keyword.strip(\",\")}\"' for keyword in row['keywords'].replace('\"', '').split()])\n\n      q = f\"\"\"\n        SELECT text, rank\n        FROM fastbook_text\n        WHERE fastbook_text MATCH ?\n        AND chapter = ?\n        ORDER BY rank\n        LIMIT ?\n        \"\"\"\n      res = cur.execute(q, (keywords, str(row['chapter']), limit)).fetchall()\n      # grab the retrieved chunk from the query results\n      res = [item[0] for item in res]\n\n      # if there are multiple chunks retrieved, combine them into a single string\n      results.append(res)\n\n    return results\n\n\n\n\nfts_retrieval function\ndef fts_retrieval(data, df, chunk_size):\n    if os.path.exists(\"fastbook.db\"):\n        os.remove(\"fastbook.db\")\n\n    for chapter, chunks in data.items():\n        documents = corpus_processor.process_corpus(chunks, chunk_size=chunk_size)\n        documents = [doc['content'] for doc in documents]\n        assert load_data(documents, 'fastbook.db', chapter)\n\n    results = db_search(df, limit=10)\n    assert len(results) == 191\n\n    for res in results:\n        assert len(res) <= 10\n\n    return results\n\n\n\n\nsingle_vector_retrieval function\ndef single_vector_retrieval(data, benchmark, chunk_size):\n    # Group questions by chapter\n    questions = {}\n    for q in benchmark[\"questions\"]:\n        chapter = str(q[\"chapter\"])\n        if chapter not in questions:\n            questions[chapter] = []\n        questions[chapter].append(q['question_text'].strip('\"\\''))\n\n    q_embs = {}\n    for chapter, _ in data.items():\n        qs = questions[chapter]\n        q_embs[chapter] = emb_model.encode(qs, convert_to_tensor=True)\n\n    results = []\n    for chapter, chunks in data.items():\n        # chunk chapter text\n        documents = corpus_processor.process_corpus(chunks, chunk_size=chunk_size)\n        documents = [doc['content'] for doc in documents]\n\n        # Embed documents\n        data_embs = emb_model.encode(documents, convert_to_tensor=True)\n\n        # Compute cosine similarity and get top 10 indices for each row\n        idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_embs.unsqueeze(0), dim=2).sort(descending=True)[1]\n        top_10_idxs = idxs[:, :10]  # Get the top 10 indices for each row\n\n        # Extract top 10 chunks for each row\n        top_10_chunks = [\n            [documents[idx.item()] for idx in row_idxs]\n            for row_idxs in top_10_idxs\n        ]\n        results.extend(top_10_chunks)\n\n    assert len(results) == 191\n\n    for res in results:\n        assert len(res) <= 10\n\n    return results\n\n\n\n\nindex_free_retrieval function\ndef index_free_retrieval(data, model_nm, chunk_size, benchmark):\n    questions_by_chapter = {}\n    for q in benchmark[\"questions\"]:\n        chapter = str(q[\"chapter\"])\n        if chapter not in questions_by_chapter:\n            questions_by_chapter[chapter] = []\n        questions_by_chapter[chapter].append(q)\n\n    # Dictionary to store results per chapter\n    chapter_results = {}\n\n    # Process each chapter separately\n    for chapter in nbs.keys():\n        # instantiate new RAG object\n        RAG = RAGPretrainedModel.from_pretrained(model_nm)\n\n        # Get questions for this chapter\n        chapter_questions = questions_by_chapter[chapter]\n\n        # encode chapter documents\n        documents = corpus_processor.process_corpus(data[chapter], chunk_size=chunk_size)\n        RAG.encode([x['content'] for x in documents], document_metadatas=[{\"chapter\": chapter} for _ in range(len(documents))])\n\n        # Perform retrieval for each question in this chapter\n        results = []\n        for q in chapter_questions:\n            top_k = min(10, len(documents))\n            retrieved = RAG.search_encoded_docs(query = q[\"question_text\"].strip('\"\\''), k=top_k)\n            results.append(retrieved)\n\n        # Store results\n        chapter_results[chapter] = results\n\n    results = []\n    for chapter, res in chapter_results.items():\n        results.extend(res)\n\n    assert len(results) == 191\n\n    final_results = []\n    for res in results:\n        assert len(res) <= 10\n        intermediate_results = [r['content'] for r in res]\n        final_results.append(intermediate_results)\n\n    assert len(final_results) == 191\n    return final_results\n\n\n\n\ndo_retrieval function\ndef do_retrieval(method, chunk_size, data, benchmark, questions=None, benchmark_results=None):\n  if method == \"bm25\": results = fts_retrieval(data, questions, chunk_size)\n  if method == \"single_vector\": results = single_vector_retrieval(data, benchmark, chunk_size)\n  if method == \"colbertv2\": results = index_free_retrieval(data=data, model_nm=\"colbert-ir/colbertv2.0\", chunk_size=chunk_size, benchmark=benchmark)\n  if method == \"answerai_colbert\": results = index_free_retrieval(data=data, model_nm=\"answerdotai/answerai-colbert-small-v1\", chunk_size=chunk_size, benchmark=benchmark)\n\n  #name = f\"{method}_{chunking_strategy}\"\n  q_mrr, q_recall = score_retrieval(results, benchmark)\n  #benchmark_results = save_results(results, benchmark_results, q_mrr, q_recall, name=name)\n\n  return pd.Series(q_mrr).mean(), pd.Series(q_recall).mean()\n\n\n\n\nscore_retrieval function\ndef score_retrieval(results, benchmark):\n    q_mrr = []\n    q_recall = []\n\n    for i, question in enumerate(benchmark[\"questions\"]):\n        mrr = calculate_mrr(question, results[i], cutoff=10)\n        recall = calculate_recall(question, results[i], cutoff=10)\n        q_mrr.append(mrr)\n        q_recall.append(recall)\n\n    assert len(q_mrr) == len(benchmark[\"questions\"])\n    assert len(q_recall) == len(benchmark[\"questions\"])\n\n    return q_mrr, q_recall\n\n\n\n\nsave_results function\ndef save_results(results, df, q_mrr, q_recall, name):\n    flat_results = []\n    for res in results:\n        flat_results.append(\"\\n\\n\".join(res))\n\n    assert len(flat_results) == 191\n\n    df[f'{name}_retrieval'] = flat_results\n    df[f'{name}_mrr10'] = q_mrr\n    df[f'{name}_recall10'] = q_recall\n\n    return df\n\n\n\n\nnotebook_to_string function\ndef notebook_to_string(path):\n  with open(path, 'r', encoding='utf-8') as f:\n    notebook = json.load(f)\n\n  all_text = ''\n  found_questionnaire = False\n\n  for cell in notebook['cells']:\n    if cell['cell_type'] == 'markdown' and any('## Questionnaire' in line for line in cell['source']):\n      found_questionnaire = True\n      break\n\n    if cell['cell_type'] in ['markdown', 'code']:\n      all_text += ''.join(cell['source']) + '\\n'\n  return all_text\n\n\n\n\nchunk_string function\ndef chunk_string(text, n):\n    \"\"\"Split text into n chunks.\"\"\"\n    skip = int(len(text) / n)\n    return [text[i:i + skip] for i in range(0, len(text), skip)]\n\n\n\n\nclean_html function\ndef clean_html(text):\n    # Step 1: Temporarily replace double-bracketed content with a placeholder\n    import uuid\n    placeholder = f\"PLACEHOLDER_{uuid.uuid4()}\"\n    double_bracketed = re.findall(r'<<[^>]*>>', text)\n    step1 = re.sub(r'<<[^>]*>>', placeholder, text)\n\n    # Step 2: Remove HTML tags\n    step2 = re.sub(r'<[/]?[a-zA-Z][^>]*>', '', step1)\n\n    # Step 3: Restore double-bracketed content\n    if double_bracketed:\n        step3 = step2.replace(placeholder, double_bracketed[0])\n        return step3\n    return step2\n\n\n\n\nremove_punctuation function\ndef remove_punctuation(text):\n    import string\n    return ''.join(char if char.isalnum() else ' ' if char in string.punctuation else char for char in text)\n\n\n\n\nprocess_contexts function\ndef process_contexts(data):\n    # Process questions\n    for question in data['questions']:\n        # Process only answer_context\n        if 'answer_context' in question:\n            for context_item in question['answer_context']:\n                if 'context' in context_item:\n                    if isinstance(context_item['context'], list):\n                        # If context is a list, process each string in the list\n                        context_item['context'] = [\n                            remove_punctuation(text) if text else text\n                            for text in context_item['context']\n                        ]\n                    elif isinstance(context_item['context'], str):\n                        # If context is a single string, process it directly\n                        context_item['context'] = remove_punctuation(context_item['context'])\n\n    return data"
  },
  {
    "objectID": "posts/2024-11-29-fastbook-benchmark-results/index.html#background",
    "href": "posts/2024-11-29-fastbook-benchmark-results/index.html#background",
    "title": "Scoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens",
    "section": "Background",
    "text": "Background\nIn this notebook, I evaluate four retrieval methods on my fastbook-benchmark dataset using various chunk sizes:\n\nFull text search (using sqlite and Claude-generated keywords).\nSingle-vector cosine similarity (using SentenceTransformer(\"BAAI/bge-small-en-v1.5\")).\nColBERTv2 (using ragatouille).\nanswerai-colbert-small-v1 (ragatouille).\n\nFor each retrieval method, I’m looking to find the best MRR@10 and Recall@10 for three chunk size ranges:\n\nSmall: 100-500 tokens\nMedium: 500-1000 tokens\nLarge: 1000+ tokens\n\nI chose these ranges based on trends I saw during some experiments I ran with ColBERTv2 and answerai-colbert-small-v1 for a chunk size range of 100-3000 tokens.\nI’m using corpus_processor.process_corpus for chunking."
  },
  {
    "objectID": "posts/2024-11-29-fastbook-benchmark-results/index.html#data-preprocessing",
    "href": "posts/2024-11-29-fastbook-benchmark-results/index.html#data-preprocessing",
    "title": "Scoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nFor each retrieval method/chunk size combination, I’ll preprocess my data three ways:\n\nNo preprocessing.\nRemove HTML tags.\nRemove punctuation.\n\nNote that in each case, I am first converting the .ipynb for each chapter into a single string and splitting them into 2-3 chunks since for full-length texts were causing OOM during ColBERTv2 encoding.\n\n\nPrep no preprocessing dataset\ndata_no_pp = {}\nn_chars = 0\n\nfor chapter, nb in nbs.items():\n    data_no_pp[chapter] = chunk_string(notebook_to_string(nb), 2)\n    for c in data_no_pp[chapter]:\n        n_chars += len(c)\n\nassert n_chars == 503769\n\n\n\n\nPrep no HTML dataset\ndata_no_html = {}\n\nn_chars = 0\nfor chapter, nb in nbs.items():\n    data_no_html[chapter] = chunk_string(notebook_to_string(nb), 2)\n    for c in data_no_html[chapter]:\n        n_chars += len(c)\n\nassert n_chars == 503769\n\nn_chars = 0\nfor chapter, chunks in data_no_html.items():\n    data_no_html[chapter] = [clean_html(chunk) for chunk in chunks]\n    for string in data_no_html[chapter]: n_chars += len(string)\n\nassert n_chars == 493604\n\n\n\n\nPrep no punctuation dataset\ndata_no_punc = {}\n\nn_chars = 0\nfor chapter, nb in nbs.items():\n    data_no_punc[chapter] = chunk_string(notebook_to_string(nb), 2)\n    for c in data_no_punc[chapter]:\n        n_chars += len(c)\n\nassert n_chars == 503769\n\nn_chars = 0\nfor chapter, chunks in data_no_punc.items():\n    data_no_punc[chapter] = [remove_punctuation(chunk) for chunk in chunks]\n    for string in data_no_punc[chapter]: n_chars += len(string)\n\n# we are replacing punctuation with single space so n_chars doesn't change\nassert n_chars == 503769"
  },
  {
    "objectID": "posts/2024-11-29-fastbook-benchmark-results/index.html#benchmark-dataset",
    "href": "posts/2024-11-29-fastbook-benchmark-results/index.html#benchmark-dataset",
    "title": "Scoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens",
    "section": "Benchmark Dataset",
    "text": "Benchmark Dataset\n\nbenchmark = load_benchmark()\nassert len(benchmark['questions']) == 191\n\nprocessed_benchmark = process_contexts(benchmark)\nassert len(processed_benchmark['questions']) == 191"
  },
  {
    "objectID": "posts/2024-11-29-fastbook-benchmark-results/index.html#running-retrieval-100-2000-token-chunks",
    "href": "posts/2024-11-29-fastbook-benchmark-results/index.html#running-retrieval-100-2000-token-chunks",
    "title": "Scoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens",
    "section": "Running Retrieval (100-2000 token chunks)",
    "text": "Running Retrieval (100-2000 token chunks)\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nData: No Preprocessing\n\nresults = []\n\nfor method in [\"bm25\", \"single_vector\", \"colbertv2\", \"answerai_colbert\"]:\n  for chunk_size in range(100, 2100, 100):\n    mrr, recall = do_retrieval(method, chunk_size, data_no_pp, load_benchmark(), questions)\n    results.append(('no preprocessing', method, chunk_size, mrr, recall))\n\ndf = pd.DataFrame(results, columns=['data', 'method', 'chunk_size', 'MRR@10', 'Recall@10'])\ndf.to_csv(\"/content/drive/MyDrive/2024-11-29-fastbook-benchmark-results_no preprocessing.csv\", index=False)\n\n\n\nData: No HTML Tags\n\nresults = []\n\nfor method in [\"bm25\", \"single_vector\", \"colbertv2\", \"answerai_colbert\"]:\n  for chunk_size in range(100, 2100, 100):\n    mrr, recall = do_retrieval(method, chunk_size, data_no_html, load_benchmark(), questions)\n    results.append(('no HTML', method, chunk_size, mrr, recall))\n\ndf = pd.DataFrame(results, columns=['data', 'method', 'chunk_size', 'MRR@10', 'Recall@10'])\ndf.to_csv(\"/content/drive/MyDrive/2024-11-29-fastbook-benchmark-results_no HTML.csv\", index=False)\n\n\n\nData: No Punctuation\n\nresults = []\n\nfor method in [\"bm25\", \"single_vector\", \"colbertv2\", \"answerai_colbert\"]:\n  for chunk_size in range(100, 2100, 100):\n    mrr, recall = do_retrieval(method, chunk_size, data_no_punc, process_contexts(benchmark), questions)\n    results.append(('no punctuation', method, chunk_size, mrr, recall))\n\ndf = pd.DataFrame(results, columns=['data', 'method', 'chunk_size', 'MRR@10', 'Recall@10'])\ndf.to_csv(\"/content/drive/MyDrive/2024-11-29-fastbook-benchmark-results_no punctuation.csv\", index=False)"
  },
  {
    "objectID": "posts/2024-11-29-fastbook-benchmark-results/index.html#analyzing-results",
    "href": "posts/2024-11-29-fastbook-benchmark-results/index.html#analyzing-results",
    "title": "Scoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens",
    "section": "Analyzing Results",
    "text": "Analyzing Results\n\n\nLoad saved results CSVs\nno_pp = pd.read_csv(\"/content/drive/MyDrive/2024-11-29-fastbook-benchmark-results_no preprocessing.csv\")\nno_html = pd.read_csv(\"/content/drive/MyDrive/2024-11-29-fastbook-benchmark-results_no HTML.csv\")\nno_punc = pd.read_csv(\"/content/drive/MyDrive/2024-11-29-fastbook-benchmark-results_no punctuation.csv\")\ndf = pd.concat([no_pp, no_html, no_punc])\ndf.shape\n\n\n(240, 5)\n\n\n\nBest Overall MRR@10\nSurprisingly, full text search had the highest-overall MRR@10 with 0.67 for 2000-token chunks, for text with punctuation removed.\n\ndf[df['MRR@10'] == df['MRR@10'].max()]\n\n\n\n  \n    \n\n\n  \n    \n      \n      data\n      method\n      chunk_size\n      MRR@10\n      Recall@10\n    \n  \n  \n    \n      19\n      no punctuation\n      bm25\n      2000\n      0.668046\n      0.94315\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\nBest Overall Recall@10\nFull text search also yielded the best overall Recall@10 at 95%, also at a chunk size of 2000 tokens, but for text with no preprocessing on it (other than splitting the notebook into 2-3 chunks). Wow!\n\ndf[df['Recall@10'] == df['Recall@10'].max()]\n\n\n\n  \n    \n\n\n  \n    \n      \n      data\n      method\n      chunk_size\n      MRR@10\n      Recall@10\n    \n  \n  \n    \n      19\n      no preprocessing\n      bm25\n      2000\n      0.658541\n      0.95493\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\nAnalyzing Metrics by Chunk Size Range\nI care about chunk size because eventually I am going to pass on the retrieved context to an LLM for answer generation and context size will dictate latency and potentially performance.\n\ndf['category'] = pd.cut(df['chunk_size'], bins=[100, 500, 1000, 2000], labels=['small', 'medium', 'large'], include_lowest=True)\n\n\nMRR@10\n\n\n\n\n\n\n\n\n\n\nChunk Size Category\nRetrieval Method\nMRR@10\nChunk Size\nDataset\n\n\n\n\nSmall (100-500 tokens)\nanswer-colbert-small-v1\n0.59\n500 tokens\nNo punctuation\n\n\nMedium (501-1000 tokens)\nColBERTv2\n0.61\n600 tokens\nNo punctuation\n\n\nLarge (1001-2000 tokens)\nFull text search\n0.67\n2000 tokens\nNo punctuation\n\n\n\nInteresting to note:\n\n600 tokens was the best-performing chunk size for medium chunks.\nIn all three cases, text without punctuation resulted in the best MRR@10.\n\n\n\nRecall@10\n\n\n\n\n\n\n\n\n\n\nChunk Size Category\nRetrieval Method\nRecall@10\nChunk Size\nData\n\n\n\n\nSmall (100-500 tokens)\nanswer-colbert-small-v1\n0.88\n500 tokens\nNo punctuation\n\n\nMedium (501-1000 tokens)\nFull text search\n0.92\n1000 tokens\nNo preprocessing\n\n\nLarge (1001-2000 tokens)\nFull text search\n0.95\n2000 tokens\nNo preprocessing\n\n\n\nInteresting to note:\n\nThe highest chunk size in each range yielded the best MRR@10.\nText without punctuation worked best for small chunks while unprocessed text worked best for medium and large chunks.\n\n\n\n\nVisualizing Metrics by Chunk Size and Method\n\n\nShow plot_metrics function\ndef plot_metrics(df, metric=\"MRR@10\"):\n  plt.style.use('seaborn-v0_8-bright')\n  fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n  colors = ['#268BD2', '#DC322F', '#859900', '#6C71C4']\n\n  for i, dataset in enumerate(['no preprocessing', 'no HTML', 'no punctuation']):\n    data_df = df[df['data'] == dataset]\n\n    for j, method in enumerate(sorted(data_df['method'].unique())):\n      method_data = data_df[data_df['method'] == method]\n      axs[i].plot(method_data['chunk_size'], method_data[metric],\n                        linestyle='--', linewidth=2,\n                        color=colors[j],\n                        label=method)\n      #axs[i].scatter(method_data['chunk_size'], method_data[metric], color=colors[j], s=25)\n\n      axs[i].set_xlabel('Chunk Size (Tokens)', fontsize=14)\n      axs[i].set_ylabel('Score' if i == 0 else '', fontsize=14)\n      axs[i].set_title(dataset.replace('no ', 'No '), fontsize=16, pad=10)\n      axs[i].tick_params(axis='both', labelsize=14)\n      axs[i].grid(True, alpha=0.8)\n      axs[i].set_xlim(50, 2050)\n      if metric == 'MRR@10': axs[i].set_ylim(0.15, 0.7)\n      else: axs[i].set_ylim(0.4, 1.0)\n  handles, labels = axs[0].get_legend_handles_labels()\n  fig.legend(handles, labels, loc='right', bbox_to_anchor=(1.03, 0.5), fontsize=18)\n\n  fig.suptitle(f\"fastbook-benchmark {metric} by Chunk Size\", fontsize=24, y=1.05)\n  plt.tight_layout()\n  plt.subplots_adjust(right=0.85)\n  plt.show()\n\n\n\nplot_metrics(df)\n\n\n\n\n\n\n\n\nTakeaways:\n\nAll semantic search methods peak at a chunk size of ~500 tokens.\nFull text search (BM25) mostly monotonically improves with chunk size.\nSingle-vector cosine simililarity is the worst-performing method after ~250 tokens.\nFor semantic search methods, in general, answerai-colbert-small-v1 > ColBERTv2 > Single-vector cosine similarity.\n\n\nplot_metrics(df, metric=\"Recall@10\")\n\n\n\n\n\n\n\n\nTakeaways:\n\nWhen the data is not preprocessed, all semantic search methods’ Recall@10 decreases from ~500 tokens to ~1000 tokens, then increase again to 2000 tokens.\nAfter 500 tokens, full text search (BM25) has the best Recall@10 for most of the way."
  },
  {
    "objectID": "posts/2024-11-29-fastbook-benchmark-results/index.html#final-thoughts",
    "href": "posts/2024-11-29-fastbook-benchmark-results/index.html#final-thoughts",
    "title": "Scoring Full Text and Semantic Search on Chunk Sizes from 100 to 2000 Tokens",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis was an incredibly awesome experience. I did a couple iterations of these experiments before this notebook, each time refactoring my approach so that I could relatively concisely run these chunk size experiments. That process was tough at times but really rewarding.\nMy three main takeaways:\n\nFor small chunks, answerai-colbert-small-v1 has the best MRR@10 and Recall@10.\nFor medium chunks, use ColBERTv2 for the best MRR@10 and full text search for the best Recall@10.\nFor large chunks, full text search has the best MRR@10 and Recall@10.\n\nI expect to try out all of these approaches after I integrate an LLM to turn this information retrieval pipeline to a RAG pipeline. My hypothesis is that smaller chunks will yield better responses from the LLM since there will be less “noise” and more “signal” in the context."
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "",
    "text": "!pip install RAGatouille -qq"
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#background",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#background",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "Background",
    "text": "Background\nThis walkthrough reconstructs the PLAID scoring pipeline by tracing code paths in the ColBERT research codebase that reproduce RAGatouille’s verified results. This is a reverse engineering process - I pulled at promising threads (code related to centroids, passage IDs, and scores) and validated my understanding by comparing against RAGatouille’s known-correct outputs.\n\n\n\n4-Stage PLAID Scoring Pipeline\n\n\nHere’s my video walkthrough of the code in this notebook:"
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#ragatouille-results",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#ragatouille-results",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "RAGatouille Results",
    "text": "RAGatouille Results\nIn this notebook, the gold truth scores for the documents given a query are determined by the RAGatouille library. I create a query (What is Python?) and a simple set of documents where one document is the obvious right answer (Python is a programming language. It is easy to learn) , one is a hard negative in that it’s about Python (Python was created by Guido van Rossum in 1991) and one is a easier negative as it’s related to the programming but not about Python (Java is a popular coding language used in many applications).\n\nRAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n\n\nq = \"What is Python?\"\nq\n\n'What is Python?'\n\n\n\ndocuments = [\n    \"Python is a programming language. It is easy to learn\",\n    \"Java is a popular coding language used in many applications\",\n    \"Python was created by Guido van Rossum in 1991\"\n]\ndocuments\n\n['Python is a programming language. It is easy to learn',\n 'Java is a popular coding language used in many applications',\n 'Python was created by Guido van Rossum in 1991']\n\n\n\nindex_path = RAG.index(index_name=\"toy_example\", collection=documents)\n\n\nindex_path\n\n'.ragatouille/colbert/indexes/toy_example'\n\n\n\nresults = RAG.search(q)\nresults\n\nWARNING: k value is larger than the number of documents in the index! Lowering k to 3...\n\n\n[{'content': 'Python is a programming language. It is easy to learn',\n  'score': 27.6875,\n  'rank': 1,\n  'document_id': '8aa5aade-10d5-4144-b634-c7866578b43c',\n  'passage_id': 0},\n {'content': 'Python was created by Guido van Rossum in 1991',\n  'score': 22.28125,\n  'rank': 2,\n  'document_id': '12352328-9bb1-4a51-896e-a002d3548adc',\n  'passage_id': 2},\n {'content': 'Java is a popular coding language used in many applications',\n  'score': 13.953125,\n  'rank': 3,\n  'document_id': '1fe9788e-ccec-4b9e-a004-14acc657915e',\n  'passage_id': 1}]\n\n\nNote the score and passage_id—these are values I’ll continuously reference throughout my walkthrough:\n\n\n\n\n\n\n\n\npassage_id\nscore\npassage text\n\n\n\n\n0\n27.6875\n“Python is a programming language. It is easy to learn”\n\n\n2\n22.28125\n“Python was created by Guido van Rossum in 1991”\n\n\n1\n13.953125\n“Java is a popular coding language used in many applications”"
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#where-to-start",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#where-to-start",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "Where to Start?",
    "text": "Where to Start?\nStarting at the top—the model in play: of type ColBERT\n\nm = RAG.model\nm\n\n<ragatouille.models.colbert.ColBERT at 0x7d0a4cc14670>\n\n\nAnd its index—of type PLAIDModelIndex.\n\nindex = m.model_index\nindex\n\n<ragatouille.models.index.PLAIDModelIndex at 0x7d0bd8bb7e80>\n\n\nInside the source code for PLAIDModelIndex the most promising method seemed be the _search method, which contains the following line:\nreturn self.searcher.search(query, k=k, pids=pids)\nThis contained three things I recognized: the query, the top k value and the passage IDs pids.\n\nindex.searcher\n\n<colbert.searcher.Searcher at 0x7d0a4cdc6530>"
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#the-searcher-class",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#the-searcher-class",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "The Searcher Class",
    "text": "The Searcher Class\nI couldn’t find the Searcher in RAGAtouille’s codebase as it was imported from colbert. Thankfully, installing RAGatouille gives you access to this library!\n\ncolbert.Searcher\n\n\n    colbert.searcher.Searcherdef __init__(index, checkpoint=None, collection=None, config=None, index_root=None, verbose: int=3)/usr/local/lib/python3.10/dist-packages/colbert/searcher.py<no docstring>\n      \n      \n\n\nThe Searcher takes as a required argument an index path—which we have!\n\nsearcher = colbert.Searcher(index='toy_example')\n\n[Dec 24, 15:59:52] #> Loading codec...\n[Dec 24, 15:59:52] #> Loading IVF...\n[Dec 24, 15:59:52] #> Loading doclens...\n\n\n100%|██████████| 1/1 [00:00<00:00, 4315.13it/s]\n\n\n[Dec 24, 15:59:52] #> Loading codes and residuals...\n\n\n\n100%|██████████| 1/1 [00:00<00:00, 454.67it/s]\n\n\nNotice that it loads the codec, IVF and doclens, all things we’ll look at throughout this notebook. Looking inside the Searcher.search method (which was called in the PLAIDModelIndex._search method) I see the following:\ndef search(self, text: str, k=10, filter_fn=None, full_length_search=False, pids=None):\n    Q = self.encode(text, full_length_search=full_length_search)\n    return self.dense_search(Q, k, filter_fn=filter_fn, pids=pids)\nThis is encoding the text into Q. Looks promising. I’ll see what that gets me:\n\nQ = searcher.encode(q)\nQ.shape\n\ntorch.Size([1, 32, 128])\n\n\nLooks good! It has 32 tokens, each with a 128-dimension encoding.\nSearcher.search returns the output of Searcher.dense_search, and the Searcher.dense_search method looks very promising. It takes queries Q and passages IDs and returns passage IDs and scores.\npids, scores = self.ranker.rank(self.config, Q, filter_fn=filter_fn, pids=pids)\n\nreturn pids[:k], list(range(1, k+1)), scores[:k]\n\nsearcher.dense_search\n\n\n    colbert.searcher.Searcher.dense_searchdef dense_search(Q: torch.Tensor, k=10, filter_fn=None, pids=None)/usr/local/lib/python3.10/dist-packages/colbert/searcher.py<no docstring>\n      \n      \n\n\n\nsearcher.dense_search(Q)\n\n([0], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [27.6875])\n\n\nExcellent!! The first value is pids[:k], which I understand to be the passage IDs corresponding to the top-k results. In this case it’s just 1 passage ID, 0, the first passage (“Python is a programming language. It is easy to learn”). The second value is list(range(1,k+1)) which is just a list from 1 to k. The final value is scores[:k] which is score corresponding to the top-k values. In this case it’s just one score, but an important one in our journey, as it is the same as RAGatouille for this passage (27.6875) !\n\nresults[0]\n\n{'content': 'Python is a programming language. It is easy to learn',\n 'score': 27.6875,\n 'rank': 1,\n 'document_id': '8aa5aade-10d5-4144-b634-c7866578b43c',\n 'passage_id': 0}\n\n\nThis is incredibly exciting, but I’m only getting 1 score instead of 3. Looking more closely at dense_search I see that the value of k determines the value of config.ncells:\nif k <= 10:\n    if self.config.ncells is None:\n        self.configure(ncells=1)\n    if self.config.centroid_score_threshold is None:\n        self.configure(centroid_score_threshold=0.5)\n    if self.config.ndocs is None:\n        self.configure(ndocs=256)\nAfterwhich dense_search calls:\npids, scores = self.ranker.rank(self.config, Q, filter_fn=filter_fn, pids=pids)\nI’ll look at Searcher.ranker.rank next."
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#searcher.ranker.rank",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#searcher.ranker.rank",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "Searcher.ranker.rank",
    "text": "Searcher.ranker.rank\n\nsearcher.ranker.rank\n\n\n    colbert.search.index_storage.IndexScorer.rankdef rank(config, Q, filter_fn=None, pids=None)/usr/local/lib/python3.10/dist-packages/colbert/search/index_storage.py<no docstring>\n      \n      \n\n\nThe first parameter, config, is found in the Searcher:\n\nsearcher.config\n\nColBERTConfig(query_token_id='[unused0]', doc_token_id='[unused1]', query_token='[Q]', doc_token='[D]', ncells=1, centroid_score_threshold=0.5, ndocs=256, load_index_with_mmap=False, index_path=None, index_bsize=32, nbits=4, kmeans_niters=20, resume=False, similarity='cosine', bsize=64, accumsteps=1, lr=1e-05, maxsteps=400000, save_every=None, warmup=20000, warmup_bert=None, relu=False, nway=64, use_ib_negatives=True, reranker=False, distillation_alpha=1.0, ignore_scores=False, model_name=None, query_maxlen=32, attend_to_mask_tokens=False, interaction='colbert', dim=128, doc_maxlen=256, mask_punctuation=True, checkpoint='colbert-ir/colbertv2.0', triples='/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json', collection=<colbert.data.collection.Collection object at 0x7d0a4cc052d0>, queries='/future/u/okhattab/data/MSMARCO/queries.train.tsv', index_name='toy_example', overwrite=False, root='.ragatouille/', experiment='colbert', index_root=None, name='2024-12/24/15.52.48', rank=0, nranks=1, amp=True, gpus=1, avoid_fork_if_possible=False)\n\n\nncells is 1.\n\nsearcher.config.ncells\n\n1\n\n\nFrom my previous amblings through the codebase, and which will see later on, I learned that ncells is synonymous with the \\(n_{probe}\\) parameter in the ColBERTv2 and PLAID ColBERTv2 papers (i.e. the number of centroids closest to each query token). In the papers they use values of 1, 4, and 8. I’ll pick a value of 4.\n\nsearcher.config.configure(ncells=4)\nsearcher.config.ncells\n\n4\n\n\nI’ll now pass config to Searcher.ranker.rank along with my encoded query Q:\n\nsearcher.ranker.rank(config=searcher.config, Q=Q)\n\n([0, 2, 1], [27.6875, 22.28125, 13.953125])\n\n\nMajor success!! I now see three passage IDs and their corresponding scores, an exact match with RAGatouille’s results.\n\nresults\n\n[{'content': 'Python is a programming language. It is easy to learn',\n  'score': 27.6875,\n  'rank': 1,\n  'document_id': '8aa5aade-10d5-4144-b634-c7866578b43c',\n  'passage_id': 0},\n {'content': 'Python was created by Guido van Rossum in 1991',\n  'score': 22.28125,\n  'rank': 2,\n  'document_id': '12352328-9bb1-4a51-896e-a002d3548adc',\n  'passage_id': 2},\n {'content': 'Java is a popular coding language used in many applications',\n  'score': 13.953125,\n  'rank': 3,\n  'document_id': '1fe9788e-ccec-4b9e-a004-14acc657915e',\n  'passage_id': 1}]\n\n\nAt this point I felt confident in the threads I was pulling and could now dig deeper and start recreating each stage of the PLAID scoring pipeline, starting with Stage 1."
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-1-initial-candidate-generation",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-1-initial-candidate-generation",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "Stage 1: Initial Candidate Generation",
    "text": "Stage 1: Initial Candidate Generation\nIn Stage 1, we retrieve the passage IDs corresponding to the ncells centroid IDs neareest to each of the query tokens. I have set ncells to 4 and have 32 query tokens so we’re dealing with a maximum of 4 x 32 = 128 centroid IDs.\nThe first promising line in Searcher.ranker.rank is the call to retrieve:\nif pids is None:\n    pids, centroid_scores = self.retrieve(config, Q)\n\nSearcher.ranker.retrieve\n\nsearcher.ranker.retrieve\n\n\n    colbert.search.index_storage.IndexScorer.retrievedef retrieve(config, Q)/usr/local/lib/python3.10/dist-packages/colbert/search/index_storage.py<no docstring>\n      \n      \n\n\n\npids, centroid_scores = searcher.ranker.retrieve(searcher.config, Q)\npids, centroid_scores.shape\n\n(tensor([0, 1, 2], device='cuda:0', dtype=torch.int32), torch.Size([64, 32]))\n\n\nThis is where I start to get really excited. I’m seeing abstract concepts in a research paper come to life! pids is now familiar—the three indexes 0, 1, and 2. What’s more interesting is centroid_scores which has 64 rows and 32 columns. We know from the PLAID paper that the MaxSim scores between centroids and query tokens is matrix with number of rows being the number of centroids and the number of columns being the number of tokens.\n\n\n\nSection 4.1 from the PLAID ColBERTv2 Paper\n\n\nLooking inside of Searcher.ranker.retrieve we see that it calls Searcher.ranker.generate_candidates:\npids, centroid_scores = self.generate_candidates(config, Q)\n\n\ngenerate_candidates\n\nsearcher.ranker.generate_candidates\n\n\n    colbert.search.candidate_generation.CandidateGeneration.generate_candidatesdef generate_candidates(config, Q)/usr/local/lib/python3.10/dist-packages/colbert/search/candidate_generation.py<no docstring>\n      \n      \n\n\nThis will only be a brief pit stop. There are three lines of interest:\nQ = Q.squeeze(0)\nQ = Q.cuda().half()\n\npids, centroid_scores = self.generate_candidate_pids(Q, ncells)\n\nQ = Q.squeeze(0)\nQ.shape\n\ntorch.Size([32, 128])\n\n\n\nQ = Q.cuda().half()\n\n\nsearcher.ranker.generate_candidates\n\n\n    colbert.search.candidate_generation.CandidateGeneration.generate_candidatesdef generate_candidates(config, Q)/usr/local/lib/python3.10/dist-packages/colbert/search/candidate_generation.py<no docstring>\n      \n      \n\n\n\ncells, scores = searcher.ranker.generate_candidates(searcher.config, Q)\ncells.shape, scores.shape\n\n(torch.Size([3]), torch.Size([64, 32]))\n\n\nThere’s that 64 x 32 shape again.\n\ncells\n\ntensor([0, 1, 2], device='cuda:0', dtype=torch.int32)\n\n\nI’m not entirely sure what cells is (I’m tempted to say it’s our passage IDs based on the values). Let’s look at get_candidate_pids first:\n\n\ngenerate_candidate_pids\nAnother quick stop, we see the following two very interesting line:\ncells, scores = self.get_cells(Q, ncells)\n\npids, cell_lengths = self.ivf.lookup(cells)\nFrom reading the paper, I know that in Stage 1 we get the centroid IDs that are close to the query tokens, which I think is what get_cells does. I also know that the PLAID index stores a mapping between passage IDs and centroid IDs, which is what I think ivf.lookup is looking up!\n\n\nget_cells\ndef get_cells(self, Q, ncells):\n    scores = (self.codec.centroids @ Q.T)\n    if ncells == 1:\n        cells = scores.argmax(dim=0, keepdim=True).permute(1, 0)\n    else:\n        cells = scores.topk(ncells, dim=0, sorted=False).indices.permute(1, 0)  # (32, ncells)\n    cells = cells.flatten().contiguous()  # (32 * ncells,)\n    cells = cells.unique(sorted=False)\n    return cells, scores\nThe first line is critical: self.codec.centroids @ Q.T is almost verbatim the matrix multiplication formula in the paper. This confirms that the 64 x 32 shape of scores is number of centroids x number of query tokens!\n\nsearcher.ranker.codec.centroids.shape\n\ntorch.Size([64, 128])\n\n\nThere are indeed 64 centroids, each of them with 128 dimensions. So cool to see!!\n\n_scores = (searcher.ranker.codec.centroids @ Q.T)\n_scores.shape\n\ntorch.Size([64, 32])\n\n\n\n(scores == _scores).float().mean()\n\ntensor(1., device='cuda:0')\n\n\nThe next line of interest is:\ncells = scores.topk(ncells, dim=0, sorted=False).indices.permute(1, 0)  # (32, ncells)\n\nscores.shape\n\ntorch.Size([64, 32])\n\n\n\nscores\n\ntensor([[ 0.0879,  0.0498,  0.0136,  ...,  0.0586,  0.0564,  0.0699],\n        [ 0.0798, -0.0043,  0.0223,  ..., -0.0271, -0.0166, -0.0172],\n        [ 0.2053,  0.0300,  0.1144,  ...,  0.0100,  0.0226,  0.0177],\n        ...,\n        [ 0.4587,  0.9458,  0.1954,  ...,  0.9507,  0.9482,  0.9482],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n       device='cuda:0', dtype=torch.float16)\n\n\nscores.topk seems to return the scores corresponding to the top-4 cosine similarities (between centroids and query tokens) for each token.\n\nscores.topk(4, dim=0, sorted=False).values\n\ntensor([[0.5786, 0.9521, 0.7397, 0.7510, 0.9395, 0.5850, 0.5859, 0.9131, 0.6782,\n         0.5186, 0.6353, 0.7104, 0.9209, 0.9014, 0.9443, 0.9473, 0.9434, 0.9497,\n         0.9468, 0.9331, 0.9355, 0.9233, 0.9370, 0.9336, 0.7026, 0.9468, 0.9131,\n         0.7529, 0.9517, 0.9482, 0.9448, 0.9429],\n        [0.5361, 0.8901, 0.6255, 0.6245, 0.9097, 0.5220, 0.7354, 0.8682, 0.6260,\n         0.4663, 0.5654, 0.6030, 0.8486, 0.8306, 0.9429, 0.8979, 0.8950, 0.8960,\n         0.8965, 0.8799, 0.8892, 0.8770, 0.8794, 0.8774, 0.5869, 0.8921, 0.8418,\n         0.6328, 0.8931, 0.9009, 0.8970, 0.8994],\n        [0.7129, 0.9458, 0.4612, 0.4766, 0.9658, 0.4229, 0.5332, 0.9219, 0.7002,\n         0.4092, 0.4729, 0.5034, 0.9111, 0.8955, 0.8872, 0.9487, 0.9458, 0.9497,\n         0.9492, 0.9375, 0.9419, 0.9312, 0.9380, 0.9365, 0.4958, 0.9453, 0.9048,\n         0.4829, 0.9482, 0.9507, 0.9482, 0.9482],\n        [0.4753, 0.8882, 0.3442, 0.3616, 0.8799, 0.3958, 0.5195, 0.8589, 0.6108,\n         0.4077, 0.4324, 0.4333, 0.8438, 0.8281, 0.8872, 0.8945, 0.8901, 0.8955,\n         0.8926, 0.8745, 0.8813, 0.8687, 0.8774, 0.8745, 0.4360, 0.8911, 0.8394,\n         0.3711, 0.8911, 0.8955, 0.8911, 0.8916]], device='cuda:0',\n       dtype=torch.float16)\n\n\nThe first value, 0.5786 represents the cosine similarity (dot product) between a centroid and the first query token.\n\ncells = scores.topk(4, dim=0, sorted=False).indices.permute(1,0)\ncells.shape\n\ntorch.Size([32, 4])\n\n\nThe following line flattens out the 32 x 4 matrix into a 128-value 1D tensor.\n\ncells = cells.flatten().contiguous()  # (32 * ncells,)\ncells.shape\n\ntorch.Size([128])\n\n\nThe following gets the unique centroid IDs. In this case there are 10 unique centroids that give the top-4 cosine similarity with all 32 tokens.\n\ncells = cells.unique(sorted=False)\ncells.shape\n\ntorch.Size([10])\n\n\n\ncells\n\ntensor([ 7,  8, 14, 19, 24, 29, 31, 38, 41, 61], device='cuda:0')\n\n\nConfirming that we can recreate the cosine similarity (0.5768) betweeen the first centroid (with ID = 7) and the first query token:\n\n(searcher.ranker.codec.centroids[7] * Q[0].T).sum()\n\nUserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n  (searcher.ranker.codec.centroids[7] * Q[0].T).sum()\n\n\ntensor(0.5786, device='cuda:0', dtype=torch.float16)\n\n\n\n\nivf\nThe next line in generate_candidate_pids gets to the core of PLAID: the mapping between centroid IDs and passage IDs:\npids, cell_lengths = self.ivf.lookup(cells)\n\nsearcher.ranker.ivf\n\n<colbert.search.strided_tensor.StridedTensor at 0x7d0a4f48e350>\n\n\n\npids, cell_lengths = searcher.ranker.ivf.lookup(cells)\npids.shape, cell_lengths.shape\n\n(torch.Size([10]), torch.Size([10]))\n\n\n\ncells\n\ntensor([ 7,  8, 14, 19, 24, 29, 31, 38, 41, 61], device='cuda:0')\n\n\n\npids\n\ntensor([0, 0, 1, 0, 1, 2, 2, 2, 0, 0], device='cuda:0', dtype=torch.int32)\n\n\n\ncell_lengths\n\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\nHow I interpret this: we have 10 centroid IDs (cells) and each are mapped to 1 passage ID. Now, given that we only have 3 passages to work with, there are some repeats (passage ID 0 corresponds to both centroid IDs 7 and 8).\nI’ll leave my full exploration of how ivf is constructed for a future video/blog post, but as an aside, I do want to highlight some things I learned, starting with the fact that our index_path contains a lot of interesting files!\n\npath = Path(index_path)\npath.ls()\n\n(#12) [Path('.ragatouille/colbert/indexes/toy_example/0.residuals.pt'),Path('.ragatouille/colbert/indexes/toy_example/collection.json'),Path('.ragatouille/colbert/indexes/toy_example/pid_docid_map.json'),Path('.ragatouille/colbert/indexes/toy_example/metadata.json'),Path('.ragatouille/colbert/indexes/toy_example/buckets.pt'),Path('.ragatouille/colbert/indexes/toy_example/doclens.0.json'),Path('.ragatouille/colbert/indexes/toy_example/centroids.pt'),Path('.ragatouille/colbert/indexes/toy_example/0.codes.pt'),Path('.ragatouille/colbert/indexes/toy_example/avg_residual.pt'),Path('.ragatouille/colbert/indexes/toy_example/plan.json'),Path('.ragatouille/colbert/indexes/toy_example/0.metadata.json'),Path('.ragatouille/colbert/indexes/toy_example/ivf.pid.pt')]\n\n\nEach passage contains 13 tokens:\n\njson.load(open(path/'doclens.0.json'))\n\n[13, 13, 13]\n\n\nFor a total of 39 token embeddings:\n\nmetadata = json.load(open(path/'0.metadata.json'))\nmetadata\n\n{'passage_offset': 0,\n 'num_passages': 3,\n 'num_embeddings': 39,\n 'embedding_offset': 0}\n\n\nThere are indeed 64 centroids.\n\ncentroids = torch.load(path/'centroids.pt', weights_only=True)\ncentroids.shape\n\ntorch.Size([64, 128])\n\n\nThere exists a mapping between the 64 centroid IDs and the 39 passage token embeddings\n\ncodes = torch.load(path/'0.codes.pt', weights_only=True)\ncodes.shape, codes.min(), codes.max()\n\n(torch.Size([39]), tensor(0, dtype=torch.int32), tensor(61, dtype=torch.int32))\n\n\nThere exists 39 residuals, one for each passage token! I’m not sure why there are only 64 dimensions.\n\nresiduals = torch.load(path/'0.residuals.pt', weights_only=True)\nresiduals.shape\n\ntorch.Size([39, 64])\n\n\nThe values of the residuals are integers!\n\nresiduals[0][:5]\n\ntensor([191, 183,  50,  66, 203], dtype=torch.uint8)\n\n\nOkay, that’s enough of an aside. The final piece of interest in generate_candidates are the lines\npids, centroid_scores = self.generate_candidate_pids(Q, ncells)\nsorter = pids.sort()\npids = sorter.values\n\npids, pids_counts = torch.unique_consecutive(pids, return_counts=True)\n\npids, centroid_scores = searcher.ranker.generate_candidate_pids(Q, ncells=4)\npids.shape, centroid_scores.shape\n\n(torch.Size([10]), torch.Size([64, 32]))\n\n\n\nsorter = pids.sort()\nsorter\n\ntorch.return_types.sort(\nvalues=tensor([0, 0, 0, 0, 0, 1, 1, 2, 2, 2], device='cuda:0', dtype=torch.int32),\nindices=tensor([1, 0, 8, 3, 9, 4, 2, 7, 5, 6], device='cuda:0'))\n\n\n\npids = sorter.values\npids, pids_counts = torch.unique_consecutive(pids, return_counts=True)\npids.shape, pids_counts.shape\n\n(torch.Size([3]), torch.Size([3]))\n\n\n\npids\n\ntensor([0, 1, 2], device='cuda:0', dtype=torch.int32)\n\n\n\npids_counts\n\ntensor([5, 2, 3], device='cuda:0')\n\n\nLet’s recap what we’ve done:\n\nWe picked a number of centroid IDs nearest to each query token that we’re interested in (ncells = 4)\nWe calculated the cosine similarity between centroids and query tokens (searcher.ranker.codec.centroids @ Q.T)\nWe picked the top-4 scores per query token (scores.topk) and grabbed their indices along dim=0 (rows).\nWe then reduced those 32 x 4 = 128 centroid IDs down to the 10 unique ones.\nAnd looked them up in our index to get the corresponding 10 passage IDs.\nWe reduced those to the 3 unique passage IDs.\n\nThese are our candidate passages at the end of Stage 1!"
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-2-centroid-interaction-with-pruning",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-2-centroid-interaction-with-pruning",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "Stage 2: Centroid Interaction with Pruning",
    "text": "Stage 2: Centroid Interaction with Pruning\nWe now want to keep PIDs corresponding to centroids that exceed a minimum threshold cosine similarity with query tokens.\nNow that we’ve gotten the initial candidate pids, the next line of interest in rank is:\nscores, pids = self.score_pids(config, Q, pids, centroid_scores)\n\nsearcher.ranker.score_pids\n\n\n    colbert.search.index_storage.IndexScorer.score_pidsdef score_pids(config, Q, pids, centroid_scores)/usr/local/lib/python3.10/dist-packages/colbert/search/index_storage.pyAlways supply a flat list or tensor for `pids`.\n\nSupply sizes Q = (1 | num_docs, *, dim) and D = (num_docs, *, dim).\nIf Q.size(0) is 1, the matrix will be compared with all passages.\nOtherwise, each query matrix will be compared against the *aligned* passage.\n      \n      \n\n\nscore_pids will handle both Stage 2 and Stage 3.\n\nPicking centroids with scores above threshold\nThe first important line in score_pids is the following:\nidx = centroid_scores.max(-1).values >= config.centroid_score_threshold\nI’ll refresh our pids and centroid_scores to the values they would have inside of rank:\n\nQ = searcher.encode(q)\nQ.shape\n\ntorch.Size([1, 32, 128])\n\n\n\npids, centroid_scores = searcher.ranker.retrieve(searcher.config, Q)\npids.shape, centroid_scores.shape\n\n(torch.Size([3]), torch.Size([64, 32]))\n\n\nThe centroid score threshold is set in the config\n\nsearcher.config.centroid_score_threshold\n\n0.5\n\n\n\nidx = centroid_scores.max(-1).values >= searcher.config.centroid_score_threshold\nidx.shape\n\ntorch.Size([64])\n\n\nidx is a boolean tensor, True for rows where the maximum cosine similarity is at or above the threshold and False for where it’s under it.\n\nidx\n\ntensor([False, False, False, False, False, False, False,  True,  True, False,\n        False, False, False, False,  True, False, False, False, False,  True,\n        False, False, False, False, False, False, False, False, False,  True,\n        False,  True, False, False, False, False, False, False,  True, False,\n        False,  True, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False,  True, False, False], device='cuda:0')\n\n\n\n\nLooking up codes (centroid IDs corresponding to passage tokens)\nThe next line of interest in score_pids is:\ncodes_packed, codes_lengths = self.embeddings_strided.lookup_codes(pids_)\n\nsearcher.ranker.embeddings_strided\n\n<colbert.indexing.codecs.residual_embeddings_strided.ResidualEmbeddingsStrided at 0x7d0a4f48d180>\n\n\nA key point here, which aligns with what I understood from the paper: we are looking up centroid IDs corresponding to the passage tokens in our passage IDs. Note that there are 39 values in codes_packed which correspond to the 39 passage token embeddings. I’ll leave me exploration for how I got this conclusion for a future video/blog post.\n\ncodes_packed, codes_lengths = searcher.ranker.embeddings_strided.lookup_codes(pids)\ncodes_packed.shape, codes_lengths.shape\n\n(torch.Size([39]), torch.Size([3]))\n\n\n\ncodes_packed\n\ntensor([41,  8, 61,  7,  7, 47, 60, 19, 19,  1, 42, 46, 41, 24, 58, 58, 14, 14,\n         2,  3, 60, 23, 12,  2, 16, 48, 29, 38, 31, 37,  9,  6, 57,  5, 17, 13,\n         0,  0, 29], device='cuda:0', dtype=torch.int32)\n\n\nNote that codes_lengths tells us how many tokens there are in each passage.\n\ncodes_lengths\n\ntensor([13, 13, 13])\n\n\nFrom the PLAID paper (emphasis mine):\n\nThe procedure works as follows. Recall that \\(S_{c,q}\\) from Equation 2 stores the relevance scores for each centroid with respect to the query tokens. Suppose \\(I\\) is the list of the centroid indices mapped to each of the tokens in the candidate set. Furthermore, let \\(S_{c,q}\\) denote the i-th row of \\(S_{c,q}\\). Then PLAID constructs the centroid-based approximate scores \\(\\tilde{D}\\) as:\n\n\n\nPicking scores for centroid IDs corresponding to passage tokens\nThe next line in score_pids does just that: pick the centroid IDs that correspond to the passage tokens of interest:\n\nidx_ = idx[codes_packed.long()]\nidx_.shape\n\ntorch.Size([39])\n\n\n\nidx_\n\ntensor([ True,  True,  True,  True,  True, False, False,  True,  True, False,\n        False, False,  True, False, False, False,  True,  True, False, False,\n        False, False, False, False, False, False,  True,  True,  True, False,\n        False, False, False, False, False, False, False, False,  True],\n       device='cuda:0')\n\n\nWe then use this to index back into codes_packed to select the centroid IDs that are at or above the threshold of 0.5. There are 14 such centroid IDs.\n\ncodes_packed_ = codes_packed[idx_]\ncodes_packed_, codes_packed_.shape\n\n(tensor([41,  8, 61,  7,  7, 19, 19, 41, 14, 14, 29, 38, 31, 29],\n        device='cuda:0', dtype=torch.int32),\n torch.Size([14]))\n\n\nFinally, we can index into our scores and pick out the scores that correspond to these centroid IDs:\n\napprox_scores_ = centroid_scores[codes_packed_.long()]\napprox_scores_.shape\n\ntorch.Size([14, 32])\n\n\nWe now have the scores between 14 centroids that are mapped to our candidate passage ID’s tokens and all 32 query tokens.\n\n\nMax-reducing scores to get 1 score per passage ID\nThe last step of Stage 2 is to max-reduce the scores down to 1 per passage. The following lines are of interest:\npruned_codes_strided = StridedTensor(idx_, codes_lengths, use_gpu=self.use_gpu)\npruned_codes_padded, pruned_codes_mask = pruned_codes_strided.as_padded_tensor()\npruned_codes_lengths = (pruned_codes_padded * pruned_codes_mask).sum(dim=1)\n\n...\n\napprox_scores_strided = StridedTensor(approx_scores_, pruned_codes_lengths, use_gpu=self.use_gpu)\napprox_scores_padded, approx_scores_mask = approx_scores_strided.as_padded_tensor()\napprox_scores_ = colbert_score_reduce(approx_scores_padded, approx_scores_mask, config)\n\nfrom colbert.search.strided_tensor import StridedTensor, StridedTensorCore\n\n\nidx_.shape\n\ntorch.Size([39])\n\n\n\npruned_codes_strided = StridedTensor(idx_, codes_lengths)\npruned_codes_strided\n\n<colbert.search.strided_tensor.StridedTensor at 0x7d0a4f688e20>\n\n\n\npruned_codes_padded, pruned_codes_mask = pruned_codes_strided.as_padded_tensor()\npruned_codes_padded.shape, pruned_codes_mask.shape\n\n(torch.Size([3, 13]), torch.Size([3, 13]))\n\n\n\npruned_codes_padded\n\ntensor([[ True,  True,  True,  True,  True, False, False,  True,  True, False,\n         False, False,  True],\n        [False, False, False,  True,  True, False, False, False, False, False,\n         False, False, False],\n        [ True,  True,  True, False, False, False, False, False, False, False,\n         False, False,  True]], device='cuda:0')\n\n\nMy understanding of StridedTensor is still spotty, but from what I can see it allows the reshaping of values from a 1-D 39 to 3 x 13. In this way, we are organizing centroid IDs by both passage ID and by passage token.\n\npruned_codes_lengths = (pruned_codes_padded * pruned_codes_mask).sum(dim=1)\npruned_codes_lengths\n\ntensor([8, 2, 4], device='cuda:0')\n\n\nThere are 8 centroids corresponding to tokens in the first passage that cross the score threshold, 2 for the second passage and 4 for the third.\nNote that pruned_codes_mask is True everywhere so multiplying pruned_codes_padded by it keeps all of its values intact:\n\npruned_codes_mask\n\ntensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n         True],\n        [True, True, True, True, True, True, True, True, True, True, True, True,\n         True],\n        [True, True, True, True, True, True, True, True, True, True, True, True,\n         True]], device='cuda:0')\n\n\nThe next lines reshapes our approx_scores_ similarly.\n\napprox_scores_.shape\n\ntorch.Size([14, 32])\n\n\n\napprox_scores_strided = StridedTensor(approx_scores_, pruned_codes_lengths)\napprox_scores_padded, approx_scores_mask = approx_scores_strided.as_padded_tensor()\napprox_scores_padded.shape, approx_scores_mask.shape\n\n(torch.Size([3, 8, 32]), torch.Size([3, 8, 1]))\n\n\nWhat’s interesting to note here is approx_scores_mask. We know from pruned_codes_lengths that there are 8 centroid IDs for the first passage, 2 for the second, and 4 for the third. approx_scores_mask flags True for valid values and False for padding values.\n\napprox_scores_mask\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True]],\n\n        [[ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False]],\n\n        [[ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False]]], device='cuda:0')\n\n\nWe then pass these padded scores to colbert_reduce which performs the famous MaxSim operation:\n\nfrom colbert.modeling.colbert import colbert_score, colbert_score_packed, colbert_score_reduce\n\n\napprox_scores_ = colbert_score_reduce(approx_scores_padded, approx_scores_mask, searcher.config)\napprox_scores_\n\ntensor([27.2812, 12.9844, 22.1719], device='cuda:0', dtype=torch.float16)\n\n\nNote that these are not the same values as our RAGatouille results. Instead, these are intermediate scores.\nTaking a look at colbert_score_reduce there are four main lines of interest in my opinion:\nD_padding = ~D_mask.view(scores_padded.size(0), scores_padded.size(1)).bool()\nscores_padded[D_padding] = -9999\nscores = scores_padded.max(1).values\nscores.sum(-1)\n\nD_padding = ~approx_scores_mask.view(approx_scores_padded.size(0), approx_scores_padded.size(1)).bool()\napprox_scores_padded[D_padding] = -9999\napprox_scores_padded.shape\n\ntorch.Size([3, 8, 32])\n\n\nWe take the max along dim=1, which is the dimension with centroid IDs corresponding to passage tokens. So, in other words, we are finding the maximum score between centroid and query token for each query token per passage.\n\nscores = approx_scores_padded.max(1).values\nscores.shape\n\ntorch.Size([3, 32])\n\n\nFinally, we sum across query tokens per passage ID.\n\nscores.sum(-1)\n\ntensor([27.2812, 12.9844, 22.1719], device='cuda:0', dtype=torch.float16)\n\n\n\n\nPicking the top-ndocs passages\nThe last step is to pick the top-ndocs passage IDs.\n\nsearcher.config.ndocs\n\n256\n\n\nndocs is 256, much larger than the number of passages we have, so I’ll rank with k=3:\n\nif searcher.config.ndocs // 4 < len(approx_scores_):\n    pids = pids[torch.topk(approx_scores_, k=(searcher.config.ndocs // 4)).indices]\n\n\npids = pids[torch.topk(approx_scores_, k=3).indices]\npids\n\ntensor([0, 2, 1], device='cuda:0', dtype=torch.int32)\n\n\nTo recap Stage 2:\n\nWe select centroid IDs that were both at/above our threshold of 0.5 AND corresponded to passage tokens for the passage IDs in our initial candidate pool from Stage 1.\nWe then do some reshaping so we can calculate the MaxSim score for each passage ID.\nWe pick the top-ndocs passages."
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-3-centroid-interaction-wo-pruning",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-3-centroid-interaction-wo-pruning",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "Stage 3: Centroid Interaction w/o Pruning",
    "text": "Stage 3: Centroid Interaction w/o Pruning\nWe now have our candidate set from Stage 2. We lookup the centroid IDs for the passage tokens in these passage IDs:\n\ncodes_packed, codes_lengths = searcher.ranker.embeddings_strided.lookup_codes(pids)\ncodes_packed.shape, codes_lengths.shape\n\n(torch.Size([39]), torch.Size([3]))\n\n\nWe don’t use the threshold for this step—all centroids, even those who have a maximum cosine similarity with query tokens being less than our threshold:\n\napprox_scores = centroid_scores[codes_packed.long()]\napprox_scores.shape\n\ntorch.Size([39, 32])\n\n\nNote how we are now dealing with 39 centroids, not 14 like we did in Stage 2.\nWe do the same reshaping/padding as we did in Stage 2, and use colbert_score_reduce again:\n\napprox_scores_strided = StridedTensor(approx_scores, codes_lengths)\napprox_scores_padded, approx_scores_mask = approx_scores_strided.as_padded_tensor()\napprox_scores = colbert_score_reduce(approx_scores_padded, approx_scores_mask, searcher.config)\napprox_scores\n\ntensor([27.2812, 22.1875, 13.7266], device='cuda:0', dtype=torch.float16)\n\n\nWe then pick the top-ndocs//4 passage IDs, in this case all 3 of our passage IDs.\n\nif searcher.config.ndocs // 4 < len(approx_scores):\n    pids = pids[torch.topk(approx_scores, k=(searcher.config.ndocs // 4)).indices]\n\n\npids = pids[torch.topk(approx_scores, k=3).indices]\npids\n\ntensor([0, 2, 1], device='cuda:0', dtype=torch.int32)\n\n\nThis is the candidate set at the end of Stage 3. Note that the scores are still not quite the same as RAGatouille, as these are intermediate scores."
  },
  {
    "objectID": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-4-final-ranking-with-decompression",
    "href": "posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/index.html#stage-4-final-ranking-with-decompression",
    "title": "Recreating the PLAID ColBERTv2 Scoring Pipeline: From Research Code to RAGatouille",
    "section": "Stage 4: Final ranking with decompression",
    "text": "Stage 4: Final ranking with decompression\nWe’re now in the final Stage of the PLAID scoring pipeline. We now get the full 128-dimension vectors for all of our passage ID’s tokens:\n\nD_packed, D_mask = searcher.ranker.lookup_pids(pids)\nD_packed.shape, D_mask.shape\n\n(torch.Size([39, 128]), torch.Size([3]))\n\n\n\nD_packed[0][0]\n\ntensor(-0.0102, device='cuda:0', dtype=torch.float16)\n\n\nSince we only have one query:\n\nQ.size(0)\n\n1\n\n\nWe use the following lines of code:\n\ncolbert_score_packed(Q, D_packed, D_mask, searcher.config)\n\ntensor([27.6875, 22.2812, 13.9531], device='cuda:0', dtype=torch.float16)\n\n\nLooking into colbert_score_packed, here are the lines of interest:\n\nQ = Q.squeeze(0)\nQ.shape\n\ntorch.Size([32, 128])\n\n\nWe calculate the cosine similarity between each passage token and each query token:\n\nscores = D_packed @ Q.to(dtype=D_packed.dtype).T\nscores.shape\n\ntorch.Size([39, 32])\n\n\nReshape them so we can max-reduce them by passage ID:\n\nscores_padded, scores_mask = StridedTensor(scores, D_mask).as_padded_tensor()\nscores_padded.shape, scores_mask.shape\n\n(torch.Size([3, 13, 32]), torch.Size([3, 13, 1]))\n\n\nAnd max-reduce with colbert_score_reduce:\n\ncolbert_score_reduce(scores_padded, scores_mask, searcher.config)\n\ntensor([27.6875, 22.2812, 13.9531], device='cuda:0', dtype=torch.float16)\n\n\nThis matches exactly the scores we got using RAGatouille!\n\nresults\n\n[{'content': 'Python is a programming language. It is easy to learn',\n  'score': 27.6875,\n  'rank': 1,\n  'document_id': '8aa5aade-10d5-4144-b634-c7866578b43c',\n  'passage_id': 0},\n {'content': 'Python was created by Guido van Rossum in 1991',\n  'score': 22.28125,\n  'rank': 2,\n  'document_id': '12352328-9bb1-4a51-896e-a002d3548adc',\n  'passage_id': 2},\n {'content': 'Java is a popular coding language used in many applications',\n  'score': 13.953125,\n  'rank': 3,\n  'document_id': '1fe9788e-ccec-4b9e-a004-14acc657915e',\n  'passage_id': 1}]\n\n\nTo recap Stage 4:\n\nWe lookup the full vectors corresponding to all passage tokens in our passage IDs.\nWe reshape them to allow for max-reduction by passage ID.\nWe calculate the MaxSim score for each passage.\n\nIn this way, we were able to recreate the entire 4-stage PLAID pipeline to match RAGatouille results!"
  },
  {
    "objectID": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html",
    "href": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html",
    "title": "DoRA’s Magnitude Vector",
    "section": "",
    "text": "from peft import LoraConfig, get_peft_model\nimport transformers\n\n# the following imports are from dora.py\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom peft.utils.integrations import dequantize_module_weight, gather_params_ctx\nfrom peft.utils.other import transpose"
  },
  {
    "objectID": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#background",
    "href": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#background",
    "title": "DoRA’s Magnitude Vector",
    "section": "Background",
    "text": "Background\nI am currently re-reading the DoRA (Weight-Decomposed Low-Rank Adaptation) paper. I took a bit of a detour and worked through the fantastic article Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch by Sebastian Raschka (I am also reading his book Building a Large Language Model (from scratch) as part of a fastai study group). The article is full of helpful diagrams and breakdowns of concepts as well as easily digestible implementation in code. One particular breakthrough for me while reading the article was his demonstration of the distributive law of multiplication:\n\nx.(W+ΔW) = x.W + x.ΔW\nSimilarly, we can write the following for LoRA:\nx.(W+A.B) = x.W + x.A.B\n\nReading this made it click for me why and how LoRA adapters are such an efficient way of handling downstream tasks.\nI also took a deep dive into the peft library’s implementation of DoRA. I recently made a video of this deep dive.\nIn this blog post I am going to compare Raschka’s article’s implementation with peft’s and highlight a key difference that I found between them in how they implement the decomposition of a weight matrix into its magnitude and directional components.\nI’ll start by reviewing both approaches."
  },
  {
    "objectID": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#raschkas-implementation",
    "href": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#raschkas-implementation",
    "title": "DoRA’s Magnitude Vector",
    "section": "Raschka’s Implementation",
    "text": "Raschka’s Implementation\nI want to add a caveat that this implementation I assume is by no means a “final” or “production” implementation, as I understand it to be more educational and illustrative.\nI’ll start by copy/pasting relevant code: LoRALayer (DoRA uses LoRA to fine-tune the directional component) and LinearWithDoRAMerged.\n\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank, alpha):\n        super().__init__()\n        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.alpha * (x @ self.A @ self.B)\n        return x\n\n\nclass LinearWithDoRAMerged(nn.Module):\n\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.lora = LoRALayer(\n            linear.in_features, linear.out_features, rank, alpha\n        )\n        self.m = nn.Parameter(\n            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n\n\n  # Code loosely inspired by\n  # https://github.com/catid/dora/blob/main/dora.py\n\n    def forward(self, x):\n        lora = self.lora.A @ self.lora.B\n        numerator = self.linear.weight + self.lora.alpha*lora.T\n        denominator = numerator.norm(p=2, dim=0, keepdim=True)\n        directional_component = numerator / denominator\n        new_weight = self.m * directional_component\n        return F.linear(x, new_weight, self.linear.bias)\n\nI’ll also create a regular linear layer using one of the in/out feature values in the Raschka article:\n\nlinear = nn.Linear(in_features=784, out_features=128, bias=True)\nlinear\n\nLinear(in_features=784, out_features=128, bias=True)\n\n\n\ndora_layer = LinearWithDoRAMerged(linear, 256, 512)\ndora_layer\n\nLinearWithDoRAMerged(\n  (linear): Linear(in_features=784, out_features=128, bias=True)\n  (lora): LoRALayer()\n)\n\n\nHere’s the key value: the shape of the magnitude vector. In Raschka’s code, it’s 1 x 784, where 784 is the number of linear in_features.\n\ndora_layer.m.shape\n\ntorch.Size([1, 784])\n\n\nLooking at LinearWithDoRAMerged.__init__:\nself.m = nn.Parameter(\n            self.linear.weight.norm(p=2, dim=0, keepdim=True))\nThe norm is taking over dim=0, which is the dimension of out_features:\n\nlinear.weight.shape\n\ntorch.Size([128, 784])\n\n\nIn other words, we end up with 1 magnitude value for each of the 784 input neurons."
  },
  {
    "objectID": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#peft-implementation",
    "href": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#peft-implementation",
    "title": "DoRA’s Magnitude Vector",
    "section": "peft Implementation",
    "text": "peft Implementation\nFrom src/peft/tuners/lora/dora.py:\nclass DoraLinearLayer(nn.Module):\n    def __init__(self, fan_in_fan_out):\n        super().__init__()\n        self.fan_in_fan_out = fan_in_fan_out\n\n    def get_weight_norm(self, weight, lora_weight, scaling) -> torch.Tensor:\n        # calculate L2 norm of weight matrix, column-wise\n        weight = transpose(weight, self.fan_in_fan_out)\n        weight = weight + scaling * lora_weight\n        weight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)\n        return weight_norm\n\n    ...\nThe very important attribute here is fan_in_fan_out. I found a few places in the peft codebase which documented it as follows:\nSet this to True if the layer to replace stores weight like (fan_in, fan_out)\n\nsrc/peft/tuners/ia3/config.py\nsrc/peft/tuners/vblora/layer.py\nsrc/peft/tuners/vblora/layer.py\n\nHow I interpret this: if the weights are stored as (in, out), fan_in_fan_out is True, if stored as (out, in) fan_in_fan_out is False.\nLooking at an example, I’ll peft-ify SmolLM2-135M:\n\nmodel_nm = 'HuggingFaceTB/SmolLM2-135M'\nmodel_nm\n\n'HuggingFaceTB/SmolLM2-135M'\n\n\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n\n\npeft_config = LoraConfig(r=256, use_rslora=False, use_dora=True, target_modules=['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'])\n\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\ntrainable params: 78,307,200 || all params: 212,823,360 || trainable%: 36.7945\n\n\nLooking at one of the layers which has a different number of input and output features, k_proj:\n\nk_proj = model.base_model.model.model.layers[0].self_attn.k_proj\nk_proj\n\nlora.Linear(\n  (base_layer): Linear(in_features=576, out_features=192, bias=False)\n  (lora_dropout): ModuleDict(\n    (default): Identity()\n  )\n  (lora_A): ModuleDict(\n    (default): Linear(in_features=576, out_features=256, bias=False)\n  )\n  (lora_B): ModuleDict(\n    (default): Linear(in_features=256, out_features=192, bias=False)\n  )\n  (lora_embedding_A): ParameterDict()\n  (lora_embedding_B): ParameterDict()\n  (lora_magnitude_vector): ModuleDict(\n    (default): lora.dora.DoraLinearLayer()\n  )\n)\n\n\nThe base layer has 576 in_features and 192 out_features:\n\nk_proj.base_layer.weight.shape\n\ntorch.Size([192, 576])\n\n\nThe fan_in_fan_out attribute is False which checks out by looking at the shape above which is (out, in).\n\nk_proj.fan_in_fan_out\n\nFalse\n\n\nWhy is fan_in_fan_out such a big deal to me? Well, because look at how get_weight_norm is written:\ndef get_weight_norm(self, weight, lora_weight, scaling) -> torch.Tensor:\n    # calculate L2 norm of weight matrix, column-wise\n    weight = transpose(weight, self.fan_in_fan_out)\n    weight = weight + scaling * lora_weight\n    weight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)\n    return weight_norm\nI’ll walk through each line, starting with the base layers weight matrix:\n\nweight = k_proj.base_layer.weight\nweight.shape\n\ntorch.Size([192, 576])\n\n\nWe then pass the weight and fan_in_fan_out to transpose:\n\nweight = transpose(weight, k_proj.fan_in_fan_out)\nweight.shape\n\ntorch.Size([192, 576])\n\n\nIt doesn’t transpose it! That’s because in tranpose if fan_in_fan_out is False it returns the weight as is:\ndef transpose(weight, fan_in_fan_out):\n    if not fan_in_fan_out:\n        return weight\n\n    if isinstance(weight, torch.nn.Parameter):\n        return torch.nn.Parameter(weight.T)\n    return weight.T\nThis is absolutely critical in determining the shape of the magnitude vector:\n\nweight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)\nweight_norm.shape\n\ntorch.Size([192])\n\n\nNote two things here: - they take the norm across dim=1 (the number of in_features) - we end up with 1 magnitude value for each of the 192 output neurons. Contrast this with Raschka’s article where we had 1 magnitude value for each of the input neurons."
  },
  {
    "objectID": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#aside-claude-conversation",
    "href": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#aside-claude-conversation",
    "title": "DoRA’s Magnitude Vector",
    "section": "Aside: Claude Conversation",
    "text": "Aside: Claude Conversation\nWhile I was re-reading the DoRA paper, I found the emphasis on column-wise vector norm interesting so I asked Claude about it:\n\nPrompt: Why columns? What is special about doing this for columns and not rows. Is there something fundamental about columns in a matrix of pretrained weights?\n\n\nResponse: For neural networks, columns of weight matrices map features from one layer’s dimension to another’s. Each column represents all the connections to a single output neuron. So columns form natural functional units, making column-wise normalization more meaningful than row-wise for understanding and modifying the network’s learned transformations.\n\n\nPrompt: So suppose we have an input vector x which contains values corresponding to each input variable, the pretrained weight matrix would is d x k so x would be 1 x d. Each row in pretrained weight matrix would represent one input variable. Multiplying 1 x d with d x k gives us 1 x k where k is the number of activations (i.e. the number of neurons).\n\n\nResponse: Yes, that’s correct. The output for a single neuron (one column of the weight matrix) is the dot product between the input vector and that column’s weights. So when we do column-wise normalization, we’re normalizing the set of weights that feed into each individual output neuron. This helps explain why column-wise normalization is natural - each column represents a complete “unit” in terms of how the network processes information to produce one output activation.\n\nSo, it seems like decomposing the weight matrix into a magnitude vector and direction matrix corresponding to output neurons seems really important."
  },
  {
    "objectID": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#final-thoughts",
    "href": "posts/2025-02-01-DoRA-Magnitude-Vector/index.html#final-thoughts",
    "title": "DoRA’s Magnitude Vector",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nExplorations like this is why I’m grateful for open source code. The efforts of folks like HuggingFace and Raschka to democratize LLMs allows folks like me to curate insightful explorations for myself, and I’m obviously happy to share them out in this blog post. If you find something incorrect in my interpretation of Raschka’s or peft’s code, please at me on Twitter @vishal_learner.\nA future exercise that I want to do: train DoRA implementations with a column-wise norm and a row-wise norm and compare results. This would truly highlight whether it’s important to align the decomposition with the output neurons of the weight matrix."
  },
  {
    "objectID": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html",
    "href": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html",
    "title": "Evaluating the DAPR ConditionalQA Dataset with RAGatouille",
    "section": "",
    "text": "!pip install datasets ragatouille pytrec_eval ranx\n\n\nfrom datasets import load_dataset\nfrom ragatouille import RAGPretrainedModel\n\nimport numpy as np\nimport pandas as pd\n\nimport pytrec_eval\nfrom ranx import evaluate\nfrom ranx import Qrels, Run"
  },
  {
    "objectID": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#background",
    "href": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#background",
    "title": "Evaluating the DAPR ConditionalQA Dataset with RAGatouille",
    "section": "Background",
    "text": "Background\nI wanted to get familiar with classic information retrieval datasets, especially those with explicit documents. I searched with Perplexity and ChatGPT and came across DAPR: Document-Aware Passage Retrieval which sounded perfect for my use case.\nIn this blog post I’ll work through evaluating the test split of the ConditionalQA dataset in DAPR using RAGatouille and the answerai-colbert-small-v1 model for retrieval and the pytrec and ranx libraries for evaluation. I’ll use the simple Recall@10 metric as it’s the easiest to manually check."
  },
  {
    "objectID": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#load-and-view-data",
    "href": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#load-and-view-data",
    "title": "Evaluating the DAPR ConditionalQA Dataset with RAGatouille",
    "section": "Load and View Data",
    "text": "Load and View Data\nHere are the three datasets we are going to use for this evaluation:\n\nConditionalQA-corpus, our passages\nConditionalQA_queries, our queries\nand ConditionalQA_qrels, the mapping between queries and passages.\n\n\npassages = load_dataset(\"UKPLab/dapr\", \"ConditionalQA-corpus\", split=\"test\")\npassages\n\nDataset({\n    features: ['_id', 'text', 'title', 'doc_id', 'paragraph_no', 'total_paragraphs', 'is_candidate'],\n    num_rows: 69199\n})\n\n\n\npassages[0]\n\n{'_id': '0-0',\n 'text': 'Overview',\n 'title': 'Child Tax Credit',\n 'doc_id': '0',\n 'paragraph_no': 0,\n 'total_paragraphs': 77,\n 'is_candidate': True}\n\n\n\nqueries = load_dataset(\"UKPLab/dapr\", \"ConditionalQA-queries\", split=\"test\")\nqueries\n\nDataset({\n    features: ['_id', 'text'],\n    num_rows: 271\n})\n\n\n\nqueries[0]\n\n{'_id': 'dev-0',\n 'text': 'My brother and his wife are in prison for carrying out a large fraud scheme. Their 7 and 8 year old children have been living with me for the last 4 years. I want to become their Special Guardian to look after them permanently How long will it be before I hear back from the court?'}\n\n\n\nqrels_rows = load_dataset(\"UKPLab/dapr\", \"ConditionalQA-qrels\", split=\"test\")\nqrels_rows\n\nDataset({\n    features: ['query_id', 'corpus_id', 'score'],\n    num_rows: 1165\n})\n\n\n\nqrels_rows[0]\n\n{'query_id': 'dev-0', 'corpus_id': '86-41', 'score': 1}\n\n\nLoad answerai-colbert-small-v1:\n\nRAG = RAGPretrainedModel.from_pretrained(\"answerdotai/answerai-colbert-small-v1\")\nRAG\n\n<ragatouille.RAGPretrainedModel.RAGPretrainedModel at 0x7e5328fdced0>\n\n\nStructure the passages for indexing:\n\npassages[:5]\n\n{'_id': ['0-0', '0-1', '0-2', '0-3', '0-4'],\n 'text': ['Overview',\n  'You can only make a claim for Child Tax Credit if you already get Working Tax Credit.',\n  'If you cannot apply for Child Tax Credit, you can apply for Universal Credit instead.',\n  'You might be able to apply for Pension Credit if you and your partner are State Pension age or over.',\n  'What you’ll get'],\n 'title': ['Child Tax Credit',\n  'Child Tax Credit',\n  'Child Tax Credit',\n  'Child Tax Credit',\n  'Child Tax Credit'],\n 'doc_id': ['0', '0', '0', '0', '0'],\n 'paragraph_no': [0, 1, 2, 3, 4],\n 'total_paragraphs': [77, 77, 77, 77, 77],\n 'is_candidate': [True, True, True, True, True]}\n\n\n\npassage_texts = [p['text'] for p in passages]\npassage_texts[:5]\n\n['Overview',\n 'You can only make a claim for Child Tax Credit if you already get Working Tax Credit.',\n 'If you cannot apply for Child Tax Credit, you can apply for Universal Credit instead.',\n 'You might be able to apply for Pension Credit if you and your partner are State Pension age or over.',\n 'What you’ll get']\n\n\n\npassage_ids = [p['_id'] for p in passages]\npassage_ids[:5]\n\n['0-0', '0-1', '0-2', '0-3', '0-4']"
  },
  {
    "objectID": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#build-the-index-and-run-search",
    "href": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#build-the-index-and-run-search",
    "title": "Evaluating the DAPR ConditionalQA Dataset with RAGatouille",
    "section": "Build the index and run search",
    "text": "Build the index and run search\n\nindex_path = RAG.index(\n    index_name=\"conditionalqa_index\",\n    collection=passage_texts,\n    document_ids=passage_ids\n)\n\nTaking a look at the results for a single query. Each result has a content, score, rank, document_id, and passage_id. Note a bit of confusing terminology: document_id is actually the id of the item in the passages dataset and passage_id is an identifier created by RAGatouille, unrelated to the datasets.\n\nresults = RAG.search(queries[0]['text'], k=10)\nresults\n\n[{'content': 'You must advertise your claim within 14 days from the day you get a date for the first court hearing. The advert must appear in a print or online newspaper that covers the missing person’s last known usual address.',\n  'score': 70.0,\n  'rank': 1,\n  'document_id': '107-103',\n  'passage_id': 10480},\n {'content': 'The guardianship order will make you a guardian for a maximum of 4 years.',\n  'score': 70.0,\n  'rank': 2,\n  'document_id': '107-242',\n  'passage_id': 10619},\n {'content': 'You can claim joint Housing Benefit for up to 13 weeks if one of you has gone to prison and is likely to return home in 13 weeks or less - including any time on remand.',\n  'score': 69.9375,\n  'rank': 3,\n  'document_id': '8-67',\n  'passage_id': 911},\n {'content': 'The date will be either 14 or 28 days after your court hearing. If you’re in an exceptionally difficult situation, you may be able to convince the judge to delay this for up to 6 weeks.',\n  'score': 69.9375,\n  'rank': 4,\n  'document_id': '496-116',\n  'passage_id': 47939},\n {'content': 'You can claim or continue to claim joint Council Tax Reduction if your partner’s expected to be in prison for 13 weeks or less – including any time on remand.',\n  'score': 69.875,\n  'rank': 5,\n  'document_id': '8-80',\n  'passage_id': 924},\n {'content': 'Sometimes you’ll be given a 2 to 4 week period that you’ll need to keep free - this is known as a ‘warned period’ or ‘floating trial’. If this happens, you’ll be given 1 working day’s notice before you are due to go to court.',\n  'score': 69.875,\n  'rank': 6,\n  'document_id': '254-4',\n  'passage_id': 23999},\n {'content': 'Your Child Benefit payments will stop after 8 weeks if your child goes to prison or is on remand. You’ll get arrears if they’re cleared of the offence.',\n  'score': 69.8125,\n  'rank': 7,\n  'document_id': '8-116',\n  'passage_id': 960},\n {'content': 'You may be able to make a claim if you’re the dependant of someone who suffered from a dust-related disease but who has died. A dependant claim must be made within 12 months of the death of the sufferer.',\n  'score': 69.8125,\n  'rank': 8,\n  'document_id': '45-133',\n  'passage_id': 4921},\n {'content': 'You’ll be responsible for looking after the child until they’re 18 (unless the court takes your responsibility away earlier).',\n  'score': 69.8125,\n  'rank': 9,\n  'document_id': '86-2',\n  'passage_id': 8150},\n {'content': 'If it’s less than 90 days since the person went missing, explain you need the guardianship order urgently, for example, because the person is going to lose their house.',\n  'score': 69.8125,\n  'rank': 10,\n  'document_id': '107-43',\n  'passage_id': 10420}]"
  },
  {
    "objectID": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#evaluation",
    "href": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#evaluation",
    "title": "Evaluating the DAPR ConditionalQA Dataset with RAGatouille",
    "section": "Evaluation",
    "text": "Evaluation\nI’ll prepare qrels for the pytrec evaluator as is done in the DAPR dataset card example on HF:\n\nqrels = {}\nfor qrel_row in qrels_rows:\n    qid = qrel_row[\"query_id\"]\n    pid = qrel_row[\"corpus_id\"]\n    rel = qrel_row[\"score\"]\n    qrels.setdefault(qid, {})\n    qrels[qid][pid] = rel\n\ndev-5 is a query ID with multiple passages so I’ve chosen it as the test example:\n\nqid = 'dev-5'\n\n\nqrels[qid]\n\n{'61-1': 1, '61-4': 1, '61-5': 1, '61-17': 1, '61-37': 1, '61-39': 1}\n\n\n\npytrec_results = {}\npytrec_results\n\n{}\n\n\nNext we’ll run retrieval and structure results for the pytrec evaluator, again copying the DAPR example which structures the retrieval results as:\nretrieval_scores[query_id][passage_id] = score\nNote again that document_id means passage_id.\n\nfor q in queries:\n    results = RAG.search(q['text'], k=10)\n    pytrec_results[q['_id']] = {result['document_id']: float(result['score']) for result in results}\n\nWe can see the 10 passages and each one has a corresponding score.\n\npytrec_results[qid]\n\n{'61-1': 71.125,\n '423-16': 70.5625,\n '61-27': 70.4375,\n '61-109': 70.375,\n '61-110': 70.25,\n '61-113': 70.25,\n '61-114': 70.25,\n '426-22': 70.1875,\n '420-42': 70.1875,\n '423-7': 70.125}\n\n\nCalculate Recall for all queries and viewing a single query’s Recall:\n\nevaluator = pytrec_eval.RelevanceEvaluator(qrels, {'recall.10'})\n\nThere are 271 queries and 271 metrics (one per query):\n\nmetrics = evaluator.evaluate(pytrec_results)\nlen(metrics)\n\n271\n\n\nFor our dev-5 query the Recall@10 is 0.167 or 1/6.\n\nmetrics[qid]\n\n{'recall_10': 0.16666666666666666}\n\n\nHere are the 6 passages that we needed to retrieve to fully answer this question:\n\nqrels[qid]\n\n{'61-1': 1, '61-4': 1, '61-5': 1, '61-17': 1, '61-37': 1, '61-39': 1}\n\n\nAnd here are the results again—only 1 relevant passage, 61-1, was retrieved.\n\npytrec_results[qid]\n\n{'61-1': 71.125,\n '423-16': 70.5625,\n '61-27': 70.4375,\n '61-109': 70.375,\n '61-110': 70.25,\n '61-113': 70.25,\n '61-114': 70.25,\n '426-22': 70.1875,\n '420-42': 70.1875,\n '423-7': 70.125}\n\n\nCalculating mean Recall across all queries to get our mean Recall@10 for the entire collection of queries:\n\nmean_recall = sum(metrics[qid]['recall_10'] for qid in metrics.keys()) / len(metrics)\nmean_recall\n\n0.28046940381859803\n\n\nSo, about 28% of all queries’ relevant passages were present in the top-10 passages retrieved.\nI wanted to confirm my calculation so I’ll also calculate Recall@10 using the ranx library.\n\nqrels_ranx = Qrels(qrels)\nranx_results = Run(pytrec_results)\n\n\nevaluate(qrels_ranx, ranx_results, \"recall@10\")\n\n0.2804694038185978\n\n\nAnd we get the same results. Great!"
  },
  {
    "objectID": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#final-thoughts",
    "href": "posts/2025-02-08-ConditionalQA-RAGatouille/index.html#final-thoughts",
    "title": "Evaluating the DAPR ConditionalQA Dataset with RAGatouille",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIn a future blog post I’ll calculate Recall@10 (and other metrics) on all of the datasets included in DAPR:\n\nConditionalQA\nMS MARCO\nGenomics\nMIRACL\nNatural Questions\n\nOnce that’s done, I’ll pick a few different retrieval models and compare their results across these datasets.\nI think by the end of these experiments I’ll have a better grasp on how to work with classic IR datasets and metrics."
  },
  {
    "objectID": "posts/2025-02-12-indexing-memory/index.html",
    "href": "posts/2025-02-12-indexing-memory/index.html",
    "title": "Estimating Storage and CPU RAM Requirements for Indexing 12.6M Documents",
    "section": "",
    "text": "After a few days of flailing about trying to index the 12.6M document Genomics dataset (from UKPLab/DAPR) in Google Colab Pro using RAGatouille, I decided to plan the attempt in a more organized way. In this blog post I’ll share my findings and next actions.\nHere’s an example text from the corpus:\nThe 33D1 rat MoAb92  identifies a low-density Ag on mouse (marginal zone) spleen DC. The antibody does not stain DC in cryostat sections and does not react with LC. No biochemical data on the Ag are available. Nonetheless, this antibody has proved extremely useful for C lysis of mouse spleen DC.\\r\\n\nThe average length of text in this corpus is ~540 characters."
  },
  {
    "objectID": "posts/2025-02-12-indexing-memory/index.html#rag.index",
    "href": "posts/2025-02-12-indexing-memory/index.html#rag.index",
    "title": "Estimating Storage and CPU RAM Requirements for Indexing 12.6M Documents",
    "section": "RAG.index",
    "text": "RAG.index\nThe main function of interest if RAG.index which takes a list of documents and indexes them in preparation for retrieval.\nindex_path = RAG.index(\n            index_name=f\"{dataset_name}_index\",\n            collection=passages[:ndocs][\"text\"],\n            document_ids=passages[:ndocs][\"_id\"]\n        )\nI used the following code to log the RAM memory usage, with ndocs being defined globally:\ndef memory_monitor(stop_event, readings):\n    while not stop_event.is_set():\n        mem = psutil.Process().memory_info().rss / 1024 / 1024 / 1024\n        readings.append((datetime.now(), mem))\n        time.sleep(5)\n\ndef log_memory_during_index():\n    stop_event = threading.Event()\n    readings = []\n    monitor_thread = threading.Thread(target=memory_monitor, args=(stop_event, readings))\n    monitor_thread.start()\n    \n    try:\n        index_path = RAG.index(\n            index_name=f\"{dataset_name}_index\",\n            collection=passages[:ndocs][\"text\"],\n            document_ids=passages[:ndocs][\"_id\"]\n        )\n    finally:\n        stop_event.set()\n        monitor_thread.join()\n    \n    return index_path, readings\n\nindex_path, memory_readings = log_memory_during_index()"
  },
  {
    "objectID": "posts/2025-02-12-indexing-memory/index.html#memory-logging-results",
    "href": "posts/2025-02-12-indexing-memory/index.html#memory-logging-results",
    "title": "Estimating Storage and CPU RAM Requirements for Indexing 12.6M Documents",
    "section": "Memory Logging Results",
    "text": "Memory Logging Results\nI used two machines for these experiments:\n\nT4 GPU (16 GB vRAM, 51GB RAM) using Google Colab Pro.\nRTX6000Ada (48GB vRAM, 128GB RAM) using Jarvis Labs.\n\nI chose the following number of documents to index: - 100k - 250k - 500k - 1M - 2M\nHere are the results:\nRTX6000Ada (48GB vRAM, 128GB RAM)\n\n\n\n# Docs\nindex_path Size\nMax RAM\nTime\n\n\n\n\n100k\n0.41 GB\n6.96 GB\n4 min\n\n\n250k\n1.1 GB\n8.4 GB\n6.4 min\n\n\n500k\n2.2 GB\n11.4 GB\n12 min\n\n\n1M\n4.5 GB\n16.3 GB\n24 min\n\n\n2M\n9.1 GB\n24 GB\n47 min\n\n\n\nT4 w/High-RAM (16GB vRAM, 51GB RAM)\n\n\n\n# Docs\nindex_path Size\nMax RAM\nTime\n\n\n\n\n100k\n0.41 GB\n6.5 GB\n8 min\n\n\n250k\n1.1 GB\n8.8 GB\n20 min\n\n\n500k\n2.2 GB\n11.8 GB\n36 min\n\n\n1M\n4.5 GB\n18.8 GB\n78 min\n\n\n2M\n9.1 GB\n28.6 GB\n145 min\n\n\n\nI also used the A100 instance on Google Colab Pro for some initial experiments. It’s interesting to note the difference in speed of encoding 25k passages:\n\n\n\nGPU\nseconds/25k\n\n\n\n\nRTX6000Ada\n12\n\n\nA100\n22\n\n\nT4\n44"
  },
  {
    "objectID": "posts/2025-02-12-indexing-memory/index.html#extrapolating-to-12.6m-documents",
    "href": "posts/2025-02-12-indexing-memory/index.html#extrapolating-to-12.6m-documents",
    "title": "Estimating Storage and CPU RAM Requirements for Indexing 12.6M Documents",
    "section": "Extrapolating to 12.6M Documents",
    "text": "Extrapolating to 12.6M Documents\nI’ll start with the easier one: the size of the directory created by RAG.index. Doubling the number of documents doubles its size (approximately) so if 1M documents takes up 4.5GB of space I expect 12.6M documents to take up ~54GB of space. I’ll set my storage size to 100GB just in case.\nThe maximum RAM used (by the CPU, not the GPU vRAM) for 12.6M documents is a bit more involved. I’m planning to use the RTX6000Ada machine so I’ll use its numbers.\nRTX6000Ada (48GB vRAM, 128GB RAM)\n\n\n\n# Docs\nMax RAM\nIncrease\n\n\n\n\n100k\n6.96 GB\n–\n\n\n250k\n8.4 GB\n20%\n\n\n500k\n11.4 GB\n36%\n\n\n1M\n16.3 GB\n43%\n\n\n2M\n24 GB\n47%\n\n\n\nThe percent increase amount is slowing down. Let’s say it plateaus at a 50% increase going from 2M to 4M documents (doubling). 2M to 12.6M is ~2.66 doublings (is that a word?). 24 GB x 1.5^2.66 = 70GB. If I was using Colab numbers: 28.6 x 1.5^2.66 = 84 GB. When I tried to index 12.6M documents with an A100 High-RAM (83.5 GB CPU) instance on Google Colab Pro, the runtime crashed as it ran out of System RAM so this checks out.\nFinally, let’s say the time it takes to index documents doubles when the number of documents doubles from 2M onwards. 47 min x 2^2.66 = 300 minutes or 5 hours. At about $1/hr, this would take $5 on an RTX6000Ada.\nI should note that in all my experiments, the GPU vRAM usage didn’t go past 3-4 GB.\nWhile the peak CPU RAM usage varied, in all instances the plots looked like the following (2M documents on RTX6000Ada):\n\n\n\nSystem RAM Usage over Indexing Time\n\n\nI couldn’t figure out from my profiler the exact function call during that largest spike. Also note the spike near the end before indexing is finished."
  },
  {
    "objectID": "posts/2025-02-12-indexing-memory/index.html#final-thoughts",
    "href": "posts/2025-02-12-indexing-memory/index.html#final-thoughts",
    "title": "Estimating Storage and CPU RAM Requirements for Indexing 12.6M Documents",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nTime will tell if these calculations are worth anything, but it seems like my best option is to use Jarvis Labs’ RTX6000Ada machine with 128GB CPU RAM. Once I successfully index the 12.6M-document Genomics dataset, I’ll have a better estimate for how much it will cost to index the largest dataset in the DAPR collection: MIRACL (32.9M documents). Stay tuned!"
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "",
    "text": "A disclaimer: this is the first time I’ve done memory profiling, and while I’ve probably spent 8-10 hours poring through the RAGatouille and ColBERT codebases I still consider myself a beginner, and don’t have a solid mental model of how indexing (and search) work.\nWith that out of the way, let’s dig in!\nIn a previous blog post I used psutil.Process().memory_info().rss in a separate thread to monitor memory usage while indexing 100k, 250k, 500k, 1M and 2M documents from the Genomics datasets (via UKPLab/DAPR) with RAGatouille. I have also run this for raw ColBERT. Here’s an example comparison (for 250k docs on an RTX6000Ada instance) with RAGatouille on the left and raw ColBERT on the right:\n\n\n\nCPU memory usage while indexing 250k documents\n\n\nWhile the peak memory increased with number of documents, they all follow the same trend. ColBERT always has a significantly lower peak memory. The ColBERT runs in total took about an hour and the RAGatouille runs took about 1.5 hours. Comparison of all collection sizes can be seen in this folder.\nIn this blog post I go deeper and use the memory_profiler package to understand how much memory is being consumed by different functions down the chain of calls when you index 100k, 250k, 500k, 1M and 2M documents using raw ColBERT and RAGatouille. For all of these runs I use a RTX6000Ada instance on Jarvis Labs. When using RAGatouille, I execute all runs with use_faiss=False (since that’s the default value in RAGatouille) and runs of 100k, 250k and 500k with use_faiss=True."
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#repo-setup-and-installation",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#repo-setup-and-installation",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "Repo Setup and Installation",
    "text": "Repo Setup and Installation\nSince I needed to add the @profile decorator above each function I wanted to profile, I created my own forks of the raw ColBERT and RAGatouille repos and created a profiling branch. Since RAGatouille is built on top of ColBERT, I switched the colbert-ai dependency in my RAGatouille fork from \"colbert-ai>=0.2.19\" to:\n\"colbert-ai @ git+https://github.com/vishalbakshi/ColBERT.git@profiling\"\nI also added memory-profiler as a dependency for both ColBERT and RAGatouille.\nI used the terminal for all experiments. Here are the commands to install RAGatouille:\npython -m venv ragatouille-env\nsource ragatouille-env/bin/activate\ngit clone -b profiling https://github.com/vishalbakshi/RAGatouille.git\ncd RAGatouille\npip install -e .\npip install datasets\npip uninstall --y faiss-cpu\npip install faiss-gpu-cu12\nNote that I uninstalled faiss-cpu and installed faiss-gpu-cu12.\nHere are the commands to install ColBERT (which took considerably more effort, and assistance from Claude, to figure out):\ngit clone -b profiling https://github.com/vishalbakshi/ColBERT.git\ncd ColBERT\nconda env create -f conda_env.yml\nconda init\nsource ~/.bashrc\nconda activate colbert\npip install -e .\nconda remove -y --force pytorch torchvision torchaudio cudatoolkit\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\napt-get update\napt-get install -y gcc-11 g++-11\nexport CC=gcc-11\nexport CXX=g++-11\nI had to uninstall pytorch, torchvision, torchaudio, cudatoolkit and reinstall them to resolve the following error:\nFile \"/home/ColBERT/colbert/utils/utils.py\", line 3, in <module>\n    import torch\n  File \"/root/miniconda3/envs/colbert/lib/python3.8/site-packages/torch/__init__.py\", line 218, in <module>\n    from torch._C import *  # noqa: F403\nImportError: /root/miniconda3/envs/colbert/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent\nThe last four commands I ran:\napt-get update\napt-get install -y gcc-11 g++-11\nexport CC=gcc-11\nexport CXX=g++-11\nResolved fatal error: crypt.h: No such file or directory/ninja: build stopped: subcommand failed as is detailed in ColBERT issue #371."
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#functions-selected-for-profiling",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#functions-selected-for-profiling",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "Functions Selected for Profiling",
    "text": "Functions Selected for Profiling\nI determined which functions to profile by trial and error, adding/removing the @profile decorator to see which function was being called. Again, lots of Claude assistance was needed. Here are the filenames and method names that I chose to profile:\n\nColBERT\n\n\n\nFilename\nMethod\n\n\n\n\nindexer.py\nindex\n\n\nindexer.py\n__launch\n\n\nindexing/collection_indexer.py\nencode\n\n\nindexing/collection_indexer.py\nrun\n\n\nindexing/collection_indexer.py\nsetup\n\n\nindexing/collection_indexer.py\n__sample_pids\n\n\nindexing/collection_indexer.py\n__sample_embeddings\n\n\nindexing/collection_indexer.py\nencoder.encode_passages\n\n\ninfra/launcher.py\nlaunch\n\n\ninfra/launcher.py\nlaunch_without_fork\n\n\ninfra/launcher.py\nrun_process_without_mp\n\n\ninfra/launcher.py\ncallee\n\n\n\n\n\nRAGatouille\n\n\n\nFilename\nMethod\n\n\n\n\nRAGPretrainedModel.py\n_process_corpus\n\n\nRAGPretrainedModel.py\nmodel.index\n\n\nmodels/colbert.py\nModelIndexFactory.construct\n\n\nmodels/index.py\nPLAIDModelIndex.__init__\n\n\nmodels/index.py\nPLAIDModelIndex.construct\n\n\nmodels/index.py\nPLAIDModelIndex.build\n\n\nmodels/index.py\nPLAIDModelIndex.indexer.index\n\n\n\nNote that in RAGatouille, PLAIDModelIndex.indexer is of class Indexer which is imported from ColBERT, so I understood this to be the “bridge” between the RAGatouille and ColBERT repos during profiling."
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#scripts",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#scripts",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "Scripts",
    "text": "Scripts\nHere’s the script for indexing using ColBERT:\nimport colbert\nfrom colbert import Indexer, Searcher\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\nfrom colbert.data import Queries, Collection\nfrom datasets import load_dataset\nfrom memory_profiler import profile\n\n@profile\ndef _index(indexer, name, collection):\n    return indexer.index(name=name, collection=collection, overwrite=True)\n\ndef main():\n    nbits = 2  \n    ndocs = 100_000\n    dataset_name = \"Genomics\"\n    index_name = f'{dataset_name}.{nbits}bits'\n\n    passages = load_dataset(\"UKPLab/dapr\", f\"{dataset_name}-corpus\", split=\"test\")\n    checkpoint = 'answerdotai/answerai-colbert-small-v1'\n\n    with Run().context(RunConfig(nranks=1, experiment='notebook')):\n        config = ColBERTConfig(doc_maxlen=256, nbits=nbits, kmeans_niters=4, avoid_fork_if_possible=True)\n        indexer = Indexer(checkpoint=checkpoint, config=config)\n        _index(indexer, index_name, passages[:ndocs][\"text\"])\n\nif __name__ == '__main__':\n    main()\nand the script for RAGatouille:\nfrom memory_profiler import profile\nfrom datasets import load_dataset\nfrom ragatouille import RAGPretrainedModel\n\ndataset_name = \"Genomics\"\npassages = load_dataset(\"UKPLab/dapr\", f\"{dataset_name}-corpus\", split=\"test\")\nRAG = RAGPretrainedModel.from_pretrained(\"answerdotai/answerai-colbert-small-v1\")\nndocs=250_000\n\n@profile\ndef _index():\n    return RAG.index(\n        index_name=f\"{dataset_name}_index\",\n        collection=passages[:ndocs][\"text\"],\n        document_ids=passages[:ndocs][\"_id\"],\n        use_faiss=True # or False\n    )\n\n_index()\nFinally, here’s the terminal command to run the scripts and profile them:\npython -m memory_profiler ../colbert_index_2M.py > ../colbert_2M_RTX6000Ada.txt"
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#profiling-results",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#profiling-results",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "Profiling Results",
    "text": "Profiling Results\nThe profile logs were 400+ lines each (you can see the full files here) so I have only included some of the lines with non-zero memory changes. I have showed the starting memory, memory increment and final memory.\nHere’s how I’m interpreting the profiler logs–given this log:\nFilename: /home/RAGatouille/ragatouille/models/index.py\n\nLine #    Mem usage    Increment  Occurrences   Line Contents\n=============================================================\n   198   3406.4 MiB   3406.4 MiB           1           @profile\n   199                                                 def _index_with_profiling(indexer, name, collection, overwrite):\n   200   4872.2 MiB   1465.8 MiB           1               return indexer.index(name=name, collection=collection, overwrite=overwrite)\nI would interpret that to mean that before indexer.index was called, 3406.4 MB memory was used, and the indexer.index call increased it by 1465.8 MB to 4872.2 MB.\n\ncolbert/indexer.py/indexer.index\nFor RAGatouille, this call takes place in ragatouille/models/index.py.\nIt’s interesting to note that even before indexer.index is called, the starting memory varies between raw ColBERT and RAGatouille. Most notably, for 2M documents, ColBERT starts at ~4GB while RAGatouille starts at ~8 GB.\nEven more interesting, the memory increments for ColBERT are 2x to 35x smaller than RAGatouille for each collection size.\n\n\n\n\n\n\n\n\n\n\nIndexing Method\nDocument Size\nStarting Memory\nMemory Increment\nFinal Memory\n\n\n\n\nColBERT\n100k\n1596.9 MB\n36.7 MB\n1633.6 MB\n\n\nColBERT\n250k\n1754.0 MB\n92.8 MB\n1846.8 MB\n\n\nColBERT\n500k\n2072.1 MB\n199.1 MB\n2271.2 MB\n\n\nColBERT\n1M\n2707.3 MB\n421.9 MB\n3129.2 MB\n\n\nColBERT\n2M\n4000.6 MB\n876.4 MB\n4877.1 MB\n\n\nRAGatouille (use_faiss=True)\n100k\n2114.2 MB\n1320.1 MB\n3434.3 MB\n\n\nRAGatouille (True)\n250k\n2592.5 MB\n1175.0 MB\n3767.5 MB\n\n\nRAGatouille (True)\n500k\n3405.0 MB\n1430.0 MB\n4835.0 MB\n\n\nRAGatouille (use_faiss=False)\n100k\n1750.9 MB\n1203.9 MB\n2954.8 MB\n\n\nRAGatouille (False)\n250k\n2597.4 MB\n1341.4 MB\n3938.8 MB\n\n\nRAGatouille (False)\n500k\n3406.4 MB\n1465.8 MB\n4872.2 MB\n\n\nRAGatouille (False)\n1M\n5040.1 MB\n1593.3 MB\n6633.3 MB\n\n\nRAGatouille (False)\n2M\n8354.7 MB\n1882.0 MB\n10236.8 MB\n\n\n\n\n\ncolbert/indexing/collection_indexer.py/encoder.encode_passages\nencoder.encode_passages involves the following code:\n\ndef encode_passages(self, passages):\n        Run().print(f\"#> Encoding {len(passages)} passages..\")\n\n        if len(passages) == 0:\n            return None, None\n\n        with torch.inference_mode():\n            embs, doclens = [], []\n\n            for passages_batch in batch(passages, self.config.index_bsize * 50):\n                embs_, doclens_ = self.checkpoint.docFromText(\n                    passages_batch,\n                    bsize=self.config.index_bsize,\n                    keep_dims=\"flatten\",\n                    showprogress=(not self.use_gpu),\n                    pool_factor=self.config.pool_factor,\n                    clustering_mode=self.config.clustering_mode,\n                    protected_tokens=self.config.protected_tokens,\n                )\n                embs.append(embs_)\n                doclens.extend(doclens_)\n\n            embs = torch.cat(embs)\n\n        return embs, doclens\nIIUC, this is calling docFromText on the ColBERT model (answerai-colbert-small-v1 in our case). I would expect raw ColBERT and RAGatouille to experience equal memory change during this method call but RAGatouille uses 10-15% more memory for each dataset size.\n\n\n\n\n\n\n\n\n\n\nIndexing Method\nDocument Size\nInitial Memory\nMemory Change\nFinal Memory\n\n\n\n\nColBERT\n100k\n732.9 MB\n1502.4 MB\n2235.3 MB\n\n\nColBERT\n250k\n829.7 MB\n1991.1 MB\n2820.8 MB\n\n\nColBERT\n500k\n1000.2 MB\n2549.8 MB\n3550.0 MB\n\n\nColBERT\n1M\n1351.6 MB\n3462.0 MB\n4813.6 MB\n\n\nColBERT\n2M\n1997.3 MB\n4692.3 MB\n6689.6 MB\n\n\nRAGatouille (use_faiss=True)\n100k\n2115.0 MB\n1677.3 MB\n3792.3 MB\n\n\nRAGatouille (True)\n250k\n2593.5 MB\n2279.7 MB\n4873.2 MB\n\n\nRAGatouille (True)\n500k\n3405.1 MB\n3004.6 MB\n6409.6 MB\n\n\nRAGatouille (use_faiss=False)\n100k\n1751.0 MB\n1685.6 MB\n3436.6 MB\n\n\nRAGatouille (False)\n250k\n2597.9 MB\n2270.4 MB\n4868.3 MB\n\n\nRAGatouille (False)\n500k\n3406.4 MB\n3003.8 MB\n6410.2 MB\n\n\nRAGatouille (False)\n1M\n5040.7 MB\n3915.3 MB\n8956.0 MB\n\n\nRAGatouille (False)\n2M\n8355.1 MB\n5349.5 MB\n13704.6 MB\n\n\n\n\n\ncolbert/indexing/collection_indexer.py/_sample_embeddings\nencode_passages is called from inside _sample_embeddings. For ColBERT, _sample_embeddings has different starting/final memory values than _encode_passages while for RAGatouille they are the same.\nFor example, for 100k documents using raw ColBERT, _sample_embeddings increases memory by 797 MB while for encoder.encode_passages the memory increases by 1488.8MB.\nFor 100k using RAGatouille, both memory increases the same (1677.3 MB for use_faiss=True and 1685.6 MB for use_faiss=False). I’m not sure what this means so I asked Claude and got the response:\n\nThis discrepancy reveals memory reuse patterns between function calls. In ColBERT, the 1488.8 MB used by encode_passages is partially freed before returning to _sample_embeddings, resulting in a net increase of 797 MB. In RAGatouille, the memory appears to be retained between calls, showing the same 1677.3 MB increase at both levels.\n\n\n\n\n\n\n\n\n\n\n\nIndexing Method\nDocument Size\nInitial Memory\nMemory Change\nFinal Memory\n\n\n\n\nColBERT\n100k\n732.9 MB\n813.8 MB\n1546.7 MB\n\n\nColBERT\n250k\n829.7 MB\n809.0 MB\n1638.7 MB\n\n\nColBERT\n500k\n1000.2 MB\n770.1 MB\n1770.3 MB\n\n\nColBERT\n1M\n1351.6 MB\n813.3 MB\n2164.9 MB\n\n\nColBERT\n2M\n1997.3 MB\n782.4 MB\n2779.7 MB\n\n\nRAGatouille (use_faiss=True)\n100k\n2115.0 MB\n1677.3 MB\n3792.3 MB\n\n\nRAGatouille (True)\n250k\n2593.5 MB\n2279.7 MB\n4873.2 MB\n\n\nRAGatouille (True)\n500k\n3405.1 MB\n3004.6 MB\n6409.6 MB\n\n\nRAGatouille (use_faiss=False)\n100k\n1751.0 MB\n1685.6 MB\n3436.6 MB\n\n\nRAGatouille (False)\n250k\n2597.9 MB\n2270.4 MB\n4868.3 MB\n\n\nRAGatouille (False)\n500k\n3406.4 MB\n3003.8 MB\n6410.2 MB\n\n\nRAGatouille (False)\n1M\n5040.7 MB\n3915.3 MB\n8956.0 MB\n\n\nRAGatouille (False)\n2M\n8355.1 MB\n5349.5 MB\n13704.6 MB\n\n\n\n\n\ncolbert/indexing/collection_indexer.py/setup\nA similar pattern for setup, within which _sample_embeddings is called. Raw ColBERT seems more efficient in releasing memory while RAGatouille retains it.\n\n\n\n\n\n\n\n\n\n\nIndexing Method\nDocument Size\nInitial Memory\nMemory Change\nFinal Memory\n\n\n\n\nColBERT\n100k\n727.9 MB\n817.5 MB\n1545.5 MB\n\n\nColBERT\n250k\n815.7 MB\n816.4 MB\n1632.1 MB\n\n\nColBERT\n500k\n978.2 MB\n787.9 MB\n1766.1 MB\n\n\nColBERT\n1M\n1305.6 MB\n840.2 MB\n2145.8 MB\n\n\nColBERT\n2M\n1966.3 MB\n822.2 MB\n2788.5 MB\n\n\nRAGatouille (use_faiss=True)\n100k\n3434.3 MB\n1677.3 MB\n3792.3 MB\n\n\nRAGatouille (True)\n250k\n3767.5 MB\n2279.7 MB\n4873.2 MB\n\n\nRAGatouille (True)\n500k\n4835.0 MB\n3004.6 MB\n6409.6 MB\n\n\nRAGatouille (use_faiss=False)\n100k\n2954.8 MB\n1685.6 MB\n3436.6 MB\n\n\nRAGatouille (False)\n250k\n3938.8 MB\n2270.4 MB\n4868.3 MB\n\n\nRAGatouille (False)\n500k\n4872.2 MB\n3003.8 MB\n6410.2 MB\n\n\nRAGatouille (False)\n1M\n6633.3 MB\n3915.3 MB\n8956.0 MB\n\n\nRAGatouille (False)\n2M\n10236.8 MB\n5349.5 MB\n13704.6 MB\n\n\n\n\n\ncolbert/indexing/collection_indexer.py/train\nIIUC, this function call finds centroids based on a sample of document token embeddings. Interesting to note that the memory change for raw ColBERT is smallest for 1M documents (87.2 MB) and for RAGatouille, 2M docs is the smallest (23.4 MB). For most collection sizes, RAGatouille uses 40-50% more memory for this operation.\n\n\n\n\n\n\n\n\n\n\nIndexing Method\nDocument Size\nInitial Memory\nMemory Change\nFinal Memory\n\n\n\n\nColBERT\n100k\n1545.5 MB\n115.8 MB\n1661.3 MB\n\n\nColBERT\n250k\n1632.1 MB\n128.8 MB\n1760.9 MB\n\n\nColBERT\n500k\n1766.1 MB\n124.3 MB\n1890.4 MB\n\n\nColBERT\n1M\n2145.8 MB\n87.2 MB\n2233.0 MB\n\n\nColBERT\n2M\n2788.5 MB\n133.5 MB\n2921.9 MB\n\n\nRAGatouille (use_faiss=True)\n100k\n3792.3 MB\n179.6 MB\n3971.9 MB\n\n\nRAGatouille (True)\n250k\n4873.2 MB\n182.7 MB\n5055.9 MB\n\n\nRAGatouille (True)\n500k\n6409.6 MB\n174.1 MB\n6583.8 MB\n\n\nRAGatouille (use_faiss=False)\n100k\n3436.6 MB\n175.9 MB\n3612.6 MB\n\n\nRAGatouille (False)\n250k\n4868.3 MB\n181.5 MB\n5049.8 MB\n\n\nRAGatouille (False)\n500k\n6410.2 MB\n179.2 MB\n6589.4 MB\n\n\nRAGatouille (False)\n1M\n8956.0 MB\n191.5 MB\n9147.5 MB\n\n\nRAGatouille (False)\n2M\n13704.6 MB\n23.4 MB\n13728.1 MB\n\n\n\n\n\ncolbert/indexing/collection_indexer.py/index\nThis is one of the more interesting results—raw ColBERT has a positive memory change during this operation (which IIUC is the indexing of all document token embeddings) while all RAGatouille index() operations actually reduce the memory usage. Not sure what that means. The final memory for raw ColBERT is less than RAGatouille.\n\n\n\n\n\n\n\n\n\n\nIndexing Method\nDocument Size\nInitial Memory\nMemory Change\nFinal Memory\n\n\n\n\nColBERT\n100k\n1661.3 MB\n287.0 MB\n1948.3 MB\n\n\nColBERT\n250k\n1760.9 MB\n263.5 MB\n2024.4 MB\n\n\nColBERT\n500k\n1890.4 MB\n371.9 MB\n2262.2 MB\n\n\nColBERT\n1M\n2233.0 MB\n599.9 MB\n2832.9 MB\n\n\nColBERT\n2M\n2921.9 MB\n958.0 MB\n3880.0 MB\n\n\nRAGatouille (use_faiss=True)\n100k\n3971.9 MB\n-536.3 MB\n3435.6 MB\n\n\nRAGatouille (True)\n250k\n5055.9 MB\n-1375.8 MB\n3680.1 MB\n\n\nRAGatouille (True)\n500k\n6583.8 MB\n-1936.3 MB\n4647.5 MB\n\n\nRAGatouille (use_faiss=False)\n100k\n3612.6 MB\n-652.4 MB\n2960.2 MB\n\n\nRAGatouille (False)\n250k\n5049.8 MB\n-1112.5 MB\n3937.3 MB\n\n\nRAGatouille (False)\n500k\n6589.4 MB\n-1906.8 MB\n4682.6 MB\n\n\nRAGatouille (False)\n1M\n9147.5 MB\n-2917.3 MB\n6230.1 MB\n\n\nRAGatouille (False)\n2M\n13728.1 MB\n-4910.2 MB\n8817.9 MB\n\n\n\n\n\ncolbert/indexing/collection_indexer.py/finalize\nThis function maps passage IDs to centroid IDs—one of the efficiencies of the PLAID indexing approach. Within each approach (raw ColBERT and RAGatouille) the memory change varies drastically between less than 0 and up to ~500MB.\n\n\n\n\n\n\n\n\n\n\nIndexing Method\nDocument Size\nInitial Memory\nMemory Change\nFinal Memory\n\n\n\n\nColBERT\n100k\n1948.3 MB\n35.1 MB\n1983.3 MB\n\n\nColBERT\n250k\n2024.4 MB\n-0.4 MB\n2024.0 MB\n\n\nColBERT\n500k\n2262.2 MB\n59.2 MB\n2321.5 MB\n\n\nColBERT\n1M\n2832.9 MB\n201.5 MB\n3034.4 MB\n\n\nColBERT\n2M\n3880.0 MB\n490.2 MB\n4370.2 MB\n\n\nRAGatouille (use_faiss=True)\n100k\n3435.6 MB\n-1.3 MB\n3434.3 MB\n\n\nRAGatouille (True)\n250k\n3680.1 MB\n87.4 MB\n3767.5 MB\n\n\nRAGatouille (True)\n500k\n4647.5 MB\n187.5 MB\n4835.0 MB\n\n\nRAGatouille (use_faiss=False)\n100k\n2960.2 MB\n-5.3 MB\n2954.8 MB\n\n\nRAGatouille (False)\n250k\n3937.3 MB\n1.5 MB\n3938.8 MB\n\n\nRAGatouille (False)\n500k\n4682.6 MB\n189.6 MB\n4872.2 MB\n\n\nRAGatouille (False)\n1M\n6230.1 MB\n403.2 MB\n6633.3 MB\n\n\nRAGatouille (False)\n2M\n8817.9 MB\n1418.9 MB\n10236.8 MB"
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#indexing-time",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#indexing-time",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "Indexing Time",
    "text": "Indexing Time\nI didn’t measure runtime for each run, but some observations:\n\nDuring passage encoding (25k passages per iteration) ColBERT took about 20 seconds/it and RAGatouille took about 110 seconds/it. Note that without profiling ColBERT took about 9/seconds/it and RAGatouille 12 seconds/it.\nColBERT encoding lasted 4, 10, 20, 40 and 80 iterations for 100k, 250k, 500k, 1M and 2M docs. RAGatouille always overshot it (e.g. 14 iters for 250k docs or 22 iters for 500k docs).\nOverall ColBERT profiling took ~2 hours while RAGatouille took ~16 hours.\nIt took a lot of time before the final encoding takes place, I think that’s because of the initial “planning” step that ColBERT and RAGatouille both do."
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#indexing-10k-documents-pytorch-vs-faiss-k-means-clustering",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#indexing-10k-documents-pytorch-vs-faiss-k-means-clustering",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "Indexing 10k Documents (PyTorch vs FAISS K-means Clustering)",
    "text": "Indexing 10k Documents (PyTorch vs FAISS K-means Clustering)\nWhile I was experimenting indexing scripts with 10k documents I noticed curious behavior. For 10k documents, with use_faiss=False, RAGatouille attempts to use PyTorch for K-means clustering. The memory usage for encoder.encode_passages during this attempt:\nLine #    Mem usage    Increment  Occurrences   Line Contents\n=============================================================\n   146   1849.2 MiB   1849.2 MiB           1           @profile\n   147                                                 def _encode_passages_profiled(*args, **kwargs):\n   148   2675.7 MiB    826.5 MiB           1               return self.encoder.encode_passages(*args, **kwargs)\nIt then runs into an OOM error:\nPyTorch-based indexing did not succeed with error: CUDA out of memory. Tried to allocate 27.55 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.88 GiB is free.\nAnd switches to FAISS K-means. The memory usage for encoder.encode_passages changes (note the drop from an increase of 826.5 MB to an increase of 373 MB, but an increase in initial memory usage from 1849.2 MB to 2652.6MB):\nLine #    Mem usage    Increment  Occurrences   Line Contents\n=============================================================\n   146   2652.6 MiB   2652.6 MiB           1           @profile\n   147                                                 def _encode_passages_profiled(*args, **kwargs):\n   148   3025.6 MiB    373.0 MiB           1               return self.encoder.encode_passages(*args, **kwargs)\nWhen I run the script with use_faiss=True, the encoder.encode_passages memory usage reflects the PyTorch attempt, whereas I would expect the memory increase to be 373 MB:\nLine #    Mem usage    Increment  Occurrences   Line Contents\n=============================================================\n   146   1853.4 MiB   1853.4 MiB           1           @profile\n   147                                                 def _encode_passages_profiled(*args, **kwargs):\n   148   2678.8 MiB    825.4 MiB           1               return self.encoder.encode_passages(*args, **kwargs)"
  },
  {
    "objectID": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#final-thoughts",
    "href": "posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/index.html#final-thoughts",
    "title": "Memory Profiling raw ColBERT and RAGatouille",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis exercise has left me with more questions than answers that I need to explore:\n\nIs this the best way to go about profiling memory?\nAm I interpreting the memory profiling results correctly?\nWhy does RAGatouille have a higher initial memory before indexing starts?\nWhy does RAGatouille retain more memory after indexing than ColBERT?\nWhy does RAGatouille memory usage drastically decrease during index()?\nWhy does RAGatouille max out CUDA memory for 10k documents? Related to Issue #247.\nWhy does RAGatouille’s memory usage when use_faiss=True match PyTorch K-means’ memory usage and not the FAISS K-means’ memory usage after PyTorch’s attempt fails with OOM?\n\nAdditionally, and probably relatedly, I still haven’t figured out what is causing the large memory spike in the diagram below:\n\n\n\nCPU memory usage while indexing 250k documents\n\n\nThe largest memory value profiled while indexing 250k docs using RAGatouille was 5 GB but the chart shows a spike up to ~8GB. Where’s the ghost 3GB?\nTBD."
  },
  {
    "objectID": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html",
    "href": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html",
    "title": "TIL: PeftModel Base Model Behavior",
    "section": "",
    "text": "In this TIL blog post I share some unexpected behavior when using PeftModel. In short, when merging LoRA adapter weights with the base model, the base model gets overwritten. While unexpected, in hindsight this makes sense if you want to minimize memory usage.\n\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nimport os\nimport torch\nimport psutil\nimport copy\nimport gc\n\n\nfrom google.colab import userdata\n\nos.environ['HUGGING_FACE_HUB_TOKEN'] = userdata.get('HUGGING_FACE_HUB_TOKEN')\n\n\ndef _mem(): print(f\"RAM Usage: {psutil.virtual_memory().percent}% (Used: {psutil.virtual_memory().used / (1024**3):.2f} GB / Total: {psutil.virtual_memory().total / (1024**3):.2f} GB)\")"
  },
  {
    "objectID": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#merging-lora-adapter-weights",
    "href": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#merging-lora-adapter-weights",
    "title": "TIL: PeftModel Base Model Behavior",
    "section": "Merging LoRA Adapter Weights",
    "text": "Merging LoRA Adapter Weights\nBefore loading any model, here is the memory usage. I’m using an A100 GPU with Colab Pro.\n\n_mem()\n\nRAM Usage: 3.5% (Used: 2.10 GB / Total: 83.48 GB)\n\n\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\").to(\"cpu\")\n\nAfter loading the base model (Llama2-7B) the memory usage increases to 27GB.\n\n_mem()\n\nRAM Usage: 33.8% (Used: 27.35 GB / Total: 83.48 GB)\n\n\nLoading the LoRA adapter weights increases the memory usage to 28 GB.\n\nmodel_to_merge = PeftModel.from_pretrained(\n    model=base_model,\n    model_id=\"LoRA-TMLR-2024/magicoder-lora-rank-64-alpha-128\"\n).to(\"cpu\")\n\n\n_mem()\n\nRAM Usage: 34.8% (Used: 28.22 GB / Total: 83.48 GB)\n\n\n\nmerged_model = model_to_merge.merge_and_unload()\n\nMerging the model essentially keeps the memory usage constant at 28GB.\n\n_mem()\n\nRAM Usage: 34.9% (Used: 28.28 GB / Total: 83.48 GB)"
  },
  {
    "objectID": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#comparing-base_model-and-merged_model-weights",
    "href": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#comparing-base_model-and-merged_model-weights",
    "title": "TIL: PeftModel Base Model Behavior",
    "section": "Comparing base_model and merged_model Weights",
    "text": "Comparing base_model and merged_model Weights\nHowever, saving memory comes at a cost! You no longer have access to the base model. I’ll first do a visual inspection of one of the weight matrices.\n\nbase_model.model.layers[0].self_attn.q_proj.weight\n\nParameter containing:\ntensor([[-0.0020, -0.0156,  0.0023,  ...,  0.0098, -0.0017, -0.0031],\n        [ 0.0283, -0.0176,  0.0062,  ..., -0.0076,  0.0004,  0.0087],\n        [-0.0230,  0.0225,  0.0001,  ...,  0.0028,  0.0190, -0.0063],\n        ...,\n        [ 0.0003,  0.0016, -0.0013,  ...,  0.0081, -0.0308,  0.0110],\n        [ 0.0259,  0.0203,  0.0045,  ..., -0.0310, -0.0147, -0.0111],\n        [-0.0077, -0.0174,  0.0012,  ...,  0.0182,  0.0181, -0.0070]])\n\n\n\nmerged_model.model.layers[0].self_attn.q_proj.weight\n\nParameter containing:\ntensor([[-0.0020, -0.0156,  0.0023,  ...,  0.0098, -0.0017, -0.0031],\n        [ 0.0283, -0.0176,  0.0062,  ..., -0.0076,  0.0004,  0.0087],\n        [-0.0230,  0.0225,  0.0001,  ...,  0.0028,  0.0190, -0.0063],\n        ...,\n        [ 0.0003,  0.0016, -0.0013,  ...,  0.0081, -0.0308,  0.0110],\n        [ 0.0259,  0.0203,  0.0045,  ..., -0.0310, -0.0147, -0.0111],\n        [-0.0077, -0.0174,  0.0012,  ...,  0.0182,  0.0181, -0.0070]])\n\n\nBoth matrices are equal. Analyzing weight matrix differences more systematically:\n\ndef _diffs(model1, model2):\n    n_diff = 0\n    for layer_idx in range(32):\n        for component in [\"q_proj\", \"k_proj\", \"o_proj\", \"v_proj\"]:\n            W1 = getattr(model1.model.layers[layer_idx].self_attn, component).weight\n            W2= getattr(model2.model.layers[layer_idx].self_attn, component).weight\n            if not torch.allclose(W1, W2, rtol=1e-5, atol=1e-8): n_diff += 1\n    print(f\"Different Self-Attention Matrices: {n_diff}\")\n    n_diff = 0\n    for layer_idx in range(32):\n        for component in [\"up_proj\", \"down_proj\", \"gate_proj\"]:\n            W1 = getattr(model1.model.layers[layer_idx].mlp, component).weight\n            W2 = getattr(model2.model.layers[layer_idx].mlp, component).weight\n            if not torch.allclose(W1, W2, rtol=1e-5, atol=1e-8): n_diff += 1\n    print(f\"Different MLP Weight Matrices: {n_diff}\")\n\n\n_diffs(base_model, merged_model)\n\nDifferent Self-Attention Matrices: 0\nDifferent MLP Weight Matrices: 0\n\n\nFor both self-attention and MLP modules, all weight matrices between the base_model and the merged_model are the same. Using the is operator we can see that they reference the same object in memory (which is where the memory savings come from):\n\nbase_model is merged_model\n\nTrue"
  },
  {
    "objectID": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#copying-the-base-model-for-comparison",
    "href": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#copying-the-base-model-for-comparison",
    "title": "TIL: PeftModel Base Model Behavior",
    "section": "Copying the Base Model for Comparison",
    "text": "Copying the Base Model for Comparison\nI’ll now load the base model again to compare with the merged model weights.\n\n_mem()\n\nRAM Usage: 35.4% (Used: 28.68 GB / Total: 83.48 GB)\n\n\n\ndel base_model\n\n\ngc.collect()\n\n483\n\n\n\n_mem()\n\nRAM Usage: 35.5% (Used: 28.78 GB / Total: 83.48 GB)\n\n\nNote that deleting the base model did not change the memory usage.\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\").to(\"cpu\")\n\n\n\n\n\n_mem()\n\nRAM Usage: 65.7% (Used: 53.94 GB / Total: 83.48 GB)\n\n\nWith a new base model loaded, the memory usage jumps up to 54 GB.\n\n_diffs(base_model, merged_model)\n\nDifferent Self-Attention Matrices: 128\nDifferent MLP Weight Matrices: 96\n\n\nThere are 32 layers in this Llama model, and each model’s self-attention module has 4 weight matrices we are comparing, resulting in 128 matrices in total. The MLP module has 3 weight matrices we are comparing, resulting in 96 total across the model. The base model and merged model are fully different models (in terms of weight matrix values)."
  },
  {
    "objectID": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#using-.get_base_model",
    "href": "posts/2025-03-10-TIL-PeftModel-Behavior/index.html#using-.get_base_model",
    "title": "TIL: PeftModel Base Model Behavior",
    "section": "Using .get_base_model",
    "text": "Using .get_base_model\nLooking at the PeftModel documentation, I noted the method get_base_model which seems relevant to this exercise. However, using that method results in the same weights as the merged model:\n\nmodel_to_merge.get_base_model\n\n\n    peft.peft_model.PeftModel.get_base_modeldef get_base_model() -> torch.nn.Module/usr/local/lib/python3.11/dist-packages/peft/peft_model.pyReturns the base model.\n      \n      \n\n\n\n_diffs(merged_model, model_to_merge.get_base_model())\n\nDifferent Self-Attention Matrices: 0\nDifferent MLP Weight Matrices: 0\n\n\n\nI am planning to do more of these short TIL blog posts this year! It helps me solidify concepts as I come across them. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "",
    "text": "In this blog post I dive deeply into the internals of the RAGatouille and ColBERT libraries to understand the intermediate steps taken when building an index for a collection of documents.\n\nRAGatouille\n\nragatouille/RAGPretrainedModel.py\n\n_process_corpus\nindex\n\nragatouille/models/colbert.py\n\nindex\n\nragatouille/models/index.py\n\nModelIndexFactory.construct\nPLAIDModelIndex.construct\nPLAIDModelIndex.build\nindexer.index\n\n\nColBERT\n\ncolbert/indexer.py\n\nLauncher(encode)\n\ncolbert/indexing/collection_indexer.py\n\nencode\nCollection.cast\nCollectionIndexer.run\n\nCollectionIndexer.setup\n\nCollectionIndexer._sample_pids\nCollectionIndexer._sample_embeddings\n\ncolbert/indexing/collection_encoder.py: CollectionEncoder.encode_passages\nCheckpoint.docFromText\n\nColBERT.doc\n\n\nCollectionIndexer._save_plan\n\nCollectionIndexer.train\n\ncompute_faiss_kmeans\nCollectionIndexer._compute_avg_residual\n\nResidualCodec.compress_into_codes\nResidualCodec.lookup_centroids\n\nIndexSaver.save_codec\n\nCollectionIndexer.index\n\ncolbert/indexing/collection_encoder.py: CollectionEncoder.encode_passages\nIndexSaver.save_chunk\nResidualCodec.compress\n\nResidualCodec.binarize\n\n\nCollectionIndexer.finalize\n\nCollectionIndexer._build_ivf\noptimize_ivf\nCollectionIndexer._update_metadata\n\n\n\n\n\n\n!pip install datasets ragatouille -qq\n\n\nfrom datasets import load_dataset\nfrom ragatouille import RAGPretrainedModel\nfrom fastcore.utils import Path\nimport torch\nimport srsly\nimport uuid\nfrom ragatouille.data import CorpusProcessor\nfrom llama_index.core.text_splitter import SentenceSplitter\nimport pandas as pd\nfrom ragatouille.models.index import PLAIDModelIndex\nfrom colbert.infra import ColBERTConfig, RunConfig\nfrom colbert.data.collection import Collection\nfrom colbert.modeling.checkpoint import Checkpoint\nfrom colbert.indexing.collection_encoder import CollectionEncoder\nfrom colbert.indexing.collection_indexer import CollectionIndexer\nimport numpy as np\nimport random\nfrom colbert.indexing.collection_indexer import compute_faiss_kmeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom colbert.indexing.codecs.residual import ResidualCodec\nfrom colbert.utils.utils import flatten\nimport tqdm\n\n\ndef set_all_seeds(seed=123):\n    \"\"\"Set seeds for all random number generators\"\"\"\n    import random\n    import numpy as np\n    import torch\n    import os\n\n    # Python's random module\n    random.seed(seed)\n\n    # NumPy\n    np.random.seed(seed)\n\n    # PyTorch\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)  # For multi-GPU\n\n    # Set PYTHONHASHSEED for reproducibility across runs\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n    # Set deterministic algorithms for PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    print(f\"All seeds set to {seed}\")\n\n# Call this at the beginning of your script\nset_all_seeds(123)\n\nAll seeds set to 123"
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#ragatouille-rag.index",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#ragatouille-rag.index",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "RAGatouille RAG.index",
    "text": "RAGatouille RAG.index\nEverything in this notebook will be compared to what’s generated with RAG.index.\nFor this exercise, I’ll use 1000 passages from the UKPLab/DAPR ConditionalQA dataset.\n\npassages = load_dataset(\"UKPLab/dapr\", f\"ConditionalQA-corpus\", split=\"test[:1000]\")\npassages\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\nDataset({\n    features: ['_id', 'text', 'title', 'doc_id', 'paragraph_no', 'total_paragraphs', 'is_candidate'],\n    num_rows: 1000\n})\n\n\n\npassages[0]['text']\n\n'Overview'\n\n\n\nmodel_nm = \"answerdotai/answerai-colbert-small-v1\"\n\n\nRAG = RAGPretrainedModel.from_pretrained(model_nm)\nindex_path = RAG.index(index_name=\"cqa_index\", collection=passages['text'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n\n\n---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\nThis is a behaviour change from RAGatouille 0.8.0 onwards.\nThis works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\nIf you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n--------------------\n\n\n[Mar 13, 01:15:09] #> Creating directory .ragatouille/colbert/indexes/cqa_index \n\n\n[Mar 13, 01:15:11] [0]       #> Encoding 1000 passages..\n\n\n/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n\n\n[Mar 13, 01:15:14] [0]       avg_doclen_est = 15.197999954223633     len(local_sample) = 1,000\n[Mar 13, 01:15:14] [0]       Creating 1,024 partitions.\n[Mar 13, 01:15:14] [0]       *Estimated* 15,197 embeddings.\n[Mar 13, 01:15:14] [0]       #> Saving the indexing plan to .ragatouille/colbert/indexes/cqa_index/plan.json ..\n\n\n/usr/local/lib/python3.11/dist-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  sub_sample = torch.load(sub_sample_path)\n\n\nused 20 iterations (0.5189s) to cluster 14439 items into 1024 clusters\n[Mar 13, 01:15:14] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n\n\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\n\n\n[Mar 13, 01:16:51] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n\n\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\n\n\n[0.015, 0.016, 0.015, 0.015, 0.013, 0.016, 0.015, 0.016, 0.017, 0.014, 0.013, 0.015, 0.017, 0.014, 0.015, 0.017, 0.014, 0.015, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.014, 0.014, 0.015, 0.014, 0.015, 0.015, 0.014, 0.015, 0.016, 0.015, 0.014, 0.015, 0.015, 0.014, 0.014, 0.017, 0.016, 0.017, 0.014, 0.015, 0.016, 0.015, 0.016, 0.016, 0.012, 0.015, 0.016, 0.015, 0.016, 0.016, 0.015, 0.015, 0.016, 0.014, 0.015, 0.017, 0.016, 0.015, 0.014, 0.015, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.014, 0.015, 0.015, 0.014, 0.014, 0.014, 0.016, 0.015, 0.016, 0.015, 0.014, 0.014, 0.014, 0.015, 0.016, 0.014, 0.014, 0.016, 0.014, 0.015]\n\n\n/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  centroids = torch.load(centroids_path, map_location='cpu')\n/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  avg_residual = torch.load(avgresidual_path, map_location='cpu')\n/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')\n0it [00:00, ?it/s]\n\n\n[Mar 13, 01:18:16] [0]       #> Encoding 1000 passages..\n\n\n/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n1it [00:00,  1.39it/s]\n  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/indexing/codecs/residual_embeddings.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(codes_path, map_location='cpu')\n100%|██████████| 1/1 [00:00<00:00, 787.81it/s]\n\n\n[Mar 13, 01:18:16] #> Optimizing IVF to store map from centroids to list of pids..\n[Mar 13, 01:18:16] #> Building the emb2pid mapping..\n[Mar 13, 01:18:16] len(emb2pid) = 15198\n\n\n\n100%|██████████| 1024/1024 [00:00<00:00, 61154.86it/s]\n\n\n[Mar 13, 01:18:16] #> Saved optimized IVF to .ragatouille/colbert/indexes/cqa_index/ivf.pid.pt\nDone indexing!\n\n\n\n\n\n\nindex_path = Path(index_path)\nindex_path\n\nPath('.ragatouille/colbert/indexes/cqa_index')\n\n\n\nfor o in index_path.ls(): print(o)\n\n.ragatouille/colbert/indexes/cqa_index/buckets.pt\n.ragatouille/colbert/indexes/cqa_index/collection.json\n.ragatouille/colbert/indexes/cqa_index/metadata.json\n.ragatouille/colbert/indexes/cqa_index/ivf.pid.pt\n.ragatouille/colbert/indexes/cqa_index/doclens.0.json\n.ragatouille/colbert/indexes/cqa_index/0.residuals.pt\n.ragatouille/colbert/indexes/cqa_index/centroids.pt\n.ragatouille/colbert/indexes/cqa_index/0.metadata.json\n.ragatouille/colbert/indexes/cqa_index/plan.json\n.ragatouille/colbert/indexes/cqa_index/0.codes.pt\n.ragatouille/colbert/indexes/cqa_index/avg_residual.pt\n.ragatouille/colbert/indexes/cqa_index/pid_docid_map.json\n\n\nWhile it’s a bit tedious to do so (since I’m chomping at the bit to get to the deep dive!) I think it’s worth analyzing the contents of each of these files, as we’ll be recreating them in this notebook.\n\nbuckets.pt\nLooking at Line 160 of the ColBERT repo’s residual.py, buckets.py stores bucket_cutoffs and bucket_weights. We’ll go into detail into what these exactly are later on.\n\n_bucket_cutoffs, _bucket_weights = torch.load(index_path/'buckets.pt', weights_only=True)\n\n\n_bucket_cutoffs.shape, _bucket_weights.shape\n\n(torch.Size([15]), torch.Size([16]))\n\n\n\n_bucket_cutoffs\n\ntensor([-0.0307, -0.0205, -0.0146, -0.0099, -0.0064, -0.0037, -0.0016,  0.0000,\n         0.0017,  0.0038,  0.0066,  0.0102,  0.0150,  0.0211,  0.0313],\n       device='cuda:0')\n\n\n\n_bucket_weights\n\ntensor([-0.0411, -0.0247, -0.0173, -0.0121, -0.0081, -0.0050, -0.0026, -0.0007,\n         0.0007,  0.0027,  0.0052,  0.0083,  0.0124,  0.0178,  0.0253,  0.0421],\n       device='cuda:0', dtype=torch.float16)\n\n\n\n\n0.residuals.pt\nIIUC, there are 15198 tokens in our collection, each with a 48-dimension vector representation, and each integer value represents two 4-bit codes that each correspond to a quantized value. So there are actually 96 values in each vector.\n\n_residuals = torch.load(index_path/'0.residuals.pt', weights_only=True)\n_residuals\n\ntensor([[ 30, 225, 225,  ..., 238, 238,  30],\n        [230,  22, 158,  ..., 233, 106, 170],\n        [238, 238, 238,  ..., 238, 238, 238],\n        ...,\n        [ 43,  22,  23,  ..., 104,  31, 208],\n        [222, 254,  91,  ..., 128,   8, 189],\n        [229,  82,  22,  ..., 170,  94, 154]], dtype=torch.uint8)\n\n\n\n_residuals.shape\n\ntorch.Size([15198, 48])\n\n\n\n48*4/2\n\n96.0\n\n\n\n\nivf.pid.pt\nIIRC, ivf contains a flattened sequence of passage IDs corresponding to each centroid. There are 1024 centroids and the first 8 passage IDs in ivf correspond to the 0-th centroid.\n\n_ivf, _ivf_lengths = torch.load(index_path/'ivf.pid.pt', weights_only=True)\n_ivf.shape, _ivf_lengths.shape\n\n(torch.Size([11759]), torch.Size([1024]))\n\n\n\n_ivf[:5]\n\ntensor([895, 896, 902, 904, 909], dtype=torch.int32)\n\n\n\n_ivf_lengths[0]\n\ntensor(8)\n\n\n\n\n0.metadata.json\nThere are 1000 passages totaling 15198 tokens.\n\ndef load_json(path, filename): return srsly.read_json(str(Path(path) / filename))\n\n\nload_json(index_path, \"0.metadata.json\")\n\n{'passage_offset': 0,\n 'num_passages': 1000,\n 'num_embeddings': 15198,\n 'embedding_offset': 0}\n\n\n\n\ncollection.json\nThis JSON contains, as a list, the strings of the 1000 passages in our collection.\n\n_collection = load_json(index_path, \"collection.json\")\nlen(_collection), _collection[0]\n\n(1000, 'Overview')\n\n\n\n\navg_residual.pt\nI believe this is the average residual across the 15198 tokens (i.e. the average distance in vector-space between the tokens and their closest centroids).\n\n_avg_residual = torch.load(index_path/'avg_residual.pt', weights_only=True)\n_avg_residual\n\ntensor(0.0150, device='cuda:0', dtype=torch.float16)\n\n\n\n\ndoclens.0.json\nThis contains a mapping (list) between passages IDs (indices) and the number of tokens in the document (values).\n\n_doclens = load_json(index_path, \"doclens.0.json\")\nlen(_doclens), _doclens[:5]\n\n(1000, [4, 20, 18, 23, 8])\n\n\n\nsum(doclens)\n\n15198\n\n\n\n\nmetadata.json\nLots of information in here, will highlight the number of centroids and the number of token embeddings in the collection:\n'num_partitions': 1024,\n'num_embeddings': 15198\n\nload_json(index_path, \"metadata.json\")\n\n{'config': {'query_token_id': '[unused0]',\n  'doc_token_id': '[unused1]',\n  'query_token': '[Q]',\n  'doc_token': '[D]',\n  'ncells': None,\n  'centroid_score_threshold': None,\n  'ndocs': None,\n  'load_index_with_mmap': False,\n  'index_path': None,\n  'index_bsize': 32,\n  'nbits': 4,\n  'kmeans_niters': 20,\n  'resume': False,\n  'pool_factor': 1,\n  'clustering_mode': 'hierarchical',\n  'protected_tokens': 0,\n  'similarity': 'cosine',\n  'bsize': 64,\n  'accumsteps': 1,\n  'lr': 1e-05,\n  'maxsteps': 15626,\n  'save_every': None,\n  'warmup': 781,\n  'warmup_bert': None,\n  'relu': False,\n  'nway': 32,\n  'use_ib_negatives': False,\n  'reranker': False,\n  'distillation_alpha': 1.0,\n  'ignore_scores': False,\n  'model_name': 'answerdotai/AnswerAI-ColBERTv2.5-small',\n  'query_maxlen': 32,\n  'attend_to_mask_tokens': False,\n  'interaction': 'colbert',\n  'dim': 96,\n  'doc_maxlen': 256,\n  'mask_punctuation': True,\n  'checkpoint': 'answerdotai/answerai-colbert-small-v1',\n  'triples': '/home/bclavie/colbertv2.5_en/data/msmarco/triplets.jsonl',\n  'collection': ['list with 1000 elements starting with...',\n   ['Overview',\n    'You can only make a claim for Child Tax Credit if you already get Working Tax Credit.',\n    'If you cannot apply for Child Tax Credit, you can apply for Universal Credit instead.']],\n  'queries': '/home/bclavie/colbertv2.5_en/data/msmarco/queries.tsv',\n  'index_name': 'cqa_index',\n  'overwrite': False,\n  'root': '.ragatouille/',\n  'experiment': 'colbert',\n  'index_root': None,\n  'name': '2025-03/13/01.14.35',\n  'rank': 0,\n  'nranks': 1,\n  'amp': True,\n  'gpus': 1,\n  'avoid_fork_if_possible': False},\n 'num_chunks': 1,\n 'num_partitions': 1024,\n 'num_embeddings': 15198,\n 'avg_doclen': 15.198,\n 'RAGatouille': {'index_config': {'index_type': 'PLAID',\n   'index_name': 'cqa_index'}}}\n\n\n\n\ncentroids.pt\nThere are 1024 96-dimension centroid vectors stored.\n\n_centroids = torch.load(index_path/'centroids.pt', weights_only=True)\n_centroids.shape\n\ntorch.Size([1024, 96])\n\n\nThey store the full uncompressed values for the centroids.\n\n_centroids[0][:5]\n\ntensor([-0.0649,  0.1193, -0.0551,  0.0561, -0.0826], device='cuda:0',\n       dtype=torch.float16)\n\n\n\n\n0.codes.pt\nI believe this is a mapping (list) between tokens (indices) and centroid IDs (values).\n\n_codes = torch.load(index_path/'0.codes.pt', weights_only=True)\n_codes.shape\n\ntorch.Size([15198])\n\n\n\n_codes[:5]\n\ntensor([138, 843, 273, 138, 561], dtype=torch.int32)\n\n\n\n\npid_docid_map.json\nA mapping between passage ID (0-999) and document ID (UUID).\n\n_pid_docid_map = load_json(index_path, \"pid_docid_map.json\")\n_pid_docid_map['999']\n\n'2be086c6-04cc-4d73-b372-08236f76cbe6'\n\n\n\n\nplan.json\nThis seems to contain the same information as metadata.json.\n\n_plan = load_json(index_path, \"plan.json\")\n_plan\n\n{'config': {'query_token_id': '[unused0]',\n  'doc_token_id': '[unused1]',\n  'query_token': '[Q]',\n  'doc_token': '[D]',\n  'ncells': None,\n  'centroid_score_threshold': None,\n  'ndocs': None,\n  'load_index_with_mmap': False,\n  'index_path': None,\n  'index_bsize': 32,\n  'nbits': 4,\n  'kmeans_niters': 20,\n  'resume': False,\n  'pool_factor': 1,\n  'clustering_mode': 'hierarchical',\n  'protected_tokens': 0,\n  'similarity': 'cosine',\n  'bsize': 64,\n  'accumsteps': 1,\n  'lr': 1e-05,\n  'maxsteps': 15626,\n  'save_every': None,\n  'warmup': 781,\n  'warmup_bert': None,\n  'relu': False,\n  'nway': 32,\n  'use_ib_negatives': False,\n  'reranker': False,\n  'distillation_alpha': 1.0,\n  'ignore_scores': False,\n  'model_name': 'answerdotai/AnswerAI-ColBERTv2.5-small',\n  'query_maxlen': 32,\n  'attend_to_mask_tokens': False,\n  'interaction': 'colbert',\n  'dim': 96,\n  'doc_maxlen': 256,\n  'mask_punctuation': True,\n  'checkpoint': 'answerdotai/answerai-colbert-small-v1',\n  'triples': '/home/bclavie/colbertv2.5_en/data/msmarco/triplets.jsonl',\n  'collection': ['list with 1000 elements starting with...',\n   ['Overview',\n    'You can only make a claim for Child Tax Credit if you already get Working Tax Credit.',\n    'If you cannot apply for Child Tax Credit, you can apply for Universal Credit instead.']],\n  'queries': '/home/bclavie/colbertv2.5_en/data/msmarco/queries.tsv',\n  'index_name': 'cqa_index',\n  'overwrite': False,\n  'root': '.ragatouille/',\n  'experiment': 'colbert',\n  'index_root': None,\n  'name': '2025-03/13/01.14.35',\n  'rank': 0,\n  'nranks': 1,\n  'amp': True,\n  'gpus': 1,\n  'avoid_fork_if_possible': False},\n 'num_chunks': 1,\n 'num_partitions': 1024,\n 'num_embeddings_est': 15197.999954223633,\n 'avg_doclen_est': 15.197999954223633}\n\n\nIn the following sections, I’ll try to recreate each of these index_path elements."
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#process_corpus",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#process_corpus",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "_process_corpus",
    "text": "_process_corpus\nInside RAG.index, _process_corpus is called on the documents and document IDs.\n\npassage_ids = [str(uuid.uuid4()) for _ in range(len(passages))]\npassage_ids[0]\n\n'd4cdfec5-a949-43e0-94b3-feb24caeac5e'\n\n\nUse the corpus processor to convert the passages into {'document_id': '...', 'content': '...'} dictionaries with 256-token max length.\n\ncp = CorpusProcessor()\ncp\n\n<ragatouille.data.corpus_processor.CorpusProcessor at 0x794fa0323690>\n\n\n\ncollection_with_ids = cp.process_corpus(passages['text'], passage_ids, chunk_size=256)\nlen(collection_with_ids), collection_with_ids[0]\n\n(1000,\n {'document_id': 'd4cdfec5-a949-43e0-94b3-feb24caeac5e',\n  'content': 'Overview'})\n\n\nAs a brief aside, I’ll take a look at the maximum token length of the passages.\n\nnode_parser = SentenceSplitter(chunk_size=256)\nnode_parser._token_size\n\n\n    llama_index.core.node_parser.text.sentence.SentenceSplitter._token_sizedef _token_size(text: str) -> int/usr/local/lib/python3.11/dist-packages/llama_index/core/node_parser/text/sentence.py<no docstring>\n      \n      \n\n\n\ntk_szs = []\nfor p in passages['text']: tk_szs.append(node_parser._token_size(p))\npd.Series(tk_szs).describe()\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      1000.000000\n    \n    \n      mean\n      13.217000\n    \n    \n      std\n      10.192635\n    \n    \n      min\n      1.000000\n    \n    \n      25%\n      5.000000\n    \n    \n      50%\n      10.000000\n    \n    \n      75%\n      19.000000\n    \n    \n      max\n      65.000000\n    \n  \n\ndtype: float64\n\n\nThis collection of passages has relatively short passages (a max of 65 tokens).\n_process_corpus then creates\n\npid_docid_map = {index: item[\"document_id\"] for index, item in enumerate(collection_with_ids)}\n\n\npid_docid_map[999]\n\n'096e054e-3041-4881-ac48-b20f1804f650'\n\n\nThis matches the content of pid_docid_map.json.\n_process_corpus also defines a list of strings, collection:\n\ncollection = [x[\"content\"] for x in collection_with_ids]\ncollection[0]\n\n'Overview'\n\n\n_process_corpus also calls _process_metadata which defines docid_metadata_map as None when document_metadatas is None (which it is in our case).\n\ndocid_metadata_map = None"
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#rag.index-internals",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#rag.index-internals",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "RAG.index Internals",
    "text": "RAG.index Internals\nAfter calling _process_corpus, RAG.index calls model.index, where model is:\ninstance.model = ColBERT(\n            pretrained_model_name_or_path, n_gpu, index_root=index_root, verbose=verbose\n        )\nColBERT.index in turn calls:\nModelIndexFactory.construct\nBy default the type of index is PLAID, so the following is called:\nPLAIDModelIndex(config).build(\n            checkpoint, collection, index_name, overwrite, verbose, **kwargs\n        )\n\nPLAIDModelIndex.build\nA couple of key configuration values are set in this method, starting with the bsize (which I think is batch size?) defaulting to 32.\n\nPLAIDModelIndex._DEFAULT_INDEX_BSIZE\n\n32\n\n\n\nbsize = PLAIDModelIndex._DEFAULT_INDEX_BSIZE\nbsize\n\n32\n\n\nThe size of compressed residual embedding values is determined based on the size of the collection.\n\nif len(collection) < 10000: nbits = 4\nnbits\n\n4\n\n\nIt then defines a ColBERTConfig object, which I believe is instantiated as follows when the ColBERT checkpoint is instantiated:\n\nckpt_config = ColBERTConfig.load_from_checkpoint(str(model_nm))\nckpt_config\n\nColBERTConfig(query_token_id='[unused0]', doc_token_id='[unused1]', query_token='[Q]', doc_token='[D]', ncells=None, centroid_score_threshold=None, ndocs=None, load_index_with_mmap=False, index_path=None, index_bsize=64, nbits=1, kmeans_niters=4, resume=False, pool_factor=1, clustering_mode='hierarchical', protected_tokens=0, similarity='cosine', bsize=32, accumsteps=1, lr=1e-05, maxsteps=15626, save_every=None, warmup=781, warmup_bert=None, relu=False, nway=32, use_ib_negatives=False, reranker=False, distillation_alpha=1.0, ignore_scores=False, model_name='answerdotai/AnswerAI-ColBERTv2.5-small', query_maxlen=32, attend_to_mask_tokens=False, interaction='colbert', dim=96, doc_maxlen=300, mask_punctuation=True, checkpoint='/root/.cache/huggingface/hub/models--answerdotai--answerai-colbert-small-v1/snapshots/be1703c55532145a844da800eea4c9a692d7e267/', triples='/home/bclavie/colbertv2.5_en/data/msmarco/triplets.jsonl', collection='/home/bclavie/colbertv2.5_en/data/msmarco/collection.tsv', queries='/home/bclavie/colbertv2.5_en/data/msmarco/queries.tsv', index_name=None, overwrite=False, root='/home/bclavie/colbertv2.5_en/experiments', experiment='minicolbertv2.5', index_root=None, name='2024-08/07/08.16.20', rank=0, nranks=4, amp=True, gpus=4, avoid_fork_if_possible=False)\n\n\n\nconfig = ColBERTConfig.from_existing(ckpt_config)\nconfig\n\nColBERTConfig(query_token_id='[unused0]', doc_token_id='[unused1]', query_token='[Q]', doc_token='[D]', ncells=None, centroid_score_threshold=None, ndocs=None, load_index_with_mmap=False, index_path=None, index_bsize=64, nbits=1, kmeans_niters=4, resume=False, pool_factor=1, clustering_mode='hierarchical', protected_tokens=0, similarity='cosine', bsize=32, accumsteps=1, lr=1e-05, maxsteps=15626, save_every=None, warmup=781, warmup_bert=None, relu=False, nway=32, use_ib_negatives=False, reranker=False, distillation_alpha=1.0, ignore_scores=False, model_name='answerdotai/AnswerAI-ColBERTv2.5-small', query_maxlen=32, attend_to_mask_tokens=False, interaction='colbert', dim=96, doc_maxlen=300, mask_punctuation=True, checkpoint='/root/.cache/huggingface/hub/models--answerdotai--answerai-colbert-small-v1/snapshots/be1703c55532145a844da800eea4c9a692d7e267/', triples='/home/bclavie/colbertv2.5_en/data/msmarco/triplets.jsonl', collection='/home/bclavie/colbertv2.5_en/data/msmarco/collection.tsv', queries='/home/bclavie/colbertv2.5_en/data/msmarco/queries.tsv', index_name=None, overwrite=False, root='/home/bclavie/colbertv2.5_en/experiments', experiment='minicolbertv2.5', index_root=None, name='2024-08/07/08.16.20', rank=0, nranks=4, amp=True, gpus=4, avoid_fork_if_possible=False)\n\n\nWe also need to create a RunConfig object:\n\nrun_config = RunConfig(nranks=-1, experiment=\"colbert\", root=\".ragatouille/\")\nrun_config\n\nRunConfig(overwrite=False, root='.ragatouille/', experiment='colbert', index_root=None, name='2025-03/13/01.14.35', rank=0, nranks=-1, amp=True, gpus=1, avoid_fork_if_possible=False)\n\n\nA couple more config values are set:\n\nconfig.avoid_fork_if_possible = True\n\nif len(collection) > 100000:\n    config.kmeans_niters = 4\nelif len(collection) > 50000:\n    config.kmeans_niters = 10\nelse:\n    config.kmeans_niters = 20\nconfig.avoid_fork_if_possible, config.kmeans_niters\n\n(True, 20)\n\n\nAfter determining whether the PyTorch or FAISS k-means implementation will be used, Indexer.index is called."
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#indexer.index",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#indexer.index",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "Indexer.index",
    "text": "Indexer.index\nThe Indexer comes from the ColBERT repo, so this is essentially the connection point between the RAGatouille and ColBERT libraries.\n\nLauncher\nInside Indexer.index, __launch is called, from within which a Launcher instance is created with the encode function.\nI’m a bit fuzzy on the next part but I’ll give it a shot:\nwhen Launcher.launch is called, the following two lines are called (where callee is the encode function):\nargs_ = (self.callee, port, return_value_queue, new_config, *args)\nall_procs.append(mp.Process(target=setup_new_process, args=args_))\nsetup_new_process contains the following lines:\nwith Run().context(config, inherit_config=False):\n    return_val = callee(config, *args)\nWith callee being called, let’s look at the function that callee is : encode, which is part of the collection_indexer.py file."
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#encode",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#encode",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "encode",
    "text": "encode\ndef encode(config, collection, shared_lists, shared_queues, verbose: int = 3):\n    encoder = CollectionIndexer(config=config, collection=collection, verbose=verbose)\n    encoder.run(shared_lists)\nThis leads us to encoder.run which is CollectionIndexer.run. But before that, we need to look at how the collection is transformed when CollectionIndexer is instantiated."
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.__init__",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.__init__",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "CollectionIndexer.__init__",
    "text": "CollectionIndexer.__init__\nThere are two important objects created when the CollectionIndexer is instantiated. First is the Collection object, which turns our list collection:\n\ntype(collection)\n\nlist\n\n\ninto a Collection object:\n\ncollection = Collection.cast(collection)\ncollection\n\n<colbert.data.collection.Collection at 0x794fa037e410>\n\n\n\ntype(collection)\n\n\n    colbert.data.collection.Collectiondef __init__(path=None, data=None)/usr/local/lib/python3.11/dist-packages/colbert/data/collection.py<no docstring>\n      \n      \n\n\nNext, it creates a CollectionEncoder object:\n\ncheckpoint = Checkpoint(config.checkpoint, colbert_config=config)\nencoder = CollectionEncoder(config, checkpoint)\n\n/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n\n\ncheckpoint is our model:\n\ncheckpoint\n\nCheckpoint(\n  (model): HF_ColBERT(\n    (linear): Linear(in_features=384, out_features=96, bias=False)\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 384, padding_idx=0)\n        (position_embeddings): Embedding(512, 384)\n        (token_type_embeddings): Embedding(2, 384)\n        (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSdpaSelfAttention(\n                (query): Linear(in_features=384, out_features=384, bias=True)\n                (key): Linear(in_features=384, out_features=384, bias=True)\n                (value): Linear(in_features=384, out_features=384, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=384, out_features=384, bias=True)\n                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=384, out_features=1536, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=1536, out_features=384, bias=True)\n              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=384, out_features=384, bias=True)\n        (activation): Tanh()\n      )\n    )\n  )\n)\n\n\nThe CollectionEncoder will be used later on to encode the passages.\n\nencoder\n\n<colbert.indexing.collection_encoder.CollectionEncoder at 0x794fa0683dd0>\n\n\nNext we’ll dive into CollectionIndexer.run within which all of the indexing operations that I’m most interested in take place, starting with setup."
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.run",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.run",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "CollectionIndexer.run",
    "text": "CollectionIndexer.run\n\nCollectionIndexer.setup\n'''\nCalculates and saves plan.json for the whole collection.\n\nplan.json { config, num_chunks, num_partitions, num_embeddings_est, avg_doclen_est}\nnum_partitions is the number of centroids to be generated.\n'''\nWe’ll see where num_chunks is used, but for now I’ll just define it:\n\nnum_chunks = int(np.ceil(len(collection) / collection.get_chunksize()))\nlen(collection), collection.get_chunksize(), num_chunks\n\n(1000, 1001, 1)\n\n\nNext we look at _sample_pids and _sample_embeddings which are later clustered to get our centroids.\n\n_sample_pids\n\nnum_passages = len(collection)\nnum_passages\n\n1000\n\n\nIt’s awesome to see one of the heuristics mentioned in the ColBERTv2 paper:\n\nTo reduce memory consumption, we apply k-means clustering to the embeddings produced by invoking our BERT encoder over only a sample of all passages, proportional to the square root of the collection size, an approach we found to perform well in practice.\n\n\ntypical_doclen = 120\nsampled_pids = 16 * np.sqrt(typical_doclen * num_passages)\nsampled_pids\n\n5542.562584220407\n\n\n\nsampled_pids = min(1 + int(sampled_pids), num_passages)\n\nIn this case because my toy collection is so small (1000 passages) we will use all of them for centroid clustering.\n\nsampled_pids = random.sample(range(num_passages), sampled_pids)\nsampled_pids = set(sampled_pids)\nlen(sampled_pids), min(sampled_pids), max(sampled_pids)\n\n(1000, 0, 999)\n\n\n\n\n_sample_embeddings\n\nlocal_pids = collection.enumerate(rank=config.rank)\nlocal_pids\n\n<generator object Collection.enumerate at 0x794fa067dc40>\n\n\nsampled_pids contains all of our passages\n\nlocal_sample = [passage for pid, passage in local_pids if pid in sampled_pids]\nlen(local_sample)\n\n1000\n\n\nNext come another critical process—encoding our passages!\n\n\nCollectionEncoder.encode_passages\nInside encode_passages we call checkpoint.docFromText.\n\n\ncheckpoint.docFromText\nAnd inside checkpoint.docFromText we call checkpoint.doc\n\n\ncheckpoint.doc\nInside ColBERT.doc we finally call the lowest-level method in this chain:\nD = self.bert(input_ids, attention_mask=attention_mask)[0]\nOne key point to visualize is that the BERT output is normalized:\n D = torch.nn.functional.normalize(D, p=2, dim=2)\nI’ll zoom out again and call encode_passages.\n\nlocal_sample_embs, doclens = encoder.encode_passages(local_sample)\n\n[Mar 13, 01:18:18] [0]       #> Encoding 1000 passages..\n\n\n/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n\n\n\nlocal_sample_embs.shape\n\ntorch.Size([15198, 96])\n\n\nNote that the token embeddings are a unit vector:\n\nlocal_sample_embs[0].norm()\n\ntensor(1., dtype=torch.float16)\n\n\n\nlen(doclens), doclens[:5]\n\n(1000, [4, 20, 18, 23, 8])\n\n\nWe have 15198 token embeddings (embedded into answerai-colbert-small-v1’s 96-dimension space) and a mapping (list) of passage ID (indices) to number of tokens (values).\n\navg_doclen_est = sum(doclens) / len(doclens) if doclens else 0\navg_doclen_est\n\n15.198\n\n\nOn average, each passage (document) is 15 tokens long.\nZooming back out to CollectionIndexer.setup we have a few more steps before our planning is complete:\n\nnum_passages = len(collection)\nnum_embeddings_est = num_passages * avg_doclen_est\nnum_partitions = int(2 ** np.floor(np.log2(16 * np.sqrt(num_embeddings_est))))\nnum_passages, num_embeddings_est, num_partitions\n\n(1000, 15198.0, 1024)\n\n\nnum_partitions is the number of clusters we will cluster our 15198 token embeddings into.\nThis is the information that was in plan.json in index_path (in addition to the other ColBERTConfig information)."
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.train",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.train",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "CollectionIndexer.train",
    "text": "CollectionIndexer.train\nAfter setup is complete, the next method called in run is train.\nThe first step in train is to split our local_sample_embs into sample and sample_heldout.\n\nlocal_sample_embs = local_sample_embs[torch.randperm(local_sample_embs.size(0))]\n\n\nheldout_fraction = 0.05\nheldout_size = int(min(heldout_fraction * local_sample_embs.size(0), 50_000))\nheldout_size\n\n759\n\n\n\nsample, sample_heldout = local_sample_embs.split([local_sample_embs.size(0) - heldout_size, heldout_size], dim=0)\nsample.shape, sample_heldout.shape\n\n(torch.Size([14439, 96]), torch.Size([759, 96]))\n\n\n\ncompute_faiss_kmeans\nNext we get the centroids using compute_faiss_kmeans\n\nargs_ = [config.dim, num_partitions, config.kmeans_niters, [[sample]]]\n\n\ncentroids = compute_faiss_kmeans(*args_)\ncentroids.shape\n\ntorch.Size([1024, 96])\n\n\nWe then normalize the centroids\n\ncentroids = torch.nn.functional.normalize(centroids, dim=-1)\ncentroids.shape, centroids[0].norm()\n\n(torch.Size([1024, 96]), tensor(1.))\n\n\n\ncentroids = centroids.half()\n\nI was hoping to get the same values as RAG.index centroids by setting seeds at the start of this notebook, but I am not getting the same result.\n\n_centroids[0][:5]\n\ntensor([-0.0649,  0.1193, -0.0551,  0.0561, -0.0826], device='cuda:0',\n       dtype=torch.float16)\n\n\n\ncentroids[0][:5]\n\ntensor([-0.0587,  0.0379, -0.0847, -0.0224, -0.0636], dtype=torch.float16)\n\n\nI’ll use PCA to compare the two sets of centroids:\n\n# Project to 2D\npca = PCA(n_components=2)\nprev_2d = pca.fit_transform(_centroids.cpu().numpy())\nnew_2d = pca.transform(centroids.cpu().numpy())\n\n# Plot\nplt.figure(figsize=(10, 8))\nplt.scatter(prev_2d[:, 0], prev_2d[:, 1], alpha=0.5, label='Previous')\nplt.scatter(new_2d[:, 0], new_2d[:, 1], alpha=0.5, label='New')\nplt.legend()\nplt.title('PCA projection of centroids')\nplt.show()\n\n\n\n\n\n\n\n\nI’m not super familiar with interpreting PCA plots, so I asked Claude what it thought about this result:\n\nI would describe this as showing “good structural similarity but with expected local variations.” The centroids aren’t identical (which would show perfect overlap), but they capture similar patterns in the embedding space. This suggests that while individual centroid positions differ, the overall index structure should perform similarly for retrieval tasks.\n\nFor now, I’ll consider this part of indexing completing, as we have generated similar contents to what’s in centroids.pt.\n\n\n_compute_avg_residual\nThis next section was quite eye opening for me, as it was the first time I understood how quantization is implemented.\nThe ResidualCodec does all of the compression/binarization/decompress of residuals.\n\ncompressor = ResidualCodec(config=config, centroids=centroids, avg_residual=None)\ncompressor\n\n<colbert.indexing.codecs.residual.ResidualCodec at 0x794f9aacfdd0>\n\n\n\nheldout_reconstruct = compressor.compress_into_codes(sample_heldout, out_device='cuda' )\nheldout_reconstruct.shape\n\ntorch.Size([759])\n\n\ncompress_into_codes finds the nearest centroid IDs to the token embeddings. It does so using cosine similarity:\nindices = (self.centroids @ batch.T.cuda().half()).max(dim=0).indices.to(device=out_device)\n\nheldout_reconstruct[:5]\n\ntensor([633, 667, 738, 641, 443], device='cuda:0')\n\n\nlookup_centroids gets the full vectors related to the centroid IDs in heldout_reconstruct\n\nheldout_reconstruct = compressor.lookup_centroids(heldout_reconstruct, out_device='cuda')\nheldout_reconstruct.shape\n\ntorch.Size([759, 96])\n\n\nThe residual between the heldout token embeddings and the closest centroids is then calculated:\n\nheldout_avg_residual = sample_heldout.cuda() - heldout_reconstruct\nheldout_avg_residual.shape\n\ntorch.Size([759, 96])\n\n\nWe then calculate the average residual vector (96 dimensions):\n\navg_residual = torch.abs(heldout_avg_residual).mean(dim=0).cpu()\navg_residual.shape\n\ntorch.Size([96])\n\n\nThe average residual is somewhat similar to the stored value in avg_residual.pt.\n\n_avg_residual, avg_residual.mean()\n\n(tensor(0.0150, device='cuda:0', dtype=torch.float16),\n tensor(0.0158, dtype=torch.float16))\n\n\nTo match the RAG.index defaults, I’m going to set nbits to 4.\n\nconfig.nbits\n\n1\n\n\n\nconfig.nbits = 4\nconfig.nbits\n\n4\n\n\n\nnum_options = 2 ** config.nbits\nconfig.nbits, num_options\n\n(4, 16)\n\n\nA 4-bit value has four 0 or 1 values and there are 16 possible combinations:\n\n\n\nBinary\n\n\n\n\n0000\n\n\n0001\n\n\n0010\n\n\n0011\n\n\n0100\n\n\n0101\n\n\n0110\n\n\n0111\n\n\n1000\n\n\n1001\n\n\n1010\n\n\n1011\n\n\n1100\n\n\n1101\n\n\n1110\n\n\n1111\n\n\n\nWe split 0-to-1 into 16 equal parts:\n\nquantiles = torch.arange(0, num_options, device=heldout_avg_residual.device) * (1 / num_options)\nquantiles.shape, quantiles\n\n(torch.Size([16]),\n tensor([0.0000, 0.0625, 0.1250, 0.1875, 0.2500, 0.3125, 0.3750, 0.4375, 0.5000,\n         0.5625, 0.6250, 0.6875, 0.7500, 0.8125, 0.8750, 0.9375],\n        device='cuda:0'))\n\n\n\nbucket_cutoffs_quantiles, bucket_weights_quantiles = quantiles[1:], quantiles + (0.5 / num_options)\nbucket_cutoffs_quantiles, bucket_weights_quantiles\n\n(tensor([0.0625, 0.1250, 0.1875, 0.2500, 0.3125, 0.3750, 0.4375, 0.5000, 0.5625,\n         0.6250, 0.6875, 0.7500, 0.8125, 0.8750, 0.9375], device='cuda:0'),\n tensor([0.0312, 0.0938, 0.1562, 0.2188, 0.2812, 0.3438, 0.4062, 0.4688, 0.5312,\n         0.5938, 0.6562, 0.7188, 0.7812, 0.8438, 0.9062, 0.9688],\n        device='cuda:0'))\n\n\nIIUC, the weights’ quantiles are the midpoints between adjacent cutoffs’ quantiles.\n\n(bucket_cutoffs_quantiles[1] + bucket_cutoffs_quantiles[2])/2\n\ntensor(0.1562, device='cuda:0')\n\n\n\n(bucket_cutoffs_quantiles[3] + bucket_cutoffs_quantiles[4])/2\n\ntensor(0.2812, device='cuda:0')\n\n\n\n(bucket_cutoffs_quantiles[-2] + bucket_cutoffs_quantiles[-1])/2\n\ntensor(0.9062, device='cuda:0')\n\n\n\nbucket_cutoffs = heldout_avg_residual.float().quantile(bucket_cutoffs_quantiles)\nbucket_weights = heldout_avg_residual.float().quantile(bucket_weights_quantiles)\n\nIIUC, bucket_cutoffs are the values with which we can group our (flattened) heldout_avg_residuals into 16 equal groups. Visualized here by setting the bins to bucket_cutoffs.\n\npd.Series(heldout_avg_residual.flatten().cpu()).hist(bins=bucket_cutoffs.cpu())\nplt.xlim([-0.035, 0.035])\nplt.show()\n\n\n\n\n\n\n\n\nPerhaps due to randomness during the sample split, my manually calculated cutoffs are not quite the same as the RAG.index values.\n\nbucket_cutoffs\n\ntensor([-0.0322, -0.0219, -0.0156, -0.0108, -0.0070, -0.0040, -0.0017,  0.0000,\n         0.0019,  0.0042,  0.0071,  0.0108,  0.0155,  0.0220,  0.0327],\n       device='cuda:0')\n\n\n\n_bucket_cutoffs\n\ntensor([-0.0307, -0.0205, -0.0146, -0.0099, -0.0064, -0.0037, -0.0016,  0.0000,\n         0.0017,  0.0038,  0.0066,  0.0102,  0.0150,  0.0211,  0.0313],\n       device='cuda:0')\n\n\n\nbucket_weights\n\ntensor([-0.0434, -0.0261, -0.0185, -0.0131, -0.0088, -0.0054, -0.0028, -0.0009,\n         0.0009,  0.0029,  0.0055,  0.0088,  0.0130,  0.0184,  0.0262,  0.0441],\n       device='cuda:0')\n\n\n\n_bucket_weights\n\ntensor([-0.0411, -0.0247, -0.0173, -0.0121, -0.0081, -0.0050, -0.0026, -0.0007,\n         0.0007,  0.0027,  0.0052,  0.0083,  0.0124,  0.0178,  0.0253,  0.0421],\n       device='cuda:0', dtype=torch.float16)\n\n\nThere seems to be some rounding differences (or perhaps it depends on the distribution?) but the weights again seem to be the midpoints-ish between the cutoffs.\n\n(bucket_cutoffs[0] + bucket_cutoffs[1])/2\n\ntensor(-0.0270, device='cuda:0')\n\n\n\n(bucket_cutoffs[3] + bucket_cutoffs[4])/2\n\ntensor(-0.0089, device='cuda:0')\n\n\n\n(bucket_cutoffs[-2] + bucket_cutoffs[-1])/2\n\ntensor(0.0273, device='cuda:0')"
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.index",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#collectionindexer.index",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "CollectionIndexer.index",
    "text": "CollectionIndexer.index\nThus far we have found centroids from a sample of our token embeddings (5%, or 759) and calculated bucket cutoffs and bucket weights for quantization. We also know what the average residual mean value is.\nNow we find the closest centroids and residuals for all passages’ token embeddings, starting first by encoding all 15198 tokens with our model:\n\nembs, doclens = encoder.encode_passages(collection)\nembs.shape, len(doclens), doclens[:5]\n\n[Mar 13, 01:18:21] [0]       #> Encoding 1000 passages..\n\n\n/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n\n\n(torch.Size([15198, 96]), 1000, [4, 20, 18, 23, 8])\n\n\nThen we call save_chunk which is inside the IndexSaver within which some interesting things take place:\ndef save_chunk(self, chunk_idx, offset, embs, doclens):\n        compressed_embs = self.codec.compress(embs)\nLooking into ResidualCodec.compress:\ndef compress(self, embs):\n        codes, residuals = [], []\n\n        for batch in embs.split(1 << 18):\n            if self.use_gpu:\n                batch = batch.cuda().half()\n            codes_ = self.compress_into_codes(batch, out_device=batch.device)\n            centroids_ = self.lookup_centroids(codes_, out_device=batch.device)\n\n            residuals_ = (batch - centroids_)\n\n            codes.append(codes_.cpu())\n            residuals.append(self.binarize(residuals_).cpu())\n\n        codes = torch.cat(codes)\n        residuals = torch.cat(residuals)\n\n        return ResidualCodec.Embeddings(codes, residuals)\nWe’ve seen compress_into_codes and lookup_centroids before:\n\ncodes_ = compressor.compress_into_codes(embs, out_device='cuda')\ncodes_.shape\n\ntorch.Size([15198])\n\n\nThese codes are the centroids ID closest to each token embeddings.\n\ncodes_[:5]\n\ntensor([654, 843, 401, 654, 926], device='cuda:0')\n\n\nWe then get those 15198 centroid vectors:\n\ncentroids_ = compressor.lookup_centroids(codes_, out_device='cuda')\ncentroids_.shape\n\ntorch.Size([15198, 96])\n\n\nA reminder that our centroids and our token embeddings are unit vectors:\n\ncentroids_[0].norm(), embs[0].norm()\n\n(tensor(1., device='cuda:0', dtype=torch.float16),\n tensor(1., dtype=torch.float16))\n\n\nWe then find the residuals between token embeddings and centroids:\n\nresiduals_ = (embs.cpu() - centroids_.cpu())\nresiduals_.shape\n\ntorch.Size([15198, 96])\n\n\nThe next piece is super cool. We binarize the residuals, starting by using bucketize:\n\nresiduals = torch.bucketize(residuals_.float().cpu(), bucket_cutoffs.cpu()).to(dtype=torch.uint8)\nresiduals.shape\n\ntorch.Size([15198, 96])\n\n\n\nresiduals[0][:5]\n\ntensor([8, 7, 7, 8, 7], dtype=torch.uint8)\n\n\n\nresiduals_[0][:5]\n\ntensor([6.1035e-05, 0.0000e+00, 0.0000e+00, 1.9073e-06, 0.0000e+00],\n       dtype=torch.float16)\n\n\n\nresiduals_[1][10:20]\n\ntensor([ 0.0234, -0.0100,  0.0046, -0.0078, -0.0111,  0.0249, -0.0081, -0.0048,\n         0.0270,  0.0037], dtype=torch.float16)\n\n\n\nbucket_cutoffs[6:10]\n\ntensor([-0.0017,  0.0000,  0.0019,  0.0042], device='cuda:0')\n\n\n\nresiduals.min(), residuals.max()\n\n(tensor(0, dtype=torch.uint8), tensor(15, dtype=torch.uint8))\n\n\nThe values of residuals are now the ID (indices) of the buckets that the residual values fall into!\n\nresiduals = residuals.unsqueeze(-1).expand(*residuals.size(), config.nbits)\nresiduals.shape\n\ntorch.Size([15198, 96, 4])\n\n\nWe add a space for 4-bits per residual.\n\narange_bits = torch.arange(0, config.nbits, device='cuda', dtype=torch.uint8)\narange_bits\n\ntensor([0, 1, 2, 3], device='cuda:0', dtype=torch.uint8)\n\n\n\nresiduals = residuals.cpu() >> arange_bits.cpu()\nresiduals.shape\n\ntorch.Size([15198, 96, 4])\n\n\n\nresiduals[0][:5]\n\ntensor([[8, 4, 2, 1],\n        [7, 3, 1, 0],\n        [7, 3, 1, 0],\n        [8, 4, 2, 1],\n        [7, 3, 1, 0]], dtype=torch.uint8)\n\n\n\nresiduals = residuals & 1\n\n\nresiduals[0][:5]\n\ntensor([[0, 0, 0, 1],\n        [1, 1, 1, 0],\n        [1, 1, 1, 0],\n        [0, 0, 0, 1],\n        [1, 1, 1, 0]], dtype=torch.uint8)\n\n\nWe have now converted the bucket ID into the actual 4-bit binary value it represents.\n\nresiduals_packed = np.packbits(np.asarray(residuals.contiguous().flatten()))\nresiduals_packed = torch.as_tensor(residuals_packed, dtype=torch.uint8)\nresiduals_packed = residuals_packed.reshape(residuals.size(0), config.dim // 8 * config.nbits)\nresiduals_packed.shape\n\ntorch.Size([15198, 48])\n\n\n\nresiduals_packed[0][:5]\n\ntensor([ 30, 225, 225, 238, 238], dtype=torch.uint8)\n\n\n\nf\"30 in binary: {bin(30)[2:].zfill(8)}\"\n\n'30 in binary: 00011110'\n\n\nFor each residual vector with 96 values, each value is represented with 4-bits (e.g. 0, 0, 0, 1). Every 8 bits are stored into an integer (e.g. 0001 and 1110 concatenate to become the integer 30) so we have cut the number of values in half (from 96 to 48).\nThese residuals would be stored in 0.residuals.pt.\n\n_build_ivf\nThis is a critical piece—the mapping between passages and centroids!\n\ncodes = codes_.sort()\nivf, values = codes.indices, codes.values\n\nToken embeddings IDs:\n\nivf\n\ntensor([  936,  1171,  2363,  ..., 12051, 12147, 12161], device='cuda:0')\n\n\nCentroid IDs:\n\nvalues\n\ntensor([   0,    0,    0,  ..., 1023, 1023, 1023], device='cuda:0')\n\n\n\nivf.shape, values.shape\n\n(torch.Size([15198]), torch.Size([15198]))\n\n\nivf contains the token embedding ID (the indices of codes_) and values contains the centroid ID (the values of codes_).\nWe then get the number of tokens per centroid ID:\n\nivf_lengths = torch.bincount(values, minlength=num_partitions)\nivf_lengths\n\ntensor([10, 11, 17,  ..., 17,  9, 29], device='cuda:0')\n\n\n\nivf_lengths.shape\n\ntorch.Size([1024])\n\n\n\nivf_lengths.sum()\n\ntensor(15198, device='cuda:0')\n\n\n\n\ncolbert/indexing/utils.py: optimize_ivf\nWe have 1000 documents containing a total of 15198 tokens.\n\ntotal_num_embeddings = sum(doclens)\nlen(doclens), total_num_embeddings\n\n(1000, 15198)\n\n\nInstantiating an empty mapping between token embeddings IDs and passage IDs\n\nemb2pid = torch.zeros(total_num_embeddings, dtype=torch.int)\nemb2pid.shape\n\ntorch.Size([15198])\n\n\nThe indices of doclens are passage IDs pid. The values are the number of tokens in the document dlength.\n\noffset_doclens = 0\nfor pid, dlength in enumerate(doclens):\n    emb2pid[offset_doclens: offset_doclens + dlength] = pid\n    offset_doclens += dlength\n\n\nemb2pid.shape\n\ntorch.Size([15198])\n\n\nThe first 4 token embeddings correspond to the first passage, the next 20 token embeddings to the second passage, and so on.\n\nemb2pid[:50]\n\ntensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n        3, 3], dtype=torch.int32)\n\n\nRecall that ivf contained as values the token embeddings IDs which are the indices of emb2pid. The values of emb2pid are passage IDs. Indexing into emb2pid with ivf pulls out the passage IDs corresponding to tokens. Note that ivf is sorted by centroid ID.\n\nivf\n\ntensor([  936,  1171,  2363,  ..., 12051, 12147, 12161], device='cuda:0')\n\n\n\nvalues\n\ntensor([   0,    0,    0,  ..., 1023, 1023, 1023], device='cuda:0')\n\n\n\nnew_ivf = emb2pid[ivf.cpu()]\nnew_ivf.shape, new_ivf[:5]\n\n(torch.Size([15198]), tensor([ 55,  69, 143, 416, 471], dtype=torch.int32))\n\n\n\nnew_ivf\n\ntensor([ 55,  69, 143,  ..., 795, 800, 800], dtype=torch.int32)\n\n\nThe first token embedding corresponding to centroid ID of 0 corresponds to passage ID 55.\n\nemb2pid[936]\n\ntensor(55, dtype=torch.int32)\n\n\nnew_ivf is a mapping from its indices (token embeddings) to values (passage IDs) which is now aligned to the ivf_lengths tensor which contains number of tokens per centroid ID (which came from values).\nNext, we iterate through ivf_lengths, which contains the number of tokens per centroid ID. For each length we get the unique passages IDs from new_ivf, and append that to unique_pids_per_centroid. The number of unique pids for that centroid is added to new_ivf_lengths.\n\nunique_pids_per_centroid = []\nnew_ivf_lengths = []\n\noffset = 0\nfor length in tqdm.tqdm(ivf_lengths.tolist()):\n    pids = torch.unique(new_ivf[offset:offset+length])\n    unique_pids_per_centroid.append(pids)\n    new_ivf_lengths.append(pids.shape[0])\n    offset += length\nivf = torch.cat(unique_pids_per_centroid)\nnew_ivf_lengths = torch.tensor(new_ivf_lengths)\n\n100%|██████████| 1024/1024 [00:00<00:00, 35975.77it/s]\n<ipython-input-138-6c68981e98f9>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_ivf_lengths = torch.tensor(ivf_lengths)\n\n\n\nivf.shape, new_ivf_lengths.shape\n\n(torch.Size([11593]), torch.Size([1024]))\n\n\nNote that there are now fewer values in ivf than 15198 since we are only capturing the unique pids per centroid.\n\nivf[:5]\n\ntensor([ 55,  69, 143, 416, 471], dtype=torch.int32)\n\n\n\nnew_ivf_lengths[:5]\n\ntensor([10, 11, 17,  1, 34], device='cuda:0')\n\n\nnew_ivf_lengths is the count of unique passage IDs per centroid. So, for example the first 10 pids correspond to centroid ID 0.\nivf and new_ivf_lengths would be stored in ivf.pid.pt.\nAfter updating metadata, this completes the indexing process in RAGatouille and ColBERT!"
  },
  {
    "objectID": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#final-thoughts",
    "href": "posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/index.html#final-thoughts",
    "title": "RAGatouille/ColBERT Indexing Deep Dive",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThere were of course many details that I didn’t fully explain in this walkthrough, and since I wasn’t able to exactly replicate some of the indexing artifacts there may be some errors in my code, but I think I both covered and understood the main components to creating an index. Getting to this stage involved a lot of discussion with Claude. I used AnswerAI’s toolslm to create context from the RAGatouille and ColBERT repos to provide as Claude project knowledge. I also pored through the codebase for hours, making sure to trace my steps from method-to-method. While I could do more deep dives into the individual components of indexing, I feel satisfied with this walk through for now. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2025-03-30-Composer-Callback/index.html",
    "href": "posts/2025-03-30-Composer-Callback/index.html",
    "title": "TIL: Creating a Custom Composer Callback",
    "section": "",
    "text": "I’m learning to use LLM-Foundry to finetune SLMs. To better understand what’s going on in the training loop when using Flash Attention 2 (for SmolLM2-135M), I decided to ask Claude to write me a custom callback. Here is my full Claude conversation."
  },
  {
    "objectID": "posts/2025-03-30-Composer-Callback/index.html#initial-plan",
    "href": "posts/2025-03-30-Composer-Callback/index.html#initial-plan",
    "title": "TIL: Creating a Custom Composer Callback",
    "section": "Initial Plan",
    "text": "Initial Plan\nAt first, I was planning to fork Composer (which I did), create a new branch for edits (print statements of datatypes in the Trainer code), and install that repo/branch for training. However, as I was chatting with Claude, it offered an option to write a callback instead. Being that this is a core philosophy of how Composer is built, it was a no brainer for me to pursue."
  },
  {
    "objectID": "posts/2025-03-30-Composer-Callback/index.html#first-callback",
    "href": "posts/2025-03-30-Composer-Callback/index.html#first-callback",
    "title": "TIL: Creating a Custom Composer Callback",
    "section": "First Callback",
    "text": "First Callback\nThe first callback Claude wrote (I guided it a little bit by feeding it Composer’s trainer.py and giving it their callback example from the docs) was as follows:\nclass WeightDtypeMonitor(Callback):\n    def __init__(self, backward_log_interval=5):\n        self.backward_log_interval = backward_log_interval\n    \n    def fit_start(self, state: State, logger: Logger) -> None:\n        self._log_dtypes(state, logger, \"fit_start\")\n    \n    def after_backward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.backward_log_interval == 0:\n            self._log_dtypes(state, logger, f\"backward_{state.timestamp.batch.value}\")\n    \n    def epoch_end(self, state: State, logger: Logger) -> None:\n        self._log_dtypes(state, logger, f\"epoch_{state.timestamp.epoch.value}\")\n    \n    def _log_dtypes(self, state: State, logger: Logger, prefix: str) -> None:\n        model = state.model\n        logger.log_metrics({\n            f\"dtype/{prefix}/lm_head\": str(model.model.base_model.model.lm_head.weight.dtype),\n            f\"dtype/{prefix}/q_proj_base\": str(model.model.base_model.model.model.layers[0].self_attn.q_proj.base_layer.weight.dtype),\n            f\"dtype/{prefix}/q_proj_lora_A\": str(model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight.dtype),\n            f\"dtype/{prefix}/q_proj_lora_B\": str(model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_B.default.weight.dtype)\n        })\nFor reference, here is the list of events available in Composer during the training loop:\n# <INIT>\n# <BEFORE_LOAD>\n# <AFTER_LOAD>\n# <FIT_START>\nfor epoch in range(NUM_EPOCHS):\n    # <EPOCH_START>\n    while True:\n        # <BEFORE_DATALOADER>\n        batch = next(dataloader)\n        if batch is None:\n            break\n        inputs, targets = batch\n        # <AFTER_DATALOADER>\n\n        # <BATCH_START>\n\n        # <BEFORE_FORWARD>\n        outputs = model.forward(inputs)\n        # <AFTER_FORWARD>\n\n        # <BEFORE_LOSS>\n        loss = model.loss(outputs, targets)\n        # <AFTER_LOSS>\n\n        # <BEFORE_BACKWARD>\n        loss.backward()\n        # <AFTER_BACKWARD>\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # <BATCH_END>\n    # <EPOCH_END>\nFor each event I wanted to log data types for, the callback passes state (where the model is stored), logger (to do the logging) and a prefix (to denote what’s being logged). Only every backward_log_interval-th batch’s backward pass is logged, to avoid clutter.\nHere is example output:\n# fit_start\nTrain dtype/fit_start/lm_head: \"torch.float32\"\nTrain dtype/fit_start/q_proj_base: \"torch.float32\"\nTrain dtype/fit_start/q_proj_lora_A: \"torch.float32\" \nTrain dtype/fit_start/q_proj_lora_B: \"torch.float32\"\n\n# after_backward\nTrain dtype/backward_0/lm_head: \"torch.float32\"\nTrain dtype/backward_0/q_proj_base: \"torch.float32\"\nTrain dtype/backward_0/q_proj_lora_A: \"torch.float32\"\nTrain dtype/backward_0/q_proj_lora_B: \"torch.float32\"\nI was surprised to see that everything was in float32, I thought Flash Attention 2 (FA2) used mixed precision? Note that I haven’t read the FA2 paper."
  },
  {
    "objectID": "posts/2025-03-30-Composer-Callback/index.html#second-callback",
    "href": "posts/2025-03-30-Composer-Callback/index.html#second-callback",
    "title": "TIL: Creating a Custom Composer Callback",
    "section": "Second Callback",
    "text": "Second Callback\nNow that I understood a basic logging callback, I asked Claude to generate a callback that would help me see where mixed precision came into play. This one was a bit more involved:\nclass DtypeMonitor(Callback):\n    def __init__(self, log_interval=100):\n        self.log_interval = log_interval\n        self.hooks = []\n    \n    def fit_start(self, state: State, logger: Logger) -> None:\n        self._log_weight_dtypes(state, logger, \"fit_start\")\n    \n    def before_forward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            # Log input tensor dtypes\n            if isinstance(state.batch, dict) and 'input_ids' in state.batch:\n                logger.log_metrics({\n                    \"dtype/input/input_ids\": str(state.batch['input_ids'].dtype)\n                })\n            \n            # Register hooks to capture activation dtypes\n            layer = state.model.model.base_model.model.model.layers[0].self_attn\n            \n            def hook_fn(name):\n                def _hook(module, inputs, outputs):\n                    # Log input activation dtype\n                    if isinstance(inputs, tuple) and len(inputs) > 0:\n                        logger.log_metrics({f\"dtype/activation/{name}_input\": str(inputs[0].dtype)})\n                    \n                    # Log output activation dtype\n                    if isinstance(outputs, torch.Tensor):\n                        logger.log_metrics({f\"dtype/activation/{name}_output\": str(outputs.dtype)})\n                    elif isinstance(outputs, tuple) and len(outputs) > 0:\n                        logger.log_metrics({f\"dtype/activation/{name}_output\": str(outputs[0].dtype)})\n                return _hook\n            \n            # Clear old hooks\n            for hook in self.hooks:\n                hook.remove()\n            self.hooks = []\n            \n            # Register new hooks\n            self.hooks.append(layer.q_proj.register_forward_hook(hook_fn(\"q_proj\")))\n            self.hooks.append(layer.register_forward_hook(hook_fn(\"self_attn\")))\n    \n    def after_forward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            # Log model output dtype\n            if isinstance(state.outputs, torch.Tensor):\n                logger.log_metrics({\n                    \"dtype/computation/output\": str(state.outputs.dtype)\n                })\n    \n    def after_loss(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            # Log loss dtype\n            if isinstance(state.loss, torch.Tensor):\n                logger.log_metrics({\n                    \"dtype/computation/loss\": str(state.loss.dtype)\n                })\n            elif isinstance(state.loss, dict) and 'total' in state.loss:\n                logger.log_metrics({\n                    \"dtype/computation/loss\": str(state.loss['total'].dtype)\n                })\n    \n    def after_backward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_weight_dtypes(state, logger, f\"backward_{state.timestamp.batch.value}\")\n            \n            # Check gradient dtypes\n            model = state.model\n            lora_A = model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default\n            if hasattr(lora_A, 'weight') and lora_A.weight.grad is not None:\n                logger.log_metrics({\n                    \"dtype/gradient/q_proj_lora_A\": str(lora_A.weight.grad.dtype)\n                })\n    \n    def epoch_end(self, state: State, logger: Logger) -> None:\n        self._log_weight_dtypes(state, logger, f\"epoch_{state.timestamp.epoch.value}\")\n        # Remove any remaining hooks\n        for hook in self.hooks:\n            hook.remove()\n        self.hooks = []\n    \n    def _log_weight_dtypes(self, state: State, logger: Logger, prefix: str) -> None:\n        model = state.model\n        logger.log_metrics({\n            f\"dtype/{prefix}/lm_head\": str(model.model.base_model.model.lm_head.weight.dtype),\n            f\"dtype/{prefix}/q_proj_base\": str(model.model.base_model.model.model.layers[0].self_attn.q_proj.base_layer.weight.dtype),\n            f\"dtype/{prefix}/q_proj_lora_A\": str(model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight.dtype),\n            f\"dtype/{prefix}/q_proj_lora_B\": str(model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_B.default.weight.dtype)\n        })\nFortunately, I had just recently learned about register_forward_hook and created a short TIL video about it:\n\nIn short, register_forward_hook exposes the forward pass inputs and outputs. You can manipulate both but you have access to inputs/outputs after the forward pass so you can’t change the inputs before they go into the forward pass. Thankfully that restriction doesn’t matter in my case, as I only want to log data types.\nRunning the training loop with this callback generated the following logs:\n Train dtype/input/input_ids: \"torch.int64\"\n Train dtype/activation/q_proj_input: \"torch.float32\"\n Train dtype/activation/q_proj_output: \"torch.bfloat16\"\n Train dtype/activation/self_attn_output: \"torch.bfloat16\"\n Train dtype/computation/loss: \"torch.float32\"\n Train dtype/backward_0/lm_head: \"torch.float32\"\n Train dtype/backward_0/q_proj_base: \"torch.float32\"\n Train dtype/backward_0/q_proj_lora_A: \"torch.float32\"\n Train dtype/backward_0/q_proj_lora_B: \"torch.float32\"\n Train dtype/gradient/q_proj_lora_A: \"torch.float32\"\nThis shed some more light into what’s going on! The inputs to q_proj is float32 but the outputs are bfloat16. The loss and gradients are both in float32."
  },
  {
    "objectID": "posts/2025-03-30-Composer-Callback/index.html#final-thoughts",
    "href": "posts/2025-03-30-Composer-Callback/index.html#final-thoughts",
    "title": "TIL: Creating a Custom Composer Callback",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis exercise has blown up the possibilities available to me for better understanding what goes on during training! I have only gotten a cursory glimpse at the internal mechanism of mixed precision training, but it’s relatively simple for me take this a step further by analyzing more data types during all training events for all model components. That’ll be a future blog post or video this week.\nThanks for reading! Lots more content on my YouTube channel that I’m working on growing this year so please subscribe to stay in the loop."
  },
  {
    "objectID": "posts/2025-04-01-Python-Descriptor/index.html",
    "href": "posts/2025-04-01-Python-Descriptor/index.html",
    "title": "Understanding Python Descriptors",
    "section": "",
    "text": "When monkey-patching the Llama self-attention forward pass (to log its inputs’ data type) I was vibe coding with Claude and it generated the following line to pass the necessary arguments to the original forward pass of the module:\norig_forward.__get__(self_attn, type(self_attn))(**kwargs)\nIn a prior iteration, I was using the following line suggested by Claude, with the intention of passing self_attn as self:\norig_forward(self_attn, *args, **kwargs)\nThis was essentially doing the following:\norig_forward(self_attn, hidden_states=hidden_states, attention_mask=attention_mask, ...)\nWhich caused the following error:\nTypeError: LlamaFlashAttention2.forward() got multiple values for argument 'hidden_states'\nself_attn was being passed as the argument to the hidden_states parameter, and then hidden_states=hidden_states was again assigning an argument to the hidden_states parameter. So how do we pass self_attn as self? This is where the __get__ method comes in which is part of the Python Descriptor. Descriptors are:\n\nAny object which defines the methods __get__(), __set__(), or __delete__(). When a class attribute is a descriptor, its special binding behavior is triggered upon attribute lookup. Normally, using a.b to get, set or delete an attribute looks up the object named b in the class dictionary for a, but if b is a descriptor, the respective descriptor method gets called. Understanding descriptors is a key to a deep understanding of Python because they are the basis for many features including functions, methods, properties, class methods, static methods, and reference to super classes.\n\nAfter reading that a few times I still didn’t understand it! Though I think the key is:\n\nWhen a class attribute is a descriptor, its special binding behavior is triggered upon attribute lookup.\n\nClaude explained it this way:\n\n__get__ is a special method that converts a function into a bound method. It’s like saying “make this function a method of this object.”\n\nTranslating that to my use case: __get__ makes orig_forward a method of self_attn, no longer requiring us to pass self_attn as it now is self.\nThat certainly makes sense (i.e. I understand those words) but I don’t really understand why or how. That led me to the Python documentation’s Descriptor Guide which I’ll walk through here.\n(There was also this interesting discussion about changing the name to __bind__ when calling it on a function as it binds the function as a method of the given object, which we’ll see later on)."
  },
  {
    "objectID": "posts/2025-04-01-Python-Descriptor/index.html#primer",
    "href": "posts/2025-04-01-Python-Descriptor/index.html#primer",
    "title": "Understanding Python Descriptors",
    "section": "Primer",
    "text": "Primer\n\nSimple example: A descriptor that returns a constant\n\nclass Ten:\n    def __get__(self, obj, objtype=None):\n        return 10\n\n\nt = Ten()\nt\n\n<__main__.Ten at 0x78b2fd072c50>\n\n\n\ntype(t)\n\n__main__.Ten\n\n\n\nt.__get__(4)\n\n10\n\n\nI think the only reason Ten is a descriptor is because it “defines the methods __get__(), __set__(), or __delete__()”.\n\nTo use the descriptor, it must be stored as a class variable in another class:\n\n\nclass A:\n    x = 5                       # Regular class attribute\n    y = Ten()                   # Descriptor instance\n\n\na = A()                     # Make an instance of class A\na\n\n<__main__.A at 0x78b2fd0707d0>\n\n\n\na.x                         # Normal attribute lookup\n\n5\n\n\n\na.y                         # Descriptor lookup\n\n10\n\n\n\nNote that the value 10 is not stored in either the class dictionary or the instance dictionary. Instead, the value 10 is computed on demand.\n\n\nA.__dict__\n\nmappingproxy({'__module__': '__main__',\n              'x': 5,\n              'y': <__main__.Ten at 0x78b2fd0722d0>,\n              '__dict__': <attribute '__dict__' of 'A' objects>,\n              '__weakref__': <attribute '__weakref__' of 'A' objects>,\n              '__doc__': None})\n\n\nModifying Ten a bit to visualize this:\n\nclass Ten2:\n    def __get__(self, obj, objtype=None):\n        print(f\"__get__ called with obj={obj}, objtype={objtype}\")\n        return 10\n\nclass A2:\n    x = 5\n    y = Ten2()  # Descriptor instance\n\n\na2 = A2()\n\n\na2.y\n\n__get__ called with obj=<__main__.A2 object at 0x78b2fd089710>, objtype=<class '__main__.A2'>\n\n\n10\n\n\nCool!\n\n\nDynamic Lookups\n\nimport os\n\nclass DirectorySize:\n\n    def __get__(self, obj, objtype=None):\n        return len(os.listdir(obj.dirname))\n\nclass Directory:\n\n    size = DirectorySize()              # Descriptor instance\n\n    def __init__(self, dirname):\n        self.dirname = dirname          # Regular instance attribute\n\n\ns = Directory('songs')\ng = Directory('games')\n\n\ns.size\n\n4\n\n\n\ng.size\n\n2\n\n\nRemoving a file then calling the descriptor’s __get__ dynamically calculates the new value:\n\nos.remove('games/game1.txt')            # Delete a game\ng.size\n\n1\n\n\n\n\nManaged attributes\n\nThe descriptor is assigned to a public attribute in the class dictionary while the actual data is stored as a private attribute in the instance dictionary.\n\nNote that I wasn’t able to see the logging output in this notebook so I’m using print statements instead.\n\nclass LoggedAgeAccess:\n\n    def __get__(self, obj, objtype=None):\n        value = obj._age\n        print(f'Accessing age giving {value}')\n        return value\n\n    def __set__(self, obj, value):\n        print(f'Updating age to {value}')\n        obj._age = value\n\nclass Person:\n\n    age = LoggedAgeAccess()             # Descriptor instance\n\n    def __init__(self, name, age):\n        self.name = name                # Regular instance attribute\n        self.age = age                  # Calls __set__()\n\n    def birthday(self):\n        self.age += 1                   # Calls both __get__() and __set__()\n\n\nmary = Person('Mary M', 30)         # The initial age update is logged\ndave = Person('David D', 40)\n\nUpdating age to 30\nUpdating age to 40\n\n\n\nvars(mary), vars(dave)\n\n({'name': 'Mary M', '_age': 30}, {'name': 'David D', '_age': 40})\n\n\n\nmary.age\n\nAccessing age giving 30\n\n\n30\n\n\n\nmary.birthday()\n\nAccessing age giving 30\nUpdating age to 31\n\n\n\nmary.age\n\nAccessing age giving 31\n\n\n31\n\n\n\ndave.name\n\n'David D'\n\n\n\ndave.age\n\nAccessing age giving 40\n\n\n40\n\n\n\n\nCustomized names\n\nWhen a class uses descriptors, it can inform each descriptor about which variable name was used.\n\n\nclass LoggedAccess:\n\n    def __set_name__(self, owner, name):\n        self.public_name = name\n        self.private_name = '_' + name\n\n    def __get__(self, obj, objtype=None):\n        value = getattr(obj, self.private_name)\n        print(f'Accessing {self.public_name} giving {value}')\n        return value\n\n    def __set__(self, obj, value):\n        print(f'Updating {self.public_name} to {value}')\n        setattr(obj, self.private_name, value)\n\nclass Person:\n\n    name = LoggedAccess()                # First descriptor instance\n    age = LoggedAccess()                 # Second descriptor instance\n\n    def __init__(self, name, age):\n        self.name = name                 # Calls the first descriptor\n        self.age = age                   # Calls the second descriptor\n\n    def birthday(self):\n        self.age += 1\n\n\nvars(Person)['name']\n\n<__main__.LoggedAccess at 0x78b2edeb8950>\n\n\n\nvars(vars(Person)['name'])\n\n{'public_name': 'name', 'private_name': '_name'}\n\n\n\nvars(vars(Person)['age'])\n\n{'public_name': 'age', 'private_name': '_age'}\n\n\n\npete = Person('Peter P', 10)\n\nUpdating name to Peter P\nUpdating age to 10\n\n\n\nkate = Person('Catherine C', 20)\n\nUpdating name to Catherine C\nUpdating age to 20\n\n\n\nvars(pete)\n\n{'_name': 'Peter P', '_age': 10}\n\n\n\nvars(kate)\n\n{'_name': 'Catherine C', '_age': 20}\n\n\nI think the main takeaway here is that we didn’t specify the name of the field so we could use the same descriptor for both name and age.\n\n\nClosing thoughts\nLooking at how __set_name__ behaves (the example in the docs):\n\nclass C:\n    def __set_name__(self, owner, name):\n        print(f\"__set_name__ called with owner={owner.__name__}, name='{name}'\")\n        self.name = name\n\nclass A:\n    x = C()  # This will trigger __set_name__\n    y = C()  # This will trigger it again with a different name\n    bananas = C()\n\n__set_name__ called with owner=A, name='x'\n__set_name__ called with owner=A, name='y'\n__set_name__ called with owner=A, name='bananas'\n\n\n\na = A()\na.x, a.y, a.x.name, a.y.name, a.bananas.name\n\n(<__main__.C at 0x78b331674190>,\n <__main__.C at 0x78b2df52ccd0>,\n 'x',\n 'y',\n 'bananas')\n\n\nThe part of particular interest to me is:\n\nDescriptors are used throughout the language. It is how functions turn into bound methods."
  },
  {
    "objectID": "posts/2025-04-01-Python-Descriptor/index.html#complete-practical-example",
    "href": "posts/2025-04-01-Python-Descriptor/index.html#complete-practical-example",
    "title": "Understanding Python Descriptors",
    "section": "Complete practical example",
    "text": "Complete practical example\n\nValidator class\n\nA validator is a descriptor for managed attribute access. Prior to storing any data, it verifies that the new value meets various type and range restrictions. If those restrictions aren’t met, it raises an exception to prevent data corruption at its source.\n\n\nfrom abc import ABC, abstractmethod\n\nclass Validator(ABC):\n\n    def __set_name__(self, owner, name):\n        print(\"__set_name__ is called\")\n        self.private_name = '_' + name\n\n    def __get__(self, obj, objtype=None):\n        print(\"__get__ is called\")\n        return getattr(obj, self.private_name)\n\n    def __set__(self, obj, value):\n        print(\"__set__ is called\")\n        self.validate(value)\n        setattr(obj, self.private_name, value)\n\n    @abstractmethod\n    def validate(self, value):\n        print(\"validate is called\")\n        pass\n\n\n\nCustom validators\n\nHere are three practical data validation utilities:\n\nOneOf verifies that a value is one of a restricted set of options.\nNumber verifies that a value is either an int or float. Optionally, it verifies that a value is between a given minimum or maximum.\nString verifies that a value is a str. Optionally, it validates a given minimum or maximum length. It can validate a user-defined predicate as well.\n\n\n\nclass OneOf(Validator):\n\n    def __init__(self, *options):\n        self.options = set(options)\n\n    def validate(self, value):\n        if value not in self.options:\n            raise ValueError(\n                f'Expected {value!r} to be one of {self.options!r}'\n            )\n\nclass Number(Validator):\n\n    def __init__(self, minvalue=None, maxvalue=None):\n        self.minvalue = minvalue\n        self.maxvalue = maxvalue\n\n    def validate(self, value):\n        if not isinstance(value, (int, float)):\n            raise TypeError(f'Expected {value!r} to be an int or float')\n        if self.minvalue is not None and value < self.minvalue:\n            raise ValueError(\n                f'Expected {value!r} to be at least {self.minvalue!r}'\n            )\n        if self.maxvalue is not None and value > self.maxvalue:\n            raise ValueError(\n                f'Expected {value!r} to be no more than {self.maxvalue!r}'\n            )\n\nclass String(Validator):\n\n    def __init__(self, minsize=None, maxsize=None, predicate=None):\n        self.minsize = minsize\n        self.maxsize = maxsize\n        self.predicate = predicate\n\n    def validate(self, value):\n        if not isinstance(value, str):\n            raise TypeError(f'Expected {value!r} to be an str')\n        if self.minsize is not None and len(value) < self.minsize:\n            raise ValueError(\n                f'Expected {value!r} to be no smaller than {self.minsize!r}'\n            )\n        if self.maxsize is not None and len(value) > self.maxsize:\n            raise ValueError(\n                f'Expected {value!r} to be no bigger than {self.maxsize!r}'\n            )\n        if self.predicate is not None and not self.predicate(value):\n            raise ValueError(\n                f'Expected {self.predicate} to be true for {value!r}'\n            )\n\n\n\nPractical application\n\nclass Component:\n\n    name = String(minsize=3, maxsize=10, predicate=str.isupper)\n    kind = OneOf('wood', 'metal', 'plastic')\n    quantity = Number(minvalue=0)\n\n    def __init__(self, name, kind, quantity):\n        self.name = name\n        self.kind = kind\n        self.quantity = quantity\n\n__set_name__ is called\n__set_name__ is called\n__set_name__ is called\n\n\n\nThe descriptors prevent invalid instances from being created:\n\n\nComponent('Widget', 'metal', 5)      # Blocked: 'Widget' is not all uppercase\n\n__set__ is called\n\n\nValueError: Expected <method 'isupper' of 'str' objects> to be true for 'Widget'\n\n\n\nComponent('WIDGET', 'metle', 5)      # Blocked: 'metle' is misspelled\n\n__set__ is called\n__set__ is called\n\n\nValueError: Expected 'metle' to be one of {'metal', 'plastic', 'wood'}\n\n\n\nComponent('WIDGET', 'metal', -5)     # Blocked: -5 is negative\n\n__set__ is called\n__set__ is called\n__set__ is called\n\n\nValueError: Expected -5 to be at least 0\n\n\n\nComponent('WIDGET', 'metal', 'V')    # Blocked: 'V' isn't a number\n\n__set__ is called\n__set__ is called\n__set__ is called\n\n\nTypeError: Expected 'V' to be an int or float\n\n\n\nc = Component('WIDGET', 'metal', 5)  # Allowed:  The inputs are valid\n\n__set__ is called\n__set__ is called\n__set__ is called\n\n\n\nc.name\n\n__get__ is called\n\n\n'WIDGET'"
  },
  {
    "objectID": "posts/2025-04-01-Python-Descriptor/index.html#technical-tutorial",
    "href": "posts/2025-04-01-Python-Descriptor/index.html#technical-tutorial",
    "title": "Understanding Python Descriptors",
    "section": "Technical tutorial",
    "text": "Technical tutorial\nAfter the reading the introduction of this guide I assumed I would skip the technical tutorial, expecting it to be too technical, but after skimming it I’ve decided to go through it as it might clear some things up for me and the following line was attractive:\n\nLearning about descriptors not only provides access to a larger toolset, it creates a deeper understanding of how Python works.\n\n\nDefinition and introduction\nReiterating the important definition that a descriptor is anything that has one of the methods in the descriptor protocol:\n\nIn general, a descriptor is an attribute value that has one of the methods in the descriptor protocol. Those methods are __get__(), __set__(), and __delete__(). If any of those methods are defined for an attribute, it is said to be a descriptor.\n\nAnd the main goal of descriptors:\n\nThe default behavior for attribute access is to get, set, or delete the attribute from an object’s dictionary.\n\n\n\nDescriptor protocol\nI don’t have any comments for this section other than reiterating the following points:\n\ndescr.__get__(self, obj, type=None)\ndescr.__set__(self, obj, value)\ndescr.__delete__(self, obj)\nThat is all there is to it. Define any of these methods and an object is considered a descriptor and can override default behavior upon being looked up as an attribute.\n\n\nIf an object defines __set__() or __delete__(), it is considered a data descriptor. Descriptors that only define __get__() are called non-data descriptors (they are often used for methods but other uses are possible).\n\n\n\nOverview of descriptor invocation\n\nA descriptor can be called directly with desc.__get__(obj) or desc.__get__(None, cls).\n\n\nBut it is more common for a descriptor to be invoked automatically from attribute access.\n\nWe saw this earlier, but putting that example here again:\n\nclass Ten2:\n    def __get__(self, obj, objtype=None):\n        print(f\"__get__ called with obj={obj}, objtype={objtype}\")\n        return 10\n\nclass A2:\n    x = 5\n    y = Ten2()  # Descriptor instance\n\na2 = A2()\na2.y\n\n__get__ called with obj=<__main__.A2 object at 0x78b2ded96890>, objtype=<class '__main__.A2'>\n\n\n10\n\n\n\n\nInvocation from an instance\n\nInstance lookup scans through a chain of namespaces giving data descriptors the highest priority, followed by instance variables, then non-data descriptors, then class variables, and lastly __getattr__() if it is provided.\n\nI’ve added some print statements in their example code to show which option is triggered:\n\ndef find_name_in_mro(cls, name, default):\n    \"Emulate _PyType_Lookup() in Objects/typeobject.c\"\n    for base in cls.__mro__:\n        if name in vars(base):\n            return vars(base)[name]\n    return default\n\ndef object_getattribute(obj, name):\n    \"Emulate PyObject_GenericGetAttr() in Objects/object.c\"\n    null = object()\n    objtype = type(obj)\n    cls_var = find_name_in_mro(objtype, name, null)\n    descr_get = getattr(type(cls_var), '__get__', null)\n    if descr_get is not null:\n        if (hasattr(type(cls_var), '__set__')\n            or hasattr(type(cls_var), '__delete__')):\n            print(\"returning data descriptor set/delete\")\n            return descr_get(cls_var, obj, objtype)     # data descriptor\n    if hasattr(obj, '__dict__') and name in vars(obj):\n        print(\"returning instance variable\")\n        return vars(obj)[name]                          # instance variable\n    if descr_get is not null:\n        print(\"returning descr_get\")\n        return descr_get(cls_var, obj, objtype)         # non-data descriptor\n    if cls_var is not null:\n        print(\"returning class variable\")\n        return cls_var                                  # class variable\n    raise AttributeError(name)\n\n\nobject_getattribute(a2, 'y')\n\nreturning descr_get\n__get__ called with obj=<__main__.A2 object at 0x78b2ded96890>, objtype=<class '__main__.A2'>\n\n\n10\n\n\n\nobject_getattribute(a2, 'x')\n\nreturning class variable\n\n\n5\n\n\n\ndef getattr_hook(obj, name):\n    \"Emulate slot_tp_getattr_hook() in Objects/typeobject.c\"\n    try:\n        print(\"__getattribute__\")\n        return obj.__getattribute__(name)\n    except AttributeError:\n        if not hasattr(type(obj), '__getattr__'):\n            raise\n    print(\"__getattr__\")\n    return type(obj).__getattr__(obj, name)\n\n\ngetattr_hook(a2, 'y')\n\n__getattribute__\n__get__ called with obj=<__main__.A2 object at 0x78b2ded96890>, objtype=<class '__main__.A2'>\n\n\n10\n\n\n\ngetattr_hook(a2, 'x')\n\n__getattribute__\n\n\n5\n\n\n\n\nInvocation from a class\n\nThe logic for a dotted lookup such as A.x is in type.__getattribute__().\n\n\nA2.__getattribute__??\n\nSignature:   A2.__getattribute__(*args, **kwargs)\nType:        wrapper_descriptor\nString form: <slot wrapper '__getattribute__' of 'object' objects>\nDocstring:   Return getattr(self, name).\n\nA2.__getattribute__(A2, 'y')\n\n<__main__.Ten2 at 0x78b2dee79310>\n\n\n\nA2.__getattribute__(A2, 'x')\n\n5\n\n\n\n\nInvocation from super\n\nA dotted lookup such as super(A, obj).m searches obj.__class__.__mro__ for the base class B immediately following A and then returns B.__dict__['m'].__get__(obj, A). If not a descriptor, m is returned unchanged.\n\n\nclass Base:\n    z = Ten2()  # Descriptor in the base class\n\nclass A2(Base):\n    x = 5\n    y = Ten2()  # Descriptor instance in A2\n\n    def show_super_lookup(self):\n        # This will trigger the descriptor lookup through super()\n        return super().z\n\n\na = A2()\na.y\n\n__get__ called with obj=<__main__.A2 object at 0x78b2dededa90>, objtype=<class '__main__.A2'>\n\n\n10\n\n\n\nsuper(A2, a).z\n\n__get__ called with obj=<__main__.A2 object at 0x78b2dededa90>, objtype=<class '__main__.A2'>\n\n\n10\n\n\n\nBase.__dict__['z'].__get__(a, A2)\n\n__get__ called with obj=<__main__.A2 object at 0x78b2dededa90>, objtype=<class '__main__.A2'>\n\n\n10\n\n\n\na.__class__.__mro__\n\n(__main__.A2, __main__.Base, object)\n\n\n\n\nSummary of invocation logic\nShowing examples of some of the bullet points in the summary:\n\nDescriptors are invoked by the __getattribute__() method.\n\n\na.__getattribute__('y')\n\n__get__ called with obj=<__main__.A2 object at 0x78b2dededa90>, objtype=<class '__main__.A2'>\n\n\n10\n\n\n\nOverriding __getattribute__() prevents automatic descriptor calls because all the descriptor logic is in that method.\n\n\nclass MyDescriptor:\n    def __get__(self, obj, objtype=None):\n        print(f\"Descriptor __get__ called!\")\n        return 42\n\nclass Normal:\n    x = MyDescriptor()\n\nn = Normal()\nn.x\n\nDescriptor __get__ called!\n\n\n42\n\n\n\nclass OverrideGetattribute:\n    x = MyDescriptor()\n    y = 5\n\n    def __getattribute__(self, name):\n        print(f\"Custom __getattribute__ called for {name}\")\n        if name == 'x':\n            return \"Bypassed descriptor\"\n        return object.__getattribute__(self, name)\n\no = OverrideGetattribute()\no.x\n\nCustom __getattribute__ called for x\n\n\n'Bypassed descriptor'\n\n\n\no.y\n\nCustom __getattribute__ called for y\n\n\n5\n\n\n\nobject.__getattribute__() and type.__getattribute__() make different calls to __get__(). The first includes the instance and may include the class. The second puts in None for the instance and always includes the class.\n\n\nclass DetailedDescriptor:\n    def __get__(self, obj, objtype=None):\n        print(f\"__get__ called with obj={obj}, objtype={objtype}\")\n        return 42\n\nclass Normal:\n    x = DetailedDescriptor()\n\nn = Normal()\n\n\nn.x\n\n__get__ called with obj=<__main__.Normal object at 0x78b2dedf0750>, objtype=<class '__main__.Normal'>\n\n\n42\n\n\n\nNormal.x\n\n__get__ called with obj=None, objtype=<class '__main__.Normal'>\n\n\n42\n\n\n\nData descriptors always override instance dictionaries.\n\n\nclass DataDescriptor:\n    def __init__(self, initial_value=None):\n        self.value = initial_value\n\n    def __get__(self, obj, objtype=None):\n        print(\"DataDescriptor.__get__ called\")\n        return self.value\n\n    def __set__(self, obj, value):\n        print(f\"DataDescriptor.__set__ called with value: {value}\")\n        self.value = value\n\nclass Example:\n    x = DataDescriptor(42)  # Data descriptor defined in class\n\n    def __init__(self):\n        # Try to override with instance attribute\n        self.__dict__['x'] = \"Instance value\"\n\n\nexample = Example()\n\n\nexample.__dict__\n\n{'x': 'Instance value'}\n\n\n\nexample.x\n\nDataDescriptor.__get__ called\n\n\n42\n\n\n\nexample.x = 100\nexample.__dict__['x']\n\nDataDescriptor.__set__ called with value: 100\n\n\n'Instance value'\n\n\n\nexample.x\n\nDataDescriptor.__get__ called\n\n\n100\n\n\n\nNon-data descriptors may be overridden by instance dictionaries.\n\n\nclass NonDataDescriptor:\n    def __init__(self, initial_value=None):\n        self.value = initial_value\n\n    def __get__(self, obj, objtype=None):\n        print(\"DataDescriptor.__get__ called\")\n        return self.value\n\nclass Example:\n    x = NonDataDescriptor(42)  # Data descriptor defined in class\n\n    def __init__(self):\n        # Try to override with instance attribute\n        self.__dict__['x'] = \"Instance value\"\n\n\nexample = Example()\n\n\nexample.__dict__\n\n{'x': 'Instance value'}\n\n\n\nexample.x\n\n'Instance value'\n\n\n\n\nAutomatic name notification\n\nSometimes it is desirable for a descriptor to know what class variable name it was assigned to. When a new class is created, the type metaclass scans the dictionary of the new class. If any of the entries are descriptors and if they define __set_name__(), that method is called with two arguments. The owner is the class where the descriptor is used, and the name is the class variable the descriptor was assigned to.\n\n\nclass NameTracker:\n   def __set_name__(self, owner, name): self.name = name\n\n\nclass_dict = {\n        'x': NameTracker(),\n        'y': NameTracker(),\n        'z': 5\n    }\n\n\nDemo = type('Demo', (), class_dict)\n\n\nDemo.x.name\n\n'x'\n\n\n\nDemo.y.name\n\n'y'\n\n\nI’m skipping the ORM example since I don’t have access to the example database."
  },
  {
    "objectID": "posts/2025-04-01-Python-Descriptor/index.html#pure-python-equivalents",
    "href": "posts/2025-04-01-Python-Descriptor/index.html#pure-python-equivalents",
    "title": "Understanding Python Descriptors",
    "section": "Pure Python Equivalents",
    "text": "Pure Python Equivalents\nFinally! The section I’m most interested in.\n\nProperties, bound methods, static methods, class methods, and __slots__ are all based on the descriptor protocol.\n\nI’m going to focus on the functions and methods section.\n\nFunctions and methods\n\nFunctions stored in class dictionaries get turned into methods when invoked. Methods only differ from regular functions in that the object instance is prepended to the other arguments. By convention, the instance is called self but could be called this or any other variable name.\n\n\nMethods can be created manually with types.MethodType which is roughly equivalent to:\n\n\nclass MethodType:\n    \"Emulate PyMethod_Type in Objects/classobject.c\"\n\n    def __init__(self, func, obj):\n        self.__func__ = func\n        self.__self__ = obj\n\n    def __call__(self, *args, **kwargs):\n        func = self.__func__\n        obj = self.__self__\n        return func(obj, *args, **kwargs)\n\n    def __getattribute__(self, name):\n        \"Emulate method_getset() in Objects/classobject.c\"\n        if name == '__doc__':\n            return self.__func__.__doc__\n        return object.__getattribute__(self, name)\n\n    def __getattr__(self, name):\n        \"Emulate method_getattro() in Objects/classobject.c\"\n        return getattr(self.__func__, name)\n\n    def __get__(self, obj, objtype=None):\n        \"Emulate method_descr_get() in Objects/classobject.c\"\n        return self\n\nThe key dunder method of interest is __call__:\ndef __call__(self, *args, **kwargs):\n    func = self.__func__\n    obj = self.__self__\n    return func(obj, *args, **kwargs)\nIn the example of the self attention module, it has no positional arguments *args and so when I passed self_attn to the obj parameter in func(obj, *args, **kwargs) it understood it to be the first keyword argument.\n\nThe interesting behavior occurs during dotted access from an instance. The dotted lookup calls get() which returns a bound method object:\n\n\nclass D:\n    def f(self):\n         return self\n\n\nd = D()\nprint(d.f)\n\n<bound method D.f of <__main__.D object at 0x78b2dec54790>>\n\n\n\nInternally, the bound method stores the underlying function and the bound instance:\n\n\nprint(d.f.__func__)\n\n<function D.f at 0x78b2dedd3ba0>\n\n\n\nprint(d.f.__self__)\n\n<__main__.D object at 0x78b2dec54790>\n\n\n\nIf you have ever wondered where self comes from in regular methods or where cls comes from in class methods, this is it!\n\n\n\nKinds of methods\nHere’s the crux of what I was looking for:\n\nTo recap, functions have a __get__() method so that they can be converted to a method when accessed as attributes. The non-data descriptor transforms an obj.f(*args) call into f(obj, *args). Calling cls.f(*args) becomes f(*args).\n\nIf I call __get__(d) on d.f it creates a bound method which passes in the object as self, the first argument of a bound method.\n\nprint(d.f.__get__(d))\n\n<bound method D.f of <__main__.D object at 0x78b2dec54790>>\n\n\nNow when I call d.f.__get__(d)() I don’t need to explicitly pass in the object:\n\nd.f.__get__(d)()\n\n<__main__.D at 0x78b2dec54790>"
  },
  {
    "objectID": "posts/2025-04-01-Python-Descriptor/index.html#final-thoughts",
    "href": "posts/2025-04-01-Python-Descriptor/index.html#final-thoughts",
    "title": "Understanding Python Descriptors",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThanks to vibe coding, Claude introduced me to Python behavior I was unfamiliar with, and thanks to the excellent Python documentation, I understood it at a much deeper level than I was planning to.\nI think something that still confuses me, and where I feel empathy for this poster, is how __get__ has special behavior for functions where it binds it to the given object.\nIn the Primer, initial examples of __get__ all, well, get a value:\ndef __get__(self, obj, objtype=None):\n    print(f\"__get__ called with obj={obj}, objtype={objtype}\")\n    return 10\n\n\ndef __get__(self, obj, objtype=None):\n    return len(os.listdir(obj.dirname))\n\n\ndef __get__(self, obj, objtype=None):\n    value = obj._age\n    print(f'Accessing age giving {value}')\n    return value\nHow that behavior is related to binding a function to an object is beyond my current understanding.\nThis poster’s response does make sense:\n\nIf descriptors were only callables that bind as methods when accessed as an attribute, then perhaps __bind__() would be a reasonable name for the method. But the descriptor protocol (i.e. __get__, __set__, and __delete__) is a means of implementing a computed attribute in general, which is not necessarily about binding a callable to the instance or type. For example, the __get__() method of a property named x might return the instance attribute _x.\n\nSo perhaps of a computed attributed is generalizable whether your using __get__ on a callable descriptor or otherwise. For a function, the “computation” of the attribute is binding it to the object.\nI hope you enjoyed this blog post! I’m trying to grow my YouTube channel so please give that a look/subscribe."
  },
  {
    "objectID": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html",
    "href": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html",
    "title": "Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry",
    "section": "",
    "text": "In a previous blog post I shared my first couple of iterations of custom Composer callback used to log data types of different entities (activations, gradients, weights, optimizer states, and loss) during training with LLM-Foundry. In this blog post I’ll share my final callback iteration’s code, some lessons I learned along the way (i.e. LLaMA’s self-attention module doesn’t have positional arguments!) and analyze the logging results to observe entity data types throughout the training loop."
  },
  {
    "objectID": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#composer-callback-walkthrough",
    "href": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#composer-callback-walkthrough",
    "title": "Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry",
    "section": "Composer Callback Walkthrough",
    "text": "Composer Callback Walkthrough\nThe data types of entities (activations, gradients, weights, loss, and optimizer states) are logged during training with a custom Composer callback DtypeLogger passed to the Composer Trainer. This callback was built up and tested event-by-event using Claude. There is one event handler in the callback for each Composer event from <FIT_START> to <BATCH_END>:\n# <INIT>\n# <BEFORE_LOAD>\n# <AFTER_LOAD>\n# <FIT_START>\nfor epoch in range(NUM_EPOCHS):\n    # <EPOCH_START>\n    while True:\n        # <BEFORE_DATALOADER>\n        batch = next(dataloader)\n        if batch is None:\n            break\n        inputs, targets = batch\n        # <AFTER_DATALOADER>\n\n        # <BATCH_START>\n\n        # <BEFORE_FORWARD>\n        outputs = model.forward(inputs)\n        # <AFTER_FORWARD>\n\n        # <BEFORE_LOSS>\n        loss = model.loss(outputs, targets)\n        # <AFTER_LOSS>\n\n        # <BEFORE_BACKWARD>\n        loss.backward()\n        # <AFTER_BACKWARD>\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # <BATCH_END>\n    # <EPOCH_END>\nThere are four explicit logging functions:\n\n_log_model_weight_dtypes\n_log_gradient_dtypes\n_log_optimizer_state_dtypes\n_log_loss_dtype\n\nAdditionally, activations are logged using register_forward_hook for all modules except self-attention (more on that below). Self-attention inputs are logged using a monkey-patched forward pass.\nclass DtypeLogger(Callback):\n    def __init__(self, save_path=\"/model-checkpoints/dtype_tracking\", log_interval=10):\n        self.save_path = Path(save_path)\n        self.dtype_logs = {'log': {}}\n        self.log_interval = log_interval\n        self.hooks = []\n        \n    def fit_start(self, state: State, logger: Logger) -> None:\n        self._log_model_weight_dtypes(state, \"fit_start\")\n        self._save_logs()\n        \n    def epoch_start(self, state: State, logger: Logger) -> None:\n        self._log_model_weight_dtypes(state, \"epoch_start\")\n        self._save_logs()\n    \n    def before_dataloader(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"before_dataloader\")\n            self._save_logs()\n            \n    def after_dataloader(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"after_dataloader\")\n            self._save_logs()\n            \n    def batch_start(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"batch_start\")\n            self._save_logs()\n            \n    def before_forward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"before_forward\")\n            \n            # Clear old hooks\n            for hook in self.hooks:\n                hook.remove()\n            self.hooks = []\n            \n            # Get the model\n            model = state.model.model.base_model.model\n            transformer_model = model.model  # This is the transformer part\n            batch_id = state.timestamp.batch.value\n            \n            # Store original forward methods to restore later\n            self.original_forward_methods = {}\n            \n            def hook_fn(layer_name, module_name):\n                def _hook(module, inputs, outputs):\n                    # Log input activation dtype\n                    if isinstance(inputs, tuple) and len(inputs) > 0:\n                        self.dtype_logs[\"log\"][f\"forward:{module_name}:{layer_name}:activation_input\"] = str(inputs[0].dtype)\n                    \n                    # Log output activation dtype\n                    if isinstance(outputs, torch.Tensor):\n                        self.dtype_logs[\"log\"][f\"forward:{module_name}:{layer_name}:activation_output\"] = str(outputs.dtype)\n                    elif isinstance(outputs, tuple) and len(outputs) > 0:\n                        self.dtype_logs[\"log\"][f\"forward:{module_name}:{layer_name}:activation_output\"] = str(outputs[0].dtype)\n                return _hook\n            \n            # Monkey patch self-attention modules\n            for layer_idx, layer in enumerate(transformer_model.layers):\n                # Store the original forward method\n                original_forward = layer.self_attn.forward\n                self.original_forward_methods[layer_idx] = original_forward\n                \n                # Define a closure to capture the current layer_idx\n                def make_patched_forward(layer_idx, orig_forward):\n                    def patched_forward(self_attn, *args, **kwargs):\n                        # Log the hidden_states dtype\n                        if 'hidden_states' in kwargs and hasattr(kwargs['hidden_states'], 'dtype'):\n                            self.dtype_logs[\"log\"][f\"forward:self_attn:layer_{layer_idx}:activation_input\"] = str(kwargs['hidden_states'].dtype)\n                        \n                        # Call the original method as a bound method\n                        # This ensures 'self_attn' is correctly passed as 'self'\n                        return orig_forward.__get__(self_attn, type(self_attn))(**kwargs)\n                    \n                    return patched_forward\n                \n                # Replace the forward method\n                layer.self_attn.forward = make_patched_forward(layer_idx, original_forward).__get__(layer.self_attn, type(layer.self_attn))\n            \n            # Register hook for lm_head\n            if hasattr(model, 'lm_head'):\n                self.hooks.append(model.lm_head.register_forward_hook(hook_fn(\"output\", \"lm_head\")))\n            \n            # Register hook for embedding layer\n            self.hooks.append(transformer_model.embed_tokens.register_forward_hook(hook_fn(\"embeddings\", \"embed_tokens\")))\n            \n            # Register hooks for each transformer layer\n            for layer_idx, layer in enumerate(transformer_model.layers):\n                # Self-attention components - we still register hooks for outputs\n                self.hooks.append(layer.self_attn.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"self_attn\")))\n                self.hooks.append(layer.self_attn.q_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"q_proj\")))\n                self.hooks.append(layer.self_attn.k_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"k_proj\")))\n                self.hooks.append(layer.self_attn.v_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"v_proj\")))\n                self.hooks.append(layer.self_attn.o_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"o_proj\")))\n                \n                # MLP components\n                self.hooks.append(layer.mlp.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"mlp\")))\n                self.hooks.append(layer.mlp.gate_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"gate_proj\")))\n                self.hooks.append(layer.mlp.up_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"up_proj\")))\n                self.hooks.append(layer.mlp.down_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"down_proj\")))\n                \n                # Layer norms\n                self.hooks.append(layer.input_layernorm.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"input_layernorm\")))\n                self.hooks.append(layer.post_attention_layernorm.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"post_attention_layernorm\")))\n            \n            # Final layer norm\n            self.hooks.append(transformer_model.norm.register_forward_hook(hook_fn(\"final\", \"norm\")))\n            \n            self._save_logs()\n            \n    def after_forward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"after_forward\")\n            \n            # Restore original forward methods\n            if hasattr(self, 'original_forward_methods'):\n                model = state.model.model.base_model.model\n                transformer_model = model.model\n                \n                for layer_idx, original_forward in self.original_forward_methods.items():\n                    transformer_model.layers[layer_idx].self_attn.forward = original_forward\n                \n                self.original_forward_methods = {}\n            \n            # Clear hooks\n            for hook in self.hooks:\n                hook.remove()\n            self.hooks = []\n            \n            self._save_logs()\n            \n    def before_loss(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"before_loss\")\n            self._save_logs()\n            \n    def after_loss(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"after_loss\")\n            self._log_loss_dtype(state, \"after_loss\")\n            self._save_logs()\n            \n    def before_backward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            self._log_model_weight_dtypes(state, \"before_backward\")\n            self._save_logs()\n            \n    def after_backward(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            # Log gradient dtypes as before\n            self._log_gradient_dtypes(state, \"after_backward\")\n            \n            # Track weight dtypes before optimizer step\n            self._log_model_weight_dtypes(state, \"before_optim_step\")\n            \n            # Log optimizer state dtypes\n            self._log_optimizer_state_dtypes(state, \"optimizer_step\")\n            \n            self._save_logs()\n                    \n    def batch_end(self, state: State, logger: Logger) -> None:\n        if state.timestamp.batch.value % self.log_interval == 0:\n            # Track weight dtypes after optimizer step to detect precision changes\n            self._log_model_weight_dtypes(state, \"after_optim_step\")\n            self._save_logs()\n\n    def epoch_end(self, state: State, logger: Logger) -> None:\n        self._log_model_weight_dtypes(state, \"epoch_end\")\n        self._save_logs()\n        \n    def _log_model_weight_dtypes(self, state: State, event_name: str) -> None:\n        model = state.model\n        for name, param in model.named_parameters():\n            name = name.removeprefix(\"model.base_model.model.model.\")\n            self.dtype_logs[\"log\"][f\"{event_name}:{name}:weights\"] = str(param.dtype)\n\n    def _log_gradient_dtypes(self, state: State, event_name: str) -> None:\n        model = state.model\n        for name, param in model.named_parameters():\n            name = name.removeprefix(\"model.base_model.model.model.\")\n            if param.grad is not None: self.dtype_logs['log'][f\"{event_name}:{name}:gradients\"] = str(param.grad.dtype)\n            else: self.dtype_logs['log'][f\"{event_name}:{name}:gradients\"] = \"None\"\n    \n    def _log_loss_dtype(self, state: State, event_name: str) -> None:\n        if hasattr(state, 'loss') and hasattr(state.loss, 'dtype'):\n            self.dtype_logs[\"log\"][f\"{event_name}:loss\"] = str(state.loss.dtype)\n            \n    def _log_optimizer_state_dtypes(self, state: State, event_name: str) -> None:\n        if hasattr(state, 'optimizers') and state.optimizers is not None:\n            # Handle single optimizer or list of optimizers\n            optimizers = state.optimizers if isinstance(state.optimizers, list) else [state.optimizers]\n            \n            for opt_idx, optimizer in enumerate(optimizers):\n                # Get optimizer state dict\n                opt_state = optimizer.state_dict()\n                \n                # Check if 'state' exists in the optimizer state dict\n                if 'state' in opt_state:\n                    for param_id, param_state in opt_state['state'].items():\n                        for state_name, state_value in param_state.items():\n                            if isinstance(state_value, torch.Tensor):\n                                # Store dtype of optimizer state tensors (momentum buffers, etc.)\n                                key = f\"optimizer_{opt_idx}_param_{param_id}_{state_name}\"\n                                self.dtype_logs[\"log\"][f\"{event_name}:{key}:optimizer_states\"] = str(state_value.dtype)\n            \n    def _save_logs(self) -> None:\n        os.makedirs(self.save_path, exist_ok=True)\n        log_file = self.save_path / \"dtype_logs.json\"\n        with open(log_file, 'w') as f:\n            json.dump(self.dtype_logs, f, indent=2)\nThe most involved event handler is before_forward which involves creating a hook function (hook_fn) passed to PyTorch’s register_forward_hook which exposes the positional inputs and outputs of a module’s forward pass. The hook function modifies self.dtype_logs directly by storing the data type string of inputs and outputs. hook_fn is used for all modules except self attention.\nSelf attention cannot utilize register_forward_hook because the LlamaDecoderLayer does not call self attention forward pass with any positional arguments:\nhidden_states, self_attn_weights = self.self_attn(\n    hidden_states=hidden_states,\n    attention_mask=attention_mask,\n    position_ids=position_ids,\n    past_key_value=past_key_value,\n    output_attentions=output_attentions,\n    use_cache=use_cache,\n    cache_position=cache_position,\n    position_embeddings=position_embeddings,\n    **kwargs,\n)\nContrast this with how the forward pass of other modules are called with positional arguments only:\n# self attention sublayers\nquery_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\nkey_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\nvalue_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\nattn_output = self.o_proj(attn_output)\n\n# mlp sublayers\ndown_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n# non-self attention modules\nhidden_states = self.input_layernorm(hidden_states)\nhidden_states = self.post_attention_layernorm(hidden_states)\nhidden_states = self.mlp(hidden_states)\nhidden_states = self.norm(hidden_states)\nSince self-attention inputs can’t be captured by a hook I had to monkey patch its forward pass to log its inputs’ data type:\nfor layer_idx, layer in enumerate(transformer_model.layers):\n    # Store the original forward method\n    original_forward = layer.self_attn.forward\n    self.original_forward_methods[layer_idx] = original_forward\n    \n    # Define a closure to capture the current layer_idx\n    def make_patched_forward(layer_idx, orig_forward):\n        def patched_forward(self_attn, *args, **kwargs):\n            # Log the hidden_states dtype\n            if 'hidden_states' in kwargs and hasattr(kwargs['hidden_states'], 'dtype'):\n                self.dtype_logs[\"log\"][f\"forward:self_attn:layer_{layer_idx}:activation_input\"] = str(kwargs['hidden_states'].dtype)\n            \n            # Call the original method as a bound method\n            # This ensures 'self_attn' is correctly passed as 'self'\n            return orig_forward.__get__(self_attn, type(self_attn))(**kwargs)\n        \n        return patched_forward\n\n    # Replace the forward method\n    layer.self_attn.forward = make_patched_forward(layer_idx, original_forward).__get__(layer.self_attn, type(layer.self_attn))\npatched_forward receives positional arguments *args (of which there are none) and keyword arguments **kwargs (all of the arguments to the self-attention forward) and logs the data types of the inputs to self-attention (hidden_states) as self_attn_input before returning the outputs of the original forward pass.\nA key line is orig_forward.__get__(self_attn, type(self_attn))(**kwargs). As Claude’s comment mentions, this is to avoid using orig_forward(self_attn, **kwargs) which was causing the following error because the first argument, self_attn, was being interpreted as hidden_states whereas it was intended to represent self:\nTypeError: LlamaFlashAttention2.forward() got multiple values for argument 'hidden_states'\nIn short, when you call __get__(obj, type) on a function it will bind that function as a method to the given object, thus no longer requiring you to pass in self as an argument. This is critical because self_attn.forward has no positional arguments. We can then pass in the keyword arguments to the bound method orig_forward.__get__(self_attn, type(self_attn))(**kwargs), and let the model continue using self-attention correctly. See the Descriptor Guide in the Python docs for more information."
  },
  {
    "objectID": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#helper-functions",
    "href": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#helper-functions",
    "title": "Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry",
    "section": "Helper Functions",
    "text": "Helper Functions\n\nimport re\nimport pandas as pd\nimport json\nimport requests\n\n\ndef parse_index(string):\n    \"\"\"Extract structured information from parameter names\"\"\"\n    info = {\n        'layer_number': None,\n        'module': None,\n        'layer_name': None,\n        'lora_layer': None,\n        'training_step': None,\n        'entity': None\n    }\n\n    # layer = string.split(\":\")[1]\n    # info[\"layer\"] = layer\n\n    layer_number_match = re.search(r'layers\\.(\\d+)', string)\n    if layer_number_match: info['layer_number'] = int(layer_number_match.group(1))\n\n    modules = [\n        \"embed_tokens\",\n        \"input_layernorm\",\n        \"self_attn\",\n        \"post_attention_layernorm\",\n        \"mlp\",\n        \"norm\",\n        \"lm_head\"\n    ]\n\n    module_match = re.search(r'(mlp|self_attn|input_layernorm|post_attention_layernorm|embed_tokens|norm|lm_head)', string)\n    if module_match: info['module'] = str(modules.index(module_match.group(1))).zfill(2) + '_' + module_match.group(1)\n\n    layer_name_match = re.search(r'(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)', string)\n    if layer_name_match: info['layer_name'] = layer_name_match.group(1)\n\n    lora_match = re.search(r'(base_layer|lora_A|lora_B)', string)\n    if lora_match: info['lora_layer'] = lora_match.group(1)\n    else: info['lora_layer'] = \"Not a LoRA Layer\"\n\n    training_steps = [\n        \"fit_start\",\n        \"epoch_start\",\n        \"before_dataloader\",\n        \"after_dataloader\",\n        \"batch_start\",\n        \"before_forward\",\n        \"forward\",\n        \"after_forward\",\n        \"before_loss\",\n        \"after_loss\",\n        \"before_backward\",\n        \"after_backward\",\n        \"before_optim_step\",\n        \"optimizer_step\",\n        \"after_optim_step\"\n        ]\n\n    training_step = string.split(\":\")[0]\n    info['training_step'] = str(training_steps.index(training_step)).zfill(2) + '_' + training_step\n\n    info['entity'] = string.split(\":\")[-1]\n\n\n    return info\n\n\ndef _df(url):\n    dtype_data = json.loads(requests.get(url).text)\n\n    df = pd.DataFrame(dtype_data).reset_index()\n    df = df.rename(columns={\"index\": \"index\", \"log\": \"dtype\"})\n\n    parsed_info = df['index'].apply(lambda x: parse_index(x))\n\n    df['layer_number'] = parsed_info.apply(lambda x: x['layer_number'])\n    df['module'] = parsed_info.apply(lambda x: x['module'])\n    df['layer_name'] = parsed_info.apply(lambda x: x['layer_name'])\n    df['lora_layer'] = parsed_info.apply(lambda x: x['lora_layer'])\n    df['training_step'] = parsed_info.apply(lambda x: x['training_step'])\n    df['entity'] = parsed_info.apply(lambda x: x['entity'])\n\n    return df"
  },
  {
    "objectID": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#model-in-fp32-master_weights_dtypenone",
    "href": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#model-in-fp32-master_weights_dtypenone",
    "title": "Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry",
    "section": "Model in fp32 (master_weights_dtype==None)",
    "text": "Model in fp32 (master_weights_dtype==None)\nIn this case, master_weights_dtype is not provided in the training YAML file.\n\nurl = \"https://gist.githubusercontent.com/vishalbakshi/9ade8d501629d4c30e8aecfa1c6f67cf/raw/0c162e2305002fbe57fd2570ade302c3659140a1/dtypes_logs_1ba_fp32.json\"\ndf = _df(url)\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      index\n      dtype\n      layer_number\n      module\n      layer_name\n      lora_layer\n      training_step\n      entity\n    \n  \n  \n    \n      0\n      fit_start:embed_tokens.weight:weights\n      torch.float32\n      NaN\n      00_embed_tokens\n      None\n      Not a LoRA Layer\n      00_fit_start\n      weights\n    \n    \n      1\n      fit_start:layers.0.self_attn.q_proj.base_layer...\n      torch.float32\n      0.0\n      02_self_attn\n      q_proj\n      base_layer\n      00_fit_start\n      weights\n    \n    \n      2\n      fit_start:layers.0.self_attn.q_proj.lora_A.def...\n      torch.float32\n      0.0\n      02_self_attn\n      q_proj\n      lora_A\n      00_fit_start\n      weights\n    \n    \n      3\n      fit_start:layers.0.self_attn.q_proj.lora_B.def...\n      torch.float32\n      0.0\n      02_self_attn\n      q_proj\n      lora_B\n      00_fit_start\n      weights\n    \n    \n      4\n      fit_start:layers.0.self_attn.k_proj.base_layer...\n      torch.float32\n      0.0\n      02_self_attn\n      k_proj\n      base_layer\n      00_fit_start\n      weights\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nData Types by lora_layer\nAll LoRA layer entities are in fp32.\n\ndf.groupby(['lora_layer', 'dtype'])['dtype'].count()\n\n\n\n\n\n  \n    \n      \n      \n      dtype\n    \n    \n      lora_layer\n      dtype\n      \n    \n  \n  \n    \n      Not a LoRA Layer\n      None\n      62\n    \n    \n      torch.bfloat16\n      331\n    \n    \n      torch.float32\n      2339\n    \n    \n      torch.int64\n      1\n    \n    \n      base_layer\n      None\n      210\n    \n    \n      torch.float32\n      2520\n    \n    \n      lora_A\n      torch.float32\n      2730\n    \n    \n      lora_B\n      torch.float32\n      2730\n    \n  \n\ndtype: int64\n\n\n\n\nData Types by entity (Activations, Gradients, Loss, Optimizer States and Weights)\nEvery entity except activations are in fp32. Some parameters don’t have gradients because we are training with LoRA.\n\ndf.groupby(['entity', 'dtype'])['dtype'].count()\n\n\n\n\n\n  \n    \n      \n      \n      dtype\n    \n    \n      entity\n      dtype\n      \n    \n  \n  \n    \n      activation_input\n      torch.bfloat16\n      60\n    \n    \n      torch.float32\n      272\n    \n    \n      torch.int64\n      1\n    \n    \n      activation_output\n      torch.bfloat16\n      271\n    \n    \n      torch.float32\n      62\n    \n    \n      gradients\n      None\n      272\n    \n    \n      torch.float32\n      420\n    \n    \n      loss\n      torch.float32\n      1\n    \n    \n      optimizer_states\n      torch.float32\n      1260\n    \n    \n      weights\n      torch.float32\n      8304\n    \n  \n\ndtype: int64\n\n\n\n\nData Types by Composer Training Step\n\ndf.groupby(['training_step', 'entity', 'dtype'])['dtype'].count()\n\n\n\n\n\n  \n    \n      \n      \n      \n      dtype\n    \n    \n      training_step\n      entity\n      dtype\n      \n    \n  \n  \n    \n      00_fit_start\n      weights\n      torch.float32\n      692\n    \n    \n      01_epoch_start\n      weights\n      torch.float32\n      692\n    \n    \n      02_before_dataloader\n      weights\n      torch.float32\n      692\n    \n    \n      03_after_dataloader\n      weights\n      torch.float32\n      692\n    \n    \n      04_batch_start\n      weights\n      torch.float32\n      692\n    \n    \n      05_before_forward\n      weights\n      torch.float32\n      692\n    \n    \n      06_forward\n      activation_input\n      torch.bfloat16\n      60\n    \n    \n      torch.float32\n      272\n    \n    \n      torch.int64\n      1\n    \n    \n      activation_output\n      torch.bfloat16\n      271\n    \n    \n      torch.float32\n      62\n    \n    \n      07_after_forward\n      weights\n      torch.float32\n      692\n    \n    \n      08_before_loss\n      weights\n      torch.float32\n      692\n    \n    \n      09_after_loss\n      loss\n      torch.float32\n      1\n    \n    \n      weights\n      torch.float32\n      692\n    \n    \n      10_before_backward\n      weights\n      torch.float32\n      692\n    \n    \n      11_after_backward\n      gradients\n      None\n      272\n    \n    \n      torch.float32\n      420\n    \n    \n      12_before_optim_step\n      weights\n      torch.float32\n      692\n    \n    \n      13_optimizer_step\n      optimizer_states\n      torch.float32\n      1260\n    \n    \n      14_after_optim_step\n      weights\n      torch.float32\n      692\n    \n  \n\ndtype: int64"
  },
  {
    "objectID": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#model-in-bf16-master_weights_dtypebfloat16",
    "href": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#model-in-bf16-master_weights_dtypebfloat16",
    "title": "Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry",
    "section": "Model in bf16 (master_weights_dtype==bfloat16)",
    "text": "Model in bf16 (master_weights_dtype==bfloat16)\nI also logged data types after setting master_weights_dtype in the training YAML to bfloat16.\n\nurl = \"https://gist.githubusercontent.com/vishalbakshi/ec91a59754633611fd8eb33b59031243/raw/5b83a7ebd5759cf6bd2db2369edf1c73e1fb67cf/dtypes_logs_1ba_bf16.json\"\ndf = _df(url)\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      index\n      dtype\n      layer_number\n      module\n      layer_name\n      lora_layer\n      training_step\n      entity\n    \n  \n  \n    \n      0\n      fit_start:embed_tokens.weight:weights\n      torch.bfloat16\n      NaN\n      00_embed_tokens\n      None\n      Not a LoRA Layer\n      00_fit_start\n      weights\n    \n    \n      1\n      fit_start:layers.0.self_attn.q_proj.base_layer...\n      torch.bfloat16\n      0.0\n      02_self_attn\n      q_proj\n      base_layer\n      00_fit_start\n      weights\n    \n    \n      2\n      fit_start:layers.0.self_attn.q_proj.lora_A.def...\n      torch.bfloat16\n      0.0\n      02_self_attn\n      q_proj\n      lora_A\n      00_fit_start\n      weights\n    \n    \n      3\n      fit_start:layers.0.self_attn.q_proj.lora_B.def...\n      torch.bfloat16\n      0.0\n      02_self_attn\n      q_proj\n      lora_B\n      00_fit_start\n      weights\n    \n    \n      4\n      fit_start:layers.0.self_attn.k_proj.base_layer...\n      torch.bfloat16\n      0.0\n      02_self_attn\n      k_proj\n      base_layer\n      00_fit_start\n      weights\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nData Type by lora_layer\nInterestingly, setting master_weights_dtype makes all LoRA layers bfloat16 but some non-LoRA layers’ entities are still in fp32.\n\ndf.groupby(['lora_layer', 'dtype'])['dtype'].count()\n\n\n\n\n\n  \n    \n      \n      \n      dtype\n    \n    \n      lora_layer\n      dtype\n      \n    \n  \n  \n    \n      Not a LoRA Layer\n      None\n      62\n    \n    \n      torch.bfloat16\n      2249\n    \n    \n      torch.float32\n      421\n    \n    \n      torch.int64\n      1\n    \n    \n      base_layer\n      None\n      210\n    \n    \n      torch.bfloat16\n      2520\n    \n    \n      lora_A\n      torch.bfloat16\n      2730\n    \n    \n      lora_B\n      torch.bfloat16\n      2730\n    \n  \n\ndtype: int64\n\n\n\n\nData Types by entity (Activations, Gradients, Loss, Optimizer States and Weights)\nAll floating point values are in bfloat16 except for the loss and some of the optimizer states. I’m not sure why some optimizer states are in bf16, even though it says in the Composer docs:\n\nStore the weights and perform the optimizer step in single precision, enabling the weight update to be done more precisely.\n\n\ndf.groupby(['entity', 'dtype'])['dtype'].count()\n\n\n\n\n\n  \n    \n      \n      \n      dtype\n    \n    \n      entity\n      dtype\n      \n    \n  \n  \n    \n      activation_input\n      torch.bfloat16\n      332\n    \n    \n      torch.int64\n      1\n    \n    \n      activation_output\n      torch.bfloat16\n      333\n    \n    \n      gradients\n      None\n      272\n    \n    \n      torch.bfloat16\n      420\n    \n    \n      loss\n      torch.float32\n      1\n    \n    \n      optimizer_states\n      torch.bfloat16\n      840\n    \n    \n      torch.float32\n      420\n    \n    \n      weights\n      torch.bfloat16\n      8304\n    \n  \n\ndtype: int64\n\n\n\n\nData Type by Composer Training Step\n\ndf.groupby(['training_step', 'entity', 'dtype'])['dtype'].count()\n\n\n\n\n\n  \n    \n      \n      \n      \n      dtype\n    \n    \n      training_step\n      entity\n      dtype\n      \n    \n  \n  \n    \n      00_fit_start\n      weights\n      torch.bfloat16\n      692\n    \n    \n      01_epoch_start\n      weights\n      torch.bfloat16\n      692\n    \n    \n      02_before_dataloader\n      weights\n      torch.bfloat16\n      692\n    \n    \n      03_after_dataloader\n      weights\n      torch.bfloat16\n      692\n    \n    \n      04_batch_start\n      weights\n      torch.bfloat16\n      692\n    \n    \n      05_before_forward\n      weights\n      torch.bfloat16\n      692\n    \n    \n      06_forward\n      activation_input\n      torch.bfloat16\n      332\n    \n    \n      torch.int64\n      1\n    \n    \n      activation_output\n      torch.bfloat16\n      333\n    \n    \n      07_after_forward\n      weights\n      torch.bfloat16\n      692\n    \n    \n      08_before_loss\n      weights\n      torch.bfloat16\n      692\n    \n    \n      09_after_loss\n      loss\n      torch.float32\n      1\n    \n    \n      weights\n      torch.bfloat16\n      692\n    \n    \n      10_before_backward\n      weights\n      torch.bfloat16\n      692\n    \n    \n      11_after_backward\n      gradients\n      None\n      272\n    \n    \n      torch.bfloat16\n      420\n    \n    \n      12_before_optim_step\n      weights\n      torch.bfloat16\n      692\n    \n    \n      13_optimizer_step\n      optimizer_states\n      torch.bfloat16\n      840\n    \n    \n      torch.float32\n      420\n    \n    \n      14_after_optim_step\n      weights\n      torch.bfloat16\n      692\n    \n  \n\ndtype: int64"
  },
  {
    "objectID": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#final-thoughts",
    "href": "posts/2025-04-02-Composer-Callback-Logging-dtypes/index.html#final-thoughts",
    "title": "Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI absolutely loved this exercise. I learned a ton about callbacks, data types during mixed precision training, and Python fundamentals. Working with LLM-Foundry has opened up a whole universe of learning opportunities as I try to better understand what’s going on under the hood. It’s a gift that keeps giving!\nI’m trying to grow my YouTube channel so please give it a visit and subscribe if you want to stay in the loop."
  },
  {
    "objectID": "posts/2025-04-21-Matrix-Multiplication/index.html",
    "href": "posts/2025-04-21-Matrix-Multiplication/index.html",
    "title": "Optimizing Matrix Multiplication Using Numba and Broadcasting",
    "section": "",
    "text": "In this notebook I’ll solidy the matrix multiplication concepts taught in Lesson 11 of the fastai course (part 2). Most importantly, I want to make sure I understand the use of broadcasting to make the matmul operation 7000x faster!\nHere’s a summary of run times for the five methods explored in this blog post:\n\n\n\nMethod\nImages\nRun Time (ms)\n\n\n\n\nPython for-loops\n5\n1090ms\n\n\nNumba-compiled Dot Product\n5\n0.555ms\n\n\nPython Dot Product\n5\n1.47ms\n\n\nPyTorch Dot Product\n5\n1.22ms\n\n\nPyTorch Broadcasting\n5\n0.158ms\n\n\nNumba-compiled Broadcasting\n5\n0.0936ms\n\n\n\nHere’s my video walkthrough of the code in this notebook:"
  },
  {
    "objectID": "posts/2025-04-21-Matrix-Multiplication/index.html#load-the-data",
    "href": "posts/2025-04-21-Matrix-Multiplication/index.html#load-the-data",
    "title": "Optimizing Matrix Multiplication Using Numba and Broadcasting",
    "section": "Load the Data",
    "text": "Load the Data\nWe’ll use the MNIST dataset for this exercise.\n\nimport torch\nfrom torch import tensor\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt\nfrom urllib.request import urlretrieve\nimport numpy as np\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 root root 17051982 Apr 21 22:56 mnist.pkl.gz\n\n\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\n\n\n\n\nFor our weights, we’ll create a set of random floats with shape 784 (rows) x 10 (columns). In an applied sense, these 10 outputs would be the logits associated with the ten possible digits (0-9) for each 28x28 image.\n\ntorch.manual_seed(1)\nweights = torch.randn(784, 10)\nweights.shape\n\ntorch.Size([784, 10])\n\n\nFor our inputs (which get multiplied by our weights) we’ll use the first 5 digits (28x28 images) from the validation set. These inputs and our weights are the two matrices we want to multiply!\n\nm1 = x_valid[:5]\nm2 = weights\n\n\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))"
  },
  {
    "objectID": "posts/2025-04-21-Matrix-Multiplication/index.html#initial-solution-python-for-loops",
    "href": "posts/2025-04-21-Matrix-Multiplication/index.html#initial-solution-python-for-loops",
    "title": "Optimizing Matrix Multiplication Using Numba and Broadcasting",
    "section": "Initial Solution: Python for-Loops",
    "text": "Initial Solution: Python for-Loops\n\n\n\nNaive implementation of matrix multiplication\n\n\nFor our first iteration, we’ll do a nested for-loop—the most naive implementation of matrix multiplication in this exercise.\nWe iterate through the 5 rows of our input matrix (images). For each row, we iterate through each column of our weights matrix. For each of the 784 elements in that row/column (i,j) combination, we take the dot product and store it in the output matrix. 5 images x 10 outputs x 784 elements = 39200 total items operated on.\n\n5*10*784\n\n39200\n\n\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc) # resultant tensor\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar): #5\n    for j in range(bc): # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\nThe resulting matrix has 5 rows (1 for each image) and 10 columns (one for each “neuron” in our weights matrix).\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n\nWrapping this code into a function we can time it.\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\n%time _=matmul(m1, m2)\n\nCPU times: user 1.09 s, sys: 544 µs, total: 1.09 s\nWall time: 1.09 s\n\n\nIt takes a whopping 1.09 seconds to perform this matrix multiplication for 5 images! Let’s optimize this with numba."
  },
  {
    "objectID": "posts/2025-04-21-Matrix-Multiplication/index.html#compiling-the-dot-product-with-numba",
    "href": "posts/2025-04-21-Matrix-Multiplication/index.html#compiling-the-dot-product-with-numba",
    "title": "Optimizing Matrix Multiplication Using Numba and Broadcasting",
    "section": "Compiling the Dot Product with Numba",
    "text": "Compiling the Dot Product with Numba\n\n\n\nMatrix multiplication using a numba-compiled dot product operation\n\n\nTo reduce the number of python for-loops, we write the dot product (between the two 784-element vectors) in numba:\n\nfrom numba import njit\n\n\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\nThe first run of dot takes longer as it includes the compile time:\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 123 ms, sys: 0 ns, total: 123 ms\nWall time: 124 ms\n\n\n20.0\n\n\nThe second run is 250x times faster.\n\n0.124/0.000489\n\n253.5787321063395\n\n\n\n%time dot(array([1.,2,3]),array([2.,3,4]))\n\nCPU times: user 40 µs, sys: 5 µs, total: 45 µs\nWall time: 48.9 µs\n\n\n20.0\n\n\nWe replace the third for-loop with our numba dot function:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\n\nfrom fastcore.test import *\n\nWe test that it yields the same result:\n\ntest_close(t1,matmul(m1a, m2a))\n\n\n%timeit -n 50 matmul(m1a,m2a)\n\n555 µs ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nOur numba-compiled dot operation makes our matrix multiplication 2000x faster!\n\n1.09/555e-6\n\n1963.963963963964\n\n\nThe same operation can be done in Python:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\nBut it’s three times slower than numba:\n\n%timeit -n 50 _=matmul(m1, m2)\n\n1.47 ms ± 32.3 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nUsing torch.dot is a smidge faster than Python:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = torch.dot(a[i,:], b[:,j])\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n1.22 ms ± 39.1 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)"
  },
  {
    "objectID": "posts/2025-04-21-Matrix-Multiplication/index.html#faster-use-broadcasting",
    "href": "posts/2025-04-21-Matrix-Multiplication/index.html#faster-use-broadcasting",
    "title": "Optimizing Matrix Multiplication Using Numba and Broadcasting",
    "section": "Faster: Use Broadcasting!",
    "text": "Faster: Use Broadcasting!\n\n\n\nUsing broadcasting to compute all image/weight dot products simultaneously!\n\n\nBroadcasting effectively expands the smaller matrix to match the size of the larger one so that element-wise operations can take place.\nSuppose we wanted to take the dot product between the first image and all 10 columns of weights. Adding a None during indexing adds a unit axis at that position:\n\nm1[0,:].shape\n\ntorch.Size([784])\n\n\n\nm1[0, :, None].shape\n\ntorch.Size([784, 1])\n\n\n\nm2.shape\n\ntorch.Size([784, 10])\n\n\nMultiplying (element-wise) m1[0, :, None] with m2 broadcasts m1[0, :, None] across the 10 columns of m2. In other words, each row of m1[0] is virtually copied over 10 times, one for each column of m2.\n\n(m1[0, :, None] * m2).shape\n\ntorch.Size([784, 10])\n\n\n\nm1.shape\n\ntorch.Size([5, 784])\n\n\n\nm1\n\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\nHere’s a smaller example. a has 5 rows, “images”, each with 4 pixels.\n\na = torch.randint(low=1, high=5, size=(5,4))\na\n\ntensor([[1, 4, 3, 4],\n        [1, 4, 1, 3],\n        [3, 2, 2, 4],\n        [3, 1, 3, 1],\n        [2, 3, 1, 1]])\n\n\nWe pluck out the first “image” with 0, then add a unit axis at the end with None to make it “broadcastable”\n\na[0, :, None]\n\ntensor([[1],\n        [4],\n        [3],\n        [4]])\n\n\n\na.shape, a[0, :, None].shape\n\n(torch.Size([5, 4]), torch.Size([4, 1]))\n\n\nSuppose we have weights w with 4 rows, each 10 columns wide.\n\nw = torch.randint(low=1, high=5, size=(4, 10))\nw\n\ntensor([[2, 2, 1, 1, 4, 3, 4, 2, 4, 3],\n        [1, 2, 4, 2, 4, 1, 3, 1, 2, 1],\n        [4, 3, 4, 3, 1, 2, 1, 3, 3, 4],\n        [1, 3, 3, 3, 3, 1, 1, 1, 4, 4]])\n\n\nWe broadcast the 4-vector a[0, :, None] across all 10 4-vectors in w:\n\na[0, :, None] * w\n\ntensor([[ 2,  2,  1,  1,  4,  3,  4,  2,  4,  3],\n        [ 4,  8, 16,  8, 16,  4, 12,  4,  8,  4],\n        [12,  9, 12,  9,  3,  6,  3,  9,  9, 12],\n        [ 4, 12, 12, 12, 12,  4,  4,  4, 16, 16]])\n\n\nThen take the sum down the columns (along the row axis) to get the 10 output “activations” for this “image”:\n\n(a[0, :, None] * w).sum(dim=0)\n\ntensor([22, 31, 41, 30, 35, 17, 23, 19, 37, 35])\n\n\nLooking at the first value of 22, it comes from the dot product between the first “image” in a and the first row of weights (the “neuron”) in w:\n22 = 1*2 + 4*1 + 3*4 + 4*1 = 2 + 4 + 12 + 4\nIn this way, we have the dot product between the first image and all 10 columns. This is the first row of the matrix product between a and w.\nWe can then loop over the images, broadcasting it across the weight matrix, summing down the columns to get each subsequent row of our resultant matrix product:\n\n(ar,ac),(wr,wc) = a.shape,w.shape\nar,ac,wr,wc\n\n(5, 4, 4, 10)\n\n\n\nc = torch.zeros(ar, wc)\nc.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):\n    c[i] = (a[i, :, None] * w).sum(dim=0)\nc\n\ntensor([[22., 31., 41., 30., 35., 17., 23., 19., 37., 35.],\n        [13., 22., 30., 21., 30., 12., 20., 12., 27., 23.],\n        [20., 28., 31., 25., 34., 19., 24., 18., 38., 35.],\n        [20., 20., 22., 17., 22., 17., 19., 17., 27., 26.],\n        [12., 16., 21., 14., 24., 12., 19., 11., 21., 17.]])\n\n\nIn this way, we have performed matrix multiplication by taking the dot product of each row/column using broadcasting! Returning to our original dataset:\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar): c[i]  = (a[i,:,None] * b).sum(dim=0)\n    return c\n\n\ntest_close(t1,matmul(m1, m2))\n\n\n%timeit -n 50 _=matmul(m1, m2)\n\n158 µs ± 15.1 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nThis gives us a 8x speedup from the numba-compiled dot product (1.22ms –> 0.158 ms).\nNow, instead of 5 images we can perform matmul with all 50k images in our dataset.\n\ntr = matmul(x_train, weights)\ntr\n\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n\n\n\ntr.shape\n\ntorch.Size([50000, 10])\n\n\nThis operation now takes less than two seconds!\n\n%time _=matmul(x_train, weights)\n\nCPU times: user 1.62 s, sys: 0 ns, total: 1.62 s\nWall time: 1.63 s"
  },
  {
    "objectID": "posts/2025-04-21-Matrix-Multiplication/index.html#fastest-numba-broadcasting",
    "href": "posts/2025-04-21-Matrix-Multiplication/index.html#fastest-numba-broadcasting",
    "title": "Optimizing Matrix Multiplication Using Numba and Broadcasting",
    "section": "Fastest: Numba Broadcasting",
    "text": "Fastest: Numba Broadcasting\n\nm1a.shape, m2a.shape\n\n((5, 784), (784, 10))\n\n\n\n@njit\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = np.zeros((ar, bc))\n    for i in range(ar): c[i] = (a[i,:,None] * b).sum(axis=0)\n    return c\n\n\ntest_close(t1,matmul(m1a, m2a))\n\n\n%timeit -n 50 _=matmul(m1a, m2a)\n\n93.6 µs ± 9.26 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nWe can now perform matrix multiplication for all 50_000 images in less time than we could for 5 images using nested for-loops. AMAZING!\n\n%time _=matmul(x_train.numpy(), weights.numpy())\n\nCPU times: user 885 ms, sys: 0 ns, total: 885 ms\nWall time: 881 ms"
  },
  {
    "objectID": "posts/2025-04-21-Matrix-Multiplication/index.html#final-thoughts",
    "href": "posts/2025-04-21-Matrix-Multiplication/index.html#final-thoughts",
    "title": "Optimizing Matrix Multiplication Using Numba and Broadcasting",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’ve been busy with other ML projects over the past few months but I’m so glad I have gotten back in the driver’s seat for fastai course part 2! The videos, content, and potential projects/exercises that spring forth are absolutely delicious. Using relatively simple building blocks, I was able to understand matrix multiplication through Python loops, numba dot product, and Yorick-inspired PyTorch broadcasting. Creating the visuals (in excalidraw) was a must because I really needed to cement these concepts in my mind, as encouraged by Jeremy in the video.\nHere’s the summary again of run times for each of the methods shown above:\n\n\n\nMethod\nImages\nRun Time (ms)\n\n\n\n\nPython for-loops\n5\n1090ms\n\n\nNumba-compiled Dot Product\n5\n0.555ms\n\n\nPython Dot Product\n5\n1.47ms\n\n\nPyTorch Dot Product\n5\n1.22ms\n\n\nPyTorch Broadcasting\n5\n0.158ms\n\n\nNumba-compiled Broadcasting\n5\n0.0936ms\n\n\n\nUsing numba-compiled broadcasting, the 5-image matrix multiplication with weights experienced a 12000x speedup compared to the naive Python nested for-loop implementation! Amazing!!"
  },
  {
    "objectID": "posts/2025-04-22-LossInspector/index.html",
    "href": "posts/2025-04-22-LossInspector/index.html",
    "title": "LossInspector: A Deep Dive Into LLM-Foundry’s Next-Token Prediction with a Custom Composer Callback",
    "section": "",
    "text": "I’m working on a research project where we’ll be fine-tuning small models with various techniques and datasets using LLM-Foundry. As part of our infrastructure setup, we wanted to make sure that we thoroughly understood how a batch of data is prepared by LLM-Foundry, and how the outputs of a model, along with the labels, are passed to the loss function to calculate the loss. To do so, with the help of Claude, I wrote up a custom Composer Callback. This is the third custom callback I’ve written for Composer/LLM-Foundry, you can read more about my first and second callbacks.\nI was initially going to have two or three callbacks: one to inspect inputs/outputs to the embedding, one to inspect the input/outputs to the model’s forward pass, and one to inspect the loss function. 27 commits later, I had a relatively lean single callback that gave me all the information I needed.\nI focused on three events during Composer’s training loop:\n\nbefore_loss: to store the “untouched” batch from Composer’s state.\nbefore_forward: to store the untouched input_ids and labels from the state’s batch.\nafter_loss: to both capture the calculated loss and “manually” calculate the loss using the model’s loss function.\n\nBefore we go further into detail, here’s the callback code (and necessary imports):\nHere’s my video walkthrough of the code in this notebook:"
  },
  {
    "objectID": "posts/2025-04-22-LossInspector/index.html#lossinspector-callback",
    "href": "posts/2025-04-22-LossInspector/index.html#lossinspector-callback",
    "title": "LossInspector: A Deep Dive Into LLM-Foundry’s Next-Token Prediction with a Custom Composer Callback",
    "section": "LossInspector Callback",
    "text": "LossInspector Callback\nfrom composer.core.callback import Callback\nfrom composer.core import State\nfrom composer.loggers import Logger\nimport torch\n\n\nclass LossInspector(Callback):       \n    def __init__(self):\n        super().__init__()\n        self.inspected = False\n        self.input_ids = None\n        self.labels = None\n    \n    def before_loss(self, state: State, logger: Logger) -> None:\n        if self.inspected:\n            return\n        self.state_outputs = state.outputs\n        self.state_batch = state.batch\n        \n\n    def before_forward(self, state: State, logger: Logger) -> None:\n        # check that input_ids and labels are the same as after loss\n        self.input_ids = state.batch['input_ids'][0].detach().cpu()\n        self.labels = state.batch['labels'][0].detach().cpu()\n    \n    def after_loss(self, state: State, logger: Logger) -> None:\n        if self.inspected:\n            return\n            \n        print(\"\\n=== LOSS CALCULATION INSPECTION ===\")\n        \n        # Get the framework loss from state\n        framework_loss = state.loss.item()\n        print(f\"Framework loss: {framework_loss:.6f}\")\n        \n        # Access model's loss_function directly\n        logits = self.state_outputs['logits']\n        labels = self.state_batch['labels']\n        vocab_size = state.model.model.config.vocab_size\n        \n        direct_loss = state.model.model.loss_function(\n            logits=logits,\n            labels=labels,\n            vocab_size=vocab_size\n        )\n        \n        print(f\"Direct call to model.loss_function: {direct_loss.item():.6f}\")\n        \n        print(\"\\n-------- input_ids --------\")\n        input_ids = self.state_batch['input_ids'][0].detach().cpu()\n        print(input_ids.tolist())\n        decoded_input = state.model.tokenizer.decode(input_ids)\n        print(decoded_input[:1000])\n        \n        print(\"\\n-------- labels --------\")\n        labels = self.state_batch['labels'][0].detach().cpu()\n        print(labels.tolist())\n        valid_labels = labels[labels != -100]\n        decoded_labels = state.model.tokenizer.decode(valid_labels)\n        print(decoded_labels)\n\n        print(\"\\n-------- matches before_forward values? --------\")\n        print(f\"input_ids: {torch.allclose(input_ids, self.input_ids)}\")\n        print(f\"labels: {torch.allclose(labels, self.labels)}\")\n        \n        self.inspected = True\nThe callback is then appended to the callbacks list before passed to the Composer trainer."
  },
  {
    "objectID": "posts/2025-04-22-LossInspector/index.html#smollm2-135m-loss-function",
    "href": "posts/2025-04-22-LossInspector/index.html#smollm2-135m-loss-function",
    "title": "LossInspector: A Deep Dive Into LLM-Foundry’s Next-Token Prediction with a Custom Composer Callback",
    "section": "SmolLM2-135M Loss Function",
    "text": "SmolLM2-135M Loss Function\nIt was surprisingly difficult to inspect the loss function. Or rather my lack of Composer/HuggingFace internals knowledge immediately surfaced with this task! Looking through the Composer GitHub repo and documentation, I found the following references to the model’s loss function—all quite helpful but too general:\nloss = model.loss(outputs, targets)\nfor epoch in range(NUM_EPOCHS):\n    for inputs, targets in dataloader:\n        outputs = model.forward(inputs)\n        loss = model.loss(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\ndef loss(self, outputs, batch):\n    # pass batches and `forward` outputs to the loss\n    _, targets = batch\n    return F.cross_entropy(outputs, targets)\nI looked at their MixUp algorithm’s source code in hopes for more detail but found none—though it did help me confirm how batches are handled:\nclass MixUp(Algorithm):\n    def match(self, event: Event, state: State) -> bool:\n        \"\"\"Determines whether the algorithm should run on a given event.\"\"\"\n        return event in [Event.AFTER_DATALOADER, Event.AFTER_LOSS]\n\n    def apply(self, event: Event, state: State, logger: Logger) -> None:\n        \"\"\"Run the algorithm by modifying the State.\"\"\"\n        input, target = state.batch\n\n        if event == Event.AFTER_DATALOADER:\n            new_input, self.permuted_target, self.mixing = mixup_batch(input, target, alpha=0.2)\n            state.batch = (new_input, target)\n\n        if event == Event.AFTER_LOSS:\n            modified_batch = (input, self.permuted_target)\n            new_loss = state.model.loss(state.outputs, modified_batch)\n            state.loss *= (1 - self.mixing)\n            state.loss += self.mixing * new_loss\nLooking at Composer’s HuggingFaceModel did not give me the necessary detail, but provided the key for the next step: the loss was stored in outputs.\ndef loss(self, outputs, batch):\n    if self.config.use_return_dict:\n        return outputs['loss']\n    else:\n        # loss is at index 0 in the output tuple\n        return outputs[0]\nDid this mean that the loss function was tucked away in the forward pass? Let’s take a look.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nmodel_name = \"HuggingFaceTB/SmolLM2-135M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n\nimport inspect\n\n\nforward_method = inspect.getsource(model.forward)\nprint(forward_method)\n\nI won’t print out the whole forward method, but will highlight that tucked away in there was the loss function call!\nloss = None\nif labels is not None:\n    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\nAha! The function in question is loss_function. Inspecting that in more detail:\n\nprint(hasattr(model, 'loss_function'))\n\nTrue\n\n\nThis was a great opportunity for a refresher on the next-token objective and auto-regressive nature of this model.\n\nprint(inspect.getsource(model.loss_function))\n\ndef ForCausalLMLoss(\n    logits,\n    labels,\n    vocab_size: int,\n    num_items_in_batch: Optional[int] = None,\n    ignore_index: int = -100,\n    shift_labels: Optional[torch.Tensor] = None,\n    **kwargs,\n) -> torch.Tensor:\n    # Upcast to float if we need to compute the loss to avoid potential precision issues\n    logits = logits.float()\n\n    if shift_labels is None:\n        # Shift so that tokens < n predict n\n        labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n        shift_labels = labels[..., 1:].contiguous()\n\n    # Flatten the tokens\n    logits = logits.view(-1, vocab_size)\n    shift_labels = shift_labels.view(-1)\n    # Enable model parallelism\n    shift_labels = shift_labels.to(logits.device)\n    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n    return loss\n\n\n\nThe key for understanding next-token prediction are the following lines:\nif shift_labels is None:\n    # Shift so that tokens < n predict n\n    labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n    shift_labels = labels[..., 1:].contiguous()\nnn.functional.pad adds padding tokens to labels, specifically 0 to the left-most end of the last dimension and 1 padding token to the right-most end. The token it uses as padding is ignore_index, which is -100.\nNext, it shifts the labels by 1 element to the left with labels[..., 1:]. I took a moment to realize what this meant: the input_ids and labels, in terms of position, are the same! To align the labels with the logits (which are already “shifted” in the sense that the first position in logits corresponds to the first predicted token: the second token in the context) we have to shift the labels by 1. To ensure that the final token in input_ids doesn’t predict anything, we pad labels with -100, the value ignored in the loss calculation.\nAs a reminder, if the context we’re training our model on is “the cat sat on the table”, each next token is predicted based on all previous tokens:\nthe --> cat\nthe cat --> sat\nthe cat sat --> on\nthe cat sat on --> the\nthe cat sat on the --> table\nThis is a good time to return to our callback and analyze its output, but before I do, here’s a quick demo of the label shifting operation:\n\nfrom torch.nn.functional import pad\nfrom torch import tensor\n\n\nlabels = tensor([3, 6, 4, 2])\nlabels\n\ntensor([3, 6, 4, 2])\n\n\n\npad(labels, (0,1), value=-100)\n\ntensor([   3,    6,    4,    2, -100])\n\n\n\npad(labels, (1,0), value=-100)\n\ntensor([-100,    3,    6,    4,    2])\n\n\n\npad(labels, (1,1), value=-100)\n\ntensor([-100,    3,    6,    4,    2, -100])\n\n\n\npad(labels, (0,1), value=-100)[...,1:]\n\ntensor([   6,    4,    2, -100])"
  },
  {
    "objectID": "posts/2025-04-22-LossInspector/index.html#callback-logs",
    "href": "posts/2025-04-22-LossInspector/index.html#callback-logs",
    "title": "LossInspector: A Deep Dive Into LLM-Foundry’s Next-Token Prediction with a Custom Composer Callback",
    "section": "Callback Logs",
    "text": "Callback Logs\nThere were four key print statements of interest in my callback. I’ll display each and show their printed value:\n\nprint(f\"Framework loss: {framework_loss:.6f}\")\n\nFramework loss: 1.067513\n\nprint(f\"Direct call to model.loss_function: {direct_loss.item():.6f}\")\n\nDirect call to model.loss_function: 1.067513\n\nprint(input_ids.tolist())\nprint(labels.tolist())\n\n\n\n\ninput_ids (top) and labels (bottom) with the response highlighted in yellow\n\n\nThe first two print statements confirmed that I was calling state.model.loss_function correctly. It also confirmed that the loss function doesn’t take in the input_ids.\nThe last two print statements confirmed my understanding: positionally speaking, the input_ids and labels are the same. In labels the positions of input_ids tokens that contain the prompt (and EOS tokens) are replaced with -100 and the tokens that represent the response are kept as is. For reference, here’s what input_ids looks like (both the prompt and the response) coming from an item of the MetaMathQA dataset (I have ommitted the hundreds of padding EOS tokens and formatted the text for clearer presentation):\nA box with a volume of 16 $\\text{cm}^3$ can hold X paperclips.\nHow many paperclips could a box with a volume of 48 $\\text{cm}^3$ hold?\nIf we know the answer to the above question is 150, what is the value of unknown variable X?\n\nWe are given that a box with a volume of 16 $\\text{cm}^3$ can hold $X$ paperclips.\nTo find out how many paperclips a box with a volume of 48 $\\text{cm}^3$ can hold, we can set up a proportion using the given information.\nWe can write the proportion as:\n16 $\\text{cm}^3$ / $X$ paperclips = 48 $\\text{cm}^3$ / 150 paperclips\nWe can cross-multiply and solve for $X$:\n16 * 150 = 48 * $X$\n2400 = 48 * $X$\nDividing both sides by 48, we get:\n$X$ = 50\nThe value of $X$ is 50.\nThe answer is: 50<|endoftext|>\nlabels has the prompt replaced with -100s, and the loss function then left-shifts the labels tokens by 1 spot to align with the logits for next-token prediction comparison.\nUnsurprisingly, the input_ids and labels before the forward pass and after the loss calculation are the same:\nprint(\"\\n-------- matches before_forward values? --------\")\nprint(f\"input_ids: {torch.allclose(input_ids, self.input_ids)}\")\nprint(f\"labels: {torch.allclose(labels, self.labels)}\")\n-------- matches before_forward values? --------\ninput_ids: True\nlabels: True"
  },
  {
    "objectID": "posts/2025-04-22-LossInspector/index.html#final-thoughts",
    "href": "posts/2025-04-22-LossInspector/index.html#final-thoughts",
    "title": "LossInspector: A Deep Dive Into LLM-Foundry’s Next-Token Prediction with a Custom Composer Callback",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWith this baseline established, I can use this callback everytime we have processed a new dataset for training, inspecting the tokens, decoded text and loss values to ensure that the training loop will run properly for next-token prediction, whether it’s a continued pretraining or instruction fine-tuning dataset! Working with LLM-Foundry is a steep learning curve but I am learning a TON."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#introduction",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#introduction",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Introduction",
    "text": "Introduction\nI’m excited to announce the kickoff of TinyScale Lab, a research project focused on exploring the connection between training dynamics and model capabilities. This research is motivated by two papers that I’ve studied in detail: “TinyStories: How Small Can Language Models Be and Still Speak Coherent English?” by Ronen Eldan and Yuanzhi Li, and “Small-scale proxies for Large-scale Transformer Training Instabilities” by Wortsman, et al.\nMost LLM training-related research requires computational resources that are financially out of reach for individual researchers or small teams. At the same time, recent work has shown that tiny models exhibit emergent capabilities (as demonstrated in the TinyStories paper) and exhibit large-scale training dynamics (as shown in the Small-scale proxies paper).\nWhile I don’t claim to be creating a definitive blueprint, I believe this approach—using tiny models as proxies to study phenomena relevant to models of all sizes—represents an underexplored path that could benefit other resource-constrained researchers.\nI think this is how most of the world’s potential researchers would need to work. Making ML research accessible to resource-constrained environments isn’t trivial - it’s essential for the field’s diversity and progress."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#research-hypotheses",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#research-hypotheses",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Research Hypotheses",
    "text": "Research Hypotheses\nI’ve developed four main hypotheses that will guide my research:\n\nH1: Training stability directly affects specific model capabilities in predictable ways.\nH2: Different model capabilities (like grammar or consistency) respond differently to training adjustments.\nH3: Early training signals can predict which capabilities a model will or won’t develop before training is complete.\nH4: Techniques that stabilize training will have varying effects on different types of model capabilities.\n\nI’ve kept these hypotheses general at a high level because I really don’t know what I’m going to learn, but I do have a sense based on the TinyStories and Small-scale proxies papers that there is something around these four elements that I’m going to experience, and I expect to see some relationships.\nI want to bridge the TinyStories paper analysis on emergent capabilities (grammar, consistency, factual knowledge, reasoning, etc.) with the Small-scale proxies paper training dynamics analysis (attention logits, training instabilities, learning rates, etc.)."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#experimental-design",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#experimental-design",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Experimental Design",
    "text": "Experimental Design\nFor my experimental design, I’ve decided to focus on four model sizes:\n\n~3M parameters\n~20M parameters\n~60M parameters\n~120M parameters\n\nThis follows the TinyStories paper closely, with the addition of a 120M parameter model.\nI’ll use the same learning rates as the Small-scale proxies paper, ranging from 3e-4 to 3e-1 with seven learning rates in total: {3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1}\nI’ll implement two stability techniques from the Small-scale proxies paper:\n\nQK layer norm (to mitigate attention logit growth)\nZ loss (to mitigate output logit divergence)\n\nWhat will remain fixed across all training runs are the datasets, the number of training steps, and other hyperparameters like weight decay and warm-up steps.\nThe training dynamics I’ll log throughout training include:\n\nLogits\nGradients\nParameters\nLoss\n\nFor each of these, I’ll capture norms, means, maximum values, and RMS values.\nThe capabilities I want to evaluate are split into three categories:\n\nFoundational language: Grammar and context-tracking (consistency)\nEmergent capabilities: Factual knowledge, reasoning, and creativity\nStory-related: Plot\n\nThe relationship between these training dynamics and capabilities is what I want to explore."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#success-criteria",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#success-criteria",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Success Criteria",
    "text": "Success Criteria\nMy success criteria are simple but not easy: establishing clear connections between training dynamics and tiny model capabilities. This work is exploratory, and I’m open to discovering that the relationships might be more complex or different than initially hypothesized."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#risk-assessment",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#risk-assessment",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Risk Assessment",
    "text": "Risk Assessment\nI’ve identified several risks that could impact this project:\n\nLack of connection between training dynamics and tiny model capabilities\nTechnical challenges in monitoring complex training dynamics\nSub-optimal parameter usage\nCompute and inference costs ballooning beyond budget"
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#risk-mitigation",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#risk-mitigation",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Risk Mitigation",
    "text": "Risk Mitigation\nTo mitigate these risks, I plan to:\n\nShorten the iteration loop\nEnsure evaluations are robust from the start\nStart at the tiniest scale and progressively increase model size\nImplement early stopping to avoid wasting compute\n\nI learned from the fastAI course and community that you want to shorten the iteration loop and ensure that evals are robust from the start. This gives you quick, immediate, robust, clear signal when you get feedback on how your model is performing."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#deliverables",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#deliverables",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Deliverables",
    "text": "Deliverables\nMy commitment is to produce:\n\nComprehensive research repositories including code, trained models, and detailed datasets (training dynamics and LLM Judge scores)\nWeekly video content and blog posts\nTechnical report\nInteractive visualizations\n\nThe main thing I want to emphasize is that I’ll be doing this publicly and open-source. All models, code, and findings will be freely available to enable broader participation in ML research."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#timeline-and-budget",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#timeline-and-budget",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Timeline and Budget",
    "text": "Timeline and Budget\nI’ve broken the project into four phases:\n\nPhase 1: Eval/Logging Setup, Initial Training Runs (2-3 months)\nPhase 2: Experimental Implementation (3-4 months)\nPhase 3: Analysis & Synthesis (2-3 months)\nPhase 4: Documentation & Finalization (1 month)\n\nAt minimum, I think this work will take eight months, and it could go well past a year.\nFor the budget, I’m estimating: - Training: $1700 (approximately 100 training runs on 25B tokens) - Inference: $200 (using Gemini 2.5 Flash for LLM Judge scoring) - Total: $2000\nAt this point, I’m considering whether it makes sense to buy my own GPU rig. If this is going to cost $2,000, why not spend a little more or twice as much and get a GPU rig that I can own? There are a lot of variables when it comes to budget and timeline, so I’m going to take it one week at a time and make adjustments as necessary."
  },
  {
    "objectID": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#closing-thoughts",
    "href": "posts/2025-04-26-TinyScale-Lab-Kickoff/index.html#closing-thoughts",
    "title": "TinyScale Lab: Bridging Training Dynamics and Model Capabilities",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nTo recap, TinyScale Lab aims to:\n\nBridge training dynamics and model capabilities to understand what makes tiny models effective\nCreate a systematic framework for understanding how training choices affect specific model capabilities\nDemonstrate that meaningful ML research is accessible with modest computational resources\nOpen-source all models, code, and findings to enable broader participation in ML research\n\nAs Nick Sirianni (championship winning coach of the Philadelphia Eagles) said, “You cannot be great without the greatness of others.” I truly stand on the shoulders of giants, especially the authors of the TinyStories and Small-scale proxies papers. Without their work and contributions in the open source space, I would not be able to even approach this kind of research.\nIf someone with similar interests sees this work and it inspires them, or they can use something I built that saves them time, saves them money, or gives them insight–that would be the best reward that comes out of this work.\nI hope you’ll follow along with this journey. I’ll be keeping everything in the TinyScale Lab playlist on my YouTube and will tag related posts on my blog.\n\n\n\nTinyScale-Lab bridges the gap between tiny model capabilities and training dynamics"
  }
]