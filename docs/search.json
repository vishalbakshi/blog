[
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "",
    "text": "Here’s the video walkthrough of this notebook\nIn this notebook, I’ll walkthrough the code and concepts introduced in Chapter 8 of the fastai textbook. This chapter explores the various ways fastai can handle a collaborative filtering problem."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "What is Collaborative Filtering?",
    "text": "What is Collaborative Filtering?\nIn a situation where two variables have some numeric relationship, such as users rating movies, collaborative filtering is a solution for predicting ratings that are blank, based on existing data.\nA machine learning model for collaborative filtering implicitly learns the answers to the following questions:\n\nWhat types of movies do users like?\nWhat are characteristics of each movie?\n\n\nLatent Factors\nFor the movie rating example, latent factors are the “types of movies” users like and “characteristics” of each movie. Latent factors are not explicitly categorical, they are numeric values, but they represent the implicit categories of each variable.\nThe reason that they are implicit categories, is that the model learns the ideal latent factors as it trains on the dataset, observing patterns between users and their movie ratings."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "MovieLens 100K Dataset",
    "text": "MovieLens 100K Dataset\nThe dataset used to train the collaborative filtering model is a subset (100,000 rows in length) of the full MovieLens dataset which is 25 million rows.\n\n# a first look at the data\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\n\n\nThe dataset lists users, movies, ratings and a timestamp.\n\n# load the data into a DataFrame\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nBefore we get into the training, I want to familiarize myself with how the data is structured. There are 943 unique users and 1682 unique movies.\n\n# how many unique users and movies are there?\nlen(ratings['user'].unique()), len(ratings['movie'].unique())\n\n(943, 1682)\n\n\nThe movie IDs are a consecutive range from 1 to 1682 and the user IDs are a consecutive range from 1 to 943. The movie ratings range from 1 to 5.\n\n# are movie IDs consecutive?\n(ratings['movie'].sort_values().unique() == np.array(range(1,1683))).sum()\n\n1682\n\n\n\n# are user IDs consecutive?\n(ratings['user'].sort_values().unique() == np.array(range(1,944))).sum()\n\n943\n\n\n\n# what is the range of ratings?\nratings['rating'].min(), ratings['rating'].max()\n\n(1, 5)\n\n\nTo visualize the problem we are trying to solve with collaborative filtering the book recommended that we observe a cross-tabulation of the data because then we can see that what we are trying to predict are the null values between user and movie, and what we are training our model on are the non-null ratings in the dataset.\nThe model will learn something about user 2 and movie 2 in order to predict what rating that user would give that movie. That “something” the model will learn are the latent factors for users and latent factors for movies.\n\n# view crosstab of users and movies with rating values\nct = pd.crosstab(ratings['user'], ratings['movie'], ratings['rating'],aggfunc='mean')\nct\n\n\n\n\n\n  \n    \n      movie\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      ...\n      1643\n      1644\n      1645\n      1646\n      1647\n      1648\n      1649\n      1650\n      1651\n      1652\n      1653\n      1654\n      1655\n      1656\n      1657\n      1658\n      1659\n      1660\n      1661\n      1662\n      1663\n      1664\n      1665\n      1666\n      1667\n      1668\n      1669\n      1670\n      1671\n      1672\n      1673\n      1674\n      1675\n      1676\n      1677\n      1678\n      1679\n      1680\n      1681\n      1682\n    \n    \n      user\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      5.0\n      3.0\n      4.0\n      3.0\n      3.0\n      5.0\n      4.0\n      1.0\n      5.0\n      3.0\n      2.0\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n      3.0\n      4.0\n      5.0\n      4.0\n      1.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      2.0\n      4.0\n      1.0\n      3.0\n      3.0\n      5.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      3.0\n      4.0\n      3.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      939\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      940\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      5.0\n      3.0\n      NaN\n      NaN\n      4.0\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      941\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      942\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      943\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      4.0\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n943 rows × 1682 columns\n\n\n\nInstead of movie IDs, we can get the movie titles into a DataFrame and add that column to our ratings DataFrame by merging the two.\nThe movie titles are in the u.item file, which is pipe-delimited, with latin-1 encoding. The u.item file has 24 columns, but we only want the first two which have the movie id and the title.\n\n# get movie titles\nmovies = pd.read_csv(\n    path/'u.item', \n    delimiter='|', \n    encoding='latin-1', \n    usecols=(0,1), \n    names=('movie', 'title'), \n    header=None)\n\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nThe movies DataFrame and the ratings DataFrame are merged using the movie column as the key to match title in movies to movie ID in ratings. By default, pandas uses as the key whichever column name exists in both DataFrames.\n\n# get the user ratings by title\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n\n\nDoes this change the uniqueness of the data? Yes it actually does! There are 1682 unique movie IDs but there are only 1664 unique movie titles. 18 movies have associated with it duplicate titles.\n\n# how many unique titles and movies are there?\nlen(ratings['title'].unique()), len(ratings['movie'].unique())\n\n(1664, 1682)\n\n\nThe .duplicated DataFrame method takes a list of columns for the subset parameter, finds values in those columns that are duplicated, and returns a boolean Series with a True value at indexes with duplicates. I use that as a mask and pass it to the movies DataFrame to view those duplicate titles.\n\n# 18 movies have duplicate titles\nmovies[movies.duplicated(subset=['title'])]\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      267\n      268\n      Chasing Amy (1997)\n    \n    \n      302\n      303\n      Ulee's Gold (1997)\n    \n    \n      347\n      348\n      Desperate Measures (1998)\n    \n    \n      499\n      500\n      Fly Away Home (1996)\n    \n    \n      669\n      670\n      Body Snatchers (1993)\n    \n    \n      679\n      680\n      Kull the Conqueror (1997)\n    \n    \n      864\n      865\n      Ice Storm, The (1997)\n    \n    \n      880\n      881\n      Money Talks (1997)\n    \n    \n      1002\n      1003\n      That Darn Cat! (1997)\n    \n    \n      1256\n      1257\n      Designated Mourner, The (1997)\n    \n    \n      1605\n      1606\n      Deceiver (1997)\n    \n    \n      1606\n      1607\n      Hurricane Streets (1998)\n    \n    \n      1616\n      1617\n      Hugo Pool (1997)\n    \n    \n      1624\n      1625\n      Nightwatch (1997)\n    \n    \n      1649\n      1650\n      Butcher Boy, The (1998)\n    \n    \n      1653\n      1654\n      Chairman of the Board (1998)\n    \n    \n      1657\n      1658\n      Substance of Fire, The (1996)\n    \n    \n      1679\n      1680\n      Sliding Doors (1998)\n    \n  \n\n\n\n\nfastai has a built-in constructor for DataLoaders specific to collaborative filtering. I pass it the ratings DataFrame, specify that the items are the titles, and that I want 64 rows in each batch.\n\n# create DataLoaders\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      297\n      Indian Summer (1996)\n      4\n    \n    \n      1\n      934\n      Grease (1978)\n      4\n    \n    \n      2\n      846\n      Money Train (1995)\n      2\n    \n    \n      3\n      479\n      Crumb (1994)\n      3\n    \n    \n      4\n      499\n      Local Hero (1983)\n      4\n    \n    \n      5\n      455\n      Adventures of Priscilla, Queen of the Desert, The (1994)\n      3\n    \n    \n      6\n      943\n      Rumble in the Bronx (1995)\n      4\n    \n    \n      7\n      374\n      Dead Poets Society (1989)\n      1\n    \n    \n      8\n      533\n      Deer Hunter, The (1978)\n      3\n    \n    \n      9\n      846\n      Vanya on 42nd Street (1994)\n      2\n    \n  \n\n\n\n\nMatrices for Latent Factors\nWe need the model to find relationships between users and movies. And we need to give the model something concrete and numeric to represent those relationships. We will give it latent factor matrices.\nIn this example, they have chosen to use 5 latent factors for movies and 5 latent factors for users. We represent these latent factors by creating a matrix of random values.\nThe user latent factors will have 944 rows, one for each user including a null user, and 5 columns, one for each latent factor. The movies latent factors will have 1665, one for each movie including a null movie, and 5 columns.\n\n# user and title classes contain '#na#'\nL(dls.classes['title']),L(dls.classes['user'])\n\n((#1665) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...],\n (#944) ['#na#',1,2,3,4,5,6,7,8,9...])\n\n\n\n# define dimensions for users and movies latent factor matrices\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\nn_users, n_movies, n_factors\n\n(944, 1665, 5)\n\n\n\n# build users and movies latent factor matrices\nuser_factors = torch.randn(n_users,n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\nuser_factors.shape, movie_factors.shape\n\n(torch.Size([944, 5]), torch.Size([1665, 5]))\n\n\n\n\nEmbeddings instead of One Hot Encoded Matrix Multiplication\nOkay so how do we use these latent factor matrices?\nFor each row in our batch, we have a user ID and a movie ID. We need to get the latent factors for each of those and calculate the dot product, in order to predict our rating. As it adjusts the latent factors during the training loop, the predictions will get better.\nI’ll grab one batch from the dls DataLoaders object and illustrate an example prediction calculation. Each independent variable, x, is a tensor with [user, movie]. Each dependent variable, y, is a tensor with [rating].\n\nx,y = dls.one_batch()\nx.shape, y.shape\n\n(torch.Size([64, 2]), torch.Size([64, 1]))\n\n\n\nx[0], y[0]\n\n(tensor([466, 614]), tensor([3], dtype=torch.int8))\n\n\n\ntype(x[0]), type(y[0])\n\n(torch.Tensor, torch.Tensor)\n\n\nI determined the order of the x values by looking at the maximum value in each column, specifying axis=0. The movie IDs go up to 1644, so a max value of 1608 means that the movie is the second value in each tensor.\n\nx.max(axis=0)\n\ntorch.return_types.max(values=tensor([ 935, 1642]), indices=tensor([24,  3]))\n\n\nI get the latent factors for the user and movie in the first batch item.\n\nu = user_factors[x[0][0]]\nu\n\ntensor([-0.6595, -0.3355,  1.0491,  1.1764,  0.8750])\n\n\n\nm = movie_factors[x[0][1]]\nm\n\ntensor([-0.1751, -0.5016,  0.6298,  0.2370, -0.7902])\n\n\nI calculate the dot product of the two vectors, which is the sum of the element-wise product.\n\npred = (u * m).sum()\npred\n\ntensor(0.5320)\n\n\nI pass it through sigmoid_range to get a value between 0 and 5. Sigmoid outputs a value between 0 and 1, and sigmoid_range scales and shifts that function to fit the specified range. The output is the prediction for the rating that this user would give this movie.\n\npred = sigmoid_range(pred, 0, 5)\npred\n\ntensor(3.1497)\n\n\nSince the prediction and the target are a single numeric value, we’ll use Mean Squared Error loss. For a single value, the loss is the squared error. For a batch, the mean would be calculated.\n\nloss = (pred - y[0].item()) ** 2\nloss\n\ntensor(0.0224)\n\n\n\n\nBuilding a Collaborative Filtering Model from Scratch\nI’ll create a DotProduct class which builds an Embedding to store latent factor matrices for users and movies, and calculates the prediction in its forward method using the dot product of the user and movie latent factors.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\nI’ll illustrate how this model operates by coding through the calculation of predictions for one batch.\nI create Embeddings for users and movies. Their shape corresponds to the number of users and movies and the number of latent factors.\n\nuser_factors = Embedding(n_users, n_factors)\nmovie_factors = Embedding(n_movies, n_factors)\nuser_factors, movie_factors\n\n(Embedding(944, 5), Embedding(1665, 5))\n\n\nThe weight attribute holds the latent factor values, which are parameters whose gradient can be calculated. The values are from a normal distribution with mean 0 and variance 1.\n\nuser_factors.weight.shape\n\ntorch.Size([944, 5])\n\n\n\nuser_factors.weight\n\nParameter containing:\ntensor([[-0.0066, -0.0111,  0.0091,  0.0056,  0.0075],\n        [-0.0138, -0.0014,  0.0189,  0.0028, -0.0166],\n        [ 0.0149,  0.0053, -0.0153,  0.0078, -0.0119],\n        ...,\n        [-0.0051, -0.0117,  0.0170, -0.0102, -0.0044],\n        [ 0.0037, -0.0084,  0.0042, -0.0049,  0.0186],\n        [ 0.0199, -0.0194,  0.0044,  0.0012,  0.0084]], requires_grad=True)\n\n\nThe user_factors weight has the shape 944 rows by 5 columns. You can see that they are a tensor with requires_grad equals True.\n\nmovie_factors.weight\n\nParameter containing:\ntensor([[-0.0017, -0.0051,  0.0065,  0.0050, -0.0095],\n        [-0.0065, -0.0158,  0.0062, -0.0145, -0.0087],\n        [ 0.0067,  0.0111,  0.0059, -0.0003,  0.0061],\n        ...,\n        [-0.0012,  0.0002, -0.0088, -0.0022, -0.0152],\n        [-0.0053, -0.0058, -0.0074, -0.0033, -0.0171],\n        [ 0.0030,  0.0031, -0.0037, -0.0023,  0.0157]], requires_grad=True)\n\n\nThe movie_factors weight has the shape 1665 rows by 5 columns. And here you can see it is a tensor as well with requires_grad equals True.\n\nmovie_factors.weight.shape\n\ntorch.Size([1665, 5])\n\n\nIn my batch, the 0th column of the dependent variable x holds user indexes. I pass that to user_factors and receive a tensor with those users’ latent factors. Column index 1 holds the movie indexes, I pass that to movie_factors and receive a tensor with those movies’ latent factors.\n\nusers = user_factors(x[:,0])\nusers[:5]\n\ntensor([[ 0.0029,  0.0042, -0.0093,  0.0023, -0.0053],\n        [ 0.0029,  0.0008,  0.0193,  0.0082,  0.0117],\n        [-0.0025,  0.0070, -0.0144, -0.0193,  0.0086],\n        [ 0.0103,  0.0028,  0.0172,  0.0110,  0.0084],\n        [-0.0087, -0.0109,  0.0062, -0.0018, -0.0012]],\n       grad_fn=<SliceBackward>)\n\n\n\nmovies = movie_factors(x[:,1])\nmovies[:5]\n\ntensor([[ 0.0011, -0.0009,  0.0114,  0.0017,  0.0033],\n        [ 0.0049, -0.0019,  0.0175,  0.0027, -0.0014],\n        [-0.0047, -0.0026,  0.0032,  0.0028, -0.0146],\n        [-0.0103, -0.0024,  0.0057, -0.0141, -0.0080],\n        [ 0.0099,  0.0113,  0.0022,  0.0123,  0.0096]],\n       grad_fn=<SliceBackward>)\n\n\nI take the dot product and pass it through a sigmoid_range and the calculate the predictions for the batch.\n\npreds = sigmoid_range((users * movies).sum(dim=1), 0, 5.5)\npreds, preds.shape\n\n(tensor([2.7498, 2.7505, 2.7497, 2.7497, 2.7497, 2.7501, 2.7495, 2.7506, 2.7499,\n         2.7502, 2.7503, 2.7506, 2.7501, 2.7498, 2.7497, 2.7507, 2.7498, 2.7497,\n         2.7499, 2.7500, 2.7499, 2.7500, 2.7502, 2.7501, 2.7502, 2.7499, 2.7500,\n         2.7499, 2.7499, 2.7501, 2.7503, 2.7497, 2.7500, 2.7498, 2.7497, 2.7496,\n         2.7502, 2.7502, 2.7501, 2.7498, 2.7501, 2.7502, 2.7500, 2.7501, 2.7506,\n         2.7500, 2.7498, 2.7499, 2.7501, 2.7502, 2.7502, 2.7501, 2.7498, 2.7501,\n         2.7501, 2.7499, 2.7499, 2.7499, 2.7498, 2.7502, 2.7499, 2.7498, 2.7494,\n         2.7499], grad_fn=<AddBackward0>), torch.Size([64]))\n\n\nThat’s what the DotProduct model will return. I can then take the mean squared error and calculate the loss value, which I can then call backward on, to calculate the gradients for the latent factors. I can then multiply them by the learning rate, and add them to the weights, and repeat the training loop.\n\nloss = ((preds - y) ** 2).mean()\nloss\n\ntensor(2.0156, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\n\n\nuser_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\nmovie_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nWe see here that the gradients are small but not all of them are zero:\n\nuser_factors.weight.grad.sum(), movie_factors.weight.grad.sum()\n\n(tensor(-0.0108), tensor(-0.0026))\n\n\n\nTraining the Model\nThere are five different models that I will build, from simple to complex.\nModel 1 will not use sigmoid_range for the dot product prediction calculation. Instead, I will get a value that is normally distributed with a mean of zero and a variance of 1. Model 2 will pass the predictions through sigmoid_range to get an output between 0 and 5.5. Model 3 will add a bias parameter to the dot product prediction, so that we can establish some baseline rating of each movie that’s independent of a particular latent factor. Model 4 will introduce weight decay in order to better generalize our model, and in Model 5 I’ll implement a custom class instead of using the built-in PyTorch Embedding class.\n\n\n\nModel\nsigmoid_range\nBias\nWeight Decay\nCustom Embedding\n\n\n\n\n1\nN\nN\nN\nN\n\n\n2\nY\nN\nN\nN\n\n\n3\nY\nY\nN\nN\n\n\n4\nY\nY\nY\nN\n\n\n5\nY\nY\nY\nY\n\n\n\n\nModel 1: No sigmoid_range\nThe DotProduct class will initialize an Embedding for users and movies with random values from a normal distribution with mean 0 and variance 1. In the forward method, called during the prediction step of the training loop, the latent factors for the batch’s users and movies are accessed from the corresponding Embedding and the dot product is calculated and returned as the prediction.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return (users * movies).sum(dim=1)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.339355\n      1.274582\n      00:06\n    \n    \n      1\n      1.039260\n      1.061794\n      00:06\n    \n    \n      2\n      0.945793\n      0.954731\n      00:06\n    \n    \n      3\n      0.826578\n      0.871694\n      00:06\n    \n    \n      4\n      0.752750\n      0.858231\n      00:06\n    \n  \n\n\n\nThe average prediction error on the validation ratings is the square root of the final validation loss.\n\nmath.sqrt(learn.recorder.final_record[1])\n\n0.9264077107145671\n\n\nHere are some predictions on the validation set ratings:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      554\n      37\n      4\n      4.181926\n    \n    \n      1\n      561\n      811\n      4\n      3.008064\n    \n    \n      2\n      503\n      298\n      5\n      4.451642\n    \n    \n      3\n      380\n      581\n      4\n      3.474877\n    \n    \n      4\n      666\n      422\n      4\n      4.054492\n    \n    \n      5\n      444\n      933\n      2\n      3.940120\n    \n    \n      6\n      368\n      1612\n      3\n      2.864129\n    \n    \n      7\n      537\n      457\n      3\n      2.955165\n    \n    \n      8\n      224\n      1535\n      1\n      2.940819\n    \n  \n\n\n\n\n\nModel 2 - with sigmoid_range\nIn this model, I’ll force the predictions to fall within the range of actual ratings. The book recommends, based on what they’ve experienced, using a maximum rating for predictions that is slightly larger than the maximum ground truth rating. The range of predictions we’ll use is 0 to 5.5.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.009657\n      0.977510\n      00:07\n    \n    \n      1\n      0.881551\n      0.891973\n      00:06\n    \n    \n      2\n      0.686247\n      0.853506\n      00:07\n    \n    \n      3\n      0.487054\n      0.857519\n      00:06\n    \n    \n      4\n      0.374934\n      0.862651\n      00:06\n    \n  \n\n\n\n\nmath.sqrt(learn.recorder.final_record[1])\n\n\nlearn.show_results()\n\nThe valid loss starts off lower, but by the end of the training, I don’t see an improvement. In fact, the valid loss starts to increase at the end. This is an indication that overfitting is taking place. This will be addressed in Model 4.\n\n\nModel 3 - Add bias\nBut first, let’s look at Model 3, which adds a bias parameter to the predictions, which provides a baseline rating for each movie independent of the weights related to the different latent factors.\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\n\nIn addition to initializing new Embeddings for bias, the dot product is first kept in matrix form by passing keepdim=True to the sum method. The biases are then added on afterwhich the result is passed through sigmoid_range. Here’s a illustrative example for how keepdim=True affects the dot product:\n\na = Tensor([[1,2,3], [4,5,6]])\n(a * a).sum(dim=1).shape, (a * a).sum(dim=1, keepdim=True).shape\n\n(torch.Size([2]), torch.Size([2, 1]))\n\n\nThe first tensor is a 2-vector, whereas the second tensor is a 2 x 1 matrix.\n\nLet’s train Model 3 and view the results!\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.952214\n      0.912603\n      00:07\n    \n    \n      1\n      0.825348\n      0.845193\n      00:07\n    \n    \n      2\n      0.618910\n      0.852381\n      00:07\n    \n    \n      3\n      0.406514\n      0.875637\n      00:07\n    \n    \n      4\n      0.293729\n      0.882840\n      00:07\n    \n  \n\n\n\nThe initial validation loss is lower than the previous trainings, but Model 3 is overfitting even more than Model 2. It’s time to introduce weight decay.\n\n\nModel 4 - Use Weight Decay\nSmaller weights lead to a smoother function which corresponds to fewer inflection points, leading to a better generalization of the model. Larger weights lead to a sharper function, corresponding to more inflection points which overfit the training data.\nThe text uses the basic example of a parabola to illustrate. As the weight a increases, the function becomes narrower, with a sharper trough.\n\nHTML('<iframe src=\"https://www.desmos.com/calculator/uog6rvyubg\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>')\n\n\n\n\nWe will intentionally increase the gradients so that the weights are stepped with larger increments toward a smaller value.\nThis corresponds to an intentionally larger loss value calculated for each batch:\nloss_with_wd = loss + wd * (parameters**2).sum()\nThe parameters are squared to ensure a positive value. Taking the derivative of the loss function means taking the derivative of the following function:\nwd * (parameters**2).sum()\nWhich results in:\n2 * wd * parameters\nInstead of multiplying by 2, we can just use twice the weight decay value as wd. I’ll use a weight decay of 0.1 as they do in the text.\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.939321\n      0.937423\n      00:07\n    \n    \n      1\n      0.851871\n      0.855584\n      00:07\n    \n    \n      2\n      0.720202\n      0.815807\n      00:07\n    \n    \n      3\n      0.630149\n      0.806268\n      00:07\n    \n    \n      4\n      0.491224\n      0.807063\n      00:07\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      877\n      1538\n      3\n      3.708797\n    \n    \n      1\n      601\n      1285\n      1\n      2.832736\n    \n    \n      2\n      292\n      1147\n      2\n      3.675529\n    \n    \n      3\n      132\n      400\n      4\n      3.504542\n    \n    \n      4\n      405\n      1614\n      1\n      1.930779\n    \n    \n      5\n      655\n      458\n      3\n      3.274209\n    \n    \n      6\n      453\n      60\n      4\n      3.864617\n    \n    \n      7\n      629\n      1498\n      4\n      3.598821\n    \n    \n      8\n      724\n      580\n      4\n      3.921505\n    \n  \n\n\n\n\n\nModel 5 - custom Embedding class\nThe final model I’ll train does not have a fundamentally different component than the other four. Instead of using the built-in Embedding PyTorch class, the text has us write our own class.\nOptimizers get a module’s parameters by calling the parameters method. We have to wrap parameters in nn.Parameter for them to be recognized as such. This class also calls requires_grad_ for us.\nI’ll replace each Embedding with a tensor filled with random values from a normal distribution with mean 0 and variance 0.01.\n\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0,0.01))\n\n\ntorch.zeros(3,4)\n\ntensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n\n\ntorch.zeros(3,4).normal_(0,0.01)\n\ntensor([[-0.0134,  0.0098, -0.0124, -0.0032],\n        [ 0.0056,  0.0071,  0.0005,  0.0014],\n        [-0.0236, -0.0024, -0.0060,  0.0017]])\n\n\nI redefine the DotProductBias model using the create_params method instead of Embedding, and train the model.italicized text\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = create_params([n_users, n_factors])\n    self.user_bias = create_params([n_users])\n    self.movie_factors = create_params([n_movies, n_factors])\n    self.movie_bias = create_params([n_movies])\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors[x[:,0]]\n    movies = self.movie_factors[x[:,1]]\n    res = (users * movies).sum(dim=1)\n    res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.934845\n      0.933218\n      00:08\n    \n    \n      1\n      0.841111\n      0.859618\n      00:08\n    \n    \n      2\n      0.730065\n      0.820388\n      00:08\n    \n    \n      3\n      0.599684\n      0.807086\n      00:08\n    \n    \n      4\n      0.484760\n      0.807552\n      00:08\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      487\n      6\n      3\n      2.994869\n    \n    \n      1\n      54\n      349\n      3\n      2.511689\n    \n    \n      2\n      501\n      1252\n      4\n      4.130728\n    \n    \n      3\n      244\n      861\n      4\n      2.314526\n    \n    \n      4\n      322\n      1501\n      5\n      3.823119\n    \n    \n      5\n      537\n      1294\n      2\n      3.124064\n    \n    \n      6\n      193\n      1530\n      3\n      2.546681\n    \n    \n      7\n      581\n      286\n      5\n      3.062707\n    \n    \n      8\n      450\n      154\n      4\n      4.161049\n    \n  \n\n\n\nI get similar results as before!\n\n\n\nInterpreting Embeddings and Biases\nI’ll save this model so that the embedding and bias analyses I perform can be recreated.\n\nlearn = load_learner(\"/content/gdrive/MyDrive/fastai-course-v4/dot_product_bias.pkl\")\nmodel = learn.model\n\nBias represents a baseline rating of a movie regardless of how well the latent factors of the movie match the latent factors of the user. Low bias values correspond to movies that people didn’t enjoy, even if it matched their preferences.\nWhat were the 5 generally least liked movies?\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nmovie_bias = model.movie_bias\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Robocop 3 (1993)',\n 'Showgirls (1995)',\n 'Kansas City (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)']\n\n\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'L.A. Confidential (1997)',\n 'As Good As It Gets (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Good Will Hunting (1997)']\n\n\n\nVisualizing Embeddings\nThe embeddings are a 50-dimensional matrix of latent factors. I’ll use Principal Component Analysis (PCA) to extract the two most descriptive dimensions and plot the latent factor values. I’ll also calculate the distance from 0 of each movie so that I can filter for outliers in order to reduce the number of data points on the plot, and help me understand what these latent factors may be.\n\n!pip install fbpca\nimport fbpca\n\nCollecting fbpca\n  Downloading https://files.pythonhosted.org/packages/a7/a5/2085d0645a4bb4f0b606251b0b7466c61326e4a471d445c1c3761a2d07bc/fbpca-1.0.tar.gz\nBuilding wheels for collected packages: fbpca\n  Building wheel for fbpca (setup.py) ... done\n  Created wheel for fbpca: filename=fbpca-1.0-cp37-none-any.whl size=11376 sha256=719b80446eeb8f157c99e298adb61b0978c0ae279ade82e500dbe37902c447e4\n  Stored in directory: /root/.cache/pip/wheels/53/a2/dd/9b66cf53dbc58cec1e613d216689e5fa946d3e7805c30f60dc\nSuccessfully built fbpca\nInstalling collected packages: fbpca\nSuccessfully installed fbpca-1.0\n\n\nI grab the movie_factors from the trained model, bring it over the the .cpu(), .detach() it from the gradients and convert it to a .numpy() array.\n\nmovie_embeddings = model.movie_factors.cpu().detach().numpy()\n\nI pass those embeddings to the fbpca.pca method and get back the rank-2 approximation.\n\nU, s, Va = fbpca.pca(movie_embeddings, k=2)\n\nI then create a DataFrame from the U matrix which is an m x k (1665 movies x 2 components) matrix. I also create a column with the calculated distance from 0 of each movie, based on the 2-component coordinates, and a column specifying which quadrant the movie is in (First, Second, Third or Fourth).\nMy distance function receives each DataFrame row, and returns the square root of the sum of squares of the two coordinates.\nMy quadrant function received each row and based on the sign of the x or 0 column and the y or 1 column, determines which quadrant that movie lies in.\nI apply both functions to the DataFrame and specify axis=1 so that I can access the column names.\n\n# helper functions\ndef distance(row):\n  return np.sqrt(row[0]**2 + row[1]**2)\n\ndef quadrant(row):\n  if (row[0] > 0 and row[1] > 0):\n    return \"First Quadrant\"\n  elif (row[0] < 0 and row[1] > 0):\n    return \"Second Quadrant\"\n  elif (row[0] < 0 and row[1] < 0):\n    return \"Third Quadrant\"\n  elif (row[0] > 0 and row[1] < 0):\n    return \"Fourth Quadrant\"\n  else:\n    return \"Center\"\n\n\n# create DataFrame from PCA output\ndef pca_to_df(U):\n  df = pd.DataFrame(data=U)\n\n  # calculate the distance of each Embedding from 0\n  df[2] = df.apply(lambda x: np.sqrt(x[0]**2 + x[1]**2), axis=1)\n\n  # identify which quadrant the movie is in\n  df[3] = df.apply(lambda x: quadrant(x), axis=1)\n\n  return df\n\nI’ll import the DataFrame I created from my original PCA output so that I can recreate the corresponding plots.\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movie_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.010150\n      0.004517\n      0.011110\n      First Quadrant\n    \n    \n      1\n      0.025090\n      -0.000186\n      0.025091\n      Fourth Quadrant\n    \n    \n      2\n      -0.005773\n      0.025443\n      0.026090\n      Second Quadrant\n    \n    \n      3\n      0.015933\n      -0.021972\n      0.027141\n      Fourth Quadrant\n    \n    \n      4\n      -0.056279\n      -0.013351\n      0.057841\n      Third Quadrant\n    \n  \n\n\n\n\nWhen I plot the first two columns as x,y coordinates, I can see that the movies are spread out quite evenly across those latent factors.\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06691494d0>\n\n\n\n\n\nI’m going to plot the farthest points from the origin and label them on the plot in order to get a sense of what these two latent factors may represent.\nFor each quadrant, I grab the indexes for the rows with the 5 largest distances and create a DataFrame from that and plot that data.\n\ndef plot_top_5(df):\n  # get the 5 points farthest from 0 from each quadrant\n  idxs = np.array([])\n  for quad in df['3'].unique():\n    idxs = np.append(idxs, df[df['3']==quad]['2'].sort_values(ascending=False).index[:5].values)\n  plot_df = df.loc[idxs]\n\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_top_5(df)\n\n\n\n\nThe first quadrant seems to represent comedies (Ed, Ready to Wear, The Stupids) although Barb Wire is not a comedy.\nThe second quadrant has movies with some more dark and disturbing elements to it.\nThe third quadrant has movies which are drama and I guess share some theme of gaining freedom?\nFinally, the fourth quadrant seems to have drama and action movies which have a distinct American storyline to them. I haven’t seen all of these movies, but Dirty Dancing and The American President seem like anomalies in this group.\nI should also take a look at other movies that do not fall on the extreme ends of each quadrant. For example, which movies fall close to the vertical and horizontal axes?\n\ndef plot_close_to_axes(df):\n  # get 5 points closes to each axis \n  idxs = np.array([])\n  for key in ['0', '1']:\n    idxs = np.append(idxs, df[np.abs(df[key]) < 0.0002].index.values)\n  plot_df = df.loc[idxs]\n  plot_df = plot_df.drop_duplicates()\n\n  # set figure size\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n\n  # annotate with movie titles\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_close_to_axes(df)\n\n\n\n\nThe latent factor corresponding to the vertical axis seems to represent drama (positive values) and romance (negative values) whereas the horizontal axis represents elements mystery (negative values) and comedy (positive values). However, I could just be focusing on genre whereas there are other features of a movie these may represent. Unfortunately, I’m not a movie buff, so the need for a domain expert is evident here!\n\n\nRobust PCA\nAlthough it’s out of scope for this chapter and my understanding, I’d like to at least experiment with using Robust PCA for visualizing embeddings. Robust PCA is an algorithm which decomposes a matrix M into two components: a low rank matrix L and a sparse matrix S such that M = L + S. From what I understand, the sparse matrix S contains anomalies or outliers or “corrupted” data, whereas L contains a more accurate representation of the original data. For example, if an image has some additional noise added to it, the original image matrix M can be decomposed into a noise-less “clean” image L and a sparse noise matrix S. Another example is if you have an image with a background (such as a landscape with lawn, sidewalks, buildings) and a foreground (people walking on the sidewalk) passing that image through the RPCA algorithm would yield a background matrix L with the lawn, sidewalk and buildings and a foreground sparse matrix S with the people. Since my movie_embeddings matrix may contain anomalies which would affect the effectiveness and accuracy of my 2-component PCA approximation, I will pass it through a RPCA algorithm and calculate the 2-component approximation on the low-rank L and sparse S matrices and compare the results with what I calculated above.\nThe following algorithm is from Rachel Thomas’ lesson on RPCA as part of her Computational Linear Algebra course.\n\nfrom scipy import sparse\nfrom sklearn.utils.extmath import randomized_svd\nimport fbpca\n\n\nTOL=1e-9\nMAX_ITERS=3\n\n\ndef converged(Z, d_norm):\n    err = np.linalg.norm(Z, 'fro') / d_norm\n    print('error: ', err)\n    return err < TOL\n\n\ndef shrink(M, tau):\n    S = np.abs(M) - tau\n    return np.sign(M) * np.where(S>0, S, 0)\n\n\ndef _svd(M, rank): return fbpca.pca(M, k=min(rank, np.min(M.shape)), raw=True)\n\n\ndef norm_op(M): return _svd(M, 1)[1][0]\n\n\ndef svd_reconstruct(M, rank, min_sv):\n    u, s, v = _svd(M, rank)\n    s -= min_sv\n    nnz = (s > 0).sum()\n    return u[:,:nnz] @ np.diag(s[:nnz]) @ v[:nnz], nnz\n\n\ndef pcp(X, maxiter=10, k=10): # refactored\n    m, n = X.shape\n    trans = m<n\n    if trans: X = X.T; m, n = X.shape\n        \n    lamda = 1/np.sqrt(m)\n    op_norm = norm_op(X)\n    Y = np.copy(X) / max(op_norm, np.linalg.norm( X, np.inf) / lamda)\n    mu = k*1.25/op_norm; mu_bar = mu * 1e7; rho = k * 1.5\n    \n    d_norm = np.linalg.norm(X, 'fro')\n    L = np.zeros_like(X); sv = 1\n    \n    examples = []\n    \n    for i in range(maxiter):\n        print(\"rank sv:\", sv)\n        X2 = X + Y/mu\n        \n        # update estimate of Sparse Matrix by \"shrinking/truncating\": original - low-rank\n        S = shrink(X2 - L, lamda/mu)\n        \n        # update estimate of Low-rank Matrix by doing truncated SVD of rank sv & reconstructing.\n        # count of singular values > 1/mu is returned as svp\n        L, svp = svd_reconstruct(X2 - S, sv, 1/mu)\n        \n        # If svp < sv, you are already calculating enough singular values.\n        # If not, add 20% (in this case 240) to sv\n        sv = svp + (1 if svp < sv else round(0.05*n))\n        \n        # residual\n        Z = X - L - S\n        Y += mu*Z; mu *= rho\n        \n        examples.extend([S[140,:], L[140,:]])\n        \n        if m > mu_bar: m = mu_bar\n        if converged(Z, d_norm): break\n    \n    if trans: L=L.T; S=S.T\n    return L, S, examples\n\n\nL, S, examples = pcp(movie_embeddings)\n\n\nL.shape, S.shape\n\nI’ll calculate 2-component PCA for the L and S matrices and plot those to see how they compare to the plots above.\n\nU_L, _, _ = fbpca.pca(L, k=2)\nU_S, _, _ = fbpca.pca(S, k=2)\n\nI exported the outputs to CSV for repeatability, so I’ll import them in again:\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_L_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.009254\n      -0.003454\n      0.009877\n      Fourth Quadrant\n    \n    \n      1\n      0.032954\n      0.008637\n      0.034067\n      First Quadrant\n    \n    \n      2\n      -0.014662\n      -0.047128\n      0.049356\n      Third Quadrant\n    \n    \n      3\n      0.012602\n      0.029182\n      0.031787\n      First Quadrant\n    \n    \n      4\n      -0.037841\n      0.008742\n      0.038838\n      Second Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06668a58d0>\n\n\n\n\n\nThe scatter plot for the 2-component PCA result seems much more evenly distributed across the quadrants.\nI take the 5 farthest movies from each quadrant and plot those separately.\n\nplot_top_5(df)\n\n\n\n\nHere are the patterns I observe. Again, someone who has watched these movies and is not just reading online descriptions of them would see themes and patterns that I would not.\n\n\n\nQuadrant\nObservation\n\n\n\n\n1\nRomance/Drama movies. Fausto and Castle Freak seem out of place\n\n\n2\nMore romance movies. Top Gun and Prefontaine seem out of place\n\n\n3\nMore romance movies. The Butcher Boy seems out of place.\n\n\n4\nComedies. The Carmen Miranda documentary seems out of place.\n\n\n\nAfter making these observations, either the low-rank matrix L is a poor choice to use for this type of analysis, or my understanding these movies is too shallow to see the deeper relationships between them. With so many romance movies across the plot, I don’t think these latent factors represent genres.\nI’m not too confident the S matrix will provide more clarity, but let’s see!\n\n# import previously generated CSV\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_S_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      -0.009776\n      0.005041\n      0.010999\n      Second Quadrant\n    \n    \n      1\n      -0.021503\n      0.001765\n      0.021576\n      Second Quadrant\n    \n    \n      2\n      0.005098\n      0.024645\n      0.025166\n      First Quadrant\n    \n    \n      3\n      -0.016745\n      -0.021457\n      0.027218\n      Third Quadrant\n    \n    \n      4\n      0.056964\n      -0.023095\n      0.061467\n      Fourth Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06681c3310>\n\n\n\n\n\n\nplot_top_5(df)\n\n\n\n\nInteresting! This plot looks like the plot of the original M matrix PCA results reflected across the y-axis. Similar movies are grouped together but the latent factors are showing an inverse relationship to the original 2-components.\n\n\n\nUsing fastai.collab\nfastai comes with a built-in method to create a collaborative filtering model similar to the DotProductBias model I created.\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.932735\n      0.930009\n      00:10\n    \n    \n      1\n      0.834800\n      0.862961\n      00:11\n    \n    \n      2\n      0.746893\n      0.822192\n      00:10\n    \n    \n      3\n      0.585107\n      0.811398\n      00:10\n    \n    \n      4\n      0.490022\n      0.812597\n      00:10\n    \n  \n\n\n\nThis yields similar results to what I’ve done above. Here are this model’s results:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      541\n      332\n      1\n      2.542970\n    \n    \n      1\n      899\n      1295\n      4\n      3.555057\n    \n    \n      2\n      346\n      1492\n      3\n      3.456876\n    \n    \n      3\n      933\n      1399\n      4\n      3.380442\n    \n    \n      4\n      310\n      1618\n      5\n      4.623110\n    \n    \n      5\n      276\n      1572\n      4\n      3.636531\n    \n    \n      6\n      463\n      322\n      5\n      3.901797\n    \n    \n      7\n      130\n      408\n      4\n      3.343735\n    \n    \n      8\n      914\n      1617\n      4\n      3.076288\n    \n  \n\n\n\nThe model created is a EmbeddingDotBias model\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nThe top biases can be obtained similar to how we did it before, but with a slightly different API:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Star Wars (1977)',\n \"Schindler's List (1993)\"]\n\n\nSimilar to the distance function I created, PyTorch has a nn.CosineSimilarity function which calculates the cosine of the angle between two vectors. The smaller the angle, the closer the two points are, and the more similar they are. nn.CosineSimilarity returns the similarity (cosine of the angle) between two vectors where 1.000 means the angle is 0.\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\nsimilarity = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = similarity.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Some Folks Call It a Sling Blade (1993)'\n\n\n\n\n\nDeep Learning for Collaborative Filtering\nIn this final section, we create a Deep Learning model which can make predictions on movie ratings after training on the MovieLens dataset. The model uses Embeddings (for users and movies) which are then fed into a small neural net (with one ReLu sandwiched between two Linear layers) which outputs an activation which we normalize using sigmoid_range. The embedding matrices are sized based on a heuristic built-in to fastai with the get_emb_sz method:\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nThe model used is constructed as follows: the user and item latent factors are created using Embeddings, and the neural net is created using the nn.Sequential class. Each time a prediction is needed, the user and item matrices for one batch are concatenated and passed through the neural net. The returned activation is sent to sigmoid_range and a prediction between 0 and 5.5 is calculated.\n\nclass CollabNN(Module):\n  def __init__(self, user_sz, item_sz, y_range=(0, 5.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.item_factors = Embedding(*item_sz)\n    self.layers = nn.Sequential(\n        nn.Linear(user_sz[1]+item_sz[1], n_act),\n        nn.ReLU(),\n        nn.Linear(n_act, 1))\n    self.y_range = y_range\n\n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1))\n    return sigmoid_range(x, *self.y_range)\n\nI want to visualize the forward method, so I’ll create the model and a batch, and walkthrough the code.\n\nmodel = CollabNN(*embs)\nx,y = dls.one_batch()\ndevice = \"cpu\"\nx = x.to(device)\nmodel = model.to(device)\nembs = torch.cat((model.user_factors(x[:,0]), model.item_factors(x[:,1])), dim=1)\nembs.shape\n\ntorch.Size([64, 176])\n\n\n\nx = model.layers(embs)\nsigmoid_range(x, *model.y_range)[:5]\n\ntensor([[2.8637],\n        [2.8647],\n        [2.8624],\n        [2.8696],\n        [2.8601]], grad_fn=<SliceBackward>)\n\n\nThe fastai collab_learner, instead of using the EmbeddingDotBias model, will use a neural network if passed True for its use_nn parameter. The number and size of neural network layers can also be specified.\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.988426\n      0.984418\n      00:15\n    \n    \n      1\n      0.893442\n      0.909180\n      00:16\n    \n    \n      2\n      0.900106\n      0.877499\n      00:16\n    \n    \n      3\n      0.809255\n      0.853736\n      00:16\n    \n    \n      4\n      0.769467\n      0.853571\n      00:16\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      244\n      1305\n      4\n      4.185966\n    \n    \n      1\n      902\n      965\n      3\n      3.474954\n    \n    \n      2\n      87\n      1173\n      5\n      4.206645\n    \n    \n      3\n      759\n      333\n      5\n      4.247213\n    \n    \n      4\n      109\n      1624\n      3\n      3.726794\n    \n    \n      5\n      363\n      743\n      1\n      1.774737\n    \n    \n      6\n      756\n      1216\n      5\n      4.058509\n    \n    \n      7\n      378\n      179\n      4\n      3.192873\n    \n    \n      8\n      18\n      141\n      3\n      3.296141\n    \n  \n\n\n\n\nlearn.model\n\nEmbeddingNN(\n  (embeds): ModuleList(\n    (0): Embedding(944, 74)\n    (1): Embedding(1665, 102)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=176, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=50, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=50, out_features=1, bias=True)\n    )\n    (3): SigmoidRange(low=0, high=5.5)\n  )\n)\n\n\nThe EmbeddingNN architecture extends the TabularModel class which we will explore in Chapter 9.\nThat finishes my review of Chapter 8, I’ll be working through the “Further Research” section in upcoming blog posts and associated videos:\n\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens.\n\n\n\n\nFind three other areas where collaborative filtering is being used, and identify the pros and cons of this approach in those areas.\n\n\n\n\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forums for ideas. Note that there are more columns in the full dataset–see if you can use those too (the next chapter might give you ideas).\n\n\n\n\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "title": "R Shiny Census App",
    "section": "",
    "text": "In this blog post, I’ll walk through my development process for a U.S. Census data visualization web app I created using the Shiny package in R.\nYou can access the app at vbakshi.shinyapps.io/census-app."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "title": "R Shiny Census App",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackstory\nCodebase\n\napp.R\n\nWhat’s in my ui?\n\nDropdowns\nTables\nPlot\nDownload buttons\n\nWhat does my server do?\n\nGet Data\nRender Outputs\nPrepare Dynamic Text\nHandle Downloads\n\n\nprep_db.R\n\nDatabase Tables\n\nb20005\nb20005_vars\ncodes\n\nCreate Tables\nWrite to Tables\nLoad the Data\n\nget_b20005_ruca_aggregate_earnings.R\n\nGet Variable Names\nDerive RUCA Level Estimates and MOE\n\ncalculate_median.R\n\nCreate Frequency Distribution\nCalculate Weighted Total\nApproximate Standard Error\nCalculate Median Estimate Bounds\nReshape the Data\n\nformat_query_result.R\n\nExtract data.frame Objects from List\nReshape data.frame Objects\nAdd Descriptive Labels\n\nget_b20005_labels.R\n\nGet Earnings Population Estimate Labels\nGet All Labels\n\nget_b20005_tract_earnings.R\n\nGet Variable Names\nJoin Tables\n\nget_b20005_states.R\nget_design_factor.R\nmake_plot.R"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "title": "R Shiny Census App",
    "section": "Backstory",
    "text": "Backstory\nI started this project by reading the handbook Understanding and Using American Community Survey Data: What State and Local Government Users Need to Know published by the U.S. Census Bureau. I recreated the handbook’s first case study in R, in which they make comparisons across geographic areas, create custom geographic areas from census tracts and calculate margins of error for derived estimates for Minnesota Census Tract 5-year earnings estimates.\nDuring the process of recreating the derived median earnings estimate calculations, I was unable to recreate a key value from the handbook (the Standard Error for the 50% proportion, calculated to be 0.599) because I was unable to deduce the values used in the following formula referenced from page 17 of the PUMS Accuracy of the Data documentation:\n\n\n\nStandard Error equals Design Factor times square root of the product of 95 over 5B and 50 squared\n\n\nThe documentation defines B as the base, which is the calculated weighted total. I chose the value of 1.3 for the design factor DF since it corresponds to STATE = Minnesota, CHARTYP = Population, CHARACTERISTIC = Person Earnings/Income in the Design Factors CSV published by the Census Bureau.\nI called the Census Bureau Customer Help Center for assistance and was transferred to a member of the ACS Data User Support team with whom I discussed my woes. He was unable to confirm the values of the design factor DF or B, and was unable to pull up the contact information for the statistical methodology team, so I emailed him my questions. After a few email exchanges, the statistical methodology team provided the following:\n\nDF = 1.3\nB = the total population estimate for which the median is being calculated, which is 82488 for the case study calculation (Minnesota Rural Male Full Time Workers)\nThe term 95/5 is associated with the finite population correction factor (100 - f) divided by the sample fraction (f), where f = 5% (later on I note in the documentation that this 95/5 term is based on a 68% confidence interval). The data used in the handbook case study is from 5-year estimates. 1-year estimates sample 2.5% of the population, so the 5-year estimates represent a 5 * 2.5 = 12.5% sample. Instead of 95/5, the ratio becomes (100 - 12.5)/12.5 = 87.5/12.5\n\nThe updated formula is then:\n\n\n\nStandard Error equals Design Factor times square root of the product of 87.5 over 12.5B and 50 squared\n\n\nI was able to calculate the median earnings estimate (and associated standard error and margin of error) within a few percent of the values given in the handbook. This provided me with confirmation that I was ready to expand my code to calculate median earnings estimates for other subgroups."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "title": "R Shiny Census App",
    "section": "Codebase",
    "text": "Codebase\nI built this app using the R package Shiny which handles both the UI and the server. I store the data in a sqlite database and access it with queries written using the RSQLite package which uses the DBI API. The following sections break down the R scripts based on functionality. Click on the script name to navigate to that section.\n\napp.R\n\nUI and server functions to handle people inputs and plot/table/text outputs\n\nprep_db.R\n\nImport, clean, combine and then load data into the census_app_db.sqlite database\n\nget_b20005_ruca_aggregate_earnings.R\n\nQueries the database for earnings and associated margins of error for RUCA levels derived from Census Tracts\n\ncalculate_median.R\n\nDerives estimate, standard of error and margin of error of median earnings for RUCA levels\n\nformat_query_result.R\n\nFormats calculate_median query results\n\nget_b20005_labels.R\n\nQueries the database for descriptive labels of B20005 table variables\n\nget_b20005_tract_earnings.R\n\nQueries the database for Census Tract-level earnings and associated margins of error\n\nget_b20005_states.R\n\nQueries the SQLite database for a list of U.S. states\n\nget_design_factor.R\n\nQueries database for the design factor used for the median earnings estimation calculation\n\nmake_plot.R\n\nCreates a bar plot object"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "title": "R Shiny Census App",
    "section": "app.R",
    "text": "app.R\nA shiny app has three fundamental components:\nui <- (...)\nserver <- (...)\nshinyApp(ui, server,...)\nThe ui object holds all UI layout, input and output objects which define the front-end of your app. The server object holds all rendering functions which are assigned to outputs that appear on the UI. The shinyApp function takes a ui and server object (along with other arguments) and creates a shiny app object which can be run in a browser by passing it to the runApp function. Person inputs (such as selections in a dropdown) are assigned to a global input object.\n\nWhat’s in my ui?\nAll of my UI objects are wrapped within a fluidPage call which returns a page layout which “consists of rows which in turn include columns” (from the docs).\nMy app’s UI has four sections:\n\nDropdowns to select state, sex and work status for which the person using the app wants ACS 5-year earnings estimates\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\nA table with population estimates for earnings levels for each RUCA level for the selected state, sex and work status\n\nEach section has a download button so that people can get the CSV files or plot image for their own analysis and reporting. Each section is separated with markdown('---') which renders an HTML horizontal rule (<hr>).\n\nDropdowns\nDropdowns (the HTML <select> element) are a type of UI Input. I define each with an inputId which is a character object for reference on the server-side, a label character object which is rendered above the dropdown, and a list object which defines the dropdown options.\nselectInput(\n  inputId = \"...\",\n  label = \"...\",\n  choices = list(...)\n)\nIn some cases, I want the person to see a character object in the dropdown that is more human-readable (e.g. \"Large Town\") but use a corresponding input value in the server which is more computer-readable (e.g. \"Large_Town). To achieve this, I use a named character vector where the names are displayed in the dropdown, and the assigned values are assigned to the global input:\nselectInput(\n     inputId = \"ruca_level\",\n     label = \"Select RUCA Level\",\n     choices = list(\n       \"RUCA LEVEL\" = c(\n       \"Urban\" = \"Urban\", \n       \"Large Town\" = \"Large_Town\", \n       \"Small Town\" = \"Small_Town\", \n       \"Rural\" = \"Rural\"))\n     )\nIn this case, if the person selects \"Large Town\" the value assigned to input$ruca_level is \"Large_Town\".\n\n\nTables\nTables (the HTML <table> element) are a type of UI Output. I define each with an outputId for reference in the server.\ntableOutput(outputId = \"...\")\n\n\nPlot\nSimilarly, a plot (which is rendered as an HTML <img> element) is a type of UI Output. I define each with an outputId.\nplotOutput(outputId = \"...\")\n\n\nDownload Buttons\nThe download button (an HTML <a> element) is also a type of UI Output. I define each with an outputId and label (which is displayed as the HTML textContent attribute of the <a> element).\ndownloadButton(\n  outputId = \"...\",\n  label = \"...\"\n)\n\n\n\nWhat does my server do?\nThe server function has three parameters: input, output and session. The input object is a ReactiveValues object which stores all UI Input values, which are accessed with input$inputId. The output object similarly holds UI Output values at output$outputId. I do not use the session object in my app (yet).\nMy app’s server has four sections:\n\nGet data from the SQLite database\nRender table and plot outputs\nPrepare dynamic text (for filenames and the plot title)\nHandle data.frame and plot downloads\n\n\nGet Data\nThere are three high-level functions which call query/format/calculation functions to return the data in the format necessary to produce table, text, download and plot outputs:\n\nThe earnings_data function passes the person-selected dropdown options input$sex, input$work_status and input$state to the get_b20005_ruca_aggregate_earnings function to get a query result from the SQLite database. That function call is passed to format_earnings, which in turn is passed to the reactive function to make it a reactive expression. Only reactive expressions (and reactive endpoints in the output object) are allowed to access the input object which is a reactive source. You can read more about Shiny’s “reactive programming model” in this excellent article.\n\nearnings_data <- reactive(\n  format_earnings(\n    get_b20005_ruca_aggregate_earnings(\n      input$sex, \n      input$work_status, \n      input$state)))\n\nThe design_factor function passes the input$state selection to the get_design_factor function which in turn is passed to the reactive function.\n\ndesign_factor <- reactive(get_design_factor(input$state))\n\nThe median_data function passes the return values from earnings_data() and design_factor() to the calculate_median function which in turn is passed to the reactive function.\n\nmedian_data <- reactive(calculate_median(earnings_data(), design_factor()))\n\n\nRender Outputs\nI have two reactive endpoints for table outputs, and one endpoint for a plot. The table outputs use renderTable (with row names displayed) with the data.frame coming from median_data() and earnings_data(). The plot output uses renderPlot, and a helper function make_plot to create a bar plot of earnings_data() for a person-selected input$ruca_level with a title created with the helper function earnings_plot_title().\noutput$median_data <- renderTable(\n  expr = median_data(), \n  rownames = TRUE)\n  \noutput$earnings_data <- renderTable(\n  expr = earnings_data(), \n  rownames = TRUE)\n    \noutput$earnings_histogram <- renderPlot(\n  expr = make_plot(\n    data=earnings_data(), \n    ruca_level=input$ruca_level, \n    plot_title=earnings_plot_title()))\n\n\nPrepare Dynamic Text\nI created four functions that generate filenames for the downloadHandler call when the corresponding downloadButton gets clicked, one function that generates the title used to generate the bar plot, and one function which takes computer-readable character objects (e.g. \"Large_Town\") and maps it to and returns a more human-readable character object (e.g. \"Large Town\"). I chose to keep filenames more computer-readable (to avoid spaces) and the plot title more human-readable.\nget_pretty_text <- function(raw_text){\n  text_map <- c(\"M\" = \"Male\", \n  \"F\" = \"Female\",\n  \"FT\" = \"Full Time\",\n  \"OTHER\" = \"Other\",\n  \"Urban\" = \"Urban\",\n  \"Large_Town\" = \"Large Town\",\n  \"Small_Town\" = \"Small Town\",\n  \"Rural\" = \"Rural\")\n  return(text_map[raw_text])\n  }\n \nearnings_plot_title <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    get_pretty_text(input$work_status),\n    input$ruca_level,\n    \"Workers\",\n    sep=\" \"))\n  }\n\nb20005_filename <- function(){\n    return(paste(\n      input$state,\n      get_pretty_text(input$sex),\n      input$work_status,\n      \"earnings.csv\",\n      sep=\"_\"\n    ))\n  }\n  \nmedian_summary_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex), \n    input$work_status, \n    'estimated_median_earnings_summary.csv',  \n    sep=\"_\")\n  }\n  \nruca_earnings_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex),  \n    input$work_status, \n    'estimated_median_earnings_by_ruca_level.csv',  \n    sep=\"_\")\n  }\n  \nearnings_plot_filename <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    input$work_status,\n    input$ruca_level,\n    \"Workers.png\",\n    sep=\"_\"))\n  }\n\n\nHandle downloads\nI have five download buttons in my app: two which trigger a download of a zip file with two CSVs, two that downloads a single CSV, and one that downloads a single PNG. The downloadHandler function takes a filename and a content function to write data to a file.\nIn order to create a zip file, I use the zip base package function and pass it a vector with two filepaths (to which data is written using the base package’s write.csv function) and a filename. I also specify the contentType as \"application/zip\". In the zip file, one of the CSVs contains a query result from the b20005 SQLite database table with earnings data, and the other file, \"b20005_variables.csv\" contains B20005 table variable names and descriptions. In order to avoid the files being written locally before download, I create a temporary directory with tempdir and prepend it to the filename to create the filepath.\nFor the bar plot image download, I use the ggplot2 package’s ggsave function, which takes a filename, a plot object (returned from the make_plot helper function) and the character object \"png\" (for the device parameter).\noutput$download_selected_b20005_data <- downloadHandler(\n    filename = \"b20005_data.zip\",\n    content = function(fname) {\n      # Create a temporary directory to prevent local storage of new files\n      temp_dir <- tempdir()\n      \n      # Create two filepath character objects and store them in a list\n      # which will later on be passed to the `zip` function\n      path1 <- paste(temp_dir, '/', b20005_filename(), sep=\"\")\n      path2 <- paste(temp_dir, \"/b20005_variables.csv\", sep=\"\")\n      fs <- c(path1, path2)\n      \n      # Create a CSV with person-selection input values and do not add a column\n      # with row names\n      write.csv(\n        get_b20005_earnings(input$state, input$sex, input$work_status), \n        path1,\n        row.names = FALSE)\n      \n      # Create a CSV for table B20005 variable names and labels for reference\n      write.csv(\n        get_b20005_ALL_labels(),\n        path2,\n        row.names = FALSE)\n      \n      # Zip together the files and add flags to maximize compression\n      zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_all_b20005_data <- downloadHandler(\n  filename = \"ALL_B20005_data.zip\",\n  content = function(fname){\n    path1 <- \"ALL_B20005_data.csv\"\n    path2 <- \"b20005_variables.csv\"\n    fs <- c(path1, path2)\n    \n    write.csv(\n      get_b20005_earnings('ALL', 'ALL', 'ALL'),\n      path1,\n      row.names = FALSE)\n    \n    write.csv(\n      get_b20005_ALL_labels(),\n      path2,\n      row.names = FALSE)\n    \n    zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_median_summary <- downloadHandler(\n  filename = median_summary_filename(),\n  content = function(file) {\n    write.csv(median_data(), file)\n    }\n  )\n  \noutput$download_earnings_plot <- downloadHandler(\n  filename = earnings_plot_filename(),\n  content = function(file) {\n    ggsave(\n      file, \n      plot = make_plot(\n        data=earnings_data(), \n        ruca_level=input$ruca_level, \n        plot_title=earnings_plot_title()), \n        device = \"png\")\n      }\n  )\n  \noutput$download_ruca_earnings <- downloadHandler(\n  filename = ruca_earnings_filename(),\n  content = function(file) {\n    write.csv(earnings_data(), file)\n  }\n  )"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "title": "R Shiny Census App",
    "section": "prep_db.R",
    "text": "prep_db.R\nThis script is meant to be run locally, and is not deployed, as doing so would create a long delay to load the app.\n\nDatabase Tables\nThe database diagram is shown below (created using dbdiagram.io):\n\n\n\nDatabase diagram showing the database table schemas and their relationships\n\n\nI have five tables in my database:\n\n\nb20005\nHolds the data from the ACS 2015-2019 5-year detailed table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months). This includes earnings estimates and margins of errors for Male and Female, Full Time and Other workers, for earning ranges (No earnings, $1 - $2499, $2500 - $4999, …, $100000 or more). The following table summarizes the groupings of the (non-zero earnings) variables relevant to this app:\n\n\n\n\nVariable\nDemographic\n\n\n\n\nB20005_003 to B20005_025\nMale Full Time Workers\n\n\nB20005_029 to B20005_048\nMale Other Workers\n\n\nB20005_050 to B20005_072\nFemale Full Time Workers\n\n\nB20005_076 to B20005_095\nFemale Other Workers\n\n\n\n\n\n\nb20005_vars\nHas the name (e.g. B20005_003E) and label (e.g. “Estimate!!Total!!Male!!Worked full-time, year-round in the past 12 months”) for all B20005 variables. Variable names ending with an E are estimates, and those ending with M are margins of error. - ruca contains RUCA (Rural-Urban Commuting Area) codes published by the U.S. Department of Agriculture Economic Research Service which classify U.S. census tracts using measures of population density. The following table shows the code ranges relevant to this app:\n\n\n\n\nRUCA Code\nRUCA Level\n\n\n\n\n1-3\nUrban\n\n\n4-6\nLarge Town\n\n\n7-9\nSmall Town\n\n\n10\nRural\n\n\n99\nZero Population\n\n\n\n\n\n\ncodes\nolds state FIPS (Federal Information Processing Standards) codes and RUCA levels - design_factors contains Design Factors for different characteristics (e.g. Person Earnings/Income) which are used to determine “the standard error of total and percentage sample estimates”, and “reflect the effects of the actual sample design and estimation procedures used for the ACS.” (2015-2019 PUMS 5-Year Accuracy of the Data).\nIn prep_db.R, I use the DBI package, censusapi and base R functions to perform the following protocol for each table:\n\n\nLoad the Data\n\nFor tables b20005 and b20005_vars, I use the censusapi::getCensus and censusapi::listCensusMetadata repsectively to get the data\n\n# TABLE b20005_vars ------------------------------\nb20005_vars <- listCensusMetadata(\n  name = 'acs/acs5',\n  vintage = 2015,\n  type = 'variables',\n  group = 'B20005')\n  \n # TABLE b20005 ----------------------------------\n b20005 <- getCensus(\n  name = 'acs/acs5',\n  region = \"tract:*\",\n  regionin = regionin_value,\n  vintage = 2015,\n  vars = b20005_vars$name,\n  key=\"...\"\n  )\n\nFor tables codes, ruca, and design_factors I load the data from CSVs that I either obtained (in the case of the Design Factors) or created (in the case of the codes and RUCA levels)\n\n # TABLE codes ----------------------------------\nstate_codes <- read.csv(\n  \"data/state_codes.csv\",\n  colClasses = c(\n    \"character\", \n    \"character\", \n    \"character\")\n)\n\nruca_levels <- read.csv(\n  \"data/ruca_levels.csv\",\n  colClasses = c(\n    \"character\",\n    \"character\",\n    \"character\")\n)\n\n\nCreate Tables\nOnce the data is ready, I use DBI::dbExecute to run a SQLite command to create each table. The relationships shown in the image above dictate which fields create the primary key (in some cases, a compound primary key) as listed below:\n\n\n\n\n\n\n\n\nTable\nPrimary Key\nNotes\n\n\n\n\nb20005\n(state, county, tract))\nForeign key for table ruca\n\n\nb20005_vars\nname\ne.g. B20005_001E\n\n\nruca\nTRACTFIPS\nForeign key for table b20005\n\n\ncodes\n(CODE, DESCRIPTION)\ne.g. (1, \"Urban\")\n\n\ndesign_factors\n(ST, CHARACTERISTIC)\ne.g. (\"27\", \"Person Earnings/Income\")\n\n\n\n\n\nWrite to Tables\nOnce the table has been created in the database, I write the data.frame to the corresponding table with the following call:\ndbWriteTable(census_app_db, \"<table name>\", <data.frame>, append = TRUE"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_ruca_aggregate_earnings.R",
    "text": "get_b20005_ruca_aggregate_earnings.R\nThe function inside this script (with the same name), receives inputs from the server, sends queries to the database and returns the results. This process involves two steps:\n\nGet Variable Names\nThe person using the app selects Sex (M or F), Work Status (Full Time or Other) and State (50 states + D.C. + Puerto Rico) for which they want to view and analyze earnings data. As shown above, different variables in table b20005 correspond to different sexes and work statuses, and each tract for which there is all that earnings data resides in a given state.\nI first query b20005_vars to get the relevent variables names which will be used in the query to b20005, as shown below. names that end with “M” (queried with the wilcard '%M') are for margins of error and those that end with “E” (wildcard '%E') are for estimates.\nvars <- dbGetQuery(\n    census_app_db, \n    \"SELECT name FROM b20005_vars \n    WHERE label LIKE $label_wildcard \n    AND name LIKE '%M'\",\n    params=list(label_wildcard=label_wildcard))\nThe b20005_vars.label column holds long string labels (which follow a consistent pattern, which is captured by the $label_wildcard) that describe the variable’s contents. Here are a couple of examples: \n\n\n\n\n\n\n\nb20005_vars.name\nb20005_vars.label\n\n\n\n\nB20005_053E\n\"Estimate!!Total!!Female!!Worked full-time, year-round in the past 12 months!!With earnings\")\n\n\nB20005_076M\n\"Margin of Error!!Total!!Female!!Other!!With earnings!!$1 to $2,499 or loss\"\n\n\n\n\nSince the label string contains the sex and work status, I assign a label_wildcard based on the person inputs from the sex and work status UI dropdowns.\n# Prepare wildcard for query parameter `label_wildcard`\n  if (sex == 'M') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Male!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Male!!Other%\" }\n  }\n  \n  if (sex == 'F') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Female!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Female!!Other%\" }\n  }\n\n\nDerive RUCA Level Estimates and MOE\nOnce the variables are returned, the actual values are queried from b20005, grouped by RUCA level. The ACS handbook Understanding and Using American Community Survey Data: What All Data Users Need to Know shows how to calculate that margin of error for derived estimates. In our case, the margin of error for a RUCA level such as “Urban” for a given state is derived from the margin of error of individual Census Tracts using the formula below:\n\n\n\nThe MOE for a sum of estimates is the square root of the sum of MOEs squared\n\n\nTranslating this to a SQLite query:\n# Construct query string to square root of the sum of margins of error squared grouped by ruca level\nquery_string <- paste0(\n    \"SQRT(SUM(POWER(b20005.\", vars$name, \", 2))) AS \", vars$name, collapse=\",\")\nWhere vars$name is a list of variable names, and the collapse parameter converts a list or vector to a string. The beginning of that query_string looks like:\n\"SQRT(SUM(POWER(b20005.B20005_001M, 2))) AS B20005_001M, SQRT(...\"\nThe query is further built by adding the rest of the SQL statements:\nquery_string <- paste(\n    \"SELECT ruca.DESCRIPTION,\",\n    query_string,\n    \"FROM 'b20005' \n    INNER JOIN ruca \n    ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n    WHERE \n    b20005.state = $state\n    GROUP BY ruca.DESCRIPTION\"\n  )\nThe ruca.DESCRIPTION column, which contains RUCA levels (e.g. \"Urban\") is joined onto b20005 from the ruca table using the foreign keys representing the Census Tract FIPS code (TRACTFIPS for the ruca table and the concatenated field state || county || tract for b20005). The $state parameter is assigned the person-selected state input, and the columns are aggreaggated by RUCA levels (i.e. GROUP BY ruca.DESCRIPTION). Finally, the RUCA level and square root of the sum of MOEs squared are SELECTed from the joined tables.\nThe query for estimates is simpler than MOEs, because estimates only need to be summed over RUCA levels:\n# Construct a query to sum estimates grouped by ruca level\n  query_string <- paste0(\"SUM(b20005.\",vars$name, \") AS \", vars$name, collapse=\",\")\nget_b20005_ruca_aggregate_earnings returns the query result data.frames in a named list:\nreturn(list(\"estimate\" = estimate_rs, \"moe\" = moe_rs))"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "title": "R Shiny Census App",
    "section": "calculate_median.R",
    "text": "calculate_median.R\nThe procedure for calculating a median earnings data estimate is shown starting on page 17 of the Accuracy of PUMS documentation. This script follows it closely:\n\nCreate Frequency Distribution\n\nObtain the weighted frequency distribution for the selected variable. data is a data.frame with earning estimate values. The rows are the earning ranges and the columns are ruca_levels:\n\n\ncum_percent <- 100.0 * cumsum(data[ruca_level]) / sum(data[ruca_level])\n\n\nCalculate Weighted Total\n\nCalculate the weighted total to yield the base, B.\n\n\nB <- colSums(data[ruca_level])\n\n\nApproximate Standard Error\n\nApproximate the standard error of a 50 percent proportion using the formula in Standard Errors for Totals and Percentages. The design_factor is passed to this function by the server who uses the get_design_factor function explained below to query the design_factors table.\n\n\nse_50_percent <- design_factor * sqrt(87.5/(12.5*B) * 50^2)\n\n\nCalculate Median Estimate Bounds\n\nCreate the variable p_lower by subtracting the SE from 50 percent. Create p_upper by adding the SE to 50 percent.\n\n\np_lower <- 50 - se_50_percent\np_upper <- 50 + se_50_percent\n\nDetermine the categories in the distribution that contain p_lower and p_upper…\n\n\n# Determine the indexes of the cumulative percent data.frame corresponding  \n# to the upper and lower bounds of the 50% proportion estimate\ncum_percent_idx_lower <- min(which(cum_percent > p_lower))\ncum_percent_idx_upper <- min(which(cum_percent > p_upper))\n.._If p_lower and p_upper fall in the same category, follow step 6. If p_lower and p_upper fall in different categories, go to step 7…_\n\n# The median estimation calculation is handled differently based on \n# whether the upper and lower bound indexes are equal\n    if (cum_percent_idx_lower == cum_percent_idx_upper) {\n\nIf p_lower and p_upper fall in the same category, do the following:\n\n\nDefine A1 as the smallest value in that category.\n\n\n# A1 is the minimum earnings value (e.g. 30000) of the earning range \n# (e.g. 30000 to 34999) corresponding to the lower bound cumulative percent\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\n\nDefine A2 as the smallest value in the next (higher) category.\n\n\n# A2 is the minimum earnings value of the earning range above the \n# earning range corresponding to the upper bound cumulative percent\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\n\nDefine C1 as the cumulative percent of units strictly less than A1.\n\n\n# C1 is the cumulative percentage of earnings one row below the \n# lower bound cumulative percent\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\n\nDefine C2 as the cumulative percent of units strictly less than A2.\n\n\n# C2 is the cumulative percentage of the earnings below the \n# lower bound cumulative percent\nC2 <- cum_percent[cum_percent_idx_lower, ]\n\nUse the following formulas to approximate the lower and upper bounds for a confidence interval about the median:\n\n\n# the lower bound of the median \nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n      \n# the upper bound of the median\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nIf p_lower and p_upper fall in different categories, do the following:\n\n\nFor the category containing p_lower: Define A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the lower bound.\n\n\n# A1, A2, C1 and C2 are calculated using the lower bound cumulative percent\n# to calculate the lower bound of the median estimate\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\nC2 <- cum_percent[cum_percent_idx_lower, ]\nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n\nFor the category containing p_upper: Define new values for A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the upper bound.\n\n\n# A1, A2, C1 and C2 are calculated using the upper bound cumulative percent\n# to calculate the upper bound of the median estimate\nA1 <- earnings[cum_percent_idx_upper, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_upper + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_upper - 1,]\nC2 <- cum_percent[cum_percent_idx_upper,]\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nUse the lower and upper bounds approximated in steps 6 or 7 to approximate the standard error of the median. SE(median) = 1/2 X (Upper Bound – Lower Bound)\n\n\n# The median earning estimate is the average of the upper and lower bounds\n# of the median estimates calculated above in the if-else block\nmedian_earnings <- 0.5 * (lower_bound + upper_bound)\n    \n# The median SE is half the distance between the upper and lower bounds\n# of the median estimate\nmedian_se <- 0.5 * (upper_bound - lower_bound)\n\n# The 90% confidence interval critical z-score is used to calculate \n# the margin of error\nmedian_90_moe <- 1.645 * median_se\n\n\nReshape the Data\nFinally, a data.frame is returned, which will be displayed in a tableOutput element.\n\n# A data.frame will be displayed in the UI\nmedian_data <- data.frame(\n  \"Estimate\" = median_earnings,\n  \"SE\" = median_se,\n  \"MOE\" = median_90_moe\n)"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "title": "R Shiny Census App",
    "section": "format_query_result.R",
    "text": "format_query_result.R\nThe purpose of this function is to receive two data.frame objects, one for earnings estimate values, and one for the corresponding moe values, and return a single data.frame which is ready to be displayed in a tableOutput.\n\nExtract data.frame Objects from List\nSince get_b20005_ruca_aggregate_earnings returns a named list, I first pull out the estimate and moe data.frame objects:\n\n# Pull out query result data.frames from the list\nestimate <- rs[[\"estimate\"]]\nmoe <- rs[[\"moe\"]]\n\n\nReshape data.frame Objects\nThese data.frame objects have RUCA levels in the column DESCRIPTION and one column for each population estimate. For example, the estimate for Alabama Full Time Female workers looks like this:\n\n\n\n\nDESCRIPTION\n…\nB20005_053E\nB20005_054E\nB20005_055E\n…\n\n\n\n\n1\nLarge Town\n…\n149\n257\n546\n…\n\n\n2\nRural\n…\n75\n66\n351\n…\n\n\n3\nSmall Town\n…\n28\n162\n634\n…\n\n\n4\nUrban\n…\n468\n1061\n4732\n…\n\n\n5\nZero Population\n…\n0\n0\n0\n…\n\n\n\nThe moe data.frame has a similar layout.\nHowever, in the UI, I want the table to look like this:\n\n\n\nPopulation estimates for earnings levels from $1 to $2499 up to $100000 and more for Alabama Full Time Female Workers\n\n\nTo achieve this, I first transpose the estimate and moe data.frames…\n\n# Transpose the query results\ncol_names <- estimate[,\"DESCRIPTION\"]\nestimate <- t(estimate[-1])\ncolnames(estimate) <- col_names\n  \ncol_names <- moe[,\"DESCRIPTION\"]\nmoe <- t(moe[-1])\ncolnames(moe) <- col_names\n…then zip them together, keeping in mind that not all states have tracts designated with all RUCA levels:\n\n# Create a mapping to make column names more computer-readable\nformat_ruca_level <- c(\n  \"Urban\" = \"Urban\", \n  \"Large Town\" = \"Large_Town\", \n  \"Small Town\" = \"Small_Town\", \n  \"Rural\" = \"Rural\",\n  \"Zero Population\" = \"Zero_Population\")\n\n# bind together estimate and corresponding moe columns\n# some states do not have all RUCA levels\n# for example, Connecticut does not have \"Small Town\" tracts\n\n# Create empty objects\noutput_table <- data.frame(temp = matrix(NA, nrow = nrow(estimate), ncol = 0))\ncol_names <- c()\n\nfor (ruca_level in c(\"Urban\", \"Large Town\", \"Small Town\", \"Rural\")) {\n  if (ruca_level %in% colnames(estimate)) {\n    output_table <- cbind(output_table, estimate[,ruca_level], moe[,ruca_level])\n    \n    # paste \"_MOE\" suffix for MOE columns\n    col_names <- c(\n      col_names,\n      format_ruca_level[[ruca_level]],\n      paste0(format_ruca_level[[ruca_level]], \"_MOE\"))\n  }\n}\n\n# Replace old names with more computer-readable names\ncolnames(output_table) <- col_names\n\n\n\nAdd Descriptive Labels\nFinally, merge the output_table data.frame with labels (long form description of the B20005 variables) which are retrieved from the database using the get_b20005_labels function explained later on in this post. Remember that the label is delimited with \"!!\" and the last substring contains earnings ranges (e.g. “$30,000 to $34,999”):\n\n# name rows as long-form labels, by splitting them by '!!' and \n# grabbing the last chunk which has dollar ranges e.g. \n# $30000 to $34999\noutput_table <- merge(output_table, labels, by.x = 0, by.y = \"name\")\nsplit_label <- data.frame(\n  do.call(\n    'rbind', \n    strsplit(as.character(output_table$label),'!!',fixed=TRUE)))\n\nrownames(output_table) <- split_label$X6"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_labels.R",
    "text": "get_b20005_labels.R\nThis script contains two helper functions to retrieve the label column from the b20005_vars table.\n\nGet Earnings Population Estimate Labels\nThe first one, get_b20005_labels retrieves the variable name and label for earning range strings (e.g. “$30,000 to $34,999”):\n\nget_b20005_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    WHERE \n      label LIKE '%$%'\n    ORDER BY name\"\n    )\n  dbDisconnect(census_app_db)\n  return(rs)\n}\n\n\n\nGet All Labels\nThe second function, get_b20005_ALL_labels returns the whole table:\n\nget_b20005_ALL_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    ORDER BY name\"\n  )\n  dbDisconnect(census_app_db)\n  return(rs)\n}"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_tract_earnings.R",
    "text": "get_b20005_tract_earnings.R\nThis function is similar to get_b20005_ruca_aggregate_earnings but does not aggregate by RUCA level, and also includes Census Tracts that are not designated a RUCA level. The label_wildcard is constructed the same way as before.\n\nGet Variable Names\nThe variable names are obtained for both margin of error and estimates in the same query:\n\n # Get b20005 variable names (estimates and moe)\nvars <- dbGetQuery(\n  census_app_db, \n  \"SELECT name FROM b20005_vars \n  WHERE label LIKE $label_wildcard\",\n  params=list(label_wildcard=label_wildcard)\n  )\n\n\n\nJoin Tables\nThe tract-level earnings are queried with the following, using a LEFT JOIN between b20005 and ruca tables to include tracts that do not have a RUCA level.\n\n# Construct query to get tract-level earnings data\nquery_string <- paste(\n  \"SELECT ruca.DESCRIPTION,\n  b20005.state || b20005.county || b20005.tract AS TRACTFIPS,\",\n  paste0(vars$name, collapse=\",\"),\n  \"FROM b20005 \n  LEFT JOIN ruca \n  ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n  WHERE \n  b20005.state LIKE $state\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_states.R",
    "text": "get_b20005_states.R\nThis function retrieves state codes and names from the codes table, and is used to assign choices to selectInput dropdowns. \"United States\" which has a FIPS code of \"00\" is excluded because the b20005 table contains state-level data only. The query result is sorted by the state name so that the dropdown menu choices are in ascending alphabetical order.\nstates <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESCRIPTION, CODE\n  FROM codes \n  WHERE CATEGORY = 'state'\n  AND CODE <> '00'\n  ORDER BY DESCRIPTION\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "title": "R Shiny Census App",
    "section": "get_design_factor.R",
    "text": "get_design_factor.R\nThis function retrieves a single numeric Design Factor for the “Person Earnings/Income” characteristic from the design_factors table for a given state parameter:\n\nrs <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESIGN_FACTOR FROM design_factors\n  WHERE ST = $state\n  AND CHARACTERISTIC = 'Person Earnings/Income'\",\n  params = list(state=state))\n\nrs <- as.numeric(rs[1, \"DESIGN_FACTOR\"])"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "title": "R Shiny Census App",
    "section": "make_plot.R",
    "text": "make_plot.R\nThis is function creates a ggplot.bar_plot object using a given data, RUCA level, and title. The x-axis labels are rotated, both axis labels are resized, and plot title and subtitle are formatted.\n\nmake_plot <- function(data, ruca_level, plot_title){\n  # Prepare x-axis factor for `aes` parameter\n  xs <- rownames(data)\n  xs <- factor(xs, xs)\n\n  bar_plot <- ggplot(\n    data=data,\n    aes(x=xs, y=get(ruca_level))) + \n    geom_bar(stat='identity') + \n\n    theme(\n      # Rotate x-axis labels\n      axis.text.x=element_text(\n        angle = -90, \n        vjust = 0.5, \n        hjust=1, \n        size=12),\n\n      # Resize x-axis labels and move them away from axis\n      axis.title.x=element_text(vjust=-0.75,size=14),\n\n      # Resize y-axis labels\n      axis.text.y=element_text(size=12),\n      axis.title.y=element_text(size=14),\n\n      # Set plot title and subtitle font and placement\n      plot.title = element_text(size = 18, hjust=0.5, face='bold'),\n      plot.subtitle = element_text(size = 12, hjust=0.5)) +\n\n    labs(x=\"Earnings\", y=\"Population Estimate\") + \n    ggtitle(plot_title, subtitle=\"Population Estimate by Earnings Level\")\n\n  return (bar_plot)\n}"
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "",
    "text": "In this blog post, I will work through the textbook Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari.\nBefore I get into the reading, I’ll note that I have not posted my solutions to exercises to honor the request of one of the authors, Aki Vehtari:\nHowever, to learn by writing, I will write about the process of doing the exercises, the results I got, and what I learned from it all."
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html#what-you-should-be-able-to-do-after-reading-and-working-through-this-book",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html#what-you-should-be-able-to-do-after-reading-and-working-through-this-book",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "What you should be able to do after reading and working through this book",
    "text": "What you should be able to do after reading and working through this book\n\nPart 1: Review key tools and concepts in mathematics, statistics and computing\n\nChapter 1: Have a sense of the goals and challenges of regression\nChapter 2: Explore data and be aware of issues of measurement and adjustment\nChapter 3: Graph a straight line and know some basic mathematical tools and probability distributions\nChapter 4: Understand statistical estimation and uncertainty assessment, along with the problems of hypothesis testing in applied statistics\nChapter 5: Simulate probability models and uncertainty about inferences and predictions\n\nPart 2: Build linear regression models, use them in real problems, and evaluate their assumptions and fit to data\n\nChapter 6: Distinguish between descriptive and causal interpretations of regression, understanding these in historical context\nChapter 7: Understand and work with simple linear regression with one predictor\nChapter 8: Gain a conceptual understanding of least squares fitting and be able to perform these fits on the computer\nChapter 9: Perform and understand probabilistic and simple Bayesian information aggregation, and be introduced to prior distributions and Bayesian inference\nChapter 10: Build, fit, and understand linear models with multiple predictors\nChapter 11: Understand the relative importance of different assumptions of regression models and be able to check models and evaluate their fit to data\nChapter 12: Apply linear regression more effectively by transforming and combining predictors\n\nPart 3: Build and work with logistic regression and generalized linear models\n\nChapter 13: Fit, understand, and display logistic regression models for binary data\nChapter 14: Build, understand and evaluate logistic regressions with interactions and other complexities\nChapter 15: Fit, understand, and display generalized linear models, including the Poisson and negative binomial regression, ordered logistic regression, and other models\n\nPart 4: Design studies and use data more effectively in applied settings\n\nChapter 16: Use probability theory and simulation to guide data-collection decisions, without falling into the trap of demanding unrealistic levels of certainty\nChapter 17: Use poststratification to generalize from sample to population, and use regression models to impute missing data\n\nPart 5\n\nChapter 18: Understand assumptions underlying causal inference with a focus on randomized experiments\nChapter 19: Perform causal inference in simple setting using regressions to estimate treatments and interactions\nChapter 20: Understand the challenges of causal inference from observational data and statistical tools for adjusting for differences between treatment and control groups\nChapter 21: Understand the assumptions underlying more advanced methods that use auxiliary variables or particular data structures to identify causal effects, and be able to fit these models to data\n\nPart 6\n\nChapter 22: Get a sense of the directions in which linear and generalized linear models can be extended to attack various classes of applied problems\n\nAppendixes\n\nAppendix A: Get started in the statistical software R, with a focus on data manipulation, statistical graphics, and fitting using regressions\nAppendix B: Become aware of some important ideas in regression workflow\n\n\nAfter working through the book, you should be able to fit, graph, understand, and evaluate linear and generalized linear models and use these model fits to make predictions and inferences about quantities of interest, including causal effects of treatments and exposures."
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "",
    "text": "An example image of a bear from the image dataset used in this chapter.\nIn Chapter 6, we learned to train an image recognition model for multi-label classification. In this notebook, I will apply those concepts to the bear classifier from Chapter 2.\nI’ll place the prompt of the “Further Research” section here and then answer each part.\nHere’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Setup",
    "text": "Setup\n\nfrom fastai.vision.all import *\n\n\nimport fastai\nimport pandas as pd\n\nfastai.__version__\n\n'2.3.0'\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nI have three different CSVs with Google Image URLs, one each for black, brown and grizzly bears. The script below, taken from the book, creates a directory for each of the three types of bears in the bears folder, and then downloads the corresponding bear type’s images into that directory.\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\nbear_types = ['black', 'grizzly', 'teddy']\nif not path.exists():\n  path.mkdir()\n  for o in bear_types:\n    dest = path/o\n    dest.mkdir(exist_ok=True)\n    download_images(f'/content/gdrive/MyDrive/fastai-course-v4/images/bears/{o}', url_file=Path(f'/content/gdrive/MyDrive/fastai-course-v4/download_{o}.csv'))\n\n\n# confirm that `get_image_files` retrieves all images\nfns = get_image_files(path)\nfns\n\n(#535) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000009.jpg')...]\n\n\n\n# verify all images\nfailed = verify_images(fns)\nfailed\n\n(#0) []\n\n\nSince I may need to move files around if they are incorrectly labeled, I’m going to prepend the filenames with the corresponding bear type.\n\nimport os\nfor dir in os.listdir(path):\n    for f in os.listdir(path/dir):\n      os.rename(path/dir/f, path/dir/f'{dir}_{f}')\n\n\nfns = get_image_files(path)\nfns\n\n(#723) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000006.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg')...]"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Single-Label Classifier",
    "text": "Single-Label Classifier\nI’ll train the single-digit classifier as we did in Chapter 2.\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# create DataLoaders\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# verify train batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# first training\n# use it to clean the data\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.367019\n      0.252684\n      0.080645\n      00:05\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.179421\n      0.175091\n      0.056452\n      00:04\n    \n    \n      1\n      0.155954\n      0.165824\n      0.048387\n      00:04\n    \n    \n      2\n      0.119193\n      0.173681\n      0.056452\n      00:04\n    \n    \n      3\n      0.098313\n      0.170383\n      0.048387\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\nInitial training: Clean the Dataset\n\n# plot highest loss images\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\nSome of these images are infographics containing text, illustrations and other non-photographic bear data. I’ll delete those using the cleaner\n\nfrom fastai.vision.widgets import *\n\n\n# view highest loss images\n# using ImageClassifierCleaner\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n# unlink images with \"<Delete>\" selected in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\n\n# move any images reclassified in the cleaner\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nAfter a few rounds of quickly training the model and using the cleaner, I was able to remove or change a couple dozen of the images. I’ll use lr.find() and re-train the model.\n\n\nSecond Training with Cleaned Dataset\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\n\n# create DataLoaders\ndls = bears.dataloaders(path)\n\n#verify validation batch\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n#verify training batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.0005754399462603033)\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\n\n# fit one cycle\nlr = 1e-3\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.405979\n      0.418305\n      0.145161\n      00:04\n    \n    \n      1\n      0.803087\n      0.214286\n      0.056452\n      00:04\n    \n    \n      2\n      0.557531\n      0.169275\n      0.048387\n      00:04\n    \n    \n      3\n      0.408410\n      0.163632\n      0.056452\n      00:04\n    \n    \n      4\n      0.321682\n      0.164792\n      0.040323\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n# show results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘t’ 8=‘h’ 9=‘e’ 10=’ ’ 11=‘m’ 12=‘o’ 13=‘d’ 14=‘e’ 15=‘l’}\nlearn.export(fname=path/'single_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Multi-Label Classifier",
    "text": "Multi-Label Classifier\nThere are three major differences between training a multi-label classification model and a single-label model on this dataset. I present them in a table here:\n\n\n\n\n\n\n\n\n\nClassification Model Type\nDependent Variable\nLoss Function\nget_y function\n\n\n\n\nSingle-label\nDecoded string\nCross Entropy (softmax)\nparent_label\n\n\nMulti-label\nOne-hot Encoded List\nBinary Cross Entropy (sigmoid with threshold)\n[parent_label]\n\n\n\n\n# create helper function\ndef get_y(o): return [parent_label(o)]\n\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_y,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# view validation batch\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18,  metrics=partial(accuracy_multi,thresh=0.95), loss_func=BCEWithLogitsLossFlat(thresh=0.5))\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of BCEWithLogitsLoss()\n\n\n\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.478340\n      0.436599\n      0.937695\n      00:51\n    \n    \n      1\n      0.289231\n      0.642520\n      0.887850\n      00:03\n    \n    \n      2\n      0.203213\n      0.394335\n      0.897196\n      00:03\n    \n    \n      3\n      0.159622\n      0.155405\n      0.959502\n      00:02\n    \n    \n      4\n      0.132379\n      0.090879\n      0.965732\n      00:02\n    \n  \n\n\n\n\n# verify results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘m’ 8=‘o’ 9=‘d’ 10=‘e’ 11=‘l’}\nlearn.export(path/'multi_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Model Inference",
    "text": "Model Inference\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images')\n\n\nImage with a Single Bear\n\n# grizzly bear image\nimg = PILImage.create(path/'test'/'grizzly_test_1.jpg')\nimg\n\n\n\n\n\n# load learners\nsingle_learn_inf = load_learner(path/'bears'/'single_label_bear_classifier.pkl')\nmulti_learn_inf = load_learner(path/'bears'/'multi_label_bear_classifier.pkl')\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n\n\n('teddy', tensor(2), tensor([1.7475e-04, 3.7727e-04, 9.9945e-01]))\n\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\n\n((#1) ['grizzly'],\n tensor([False,  True, False]),\n tensor([6.3334e-05, 1.0000e+00, 1.4841e-04]))\n\n\n\n\nImage with Two Bears\n\n# image with grizzly and black bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with grizzly and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with black and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\nImages without Bears\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/')\nimg = PILImage.create(path/'test'/'computer.jpg')\nimg\n\n\n\n\n\nsigle_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.1316, 0.1916, 0.0004]))\n\n\n\nsingle_learn_inf.predict(img)[2].sum()\n\n\n\n\ntensor(1.)\n\n\n\n# set loss function threshold to 0.9\nmulti_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.0275, 0.0196, 0.8457]))"
  },
  {
    "objectID": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "href": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "title": "Making Chicken Wings",
    "section": "",
    "text": "Background\nRestaurant-bought wings are pricey. Even fast-food bought wings, which can be miserably made some times—I once bought advertised Honey BBQ Chicken Wings which were great the first time, but the second time looked like plain wings with barbeque sauce drizzled over them–were about $20 for 16 wings. I don’t mind the price, gratefully, but I do mind consistent quality and want to avoide letdowns, so I chose to learn how to make wings. I also need to fill the gap caused by the lack of NFL and College Football during the offseason, so this is a tasty project which serves that purpose as well.\n\n\nFirst Attempt\nI made my first batch of chicken wings on Saturday, February 11, 2023. They were edible and the sauce was delicious. But they were at most a 5/10. I follow some recipe I found online after a google search which went something like:\n\nPreheat oven to 400°F\nBaste oil on both sides of the wings and season with salt and pepper\nBake for 20 minutes, flipping the wings after 10 minutes\nHeat the oven to 425°F\nBaste on the sauce and bake for 7 minutes then flip. Repeat a few times.\nBroil (500°F) for 5-10 minutes.\n\nMy sauce was made of Ray’s sugar-free BBQ sauce, mustard, soy sauce, and honey.\nI did not have a basting brush and was using a spoon to lather on the sauce. It did not work well. Also, the meat did not fall off the bones and it took more effort than worthwhile to eat them.\n\n\nSecond Attempt\nI was hungrier this time I suppose because I chose to make honey bbq drumsticks instead of wings. This time I got a silicone basting brush at Safeway. I modified my approach slightly:\n\nPreheat oven to 400°F\nActually baste this time and season the drumsticks\nBake on one side for 15 minutes, flip and bake for another 15 minutes\nRemove the drumsticks and heat the oven to 425°F\nRepeat four times, twice on each side: baste on sauce, bake for 7 minutes, remove and flip.\nAdd coconut flour to the sauce (note to self: use rice flour instead)\nRepeat two times, once on each side: baste on sauce/flour mix (gravy?), bake for 7 minutes and flip.\nBroil (500°F) for 5 minutes.\n\nUsing the basting brush allowed for a more even distribution of sauce on the drumsticks. The skin still wasn’t crispy enough, although the meat was (more) easily coming off the bones and was juicy + delicious.\nWhen I make it again, probably next weekend, I’ll switch back to chicken wings and find a new recipe which emphasizes the cripsiness of the wings.\n\n\n\nclose-up of my honey bbq chicken drumsticks with a few charred spots."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "",
    "text": "An example image from the image dataset used in this lesson. The image has a train going over a bridge with skyscrapers in the background.\nThis chapter introduced two more classification models:\nIn this chapter the authors walk us through in the chapter is the PASCAL dataset.\nHere’s my video walkthrough for this notebook:"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "The Data",
    "text": "The Data\nfastai comes with datasets available for download using the URLs object. We will use the PASCAL_2007 dataset.\n\n# download the dataset\npath = untar_data(URLs.PASCAL_2007)\n\n#read label CSV into a DataFrame\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\n  \n    \n      \n      fname\n      labels\n      is_valid\n    \n  \n  \n    \n      0\n      000005.jpg\n      chair\n      True\n    \n    \n      1\n      000007.jpg\n      car\n      True\n    \n    \n      2\n      000009.jpg\n      horse person\n      True\n    \n    \n      3\n      000012.jpg\n      car\n      False\n    \n    \n      4\n      000016.jpg\n      bicycle\n      True\n    \n  \n\n\n\n\nNext, they have us go through some pandas fundamentals for accessing data in a DataFrame\n\n# accessing all rows and the 0th column\ndf.iloc[:,0]\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# accessing all columns for the 0th row\ndf.iloc[0,:]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# trailing :s are not needed\ndf.iloc[0]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# accessing a column by its name\ndf['fname']\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# creating a new DataFrame and performing operations on it\ndf1 = pd.DataFrame()\n\n# adding a new column\ndf1['a'] = [1,2,3,4]\ndf1\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n    \n      3\n      4\n    \n  \n\n\n\n\n\n# adding a new column\ndf1['b'] = [10, 20, 30, 40]\n\n# adding two columns\ndf1['a'] + df1['b']\n\n0    11\n1    22\n2    33\n3    44\ndtype: int64"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Constructing a DataBlock",
    "text": "Constructing a DataBlock\nA DataBlock can be used to create Datasets from which DataLoaders can be created to use during training. A DataBlock is an object which contains the data and has helper functions which can access and transform the data.\nThey start by creating an empty DataBlock\n\ndblock = DataBlock()\ndblock\n\n<fastai.data.block.DataBlock at 0x7efe5c559d90>\n\n\nThe DataFrame with filenames and labels can be fed to the DataBlock to create a Datasets object, which is > an iterator that contains a training Dataset and validation Dataset\nEach dataset is\n\na collection that returns a tuple of your independent and dependent variable for a single item\n\nA Dataet created from an empty DataBlock (meaning, a DataBlock with no helper functions to tell it how the data is structured and accessed) will contain a tuple for each row of the DataFrame, where both values of the tuple are the same row.\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object, fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object)\n\n\nWhat we want is for the DataBlock to create Datasets of (independent, dependent) values. In this case, the independent variable is the image and the dependent variable is a list of labels.\nIn order to parse the DataFrame rows, we need to provide two helper functions to the DataBlock: get_x and get_y. In ordert to convert them to the appropriate objects that will be used in training, we need to provide two more arguments: ImageBlock and MultiCategoryBlock. In order for the DataBlock to correctly split the data into training and validation datasets, we need to define a splitter function and pass it as an argument as well.\nget_x will access the filename from each row of the DataFrame and convert it to a file path.\nget_y will access the labels from each row and split them into a list.\nImageBlock will take the file path and convert it to a PILImage object.\nMultiCategoryBlock will convert the list of labels to a one-hot encoded tensor using the Dataset’s vocab.\nsplitter will explicitly choose for the validation set the rows where is_valid is True.\nRandomResizedCrop will ensure that each image is the same size, which is a requirement for creating a tensor with all images.\n\ndef get_x(row): return path/'train'/row['fname']\ndef get_y(row): return row['labels'].split(' ')\ndef splitter(df):\n  train = df.index[~df['is_valid']].tolist()\n  valid = df.index[df['is_valid']].tolist()\n  return train, valid\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    splitter=splitter,\n    get_x=get_x,\n    get_y=get_y,\n    item_tfms = RandomResizedCrop(128, min_scale=0.35))\n\ndsets = dblock.datasets(df)\ndls = dblock.dataloaders(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0.]))\n\n\nThe Datasets vocab is a list of alphabetically ordered unique labels:\n\ndsets.train.vocab\n\n['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\n\nLet me breakdown the tuple returned by dsets.train[0]. The first value is a PILImageobject which can be viewed by calling its show() method:\n\ndsets.train[0][0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7efe5c3764d0>\n\n\n\n\n\nThe second value is a one-hot encoded list, where 1s are in the location of the labels in the corresponding vocab list. I’ll use the torch.where method to access the indices where there are 1s:\n\ntorch.where(dsets.train[0][1]==1)\n\n(TensorMultiCategory([6]),)\n\n\n\ndsets.train.vocab[torch.where(dsets.train[0][1]==1)[0]]\n\n(#1) ['car']\n\n\n\ndls.show_batch(nrows=1, ncols=3)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 4: Two-Digit MNIST Classifier",
    "text": "Chapter 4: Two-Digit MNIST Classifier\nI’ll first review the loss function used in the single-label classification models created in Chapters 4 and 5 before reviewing Binary Cross Entropy Loss introduced in this chapter.\nIn this chapter, we built a image classifier which would predict if an input image was an of the digit 3 or the digit 7.\nThe target (or expected outcome) is a list of 0s (for 7) and 1s (for 3). If we gave a batch of images of a 3, a 7 and a 3, the target would be [1, 0, 1].\nSuppose the model predicted the following values: [0.9, 0.4, 0.2] where each value represented the probability or confidence it had that each image was a 3.\nLoss represents the positive difference between the target and the prediction: - 1 - prediction when target == 1 - prediction when target == 0\nFor the first image, the model had 90% confidence it was a 3, and it was indeed a 3. The loss is 1 - 0.9 = 0.1.\nFor the second second image, the model had a 40% confidence it was a three, and the image was of a 7. The loss is 0.4.\nFor the last image, the model had a 20% confidence it was a 3, and the image was a 3. The loss is 1 - 0.2 = 0.8.\nThe average of these three losses is 1.3/3 or 0.433.\nThe following cell illustrates this with code:\n\ndef mnist_loss(predictions, targets):\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\ntargets = tensor([1,0,1])\npredictions = tensor([0.9, 0.4, 0.2])\nmnist_loss(predictions=predictions, targets=targets)\n\ntensor(0.4333)\n\n\nThe assumption that this loss function makes is that the predictions are always between 0 and 1. That may not always be true! In order to make this assumption explicit, we take the sigmoid of the prediction before calculating the loss. The sigmoid function outputs a value between 0 and 1 for any input value.\n\ntensor([0.4,-100,200]).sigmoid()\n\ntensor([0.5987, 0.0000, 1.0000])\n\n\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 5: 37 Breed Pet Classifier",
    "text": "Chapter 5: 37 Breed Pet Classifier\nIn this chapter, we train an image classifier that when given an input image, predicts which of the 37 pet breeds the image shows. The loss function needs to handle 37 activations for each image. In order to ensure the sum of those activations equals 1.0—so that the highest activation represents the model’s highest confidence—the softmax function is used. In order to increase the separation between probabilities, the softmax function’s output is passed through the logarithm function, and the negative value is taken. The combination of softmax and (negative) logarithm is called cross entropy loss.\nSuppose we had 4 images in a batch. The model would output activations something like this:\n\n# create a pseudo-random 4 x 37 tensor \n# with values from -2 to 2\nacts = (-2 - 2) * torch.rand(4, 37) + 2\nacts\n\ntensor([[-1.9994e+00,  7.0629e-01, -1.8230e+00,  8.6118e-02,  8.8579e-01,\n         -9.7763e-01,  9.7619e-01,  5.4613e-01,  9.2020e-01,  8.2653e-01,\n         -1.3831e+00,  1.2236e+00, -4.2582e-01,  1.1371e+00,  1.2409e+00,\n          1.4403e-02, -9.2988e-01, -1.1939e+00, -9.9743e-01, -1.9572e+00,\n         -6.8404e-02,  6.2455e-01,  8.6748e-01, -1.4574e+00, -1.4451e+00,\n          1.1349e-01,  1.7424e+00,  6.5414e-02, -1.2517e+00, -1.9933e+00,\n         -1.5570e+00,  1.3880e+00,  1.5099e+00,  6.2576e-01, -1.4279e-03,\n          1.7448e+00,  1.9862e+00],\n        [ 4.5219e-02,  4.6843e-01, -1.1474e+00, -1.8876e+00, -5.7879e-01,\n          6.9787e-01, -7.2457e-02, -1.7235e+00, -9.9028e-01,  1.2248e+00,\n          6.4889e-01,  5.0363e-01,  1.8472e-01, -1.0468e+00, -1.0113e+00,\n         -1.0628e+00,  1.9783e+00, -1.8394e+00, -8.0410e-02, -5.9383e-01,\n         -1.6868e+00, -2.6366e-01, -8.3354e-01,  6.8552e-01, -8.6600e-02,\n          1.6034e+00,  7.3355e-01,  1.3205e+00,  1.4004e+00, -5.2889e-01,\n          5.6740e-01, -9.6958e-01, -1.4997e+00,  4.6890e-01, -1.7328e+00,\n          1.0302e+00, -5.7672e-01],\n        [-2.0183e-01,  9.5745e-01, -6.7022e-01, -1.4942e+00, -1.7716e+00,\n         -1.5369e+00,  5.3614e-01,  2.1942e-01, -4.8692e-01, -1.0483e+00,\n         -1.3250e+00, -2.7229e-01,  7.0113e-01,  6.7435e-01,  1.3605e+00,\n         -5.5024e-01, -8.2829e-01, -3.0993e-01, -2.9132e-02, -6.5741e-01,\n         -1.8838e+00, -1.5611e+00,  1.3386e+00, -9.3677e-01,  9.4050e-01,\n          1.6461e+00, -1.7923e+00, -1.2952e+00, -1.4606e+00,  1.9617e+00,\n          1.8974e+00, -3.5640e-01, -5.1258e-01,  1.3049e+00,  9.6022e-01,\n          1.8340e+00, -1.6090e+00],\n        [ 3.3658e-01, -1.9117e+00,  1.3840e+00,  1.4359e+00,  3.0289e-01,\n         -1.9664e+00, -1.8941e+00,  4.2836e-02,  1.6804e+00,  1.5752e+00,\n         -4.4672e-01,  1.0409e+00, -2.8504e-01, -1.3567e+00,  3.1620e-01,\n         -1.9444e+00,  1.5615e+00, -5.0563e-01, -1.8748e+00, -1.1123e+00,\n         -1.9222e+00,  1.3545e+00, -2.9159e-01, -4.6669e-01,  1.2639e+00,\n         -1.4171e+00, -2.7517e-01, -1.2380e+00, -1.5908e+00,  1.4929e+00,\n          1.0642e+00, -3.4285e-01, -1.8219e+00,  1.6329e+00, -1.2953e+00,\n          1.7803e+00,  3.6970e-01]])\n\n\nPassing these through softmax will normalize them from 0 to 1:\n\nsm_acts = acts.softmax(dim=1)\nsm_acts[0], sm_acts[0].sum()\n\n(tensor([0.0020, 0.0302, 0.0024, 0.0162, 0.0361, 0.0056, 0.0396, 0.0257, 0.0374,\n         0.0341, 0.0037, 0.0507, 0.0097, 0.0465, 0.0516, 0.0151, 0.0059, 0.0045,\n         0.0055, 0.0021, 0.0139, 0.0278, 0.0355, 0.0035, 0.0035, 0.0167, 0.0851,\n         0.0159, 0.0043, 0.0020, 0.0031, 0.0597, 0.0675, 0.0279, 0.0149, 0.0853,\n         0.1086]), tensor(1.0000))\n\n\nTaking the negative log of this tensor will give us the final loss:\n\nnll_loss = -1. * torch.log(sm_acts)\nnll_loss\n\ntensor([[6.2054, 3.4997, 6.0290, 4.1199, 3.3202, 5.1836, 3.2298, 3.6599, 3.2858,\n         3.3795, 5.5891, 2.9825, 4.6318, 3.0690, 2.9651, 4.1916, 5.1359, 5.3999,\n         5.2035, 6.1632, 4.2744, 3.5815, 3.3385, 5.6635, 5.6511, 4.0925, 2.4636,\n         4.1406, 5.4577, 6.1994, 5.7630, 2.8180, 2.6961, 3.5803, 4.2074, 2.4612,\n         2.2198],\n        [3.9156, 3.4924, 5.1082, 5.8484, 4.5396, 3.2629, 4.0333, 5.6843, 4.9511,\n         2.7360, 3.3119, 3.4572, 3.7761, 5.0076, 4.9721, 5.0235, 1.9825, 5.8002,\n         4.0412, 4.5546, 5.6476, 4.2245, 4.7943, 3.2753, 4.0474, 2.3574, 3.2273,\n         2.6403, 2.5604, 4.4897, 3.3934, 4.9304, 5.4605, 3.4919, 5.6936, 2.9306,\n         4.5375],\n        [4.3197, 3.1604, 4.7881, 5.6121, 5.8895, 5.6548, 3.5817, 3.8985, 4.6048,\n         5.1662, 5.4429, 4.3902, 3.4167, 3.4435, 2.7574, 4.6681, 4.9462, 4.4278,\n         4.1470, 4.7753, 6.0016, 5.6790, 2.7793, 5.0546, 3.1774, 2.4718, 5.9102,\n         5.4131, 5.5785, 2.1562, 2.2205, 4.4743, 4.6305, 2.8130, 3.1577, 2.2839,\n         5.7269],\n        [3.8515, 6.0998, 2.8041, 2.7522, 3.8852, 6.1545, 6.0822, 4.1453, 2.5077,\n         2.6129, 4.6348, 3.1472, 4.4732, 5.5448, 3.8719, 6.1325, 2.6266, 4.6937,\n         6.0629, 5.3004, 6.1103, 2.8336, 4.4797, 4.6548, 2.9243, 5.6052, 4.4633,\n         5.4261, 5.7790, 2.6952, 3.1239, 4.5310, 6.0101, 2.5552, 5.4834, 2.4078,\n         3.8184]])\n\n\nSuppose the target for each image was given by the following tensor, where the target is an integer from 0 to 36 representing one of the pet breeds:\n\ntargs = tensor([3, 0, 34, 10])\nidx = range(4)\nnll_loss[idx, targs]\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\ndef cross_entropy(acts, targs):\n  idx = range(len(targs))\n  sm_acts = acts.softmax(dim=1)\n  nll_loss = -1. * torch.log(sm_acts)\n  return nll_loss[idx, targs].mean()\n\nI compare this with the built-in F.cross_entropy and nn.CrossEntropyLoss functions:\n\nF.cross_entropy(acts, targs,reduction='none')\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targs)\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\nNote that the nn version of the loss function returns an instantiation of that function which then must be called with the activations and targets as its inputs.\n\ntype(nn.CrossEntropyLoss(reduction='none'))\n\ntorch.nn.modules.loss.CrossEntropyLoss"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Binary Cross Entropy Loss",
    "text": "Binary Cross Entropy Loss\nThe authors begin the discussion of explaining the multi-label classification model loss function by observing the activations from the trained model. I’ll do the same—I love that approach since it grounds the concepts involved in the construction of loss function in the actual model outputs.\n\nlearn = cnn_learner(dls, resnet18)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\nx, y = dls.train.one_batch()\nif torch.cuda.is_available():\n    learn.model.cuda()\nactivs = learn.model(x)\nactivs.shape\n\ntorch.Size([64, 20])\n\n\nEach batch has 64 images and each of those images has 20 activations, one for each label in .vocab. Currently, they are not restricted to values between 0 and 1.\nNote: the activations tensor has to first be placed on the cpu and then detached from the graph (which is used to track and calculate gradients of the weights with respect to the loss function) before it can be converted to a numpy array used for the plot.\n\nys = activs[0].cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nPassing them through a sigmoid function achieves that:\n\nys = activs[0].sigmoid().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe negative log of the activations is taken in order to push the differences between loss values. For vocab where the target is 1, -log(inputs) is calculated. For vocab where the target is 0, -log(1-inputs) is calculated. This seems counterintuitive at first, but let’s take a look at the plot of these functions:\n\nys = -activs[0].sigmoid().log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe sigmoid activations that were very close to 0 (Vocab Index = 0, 2, 5, and 16) are now much larger than those that were very close to 1 (Vocab Index = 6, 14, and 18). Since the target is 1, this correctly assigns a larger loss to the inaccurate predictions and the smaller loss to the accurate ones. We can say the same (but opposite) for -log(1-inputs), which is used when the target is 0.:\n\nys = -(1- activs[0].sigmoid()).log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nFinally, the mean of all image loss values is taken for the batch. The Binary Cross Entropy Function look likes this:\n\ndef binary_cross_entropy(inputs, targets):\n  inputs = inputs.sigmoid()\n  return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\nThe inputs (the activations for each vocab value)) is the first value and the targets of each image are the second value of the dls.train.one_batch() tuple.\n\nbinary_cross_entropy(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\nI will compare this with the built-in function F.binary_cross_entropy_with_logits and function class nn.BCEWithLogitsLoss to make sure I receive the same result.\n\nF.binary_cross_entropy_with_logits(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\n\nnn.BCEWithLogitsLoss()(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Mult-Label Classification Accuracy",
    "text": "Mult-Label Classification Accuracy\nFor single-label classification, the accuracy function compared whether the index of the highest activation matched the index of the target vocab. A single index for a single label.\n\ndef accuracy(inputs, targets, axis=-1):\n  predictions = inputs.argmax(dim=axis)\n  return (predictions==targets).float().mean()\n\nFor multi-label classification, each image can have more than one correct corresponding vocab index and the corresponding activations may not be the maximum of the inputs tensor. So instead of using the maximum, a threshold is used to identify predictions. If the activation is above that threshold, it’s considered to be a prediction.\n\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n  if sigmoid: inp = inp.sigmoid()\n  return ((inp > thresh)==targ.bool()).float().mean()\n\ntarg is a one-hot encoded Tensor, so 1s are converted to True and 0s are converted to False using the .bool method."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Training the Model",
    "text": "Training the Model\nAt last! I can now train the model, setting a different accuracy threshold as needed using the built-in partial function.\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.942256\n      0.698276\n      0.239323\n      00:29\n    \n    \n      1\n      0.821279\n      0.566598\n      0.281633\n      00:28\n    \n    \n      2\n      0.602543\n      0.208145\n      0.805498\n      00:28\n    \n    \n      3\n      0.359614\n      0.125162\n      0.939801\n      00:28\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.133149\n      0.112483\n      0.947072\n      00:29\n    \n    \n      1\n      0.115643\n      0.105032\n      0.953028\n      00:29\n    \n    \n      2\n      0.096643\n      0.103564\n      0.952769\n      00:29\n    \n  \n\n\n\nIn about three and a half minutes, this model was able to achieve more than 95% accuracy. I’ll look at its predictions on the validation images:\n\nlearn.show_results(max_n=18)\n\n\n\n\n\n\n\nVarying the threshold will vary the accuracy of the model. The metrics of the learner can be changed after training, and calling the validate method will recalculate the accuracy:\n\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.930816650390625]\n\n\nA threshold of 0.1 decreases the accuracy of the model, as does a threshold of 0.99. A 0.1 threshold includes labels for which the model was not confident, and a 0.99 threshold exclused labels for which the model was not very confident. I can calculate and plot the accuracy for a range of thresholds, as they did in the book:\n\npreds, targs = learn.get_preds()\nxs = torch.linspace(0.05, 0.95, 29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\n\n\n\n\nbest_threshold = xs[np.argmax(accs)]\nbest_threshold\n\ntensor(0.4679)\n\n\n\nlearn.metrics = partial(accuracy_multi, thresh=best_threshold)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.9636053442955017]\n\n\nThe highest accuracy (96.36%) is achieved when the threshold is 0.4679."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Regression",
    "text": "Regression\nThe authors provide some context here which, while I can appreciate, judge I won’t fully understand until I experience the next 5 or 6 chapters.\n\nA model is defined by its independent and dependent variables, along with its loss function. The means that there’s really a far wider array of models than just the simple domain-based split\n\nThe “domain-based split” is a reference to the distinction between computer vision, NLP and other different types of problems.\nTo illustrate their point, they have us work through an image regression problem with much of the same process (and model) as an image classification problem.\n\n# download data\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\n# helper functions to retrieve images\n# and to retrieve text files\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\n\n\n# check that `img2pose` converts file name correctly\nimg_files[0], img2pose(img_files[0])\n\n(Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_rgb.jpg'),\n Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_pose.txt'))\n\n\n\n# check image size\nim = PILImage.create(img_files[0])\nim.shape\n\n(480, 640)\n\n\n\n# view the image\nim.to_thumb(160)\n\n\n\n\n\n# helper function to extract coordinates\n# of the subject's center of head\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n  ctr = np.genfromtxt(img2pose(f), skip_header=3)\n  c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n  c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n  return tensor([c1,c2])\n\n\n# check coordinates of the first file\nget_ctr(img_files[0])\n\ntensor([444.7946, 261.7657])\n\n\n\n# create the DataBlock\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)]\n)\n\n\n# confirm that the data looks OK\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n# view tensors\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\n\nEach batch has 64 images. Each image has 3 channels (rgb) and is 240x320 pixels in size. Each image has 1 pair of coordinates.\n\n# view a single coordinate pair\nyb[0]\n\nTensorPoint([[0.0170, 0.3403]], device='cuda:0')\n\n\n\n# create Learner object\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\nThe y_range argument shifts the final layer’s sigmoid output to a coordinate between -1 and 1. The sigmoid function is transformed using the following function.\n\ndef plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = torch.linspace(min,max)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)\n\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\nplot_function(partial(sigmoid_range, lo=-1, hi=1), min=-4, max=4)\n\n\n\n\n\n# confirm loss function\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nfastai has chosen MSE as the loss function, which is appropriate for a regression problem.\n\n# pick a learning rate\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.033113110810518265)\n\n\n\n\n\n\n# use lr = 2e-2\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.047852\n      0.011552\n      01:55\n    \n    \n      1\n      0.007220\n      0.002150\n      01:56\n    \n    \n      2\n      0.003190\n      0.001313\n      01:56\n    \n    \n      3\n      0.002376\n      0.000295\n      01:56\n    \n    \n      4\n      0.001650\n      0.000106\n      01:54\n    \n  \n\n\n\nA loss of 0.000106 is an accuracy of:\n\nmath.sqrt(0.000106)\n\n0.010295630140987\n\n\nThe conclusion to this (what has felt like a marathon of a) chapter is profound:\n\nIn problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different number of outputs. The loss function is the one thing that changes, which is why it’s important to double-check that you are using the right loss function for your problem…make sure you think hard about your loss function, and remember that you most probably want the following:\n\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "",
    "text": "A chloropleth map of Minnesota Census data\nIn this blog post, I’ll walk through my process of creating an ArcGIS geodatabase and a set of layouts visualizing U.S. Census Data. The data used for this app is from table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months).\nYou can view the final layout PDFs at the following links:"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nGet the Data\n\nTract Boundaries\nACS 5-Year Estimates\nUsing data.census.gov\nUsing the censusapi R package\n\nConnect Data to Geodatabase\n\nTract Boundaries\nACS 5-Year Estimates\n\nVisualize Data\n\nCreate a Map\nCreate a Symbology\nCreate a Layout\n\nNormalize the Data\n\nCreate Additional Layouts"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Get the Data",
    "text": "Get the Data\n\nTract Boundaries\n\nDownload and unzip 2019 TIGER Shapefile for MN (tl_2019_27_tract.zip) (corresponds to the final year, 2019, in the ACS 5-year estimates). These will contain the Census Tract geographies needed to create a map in ArcGIS.\n\n\n\nACS 5-Year Estimates\n\nUsing data.census.gov\n\nOn data.census.gov, search for B20005\n\n\n\nSelect the link to the Table B20005 with “2019 inflation-adjusted dollars”\n\n\n\nClick the dropdown at the top next to the label Product and select 2015: ACS 5-Year Estimates Detailed Tables\n\n\n\nClick Customize Table at the top right of the page\n\n\n\nIn the Geo* section, click Tract > Minnesota > All Census Tracts within Minnesota\n\n\n\nOnce it’s finished loading, click Close and then Download Table\n\n\n\nOnce downloaded, extract the zip folder and open the file ACSDT52015.B20005_data_with_overlays….xslx_ in Excel any tool that can handle tabular data\nSlice the last 11 characters of the GEO_ID (using the RIGHT function in a new column) to replace the existing GEO_ID column values. For example, a GEO_ID of 1400000US27029000100 should be replaced with 27029000100. This will later on be matched with the GEOID field in the tl_2019_27_tract shapefile\nSave/export the file as .XLSX\n\n\n\nUsing the censusapi R package\nPass the following arguments to the censusapi::listCensusMetadata function and assign its return value to B20005_vars:\n\nB20005_vars <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  type=\"variables\",\n  group=\"B20005\"\n)\n\n\nPass the following arguments to censusapi::getCensus and assign its return value to B20005:\n\n\nB20005 <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  region=\"tract:*\",\n  regionin=\"state:27\", # 27 = Minnesota state FIPS code\n  vars=c(\"GEO_ID\", \"NAME\", B20005_vars$name)\n)\n\n\nReplace GEO_ID (or create a new column) with the last 11 characters\n\n\nB20005 <- substr(B20005$GEO_ID, 10, 20)\n\n\nExport to an .XLSX file\n\n\nwrite.xlsx(B20005, “acs5_b20005_minnesota.xlsx”, row.names = FALSE)"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Connect Data to Geodatabase",
    "text": "Connect Data to Geodatabase\nOpen ArcGIS Pro and start a new project.\n\nTract Boundaries\n\nRight click Folders in the Contents pane and click Add folder connection\n\n\n\nSelect the downloaded (and extracted) tl_2019_27_tract folder and click OK\n\n\n\nClick on tl_2019_27_tract folder in the Contents pane\nIn the Catalog pane, right-click tl_2019_27.shp and then click Export > Feature Class to Geodatabase\n\n\n\nConfirm Input Features (tl_2019_27_tract.shp) and Output Geodatabase (Default.gdb or whatever geodatabase you are connected to) and then click the green Run button\nRefresh the Geodatabase and click on it in the Contents pane to view the added shapefile\n\n\n\n\nACS 5-Year Estimates\n\nUnder the View ribbon click on Geoprocessing to open that pane\nIn the Geoprocessing pane, search for Join Field and click on it\n\n\n\nNext to Input Table click on the folder icon to Browse. Select the tl_2019_27_tract table in your geodatabase\n\n\n\nClick the Input Join Field dropdown and select GEOID\nNext to Join Table click on the folder icon to Browse. Select the acs5_b20005_minnesota$ Excel table and click OK (note: the Excel table is inside the XLSX file)\n\n\n\nType GEO_ID under Join Table Field\nClick on the down arrow next to Transfer Fields and select B20005_002E, B20005_003E, B20005_049E, and B20005_050E\n\n\n\nClick on Validate Join\n\n\n\nClick on Run\nA success message should be displayed at the bottom of the Geoprocessing pane"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Visualize the Data",
    "text": "Visualize the Data\nIn this section, I’ll create maps and layouts to visualize the population estimates using Census Tract spatial data.\n\nCreate a Map\n\nIn the Catalog pane, right-click tl_2019_27_tract > Add to New > Map\n\n\n\nTo reference the raw data: from the Feature Layer ribbon, click Attribute Table\n\n\n\n\n\nCreate a Symbology\n\nSelect the tl_2019_27_tract layer in Contents pane\nClick Appearance under the Feature Layer ribbon\nClick the down arrow on Symbology and select Graduated Colors\n\n\n\nSelect B20005_002E in the Field dropdown and Natural Breaks (Jenks) for the Method\n\n\n\nThe class breaks created by this method do not reliably classify the data, which is determined using the City of New York Department of Planning Map Reliability Calculator. There’s a 10.1% chance that a tract is erroneously classified.\n\n\n\nAfter adjusting the class breaks, the following result in a reliable result (less than 10% chance of misclassifying any geography on the map and less than 20% of misclassifying estimates within a class due to sampling error)\n\n\n\nApply these breaks in the Classes tab in the Symbology pane\n\n\n\nThe Map pane displays the updated choropleth\n\n\n\n\nCreate a Layout\nUnder the Insert ribbon click on New Layout and Letter (8.5” x 11”)\n\n\n\nOn the Insert ribbon, click Map Frame and Default Extent under the Map category\n\n\n\nClick and drag the cursor to draw the Map Frame. Under the Layout ribbon select Activate and zoom/pan until the full choropleth is visible. Click Deactivate when you’re finished.\n\n\n\nAdd guides to create 0.5 inch margins by right-clicking on rulers clicking Add Guide\n\n\n\nUnder the Insert ribbon click on Legend and draw a rectangle underneath the map\n\n\n\nRight-click the legend and click Properties to format the font size, text visibility (under Legend Item in the dropdown next to Legend in the Format Legend panel) and more\n\n\n\nOn the Ribbon tab in the Graphics and Text panel, you can choose different text types to add text to your layout. I’ve added titles and explanatory text.\n\n\n\nThe census tracts for the city of Minneapolis are too small to be clearly visible. Under the Insert ribbon click Map Frame, select the map and draw a small rectangle over Wisconsin.\nWith the new Map Frame selected, click Reshape > Circle under the Insert ribbon. Draw a circle over the rectangular map.\n\n\n\nRight-click on the circular map and click Properties to add a border. Add a textbox to label it as the City of Minneapolis.\n\n\n\nFrom the Graphics and Text panel on the Insert ribbon use the straight line and circle tool to add some visual cues indicating that the map frame is a detail view of the city\n\n\n\nUnder the Share ribbon, select Export Layout and export it to a PDF file"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Normalize the Data",
    "text": "Normalize the Data\nWhile the worker population estimates gives us a sense of how workers are distributed across the state, they are a proxy for population density. Census Tracts in Urban areas, like the Minneapolis, will likely have more workers than Rural areas, because they have a higher population. To supplement this layout, I’ll create layouts that show the percentage of the total sex population who are full time workers.\n\nTo duplicate the Male Full TIme Estimates layout, right-click it in the Catalog pane, click Copy and then right click in the gray area underneath it and click Paste\n\n\n\n\nRename the layout to Male Full Time Percentages and open it\nRename the two maps in the Contents pane\n\n\n\nRight-click tl_2019_27_tract under Main Map and click Symbology to open the Symbology pane\n\n\n\nSelect B20005_002E (Total Male Estimate) in the Normalization dropdown. This will be the value that divides a Census Tract’s population estimate\n\n\n\nCalculate the Margin of Error (MOE) for the percentage of total male workers who are full time employed using equation 6 from the “Calculating Measures of Error for Derived Estimates” in the Understanding and Using American Community Survey Data: What All Data Users Need to Know handbook in order to determine the class break reliability. In the equation below, P = X/Y is the percentage of full time workers in the tract (X= B20005_003E and Y = B20005_002E)\n\n\n\nOne reliable set of class breaks, which were few and far between, was the following:\n\n\n\nApply those class breaks in the Symbology pane and update the text to match\n\n\n\nCreate Additional Layouts\n\nRepeat the process to create the following Layouts given the following class breaks\n\nFemale Full Time Estimates\n\n\n\n\nFemale Full Time Percentages\n\n\n\nI hope you enjoyed this tutorial."
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "",
    "text": "Here’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, I work through the first of four “Further Research” problems assigned at the end of Chapter 7 in the textbook “Deep Learning for Coders with fastai and PyTorch”.\nThe prompt for this exercise is:\n\nUse the fastai documentation to build a function that crops an image to a square in each of the four corners; then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai?"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "What is Test Time Augmentation?",
    "text": "What is Test Time Augmentation?\nI’ll quote directly from the text:\n\nDuring inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n\nTTA is data augmentation during validation, in hopes that objects located outside the center of the image (which is the default fastai validation image crop) can be recognized by the model in order to increase the model’s accuracy.\nThe default Learner.tta method averages the predictions on the center crop and four randomly generated crops. The method I’ll create will average the predictions between the center crop and four corner crops.\n\n\n\ntta.png"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "User-defined Test Time Augmentation",
    "text": "User-defined Test Time Augmentation\n\nRead and understand the Learner.tta and RandomCrop source code\ndef tta(self:Learner, ds_idx=1, dl=None, n=4, item_tfms=None, batch_tfms=None, beta=0.25, use_max=False):\n    \"Return predictions on the `ds_idx` dataset or `dl` using Test Time Augmentation\"\n    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n    if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n    try:\n        self(_before_epoch)\n        with dl.dataset.set_split_idx(0), self.no_mbar():\n            if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n            aug_preds = []\n            for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n        aug_preds = torch.cat(aug_preds)\n        aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n        self.epoch = n\n        with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n    finally: self(event.after_fit)\n\n    if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n    preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n    return preds,targs\nclass RandomCrop(RandTransform):\n    \"Randomly crop an image to `size`\"\n    split_idx,order = None,1\n    def __init__(self, size, **kwargs):\n        size = _process_sz(size)\n        store_attr()\n        super().__init__(**kwargs)\n\n    def before_call(self, b, split_idx):\n        self.orig_sz = _get_sz(b)\n        if split_idx: self.tl = (self.orig_sz-self.size)//2\n        else:\n            wd = self.orig_sz[0] - self.size[0]\n            hd = self.orig_sz[1] - self.size[1]\n            w_rand = (wd, -1) if wd < 0 else (0, wd)\n            h_rand = (hd, -1) if hd < 0 else (0, hd)\n            self.tl = fastuple(random.randint(*w_rand), random.randint(*h_rand))\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n        return x.crop_pad(self.size, self.tl, orig_sz=self.orig_sz)\n\n\nPractice cropping images using the .crop method on a PILImage object\nA PIL Image has a method called crop which takes a crop rectangle tuple, (left, upper, right, lower) and crops the image within those pixel bounds.\nHere’s an image with a grizzly bear at the top and a black bear on the bottom. There are four coordinates of interest: left, upper, right and bottom. The leftmost points on the image are assigned a pixel value of 0. The rightmost points are located at the image width pixel pixel value. The uppermost points are at pixel 0, and the bottommost points are at the image height pixel value.\n\nf = \"/content/gdrive/MyDrive/fastai-course-v4/images/test/grizzly_black.png\"\nimg = PILImage.create(f)\nimg.to_thumb(320)\n\n\n\n\n\nTop-Left Corner Crop\nA top-left corner crop the corresponds to a left pixel of 0, upper pixel 0, right pixel of 224, and bottom pixel of 224. The order in the tuple is left, upper, right, bottom, so 0, 0, 224, 224. You can see that this crop is taken from the top left corner of the original image.\n\nimg.crop((0,0,224,224))\n\n\n\n\n\n\nTop Right Corner Crop\nFor the top right corner, I get the image width since the left end of the crop will be 224 pixels from the right end of the image. That translates to w-224. The upper pixel is 0, and the rightmost pixel is at w, and the bottom pixel is 224. You can see that this crop is at the top right corner of the original.\n\nw = img.width\nh = img.height\nimg.crop((w-224, 0, w, 224))\n\n\n\n\n\n\nBottom Right Corner Crop\nFor the bottom right corner the left pixel is 224 from the right end, w-224, the upper pixel is 224 from the bottom, h-224, the right pixel is at w, and the bottom is at h.\n\nimg.crop((w-224, h-224, w, h))\n\n\n\n\n\n\nBottom Left Corner Crop\nThe bottom left corner’s leftmost pixel is 0, uppermost pixel is 224 pixels from the bottom of the whole image, h - 224, the rightmost pixel is 224, and bottommost pixel is the bottom of the whole image, at h.\n\nimg.crop((0, h-224, 224, h))\n\n\n\n\n\n\nCenter Crop\nFinally, for the center crop, the leftmost pixel is 112 left of the image center, w/2 - 112, the upper pixel is 112 above the image center, h/2 - 112, the rightmost pixel is 112 right of the center, w/2 + 112, and the bottom pixel is 112 below the center, h/2 + 112.\n\nimg.crop((w/2-112, h/2-112, w/2+112,h/2+112))\n\n\n\nSummary\nTo better visualize this, here are a couple of images which show the left, upper, right and bottom coordinates for the corner and center crops.\nSummary of corner crop arguments (left, upper, right, bottom)\n\n\n\ncrop_dimensions-01.png\n\n\nSummary of center crop arguments (left, upper, right, bottom)\n\n\n\ncenter_crop_dimensions-01.png\n\n\n\n\n\nDefine a function which takes an image and returns a stacked Tensor with four corner crops and a center crop\nI wrap those five lines of code into a function called corner_crop, which takes a PILImage img, and a square side length size (defaulted to 224) as its arguments. It first grabs the width and height of the image. And then goes on to save the crops of the four corners and center as TensorImages, returning them all in a single stacked Tensor.\n\ndef corner_crop(img, size=224):\n  \"\"\"Returns a Tensor with 5 cropped square images\n  img: PILImage\n  size: int\n  \"\"\"\n  w,h = img.width, img.height\n  top_left = TensorImage(img.crop((0,0,size,size)))\n  top_right = TensorImage(img.crop((w-size, 0, w, size)))\n  bottom_right = TensorImage(img.crop((w-size, h-size, w, h)))\n  bottom_left = TensorImage(img.crop((0, h-size, size, h)))\n  center = TensorImage(img.crop((w/2-size/2, h/2-size/2, w/2+size/2,h/2+size/2)))\n  return torch.stack([top_left, top_right, bottom_right, bottom_left, center])\n\nI’ll test the corner_crop function and make sure that the five images are cropped correctly.\nHere’s the top left corner.\n\nimgs = corner_crop(img)\n\n# Top Left Corner Crop\nimgs[0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1a177d0>\n\n\n\n\n\nTop right corner:\n\n# Top Right Corner Crop\nimgs[1].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e197da50>\n\n\n\n\n\nBottom right:\n\n# Bottom Right Corner Crop\nimgs[2].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e146ed50>\n\n\n\n\n\nBottom left:\n\n# Bottom Left Corner Crop\nimgs[3].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424dd0>\n\n\n\n\n\nAnd center:\n\n# Center Crop\nimgs[4].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424450>\n\n\n\n\n\n\n\nDefine a new CornerCrop transform by extending the Transform class definition\nThe main purpose for all of that was for me to wrap my head around how the crop behavior functions so that I can wrap that into a transform.\nTransforms are any function that you want to apply to your data. I’ll extend the base Transform class and add in the functionality I need for these crops. When an object of the CornerCrop class is constructed, the constructor takes size and corner_type arguments. Since I’ll use this within a for-loop, the corner_type argument is an integer from 0 to 3, corresponding to the loop counter. The transform is applied to the data during the .encodes method. I grab the original image width and height, and create a list of cropped images using the left, upper, right, bottom coordinates we saw above. Finally, based on the corner_type, the corresponding crop is returned.\n\nclass CornerCrop(Transform):\n    \"Create 4 corner and 1 center crop of `size`\"\n    def __init__(self, size, corner_type=0, **kwargs):\n      self.size = size\n      self.corner_type = corner_type\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n      self.w, self.h = x.size\n      self.crops = [\n                    x.crop((0,0,self.size, self.size)),\n                    x.crop((self.w - self.size, 0, self.w, self.size)),\n                    x.crop((self.w-self.size, self.h-self.size, self.w, self.h)),\n                    x.crop((0, self.h-self.size, self.size, self.h))\n                    ]\n      return self.crops[self.corner_type]\n\nTo test this transform, I created an image with top left, top right, bottom right and bottom left identified. I created multiple copies so that I can create batches.\n\n# test image for CornerCrop\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\nImage.open((path/'01.jpg'))\n\n\n\n\nI create a DataBlock and pass my CornerCrop to the item_tfms parameter. I’ll cycle through the different corner types. 0 corresponds to top left, 1 is top right, 2 is bottom right and 3 is bottom left. All images in my batch should be cropped to the same corner.\nI set corner_type to 0, build the DataBlock and DataLoaders and the batch shows top left.\n\n# get the data\n# path = untar_data(URLs.IMAGENETTE)\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,0))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 1, build the DataBlock and DataLoaders and the batch shows top right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,1))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 2, build the DataBlock and DataLoaders and the batch shows bottom right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,2))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 3, build the DataBlock and DataLoaders and the batch shows bottom left.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,3))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nNow, I can implement this transform into a new TTA method.\n\n\nDefine a new Learner.corner_crop_tta method by repurposing the existing Learner.tta definition\nI’ll largely rely on the definition of tta in the built-in Learner class. In this method, predictions are calculated on four sets of augmented data (images) and then averaged along with predictions on a center-crop dataset.\nIn the existing for-loop, four sets of predictions on randomly generated crops are appended into a list.\nfor i in self.progress.mbar if hasattr(self,'progress') else range(n):\n  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\nIn my loop, I create a new DataLoader each time, passing a different corner_type argument to the CornerCrop transform. I also have to pass the ToTensor transform, so that the PIL Image is converted to a Tensor. In the first iteration, it will append predictions on the top left corner crops. In the next one, it will append predictions on the top right, then the bottom right, and finally on the fourth loop, the bottom left.\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nSince I am to average these with the center-crop image predictions, I’ll create a new DataLoader without the CornerCrop transform and calculate the predictions on those images:\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\nFinally, I’ll append the center crop preds to aug_preds list, concatenate them into a single tensor and take the mean of the predictions:\naug_preds.append(preds[None])\npreds = torch.cat(aug_preds).mean(0)\nI decided to create a new Learner2 class which extends the built-in the Learner, and added the corner_crop_tta method by copying over the tta method, commenting out the lines I won’t need and adding the lines and changes I’ve written above.\n\nclass Learner2(Learner):\n  def corner_crop_tta(self:Learner, ds_idx=1, dl=None, n=4, beta=0.25, use_max=False):\n      \"Return predictions on the `ds_idx` dataset or `dl` using Corner Crop Test Time Augmentation\"\n      if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n      # if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n      try:\n          #self(_before_epoch)\n          with dl.dataset.set_split_idx(0), self.no_mbar():\n              if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n              aug_preds = []\n              # Crop image from four corners\n              for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                  dl = dl.new(after_item=[CornerCrop(224,i), ToTensor])\n                  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n         # aug_preds = torch.cat(aug_preds)\n         # aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n          self.epoch = n\n          dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n          # Crop image from center\n          with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n          aug_preds.append(preds[None])\n      finally: self(event.after_fit)\n\n     # if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n     # preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n     # preds = torch.cat([aug_preds, preds]).mean(0)\n      preds = torch.cat(aug_preds).mean(0)\n      return preds,targs\n\n\n\nImplement this new TTA method on the Imagenette classification model\nIn the last section of this notebook, I train a model on the Imagenette dataset, which a subset of the larger ImageNet dataset. Imagenette has 10 distinct classes.\n\n# get the data\npath = untar_data(URLs.IMAGENETTE)\n\n# build the DataBlock and DataLoaders \n# for a single-label classification\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label, # image folder names are the class names\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\ndls = dblock.dataloaders(path, bs=64)\n\n# view a batch\ndls.show_batch()\n\n\n\n\n\n\n\n\n# Try `CornerCrop` on a new DataLoader\n# add `ToTensor` transform to conver PILImage to TensorImage\nnew_dl = dls[1].new(after_item=[CornerCrop(224,3), ToTensor])\nnew_dl.show_batch()\n\n\n\n\n\n# baseline training\nmodel = xresnet50()\nlearn = Learner2(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.628959\n      2.382344\n      0.450336\n      02:39\n    \n    \n      1\n      1.258259\n      3.365233\n      0.386482\n      02:45\n    \n    \n      2\n      0.992097\n      1.129573\n      0.653473\n      02:49\n    \n    \n      3\n      0.709120\n      0.643617\n      0.802091\n      02:47\n    \n    \n      4\n      0.571318\n      0.571139\n      0.824122\n      02:45\n    \n  \n\n\n\nI run the default tta method, pass the predictions and targets to the accuracy function and calculate an accuracy of about 83.5% percent. Which is higher than the default center crop validation accuracy.\n\n# built-in TTA method\npreds_tta, targs_tta = learn.tta()\naccuracy(preds_tta, targs_tta).item()\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\n0.8345780372619629\n\n\nFinally, I run my new corner_crop_tta method, pass the predictions and targets to the accuracy function, and calculate an accuracy of about 70.9% percent. Which is lower than the default center crop validation accuracy.\n\n# user-defined TTA method\npreds, targs = learn.corner_crop_tta()\naccuracy(preds, targs).item()\n\n\n\n\n0.7098581194877625\n\n\nI’ll walk through the corner_crop_tta code to verify the accuracy calculated above.\nI first create an empty list for my augmented image predictions.\nThen I loop through a range of 4, each time creating a new DataLoader which applies the CornerCrop transform for each corner type and append the predictions onto the list.\n\n# get predictions on corner cropped validation images\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nlen(aug_preds), aug_preds[0].shape\n\n\n\n\n\n\n\n\n\n\n\n\n\n(4, torch.Size([1, 2678, 1000]))\n\n\nI then create a new DataLoader without my transform, and get those predictions.\n\n# get predictions on center crop validation images\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\npreds.shape\n\n\n\n\ntorch.Size([2678, 1000])\n\n\nThe shape of these predictions is missing an axis, so I pass None as a Key and it adds on a new axis.\n\n# add an axis to match augmented prediction tensor shape\npreds = preds[None]\npreds.shape\n\ntorch.Size([1, 2678, 1000])\n\n\nI append the center crop predictions onto the augmented predictions and concatenate all five sets of predictions into a Tensor and calculate the mean.\n\n# average all 5 sets of predictions\naug_preds.append(preds)\npreds = torch.cat(aug_preds).mean(0)\n\nI then pass those average predictions and the targets to the accuracy function calculate the accuracy which is slightly higher than above. I ran these five cells multiple times and got the same accuracy value. When I ran the corner_crop_tta method multiple times, I got different accuracy values each time. Something in the corner_crop_tta definition is incorrect. I’ll go with this value since it was consistent.\n\n# calculate validation set accuracy\naccuracy(preds, targs).item()\n\n0.7311426401138306\n\n\nThe following table summarize the results from this training:\n\n\n\nValidation\nAccuracy\n\n\n\n\nCenter Crop\n82.4%\n\n\nCenter Crop + 4 Random Crops: Linearly Interpolated\n83.5%\n\n\nCenter Crop + 4 Random Crops: Averaged\n73.1%\n\n\n\nThere are a few further research items I should pursue in the future:\n\nFix the corner_crop_tta method so that it returns the same accuracy each time it’s run on the same trained model\nTry corner_crop_tta on a multi-label classification dataset such as PASCAL\nTry linear interpolation (between center crop and corner crop maximum) instead of mean"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "title": "Transcribing Sherlock into Spanish",
    "section": "",
    "text": "Sherlock Holmes kneeling next to Toby the bloodhound and pointing, likely towards where he thinks Toby should go next\nTranscription Progress (00:06:07 out of 17:40:32 transcribed)"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Background",
    "text": "Background\nI have watched all four seasons of BBC’s Sherlock probably 5 times. I learn something new about it each time.\nI have tried to learn Spanish using Duolingo, stopping and re-starting every year or so, without much success.\nI don’t really recall how the thought came about but I decided to combine my love of the show with my desire to learn Spanish into one project—this one!"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Setup",
    "text": "Setup\nUsing the embedded Google Translate UI and my partner’s translator-level knowledge of the language, I am transcribing every word of the show into Spanish.\n\n\n\nA screenshot of my translation setup: Google Translate embedded underneath the search bar—the result of googling “Google Translate”. I’ve typed “Sherlock Holmes” in the “English” textbox on the left and it has translated to “Sherlock Holmes” in the Spanish output on the right.\n\n\nIn a second tab, I have the show open (with subtitles on).\n\n\n\nA screenshot of Sherlock playing in the Amazon Prime Video player\n\n\nI transcribe in a .txt file titled transcript.txt, documenting the following fields:\n\nseason number\nepisode number\ntimestamp (hours::minutes:seconds)\nwho is the speaker?\nthe english transcription of what they say\nthe spanish translation of that\nnotes which usually documents specific word translations\n\nAs an example, the first bit of dialogue in the series is John Watson’s therapist Ella asking him “How’s your blog going?” which translates to “Cómo va tu blog?” Where va = goes.\nHow goes your blog? I would say quite well heheheh.\nseason,episode,time,speaker,english,spanish,notes\n1,1,00:01:30,ella,how's your blog going?,cómo va tu blog?, va = goes"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "title": "Transcribing Sherlock into Spanish",
    "section": "What I’m Learning",
    "text": "What I’m Learning\nI’ll write in this blog post some examples of the translations and how I’m thinking through the process, as well as what I’m learning from discussions with my partner.\nThree main themes I’m seeing so far about translating from English to Spanish:\n\nwhich words to use depends a lot on context.\nwords that sound the same but mean different things will sometimes have different emphasis.\na word that is technically correct may not be used frequently in conversation.\n\nI’m not quite sure how to best document what I’m learning so I’ll just start writing.\n\nElla: “You haven’t written a word, have you?”\nSomething I enjoy doing is translating the Spanish back into English without changing word positions. The benefit of this exercise of translating and translating back is that it reveals (or focuses my attention on) nuances I wouldn’t otherwise be aware of.\nEnglish: You haven’t written a word, have you?\nSpanish: No has escrito una palabra verdad?\nBack to English: Not you have written a word true?\nI asked my partner how she would translate it and she said: No has escrito ni una palabra, verdad?\nWhich translates to: You haven’t written not even a word, true?\nIt bothers me that I don’t know why in English the question ends in have you? but in Spanish it ends with true?. Of course this may just be how Spanish works or how conversational Spanish works.\nI asked my partner how you would say just have you? in Spanish and it’s lo has?\nGoogle Translate aligns with this when it translates from Spanish to English:\nSpanish: No has escrito ni una palabra lo has?\nEnglish: You haven’t written a word, have you?\nBut recommends ending with verdad? when I translate from English to Spanish.\n\n\nSpeaker: “You can share mine”\nHere are the Google Translate forward and backward translations:\n\nEnglish: You can share mine.\nSpanish: Puedes compartir el mio.\n\nSpanish: Puedes compartir el mio.\nEnglish: Can you share mine.\n\nHowever, if I start the Spanish translation with tu the English translation matches my original prompt:\n\nSpanish: Tu puedes compartir el mio.\nEnglish: You can share mine.\n\nI think this is a good example of how what is technically correct may or may not be what’s used in conversation—saying tu may not be strictly required for conversation and may be implicitly understood because of the form used—puedes (you can).\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npuedes\nyou can\n\n\npuedo\nI can\n\n\npuedemos\nwe can\n\n\npueden\nthey can\n\n\n\n\n\nLestrade: “Well, they all took the same poison.”\nSomething else I’ve enjoyed and learned from is watching how a translation changes as you type the full sentence in Google Translate.\nFor example when translating from Spanish (pues, todos tomaron el mismo veneno) to English (well, they all took the same poison):\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npues\nwell\n\n\npues, todos\nwell, everyone\n\n\npues, todos tomaron\nwell, they all took\n\n\npues, todos tomaron el\nwell, everyone took\n\n\npues, todos tomaron el mismo\nwell, they all took the same\n\n\npues, todos tomaron el mismo veneno\nwell, they all took the same poison\n\n\n\nWhat I’m observing might have less to do with how Spanish works and more to do with how Google Translate works. Although some words seem interchangeable (todos seems to mean everyone or they all)."
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "title": "Exploring NFL Play-by-Play Analysis",
    "section": "",
    "text": "Side view of Jalen Hurts walking on the Eagles sideline with Kansas City Chiefs-colored confetti falling around him"
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "title": "Exploring NFL Play-by-Play Analysis",
    "section": "Background and Goals",
    "text": "Background and Goals\nIt’s been 5 years since I last explored NFL’s play-by-play data. It’s also been 5 years since my Eagles won the Super Bowl, which will be played in less than 24 hours from now. Go Birds.\nIt’s been so long since I’ve blogged that fastpages, the blogging library I use, has been deprecated.\nI have thoroughly enjoyed some of the statistical analyses put forth by fans of the NFL this year. My favorite analyst is Deniz Selman, a fellow Eagles fan who makes these beautiful data presentations.\nI also appreciate Deniz’ critique of analysis-without-context that often negates the brilliance of Jalen Hurts:\n\n\nAs I’ve been trying to say all year, EPA/dropback is not nearly as valuable a metric when the offense lets the QB decide whether it’s a “dropback” or not during the play by reading the defense, and that QB is the absolute best at making that decision. #FlyEaglesFly\n\n— Deniz Selman (@denizselman33) February 11, 2023\n\n\nMy second favorite analyst is Ben Baldwin, AKA Computer Cowboy especially his 4th down analysis realtime during games.\nThere has been an onslaught of statistical advances in the NFL since I last explored play-by-play data and I’m excited to learn as much as I can. In particular, I’d like to get a hang of the metrics EPA (Expected Points Added) and DVOA (Defense-adjusted Value Over Average), which may not necessarily intersect with my play-by-play analysis (I believe Football Outsiders is the proprietor of that formula).\nI’d also like to use this project to practice more advanced SQL queries than I’m used to. Given the complexity of the play-by-play dataset (by team, down, field position, etc.) I’m hoping I can get those reps in.\nLastly, I’d like to explore data presentation with these statistics using R, python, Adobe Illustrator and Photoshop. I’ve been inspired by simple, elegant graphics like those made by Peter Gorman in Barely Maps and bold, picturesque statistics posted by PFF on twitter:\n\n\nThe most clutch pass rushers face off in the Super Bowl pic.twitter.com/o50lV9Bkgk\n\n— PFF (@PFF) February 12, 2023\n\n\nI’ll work on this project in this post throughout this year–and maybe beyond if it fuels me with enough material–or it’ll fork off into something entirely new or different.\nI’ll start off by next exploring the schema of the play-by-play dataset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "vishal bakshi",
    "section": "",
    "text": "welcome to my blog.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nRegression and Other Stories - Notes and Excerpts\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nWednesday, May 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTranscribing Sherlock into Spanish\n\n\n\n\n\n\n\nspanish\n\n\n\n\nAn update on my goal to transcribe BBC’s Sherlock into Spanish.\n\n\n\n\n\n\nVishal Bakshi\n\n\nTuesday, February 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nExploring NFL Play-by-Play Analysis\n\n\n\n\n\n\n\ndata analysis\n\n\nSQL\n\n\npython\n\n\n\n\nAn update on my analysis and visualization of NFL play-by-play data.\n\n\n\n\n\n\nVishal Bakshi\n\n\nTuesday, February 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMaking Chicken Wings\n\n\n\n\n\n\n\nfood\n\n\n\n\nAn update on my goal to make 10/10 chicken wings by the 2023 NFL season.\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Classification Models\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Bear Classifier\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 7:Test Time Augmentation\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 8: Collaborative Filter Deep Dive\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR Shiny Census App\n\n\n\n\n\n\n\nR\n\n\ndata analysis\n\n\nSQL\n\n\n\n\nAn explanation of my development process for a census data shiny app\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nVisualize U.S. Census Data with ArcGIS\n\n\n\n\n\n\n\nArcGIS\n\n\ndata analysis\n\n\n\n\nA tutorial to create a geodatabase, maps and layouts to visualize U.S. Census Data in ArcGIS Pro\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "my name is vishal bakshi. i’m a data analyst at the City of Portland. i haven’t blogged in a couple of years and recently have had the desire to do so again so here i am."
  },
  {
    "objectID": "about.html#things-i-like-consistently",
    "href": "about.html#things-i-like-consistently",
    "title": "About",
    "section": "things i like consistently",
    "text": "things i like consistently\nhip hop, yoga, lifting free weights, resistance bands, coding, writing, reading, cold weather, philadelphia eagles, la lakers."
  },
  {
    "objectID": "about.html#what-im-currently-reading",
    "href": "about.html#what-im-currently-reading",
    "title": "About",
    "section": "what i’m currently reading",
    "text": "what i’m currently reading\ncheck out my currently reading list on goodreads."
  },
  {
    "objectID": "about.html#what-im-working-on-consistently",
    "href": "about.html#what-im-working-on-consistently",
    "title": "About",
    "section": "what i’m working on consistently",
    "text": "what i’m working on consistently\n\nobserving my thoughts, emotions and behaviors.\nmaking and practicing hip hop music.\nlearning about how and why our society is structured the way it is."
  },
  {
    "objectID": "about.html#what-im-currently-working-on-as-well",
    "href": "about.html#what-im-currently-working-on-as-well",
    "title": "About",
    "section": "what i’m currently working on as well",
    "text": "what i’m currently working on as well\n\nlearning spanish while watching sherlock holmes (the bbc version with cumberbatch).\nexploring NFL play-by-play data.\nlearning how to make good chicken wings (made them the first time on 2/4/23 and they were a 5/10. hoping to get to a 10/10 by the 2023 NFL season)."
  }
]