[
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "",
    "text": "In this notebook, I’ll work through the second “Further Research” exercise at the end of Chapter 4 of the Practical Deep Learning for Coders textbook:\n\nComplete all the steps in this chapter using the full MNIST datasets (for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You’ll need to do some of your own research to figure out how to overcome obstacles you’ll meet on the way."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#plan-of-attack",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#plan-of-attack",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nI’ll start by reviewing each step of the training loop covered in this chapter (for 3s and 7s) and identify what elements need to change, why, and an a brief outline of how, in order to accommodate for all 10 digits.\n\nLoad and Prep Data\nIn the chapter, we stacked tensor images of each digit to create n x 28 x 28 tensors (where n is the number of images in the training or validation folder) and then converted them to n x 784 tensors so that each pixel was in a one-dimensional row (corresponding to 784 parameters in the neural net in a one-dimensional row).\nTo handle all 10 digits, I’ll need to expand this logic without too much hard-coding—I don’t want to create 10 tensors individually (stacked_zeros, stacked_ones, …, stacked_tens) for training and validation data.\nInstead, I’ll use list comprehension. First, let’s look at how to access all the subfolders in the train and valid parent folders:\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\n\npath.ls()\n\n(#2) [Path('/root/.fastai/data/mnist_png/training'),Path('/root/.fastai/data/mnist_png/testing')]\n\n\n\nIndependent Variable: Images\nI can iterate through (path/'training').ls() to see the 10 digit subfolders containing training images for the digits:\n\n[path for path in (path/'training').ls()]\n\n[Path('/root/.fastai/data/mnist_png/training/8'),\n Path('/root/.fastai/data/mnist_png/training/6'),\n Path('/root/.fastai/data/mnist_png/training/3'),\n Path('/root/.fastai/data/mnist_png/training/1'),\n Path('/root/.fastai/data/mnist_png/training/9'),\n Path('/root/.fastai/data/mnist_png/training/2'),\n Path('/root/.fastai/data/mnist_png/training/0'),\n Path('/root/.fastai/data/mnist_png/training/4'),\n Path('/root/.fastai/data/mnist_png/training/7'),\n Path('/root/.fastai/data/mnist_png/training/5')]\n\n\nTaking it one layer deeper, I can nest a second list comprehension which collects individual file paths from each of the digit’s folders:\n\ntraining_files = [[file for file in path.ls()] for path in (path/'training').ls().sorted()]\n\nHere are the paths to the first 5 images in the first folder, which corresponds to the digit 0:\n\ntraining_files[0][:5]\n\n[Path('/root/.fastai/data/mnist_png/training/0/35012.png'),\n Path('/root/.fastai/data/mnist_png/training/0/2009.png'),\n Path('/root/.fastai/data/mnist_png/training/0/14472.png'),\n Path('/root/.fastai/data/mnist_png/training/0/7589.png'),\n Path('/root/.fastai/data/mnist_png/training/0/53401.png')]\n\n\nAnd the paths to the first 5 images in the second folder, which corresponds to the digit 1:\n\ntraining_files[1][:5]\n\n[Path('/root/.fastai/data/mnist_png/training/1/47434.png'),\n Path('/root/.fastai/data/mnist_png/training/1/27790.png'),\n Path('/root/.fastai/data/mnist_png/training/1/42000.png'),\n Path('/root/.fastai/data/mnist_png/training/1/15633.png'),\n Path('/root/.fastai/data/mnist_png/training/1/21958.png')]\n\n\nAnd so on for all 10 digits\n\ntraining_files[9][:5]\n\n[Path('/root/.fastai/data/mnist_png/training/9/57008.png'),\n Path('/root/.fastai/data/mnist_png/training/9/28984.png'),\n Path('/root/.fastai/data/mnist_png/training/9/36162.png'),\n Path('/root/.fastai/data/mnist_png/training/9/42013.png'),\n Path('/root/.fastai/data/mnist_png/training/9/18296.png')]\n\n\n\nlen(training_files)\n\n10\n\n\nI’ll illustrate the same for the validation set:\n\nvalidation_files = [[file for file in path.ls()] for path in (path/'testing').ls().sorted()]\n\n\nvalidation_files[0][:5]\n\n[Path('/root/.fastai/data/mnist_png/testing/0/9095.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/5990.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/7505.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/157.png'),\n Path('/root/.fastai/data/mnist_png/testing/0/5838.png')]\n\n\n\nvalidation_files[9][:5]\n\n[Path('/root/.fastai/data/mnist_png/testing/9/2009.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/7298.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/5565.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/9483.png'),\n Path('/root/.fastai/data/mnist_png/testing/9/5705.png')]\n\n\n\nlen(validation_files)\n\n10\n\n\nNext, I’ll flatten the list of training and validation files, convert each to a stacked tensor, convert the pixel values to floating point values, and divide by 255 so the pixel values are between 0 and 1. I referenced this Stack Overflow post for flattening a nested list.\nTo understand how it works, I find it easier to read left-to-right, broken up into two separate parts:\nfile for (sublist in training_files) for (file in sublist)\nIn pseudocode:\npopulate this list with each file in each sublist in `training_files`\n\ntraining_files = [file for sublist in training_files for file in sublist]\nvalidation_files = [file for sublist in validation_files for file in sublist]\n\nThe dataset’s Wikimedia page says that it has 60,000 training images and 10,000 testing images. This matches the counts here:\n\nlen(training_files), len(validation_files)\n\n(60000, 10000)\n\n\nI’ll open a couple of the files and make sure I can view the images as expected:\n\n# this should be an image of a handwritten zero\nshow_image(tensor(Image.open(training_files[0])));\n\n\n\n\n\n# this should be an image of a handwritten nine\nshow_image(tensor(Image.open(training_files[-1])));\n\n\n\n\n\n# this should be an image of a handwritten zero\nshow_image(tensor(Image.open(validation_files[0])));\n\n\n\n\n\n# this should be an image of a handwritten nine\nshow_image(tensor(Image.open(validation_files[-1])));\n\n\n\n\nLooks good! The images are as expected. I can now move on to creating stacked floating point tensors of the training and validation images:\n\ntrain_x = torch.stack([tensor(Image.open(o)) for o in training_files]).float()/255\ntrain_x.shape\n\ntorch.Size([60000, 28, 28])\n\n\n\nvalid_x = torch.stack([tensor(Image.open(o)) for o in validation_files]).float()/255\nvalid_x.shape\n\ntorch.Size([10000, 28, 28])\n\n\nI’ll view my data one more time before changing its shape:\n\n# this should be a zero\nshow_image(train_x[0]);\n\n\n\n\n\n# this should be a nine\nshow_image(train_x[-1]);\n\n\n\n\n\n# this should be a zero\nshow_image(valid_x[0]);\n\n\n\n\n\n# this should be a nine\nshow_image(valid_x[-1]);\n\n\n\n\nGreat! I’ll flatten the images so that they are 784 pixels long, instead of a 28 x 28 matrix.\n\ntrain_x = train_x.view(-1, 28*28)\ntrain_x.shape\n\ntorch.Size([60000, 784])\n\n\n\nvalid_x = valid_x.view(-1, 28*28)\nvalid_x.shape\n\ntorch.Size([10000, 784])\n\n\n\n\nDependent Variable: Labels\nNow that I have the x (independent) variable data prepared, I’ll do the same for the y (dependent) variable data—the labels for the images.\nI’ll reuse my training_files and validation_files lists as they already contain the paths to each image file, from which I’ll extract the label. path.parts splits the path into a tuple of its individual parts (split by “/”). The parent folder (the second-to-last part of the path) of the path is the label of the image.\n\ntraining_labels = [int(path.parts[-2]) for path in training_files]\ntraining_labels[0], training_labels[-1]\n\n(0, 9)\n\n\n\nvalidation_labels = [int(path.parts[-2]) for path in validation_files]\nvalidation_labels[0], validation_labels[-1]\n\n(0, 9)\n\n\n\ntrain_y = tensor(training_labels).unsqueeze(1)\ntrain_y.shape\n\ntorch.Size([60000, 1])\n\n\n\nvalid_y = tensor(validation_labels).unsqueeze(1)\nvalid_y.shape\n\ntorch.Size([10000, 1])\n\n\nExcellent! Now with the data in the right structure, I’ll create a DataLoaders object that will be fed to the learner during training:\n\ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([0]))\n\n\n\nvalid_dset = list(zip(valid_x, valid_y))\nx,y = valid_dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([0]))\n\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\nvalid_xb, valid_yb = first(valid_dl)\nvalid_xb.shape, valid_yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\nI combine the two DataLoaders into a single DataLoaders object:\n\ndls = DataLoaders(dl, valid_dl)\n\nAnd check that they contain the right amount of data:\n\nlen(dls.train.dataset), len(dls.valid.dataset)\n\n(60000, 10000)\n\n\nGreat! With my DataLoaders prepared, I can move on to other aspects of the training loop that will need to be modified to handle 10 digits instead of 2.\n\n\n\nCreate Our Model\nHere is the existing model that we’re using to classify two digits:\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\nIt has 784 inputs and 1 output. For 10 digits, I need to adjust th number of outputs to 10:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\nI assume that since we have more final activations, we would also need to increase the intermediate activations from 30 to a larger number, but I’ll keep it at 30 for now and then make improvements once I’ve actually got a successful training loop.\n\n\nCreate a Loss Function\nThis is the main change that will take place in our training loop: using a loss function that can handle 10 digits instead of 2. In the exercise prompt, they said that we would need to:\n\ndo some of your own research to figure out how to overcome obstacles you’ll meet on the way\n\nAnd I think this is probably the main obstacle to overcome. In the textbook chapter, when they trained the dataset using the built-in Learner, they passed it F.cross_entropy as the loss function:\nlearn = vision_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)\n\nIn the text, they introduce Cross-Entropy Loss in Chapter 5, so I’ll take a detour through that chapter’s relevant sections to inform me on how to create a loss function for a 10-digit classifier.\n\nCross-Entropy Loss\n\nWorks even when our dependent variable has more than two categories.\nResults in faster and more reliable training.\n\nAs is done in the book example, I’ll view one batch of our data:\n\nx,y = dls.one_batch()\ny[:10]\n\ntensor([[0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0]])\n\n\nCurrently, my data is not shuffled, so all the 0s are first, then all the 1s, 2s, …, and 9s. This doesn’t seem like a great way to train the model since it will learn only 1 digit’s features in each batch. I’ll recreate the DataLoaders and pass the parameter value shuffle=True:\n\ndl = DataLoader(dset, batch_size=256, shuffle=True)\nxb,yb = first(dl)\nyb[:10]\n\ntensor([[5],\n        [4],\n        [1],\n        [3],\n        [0],\n        [3],\n        [7],\n        [2],\n        [3],\n        [0]])\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\nvalid_xb, valid_yb = first(valid_dl)\nvalid_yb[:10]\n\ntensor([[4],\n        [7],\n        [9],\n        [6],\n        [1],\n        [8],\n        [9],\n        [5],\n        [3],\n        [0]])\n\n\n\ndls = DataLoaders(dl, valid_dl)\n\n\nlen(dls.train.dataset), len(dls.valid.dataset)\n\n(60000, 10000)\n\n\nNow, when I look at one batch, I can see a variety of labels:\n\nx,y = dls.one_batch()\ny[:10]\n\ntensor([[8],\n        [3],\n        [2],\n        [7],\n        [3],\n        [9],\n        [1],\n        [6],\n        [0],\n        [7]])\n\n\nThe output predictions of the model will contain 10 predictions (one for each digit) that add up to 1. According to Chapter 5, we need to use the softmax function to achieve a result like this.\n\nSoftmax\nAssume we have a scenario with 6 images and 2 possible categories (3 and 7):\n\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\nWe can’t just take the sigmoid of this directly since we want rows to add up to 1.0 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1).\n\nacts.sigmoid()\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\nIn the binary case, a single pair of activations simply indicates the relative confidence of the input being a 3 versus being a 7. The overall values, whether they are both high or both low, don’t matter—all that matters is which is higher and by how much.\nWe can take the difference between the neural net activations because that reflects how much more sure we are of the input being a 3 than a 7, and then take the sigmoid of that:\n\n(acts[:,0] - acts[:,1]).sigmoid()\n\ntensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661])\n\n\nThe second column (the probability of it being a 7) will then just be that value subtracted from 1. The function softmax does this for any number of columns:\n\ndef softmax(x): return torch.exp(x) / torch.exp(x).sum(dim=1, keepdim=True)\n\n\nsm_acts = softmax(acts)\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nsoftmax is the multi-category equivalent of sigmoid. We have to use it any time we have more than two categories and the probabilities of the categories must add up to 1. Taking the exponential ensures all of our numbers are positive, and then dividing by the sum ensures that we are going to have a bunch of numbers that add up to 1.\nsoftmax is the first part of the cross-entropy loss, the second part is log likelihood.\n\n\nLog Likelihood\nWhen we calculated the loss for our MNIST example, we used:\ndef mnist_loss(inputs, targets):\n  inputs = inputs.sigmoid()\n  return torch.where(targets==1, 1-inputs, inputs).mean()\nWe need to extend the loss function to work with more than just binary classification.\nOur activations after softmax are between 0 and 1, and sum to 1 for each row in the batch of predictions, our targets are integers between 0 and 9.\nLet’s say these are our labels:\n\ntarg = tensor([0,1,0,1,1,0])\n\nAnd these are the softmax activations:\n\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nThen for each item of targ, we can use that to select the appropriate column of sm_acts using tensor indexing like this:\n\nidx = range(6)\nsm_acts[idx, targ]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\nAs long as the activation columns sum to 1 (as they will if we use softmax), we’ll have a loss function that shows how well we’re predicting each digit.\nMaking the activation for the correct label as high as possible must mean we’re also decreasing the activations of the remaining columns.\nPyTorch provides nll_loss which does the same thing as sm_acts[range(n), targ] except it takes the negative because when applying log afterwards we want negative numbers:\n\n-sm_acts[idx, targ]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targ, reduction='none')\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\n\nTaking the log\nHere’s a plot of the log function\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\n\nplot_function(torch.log, min=0, max=4)\n\n\n\n\nWe want to transform our probabilities to larger values so we can perform mathematical operations on them. We also want our model to learn the difference between 0.99 and 0.999 (the latter is 10 times more confident). We can use the logarithm function to transform our numbers between 0 and 1 to instead be between negative infinity and positive infinity.\nApplying negative log to the softmax output:\n\ntarg\n\ntensor([0, 1, 0, 1, 1, 0])\n\n\n\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\n\nsm_acts[idx, targ]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\n\n-torch.log(sm_acts[idx, targ])\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\nThe loss is larger for incorrect predictions (2.0160 for the third image with activations of 0.1332 for 3—the target value—and 0.8668 for 7) and for unconfident correct predictions (5.6958 for the fourth image with activations of 0.9966 for 3 and 0.0034 for 7—the target value).\nPyTorch provides the nn.CrossEntropyLoss class and the F.cross_entropy function:\n\nF.cross_entropy(acts, targ, reduction='none')\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targ)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\n\n\n\n\nCreate a Function to Calculate Predictions, Loss and Gradients\nIn this section, the text has the following function to calculate predictions, loss and gradients:\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  loss = mnist_loss(preds, yb)\n  loss.backward()\nI don’t think anything needs to be changed here. In my implementation of a BasicLearner I have generalized that function as:\ndef calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    loss.backward()\nI believe that should work fine with nn.CrossEntropyLoss() as the function assigned to my self.loss_func parameter.\n\n\nCreate an Optimizer\nThe optimizer’s functionality (stepping the parameters and setting the gradients to zero) will not need to be changed to handle 10 digits.\n\n\nCreate a Function to Train One Epoch\nThe steps needed to train an epoch will not need to be changed to handle 10 digits:\ndef train_epoch(model):\n  for xb,yb in dl:\n    calc_grad(xb, yb, model)\n    opt.step()\n    opt.zero_grad()\n\n\nCreate a Function to Calculate a Metric for One Batch\nAlong with the loss function, this is the second big change when dealing with 10 digits instead of 2. Currently, in text, we use a batch_accuracy function as the metric during training:\ndef batch_accuracy(xb, yb):\n  preds = xb.sigmoid()\n  correct = (preds>0.5) == yb\n  return correct.float().mean()\nFor each batch the following steps take place in calculating accuracy:\n\nCalculate the sigmoid value of the predictions.\nDetermine which predictions are greater than 0.5 and if they are, whether they are correctly labeled as 3s.\nReturn the mean value of the previous tensor, which will calculate as number of correct predictions / number of total predictions.\n\nTo understand how to calculate accuracy for a dataset with 10 digits, I’ll create an example calculation similar to what they did in chapter 5 for illustrating the softmax/log likelihood example. Suppose we have 6 images with 10 possible categories (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) and 6 labels:\n\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,10))*2\nacts\n\ntensor([[ 3.8538,  2.9746,  1.8014, -4.2110,  1.3568, -2.4691, -0.0861, -3.2093,\n         -1.5043,  3.2974],\n        [-0.7850, -2.8072, -1.4558, -1.1189, -1.5377,  1.5249,  3.2846, -0.3192,\n         -0.9948,  0.8792],\n        [-1.5163,  2.1566,  1.6016,  3.3612,  2.5582,  2.5928,  1.2209,  2.6695,\n         -0.4632,  0.0835],\n        [-0.5032,  1.7197, -2.7693, -1.7425, -0.4467,  3.4347,  0.6378, -0.8490,\n          0.6114, -1.5492],\n        [-3.1151,  1.9913, -1.7596, -1.2023,  0.7345,  0.3508,  2.7703, -0.8917,\n          2.8903,  1.7128],\n        [ 4.4362,  1.0463,  2.3507,  1.1223, -0.9055, -1.5436, -0.3444,  1.0476,\n          0.1132,  0.8526]])\n\n\n\ntarg = tensor([1,6,4,5,8,0])\n\nWe take the softmax of the activations so that each row sums to 1:\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\ntensor([[4.4919e-01, 1.8645e-01, 5.7688e-02, 1.4122e-04, 3.6982e-02, 8.0615e-04,\n         8.7362e-03, 3.8453e-04, 2.1156e-03, 2.5751e-01],\n        [1.2639e-02, 1.6728e-03, 6.4621e-03, 9.0509e-03, 5.9539e-03, 1.2731e-01,\n         7.3978e-01, 2.0136e-02, 1.0246e-02, 6.6747e-02],\n        [2.4815e-03, 9.7686e-02, 5.6077e-02, 3.2583e-01, 1.4597e-01, 1.5110e-01,\n         3.8323e-02, 1.6314e-01, 7.1126e-03, 1.2288e-02],\n        [1.4239e-02, 1.3148e-01, 1.4766e-03, 4.1232e-03, 1.5065e-02, 7.3058e-01,\n         4.4562e-02, 1.0075e-02, 4.3404e-02, 5.0024e-03],\n        [8.6558e-04, 1.4289e-01, 3.3576e-03, 5.8621e-03, 4.0662e-02, 2.7705e-02,\n         3.1141e-01, 7.9971e-03, 3.5109e-01, 1.0816e-01],\n        [7.7830e-01, 2.6240e-02, 9.6708e-02, 2.8313e-02, 3.7265e-03, 1.9688e-03,\n         6.5311e-03, 2.6273e-02, 1.0321e-02, 2.1619e-02]])\n\n\n\nsm_acts.sum(dim=1)\n\ntensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n\n\nThe model’s prediction is the digit (index) with the largest probability.\n\nsm_acts.max(dim=1).indices\n\ntensor([0, 6, 3, 5, 8, 0])\n\n\nThe accuracy is the average of the following boolean tensor (the number of correct predictions / the number of total predictions):\n\nsm_acts.max(dim=1).indices == targ\n\ntensor([False,  True, False,  True,  True,  True])\n\n\n\n(sm_acts.max(dim=1).indices == targ).float().mean().item()\n\n0.6666666865348816\n\n\nAs a function, this is what batch_accuracy would look like for the 10-digit classifier:\ndef batch_accuracy(xb, yb):\n  preds = torch.softmax(xb, dim=1)\n  preds = preds.max(dim=1).indices\n  return (preds == yb).float().mean().item()\n\n\nCreate a Function to Calculate the Metric for One Epoch\nThis function will be the same for the 10-digit classifier as it was for the 2-digit classifier:\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n  return round(torch.stack(accs).mean().item(), 4)\n\n\nCreate a Function for the Training Loop\nThis function will be the same for the 10-digit classifier as it was for the 2-digit classifier:\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')"
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnist-training-loop",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnist-training-loop",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "MNIST Training Loop",
    "text": "MNIST Training Loop\nWith that overview under my belt, I’ll walk through the training loop with real code and data.\n\nLoad and Prep Data\nI’ve already loaded and prepared a shuffled DataLoaders object, so in this step all I need to do is check it’s size and shape:\n\nlen(dls.train.dataset), len(dls.valid.dataset)\n\n(60000, 10000)\n\n\n\nx,y = first(dls.train)\nx.shape, y.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nx,y = first(dls.valid)\nx.shape, y.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\n# check that the digits are shuffled\ny[:5]\n\ntensor([[6],\n        [8],\n        [2],\n        [0],\n        [6]])\n\n\n\n\nCreate Our Model\nFor the first iteration of the training loop, I’ll use a similarly structured model as before:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n\nCreate a Loss Function\nThe loss function that I will use is nn.CrossEntropyLoss:\n\nmnist_loss = nn.CrossEntropyLoss()\n\n\n\nCreate a function to Calculate Predictions, Loss and Gradients\nThis function is the same as before:\n\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  # yb has to be a 0- or 1-D tensor\n  loss = mnist_loss(preds, yb.squeeze())\n  loss.backward()\n\n\n\nCreate an Optimizer\nI’ll use the BasicOptim optimizer defined in chapter 4:\n\nclass BasicOptim:\n  def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n  def step(self, *args, **kwargs):\n    for p in self.params: p.data -= p.grad.data * self.lr\n\n  def zero_grad(self, *args, **kwargs):\n    for p in self.params: p.grad = None\n\n\nlr = 0.1\n\n\nopt = BasicOptim(simple_net.parameters(), lr)\n\n\n\nCreate a Function to Train One Epoch\nSame as before:\n\ndef train_epoch(model):\n  for xb,yb in dls.train:\n    calc_grad(xb, yb, model)\n    opt.step()\n    opt.zero_grad()\n\n\n\nCreate a Function to Calculate a Metric for One Batch\nThe metric of interest is accuracy, and I’ll define a new batch_accuracy function to handle 10-digits instead of 2:\n\ndef batch_accuracy(xb, yb):\n  preds = torch.softmax(xb, dim=1)\n  preds = preds.max(dim=1).indices\n  # squeeze yb so it's the same shape as preds\n  return (preds == yb.squeeze()).float().mean().item()\n\n\n\nCreate a Function to Calculate the Metric for One Epoch\nSame as before, but iterating through dls.valid instead of a valid_dl DataLoader:\n\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in dls.valid]\n  return round(torch.tensor(accs).mean().item(), 4)\n\n\n\nTrain the Model for One Epoch\nSince I have a new loss function and accuracy function, I’ll train the model for one batch manually and then use a loop for multiple epochs.\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n# create a fresh optimizer\nopt = BasicOptim(simple_net.parameters(), lr=0.01)\n\n\n# get one batch of the training data\nxb, yb = first(dls.train)\n\n\n# calculate predictions\npreds = simple_net(xb)\n\n\npreds.shape\n\ntorch.Size([256, 10])\n\n\nnn.CrossEntropyLoss() expects a 0- or 1-dimensional tensor for the labels (targets) so I have to squeeze the label tensor.\n\n# calculate loss\nloss = mnist_loss(preds, yb.squeeze())\n\n\nloss\n\ntensor(2.3140, grad_fn=<NllLossBackward0>)\n\n\n\n# calculate gradients\nloss.backward()\n\n\n# step the weights\nopt.step()\nopt.zero_grad()\n\n\n# calculate accuracy using validation set\nvalidate_epoch(simple_net)\n\n0.1239\n\n\n\n\nCreate a Function for the Training Loop\nNow that I’ve tested my loss and accuracy function for one epoch, I can loop through the training process for multiple epochs:\n\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')\n\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n# create a fresh optimizer\nopt = BasicOptim(simple_net.parameters(), lr=0.001)\n\n\ntrain_model(simple_net, 40)\n\n0.2094 0.2522 0.3821 0.5208 0.6083 0.6399 0.6674 0.6776 0.6828 0.6894 0.701 0.7071 0.7221 0.7418 0.7549 0.7735 0.7852 0.7896 0.8061 0.8089 0.8212 0.8188 0.8264 0.8309 0.8313 0.8336 0.8378 0.8397 0.8462 0.8487 0.8511 0.8546 0.8545 0.8545 0.8597 0.8604 0.8618 0.8609 0.8612 0.8662 \n\n\nAfter training the model for 40 epochs with a learning rate of 0.001, I achieve an accuracy of about 86%.\nI’ll now test my model and see if it predicts digits correctly:\n\ntest_x = dls.valid.dataset[1][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\npreds = simple_net(test_x)\npreds = torch.softmax(preds, dim=0)\n\n# this should be 0\npreds.argmax().item()\n\n0\n\n\n\ntest_x = dls.valid.dataset[-1][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\npreds = simple_net(test_x)\npreds = torch.softmax(preds, dim=0)\n\n# this should be 9\npreds.argmax().item()\n\n9\n\n\n\ntest_x = dls.valid.dataset[5000][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\npreds = simple_net(test_x)\npreds = torch.softmax(preds, dim=0)\n# this should be 4\npreds.argmax().item()\n\n4\n\n\nLook good! The model is correctly predicting images.\nNext, I’ll modify my BasicLearner class (and call it MNISTLearner) so that it can handle training the full MNIST dataset."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnistlearner-class",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#mnistlearner-class",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "MNISTLearner Class",
    "text": "MNISTLearner Class\nI’ll build off my BasicLearner class by incorporating some changes (mainly, changing yb to yb.squeeze() when calculating loss).\nI’ll also add a Time column which displays how much time it took to train each epoch.\n\nimport time\n\n\nclass MNISTLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    # calculates loss and gradients\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb.squeeze())\n    # store the loss of each batch\n    # later to be averaged across the epoch\n    self.loss = torch.cat((self.loss, tensor([loss])))\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      # steps the weights\n      self.opt.step()\n      # resets gradient to zero\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    # calculates mean accuracy across validation set\n    return round(torch.tensor(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(f\"{'Epoch':<8}{'Train Loss':<14}{self.metric.__name__:<16}{'Time (s)'}\")\n    for i in range(self.epochs):\n      start_time = time.time()\n      self.loss = tensor([])\n      self.train_epoch()\n      end_time = round(time.time() - start_time,4)\n      mean_loss = round(self.loss.mean().item(), 4)\n      mean_metric = self.validate_epoch()\n      print(f\"{i:<8}{mean_loss:<14}{mean_metric:<16}{end_time}\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    # instantiate optimizer\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    # run training loop\n    self.train_model(self.model, self.epochs)\n\n  def predict(self, x):\n    prediction = self.model(x)\n    # predictions should add up to 1.\n    prediction = torch.softmax(prediction, dim=0)\n    # return probability and label\n    return prediction.max(), prediction.argmax().item()\n\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 10)\n)\n\n\n# instantiate MNISTLearner\nlearner = MNISTLearner(dls=dls,\n                       model=simple_net,\n                       opt_func=BasicOptim,\n                       loss_func=nn.CrossEntropyLoss(),\n                       metric=batch_accuracy)\n\n\n# run training loop for 40 epochs\nlearner.fit(epochs=40, lr=0.001)\n\nEpoch   Train Loss    batch_accuracy  Time (s)\n0       2.3025        0.1194          0.7052\n1       2.2658        0.1836          0.6548\n2       2.2273        0.2968          0.6137\n3       2.1825        0.3777          0.5981\n4       2.1311        0.4608          0.6194\n5       2.0769        0.5362          0.6074\n6       2.0208        0.5938          0.7253\n7       1.9622        0.6406          0.7677\n8       1.9025        0.6746          0.6411\n9       1.8408        0.6964          0.6073\n10      1.7774        0.7209          0.6361\n11      1.7121        0.7318          0.5707\n12      1.6454        0.7428          0.886\n13      1.5776        0.7583          1.0406\n14      1.5094        0.7699          0.5713\n15      1.4416        0.7759          0.5628\n16      1.3759        0.7796          0.5508\n17      1.3118        0.7882          0.5552\n18      1.2518        0.7876          0.6065\n19      1.1946        0.7981          0.5996\n20      1.1411        0.801           0.5771\n21      1.0918        0.8123          0.5345\n22      1.0467        0.8177          0.5996\n23      1.0042        0.8211          0.7218\n24      0.966         0.8229          0.7094\n25      0.9305        0.8261          0.5933\n26      0.8984        0.8311          0.5768\n27      0.8686        0.8356          0.5928\n28      0.8416        0.8342          0.5969\n29      0.8167        0.8418          0.5945\n30      0.7935        0.8438          0.5933\n31      0.7716        0.8449          0.5819\n32      0.7519        0.8489          0.5428\n33      0.7337        0.8498          0.6047\n34      0.7166        0.8525          0.6062\n35      0.7011        0.8511          0.5831\n36      0.6862        0.8552          0.5418\n37      0.6725        0.86            0.5483\n38      0.6592        0.8539          0.5721\n39      0.6471        0.856           0.5682\n\n\nGreat! Next, I’ll test predictions of this model, one image per digit, to see how it performs. Ideally, I would have a test dataset set aside for this part, but I’ll use the validation set instead:\n\ntest_x = dls.valid.dataset[0][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 0\nlearner.predict(test_x)\n\n(tensor(0.9909, grad_fn=<MaxBackward1>), 0)\n\n\n\ntest_x = dls.valid.dataset[1000][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 1\nlearner.predict(test_x)\n\n(tensor(0.7081, grad_fn=<MaxBackward1>), 1)\n\n\n\ntest_x = dls.valid.dataset[2600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 2\nlearner.predict(test_x)\n\n(tensor(0.5737, grad_fn=<MaxBackward1>), 4)\n\n\n\ntest_x = dls.valid.dataset[3600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 3\nlearner.predict(test_x)\n\n(tensor(0.7091, grad_fn=<MaxBackward1>), 3)\n\n\n\ntest_x = dls.valid.dataset[4600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 4\nlearner.predict(test_x)\n\n(tensor(0.8390, grad_fn=<MaxBackward1>), 4)\n\n\n\ntest_x = dls.valid.dataset[5600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 5\nlearner.predict(test_x)\n\n(tensor(0.4645, grad_fn=<MaxBackward1>), 5)\n\n\n\ntest_x = dls.valid.dataset[6600][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 6\nlearner.predict(test_x)\n\n(tensor(0.9363, grad_fn=<MaxBackward1>), 6)\n\n\n\ntest_x = dls.valid.dataset[7100][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 7\nlearner.predict(test_x)\n\n(tensor(0.4163, grad_fn=<MaxBackward1>), 1)\n\n\n\ntest_x = dls.valid.dataset[8500][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 8\nlearner.predict(test_x)\n\n(tensor(0.8225, grad_fn=<MaxBackward1>), 8)\n\n\n\ntest_x = dls.valid.dataset[-1][0]\nshow_image(test_x.view(-1,28,28));\n\n\n\n\n\n# this should be 9\nlearner.predict(test_x)\n\n(tensor(0.5962, grad_fn=<MaxBackward1>), 9)\n\n\nManually testing the model, it accurately predicted 80% of the digits, which makes sense given the model accuracy was 85% at the end of training."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#improving-the-model",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#improving-the-model",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "Improving the Model",
    "text": "Improving the Model\nI wonder if adding another layer to my neural net will improve the model’s accuracy. I’ll arbitrarily choose an intermediate number of activations as 784/2:\n\n# create a fresh model\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 392),\n    nn.ReLU(),\n    nn.Linear(392, 30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\n\n# instantiate MNISTLearner\nlearner = MNISTLearner(dls=dls,\n                       model=simple_net,\n                       opt_func=BasicOptim,\n                       loss_func=nn.CrossEntropyLoss(),\n                       metric=batch_accuracy)\n\n\n# run training loop for 40 epochs\nlearner.fit(epochs=40, lr=0.005)\n\nEpoch   Train Loss    batch_accuracy  Time (s)\n0       2.2885        0.2899          1.771\n1       2.2213        0.3874          2.2249\n2       2.1157        0.5674          1.6725\n3       1.9426        0.6493          1.7144\n4       1.6701        0.708           1.5923\n5       1.3271        0.7588          1.6464\n6       1.0312        0.7899          1.7139\n7       0.8426        0.8186          1.9155\n8       0.726         0.8276          1.8586\n9       0.6467        0.8443          1.6758\n10      0.5893        0.8518          1.6447\n11      0.5452        0.8624          1.6898\n12      0.5098        0.8768          1.653\n13      0.482         0.8792          1.7703\n14      0.4598        0.8817          2.0231\n15      0.4412        0.8829          1.6538\n16      0.4255        0.8902          1.6392\n17      0.4125        0.8919          1.6541\n18      0.4017        0.8894          1.6532\n19      0.3917        0.8951          1.6696\n20      0.3834        0.8965          2.1116\n21      0.3754        0.8963          1.6413\n22      0.3687        0.8975          1.6831\n23      0.3623        0.8994          1.6442\n24      0.3567        0.9065          1.7872\n25      0.3515        0.9039          1.7696\n26      0.3464        0.9052          2.1025\n27      0.3415        0.9075          1.7063\n28      0.3372        0.9098          1.7005\n29      0.3333        0.9116          1.6846\n30      0.3293        0.9107          1.6581\n31      0.3258        0.9117          1.7834\n32      0.3214        0.9113          1.9949\n33      0.3181        0.9121          1.7621\n34      0.3146        0.9122          1.7287\n35      0.3113        0.9147          1.7505\n36      0.3078        0.9134          1.7595\n37      0.3052        0.9176          1.7835\n38      0.3017        0.9174          1.966\n39      0.2993        0.9183          1.9536\n\n\nCool! I ended up with a significant improvement in accuracy, although the training took about 3 times as long to finish.\nI’ll again test all 10 digits to see how well it predicts them:\n\nactuals = list(range(10))\nfor idx, val in enumerate([0, 1000, 2600, 3600, 4600, 5600, 6600, 7100, 8500, -1]):\n  test_x = dls.valid.dataset[val][0]\n  print(f\"{'Actual:':<8}{actuals[idx]:<4} {'Prediction: ':<12}{learner.predict(test_x)}\")\n\nActual: 0    Prediction: (tensor(0.9990, grad_fn=<MaxBackward1>), 0)\nActual: 1    Prediction: (tensor(0.9758, grad_fn=<MaxBackward1>), 1)\nActual: 2    Prediction: (tensor(0.8626, grad_fn=<MaxBackward1>), 4)\nActual: 3    Prediction: (tensor(0.9606, grad_fn=<MaxBackward1>), 3)\nActual: 4    Prediction: (tensor(0.9981, grad_fn=<MaxBackward1>), 4)\nActual: 5    Prediction: (tensor(0.9614, grad_fn=<MaxBackward1>), 5)\nActual: 6    Prediction: (tensor(0.9982, grad_fn=<MaxBackward1>), 6)\nActual: 7    Prediction: (tensor(0.8736, grad_fn=<MaxBackward1>), 7)\nActual: 8    Prediction: (tensor(0.9901, grad_fn=<MaxBackward1>), 8)\nActual: 9    Prediction: (tensor(0.9784, grad_fn=<MaxBackward1>), 9)\n\n\nMy manual testing resulted in the model predicting 90% of the digits correctly, which makes sense given the increase in accuracy of the model."
  },
  {
    "objectID": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#further-improvements",
    "href": "posts/2023-07-24-full-mnist-classifier/2023-07-24-full-mnist-classifier.html#further-improvements",
    "title": "Implementing an MNIST Classifier From Scratch",
    "section": "Further Improvements",
    "text": "Further Improvements\nThere are of course unlimited improvements when it comes to trying out different models. I could vary the number of intermediate activations, number of layers, and overall architecture.\nI could also add a validation loss to understand when the model is overfitting.\nOverall, I am thrilled that this exercise was successful, and had a really fun time working through it. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "",
    "text": "Here’s the video walkthrough of this notebook\nIn this notebook, I’ll walkthrough the code and concepts introduced in Chapter 8 of the fastai textbook. This chapter explores the various ways fastai can handle a collaborative filtering problem."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#what-is-collaborative-filtering",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "What is Collaborative Filtering?",
    "text": "What is Collaborative Filtering?\nIn a situation where two variables have some numeric relationship, such as users rating movies, collaborative filtering is a solution for predicting ratings that are blank, based on existing data.\nA machine learning model for collaborative filtering implicitly learns the answers to the following questions:\n\nWhat types of movies do users like?\nWhat are characteristics of each movie?\n\n\nLatent Factors\nFor the movie rating example, latent factors are the “types of movies” users like and “characteristics” of each movie. Latent factors are not explicitly categorical, they are numeric values, but they represent the implicit categories of each variable.\nThe reason that they are implicit categories, is that the model learns the ideal latent factors as it trains on the dataset, observing patterns between users and their movie ratings."
  },
  {
    "objectID": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "href": "posts/2021-06-04-fastai-chapter-08/2021-06-04-fastai-chapter-08.html#movielens-100k-dataset",
    "title": "fast.ai Chapter 8: Collaborative Filter Deep Dive",
    "section": "MovieLens 100K Dataset",
    "text": "MovieLens 100K Dataset\nThe dataset used to train the collaborative filtering model is a subset (100,000 rows in length) of the full MovieLens dataset which is 25 million rows.\n\n# a first look at the data\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\n\n\nThe dataset lists users, movies, ratings and a timestamp.\n\n# load the data into a DataFrame\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nBefore we get into the training, I want to familiarize myself with how the data is structured. There are 943 unique users and 1682 unique movies.\n\n# how many unique users and movies are there?\nlen(ratings['user'].unique()), len(ratings['movie'].unique())\n\n(943, 1682)\n\n\nThe movie IDs are a consecutive range from 1 to 1682 and the user IDs are a consecutive range from 1 to 943. The movie ratings range from 1 to 5.\n\n# are movie IDs consecutive?\n(ratings['movie'].sort_values().unique() == np.array(range(1,1683))).sum()\n\n1682\n\n\n\n# are user IDs consecutive?\n(ratings['user'].sort_values().unique() == np.array(range(1,944))).sum()\n\n943\n\n\n\n# what is the range of ratings?\nratings['rating'].min(), ratings['rating'].max()\n\n(1, 5)\n\n\nTo visualize the problem we are trying to solve with collaborative filtering the book recommended that we observe a cross-tabulation of the data because then we can see that what we are trying to predict are the null values between user and movie, and what we are training our model on are the non-null ratings in the dataset.\nThe model will learn something about user 2 and movie 2 in order to predict what rating that user would give that movie. That “something” the model will learn are the latent factors for users and latent factors for movies.\n\n# view crosstab of users and movies with rating values\nct = pd.crosstab(ratings['user'], ratings['movie'], ratings['rating'],aggfunc='mean')\nct\n\n\n\n\n\n  \n    \n      movie\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      40\n      ...\n      1643\n      1644\n      1645\n      1646\n      1647\n      1648\n      1649\n      1650\n      1651\n      1652\n      1653\n      1654\n      1655\n      1656\n      1657\n      1658\n      1659\n      1660\n      1661\n      1662\n      1663\n      1664\n      1665\n      1666\n      1667\n      1668\n      1669\n      1670\n      1671\n      1672\n      1673\n      1674\n      1675\n      1676\n      1677\n      1678\n      1679\n      1680\n      1681\n      1682\n    \n    \n      user\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      5.0\n      3.0\n      4.0\n      3.0\n      3.0\n      5.0\n      4.0\n      1.0\n      5.0\n      3.0\n      2.0\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n      3.0\n      4.0\n      5.0\n      4.0\n      1.0\n      4.0\n      4.0\n      3.0\n      4.0\n      3.0\n      2.0\n      4.0\n      1.0\n      3.0\n      3.0\n      5.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      3.0\n      4.0\n      3.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      939\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      940\n      NaN\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      4.0\n      5.0\n      3.0\n      NaN\n      NaN\n      4.0\n      NaN\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      941\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      942\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      943\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      4.0\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      4.0\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n943 rows × 1682 columns\n\n\n\nInstead of movie IDs, we can get the movie titles into a DataFrame and add that column to our ratings DataFrame by merging the two.\nThe movie titles are in the u.item file, which is pipe-delimited, with latin-1 encoding. The u.item file has 24 columns, but we only want the first two which have the movie id and the title.\n\n# get movie titles\nmovies = pd.read_csv(\n    path/'u.item', \n    delimiter='|', \n    encoding='latin-1', \n    usecols=(0,1), \n    names=('movie', 'title'), \n    header=None)\n\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nThe movies DataFrame and the ratings DataFrame are merged using the movie column as the key to match title in movies to movie ID in ratings. By default, pandas uses as the key whichever column name exists in both DataFrames.\n\n# get the user ratings by title\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n\n\nDoes this change the uniqueness of the data? Yes it actually does! There are 1682 unique movie IDs but there are only 1664 unique movie titles. 18 movies have associated with it duplicate titles.\n\n# how many unique titles and movies are there?\nlen(ratings['title'].unique()), len(ratings['movie'].unique())\n\n(1664, 1682)\n\n\nThe .duplicated DataFrame method takes a list of columns for the subset parameter, finds values in those columns that are duplicated, and returns a boolean Series with a True value at indexes with duplicates. I use that as a mask and pass it to the movies DataFrame to view those duplicate titles.\n\n# 18 movies have duplicate titles\nmovies[movies.duplicated(subset=['title'])]\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      267\n      268\n      Chasing Amy (1997)\n    \n    \n      302\n      303\n      Ulee's Gold (1997)\n    \n    \n      347\n      348\n      Desperate Measures (1998)\n    \n    \n      499\n      500\n      Fly Away Home (1996)\n    \n    \n      669\n      670\n      Body Snatchers (1993)\n    \n    \n      679\n      680\n      Kull the Conqueror (1997)\n    \n    \n      864\n      865\n      Ice Storm, The (1997)\n    \n    \n      880\n      881\n      Money Talks (1997)\n    \n    \n      1002\n      1003\n      That Darn Cat! (1997)\n    \n    \n      1256\n      1257\n      Designated Mourner, The (1997)\n    \n    \n      1605\n      1606\n      Deceiver (1997)\n    \n    \n      1606\n      1607\n      Hurricane Streets (1998)\n    \n    \n      1616\n      1617\n      Hugo Pool (1997)\n    \n    \n      1624\n      1625\n      Nightwatch (1997)\n    \n    \n      1649\n      1650\n      Butcher Boy, The (1998)\n    \n    \n      1653\n      1654\n      Chairman of the Board (1998)\n    \n    \n      1657\n      1658\n      Substance of Fire, The (1996)\n    \n    \n      1679\n      1680\n      Sliding Doors (1998)\n    \n  \n\n\n\n\nfastai has a built-in constructor for DataLoaders specific to collaborative filtering. I pass it the ratings DataFrame, specify that the items are the titles, and that I want 64 rows in each batch.\n\n# create DataLoaders\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      297\n      Indian Summer (1996)\n      4\n    \n    \n      1\n      934\n      Grease (1978)\n      4\n    \n    \n      2\n      846\n      Money Train (1995)\n      2\n    \n    \n      3\n      479\n      Crumb (1994)\n      3\n    \n    \n      4\n      499\n      Local Hero (1983)\n      4\n    \n    \n      5\n      455\n      Adventures of Priscilla, Queen of the Desert, The (1994)\n      3\n    \n    \n      6\n      943\n      Rumble in the Bronx (1995)\n      4\n    \n    \n      7\n      374\n      Dead Poets Society (1989)\n      1\n    \n    \n      8\n      533\n      Deer Hunter, The (1978)\n      3\n    \n    \n      9\n      846\n      Vanya on 42nd Street (1994)\n      2\n    \n  \n\n\n\n\nMatrices for Latent Factors\nWe need the model to find relationships between users and movies. And we need to give the model something concrete and numeric to represent those relationships. We will give it latent factor matrices.\nIn this example, they have chosen to use 5 latent factors for movies and 5 latent factors for users. We represent these latent factors by creating a matrix of random values.\nThe user latent factors will have 944 rows, one for each user including a null user, and 5 columns, one for each latent factor. The movies latent factors will have 1665, one for each movie including a null movie, and 5 columns.\n\n# user and title classes contain '#na#'\nL(dls.classes['title']),L(dls.classes['user'])\n\n((#1665) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...],\n (#944) ['#na#',1,2,3,4,5,6,7,8,9...])\n\n\n\n# define dimensions for users and movies latent factor matrices\nn_users = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\nn_users, n_movies, n_factors\n\n(944, 1665, 5)\n\n\n\n# build users and movies latent factor matrices\nuser_factors = torch.randn(n_users,n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\nuser_factors.shape, movie_factors.shape\n\n(torch.Size([944, 5]), torch.Size([1665, 5]))\n\n\n\n\nEmbeddings instead of One Hot Encoded Matrix Multiplication\nOkay so how do we use these latent factor matrices?\nFor each row in our batch, we have a user ID and a movie ID. We need to get the latent factors for each of those and calculate the dot product, in order to predict our rating. As it adjusts the latent factors during the training loop, the predictions will get better.\nI’ll grab one batch from the dls DataLoaders object and illustrate an example prediction calculation. Each independent variable, x, is a tensor with [user, movie]. Each dependent variable, y, is a tensor with [rating].\n\nx,y = dls.one_batch()\nx.shape, y.shape\n\n(torch.Size([64, 2]), torch.Size([64, 1]))\n\n\n\nx[0], y[0]\n\n(tensor([466, 614]), tensor([3], dtype=torch.int8))\n\n\n\ntype(x[0]), type(y[0])\n\n(torch.Tensor, torch.Tensor)\n\n\nI determined the order of the x values by looking at the maximum value in each column, specifying axis=0. The movie IDs go up to 1644, so a max value of 1608 means that the movie is the second value in each tensor.\n\nx.max(axis=0)\n\ntorch.return_types.max(values=tensor([ 935, 1642]), indices=tensor([24,  3]))\n\n\nI get the latent factors for the user and movie in the first batch item.\n\nu = user_factors[x[0][0]]\nu\n\ntensor([-0.6595, -0.3355,  1.0491,  1.1764,  0.8750])\n\n\n\nm = movie_factors[x[0][1]]\nm\n\ntensor([-0.1751, -0.5016,  0.6298,  0.2370, -0.7902])\n\n\nI calculate the dot product of the two vectors, which is the sum of the element-wise product.\n\npred = (u * m).sum()\npred\n\ntensor(0.5320)\n\n\nI pass it through sigmoid_range to get a value between 0 and 5. Sigmoid outputs a value between 0 and 1, and sigmoid_range scales and shifts that function to fit the specified range. The output is the prediction for the rating that this user would give this movie.\n\npred = sigmoid_range(pred, 0, 5)\npred\n\ntensor(3.1497)\n\n\nSince the prediction and the target are a single numeric value, we’ll use Mean Squared Error loss. For a single value, the loss is the squared error. For a batch, the mean would be calculated.\n\nloss = (pred - y[0].item()) ** 2\nloss\n\ntensor(0.0224)\n\n\n\n\nBuilding a Collaborative Filtering Model from Scratch\nI’ll create a DotProduct class which builds an Embedding to store latent factor matrices for users and movies, and calculates the prediction in its forward method using the dot product of the user and movie latent factors.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n\n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\nI’ll illustrate how this model operates by coding through the calculation of predictions for one batch.\nI create Embeddings for users and movies. Their shape corresponds to the number of users and movies and the number of latent factors.\n\nuser_factors = Embedding(n_users, n_factors)\nmovie_factors = Embedding(n_movies, n_factors)\nuser_factors, movie_factors\n\n(Embedding(944, 5), Embedding(1665, 5))\n\n\nThe weight attribute holds the latent factor values, which are parameters whose gradient can be calculated. The values are from a normal distribution with mean 0 and variance 1.\n\nuser_factors.weight.shape\n\ntorch.Size([944, 5])\n\n\n\nuser_factors.weight\n\nParameter containing:\ntensor([[-0.0066, -0.0111,  0.0091,  0.0056,  0.0075],\n        [-0.0138, -0.0014,  0.0189,  0.0028, -0.0166],\n        [ 0.0149,  0.0053, -0.0153,  0.0078, -0.0119],\n        ...,\n        [-0.0051, -0.0117,  0.0170, -0.0102, -0.0044],\n        [ 0.0037, -0.0084,  0.0042, -0.0049,  0.0186],\n        [ 0.0199, -0.0194,  0.0044,  0.0012,  0.0084]], requires_grad=True)\n\n\nThe user_factors weight has the shape 944 rows by 5 columns. You can see that they are a tensor with requires_grad equals True.\n\nmovie_factors.weight\n\nParameter containing:\ntensor([[-0.0017, -0.0051,  0.0065,  0.0050, -0.0095],\n        [-0.0065, -0.0158,  0.0062, -0.0145, -0.0087],\n        [ 0.0067,  0.0111,  0.0059, -0.0003,  0.0061],\n        ...,\n        [-0.0012,  0.0002, -0.0088, -0.0022, -0.0152],\n        [-0.0053, -0.0058, -0.0074, -0.0033, -0.0171],\n        [ 0.0030,  0.0031, -0.0037, -0.0023,  0.0157]], requires_grad=True)\n\n\nThe movie_factors weight has the shape 1665 rows by 5 columns. And here you can see it is a tensor as well with requires_grad equals True.\n\nmovie_factors.weight.shape\n\ntorch.Size([1665, 5])\n\n\nIn my batch, the 0th column of the dependent variable x holds user indexes. I pass that to user_factors and receive a tensor with those users’ latent factors. Column index 1 holds the movie indexes, I pass that to movie_factors and receive a tensor with those movies’ latent factors.\n\nusers = user_factors(x[:,0])\nusers[:5]\n\ntensor([[ 0.0029,  0.0042, -0.0093,  0.0023, -0.0053],\n        [ 0.0029,  0.0008,  0.0193,  0.0082,  0.0117],\n        [-0.0025,  0.0070, -0.0144, -0.0193,  0.0086],\n        [ 0.0103,  0.0028,  0.0172,  0.0110,  0.0084],\n        [-0.0087, -0.0109,  0.0062, -0.0018, -0.0012]],\n       grad_fn=<SliceBackward>)\n\n\n\nmovies = movie_factors(x[:,1])\nmovies[:5]\n\ntensor([[ 0.0011, -0.0009,  0.0114,  0.0017,  0.0033],\n        [ 0.0049, -0.0019,  0.0175,  0.0027, -0.0014],\n        [-0.0047, -0.0026,  0.0032,  0.0028, -0.0146],\n        [-0.0103, -0.0024,  0.0057, -0.0141, -0.0080],\n        [ 0.0099,  0.0113,  0.0022,  0.0123,  0.0096]],\n       grad_fn=<SliceBackward>)\n\n\nI take the dot product and pass it through a sigmoid_range and the calculate the predictions for the batch.\n\npreds = sigmoid_range((users * movies).sum(dim=1), 0, 5.5)\npreds, preds.shape\n\n(tensor([2.7498, 2.7505, 2.7497, 2.7497, 2.7497, 2.7501, 2.7495, 2.7506, 2.7499,\n         2.7502, 2.7503, 2.7506, 2.7501, 2.7498, 2.7497, 2.7507, 2.7498, 2.7497,\n         2.7499, 2.7500, 2.7499, 2.7500, 2.7502, 2.7501, 2.7502, 2.7499, 2.7500,\n         2.7499, 2.7499, 2.7501, 2.7503, 2.7497, 2.7500, 2.7498, 2.7497, 2.7496,\n         2.7502, 2.7502, 2.7501, 2.7498, 2.7501, 2.7502, 2.7500, 2.7501, 2.7506,\n         2.7500, 2.7498, 2.7499, 2.7501, 2.7502, 2.7502, 2.7501, 2.7498, 2.7501,\n         2.7501, 2.7499, 2.7499, 2.7499, 2.7498, 2.7502, 2.7499, 2.7498, 2.7494,\n         2.7499], grad_fn=<AddBackward0>), torch.Size([64]))\n\n\nThat’s what the DotProduct model will return. I can then take the mean squared error and calculate the loss value, which I can then call backward on, to calculate the gradients for the latent factors. I can then multiply them by the learning rate, and add them to the weights, and repeat the training loop.\n\nloss = ((preds - y) ** 2).mean()\nloss\n\ntensor(2.0156, grad_fn=<MeanBackward0>)\n\n\n\nloss.backward()\n\n\nuser_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\nmovie_factors.weight.grad\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        ...,\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nWe see here that the gradients are small but not all of them are zero:\n\nuser_factors.weight.grad.sum(), movie_factors.weight.grad.sum()\n\n(tensor(-0.0108), tensor(-0.0026))\n\n\n\nTraining the Model\nThere are five different models that I will build, from simple to complex.\nModel 1 will not use sigmoid_range for the dot product prediction calculation. Instead, I will get a value that is normally distributed with a mean of zero and a variance of 1. Model 2 will pass the predictions through sigmoid_range to get an output between 0 and 5.5. Model 3 will add a bias parameter to the dot product prediction, so that we can establish some baseline rating of each movie that’s independent of a particular latent factor. Model 4 will introduce weight decay in order to better generalize our model, and in Model 5 I’ll implement a custom class instead of using the built-in PyTorch Embedding class.\n\n\n\nModel\nsigmoid_range\nBias\nWeight Decay\nCustom Embedding\n\n\n\n\n1\nN\nN\nN\nN\n\n\n2\nY\nN\nN\nN\n\n\n3\nY\nY\nN\nN\n\n\n4\nY\nY\nY\nN\n\n\n5\nY\nY\nY\nY\n\n\n\n\nModel 1: No sigmoid_range\nThe DotProduct class will initialize an Embedding for users and movies with random values from a normal distribution with mean 0 and variance 1. In the forward method, called during the prediction step of the training loop, the latent factors for the batch’s users and movies are accessed from the corresponding Embedding and the dot product is calculated and returned as the prediction.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return (users * movies).sum(dim=1)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.339355\n      1.274582\n      00:06\n    \n    \n      1\n      1.039260\n      1.061794\n      00:06\n    \n    \n      2\n      0.945793\n      0.954731\n      00:06\n    \n    \n      3\n      0.826578\n      0.871694\n      00:06\n    \n    \n      4\n      0.752750\n      0.858231\n      00:06\n    \n  \n\n\n\nThe average prediction error on the validation ratings is the square root of the final validation loss.\n\nmath.sqrt(learn.recorder.final_record[1])\n\n0.9264077107145671\n\n\nHere are some predictions on the validation set ratings:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      554\n      37\n      4\n      4.181926\n    \n    \n      1\n      561\n      811\n      4\n      3.008064\n    \n    \n      2\n      503\n      298\n      5\n      4.451642\n    \n    \n      3\n      380\n      581\n      4\n      3.474877\n    \n    \n      4\n      666\n      422\n      4\n      4.054492\n    \n    \n      5\n      444\n      933\n      2\n      3.940120\n    \n    \n      6\n      368\n      1612\n      3\n      2.864129\n    \n    \n      7\n      537\n      457\n      3\n      2.955165\n    \n    \n      8\n      224\n      1535\n      1\n      2.940819\n    \n  \n\n\n\n\n\nModel 2 - with sigmoid_range\nIn this model, I’ll force the predictions to fall within the range of actual ratings. The book recommends, based on what they’ve experienced, using a maximum rating for predictions that is slightly larger than the maximum ground truth rating. The range of predictions we’ll use is 0 to 5.5.\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.009657\n      0.977510\n      00:07\n    \n    \n      1\n      0.881551\n      0.891973\n      00:06\n    \n    \n      2\n      0.686247\n      0.853506\n      00:07\n    \n    \n      3\n      0.487054\n      0.857519\n      00:06\n    \n    \n      4\n      0.374934\n      0.862651\n      00:06\n    \n  \n\n\n\n\nmath.sqrt(learn.recorder.final_record[1])\n\n\nlearn.show_results()\n\nThe valid loss starts off lower, but by the end of the training, I don’t see an improvement. In fact, the valid loss starts to increase at the end. This is an indication that overfitting is taking place. This will be addressed in Model 4.\n\n\nModel 3 - Add bias\nBut first, let’s look at Model 3, which adds a bias parameter to the predictions, which provides a baseline rating for each movie independent of the weights related to the different latent factors.\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = Embedding(n_users, n_factors)\n    self.user_bias = Embedding(n_users, 1)\n    self.movie_factors = Embedding(n_movies, n_factors)\n    self.movie_bias = Embedding(n_movies, 1)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors(x[:,0])\n    movies = self.movie_factors(x[:,1])\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n    return sigmoid_range(res, *self.y_range)\n\nIn addition to initializing new Embeddings for bias, the dot product is first kept in matrix form by passing keepdim=True to the sum method. The biases are then added on afterwhich the result is passed through sigmoid_range. Here’s a illustrative example for how keepdim=True affects the dot product:\n\na = Tensor([[1,2,3], [4,5,6]])\n(a * a).sum(dim=1).shape, (a * a).sum(dim=1, keepdim=True).shape\n\n(torch.Size([2]), torch.Size([2, 1]))\n\n\nThe first tensor is a 2-vector, whereas the second tensor is a 2 x 1 matrix.\n\nLet’s train Model 3 and view the results!\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.952214\n      0.912603\n      00:07\n    \n    \n      1\n      0.825348\n      0.845193\n      00:07\n    \n    \n      2\n      0.618910\n      0.852381\n      00:07\n    \n    \n      3\n      0.406514\n      0.875637\n      00:07\n    \n    \n      4\n      0.293729\n      0.882840\n      00:07\n    \n  \n\n\n\nThe initial validation loss is lower than the previous trainings, but Model 3 is overfitting even more than Model 2. It’s time to introduce weight decay.\n\n\nModel 4 - Use Weight Decay\nSmaller weights lead to a smoother function which corresponds to fewer inflection points, leading to a better generalization of the model. Larger weights lead to a sharper function, corresponding to more inflection points which overfit the training data.\nThe text uses the basic example of a parabola to illustrate. As the weight a increases, the function becomes narrower, with a sharper trough.\n\nHTML('<iframe src=\"https://www.desmos.com/calculator/uog6rvyubg\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>')\n\n\n\n\nWe will intentionally increase the gradients so that the weights are stepped with larger increments toward a smaller value.\nThis corresponds to an intentionally larger loss value calculated for each batch:\nloss_with_wd = loss + wd * (parameters**2).sum()\nThe parameters are squared to ensure a positive value. Taking the derivative of the loss function means taking the derivative of the following function:\nwd * (parameters**2).sum()\nWhich results in:\n2 * wd * parameters\nInstead of multiplying by 2, we can just use twice the weight decay value as wd. I’ll use a weight decay of 0.1 as they do in the text.\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.939321\n      0.937423\n      00:07\n    \n    \n      1\n      0.851871\n      0.855584\n      00:07\n    \n    \n      2\n      0.720202\n      0.815807\n      00:07\n    \n    \n      3\n      0.630149\n      0.806268\n      00:07\n    \n    \n      4\n      0.491224\n      0.807063\n      00:07\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      877\n      1538\n      3\n      3.708797\n    \n    \n      1\n      601\n      1285\n      1\n      2.832736\n    \n    \n      2\n      292\n      1147\n      2\n      3.675529\n    \n    \n      3\n      132\n      400\n      4\n      3.504542\n    \n    \n      4\n      405\n      1614\n      1\n      1.930779\n    \n    \n      5\n      655\n      458\n      3\n      3.274209\n    \n    \n      6\n      453\n      60\n      4\n      3.864617\n    \n    \n      7\n      629\n      1498\n      4\n      3.598821\n    \n    \n      8\n      724\n      580\n      4\n      3.921505\n    \n  \n\n\n\n\n\nModel 5 - custom Embedding class\nThe final model I’ll train does not have a fundamentally different component than the other four. Instead of using the built-in Embedding PyTorch class, the text has us write our own class.\nOptimizers get a module’s parameters by calling the parameters method. We have to wrap parameters in nn.Parameter for them to be recognized as such. This class also calls requires_grad_ for us.\nI’ll replace each Embedding with a tensor filled with random values from a normal distribution with mean 0 and variance 0.01.\n\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0,0.01))\n\n\ntorch.zeros(3,4)\n\ntensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n\n\ntorch.zeros(3,4).normal_(0,0.01)\n\ntensor([[-0.0134,  0.0098, -0.0124, -0.0032],\n        [ 0.0056,  0.0071,  0.0005,  0.0014],\n        [-0.0236, -0.0024, -0.0060,  0.0017]])\n\n\nI redefine the DotProductBias model using the create_params method instead of Embedding, and train the model.italicized text\n\nclass DotProductBias(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n    self.user_factors = create_params([n_users, n_factors])\n    self.user_bias = create_params([n_users])\n    self.movie_factors = create_params([n_movies, n_factors])\n    self.movie_bias = create_params([n_movies])\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.user_factors[x[:,0]]\n    movies = self.movie_factors[x[:,1]]\n    res = (users * movies).sum(dim=1)\n    res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5,5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.934845\n      0.933218\n      00:08\n    \n    \n      1\n      0.841111\n      0.859618\n      00:08\n    \n    \n      2\n      0.730065\n      0.820388\n      00:08\n    \n    \n      3\n      0.599684\n      0.807086\n      00:08\n    \n    \n      4\n      0.484760\n      0.807552\n      00:08\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      487\n      6\n      3\n      2.994869\n    \n    \n      1\n      54\n      349\n      3\n      2.511689\n    \n    \n      2\n      501\n      1252\n      4\n      4.130728\n    \n    \n      3\n      244\n      861\n      4\n      2.314526\n    \n    \n      4\n      322\n      1501\n      5\n      3.823119\n    \n    \n      5\n      537\n      1294\n      2\n      3.124064\n    \n    \n      6\n      193\n      1530\n      3\n      2.546681\n    \n    \n      7\n      581\n      286\n      5\n      3.062707\n    \n    \n      8\n      450\n      154\n      4\n      4.161049\n    \n  \n\n\n\nI get similar results as before!\n\n\n\nInterpreting Embeddings and Biases\nI’ll save this model so that the embedding and bias analyses I perform can be recreated.\n\nlearn = load_learner(\"/content/gdrive/MyDrive/fastai-course-v4/dot_product_bias.pkl\")\nmodel = learn.model\n\nBias represents a baseline rating of a movie regardless of how well the latent factors of the movie match the latent factors of the user. Low bias values correspond to movies that people didn’t enjoy, even if it matched their preferences.\nWhat were the 5 generally least liked movies?\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nmovie_bias = model.movie_bias\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Robocop 3 (1993)',\n 'Showgirls (1995)',\n 'Kansas City (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)']\n\n\nTo answer that question, I get the indexes of the sorted movie_bias values in ascending order, grab the first 5, and get their title from the DataLoaders classes. These 5 movies had the 5 lowest bias values.\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'L.A. Confidential (1997)',\n 'As Good As It Gets (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Good Will Hunting (1997)']\n\n\n\nVisualizing Embeddings\nThe embeddings are a 50-dimensional matrix of latent factors. I’ll use Principal Component Analysis (PCA) to extract the two most descriptive dimensions and plot the latent factor values. I’ll also calculate the distance from 0 of each movie so that I can filter for outliers in order to reduce the number of data points on the plot, and help me understand what these latent factors may be.\n\n!pip install fbpca\nimport fbpca\n\nCollecting fbpca\n  Downloading https://files.pythonhosted.org/packages/a7/a5/2085d0645a4bb4f0b606251b0b7466c61326e4a471d445c1c3761a2d07bc/fbpca-1.0.tar.gz\nBuilding wheels for collected packages: fbpca\n  Building wheel for fbpca (setup.py) ... done\n  Created wheel for fbpca: filename=fbpca-1.0-cp37-none-any.whl size=11376 sha256=719b80446eeb8f157c99e298adb61b0978c0ae279ade82e500dbe37902c447e4\n  Stored in directory: /root/.cache/pip/wheels/53/a2/dd/9b66cf53dbc58cec1e613d216689e5fa946d3e7805c30f60dc\nSuccessfully built fbpca\nInstalling collected packages: fbpca\nSuccessfully installed fbpca-1.0\n\n\nI grab the movie_factors from the trained model, bring it over the the .cpu(), .detach() it from the gradients and convert it to a .numpy() array.\n\nmovie_embeddings = model.movie_factors.cpu().detach().numpy()\n\nI pass those embeddings to the fbpca.pca method and get back the rank-2 approximation.\n\nU, s, Va = fbpca.pca(movie_embeddings, k=2)\n\nI then create a DataFrame from the U matrix which is an m x k (1665 movies x 2 components) matrix. I also create a column with the calculated distance from 0 of each movie, based on the 2-component coordinates, and a column specifying which quadrant the movie is in (First, Second, Third or Fourth).\nMy distance function receives each DataFrame row, and returns the square root of the sum of squares of the two coordinates.\nMy quadrant function received each row and based on the sign of the x or 0 column and the y or 1 column, determines which quadrant that movie lies in.\nI apply both functions to the DataFrame and specify axis=1 so that I can access the column names.\n\n# helper functions\ndef distance(row):\n  return np.sqrt(row[0]**2 + row[1]**2)\n\ndef quadrant(row):\n  if (row[0] > 0 and row[1] > 0):\n    return \"First Quadrant\"\n  elif (row[0] < 0 and row[1] > 0):\n    return \"Second Quadrant\"\n  elif (row[0] < 0 and row[1] < 0):\n    return \"Third Quadrant\"\n  elif (row[0] > 0 and row[1] < 0):\n    return \"Fourth Quadrant\"\n  else:\n    return \"Center\"\n\n\n# create DataFrame from PCA output\ndef pca_to_df(U):\n  df = pd.DataFrame(data=U)\n\n  # calculate the distance of each Embedding from 0\n  df[2] = df.apply(lambda x: np.sqrt(x[0]**2 + x[1]**2), axis=1)\n\n  # identify which quadrant the movie is in\n  df[3] = df.apply(lambda x: quadrant(x), axis=1)\n\n  return df\n\nI’ll import the DataFrame I created from my original PCA output so that I can recreate the corresponding plots.\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movie_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.010150\n      0.004517\n      0.011110\n      First Quadrant\n    \n    \n      1\n      0.025090\n      -0.000186\n      0.025091\n      Fourth Quadrant\n    \n    \n      2\n      -0.005773\n      0.025443\n      0.026090\n      Second Quadrant\n    \n    \n      3\n      0.015933\n      -0.021972\n      0.027141\n      Fourth Quadrant\n    \n    \n      4\n      -0.056279\n      -0.013351\n      0.057841\n      Third Quadrant\n    \n  \n\n\n\n\nWhen I plot the first two columns as x,y coordinates, I can see that the movies are spread out quite evenly across those latent factors.\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06691494d0>\n\n\n\n\n\nI’m going to plot the farthest points from the origin and label them on the plot in order to get a sense of what these two latent factors may represent.\nFor each quadrant, I grab the indexes for the rows with the 5 largest distances and create a DataFrame from that and plot that data.\n\ndef plot_top_5(df):\n  # get the 5 points farthest from 0 from each quadrant\n  idxs = np.array([])\n  for quad in df['3'].unique():\n    idxs = np.append(idxs, df[df['3']==quad]['2'].sort_values(ascending=False).index[:5].values)\n  plot_df = df.loc[idxs]\n\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_top_5(df)\n\n\n\n\nThe first quadrant seems to represent comedies (Ed, Ready to Wear, The Stupids) although Barb Wire is not a comedy.\nThe second quadrant has movies with some more dark and disturbing elements to it.\nThe third quadrant has movies which are drama and I guess share some theme of gaining freedom?\nFinally, the fourth quadrant seems to have drama and action movies which have a distinct American storyline to them. I haven’t seen all of these movies, but Dirty Dancing and The American President seem like anomalies in this group.\nI should also take a look at other movies that do not fall on the extreme ends of each quadrant. For example, which movies fall close to the vertical and horizontal axes?\n\ndef plot_close_to_axes(df):\n  # get 5 points closes to each axis \n  idxs = np.array([])\n  for key in ['0', '1']:\n    idxs = np.append(idxs, df[np.abs(df[key]) < 0.0002].index.values)\n  plot_df = df.loc[idxs]\n  plot_df = plot_df.drop_duplicates()\n\n  # set figure size\n  %matplotlib inline\n  plt.rcParams['figure.figsize'] = [15, 15]\n\n  # get the movie titles which will be plot annotations\n  movies = dls.classes['title'][idxs]\n\n  fig, ax = plt.subplots()\n\n  ax.scatter(plot_df['0'], plot_df['1'])\n\n  # annotate with movie titles\n  for i, idx in enumerate(idxs):\n    ax.annotate(movies[i], (plot_df.loc[idx,'0'], plot_df.loc[idx, '1']))\n\n  plt.show()\n\n\nplot_close_to_axes(df)\n\n\n\n\nThe latent factor corresponding to the vertical axis seems to represent drama (positive values) and romance (negative values) whereas the horizontal axis represents elements mystery (negative values) and comedy (positive values). However, I could just be focusing on genre whereas there are other features of a movie these may represent. Unfortunately, I’m not a movie buff, so the need for a domain expert is evident here!\n\n\nRobust PCA\nAlthough it’s out of scope for this chapter and my understanding, I’d like to at least experiment with using Robust PCA for visualizing embeddings. Robust PCA is an algorithm which decomposes a matrix M into two components: a low rank matrix L and a sparse matrix S such that M = L + S. From what I understand, the sparse matrix S contains anomalies or outliers or “corrupted” data, whereas L contains a more accurate representation of the original data. For example, if an image has some additional noise added to it, the original image matrix M can be decomposed into a noise-less “clean” image L and a sparse noise matrix S. Another example is if you have an image with a background (such as a landscape with lawn, sidewalks, buildings) and a foreground (people walking on the sidewalk) passing that image through the RPCA algorithm would yield a background matrix L with the lawn, sidewalk and buildings and a foreground sparse matrix S with the people. Since my movie_embeddings matrix may contain anomalies which would affect the effectiveness and accuracy of my 2-component PCA approximation, I will pass it through a RPCA algorithm and calculate the 2-component approximation on the low-rank L and sparse S matrices and compare the results with what I calculated above.\nThe following algorithm is from Rachel Thomas’ lesson on RPCA as part of her Computational Linear Algebra course.\n\nfrom scipy import sparse\nfrom sklearn.utils.extmath import randomized_svd\nimport fbpca\n\n\nTOL=1e-9\nMAX_ITERS=3\n\n\ndef converged(Z, d_norm):\n    err = np.linalg.norm(Z, 'fro') / d_norm\n    print('error: ', err)\n    return err < TOL\n\n\ndef shrink(M, tau):\n    S = np.abs(M) - tau\n    return np.sign(M) * np.where(S>0, S, 0)\n\n\ndef _svd(M, rank): return fbpca.pca(M, k=min(rank, np.min(M.shape)), raw=True)\n\n\ndef norm_op(M): return _svd(M, 1)[1][0]\n\n\ndef svd_reconstruct(M, rank, min_sv):\n    u, s, v = _svd(M, rank)\n    s -= min_sv\n    nnz = (s > 0).sum()\n    return u[:,:nnz] @ np.diag(s[:nnz]) @ v[:nnz], nnz\n\n\ndef pcp(X, maxiter=10, k=10): # refactored\n    m, n = X.shape\n    trans = m<n\n    if trans: X = X.T; m, n = X.shape\n        \n    lamda = 1/np.sqrt(m)\n    op_norm = norm_op(X)\n    Y = np.copy(X) / max(op_norm, np.linalg.norm( X, np.inf) / lamda)\n    mu = k*1.25/op_norm; mu_bar = mu * 1e7; rho = k * 1.5\n    \n    d_norm = np.linalg.norm(X, 'fro')\n    L = np.zeros_like(X); sv = 1\n    \n    examples = []\n    \n    for i in range(maxiter):\n        print(\"rank sv:\", sv)\n        X2 = X + Y/mu\n        \n        # update estimate of Sparse Matrix by \"shrinking/truncating\": original - low-rank\n        S = shrink(X2 - L, lamda/mu)\n        \n        # update estimate of Low-rank Matrix by doing truncated SVD of rank sv & reconstructing.\n        # count of singular values > 1/mu is returned as svp\n        L, svp = svd_reconstruct(X2 - S, sv, 1/mu)\n        \n        # If svp < sv, you are already calculating enough singular values.\n        # If not, add 20% (in this case 240) to sv\n        sv = svp + (1 if svp < sv else round(0.05*n))\n        \n        # residual\n        Z = X - L - S\n        Y += mu*Z; mu *= rho\n        \n        examples.extend([S[140,:], L[140,:]])\n        \n        if m > mu_bar: m = mu_bar\n        if converged(Z, d_norm): break\n    \n    if trans: L=L.T; S=S.T\n    return L, S, examples\n\n\nL, S, examples = pcp(movie_embeddings)\n\n\nL.shape, S.shape\n\nI’ll calculate 2-component PCA for the L and S matrices and plot those to see how they compare to the plots above.\n\nU_L, _, _ = fbpca.pca(L, k=2)\nU_S, _, _ = fbpca.pca(S, k=2)\n\nI exported the outputs to CSV for repeatability, so I’ll import them in again:\n\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_L_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.009254\n      -0.003454\n      0.009877\n      Fourth Quadrant\n    \n    \n      1\n      0.032954\n      0.008637\n      0.034067\n      First Quadrant\n    \n    \n      2\n      -0.014662\n      -0.047128\n      0.049356\n      Third Quadrant\n    \n    \n      3\n      0.012602\n      0.029182\n      0.031787\n      First Quadrant\n    \n    \n      4\n      -0.037841\n      0.008742\n      0.038838\n      Second Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06668a58d0>\n\n\n\n\n\nThe scatter plot for the 2-component PCA result seems much more evenly distributed across the quadrants.\nI take the 5 farthest movies from each quadrant and plot those separately.\n\nplot_top_5(df)\n\n\n\n\nHere are the patterns I observe. Again, someone who has watched these movies and is not just reading online descriptions of them would see themes and patterns that I would not.\n\n\n\nQuadrant\nObservation\n\n\n\n\n1\nRomance/Drama movies. Fausto and Castle Freak seem out of place\n\n\n2\nMore romance movies. Top Gun and Prefontaine seem out of place\n\n\n3\nMore romance movies. The Butcher Boy seems out of place.\n\n\n4\nComedies. The Carmen Miranda documentary seems out of place.\n\n\n\nAfter making these observations, either the low-rank matrix L is a poor choice to use for this type of analysis, or my understanding these movies is too shallow to see the deeper relationships between them. With so many romance movies across the plot, I don’t think these latent factors represent genres.\nI’m not too confident the S matrix will provide more clarity, but let’s see!\n\n# import previously generated CSV\ndf = pd.read_csv(\"/content/gdrive/MyDrive/fastai-course-v4/movies_S_pca.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      -0.009776\n      0.005041\n      0.010999\n      Second Quadrant\n    \n    \n      1\n      -0.021503\n      0.001765\n      0.021576\n      Second Quadrant\n    \n    \n      2\n      0.005098\n      0.024645\n      0.025166\n      First Quadrant\n    \n    \n      3\n      -0.016745\n      -0.021457\n      0.027218\n      Third Quadrant\n    \n    \n      4\n      0.056964\n      -0.023095\n      0.061467\n      Fourth Quadrant\n    \n  \n\n\n\n\n\nplt.scatter(df['0'], df['1'])\n\n<matplotlib.collections.PathCollection at 0x7f06681c3310>\n\n\n\n\n\n\nplot_top_5(df)\n\n\n\n\nInteresting! This plot looks like the plot of the original M matrix PCA results reflected across the y-axis. Similar movies are grouped together but the latent factors are showing an inverse relationship to the original 2-components.\n\n\n\nUsing fastai.collab\nfastai comes with a built-in method to create a collaborative filtering model similar to the DotProductBias model I created.\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.932735\n      0.930009\n      00:10\n    \n    \n      1\n      0.834800\n      0.862961\n      00:11\n    \n    \n      2\n      0.746893\n      0.822192\n      00:10\n    \n    \n      3\n      0.585107\n      0.811398\n      00:10\n    \n    \n      4\n      0.490022\n      0.812597\n      00:10\n    \n  \n\n\n\nThis yields similar results to what I’ve done above. Here are this model’s results:\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      541\n      332\n      1\n      2.542970\n    \n    \n      1\n      899\n      1295\n      4\n      3.555057\n    \n    \n      2\n      346\n      1492\n      3\n      3.456876\n    \n    \n      3\n      933\n      1399\n      4\n      3.380442\n    \n    \n      4\n      310\n      1618\n      5\n      4.623110\n    \n    \n      5\n      276\n      1572\n      4\n      3.636531\n    \n    \n      6\n      463\n      322\n      5\n      3.901797\n    \n    \n      7\n      130\n      408\n      4\n      3.343735\n    \n    \n      8\n      914\n      1617\n      4\n      3.076288\n    \n  \n\n\n\nThe model created is a EmbeddingDotBias model\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nThe top biases can be obtained similar to how we did it before, but with a slightly different API:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Star Wars (1977)',\n \"Schindler's List (1993)\"]\n\n\nSimilar to the distance function I created, PyTorch has a nn.CosineSimilarity function which calculates the cosine of the angle between two vectors. The smaller the angle, the closer the two points are, and the more similar they are. nn.CosineSimilarity returns the similarity (cosine of the angle) between two vectors where 1.000 means the angle is 0.\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\nsimilarity = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = similarity.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Some Folks Call It a Sling Blade (1993)'\n\n\n\n\n\nDeep Learning for Collaborative Filtering\nIn this final section, we create a Deep Learning model which can make predictions on movie ratings after training on the MovieLens dataset. The model uses Embeddings (for users and movies) which are then fed into a small neural net (with one ReLu sandwiched between two Linear layers) which outputs an activation which we normalize using sigmoid_range. The embedding matrices are sized based on a heuristic built-in to fastai with the get_emb_sz method:\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nThe model used is constructed as follows: the user and item latent factors are created using Embeddings, and the neural net is created using the nn.Sequential class. Each time a prediction is needed, the user and item matrices for one batch are concatenated and passed through the neural net. The returned activation is sent to sigmoid_range and a prediction between 0 and 5.5 is calculated.\n\nclass CollabNN(Module):\n  def __init__(self, user_sz, item_sz, y_range=(0, 5.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.item_factors = Embedding(*item_sz)\n    self.layers = nn.Sequential(\n        nn.Linear(user_sz[1]+item_sz[1], n_act),\n        nn.ReLU(),\n        nn.Linear(n_act, 1))\n    self.y_range = y_range\n\n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1))\n    return sigmoid_range(x, *self.y_range)\n\nI want to visualize the forward method, so I’ll create the model and a batch, and walkthrough the code.\n\nmodel = CollabNN(*embs)\nx,y = dls.one_batch()\ndevice = \"cpu\"\nx = x.to(device)\nmodel = model.to(device)\nembs = torch.cat((model.user_factors(x[:,0]), model.item_factors(x[:,1])), dim=1)\nembs.shape\n\ntorch.Size([64, 176])\n\n\n\nx = model.layers(embs)\nsigmoid_range(x, *model.y_range)[:5]\n\ntensor([[2.8637],\n        [2.8647],\n        [2.8624],\n        [2.8696],\n        [2.8601]], grad_fn=<SliceBackward>)\n\n\nThe fastai collab_learner, instead of using the EmbeddingDotBias model, will use a neural network if passed True for its use_nn parameter. The number and size of neural network layers can also be specified.\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.988426\n      0.984418\n      00:15\n    \n    \n      1\n      0.893442\n      0.909180\n      00:16\n    \n    \n      2\n      0.900106\n      0.877499\n      00:16\n    \n    \n      3\n      0.809255\n      0.853736\n      00:16\n    \n    \n      4\n      0.769467\n      0.853571\n      00:16\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      244\n      1305\n      4\n      4.185966\n    \n    \n      1\n      902\n      965\n      3\n      3.474954\n    \n    \n      2\n      87\n      1173\n      5\n      4.206645\n    \n    \n      3\n      759\n      333\n      5\n      4.247213\n    \n    \n      4\n      109\n      1624\n      3\n      3.726794\n    \n    \n      5\n      363\n      743\n      1\n      1.774737\n    \n    \n      6\n      756\n      1216\n      5\n      4.058509\n    \n    \n      7\n      378\n      179\n      4\n      3.192873\n    \n    \n      8\n      18\n      141\n      3\n      3.296141\n    \n  \n\n\n\n\nlearn.model\n\nEmbeddingNN(\n  (embeds): ModuleList(\n    (0): Embedding(944, 74)\n    (1): Embedding(1665, 102)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=176, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=50, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=50, out_features=1, bias=True)\n    )\n    (3): SigmoidRange(low=0, high=5.5)\n  )\n)\n\n\nThe EmbeddingNN architecture extends the TabularModel class which we will explore in Chapter 9.\nThat finishes my review of Chapter 8, I’ll be working through the “Further Research” section in upcoming blog posts and associated videos:\n\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens.\n\n\n\n\nFind three other areas where collaborative filtering is being used, and identify the pros and cons of this approach in those areas.\n\n\n\n\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forums for ideas. Note that there are more columns in the full dataset–see if you can use those too (the next chapter might give you ideas).\n\n\n\n\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "",
    "text": "In this blog post, I work through the book Effective Pandas by Matt Harrison. I’ll take notes, work through examples and end-of-chapter exercises."
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-4-series-introduction",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-4-series-introduction",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 4: Series Introduction",
    "text": "Chapter 4: Series Introduction\nRepresent the following data in pure python:\n\n\n\nArtist\nData\n\n\n\n\n0\n145\n\n\n1\n142\n\n\n2\n38\n\n\n3\n13\n\n\n\n\nseries = {\n    'index': [0, 1, 2, 3],\n    'data': [145, 142, 38, 13],\n    'name': 'songs'\n}\n\nseries\n\n{'index': [0, 1, 2, 3], 'data': [145, 142, 38, 13], 'name': 'songs'}\n\n\nThe get function below can pull items out of this data structure based on the index:\n\ndef get(series, idx):\n    value_idx = series['index'].index(idx)\n    return series['data'][value_idx]\n\n\nget(series, 1)\n\n142\n\n\nThe index method on the list returns the list element at the provided index value.\n\n[0, 1, 2, 3].index(1)\n\n1\n\n\nBelow is an example that has string values for the index:\n\nsongs = {\n    'index': ['Paul', 'John', 'George', 'Ringo'],\n    'data': [145, 142, 38, 13],\n    'name': 'songs'\n}\n\n\nget(songs, 'John')\n\n142\n\n\nCreate a Series object from a list:\n\nimport pandas as pd\n\n\nsongs2 = pd.Series([145, 142, 38, 13], name = 'counts')\nsongs2\n\n0    145\n1    142\n2     38\n3     13\nName: counts, dtype: int64\n\n\nThe series is one-dimensional. The leftmost column is the index, also called the axis. The data (145, 142, 38, 13) is also called the values of the series. A DataFrame has two axes, one for the rows and another for the columns.\n\nsongs2.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nThe default values for an index are monotonically increasing integers. The index can be string-based as well (datatype for the index is object).\n\nsongs3 = pd.Series([145, 142, 38, 13],\n                   name = 'counts',\n                   index = ['Paul', 'John', 'George', 'Ringo'])\nsongs3\n\nPaul      145\nJohn      142\nGeorge     38\nRingo      13\nName: counts, dtype: int64\n\n\n\nsongs3.index\n\nIndex(['Paul', 'John', 'George', 'Ringo'], dtype='object')\n\n\nWe can insert Python objects into a series:\n\nclass Foo:\n    pass\n\nringo = pd.Series(\n    ['Richard', 'Starkey', 13, Foo()],\n    name = 'ringo')\n\nringo\n\n0                                 Richard\n1                                 Starkey\n2                                      13\n3    <__main__.Foo object at 0x135016ce0>\nName: ringo, dtype: object\n\n\nThe object data type is also used for a series with string values and values that have heterogeneous or mixed types.\nHere is a series that has NaN in it:\n\nimport numpy as np\nnan_series = pd.Series([2, np.nan],\n                       index = ['Ono', 'Clapton'])\nnan_series\n\nOno        2.0\nClapton    NaN\ndtype: float64\n\n\nfloat64 supports NaN while int64 does not. As of pandas 0.24, Int64 (nullable integer type) supports NaN.\ncount ignores NaNs, .size does not.\n\nnan_series.count()\n\n1\n\n\n\nnan_series.size\n\n2\n\n\n\nnan_series2 = pd.Series([2, None],\n                        index = ['Ono', 'Clapton'],\n                        dtype = 'Int64')\nnan_series2\n\nOno           2\nClapton    <NA>\ndtype: Int64\n\n\n\nnan_series2.count()\n\n1\n\n\n\n# convert data type\nnan_series.astype('Int64')\n\nOno           2\nClapton    <NA>\ndtype: Int64\n\n\nThe Series object behaves similarly to a NumPy array.\n\nnumpy_ser = np.array([145, 142, 38, 13])\nsongs3[1], numpy_ser[1]\n\n(142, 142)\n\n\nThey both have methods in common\n\nsongs3.mean(), numpy_ser.mean()\n\n(84.5, 84.5)\n\n\nThey both have a notion of a boolean array.\n\nmask = songs3 > songs3.median()\nmask\n\nPaul       True\nJohn       True\nGeorge    False\nRingo     False\nName: counts, dtype: bool\n\n\n\n# use mask as a filter\nsongs3[mask]\n\nPaul    145\nJohn    142\nName: counts, dtype: int64\n\n\n\n# NumPy equivalent\nnumpy_ser[numpy_ser > np.median(numpy_ser)]\n\narray([145, 142])\n\n\nIf can indicate that data is categorical.\nCategorical values:\n\nUse less memory than strings\nImpove performance\nCan have an ordering\nCan perform operations on categories\nEnforce membership on values\n\n\ns = pd.Series(['m', 'l', 'xs', 's', 'xl'], dtype = 'category')\ns\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['l', 'm', 's', 'xl', 'xs']\n\n\nBy default categories don’t have an ordering.\n\ns.cat.ordered\n\nFalse\n\n\nConvert non-categorical series to an ordered category:\n\ns2 = pd.Series(['m', 'l', 'xs', 's', 'xl'])\n\nsize_type = pd.api.types.CategoricalDtype(\n    categories=['s', 'm', 'l'], ordered = True)\n\ns3 = s2.astype(size_type)\ns3\n\n0      m\n1      l\n2    NaN\n3      s\n4    NaN\ndtype: category\nCategories (3, object): ['s' < 'm' < 'l']\n\n\n\n# can perform comparisons on ordered categories\ns3 > 's'\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\n\n# add ordering information to categorical data\ns.cat.reorder_categories(['xs', 's', 'm', 'l', 'xl'], ordered=True)\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['xs' < 's' < 'm' < 'l' < 'xl']\n\n\nFor strings and dates converted to categorical types, we can still use the str or dt attributes on them:\n\ns3.str.upper()\n\n0      M\n1      L\n2    NaN\n3      S\n4    NaN\ndtype: object\n\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\npd.Series(data=None, index=None, dtype=None, name=None, copy=False)\nCreate a series from data (sequence, dictionary or scalar)\n\n\ns.index\nAccess index of series.\n\n\ns.astype(dtype, errors='raise')\nCast a series to dtype. To ignore errors (and return original object) use errors='ignore'\n\n\ns[boolean_array]\nReturn values from s where boolean_array is True\n\n\ns.cat.ordered\nDetermine if a categorical series is ordered\n\n\ns.cat.reorder_categories(new_categories, ordered=False)\nAdd categories (potentially ordered) to the series. new_categories must include all categories.\n\n\n\n\nExercises\n\nUsing Jupyter, create a series with the temperature values for the last seven days. Filter out the vaues below the mean.\nUsing Jupyter, create a series with your favorite colors. Use a categorical type.\n\n\n# temperature series\ntemps = pd.Series([88, 84, 84, 84, 88, 95, 97 ,88])\n\ntemps[temps >= temps.mean()]\n\n5    95\n6    97\ndtype: int64\n\n\n\n# favorite colors\ncolors_series = pd.Series(['orange', 'coral', 'midnight green'], dtype = 'category')\ncolors_series\n\n0            orange\n1             coral\n2    midnight green\ndtype: category\nCategories (3, object): ['coral', 'midnight green', 'orange']"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-5-series-deep-dive",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-5-series-deep-dive",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 5: Series Deep Dive",
    "text": "Chapter 5: Series Deep Dive\n\n# analyze the US Fuel Economy data\nurl = 'https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip'\n\ndf = pd.read_csv(url)\n\ncity_mpg = df.city08\nhighway_mpg = df.highway08\n\n/var/folders/5q/_bn7l90s177_2gq7rnhssjxm0000gn/T/ipykernel_52057/221626492.py:4: DtypeWarning: Columns (68,70,71,72,73,74,76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(url)\n\n\n\ncity_mpg\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\n\nhighway_mpg\n\n0        25\n1        14\n2        33\n3        12\n4        23\n         ..\n41139    26\n41140    28\n41141    24\n41142    24\n41143    21\nName: highway08, Length: 41144, dtype: int64\n\n\nBecause the type is int64 we know that none of the values are missing.\nThe dir function lists the attributes of an object. A series has 400+ attributes:\n\nlen(dir(city_mpg))\n\n412\n\n\n\nlen(dir(highway_mpg))\n\n412\n\n\nFunctionality of series attributes:\n\nDunder methods provide many numeric operations, looping, attribute access, and index access. For the numeric operations, these return Series.\nCorresponding operator methods for many of the numeric operations allow us to tweak the behavior.\nAggregate methods and properties which reduce or aggregate the values in a series down to a single scalar value.\nConversion methods. Some of these start with .to_ and export the data to other formats.\nManipulation methods that return Series objects with the same index.\nIndexing and accessor methods and attributes that return Series or scalars.\nString manipulation methods using .str.\nDate manipulation methods using .dt.\nPlotting methods using .plot.\nCategorical manipulation methods using .cat.\nTransformation methods.\nAttributes such as .index and .dtype.\nA bunch of private attributes (130 of them) that we’ll ignore.\n\n\nExercises\n\nExplore the documentation for five attributes of a series from Jupyter.\nHow many attributes are found on the .str attribute? Look at the documentation for three of them.\nHow many attributes are found on the .dt attribute? Look at the documentation for three of them.\n\n\ncity_mpg.values\n\narray([19,  9, 23, ..., 18, 18, 16])\n\n\n\ncity_mpg.axes\n\n[RangeIndex(start=0, stop=41144, step=1)]\n\n\n\ncity_mpg.empty\n\nFalse\n\n\n\ncity_mpg.at[4]\n\n17\n\n\n\ncity_mpg.loc[1:4]\n\n1     9\n2    23\n3    10\n4    17\nName: city08, dtype: int64\n\n\n\n# 98 string attributes\nlen(dir(s2.str))\n\n98\n\n\n\ns2.str.cat(sep = \".\")\n\n'm.l.xs.s.xl'\n\n\n\ns2.str.capitalize()\n\n0     M\n1     L\n2    Xs\n3     S\n4    Xl\ndtype: object\n\n\n\ns2.str.endswith('l')\n\n0    False\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n\n\n\ndt_series = pd.Series(['2023-01-01', '2023-04-05', '2023-07-06'])\n\ndt_series = pd.to_datetime(dt_series)\ndt_series\n\n0   2023-01-01\n1   2023-04-05\n2   2023-07-06\ndtype: datetime64[ns]\n\n\n\nlen(dir(dt_series.dt))\n\n83\n\n\n\ndt_series.dt.day\n\n0    1\n1    5\n2    6\ndtype: int32\n\n\n\ndt_series.dt.day_of_year\n\n0      1\n1     95\n2    187\ndtype: int32\n\n\n\ndt_series.dt.daysinmonth\n\n0    31\n1    30\n2    31\ndtype: int32"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-6-operators-dunder-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-6-operators-dunder-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 6: Operators (& Dunder Methods)",
    "text": "Chapter 6: Operators (& Dunder Methods)\nThese are the protocols that determine how the Python language reacts to operations.\n\n2 + 4\n\n6\n\n\n\n# under the cover is\n(2).__add__(4)\n\n6\n\n\n\n(city_mpg + highway_mpg) / 2\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\nWhen you operate with two series, pandas will align the index before performing the operation. Because of index alignment, you will want to make sure that the indexes: - are unique - are common to both series\n\n# example of series with repeated and non-common indexes\ns1 = pd.Series([10, 20, 30], index=[1,2,2])\ns2 = pd.Series([35, 44, 53], index=[2,2,4], name = 's2')\n\n\ns1\n\n1    10\n2    20\n2    30\ndtype: int64\n\n\n\ns2\n\n2    35\n2    44\n4    53\nName: s2, dtype: int64\n\n\n\n# index 1 and 4 have NaN\n# index 2 has four results\ns1 + s2\n\n1     NaN\n2    55.0\n2    64.0\n2    65.0\n2    74.0\n4     NaN\ndtype: float64\n\n\nWhen you perform math operations with a scalar, pandas broadcasts the operation to all values. A numeric pandas series is a block of memory, and modern CPUs leverage a technology called Single Instruction/Multiple Data (SIMD) to apply a math operation to the block of memory.\n\n# use `fill_value` parameter to replace missing operands\ns1.add(s2, fill_value = 0)\n\n1    10.0\n2    55.0\n2    64.0\n2    65.0\n2    74.0\n4    53.0\ndtype: float64\n\n\nChaining makes the code easy to read and understand\n\n(city_mpg\n    .add(highway_mpg)\n    .div(2))\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\n\n\n\n\n\n\n\n\nMethod\nOperator\nDescription\n\n\n\n\ns.add(s2)\ns + s2\nAdds series\n\n\ns.radd(s2)\ns2 + s\nAdds series\n\n\ns.sub(s2)\ns - s2\nSubtracts series\n\n\ns.rsub(s2)\ns2 - s\nSubtracts series\n\n\ns.mul(s2)\ns * s2\nMultiplies series\n\n\ns.multiply(s2)\ns * s2\nMultiplies series\n\n\ns.rmul(s2)\ns2 * s\nMultiplies series\n\n\ns.div(s2)\ns / s2\nDivides series\n\n\ns.truediv(s2)\ns / s2\nDivides series\n\n\ns.rdiv(s2)\ns2 / s\nDivides series\n\n\ns.rtruediv(s2)\ns2 / s\nDivides series\n\n\ns.mod(s2)\ns % s2\nModulo of series division\n\n\ns.rmod(s2)\ns2 % s\nModulo of series division\n\n\ns.floordiv(s2)\ns // s2\nFloor divide series\n\n\ns.rfloordiv(s2)\ns2 // s\nFloor divide series\n\n\ns.pow(s2)\ns ** s2\nExponential power of series\n\n\ns.rpow(s2)\ns2 ** s\nExponential power of series\n\n\ns.eq(s2)\ns2 == s\nElementwise equals of series\n\n\ns.ne(s2)\ns2 != s\nElementwise not equals of series\n\n\ns.gt(s2)\ns > s2\nElementwise greater than of series\n\n\ns.ge(s2)\ns >= s2\nElementwise greater than or equals of series\n\n\ns.lt(s2)\ns < s2\nElementwise less than of series\n\n\ns.le(s2)\ns <= s2\nElementwise less than or equals of series\n\n\nnp.invert(s)\n~s\nElementwise inversion of boolean series (no pandas method)\n\n\nnp.logical_and(s, s2)\ns & s2\nElementwise logical and of boolean series (no pandas method)\n\n\nnp.logical_or(s, s2)\ns \\| s2\nElementwise logical or of boolean series (no pandas method)\n\n\n\n\nExercises\nWith a dataset of your choice:\n\nAdd a numeric series to itself.\nAdd 10 to a numeric series.\nAdd a numeric series to itself using the .add method.\nRead the documentation for the .add method.\n\n\ncity_mpg + city_mpg\n\n0        38\n1        18\n2        46\n3        20\n4        34\n         ..\n41139    38\n41140    40\n41141    36\n41142    36\n41143    32\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg + 10\n\n0        29\n1        19\n2        33\n3        20\n4        27\n         ..\n41139    29\n41140    30\n41141    28\n41142    28\n41143    26\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg.add(city_mpg)\n\n0        38\n1        18\n2        46\n3        20\n4        34\n         ..\n41139    38\n41140    40\n41141    36\n41142    36\n41143    32\nName: city08, Length: 41144, dtype: int64\n\n\n\n# experimenting with fill_value parameter\nnan_series3 = pd.Series([2, None])\nnan_series4 = pd.Series([3, None])\n\n\nnan_series3\n\n0    2.0\n1    NaN\ndtype: float64\n\n\n\nnan_series4\n\n0    3.0\n1    NaN\ndtype: float64\n\n\n\n# two corresponding NaN values stay NaN\n# even with fill_value = 0\nnan_series3.add(nan_series4, fill_value=0)\n\n0    5.0\n1    NaN\ndtype: float64"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-7-aggregate-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-7-aggregate-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 7: Aggregate Methods",
    "text": "Chapter 7: Aggregate Methods\nAggregate methods collapse the values of a series down to a scalar.\n\n# calculate the mean\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ncity_mpg.is_unique\n\nFalse\n\n\n\npd.Series([1,2,3]).is_unique\n\nTrue\n\n\n\ncity_mpg.is_monotonic_increasing\n\nFalse\n\n\n\npd.Series([1,2,3]).is_monotonic_increasing\n\nTrue\n\n\n\n# default is median (50% quantile)\ncity_mpg.quantile()\n\n17.0\n\n\n\ncity_mpg.quantile(0.9)\n\n24.0\n\n\n\n# multiple quantiles returns a Series\ncity_mpg.quantile([0.1, 0.5, 0.9])\n\n0.1    13.0\n0.5    17.0\n0.9    24.0\nName: city08, dtype: float64\n\n\nIf you want the count of values that meet some criteria, you can use the .sum method:\n\n# count of cars with mileage greater than 20\n(city_mpg\n     .gt(20)\n     .sum()\n)\n\n10272\n\n\n\n# percentage of cars with mileage greater than 20\n(city_mpg\n     .gt(20)\n     .mul(100)\n     .mean()\n)\n\n24.965973167412017\n\n\nObserve the .mul(100).mean() calculation on a simpler Series:\n\n(pd.Series([1,2,3,4])\n    .gt(2)\n    .mul(100)\n)\n\n0      0\n1      0\n2    100\n3    100\ndtype: int64\n\n\n\n(pd.Series([1,2,3,4])\n     .gt(2)\n     .mul(100)\n     .mean()   \n)\n\n50.0\n\n\nIf you sum up a series of boolean values, the result is the count of True values. If you take the mean of a series of boolean values, the result is the fraction of values that are True.\n.agg can perform multiple operations.\n\ncity_mpg.agg('mean')\n\n18.369045304297103\n\n\n\ndef second_to_last(s):\n    return s.iloc[-2]\n\n\ncity_mpg.agg(['mean', np.var, max, second_to_last])\n\nmean               18.369045\nvar                62.503036\nmax               150.000000\nsecond_to_last     18.000000\nName: city08, dtype: float64\n\n\nAggregation strings and descriptions:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\n'all'\nReturns True if every value is truthy.\n\n\n'any'\nReturns True if any value is truthy.\n\n\n'autocorr'\nReturns Pearson correlation of series with shifted self. Can override lag as keyword argument (default is 1).\n\n\n'corr'\nReturns Pearson correlation of series with other series. Need to specify other\n\n\n'count'\nReturns count of non-missing values.\n\n\n'cov'\nReturns covariance of series with other series. Need to specify other\n\n\n'dtype'\nType of the series.\n\n\n'dtypes'\nType of the series.\n\n\n'empty'\nTrue is no values in series.\n\n\n'hasnans'\nTrue if missing values in series.\n\n\n'idxmax'\nReturns index value of maximum value.\n\n\n'idxmin'\nReturns index value of minimum value.\n\n\n'is_monotonic'\nTrue if values always increase.\n\n\n'is_monotonic_decreasing'\nTrue if values always decrease.\n\n\n'is_monotonic_increasing'\nTrue if values always increase.\n\n\n'kurt'\nReturns “excess” kurtosis (0 is normal distribution). Values greater than 0 have more outliers than normal.\n\n\n'mad'\nReturns the mean absolute deviation.\n\n\n'max'\nReturns the maximum value.\n\n\n'mean'\nReturns the mean value.\n\n\n'median'\nReturns the median value.\n\n\n'min'\nReturns the minimum value.\n\n\n'nbytes'\nReturns the number of bytes of the data.\n\n\n'ndim'\nReturn the number of dimensions (1) of the data.\n\n\n'nunique'\nReturns the count of unique values.\n\n\n'quantile'\nReturns the median value. Can override q to specify other quantile.\n\n\n'sem'\nReturns the unbiarsed standard error.\n\n\n'size'\nReturns the size of the data.\n\n\n'skew'\nReturns the unbiased skew of the data. Negative indicates tail is on the left side.\n\n\n'std'\nReturns the standard deviation of the data.\n\n\n'sum'\nReturns the sum of the series.\n\n\n\nAggregation methods and properties:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ns.agg(func=None, axis=0, *args, **kwargs)\nReturns a scalar if func is a single aggregation function. Returns a series if a list of aggregations are passed to func.\n\n\ns.all(axis=0, bool_only=None, skipna=True, level=None)\nReturns True if every value is truthy. Otherwise False.\n\n\ns.any(axis=0, bool_only=None, skipna=True, level=None)\nReturns True if at least one value is truthy. Otherwise False.\n\n\ns.autocorr(lag=1)\nReturns Pearson correlation between s and shifted s.\n\n\ns.corr(other, method='pearson')\nReturns correlation coefficient for 'pearson', 'spearman', 'kendall', or a callable.\n\n\ns.cov(other, min_periods=None)\nReturns covariance.\n\n\ns.max(axis=None, skipna=None, level=None, numeric_only=None)\nReturns maximum value.\n\n\ns.min(axis=None, skipna=None, level=None, numeric_only=None)\nReturns minimum value.\n\n\ns.mean(axis=None, skipna=None, level=None, numeric_only=None)\nReturns mean value.\n\n\ns.median(axis=None, skipna=None, level=None, numeric_only=None)\nReturns median value.\n\n\ns.prod(axis=None, skipna=None, level=None, numeric_only=None, min_count=0)\nReturns product of s values.\n\n\ns.quantile(q=0.5, interpolation='linear')\nReturns 50% quantile by default. Returns Series if q is a list.\n\n\ns.sem(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns unbiased standard error of mean.\n\n\ns.std(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns sample standard deviation.\n\n\ns.var(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)\nReturns unbiased variance.\n\n\ns.skew(axis=None, skipna=None, level=None, numeric_only=None)\nReturns unbiased skew.\n\n\ns.kurtosis(axis=None, skipna=None, level=None, numeric_only=None)\nReturns unbiased kurtosis.\n\n\ns.nunique(dropna=True)\nReturns count of unique items.\n\n\ns.count(level=None)\nReturns count of non-missing items.\n\n\ns.size\nNumber of items in series. (Property)\n\n\ns.is_unique\nTrue if all values are unique.\n\n\ns.is_monotonic\nTrue if all values are increasing.\n\n\ns.is_monotonic_increasing\nTrue if all values are increasing.\n\n\ns.is_monotonic_decreasing\nTrue if all values are decreasing.\n\n\n\n\nExercises\nWith a dataset of your choice:\n\nFind the count of non-missing values of a series.\nFind the number of entries of a series.\nFind the number of unique entries of a series.\nFind the mean value of a series.\nFind the maximum value of a series.\nUse the .agg method to find all of the above.\n\n\ncity_mpg.count()\n\n41144\n\n\n\ncity_mpg.size\n\n41144\n\n\n\ncity_mpg.nunique()\n\n105\n\n\n\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ncity_mpg.max()\n\n150\n\n\n\ncity_mpg.agg(['count', 'size', 'nunique', 'mean', 'max'])\n\ncount      41144.000000\nsize       41144.000000\nnunique      105.000000\nmean          18.369045\nmax          150.000000\nName: city08, dtype: float64"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-8-conversion-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-8-conversion-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 8: Conversion Methods",
    "text": "Chapter 8: Conversion Methods\n\n8.1 Automatic Conversion\n.convert_dtypes tries to convert a Series to a type that supports pd.NA. In the case of our city_mpg series it will change the type from int64 to Int64.\n\ncity_mpg.convert_dtypes()\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: Int64\n\n\n.astype works more explicitly. The maximumm 8-bit integer is 127, so we need 16-bit integer for city_mpg since it’s max is 150.\n\ncity_mpg.max()\n\n150\n\n\n\ncity_mpg.astype('Int16')\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: Int16\n\n\n\ncity_mpg.astype('Int8')\n\nTypeError: cannot safely cast non-equivalent int64 to int8\n\n\nIf you can use a narrower type, you can cut back on memory usage, giving you memory to process more data.\nUse NumPy to inspect limits on integer and float types:\n\nnp.iinfo('int64')\n\niinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)\n\n\n\nnp.iinfo('uint8')\n\niinfo(min=0, max=255, dtype=uint8)\n\n\n\nnp.finfo('float16')\n\nfinfo(resolution=0.001, min=-6.55040e+04, max=6.55040e+04, dtype=float16)\n\n\n\nnp.finfo('float64')\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\n\n\n8.2 Memory Usage\nUse the .nbytes property or the .memory_usage method to calculate memory usage of the Series.\nPass deep=True to .memory_usage when dealing with object types in the Series.\n\ncity_mpg.nbytes\n\n329152\n\n\n\ncity_mpg.astype('Int16').nbytes\n\n123432\n\n\nTo get the amount of memory that includes strings in the Series (like the make column), we need to use the .memory_usage method:\n\ndf.make\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            Subaru\n            ...    \n41139        Subaru\n41140        Subaru\n41141        Subaru\n41142        Subaru\n41143        Subaru\nName: make, Length: 41144, dtype: object\n\n\n\ndf.make.nbytes\n\n329152\n\n\n\ndf.make.memory_usage()\n\n329280\n\n\n\ndf.make.memory_usage(deep=True)\n\n2606395\n\n\n.memory_usage includes the index memory and can include the contribution from object types. .nbytes is just the memory that the data is using and not the ancillary parts of the Series.\nConverting to categorical will save a lot of memory for the make data:\n\n(df.make\n .astype('category')\n .memory_usage(deep=True)\n)\n\n95888\n\n\n\n\n8.3 String and Category Types\n\ncity_mpg.astype(str)\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: object\n\n\n\ncity_mpg.astype('category')\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: category\nCategories (105, int64): [6, 7, 8, 9, ..., 137, 138, 140, 150]\n\n\nWhen you convert to categorical data, pandas no longer uses Python strings for each value but optimizes it. Potentially large memory savings if you have many duplicate values.\n\n\n8.4 Ordered Categories\nTo cretae ordered categories you need to define your own CategoricalDtype:\n\nvalues = pd.Series(sorted(set(city_mpg)))\ncity_type = pd.CategoricalDtype(categories=values, ordered=True)\ncity_mpg.astype(city_type)\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: category\nCategories (105, int64): [6 < 7 < 8 < 9 ... 137 < 138 < 140 < 150]\n\n\n\n\n\n\n\n\n\nString or Type\nDescription\n\n\n\n\nstr 'str' 'string'\nConvert to Python string.\n\n\n'string'\nConvert type to pandas string (supports pd.NA).\n\n\nint 'int' 'int64'|Convert type to NumPy int64.| |'int32' 'uint32'|Convert type to 32 signed or unsigned NumPy integer (can also use 16 and 8).| |‘Int64’|Convert type to pandas Int64 (supportspd.NA). Might complain when you convert floats or strings.| |float ‘float’ ‘float64’|Convert type to NumPy float64 (can also support 32 or 16)| |‘category’|Convert type to categorical (supportspd.NA). Can also use instance ofCategoricalDtype| |dates|Don't use this for data conversion, usepd.to_datetime`.\n\n\n\n\n\n\n8.5 Converting to Other Types\nUsing Python lists will slow down your code significantly.\nYou can convert a Series into a DataFrame:\n\ncity_mpg.to_frame()\n\n\n\n\n\n  \n    \n      \n      city08\n    \n  \n  \n    \n      0\n      19\n    \n    \n      1\n      9\n    \n    \n      2\n      23\n    \n    \n      3\n      10\n    \n    \n      4\n      17\n    \n    \n      ...\n      ...\n    \n    \n      41139\n      19\n    \n    \n      41140\n      20\n    \n    \n      41141\n      18\n    \n    \n      41142\n      18\n    \n    \n      41143\n      16\n    \n  \n\n41144 rows × 1 columns\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ns.convert_dtypes(infer_objects=True, convert_string=True, convert_integer=True, convert_boolean=True, convert_floating=True)\nConvert types to appropriate pandas 1 types (that support NA). Doesn’t try to reduce size of integer or float types\n\n\ns.astype(dtype, copy=True, errors='raise')\nCast series into particular type. If errors='ignore' then return original series on error.\n\n\npd.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix', cache=True)\nConvert arg (a series) into datetime. Use format to specify strftime string.\n\n\ns.to_numpy(dtype=None, copy=False, na_value=object, **kwargs)\nConvert the series to a NumPy array.\n\n\ns.values\nConvert the series to a NumPy array.\n\n\ns.to_frame(name=None)\nReturn a dataframe representation of the series.\n\n\npd.CategoricalDtype(categories=None, ordered=False)\nCreate a type for categorical data.\n\n\n\n\n\n8.7 Exercises\nWith a dataset of your choice:\n1. Convert a numeric column to a smaller type.\n\n# currently a float64 type\ndf.barrels08\n\n0        15.695714\n1        29.964545\n2        12.207778\n3        29.964545\n4        17.347895\n           ...    \n41139    14.982273\n41140    14.330870\n41141    15.695714\n41142    15.695714\n41143    18.311667\nName: barrels08, Length: 41144, dtype: float64\n\n\n\n# convert to float16\ndf.barrels08.astype('float16')\n\n0        15.695312\n1        29.968750\n2        12.210938\n3        29.968750\n4        17.343750\n           ...    \n41139    14.984375\n41140    14.328125\n41141    15.695312\n41142    15.695312\n41143    18.312500\nName: barrels08, Length: 41144, dtype: float16\n\n\n2. Calculate the memory savings by converting to smaller numeric types.\n\ndf.barrels08.memory_usage() - df.barrels08.astype('float16').memory_usage()\n\n246864\n\n\n3. Convert a string column into a categorical type.\n\ndf.make\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            Subaru\n            ...    \n41139        Subaru\n41140        Subaru\n41141        Subaru\n41142        Subaru\n41143        Subaru\nName: make, Length: 41144, dtype: object\n\n\n\nvalues = pd.Series(sorted(set(df.make)))\nmake_type = pd.CategoricalDtype(categories=values, ordered=False)\ndf.make.astype(make_type)\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            Subaru\n            ...    \n41139        Subaru\n41140        Subaru\n41141        Subaru\n41142        Subaru\n41143        Subaru\nName: make, Length: 41144, dtype: category\nCategories (136, object): ['AM General', 'ASC Incorporated', 'Acura', 'Alfa Romeo', ..., 'Volvo', 'Wallace Environmental', 'Yugo', 'smart']\n\n\n4. Calculate the memory savings by converting to a categorical type.\n\ndf.make.memory_usage(deep=True) - df.make.astype(make_type).memory_usage(deep=True)\n\n2510507"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-9-manipulation-methods",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-9-manipulation-methods",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 9: Manipulation Methods",
    "text": "Chapter 9: Manipulation Methods\nComparing non-broadcasted .apply method with vectorized code:\n\n# non-vectorized function to check if value is greater than 20\ndef gt20(val): \n    return val > 20\n\n\n%%timeit\ncity_mpg.apply(gt20)\n\n7.08 ms ± 161 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\ncity_mpg.gt(20)\n\n174 µs ± 44 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nThe broadcasted .gt method is 40 times faster than non-broadcasted .apply function.\nShow the top 5 makes and label everything else as Other:\n\n# top 5 makes\ntop5 = df.make.value_counts().index[:5]\n\n# function to use in apply\ndef generalize_top5(val):\n    if val in top5:\n        return val\n    return 'Other'\n\n\n%%timeit\ndf.make.apply(generalize_top5)\n\n48.2 ms ± 4.64 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nA (10 times) faster and more idiomatic manner of doing this uses the .where method, which keeps values from the series it is called on where the boolean array is true. If the boolean array is false, it uses the value of the second parameter, other:\n\n%%timeit\ndf.make.where(df.make.isin(top5), other='Other')\n\n3.68 ms ± 693 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\ndf.make.where(df.make.isin(top5), other='Other')\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\nThe complement of .where is .mask—wherever the condition if False it keeps the original values; if it is True it replaces the value with the other parameter.\n\ndf.make.mask(~df.make.isin(top5), other='Other')\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\n\n%%timeit\ndf.make.mask(~df.make.isin(top5), other='Other')\n\n3.36 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe tilde ~ performs an inversion of the boolean array, switching all true values to false and vice versa.\n\n9.2 If Else with Pandas\nThere is no way to do the following: if I wanted to keep the top five makes and use Top10 for the remainder of the top ten makes, with Other for the rest.\n\nvc = df.make.value_counts()\ntop5 = vc.index[:5]\ntop10 = vc.index[:10]\ndef generalize(val):\n    if val in top5:\n        return val\n    elif val in top10:\n        return 'Top10'\n    else:\n        return 'Other'\n\n\ndf.make.apply(generalize)\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\n\n%%timeit\ndf.make.apply(generalize)\n\n76.4 ms ± 2.37 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nTo replicate in pandas, chain calls to .where:\n\n(df.make\n .where(df.make.isin(top5), 'Top10')\n .where(df.make.isin(top10), 'Other')\n)\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nName: make, Length: 41144, dtype: object\n\n\n\n%%timeit\n(df.make\n .where(df.make.isin(top5), 'Top10')\n .where(df.make.isin(top10), 'Other')\n)\n\n6.05 ms ± 391 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe pandas approach is still about 13 times faster.\nThe select function in NumPy works with pandas series. The interface takes a list of boolean arrays and a list with corresponding replacement values.\n\nnp.select([df.make.isin(top5), df.make.isin(top10)], [df.make, 'Top10'], 'Other')\n\narray(['Other', 'Other', 'Dodge', ..., 'Other', 'Other', 'Other'],\n      dtype=object)\n\n\n\n%%timeit\nnp.select([df.make.isin(top5), df.make.isin(top10)], [df.make, 'Top10'], 'Other')\n\n19.8 ms ± 2.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nYou can wrap it in a Series. I like this syntax for longer if statements than chaining .where calls because I think it is easier to understand.\n\npd.Series(np.select([df.make.isin(top5), df.make.isin(top10)], [df.make, 'Top10'], 'Other'), index=df.make.index)\n\n0        Other\n1        Other\n2        Dodge\n3        Dodge\n4        Other\n         ...  \n41139    Other\n41140    Other\n41141    Other\n41142    Other\n41143    Other\nLength: 41144, dtype: object\n\n\n\n\n9.3 Missing Data\nCount the number of missing items with .isna().sum():\n\n(df.cylinders\n .isna()\n .sum()\n)\n\n206\n\n\nLet’s index where the values are missing in the cylinders column and then show what those makes are:\n\nmissing = df.cylinders.isna()\ndf.make.loc[missing]\n\n7138     Nissan\n7139     Toyota\n8143     Toyota\n8144       Ford\n8146       Ford\n          ...  \n34563     Tesla\n34564     Tesla\n34565     Tesla\n34566     Tesla\n34567     Tesla\nName: make, Length: 206, dtype: object\n\n\n\n\n9.4 Filling in Missing Data\nIt seems like cylinders are missing for cars that are electric (they have zero cylinders).\n\ndf.cylinders[df.cylinders.isna()]\n\n7138    NaN\n7139    NaN\n8143    NaN\n8144    NaN\n8146    NaN\n         ..\n34563   NaN\n34564   NaN\n34565   NaN\n34566   NaN\n34567   NaN\nName: cylinders, Length: 206, dtype: float64\n\n\n\ndf.cylinders.fillna(0).loc[7136:7141]\n\n7136    6.0\n7137    6.0\n7138    0.0\n7139    0.0\n7140    6.0\n7141    6.0\nName: cylinders, dtype: float64\n\n\n\n\n9.5 Interpolating Data\n\ntemp = pd.Series([32, 40, None, 42, 39, 32])\ntemp\n\n0    32.0\n1    40.0\n2     NaN\n3    42.0\n4    39.0\n5    32.0\ndtype: float64\n\n\n\ntemp.interpolate()\n\n0    32.0\n1    40.0\n2    41.0\n3    42.0\n4    39.0\n5    32.0\ndtype: float64\n\n\n\n\n9.6 Clipping Data\n\ncity_mpg.loc[:446]\n\n0      19\n1       9\n2      23\n3      10\n4      17\n       ..\n442    15\n443    15\n444    15\n445    15\n446    31\nName: city08, Length: 447, dtype: int64\n\n\n\n(city_mpg\n     .loc[:446]\n     .clip(lower=city_mpg.quantile(0.05),\n           upper=city_mpg.quantile(0.95))\n)\n\n0      19\n1      11\n2      23\n3      11\n4      17\n       ..\n442    15\n443    15\n444    15\n445    15\n446    27\nName: city08, Length: 447, dtype: int64\n\n\n.clip uses .where under the hood.\n\n\n9.7 Sorting Values\nThe .sort_values method will sort the values in ascending order and also rearrange the index accordingly.\n\ncity_mpg.sort_values()\n\n7901       6\n34557      6\n37161      6\n21060      6\n35887      6\n        ... \n34563    138\n34564    140\n32599    150\n31256    150\n33423    150\nName: city08, Length: 41144, dtype: int64\n\n\nBecause of index alignment, you can still do math operations on a sorted series:\n\n(city_mpg.sort_values() + highway_mpg) / 2\n\n0        22.0\n1        11.5\n2        28.0\n3        11.0\n4        20.0\n         ... \n41139    22.5\n41140    24.0\n41141    21.0\n41142    21.0\n41143    18.5\nLength: 41144, dtype: float64\n\n\n\n\n9.8 Sorting the Index\nBelow we unsort the index by sorting the values, then essentially revert that:\n\ncity_mpg.sort_values().sort_index()\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\n\n\n9.9 Dropping Duplicates\nkeep='first' is the default value and keeps the first duplicate value found.\nkeep='last' keeps the last duplicate value found.\nkeep=False will remove any duplicated values (including the initial value).\ndrop_duplicates keep the original index.\n\ncity_mpg.drop_duplicates()\n\n0         19\n1          9\n2         23\n3         10\n4         17\n        ... \n34364    127\n34409    114\n34564    140\n34565    115\n34566    104\nName: city08, Length: 105, dtype: int64\n\n\n\n\n9.10 Ranking Data\nThe .rank method will return a series that keeps the original index but uses the ranks of values from the original series. By default, if two values are the same, their rank will be the average of the positions they take. You can specify method='min' to put equal values in the same rank and method='dense' to not skip any positions:\n\ncity_mpg.rank()\n\n0        27060.5\n1          235.5\n2        35830.0\n3          607.5\n4        19484.0\n          ...   \n41139    27060.5\n41140    29719.5\n41141    23528.0\n41142    23528.0\n41143    15479.0\nName: city08, Length: 41144, dtype: float64\n\n\n\ncity_mpg.rank(method='min')\n\n0        25555.0\n1          136.0\n2        35119.0\n3          336.0\n4        17467.0\n          ...   \n41139    25555.0\n41140    28567.0\n41141    21502.0\n41142    21502.0\n41143    13492.0\nName: city08, Length: 41144, dtype: float64\n\n\n\ncity_mpg.rank(method='dense')\n\n0        14.0\n1         4.0\n2        18.0\n3         5.0\n4        12.0\n         ... \n41139    14.0\n41140    15.0\n41141    13.0\n41142    13.0\n41143    11.0\nName: city08, Length: 41144, dtype: float64\n\n\n\n# a simpler example\npd.Series([1,1,2,3]).rank()\n\n0    1.5\n1    1.5\n2    3.0\n3    4.0\ndtype: float64\n\n\n\npd.Series([1,1,2,3]).rank(method='min')\n\n0    1.0\n1    1.0\n2    3.0\n3    4.0\ndtype: float64\n\n\n\npd.Series([1,1,2,3]).rank(method='dense')\n\n0    1.0\n1    1.0\n2    2.0\n3    3.0\ndtype: float64\n\n\n\n\n9.11 Replacing Data\nThe .replace method allows you to map values to new values.\n\ndf.make.replace('Subaru', 'SUBARU')\n\n0        Alfa Romeo\n1           Ferrari\n2             Dodge\n3             Dodge\n4            SUBARU\n            ...    \n41139        SUBARU\n41140        SUBARU\n41141        SUBARU\n41142        SUBARU\n41143        SUBARU\nName: make, Length: 41144, dtype: object\n\n\n\n# you can also use regex\ndf.make.replace(r'(Fer)ra(r.*)', value=r'\\2-other-\\1', regex=True)\n\n0          Alfa Romeo\n1        ri-other-Fer\n2               Dodge\n3               Dodge\n4              Subaru\n             ...     \n41139          Subaru\n41140          Subaru\n41141          Subaru\n41142          Subaru\n41143          Subaru\nName: make, Length: 41144, dtype: object\n\n\n\n\n9.12 Binning Data\nUsing the cut function, you can create bins of equal width:\n\npd.cut(city_mpg, 10)\n\n0        (5.856, 20.4]\n1        (5.856, 20.4]\n2         (20.4, 34.8]\n3        (5.856, 20.4]\n4        (5.856, 20.4]\n             ...      \n41139    (5.856, 20.4]\n41140    (5.856, 20.4]\n41141    (5.856, 20.4]\n41142    (5.856, 20.4]\n41143    (5.856, 20.4]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.856, 20.4] < (20.4, 34.8] < (34.8, 49.2] < (49.2, 63.6] ... (92.4, 106.8] < (106.8, 121.2] < (121.2, 135.6] < (135.6, 150.0]]\n\n\nYou can specify sizes for bin edges. In the following, 5 bins are created (so you need to provide 6 edges):\n\npd.cut(city_mpg, [0, 10, 20, 40, 70, 150])\n\n0        (10, 20]\n1         (0, 10]\n2        (20, 40]\n3         (0, 10]\n4        (10, 20]\n           ...   \n41139    (10, 20]\n41140    (10, 20]\n41141    (10, 20]\n41142    (10, 20]\n41143    (10, 20]\nName: city08, Length: 41144, dtype: category\nCategories (5, interval[int64, right]): [(0, 10] < (10, 20] < (20, 40] < (40, 70] < (70, 150]]\n\n\n\ncity_mpg\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\nNote the bins have a half-open interval. They do not have the start value but do include the end value. If the city_mpg series had values with 0 or values above 150, they would be missing after binning the series.\nIf you wanted 10 bins that had approximately the same number of entries in each bin (rather than each bin width being the same) use the qcut function:\n\npd.qcut(city_mpg, 10)\n\n0         (18.0, 20.0]\n1        (5.999, 13.0]\n2         (21.0, 24.0]\n3        (5.999, 13.0]\n4         (16.0, 17.0]\n             ...      \n41139     (18.0, 20.0]\n41140     (18.0, 20.0]\n41141     (17.0, 18.0]\n41142     (17.0, 18.0]\n41143     (15.0, 16.0]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.999, 13.0] < (13.0, 14.0] < (14.0, 15.0] < (15.0, 16.0] ... (18.0, 20.0] < (20.0, 21.0] < (21.0, 24.0] < (24.0, 150.0]]\n\n\nBoth allow you to set the labels to use instead of the categorical intervals they generate:\n\npd.qcut(city_mpg, 10, labels=list(range(1,11)))\n\n0        7\n1        1\n2        9\n3        1\n4        5\n        ..\n41139    7\n41140    7\n41141    6\n41142    6\n41143    4\nName: city08, Length: 41144, dtype: category\nCategories (10, int64): [1 < 2 < 3 < 4 ... 7 < 8 < 9 < 10]\n\n\nManipulation methods and properties:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ns.apply(func, convert_dtype=True, args=(), **kwds)\nPass in a NumPy function that works on the series, or a Python function that works on a single value. args and kwds are arguments for func. Returns a series, or dataframe if func returns a series.\n\n\ns.where(cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False)\nPass in a boolean series/dataframe, list, or callable as cond. If the value is True, keep it, otherwise use other value. If it is a function, it takes a series and should return a boolean sequence.\n\n\nnp.select(condlist, choicelist, default=0)\nPass in a list of boolean arrays for condlist. If the value is true use the corresponding value from choicelist. If multiple conditions are True, only use the first. Returns a NumPy array.\n\n\ns.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)\nPass in a scalar, dict, series or dataframe for value. If it is a scalar, use that value, otherwise use the index from the old value to the new value.\n\n\ns.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction=None, limit_area=None, downcast=None, **kwargs)\nPerform interpolation with missing values. method may be linear, time among others.\n\n\ns.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)\nReturn a new series with values clipped to lower and upper.\n\n\ns.sort_values(axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)\nReturn a series with values sorted. The kind option may be 'quicksort', 'mergesort' (stable), or 'heapsort'. na_position indicates location of NaNs and may be 'first' or 'last'.\n\n\ns.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)\nReturn a series with index sorted. The kind option may be 'quicksort', 'mergesort' (stable), or 'heapsort'. na_position indicates location of NaNs and may be 'first' or 'last'.\n\n\ns.drop_duplicates(keep='first', inplace=False)\nDrop duplicates. keep may be 'first', 'last', or False. (If False, it removes all values that were duplicated).\n\n\ns.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)\nReturn a series with numerical ranks. method allows you to specify tie handling. 'average', 'min', 'max', 'first' (usses order they appear in series), 'dense' (like 'min', but rank only increases by one after tie). na_option allows you to specify NaN handling. 'keep' (stay at NaN), 'top' (move to smallest), 'bottom' (move to largest).\n\n\ns.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')\nReturn a series with new values. to_replace can be many things. If it is a string, number or regular expression, you can replace it with a scalar value. It can also be a list of those things which requires value to be a list of the same size. Finally, it can be a dictionary mapping old values to new values.\n\n\npd.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise', ordered=True)\nBin values from x (a series). If bins is an integer, use equal-width bins. If bins is a list of numbers (defining minimum and maximum positions) use those for the edges. right defines whether the right edge is open or closed. labels allows you to specify the bin names. Out of bounds values will be missing.\n\n\npd.qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')\nBin values from x (a series) into q equal sized bins. Alternatively, can pass in a list of quantile edges. Out of bounds values will be missing.\n\n\n\n\n\n9.14 Exercises\n1. Create a series from a numeric column that has the value of 'high' if it is equal to or above the mean and 'low' if it is below the mean using .apply.\n\ncity_mpg\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\n\ncity_mpg.mean()\n\n18.369045304297103\n\n\n\ndef generalize_mean(x, mean_val):\n    if x >= mean_val:\n        return 'high'\n    return 'low'\n\ncity_mpg.apply(generalize_mean, mean_val=city_mpg.mean())\n\n0        high\n1         low\n2        high\n3         low\n4         low\n         ... \n41139    high\n41140    high\n41141     low\n41142     low\n41143     low\nName: city08, Length: 41144, dtype: object\n\n\n2. Create a series from a numeric column that has the value of 'high' if it is equal to or above the mean and 'low' if it is below the mean using np.select.\n\npd.Series(np.select([city_mpg.gt(city_mpg.mean())], ['high'], 'low'))\n\n0        high\n1         low\n2        high\n3         low\n4         low\n         ... \n41139    high\n41140    high\n41141     low\n41142     low\n41143     low\nLength: 41144, dtype: object\n\n\n3. Time the differences between the previous two solutions to see which is faster.\n\n%%timeit\ncity_mpg.apply(generalize_mean, mean_val=city_mpg.mean())\n\n22.3 ms ± 5.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\npd.Series(np.select([city_mpg.gt(city_mpg.mean())], ['high'], 'low'))\n\n4.45 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nnp.select is around 5 times as fast as .apply in this exercise.\n4. Replace the missing values of a numeric series with the median value.\n\ndf.cylinders.isna()[7136:7141]\n\n7136    False\n7137    False\n7138     True\n7139     True\n7140    False\nName: cylinders, dtype: bool\n\n\n\ndf.cylinders.fillna(df.cylinders.median())[7136:7141]\n\n7136    6.0\n7137    6.0\n7138    6.0\n7139    6.0\n7140    6.0\nName: cylinders, dtype: float64\n\n\n5.Clip the values of a numeric series to between the 10th and 90th percentiles.\n\nclip_s = pd.Series([1,2,3,4,5,6,7,8,9,10])\n(clip_s\n     .clip(lower=clip_s.quantile(0.1),\n           upper=clip_s.quantile(0.9))\n)\n\n0    1.9\n1    2.0\n2    3.0\n3    4.0\n4    5.0\n5    6.0\n6    7.0\n7    8.0\n8    9.0\n9    9.1\ndtype: float64\n\n\n6. Using a categorical column, replace any value that is not in the top 5 most frequent values with 'Other'.\n\ndf.fuelType.unique()\n\narray(['Regular', 'Premium', 'Diesel', 'CNG', 'Gasoline or natural gas',\n       'Gasoline or E85', 'Electricity', 'Gasoline or propane',\n       'Premium or E85', 'Midgrade', 'Premium Gas or Electricity',\n       'Regular Gas and Electricity', 'Premium and Electricity',\n       'Regular Gas or Electricity'], dtype=object)\n\n\n\ntop5 = df.fuelType.value_counts().index[:5]\ntop5\n\nIndex(['Regular', 'Premium', 'Gasoline or E85', 'Diesel', 'Electricity'], dtype='object', name='fuelType')\n\n\n\ndf.fuelType.where(df.fuelType.isin(top5), other='Other')\n\n0        Regular\n1        Regular\n2        Regular\n3        Regular\n4        Premium\n          ...   \n41139    Regular\n41140    Regular\n41141    Regular\n41142    Regular\n41143    Premium\nName: fuelType, Length: 41144, dtype: object\n\n\n7. Using a categorical column, replace any value that is not in the top 10 most frequent values with 'Other'.\n\ntop10 = df.fuelType.value_counts().index[:10]\ntop10\n\nIndex(['Regular', 'Premium', 'Gasoline or E85', 'Diesel', 'Electricity',\n       'Premium or E85', 'Midgrade', 'CNG', 'Premium and Electricity',\n       'Regular Gas and Electricity'],\n      dtype='object', name='fuelType')\n\n\n\ndf.fuelType.where(df.fuelType.isin(top10), other='Other')\n\n0        Regular\n1        Regular\n2        Regular\n3        Regular\n4        Premium\n          ...   \n41139    Regular\n41140    Regular\n41141    Regular\n41142    Regular\n41143    Premium\nName: fuelType, Length: 41144, dtype: object\n\n\n8. Make a function that takes a categorical series and a number (n) and returns a replace series that replaces any value that is not in the top n most frequent values with 'Other'.\n\ndef top_n_categorical(s, n):\n    top_n = s.value_counts().index[:n]\n    return s.where(s.isin(top_n), other='Other')\n\n\ns = top_n_categorical(df.fuelType, 10)\ns.unique()\n\narray(['Regular', 'Premium', 'Diesel', 'CNG', 'Other', 'Gasoline or E85',\n       'Electricity', 'Premium or E85', 'Midgrade',\n       'Regular Gas and Electricity', 'Premium and Electricity'],\n      dtype=object)\n\n\n\ns = top_n_categorical(df.fuelType, 5)\ns.unique()\n\narray(['Regular', 'Premium', 'Diesel', 'Other', 'Gasoline or E85',\n       'Electricity'], dtype=object)\n\n\n9. Using a numeric column, bin it into 10 groups that have the same width.\n\npd.cut(city_mpg, 10)\n\n0        (5.856, 20.4]\n1        (5.856, 20.4]\n2         (20.4, 34.8]\n3        (5.856, 20.4]\n4        (5.856, 20.4]\n             ...      \n41139    (5.856, 20.4]\n41140    (5.856, 20.4]\n41141    (5.856, 20.4]\n41142    (5.856, 20.4]\n41143    (5.856, 20.4]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.856, 20.4] < (20.4, 34.8] < (34.8, 49.2] < (49.2, 63.6] ... (92.4, 106.8] < (106.8, 121.2] < (121.2, 135.6] < (135.6, 150.0]]\n\n\n10. Using a numeric column, bin it into 10 groups that have equal sized bins.\n\npd.qcut(city_mpg, 10)\n\n0         (18.0, 20.0]\n1        (5.999, 13.0]\n2         (21.0, 24.0]\n3        (5.999, 13.0]\n4         (16.0, 17.0]\n             ...      \n41139     (18.0, 20.0]\n41140     (18.0, 20.0]\n41141     (17.0, 18.0]\n41142     (17.0, 18.0]\n41143     (15.0, 16.0]\nName: city08, Length: 41144, dtype: category\nCategories (10, interval[float64, right]): [(5.999, 13.0] < (13.0, 14.0] < (14.0, 15.0] < (15.0, 16.0] ... (18.0, 20.0] < (20.0, 21.0] < (21.0, 24.0] < (24.0, 150.0]]\n\n\nThis Stack Overflow response addresses the below error:\n\npd.qcut(df.cylinders, 10)\n\nValueError: Bin edges must be unique: array([ 2.,  4.,  4.,  4.,  5.,  6.,  6.,  6.,  8.,  8., 16.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg\n\n\nIf I decrease the number of bins to 5, the error is not raised:\n\npd.qcut(df.cylinders, 5)\n\n0        (1.999, 4.0]\n1         (8.0, 16.0]\n2        (1.999, 4.0]\n3          (6.0, 8.0]\n4        (1.999, 4.0]\n             ...     \n41139    (1.999, 4.0]\n41140    (1.999, 4.0]\n41141    (1.999, 4.0]\n41142    (1.999, 4.0]\n41143    (1.999, 4.0]\nName: cylinders, Length: 41144, dtype: category\nCategories (5, interval[float64, right]): [(1.999, 4.0] < (4.0, 5.0] < (5.0, 6.0] < (6.0, 8.0] < (8.0, 16.0]]"
  },
  {
    "objectID": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-10-indexing-operations",
    "href": "posts/2023-07-06-effective-pandas/2023-07-06-effective-pandas.html#chapter-10-indexing-operations",
    "title": "Effective Pandas - Notes and Exercises",
    "section": "Chapter 10: Indexing Operations",
    "text": "Chapter 10: Indexing Operations\nBoth a series and a dataframe have an index. Both types support the Python indexing operator ([]). Both have attributes .loc and .iloc that you can index against.\n\n10.1 Prepping the Data and Renaming the Index\nUse .rename method to hange the index labels. We can pass in a dictionary to map the previous index label to the new label:\n\nimport itertools\n\ndict(itertools.islice(df.make.to_dict().items(), 10))\n\n{0: 'Alfa Romeo',\n 1: 'Ferrari',\n 2: 'Dodge',\n 3: 'Dodge',\n 4: 'Subaru',\n 5: 'Subaru',\n 6: 'Subaru',\n 7: 'Toyota',\n 8: 'Toyota',\n 9: 'Toyota'}\n\n\n\ncity2 = city_mpg.rename(df.make.to_dict())\n\n\ncity2\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\n              ..\nSubaru        19\nSubaru        20\nSubaru        18\nSubaru        18\nSubaru        16\nName: city08, Length: 41144, dtype: int64\n\n\n\n# view the index\ncity2.index\n\nIndex(['Alfa Romeo', 'Ferrari', 'Dodge', 'Dodge', 'Subaru', 'Subaru', 'Subaru',\n       'Toyota', 'Toyota', 'Toyota',\n       ...\n       'Saab', 'Saturn', 'Saturn', 'Saturn', 'Saturn', 'Subaru', 'Subaru',\n       'Subaru', 'Subaru', 'Subaru'],\n      dtype='object', length=41144)\n\n\nThe .rename method also accepts a series, a scalar, a function that takes an old label and returns a new lable or a sequence. When we pass in a series and the index values are the same, the values from the series that we passed in are used as the index.\n\ncity2 = city_mpg.rename(df.make)\ncity2\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\n              ..\nSubaru        19\nSubaru        20\nSubaru        18\nSubaru        18\nSubaru        16\nName: city08, Length: 41144, dtype: int64\n\n\nIf you pass a scalar value (a single string) into .rename the index will stay the same but the .name attribute of the series will update:\n\ncity2.rename('citympg')\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\n              ..\nSubaru        19\nSubaru        20\nSubaru        18\nSubaru        18\nSubaru        16\nName: citympg, Length: 41144, dtype: int64\n\n\n\n\n10.2 Resetting the Index\n.reset_index by default will return a dataframe, moving the current index into a new column:\n\ncity2.reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      city08\n    \n  \n  \n    \n      0\n      Alfa Romeo\n      19\n    \n    \n      1\n      Ferrari\n      9\n    \n    \n      2\n      Dodge\n      23\n    \n    \n      3\n      Dodge\n      10\n    \n    \n      4\n      Subaru\n      17\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      41139\n      Subaru\n      19\n    \n    \n      41140\n      Subaru\n      20\n    \n    \n      41141\n      Subaru\n      18\n    \n    \n      41142\n      Subaru\n      18\n    \n    \n      41143\n      Subaru\n      16\n    \n  \n\n41144 rows × 2 columns\n\n\n\ndrop=True drops the current index and returns a Series.\n\ncity2.reset_index(drop=True)\n\n0        19\n1         9\n2        23\n3        10\n4        17\n         ..\n41139    19\n41140    20\n41141    18\n41142    18\n41143    16\nName: city08, Length: 41144, dtype: int64\n\n\nNote that .sort_values and .sort_index keep the same index but just rearrange the order so they do not impact operations that align on the index.\n\n\n10.3 The .loc Attribute\nThe .loc attribute deals with index labels. You can pass the following into an index operation on .loc:\n\nA scalar value of one of the index labels.\nA list of index labels.\nA slice of labels (closed interval so it includes the stop value).\nAn index.\nA boolean array (same index labels as the series, but with True and False values).\nA function that accepts a series and returns one of the above.\n\nIf there are duplicate labels in the index, and you pass in a scalar with the label of an index, it will return a series. If there is only one value for that label it will return a scalar.\n\ncity2.loc['Subaru']\n\nSubaru    17\nSubaru    21\nSubaru    22\nSubaru    19\nSubaru    20\n          ..\nSubaru    19\nSubaru    20\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, Length: 885, dtype: int64\n\n\n\ncity2.loc['Fisker']\n\n20\n\n\nIf you want to guarantee that a series is returned, pass in a list rather than passing in a scalar value:\n\ncity2.loc[['Fisker']]\n\nFisker    20\nName: city08, dtype: int64\n\n\n\ncity2.loc[['Ferrari', 'Lamborghini']]\n\nFerrari         9\nFerrari        12\nFerrari        11\nFerrari        10\nFerrari        11\n               ..\nLamborghini     6\nLamborghini     8\nLamborghini     8\nLamborghini     8\nLamborghini     8\nName: city08, Length: 357, dtype: int64\n\n\nSort the index if you are slicing with duplicate index labels:\n\ncity2.loc['Ferrari':'Lamborghini']\n\nKeyError: \"Cannot get left slice bound for non-unique label: 'Ferrari'\"\n\n\n\ncity2.sort_index().loc['Ferrari':'Lamborghini']\n\nFerrari        10\nFerrari        13\nFerrari        13\nFerrari         9\nFerrari        10\n               ..\nLamborghini    12\nLamborghini     9\nLamborghini     8\nLamborghini    13\nLamborghini     8\nName: city08, Length: 11210, dtype: int64\n\n\nSlicing with .loc follows the closed interval, includes both the start index and the final index.\nIf you have a sorted index, you can slice with strings that are not actual labels.\n\ncity2.sort_index().loc[\"F\":\"J\"]\n\nFederal Coach    15\nFederal Coach    13\nFederal Coach    13\nFederal Coach    14\nFederal Coach    13\n                 ..\nIsuzu            15\nIsuzu            15\nIsuzu            15\nIsuzu            27\nIsuzu            18\nName: city08, Length: 9040, dtype: int64\n\n\nYou can also pass an Index to .loc:\n\nidx = pd.Index(['Dodge'])\ncity2.loc[idx]\n\nDodge    23\nDodge    10\nDodge    12\nDodge    11\nDodge    11\n         ..\nDodge    18\nDodge    17\nDodge    14\nDodge    14\nDodge    11\nName: city08, Length: 2583, dtype: int64\n\n\nIf we duplicate 'Dodge' in the Index, the previous operation has twice as many values, a combinatoric explosion:\n\nidx = pd.Index(['Dodge', 'Dodge'])\ncity2.loc[idx]\n\nDodge    23\nDodge    10\nDodge    12\nDodge    11\nDodge    11\n         ..\nDodge    18\nDodge    17\nDodge    14\nDodge    14\nDodge    11\nName: city08, Length: 5166, dtype: int64\n\n\nYou can also pass a boolean array to .loc:\n\nmask = city2 > 50\nmask\n\nAlfa Romeo    False\nFerrari       False\nDodge         False\nDodge         False\nSubaru        False\n              ...  \nSubaru        False\nSubaru        False\nSubaru        False\nSubaru        False\nSubaru        False\nName: city08, Length: 41144, dtype: bool\n\n\n\ncity2.loc[mask]\n\nNissan     81\nToyota     81\nToyota     81\nFord       74\nNissan     84\n         ... \nTesla     140\nTesla     115\nTesla     104\nTesla      98\nToyota     55\nName: city08, Length: 236, dtype: int64\n\n\nYou can use a function with .loc. If I calculate the boolean array before taking into account the inflation, I get the wrong answer:\n\ncost = pd.Series([1.00, 2.25, 3.99, .99, 2.79],\n                 index=['Gum', 'Cookie', 'Melon', 'Roll', 'Carrots'])\n\ninflation = 1.10\n\nmask = cost > 3\n\n\n# wrong answer\n(cost\n     .mul(inflation)\n     .loc[mask]\n)\n\nMelon    4.389\ndtype: float64\n\n\n\n# right answer\n(cost\n     .mul(inflation)\n     .loc[lambda s_: s_ > 3]\n)\n\nMelon      4.389\nCarrots    3.069\ndtype: float64\n\n\nThere is an implicit return statement in the lambda function. You can only put an expression in it, you can have a statement. It is limited to a single line of code.\n\n\n10.4 The .iloc Attribute\nThe .iloc attribute supports indexing with the following:\n\nA scalar index position (an integer).\nA list of index positions.\nA slice of positions (half-open interval so it does not include stop value).\nA NumPy array (or Python list) of boolean values.\nA function that accepts a series and returns one of the above.\n\nBecause index positions are unique, we will always get the scalar value when indexing with .iloc at a position:\n\ncity2.iloc[0]\n\n19\n\n\n\ncity2.iloc[-1]\n\n16\n\n\nIf we want to return a series object, we can index it with a list of positions:\n\ncity2.iloc[[0]]\n\nAlfa Romeo    19\nName: city08, dtype: int64\n\n\n\ncity2.iloc[[0, 1, -1]]\n\nAlfa Romeo    19\nFerrari        9\nSubaru        16\nName: city08, dtype: int64\n\n\nWe can also use slices with .iloc (they follow the half-open interval):\n\ncity2.iloc[0:5]\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nDodge         10\nSubaru        17\nName: city08, dtype: int64\n\n\n\ncity2.iloc[-8:]\n\nSaturn    21\nSaturn    24\nSaturn    21\nSubaru    19\nSubaru    20\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, dtype: int64\n\n\nYou can use a NumPy array of booleans (or a Python list) but if you use what we call a boolean array (a pandas series with booleans), this will fail:\n\nmask = city2 > 50\ncity2.iloc[mask]\n\nValueError: iLocation based boolean indexing cannot use an indexable as a mask\n\n\n\ncity2.iloc[mask.to_numpy()]\n\nNissan     81\nToyota     81\nToyota     81\nFord       74\nNissan     84\n         ... \nTesla     140\nTesla     115\nTesla     104\nTesla      98\nToyota     55\nName: city08, Length: 236, dtype: int64\n\n\n\n\n10.5 Heads and Tails\n\ncity2.head(3)\n\nAlfa Romeo    19\nFerrari        9\nDodge         23\nName: city08, dtype: int64\n\n\n\ncity2.tail(3)\n\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, dtype: int64\n\n\n\n\n10.6 Sampling\nThe code below randomly pulls out six values:\n\ncity2.sample(6, random_state=42)\n\nVolvo         16\nMitsubishi    19\nBuick         27\nJeep          15\nLand Rover    13\nSaab          17\nName: city08, dtype: int64\n\n\n\n\n10.7 Filtering Index Values\nThe filter method will filter index labels by exact match (items), substring (like), or regex (regex).\n\n# exact match fails with duplicate index labels\ncity2.filter(items=['Ford', 'Subaru'])\n\nValueError: cannot reindex on an axis with duplicate labels\n\n\n\ncity2.filter(like='rd')\n\nFord    18\nFord    16\nFord    17\nFord    17\nFord    15\n        ..\nFord    26\nFord    19\nFord    21\nFord    18\nFord    19\nName: city08, Length: 3371, dtype: int64\n\n\n\ncity2.filter(regex='(Ford)|(Subaru)')\n\nSubaru    17\nSubaru    21\nSubaru    22\nFord      18\nFord      16\n          ..\nSubaru    19\nSubaru    20\nSubaru    18\nSubaru    18\nSubaru    16\nName: city08, Length: 4256, dtype: int64"
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "",
    "text": "In this blog post I’ll modify the neural net training loop example in Jeremy Howard’s Lesson 5 notebook Linear model and neural net from scratch to plot training loss, validation loss, and accuracy across a number of training runs. I’ll run 100 trainings for the neural net, record the losses and accuracy, and then plot them to see how they vary by epoch and by training loop.\nI am also inspired by (and learned from) this forum post by a fastai community member (sign-in required) where they plotted losses, gradients, parameters and accuracy for various training runs that included or excluded params.grad_zero() and L2 regularization. They found that for a simple linear model, zeroing the gradients leads to more stable training, smaller coefficients and higher accuracy than letting gradients accumulate each epoch."
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plan-of-attack",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plan-of-attack",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nI want to record values at the end of each epoch, separated for each training run. I’ll create a recorder DataFrame where I store this data. Here’s pseudocode for how the recording will take place, referencing functions defined in Jeremy’s notebook and logic used by in the fastai forum post to collect losses and accuracy:\n# code to clean data\n...\n\n# code to create training and validation xs and ys\n...\n\n\n# new function to run multiple trainings\ndef training_run(runs=100):\n  # initialize recorder object\n  recorder = pd.DataFrame(columns=[\"run\", \"epoch\", \"trn_loss\", \"val_loss\", \"acc\"])\n  for run in range(runs):\n    # get lists of losses and accuracy\n    tl, vl, a = train_model(...)\n    # create list of run and epoch values\n    r = [run] * len(tl)\n    e = [i for i in range(len(tl))]\n    # append new data to recorder DataFrame\n    row = pd.DataFrame(data={\"run\": r, \"epoch\": e, \"trn_loss\": tl, \"val_loss\": vl, \"acc\": a})\n    recorder = pd.concat(recorder, row)\n  return recorder\n    \n\n# modify existing function\ndef train_model(...):\n  tl, vl, a = [], [], []\n  for i in range(epochs):\n    trn_loss, val_loss, acc = one_epoch(...)\n    tl.append(trn_loss)\n    vl.append(val_loss)\n    a.append(acc)\n  return tl, vl, a\n\n# modify existing function\ndef one_epoch(...):\n  trn_loss = calc_loss(...)\n  val_loss = calc_loss(...)\n  trn_loss.backward()\n  with torch.no_grad(): update_coeffs(...)\n  acc = calc_acc(...)\n  return trn_loss, val_loss, acc\n\n# use existing function to calculate predictions\ndef calc_preds(...): ...\n\n# use existing function to calculate loss\ndef calc_loss(...): ...\n\n# use existing function to step the weights\ndef update_coeffs(...): ...\n\n# use existing function to calculate accuracy\ndef calc_acc(...): ...\n\n# use existing function to initiate weights\ndef init_coeffs(...): ...\nWith the pseudocode sketched out, I’ll start building out each function next."
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#building-the-functions",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#building-the-functions",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "Building the Functions",
    "text": "Building the Functions\n\nimport torch, numpy as np, pandas as pd, torch.nn.functional as F\nfrom fastai.data.transforms import RandomSplitter\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)\n\n\nInitialize Coefficients\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\n\nCalculate Predictions\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\n\n\nCalculate Loss\n\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\n\n\nUpdate the Coefficients\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\n\nCalculate Accuracy\n\ndef calc_acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\n\n\n\nTrain One Epoch\n\ndef one_epoch(coeffs, lr):\n  trn_loss = calc_loss(coeffs, trn_indep, trn_dep)\n  trn_loss.backward()\n  with torch.no_grad():\n    val_loss = calc_loss(coeffs, val_indep, val_dep)\n    update_coeffs(coeffs, lr)\n    acc = calc_acc(coeffs)\n  return trn_loss, val_loss, acc\n\n\n\nTrain a Model\n\ndef train_model(epochs, lr, n_coeff, is_seed=True):\n  if is_seed: torch.manual_seed(442)\n  tl, vl, a = [], [], []\n  coeffs = init_coeffs(n_coeff)\n  for i in range(epochs):\n    trn_loss, val_loss, acc = one_epoch(coeffs, lr)\n    tl.append(trn_loss.item())\n    vl.append(val_loss.item())\n    a.append(acc.item())\n  return tl, vl, a\n\n\n\nTrain Multiple Models\n\ndef train_multiple_models(runs=100, epochs=30, lr=4, n_coeff=12, is_seed=False):\n  # initialize recorder object\n  recorder = pd.DataFrame(columns=[\"run\", \"epoch\", \"trn_loss\", \"val_loss\", \"acc\"])\n  for run in range(runs):\n    # get lists of losses and accuracy\n    tl, vl, a = train_model(epochs, lr, n_coeff, is_seed)\n    # create list of run and epoch values\n    r = [run] * epochs\n    e = [i for i in range(epochs)]\n    # append new data to recorder DataFrame\n    row = pd.DataFrame(data={\"run\": r, \"epoch\": e, \"trn_loss\": tl, \"val_loss\": vl, \"acc\": a})\n    recorder = pd.concat([recorder, row])\n  return recorder"
  },
  {
    "objectID": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plotting-training-results",
    "href": "posts/2023-08-15-training-plots/2023_08_15_training_plots.html#plotting-training-results",
    "title": "Plotting Losses and Accuracy for 100 Deep Neural Net Training Runs",
    "section": "Plotting Training Results",
    "text": "Plotting Training Results\nIn this section, I’ll import the data, clean it, create training/validation splits, test out my above functions for a single model training loop, run my experiment for 100 training runs, and plot the results.\n\nLoad the Data\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 1.97MB/s]\n\n\n\n\n\n\n\n\n\n\nClean the Data\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n# replace NAs with the mode of the column\nmodes = df.mode().iloc[0]\n\n\ndf.fillna(modes, inplace=True)\n\n\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\n\n# take log(Fare + 1) to make the distribution more reasonable\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\n# convert categoricals to dummy variables\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\n# list out the new dummy variables\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n\n\nfrom torch import tensor\n\n# create tensor of dependent variable data\nt_dep = tensor(df.Survived)\n\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\n# create tensor of independent variable data\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep[:2]\n\ntensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000]])\n\n\n\n# normalize the independent variables\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\nt_indep[:2]\n\ntensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000]])\n\n\n\n# create indexes for training/validation splits\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\n\n# split data into training and validation sets\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\n\n# turn dependent variable into column vector\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\n\n\nTrain a Single Model\nFirst, I’ll train a single model to make sure that I’m getting a similar accuracy as Jeremy’s notebook example:\n\nres = train_model(epochs=30, lr=4, n_coeff=12)\n\n\n# accuracy is the second list in our results\n# the final accuracy should be close to 0.8258\nres[2][-1]\n\n0.8258426785469055\n\n\nGreat! My model’s accuracy matches that of the example notebook. Next, I’ll plot the training loss, validation loss and accuracy of the model across 30 epochs:\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\nxs = [i for i in range(30)]\n\nplt.plot(xs, res[0], c='green');\nplt.plot(xs, res[1], c='red');\nplt.plot(xs, res[2], c='blue');\n\nplt.xlabel(\"Epochs\");\nplt.ylabel(\"Loss\\nAccuracy\");\n\ngreen_patch = mpatches.Patch(color='green', label='Training Loss')\nred_patch = mpatches.Patch(color='red', label='Validation Loss')\nblue_patch = mpatches.Patch(color='blue', label='Accuracy')\n\n\nplt.legend(handles=[green_patch, red_patch, blue_patch]);\n\n\n\n\nExcellent! With that confirmed, I can run my trial of 100 trainings, and then plot the results:\n\n\nTraining Multiple Models\n\nrecorder = train_multiple_models()\n\n\nrecorder.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n    \n  \n  \n    \n      0\n      0\n      0\n      0.552340\n      0.540915\n      0.595506\n    \n    \n      1\n      0\n      1\n      0.488773\n      0.491162\n      0.595506\n    \n    \n      2\n      0\n      2\n      0.474533\n      0.479952\n      0.595506\n    \n    \n      3\n      0\n      3\n      0.461460\n      0.469660\n      0.595506\n    \n    \n      4\n      0\n      4\n      0.450005\n      0.460642\n      0.595506\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nrecorder.tail()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n    \n  \n  \n    \n      25\n      99\n      25\n      0.390775\n      0.414015\n      0.595506\n    \n    \n      26\n      99\n      26\n      0.390258\n      0.413608\n      0.595506\n    \n    \n      27\n      99\n      27\n      0.389781\n      0.413232\n      0.595506\n    \n    \n      28\n      99\n      28\n      0.389341\n      0.412886\n      0.595506\n    \n    \n      29\n      99\n      29\n      0.388933\n      0.412565\n      0.595506\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nrecorder.max()\n\nrun               99\nepoch             29\ntrn_loss    0.623253\nval_loss    0.604715\nacc         0.831461\ndtype: object\n\n\n\nPlot: Training Loss\n\n(recorder\n .pivot_table(values='trn_loss', index='epoch', columns='run')\n .plot(color='green', alpha=0.3, legend=False, title='Training Loss'));\n\n\n\n\n\n\nPlot: Validation Loss\n\n(recorder\n .pivot_table(values='val_loss', index='epoch', columns='run')\n .plot(color='red', alpha=0.3, legend=False, title='Validation Loss'));\n\n\n\n\n\n\nPlot: Accuracy\n\n(recorder\n .pivot_table(values='acc', index='epoch', columns='run')\n .plot(color='blue', alpha=0.3, legend=False, title='Accuracy'));\n\n\n\n\n\n\n\nFinal Thoughts\nThis exercise was fascinating, both in terms of building the code to record losses and accuracy for each epoch, as well as observing the final results of 100 training runs.\nThe main observation that stands out: for all three values (training loss, validation loss and accuracy) there were training runs where the values did not improve at all between the first and last epoch. In the case of training and validation loss, it seems like there were numerous runs where the loss was stuck at around 0.4. There were many trainings where the accuracy was stuck at around 0.6.\nOnly for a handful of training runs did the accuracy cross 0.8.\nIn a significant number of runs (as seen by the darkness of the line color on the plot) the training and validation loss gradually decreased during training.\nAfter running this experiment I am pretty surprised. I knew that training neural networks involved some variability, but it’s almost shocking to see how you can get wildly different results for training the same model. Just by happenchance, I can get a model that seemingly does not work (accuracy stuck throughout) and the same model that achieves a better accuracy than the baseline in Jeremy’s notebook. All in all, I’m grateful that I did this exercise because it gave me some perspective on how volatile neural nets can be."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html",
    "title": "R Shiny Census App",
    "section": "",
    "text": "In this blog post, I’ll walk through my development process for a U.S. Census data visualization web app I created using the Shiny package in R.\nYou can access the app at vbakshi.shinyapps.io/census-app."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#table-of-contents",
    "title": "R Shiny Census App",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nBackstory\nCodebase\n\napp.R\n\nWhat’s in my ui?\n\nDropdowns\nTables\nPlot\nDownload buttons\n\nWhat does my server do?\n\nGet Data\nRender Outputs\nPrepare Dynamic Text\nHandle Downloads\n\n\nprep_db.R\n\nDatabase Tables\n\nb20005\nb20005_vars\ncodes\n\nCreate Tables\nWrite to Tables\nLoad the Data\n\nget_b20005_ruca_aggregate_earnings.R\n\nGet Variable Names\nDerive RUCA Level Estimates and MOE\n\ncalculate_median.R\n\nCreate Frequency Distribution\nCalculate Weighted Total\nApproximate Standard Error\nCalculate Median Estimate Bounds\nReshape the Data\n\nformat_query_result.R\n\nExtract data.frame Objects from List\nReshape data.frame Objects\nAdd Descriptive Labels\n\nget_b20005_labels.R\n\nGet Earnings Population Estimate Labels\nGet All Labels\n\nget_b20005_tract_earnings.R\n\nGet Variable Names\nJoin Tables\n\nget_b20005_states.R\nget_design_factor.R\nmake_plot.R"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#backstory",
    "title": "R Shiny Census App",
    "section": "Backstory",
    "text": "Backstory\nI started this project by reading the handbook Understanding and Using American Community Survey Data: What State and Local Government Users Need to Know published by the U.S. Census Bureau. I recreated the handbook’s first case study in R, in which they make comparisons across geographic areas, create custom geographic areas from census tracts and calculate margins of error for derived estimates for Minnesota Census Tract 5-year earnings estimates.\nDuring the process of recreating the derived median earnings estimate calculations, I was unable to recreate a key value from the handbook (the Standard Error for the 50% proportion, calculated to be 0.599) because I was unable to deduce the values used in the following formula referenced from page 17 of the PUMS Accuracy of the Data documentation:\n\n\n\nStandard Error equals Design Factor times square root of the product of 95 over 5B and 50 squared\n\n\nThe documentation defines B as the base, which is the calculated weighted total. I chose the value of 1.3 for the design factor DF since it corresponds to STATE = Minnesota, CHARTYP = Population, CHARACTERISTIC = Person Earnings/Income in the Design Factors CSV published by the Census Bureau.\nI called the Census Bureau Customer Help Center for assistance and was transferred to a member of the ACS Data User Support team with whom I discussed my woes. He was unable to confirm the values of the design factor DF or B, and was unable to pull up the contact information for the statistical methodology team, so I emailed him my questions. After a few email exchanges, the statistical methodology team provided the following:\n\nDF = 1.3\nB = the total population estimate for which the median is being calculated, which is 82488 for the case study calculation (Minnesota Rural Male Full Time Workers)\nThe term 95/5 is associated with the finite population correction factor (100 - f) divided by the sample fraction (f), where f = 5% (later on I note in the documentation that this 95/5 term is based on a 68% confidence interval). The data used in the handbook case study is from 5-year estimates. 1-year estimates sample 2.5% of the population, so the 5-year estimates represent a 5 * 2.5 = 12.5% sample. Instead of 95/5, the ratio becomes (100 - 12.5)/12.5 = 87.5/12.5\n\nThe updated formula is then:\n\n\n\nStandard Error equals Design Factor times square root of the product of 87.5 over 12.5B and 50 squared\n\n\nI was able to calculate the median earnings estimate (and associated standard error and margin of error) within a few percent of the values given in the handbook. This provided me with confirmation that I was ready to expand my code to calculate median earnings estimates for other subgroups."
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#codebase",
    "title": "R Shiny Census App",
    "section": "Codebase",
    "text": "Codebase\nI built this app using the R package Shiny which handles both the UI and the server. I store the data in a sqlite database and access it with queries written using the RSQLite package which uses the DBI API. The following sections break down the R scripts based on functionality. Click on the script name to navigate to that section.\n\napp.R\n\nUI and server functions to handle people inputs and plot/table/text outputs\n\nprep_db.R\n\nImport, clean, combine and then load data into the census_app_db.sqlite database\n\nget_b20005_ruca_aggregate_earnings.R\n\nQueries the database for earnings and associated margins of error for RUCA levels derived from Census Tracts\n\ncalculate_median.R\n\nDerives estimate, standard of error and margin of error of median earnings for RUCA levels\n\nformat_query_result.R\n\nFormats calculate_median query results\n\nget_b20005_labels.R\n\nQueries the database for descriptive labels of B20005 table variables\n\nget_b20005_tract_earnings.R\n\nQueries the database for Census Tract-level earnings and associated margins of error\n\nget_b20005_states.R\n\nQueries the SQLite database for a list of U.S. states\n\nget_design_factor.R\n\nQueries database for the design factor used for the median earnings estimation calculation\n\nmake_plot.R\n\nCreates a bar plot object"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#app.r",
    "title": "R Shiny Census App",
    "section": "app.R",
    "text": "app.R\nA shiny app has three fundamental components:\nui <- (...)\nserver <- (...)\nshinyApp(ui, server,...)\nThe ui object holds all UI layout, input and output objects which define the front-end of your app. The server object holds all rendering functions which are assigned to outputs that appear on the UI. The shinyApp function takes a ui and server object (along with other arguments) and creates a shiny app object which can be run in a browser by passing it to the runApp function. Person inputs (such as selections in a dropdown) are assigned to a global input object.\n\nWhat’s in my ui?\nAll of my UI objects are wrapped within a fluidPage call which returns a page layout which “consists of rows which in turn include columns” (from the docs).\nMy app’s UI has four sections:\n\nDropdowns to select state, sex and work status for which the person using the app wants ACS 5-year earnings estimates\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\n\nA table with the estimate, standard error and margin of error for median earnings\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\n\nA bar plot of population estimates for earnings levels for the selected state, sex, work status and RUCA (Rural-Urban Commuting Areas) level\n\n\n\nA table with population estimates for earnings levels for each RUCA level for the selected state, sex and work status\n\nEach section has a download button so that people can get the CSV files or plot image for their own analysis and reporting. Each section is separated with markdown('---') which renders an HTML horizontal rule (<hr>).\n\nDropdowns\nDropdowns (the HTML <select> element) are a type of UI Input. I define each with an inputId which is a character object for reference on the server-side, a label character object which is rendered above the dropdown, and a list object which defines the dropdown options.\nselectInput(\n  inputId = \"...\",\n  label = \"...\",\n  choices = list(...)\n)\nIn some cases, I want the person to see a character object in the dropdown that is more human-readable (e.g. \"Large Town\") but use a corresponding input value in the server which is more computer-readable (e.g. \"Large_Town). To achieve this, I use a named character vector where the names are displayed in the dropdown, and the assigned values are assigned to the global input:\nselectInput(\n     inputId = \"ruca_level\",\n     label = \"Select RUCA Level\",\n     choices = list(\n       \"RUCA LEVEL\" = c(\n       \"Urban\" = \"Urban\", \n       \"Large Town\" = \"Large_Town\", \n       \"Small Town\" = \"Small_Town\", \n       \"Rural\" = \"Rural\"))\n     )\nIn this case, if the person selects \"Large Town\" the value assigned to input$ruca_level is \"Large_Town\".\n\n\nTables\nTables (the HTML <table> element) are a type of UI Output. I define each with an outputId for reference in the server.\ntableOutput(outputId = \"...\")\n\n\nPlot\nSimilarly, a plot (which is rendered as an HTML <img> element) is a type of UI Output. I define each with an outputId.\nplotOutput(outputId = \"...\")\n\n\nDownload Buttons\nThe download button (an HTML <a> element) is also a type of UI Output. I define each with an outputId and label (which is displayed as the HTML textContent attribute of the <a> element).\ndownloadButton(\n  outputId = \"...\",\n  label = \"...\"\n)\n\n\n\nWhat does my server do?\nThe server function has three parameters: input, output and session. The input object is a ReactiveValues object which stores all UI Input values, which are accessed with input$inputId. The output object similarly holds UI Output values at output$outputId. I do not use the session object in my app (yet).\nMy app’s server has four sections:\n\nGet data from the SQLite database\nRender table and plot outputs\nPrepare dynamic text (for filenames and the plot title)\nHandle data.frame and plot downloads\n\n\nGet Data\nThere are three high-level functions which call query/format/calculation functions to return the data in the format necessary to produce table, text, download and plot outputs:\n\nThe earnings_data function passes the person-selected dropdown options input$sex, input$work_status and input$state to the get_b20005_ruca_aggregate_earnings function to get a query result from the SQLite database. That function call is passed to format_earnings, which in turn is passed to the reactive function to make it a reactive expression. Only reactive expressions (and reactive endpoints in the output object) are allowed to access the input object which is a reactive source. You can read more about Shiny’s “reactive programming model” in this excellent article.\n\nearnings_data <- reactive(\n  format_earnings(\n    get_b20005_ruca_aggregate_earnings(\n      input$sex, \n      input$work_status, \n      input$state)))\n\nThe design_factor function passes the input$state selection to the get_design_factor function which in turn is passed to the reactive function.\n\ndesign_factor <- reactive(get_design_factor(input$state))\n\nThe median_data function passes the return values from earnings_data() and design_factor() to the calculate_median function which in turn is passed to the reactive function.\n\nmedian_data <- reactive(calculate_median(earnings_data(), design_factor()))\n\n\nRender Outputs\nI have two reactive endpoints for table outputs, and one endpoint for a plot. The table outputs use renderTable (with row names displayed) with the data.frame coming from median_data() and earnings_data(). The plot output uses renderPlot, and a helper function make_plot to create a bar plot of earnings_data() for a person-selected input$ruca_level with a title created with the helper function earnings_plot_title().\noutput$median_data <- renderTable(\n  expr = median_data(), \n  rownames = TRUE)\n  \noutput$earnings_data <- renderTable(\n  expr = earnings_data(), \n  rownames = TRUE)\n    \noutput$earnings_histogram <- renderPlot(\n  expr = make_plot(\n    data=earnings_data(), \n    ruca_level=input$ruca_level, \n    plot_title=earnings_plot_title()))\n\n\nPrepare Dynamic Text\nI created four functions that generate filenames for the downloadHandler call when the corresponding downloadButton gets clicked, one function that generates the title used to generate the bar plot, and one function which takes computer-readable character objects (e.g. \"Large_Town\") and maps it to and returns a more human-readable character object (e.g. \"Large Town\"). I chose to keep filenames more computer-readable (to avoid spaces) and the plot title more human-readable.\nget_pretty_text <- function(raw_text){\n  text_map <- c(\"M\" = \"Male\", \n  \"F\" = \"Female\",\n  \"FT\" = \"Full Time\",\n  \"OTHER\" = \"Other\",\n  \"Urban\" = \"Urban\",\n  \"Large_Town\" = \"Large Town\",\n  \"Small_Town\" = \"Small Town\",\n  \"Rural\" = \"Rural\")\n  return(text_map[raw_text])\n  }\n \nearnings_plot_title <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    get_pretty_text(input$work_status),\n    input$ruca_level,\n    \"Workers\",\n    sep=\" \"))\n  }\n\nb20005_filename <- function(){\n    return(paste(\n      input$state,\n      get_pretty_text(input$sex),\n      input$work_status,\n      \"earnings.csv\",\n      sep=\"_\"\n    ))\n  }\n  \nmedian_summary_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex), \n    input$work_status, \n    'estimated_median_earnings_summary.csv',  \n    sep=\"_\")\n  }\n  \nruca_earnings_filename <- function() {\n  paste(\n    input$state,  \n    get_pretty_text(input$sex),  \n    input$work_status, \n    'estimated_median_earnings_by_ruca_level.csv',  \n    sep=\"_\")\n  }\n  \nearnings_plot_filename <- function(){\n  return(paste(\n    input$state,\n    get_pretty_text(input$sex),\n    input$work_status,\n    input$ruca_level,\n    \"Workers.png\",\n    sep=\"_\"))\n  }\n\n\nHandle downloads\nI have five download buttons in my app: two which trigger a download of a zip file with two CSVs, two that downloads a single CSV, and one that downloads a single PNG. The downloadHandler function takes a filename and a content function to write data to a file.\nIn order to create a zip file, I use the zip base package function and pass it a vector with two filepaths (to which data is written using the base package’s write.csv function) and a filename. I also specify the contentType as \"application/zip\". In the zip file, one of the CSVs contains a query result from the b20005 SQLite database table with earnings data, and the other file, \"b20005_variables.csv\" contains B20005 table variable names and descriptions. In order to avoid the files being written locally before download, I create a temporary directory with tempdir and prepend it to the filename to create the filepath.\nFor the bar plot image download, I use the ggplot2 package’s ggsave function, which takes a filename, a plot object (returned from the make_plot helper function) and the character object \"png\" (for the device parameter).\noutput$download_selected_b20005_data <- downloadHandler(\n    filename = \"b20005_data.zip\",\n    content = function(fname) {\n      # Create a temporary directory to prevent local storage of new files\n      temp_dir <- tempdir()\n      \n      # Create two filepath character objects and store them in a list\n      # which will later on be passed to the `zip` function\n      path1 <- paste(temp_dir, '/', b20005_filename(), sep=\"\")\n      path2 <- paste(temp_dir, \"/b20005_variables.csv\", sep=\"\")\n      fs <- c(path1, path2)\n      \n      # Create a CSV with person-selection input values and do not add a column\n      # with row names\n      write.csv(\n        get_b20005_earnings(input$state, input$sex, input$work_status), \n        path1,\n        row.names = FALSE)\n      \n      # Create a CSV for table B20005 variable names and labels for reference\n      write.csv(\n        get_b20005_ALL_labels(),\n        path2,\n        row.names = FALSE)\n      \n      # Zip together the files and add flags to maximize compression\n      zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_all_b20005_data <- downloadHandler(\n  filename = \"ALL_B20005_data.zip\",\n  content = function(fname){\n    path1 <- \"ALL_B20005_data.csv\"\n    path2 <- \"b20005_variables.csv\"\n    fs <- c(path1, path2)\n    \n    write.csv(\n      get_b20005_earnings('ALL', 'ALL', 'ALL'),\n      path1,\n      row.names = FALSE)\n    \n    write.csv(\n      get_b20005_ALL_labels(),\n      path2,\n      row.names = FALSE)\n    \n    zip(zipfile = fname, files=fs, flags = \"-r9Xj\")\n    },\n    contentType = \"application/zip\"\n  )\n  \noutput$download_median_summary <- downloadHandler(\n  filename = median_summary_filename(),\n  content = function(file) {\n    write.csv(median_data(), file)\n    }\n  )\n  \noutput$download_earnings_plot <- downloadHandler(\n  filename = earnings_plot_filename(),\n  content = function(file) {\n    ggsave(\n      file, \n      plot = make_plot(\n        data=earnings_data(), \n        ruca_level=input$ruca_level, \n        plot_title=earnings_plot_title()), \n        device = \"png\")\n      }\n  )\n  \noutput$download_ruca_earnings <- downloadHandler(\n  filename = ruca_earnings_filename(),\n  content = function(file) {\n    write.csv(earnings_data(), file)\n  }\n  )"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#prep_db.r",
    "title": "R Shiny Census App",
    "section": "prep_db.R",
    "text": "prep_db.R\nThis script is meant to be run locally, and is not deployed, as doing so would create a long delay to load the app.\n\nDatabase Tables\nThe database diagram is shown below (created using dbdiagram.io):\n\n\n\nDatabase diagram showing the database table schemas and their relationships\n\n\nI have five tables in my database:\n\n\nb20005\nHolds the data from the ACS 2015-2019 5-year detailed table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months). This includes earnings estimates and margins of errors for Male and Female, Full Time and Other workers, for earning ranges (No earnings, $1 - $2499, $2500 - $4999, …, $100000 or more). The following table summarizes the groupings of the (non-zero earnings) variables relevant to this app:\n\n\n\n\nVariable\nDemographic\n\n\n\n\nB20005_003 to B20005_025\nMale Full Time Workers\n\n\nB20005_029 to B20005_048\nMale Other Workers\n\n\nB20005_050 to B20005_072\nFemale Full Time Workers\n\n\nB20005_076 to B20005_095\nFemale Other Workers\n\n\n\n\n\n\nb20005_vars\nHas the name (e.g. B20005_003E) and label (e.g. “Estimate!!Total!!Male!!Worked full-time, year-round in the past 12 months”) for all B20005 variables. Variable names ending with an E are estimates, and those ending with M are margins of error. - ruca contains RUCA (Rural-Urban Commuting Area) codes published by the U.S. Department of Agriculture Economic Research Service which classify U.S. census tracts using measures of population density. The following table shows the code ranges relevant to this app:\n\n\n\n\nRUCA Code\nRUCA Level\n\n\n\n\n1-3\nUrban\n\n\n4-6\nLarge Town\n\n\n7-9\nSmall Town\n\n\n10\nRural\n\n\n99\nZero Population\n\n\n\n\n\n\ncodes\nolds state FIPS (Federal Information Processing Standards) codes and RUCA levels - design_factors contains Design Factors for different characteristics (e.g. Person Earnings/Income) which are used to determine “the standard error of total and percentage sample estimates”, and “reflect the effects of the actual sample design and estimation procedures used for the ACS.” (2015-2019 PUMS 5-Year Accuracy of the Data).\nIn prep_db.R, I use the DBI package, censusapi and base R functions to perform the following protocol for each table:\n\n\nLoad the Data\n\nFor tables b20005 and b20005_vars, I use the censusapi::getCensus and censusapi::listCensusMetadata repsectively to get the data\n\n# TABLE b20005_vars ------------------------------\nb20005_vars <- listCensusMetadata(\n  name = 'acs/acs5',\n  vintage = 2015,\n  type = 'variables',\n  group = 'B20005')\n  \n # TABLE b20005 ----------------------------------\n b20005 <- getCensus(\n  name = 'acs/acs5',\n  region = \"tract:*\",\n  regionin = regionin_value,\n  vintage = 2015,\n  vars = b20005_vars$name,\n  key=\"...\"\n  )\n\nFor tables codes, ruca, and design_factors I load the data from CSVs that I either obtained (in the case of the Design Factors) or created (in the case of the codes and RUCA levels)\n\n # TABLE codes ----------------------------------\nstate_codes <- read.csv(\n  \"data/state_codes.csv\",\n  colClasses = c(\n    \"character\", \n    \"character\", \n    \"character\")\n)\n\nruca_levels <- read.csv(\n  \"data/ruca_levels.csv\",\n  colClasses = c(\n    \"character\",\n    \"character\",\n    \"character\")\n)\n\n\nCreate Tables\nOnce the data is ready, I use DBI::dbExecute to run a SQLite command to create each table. The relationships shown in the image above dictate which fields create the primary key (in some cases, a compound primary key) as listed below:\n\n\n\n\n\n\n\n\nTable\nPrimary Key\nNotes\n\n\n\n\nb20005\n(state, county, tract))\nForeign key for table ruca\n\n\nb20005_vars\nname\ne.g. B20005_001E\n\n\nruca\nTRACTFIPS\nForeign key for table b20005\n\n\ncodes\n(CODE, DESCRIPTION)\ne.g. (1, \"Urban\")\n\n\ndesign_factors\n(ST, CHARACTERISTIC)\ne.g. (\"27\", \"Person Earnings/Income\")\n\n\n\n\n\nWrite to Tables\nOnce the table has been created in the database, I write the data.frame to the corresponding table with the following call:\ndbWriteTable(census_app_db, \"<table name>\", <data.frame>, append = TRUE"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_ruca_aggregate_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_ruca_aggregate_earnings.R",
    "text": "get_b20005_ruca_aggregate_earnings.R\nThe function inside this script (with the same name), receives inputs from the server, sends queries to the database and returns the results. This process involves two steps:\n\nGet Variable Names\nThe person using the app selects Sex (M or F), Work Status (Full Time or Other) and State (50 states + D.C. + Puerto Rico) for which they want to view and analyze earnings data. As shown above, different variables in table b20005 correspond to different sexes and work statuses, and each tract for which there is all that earnings data resides in a given state.\nI first query b20005_vars to get the relevent variables names which will be used in the query to b20005, as shown below. names that end with “M” (queried with the wilcard '%M') are for margins of error and those that end with “E” (wildcard '%E') are for estimates.\nvars <- dbGetQuery(\n    census_app_db, \n    \"SELECT name FROM b20005_vars \n    WHERE label LIKE $label_wildcard \n    AND name LIKE '%M'\",\n    params=list(label_wildcard=label_wildcard))\nThe b20005_vars.label column holds long string labels (which follow a consistent pattern, which is captured by the $label_wildcard) that describe the variable’s contents. Here are a couple of examples: \n\n\n\n\n\n\n\nb20005_vars.name\nb20005_vars.label\n\n\n\n\nB20005_053E\n\"Estimate!!Total!!Female!!Worked full-time, year-round in the past 12 months!!With earnings\")\n\n\nB20005_076M\n\"Margin of Error!!Total!!Female!!Other!!With earnings!!$1 to $2,499 or loss\"\n\n\n\n\nSince the label string contains the sex and work status, I assign a label_wildcard based on the person inputs from the sex and work status UI dropdowns.\n# Prepare wildcard for query parameter `label_wildcard`\n  if (sex == 'M') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Male!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Male!!Other%\" }\n  }\n  \n  if (sex == 'F') {\n    if (work_status == 'FT') { label_wildcard <- \"%!!Female!!Worked%\" }\n    if (work_status == 'OTHER') { label_wildcard <- \"%!!Female!!Other%\" }\n  }\n\n\nDerive RUCA Level Estimates and MOE\nOnce the variables are returned, the actual values are queried from b20005, grouped by RUCA level. The ACS handbook Understanding and Using American Community Survey Data: What All Data Users Need to Know shows how to calculate that margin of error for derived estimates. In our case, the margin of error for a RUCA level such as “Urban” for a given state is derived from the margin of error of individual Census Tracts using the formula below:\n\n\n\nThe MOE for a sum of estimates is the square root of the sum of MOEs squared\n\n\nTranslating this to a SQLite query:\n# Construct query string to square root of the sum of margins of error squared grouped by ruca level\nquery_string <- paste0(\n    \"SQRT(SUM(POWER(b20005.\", vars$name, \", 2))) AS \", vars$name, collapse=\",\")\nWhere vars$name is a list of variable names, and the collapse parameter converts a list or vector to a string. The beginning of that query_string looks like:\n\"SQRT(SUM(POWER(b20005.B20005_001M, 2))) AS B20005_001M, SQRT(...\"\nThe query is further built by adding the rest of the SQL statements:\nquery_string <- paste(\n    \"SELECT ruca.DESCRIPTION,\",\n    query_string,\n    \"FROM 'b20005' \n    INNER JOIN ruca \n    ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n    WHERE \n    b20005.state = $state\n    GROUP BY ruca.DESCRIPTION\"\n  )\nThe ruca.DESCRIPTION column, which contains RUCA levels (e.g. \"Urban\") is joined onto b20005 from the ruca table using the foreign keys representing the Census Tract FIPS code (TRACTFIPS for the ruca table and the concatenated field state || county || tract for b20005). The $state parameter is assigned the person-selected state input, and the columns are aggreaggated by RUCA levels (i.e. GROUP BY ruca.DESCRIPTION). Finally, the RUCA level and square root of the sum of MOEs squared are SELECTed from the joined tables.\nThe query for estimates is simpler than MOEs, because estimates only need to be summed over RUCA levels:\n# Construct a query to sum estimates grouped by ruca level\n  query_string <- paste0(\"SUM(b20005.\",vars$name, \") AS \", vars$name, collapse=\",\")\nget_b20005_ruca_aggregate_earnings returns the query result data.frames in a named list:\nreturn(list(\"estimate\" = estimate_rs, \"moe\" = moe_rs))"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#calculate_median.r",
    "title": "R Shiny Census App",
    "section": "calculate_median.R",
    "text": "calculate_median.R\nThe procedure for calculating a median earnings data estimate is shown starting on page 17 of the Accuracy of PUMS documentation. This script follows it closely:\n\nCreate Frequency Distribution\n\nObtain the weighted frequency distribution for the selected variable. data is a data.frame with earning estimate values. The rows are the earning ranges and the columns are ruca_levels:\n\n\ncum_percent <- 100.0 * cumsum(data[ruca_level]) / sum(data[ruca_level])\n\n\nCalculate Weighted Total\n\nCalculate the weighted total to yield the base, B.\n\n\nB <- colSums(data[ruca_level])\n\n\nApproximate Standard Error\n\nApproximate the standard error of a 50 percent proportion using the formula in Standard Errors for Totals and Percentages. The design_factor is passed to this function by the server who uses the get_design_factor function explained below to query the design_factors table.\n\n\nse_50_percent <- design_factor * sqrt(87.5/(12.5*B) * 50^2)\n\n\nCalculate Median Estimate Bounds\n\nCreate the variable p_lower by subtracting the SE from 50 percent. Create p_upper by adding the SE to 50 percent.\n\n\np_lower <- 50 - se_50_percent\np_upper <- 50 + se_50_percent\n\nDetermine the categories in the distribution that contain p_lower and p_upper…\n\n\n# Determine the indexes of the cumulative percent data.frame corresponding  \n# to the upper and lower bounds of the 50% proportion estimate\ncum_percent_idx_lower <- min(which(cum_percent > p_lower))\ncum_percent_idx_upper <- min(which(cum_percent > p_upper))\n.._If p_lower and p_upper fall in the same category, follow step 6. If p_lower and p_upper fall in different categories, go to step 7…_\n\n# The median estimation calculation is handled differently based on \n# whether the upper and lower bound indexes are equal\n    if (cum_percent_idx_lower == cum_percent_idx_upper) {\n\nIf p_lower and p_upper fall in the same category, do the following:\n\n\nDefine A1 as the smallest value in that category.\n\n\n# A1 is the minimum earnings value (e.g. 30000) of the earning range \n# (e.g. 30000 to 34999) corresponding to the lower bound cumulative percent\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\n\nDefine A2 as the smallest value in the next (higher) category.\n\n\n# A2 is the minimum earnings value of the earning range above the \n# earning range corresponding to the upper bound cumulative percent\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\n\nDefine C1 as the cumulative percent of units strictly less than A1.\n\n\n# C1 is the cumulative percentage of earnings one row below the \n# lower bound cumulative percent\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\n\nDefine C2 as the cumulative percent of units strictly less than A2.\n\n\n# C2 is the cumulative percentage of the earnings below the \n# lower bound cumulative percent\nC2 <- cum_percent[cum_percent_idx_lower, ]\n\nUse the following formulas to approximate the lower and upper bounds for a confidence interval about the median:\n\n\n# the lower bound of the median \nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n      \n# the upper bound of the median\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nIf p_lower and p_upper fall in different categories, do the following:\n\n\nFor the category containing p_lower: Define A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the lower bound.\n\n\n# A1, A2, C1 and C2 are calculated using the lower bound cumulative percent\n# to calculate the lower bound of the median estimate\nA1 <- earnings[cum_percent_idx_lower, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_lower + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_lower - 1, ]\nC2 <- cum_percent[cum_percent_idx_lower, ]\nlower_bound <- (p_lower - C1) / (C2 - C1) * (A2 - A1) + A1\n\nFor the category containing p_upper: Define new values for A1, A2, C1, and C2 as described in step 6. Use these values and the formula in step 6 to obtain the upper bound.\n\n\n# A1, A2, C1 and C2 are calculated using the upper bound cumulative percent\n# to calculate the upper bound of the median estimate\nA1 <- earnings[cum_percent_idx_upper, \"min_earnings\"]\nA2 <- earnings[cum_percent_idx_upper + 1, \"min_earnings\"]\nC1 <- cum_percent[cum_percent_idx_upper - 1,]\nC2 <- cum_percent[cum_percent_idx_upper,]\nupper_bound <- (p_upper - C1) / (C2 - C1) * (A2 - A1) + A1\n\nUse the lower and upper bounds approximated in steps 6 or 7 to approximate the standard error of the median. SE(median) = 1/2 X (Upper Bound – Lower Bound)\n\n\n# The median earning estimate is the average of the upper and lower bounds\n# of the median estimates calculated above in the if-else block\nmedian_earnings <- 0.5 * (lower_bound + upper_bound)\n    \n# The median SE is half the distance between the upper and lower bounds\n# of the median estimate\nmedian_se <- 0.5 * (upper_bound - lower_bound)\n\n# The 90% confidence interval critical z-score is used to calculate \n# the margin of error\nmedian_90_moe <- 1.645 * median_se\n\n\nReshape the Data\nFinally, a data.frame is returned, which will be displayed in a tableOutput element.\n\n# A data.frame will be displayed in the UI\nmedian_data <- data.frame(\n  \"Estimate\" = median_earnings,\n  \"SE\" = median_se,\n  \"MOE\" = median_90_moe\n)"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#format_query_result.r",
    "title": "R Shiny Census App",
    "section": "format_query_result.R",
    "text": "format_query_result.R\nThe purpose of this function is to receive two data.frame objects, one for earnings estimate values, and one for the corresponding moe values, and return a single data.frame which is ready to be displayed in a tableOutput.\n\nExtract data.frame Objects from List\nSince get_b20005_ruca_aggregate_earnings returns a named list, I first pull out the estimate and moe data.frame objects:\n\n# Pull out query result data.frames from the list\nestimate <- rs[[\"estimate\"]]\nmoe <- rs[[\"moe\"]]\n\n\nReshape data.frame Objects\nThese data.frame objects have RUCA levels in the column DESCRIPTION and one column for each population estimate. For example, the estimate for Alabama Full Time Female workers looks like this:\n\n\n\n\nDESCRIPTION\n…\nB20005_053E\nB20005_054E\nB20005_055E\n…\n\n\n\n\n1\nLarge Town\n…\n149\n257\n546\n…\n\n\n2\nRural\n…\n75\n66\n351\n…\n\n\n3\nSmall Town\n…\n28\n162\n634\n…\n\n\n4\nUrban\n…\n468\n1061\n4732\n…\n\n\n5\nZero Population\n…\n0\n0\n0\n…\n\n\n\nThe moe data.frame has a similar layout.\nHowever, in the UI, I want the table to look like this:\n\n\n\nPopulation estimates for earnings levels from $1 to $2499 up to $100000 and more for Alabama Full Time Female Workers\n\n\nTo achieve this, I first transpose the estimate and moe data.frames…\n\n# Transpose the query results\ncol_names <- estimate[,\"DESCRIPTION\"]\nestimate <- t(estimate[-1])\ncolnames(estimate) <- col_names\n  \ncol_names <- moe[,\"DESCRIPTION\"]\nmoe <- t(moe[-1])\ncolnames(moe) <- col_names\n…then zip them together, keeping in mind that not all states have tracts designated with all RUCA levels:\n\n# Create a mapping to make column names more computer-readable\nformat_ruca_level <- c(\n  \"Urban\" = \"Urban\", \n  \"Large Town\" = \"Large_Town\", \n  \"Small Town\" = \"Small_Town\", \n  \"Rural\" = \"Rural\",\n  \"Zero Population\" = \"Zero_Population\")\n\n# bind together estimate and corresponding moe columns\n# some states do not have all RUCA levels\n# for example, Connecticut does not have \"Small Town\" tracts\n\n# Create empty objects\noutput_table <- data.frame(temp = matrix(NA, nrow = nrow(estimate), ncol = 0))\ncol_names <- c()\n\nfor (ruca_level in c(\"Urban\", \"Large Town\", \"Small Town\", \"Rural\")) {\n  if (ruca_level %in% colnames(estimate)) {\n    output_table <- cbind(output_table, estimate[,ruca_level], moe[,ruca_level])\n    \n    # paste \"_MOE\" suffix for MOE columns\n    col_names <- c(\n      col_names,\n      format_ruca_level[[ruca_level]],\n      paste0(format_ruca_level[[ruca_level]], \"_MOE\"))\n  }\n}\n\n# Replace old names with more computer-readable names\ncolnames(output_table) <- col_names\n\n\n\nAdd Descriptive Labels\nFinally, merge the output_table data.frame with labels (long form description of the B20005 variables) which are retrieved from the database using the get_b20005_labels function explained later on in this post. Remember that the label is delimited with \"!!\" and the last substring contains earnings ranges (e.g. “$30,000 to $34,999”):\n\n# name rows as long-form labels, by splitting them by '!!' and \n# grabbing the last chunk which has dollar ranges e.g. \n# $30000 to $34999\noutput_table <- merge(output_table, labels, by.x = 0, by.y = \"name\")\nsplit_label <- data.frame(\n  do.call(\n    'rbind', \n    strsplit(as.character(output_table$label),'!!',fixed=TRUE)))\n\nrownames(output_table) <- split_label$X6"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_labels.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_labels.R",
    "text": "get_b20005_labels.R\nThis script contains two helper functions to retrieve the label column from the b20005_vars table.\n\nGet Earnings Population Estimate Labels\nThe first one, get_b20005_labels retrieves the variable name and label for earning range strings (e.g. “$30,000 to $34,999”):\n\nget_b20005_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    WHERE \n      label LIKE '%$%'\n    ORDER BY name\"\n    )\n  dbDisconnect(census_app_db)\n  return(rs)\n}\n\n\n\nGet All Labels\nThe second function, get_b20005_ALL_labels returns the whole table:\n\nget_b20005_ALL_labels <- function() {\n  census_app_db <- dbConnect(RSQLite::SQLite(), \"census_app_db.sqlite\")\n  rs <- dbGetQuery(\n    census_app_db, \n    \"SELECT \n      name, label\n    FROM 'b20005_vars' \n    ORDER BY name\"\n  )\n  dbDisconnect(census_app_db)\n  return(rs)\n}"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_tract_earnings.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_tract_earnings.R",
    "text": "get_b20005_tract_earnings.R\nThis function is similar to get_b20005_ruca_aggregate_earnings but does not aggregate by RUCA level, and also includes Census Tracts that are not designated a RUCA level. The label_wildcard is constructed the same way as before.\n\nGet Variable Names\nThe variable names are obtained for both margin of error and estimates in the same query:\n\n # Get b20005 variable names (estimates and moe)\nvars <- dbGetQuery(\n  census_app_db, \n  \"SELECT name FROM b20005_vars \n  WHERE label LIKE $label_wildcard\",\n  params=list(label_wildcard=label_wildcard)\n  )\n\n\n\nJoin Tables\nThe tract-level earnings are queried with the following, using a LEFT JOIN between b20005 and ruca tables to include tracts that do not have a RUCA level.\n\n# Construct query to get tract-level earnings data\nquery_string <- paste(\n  \"SELECT ruca.DESCRIPTION,\n  b20005.state || b20005.county || b20005.tract AS TRACTFIPS,\",\n  paste0(vars$name, collapse=\",\"),\n  \"FROM b20005 \n  LEFT JOIN ruca \n  ON b20005.state || b20005.county || b20005.tract = ruca.TRACTFIPS\n  WHERE \n  b20005.state LIKE $state\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_b20005_states.r",
    "title": "R Shiny Census App",
    "section": "get_b20005_states.R",
    "text": "get_b20005_states.R\nThis function retrieves state codes and names from the codes table, and is used to assign choices to selectInput dropdowns. \"United States\" which has a FIPS code of \"00\" is excluded because the b20005 table contains state-level data only. The query result is sorted by the state name so that the dropdown menu choices are in ascending alphabetical order.\nstates <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESCRIPTION, CODE\n  FROM codes \n  WHERE CATEGORY = 'state'\n  AND CODE <> '00'\n  ORDER BY DESCRIPTION\")"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#get_design_factor.r",
    "title": "R Shiny Census App",
    "section": "get_design_factor.R",
    "text": "get_design_factor.R\nThis function retrieves a single numeric Design Factor for the “Person Earnings/Income” characteristic from the design_factors table for a given state parameter:\n\nrs <- dbGetQuery(\n  census_app_db, \n  \"SELECT DESIGN_FACTOR FROM design_factors\n  WHERE ST = $state\n  AND CHARACTERISTIC = 'Person Earnings/Income'\",\n  params = list(state=state))\n\nrs <- as.numeric(rs[1, \"DESIGN_FACTOR\"])"
  },
  {
    "objectID": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "href": "posts/2021-09-21-shiny-census-app/2021-09-21-shiny-census-app.html#make_plot.r",
    "title": "R Shiny Census App",
    "section": "make_plot.R",
    "text": "make_plot.R\nThis is function creates a ggplot.bar_plot object using a given data, RUCA level, and title. The x-axis labels are rotated, both axis labels are resized, and plot title and subtitle are formatted.\n\nmake_plot <- function(data, ruca_level, plot_title){\n  # Prepare x-axis factor for `aes` parameter\n  xs <- rownames(data)\n  xs <- factor(xs, xs)\n\n  bar_plot <- ggplot(\n    data=data,\n    aes(x=xs, y=get(ruca_level))) + \n    geom_bar(stat='identity') + \n\n    theme(\n      # Rotate x-axis labels\n      axis.text.x=element_text(\n        angle = -90, \n        vjust = 0.5, \n        hjust=1, \n        size=12),\n\n      # Resize x-axis labels and move them away from axis\n      axis.title.x=element_text(vjust=-0.75,size=14),\n\n      # Resize y-axis labels\n      axis.text.y=element_text(size=12),\n      axis.title.y=element_text(size=14),\n\n      # Set plot title and subtitle font and placement\n      plot.title = element_text(size = 18, hjust=0.5, face='bold'),\n      plot.subtitle = element_text(size = 12, hjust=0.5)) +\n\n    labs(x=\"Earnings\", y=\"Population Estimate\") + \n    ggtitle(plot_title, subtitle=\"Population Estimate by Earnings Level\")\n\n  return (bar_plot)\n}"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "",
    "text": "In this blog post I will train a deep learning model to classify two dependent variables in a tabular dataset. I’m not sure how this works under the hood and will use this as an opportunity to learn.\nI will reference the code and logic from Jeremy Howard’s notebook Linear model and neural net from scratch where he builds a deep learning neural net from scratch."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#plan-of-attack",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#plan-of-attack",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nThere are a few places in the existing code that I’ll need to modify to fit a two-output use case:\n\nPrepping the Data\nCurrently, there are 12 different independent variables and 1 dependent variable. In my use case, I will have 11 independent variables and 2 dependent variables (Age and Survived).\nThe current dependent variables are turned into column vectors as follows:\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\nI’m not sure if I’ll have to do this for two dependent variables as well. It’s something I’ll look into and make sure matches the expected shape before I proceed.\n\n\nInitializing Coefficients\nCurrently, the sizes of each of the layers of the deep neural net are as follows:\nhiddens = [10,10]\nsizes = [n_coeff] + hiddens + [1]\nFor a model with two outputs, the last layer will have size 2:\nsizes = [n_coeff] + hiddens + [2]\n\n\nCalculating Predictions\nWhen calculating predictions for one output, the final line returns torch.sigmoid of the result. In my case, that will work for predicting Survived since it’s between 0 and 1, but Age is a continuous variable that can be greater than 0 so instead of sigmoid I’ll use F.relu and see if that works.\n\n\nCalculating Loss\nHere’s the current loss function:\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\nGiven broadcasting, I think it should work as is, but will test it out and see.\n\n\nCalculating Metrics\nAccuracy is calculated as follows:\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nThis will work for my first output (Survived) but I will need something else like Root Mean Squared Error for the second output Age which is continuous."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#load-and-clean-the-data",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#load-and-clean-the-data",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Load and Clean the Data",
    "text": "Load and Clean the Data\nI’ll reuse the code in Jeremy’s notebook which replaces NA values with the mode of each column, adds a LogFare column, and normalizes all columns.\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 15.4MB/s]\n\n\n\n\n\n\n\n\n\nfrom fastai.tabular.all import *\n\npd.options.display.float_format = '{:.2f}'.format\nset_seed(42)\n\n\n# load the training data\ndf = pd.read_csv(path/'train.csv')\n\n\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.00\n      1\n      0\n      A/5 21171\n      7.25\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.00\n      1\n      0\n      PC 17599\n      71.28\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.00\n      0\n      0\n      STON/O2. 3101282\n      7.92\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.00\n      1\n      0\n      113803\n      53.10\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.00\n      0\n      0\n      373450\n      8.05\n      NaN\n      S\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                      0.00\nPclass                        3.00\nName           Abbing, Mr. Anthony\nSex                           male\nAge                          24.00\nSibSp                         0.00\nParch                         0.00\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\ndf.fillna(modes, inplace=True)\n\n\ndf.isna().sum() # should be all zeros\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-independent-and-dependent-variables",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-independent-and-dependent-variables",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Prepare Independent and Dependent Variables",
    "text": "Prepare Independent and Dependent Variables\n\n# transposed so it has the shape 891, 2\nt_dep = tensor(df.Survived, df.Age).T\n\n\nt_dep.shape\n\ntorch.Size([891, 2])\n\n\n\nt_dep[:5]\n\ntensor([[ 0., 22.],\n        [ 1., 38.],\n        [ 1., 26.],\n        [ 1., 35.],\n        [ 0., 35.]])\n\n\n\nindep_cols = ['SibSp', 'Parch', 'LogFare'] + added_cols\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep.shape\n\ntorch.Size([891, 11])\n\n\n\n# normalize independent variables\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\n\nt_indep[:2]\n\ntensor([[0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n         0.0000, 1.0000],\n        [0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n         0.0000, 0.0000]])"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-training-and-validation-splits",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#prepare-training-and-validation-splits",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Prepare Training and Validation Splits",
    "text": "Prepare Training and Validation Splits\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#define-training-functions",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#define-training-functions",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Define Training Functions",
    "text": "Define Training Functions\nNext, I’ll redefine the training functions in Jeremy’s notebook to handle two outputs:\n\nInitializing Coefficients\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\nTo make sure I understand what’s going on in that function with this change, I’ll run each line one at a time:\n\nn_coeff = 11\n\n\nhiddens = [10, 10]\n\n\nsizes = [n_coeff] + hiddens + [2]\n\n\nsizes\n\n[11, 10, 10, 2]\n\n\n\nn = len(sizes)\nn\n\n4\n\n\n\nlayers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n\n\nlayers[0].shape\n\ntorch.Size([11, 10])\n\n\n\nlayers[1].shape\n\ntorch.Size([10, 10])\n\n\n\nlayers[2].shape\n\ntorch.Size([10, 2])\n\n\n\nlayers[2]\n\ntensor([[-0.2763,  0.9819],\n        [-0.2831,  1.3895],\n        [-0.0237,  1.0026],\n        [ 0.6002,  0.6650],\n        [ 0.2466,  0.8107],\n        [-0.0168, -0.5426],\n        [ 0.0158,  1.1835],\n        [ 0.1367,  0.7143],\n        [ 0.0303,  1.1501],\n        [ 0.9985,  0.7530]])\n\n\n\nconsts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\nconsts\n\n[tensor(-0.0256), tensor(-0.0409), tensor(0.0019)]\n\n\n\n\nCalculating Predictions\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.stack([torch.sigmoid(res[:,0]), F.relu(res[:,1])]).T\n\nI’ll test out this function by initializing some random coefficients and using the independent variables to see what kind of outputs I get:\n\ncoeffs = init_coeffs(11)\n\n\n# three layers and three constants\ncoeffs[0][0].shape, coeffs[0][1].shape, coeffs[0][2].shape, len(coeffs[1])\n\n(torch.Size([11, 10]), torch.Size([10, 10]), torch.Size([10, 2]), 3)\n\n\n\nlayers,consts = coeffs\n\n\nn = len(layers)\nn\n\n3\n\n\n\nres = trn_indep\n\n\nres.shape\n\ntorch.Size([713, 11])\n\n\n\n# first layer\nres = res@layers[0] + consts[0]\nres = F.relu(res)\n\n\nres.shape\n\ntorch.Size([713, 10])\n\n\n\n# second layer\nres = res@layers[1] + consts[1]\nres = F.relu(res)\n\n\nres.shape\n\ntorch.Size([713, 10])\n\n\n\n# last layer\nres = res@layers[2] + consts[2]\nres.shape\n\ntorch.Size([713, 2])\n\n\n\n# final treatment of predictions\ntorch.stack([torch.sigmoid(res[:,0]), F.relu(res[:,1])]).T.shape\n\ntorch.Size([713, 2])\n\n\n\n\nCalculating Loss\n\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\n\ncalc_preds(coeffs, trn_indep)[:2]\n\ntensor([[0.7009, 1.4735],\n        [0.7032, 1.3960]], grad_fn=<SliceBackward0>)\n\n\n\ntrn_dep[:2]\n\ntensor([[ 1.0000,  1.0000],\n        [ 0.0000, 40.5000]])\n\n\n\n(calc_preds(coeffs, trn_indep)-trn_dep)[:5]\n\ntensor([[ -0.2991,   0.4735],\n        [  0.7032, -39.1040],\n        [ -0.3204, -25.7099],\n        [  0.6899, -28.6214],\n        [  0.6613,  -1.9130]], grad_fn=<SliceBackward0>)\n\n\n\ncalc_loss(coeffs,trn_indep, trn_dep)\n\ntensor(13.8010, grad_fn=<MeanBackward0>)\n\n\nThe loss function seems to work without modification so I’ll keep it as is.\n\n\nCalculating Metrics\nI am mixing a classification output (Survived) and a regression output (Age) so I’ll have to spit back two different metrics.\n\ndef calc_metrics(coeffs):\n  preds = calc_preds(coeffs, val_indep)\n  acc = (val_dep[:,0].bool()==(preds[:,0]>0.5)).float().mean()\n  rmse = ((preds[:,1]-val_dep[:,1])**2).mean().sqrt()\n  return acc, rmse\n\nI’ll walk through the steps of calculating each metric manually. First, calculating accuracy for the Survived dependent variable:\n\npreds = calc_preds(coeffs, val_indep)\n\n\n(val_dep[:,0])[:5]\n\ntensor([1., 0., 0., 0., 0.])\n\n\n\n(preds[:,0]>0.5)[:5]\n\ntensor([True, True, True, True, True])\n\n\n\n(val_dep[:,0].bool()==(preds[:,0]>0.5))[:5]\n\ntensor([ True, False, False, False, False])\n\n\n\n(val_dep[:,0].bool()==(preds[:,0]>0.5)).float().mean()\n\ntensor(0.4045)\n\n\nThen, calculating RMSE for the Age dependent variable:\n\npreds[:,1][:5]\n\ntensor([1.4279, 1.3960, 1.3844, 1.7690, 1.7181], grad_fn=<SliceBackward0>)\n\n\n\nval_dep[:,1][:5]\n\ntensor([24., 24., 24., 18., 25.])\n\n\n\n((preds[:,1]-val_dep[:,1])**2).mean().sqrt()\n\ntensor(30.2869, grad_fn=<SqrtBackward0>)\n\n\nFinally, testing the written function:\n\ncalc_metrics(coeffs)\n\n(tensor(0.4045), tensor(30.2869, grad_fn=<SqrtBackward0>))\n\n\n\n\nUpdate Coefficients\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\n\nTraining One Epoch\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad():\n      update_coeffs(coeffs, lr)\n      acc, rmse = calc_metrics(coeffs)\n    print((acc.item(), rmse.item()), end=\"\\n\")\n\n\n\nTraining the Model\n\ndef train_model(epochs=30, lr=0.01, n_coeff=11):\n    coeffs = init_coeffs(n_coeff)\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nWith everything (hopefully) in place, I’ll try and train the model and see what happens:\n\ncoeffs = train_model()\n\n(0.38764044642448425, 30.570125579833984)\n(0.38764044642448425, 30.408233642578125)\n(0.38764044642448425, 30.238630294799805)\n(0.40449437499046326, 30.06170654296875)\n(0.40449437499046326, 29.877859115600586)\n(0.40449437499046326, 29.687469482421875)\n(0.40449437499046326, 29.487010955810547)\n(0.40449437499046326, 29.277685165405273)\n(0.40449437499046326, 29.056915283203125)\n(0.40449437499046326, 28.823810577392578)\n(0.40449437499046326, 28.57526397705078)\n(0.40449437499046326, 28.313222885131836)\n(0.40449437499046326, 28.037466049194336)\n(0.40449437499046326, 27.746822357177734)\n(0.40449437499046326, 27.441844940185547)\n(0.40449437499046326, 27.11766242980957)\n(0.40449437499046326, 26.774660110473633)\n(0.40449437499046326, 26.409164428710938)\n(0.40449437499046326, 26.020383834838867)\n(0.40449437499046326, 25.609088897705078)\n(0.40449437499046326, 25.171112060546875)\n(0.40449437499046326, 24.705785751342773)\n(0.40449437499046326, 24.209741592407227)\n(0.40449437499046326, 23.682376861572266)\n(0.40449437499046326, 23.12194061279297)\n(0.40449437499046326, 22.530494689941406)\n(0.40449437499046326, 21.905399322509766)\n(0.40449437499046326, 21.243457794189453)\n(0.40449437499046326, 20.541717529296875)\n(0.42696627974510193, 19.800212860107422)\n\n\nThe model is successfully training, so I take it that the code “works” in the sense that matrix multiplications and loss calculations are being performed. However, the accuracy for predicting Survived and the error for predicting Age are not great. Looking at some predictions:\n\nval_dep[:10]\n\ntensor([[ 1., 24.],\n        [ 0., 24.],\n        [ 0., 24.],\n        [ 0., 18.],\n        [ 0., 25.],\n        [ 0., 34.],\n        [ 1.,  4.],\n        [ 1., 28.],\n        [ 0.,  1.],\n        [ 1., 24.]])\n\n\n\ncalc_preds(coeffs, val_indep)[:10]\n\ntensor([[ 0.5220, 12.6086],\n        [ 0.5365, 13.6072],\n        [ 0.5167, 14.2770],\n        [ 0.5119, 14.4791],\n        [ 0.5162, 14.1781],\n        [ 0.5163, 14.3243],\n        [ 0.5276, 13.5216],\n        [ 0.5296, 13.1540],\n        [ 0.5437, 15.1101],\n        [ 0.5275, 13.6733]], grad_fn=<SliceBackward0>)"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#running-and-recording-trainings",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#running-and-recording-trainings",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Running and Recording Trainings",
    "text": "Running and Recording Trainings\nI’ll reuse code that I implemented in my previous blog post on plotting losses and accuracy, modifying it to capture both accuracy and rmse from the training:\n\ndef one_epoch(coeffs, lr):\n  trn_loss = calc_loss(coeffs, trn_indep, trn_dep)\n  trn_loss.backward()\n  with torch.no_grad():\n    val_loss = calc_loss(coeffs, val_indep, val_dep)\n    update_coeffs(coeffs, lr)\n    acc, rmse = calc_metrics(coeffs)\n  return trn_loss, val_loss, acc, rmse\n\n\ndef train_model(epochs, lr, n_coeff, is_seed=True):\n  if is_seed: torch.manual_seed(442)\n  tl, vl, a, r = [], [], [], []\n  coeffs = init_coeffs(n_coeff)\n  for i in range(epochs):\n    trn_loss, val_loss, acc, rmse = one_epoch(coeffs, lr)\n    tl.append(trn_loss.item())\n    vl.append(val_loss.item())\n    a.append(acc.item())\n    r.append(rmse.item())\n  return tl, vl, a, r\n\n\ndef train_multiple_models(runs=100, epochs=30, lr=4, n_coeff=11, is_seed=False):\n  # initialize recorder object\n  recorder = pd.DataFrame(columns=[\"run\", \"epoch\", \"trn_loss\", \"val_loss\", \"acc\", \"rmse\"])\n  for run in range(runs):\n    # get lists of losses and accuracy\n    tl, vl, a, rmse = train_model(epochs, lr, n_coeff, is_seed)\n    # create list of run and epoch values\n    r = [run] * epochs\n    e = [i for i in range(epochs)]\n    # append new data to recorder DataFrame\n    row = pd.DataFrame(data={\"run\": r, \"epoch\": e, \"trn_loss\": tl, \"val_loss\": vl, \"acc\": a, \"rmse\": rmse})\n    recorder = pd.concat([recorder, row])\n  return recorder\n\n\nrecorder = train_multiple_models()\n\n\nPlotting Training Results\n\nrecorder.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n      rmse\n    \n  \n  \n    \n      0\n      0\n      0\n      13.62\n      13.71\n      0.40\n      1187.97\n    \n    \n      1\n      0\n      1\n      594.61\n      594.02\n      0.60\n      31.69\n    \n    \n      2\n      0\n      2\n      14.51\n      14.62\n      0.60\n      31.69\n    \n    \n      3\n      0\n      3\n      14.50\n      14.61\n      0.60\n      31.69\n    \n    \n      4\n      0\n      4\n      14.50\n      14.61\n      0.60\n      31.69\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nrecorder.tail()\n\n\n\n  \n    \n\n\n  \n    \n      \n      run\n      epoch\n      trn_loss\n      val_loss\n      acc\n      rmse\n    \n  \n  \n    \n      25\n      99\n      25\n      14.47\n      14.58\n      0.60\n      31.69\n    \n    \n      26\n      99\n      26\n      14.46\n      14.58\n      0.60\n      31.69\n    \n    \n      27\n      99\n      27\n      14.46\n      14.58\n      0.60\n      31.69\n    \n    \n      28\n      99\n      28\n      14.46\n      14.58\n      0.60\n      31.69\n    \n    \n      29\n      99\n      29\n      14.46\n      14.58\n      0.60\n      31.69\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nrecorder['acc'].max()\n\n0.5955055952072144\n\n\n\nrecorder['rmse'].min()\n\n13.89392375946045\n\n\n\ndef plot_recorder(recorder):\n  fig, axs = plt.subplots(2, 2)\n\n  (recorder\n   .pivot_table(values='trn_loss', index='epoch', columns='run')\n   .plot(color='green', alpha=0.3, legend=False, title='Training Loss', ax=axs[0, 0]));\n\n  (recorder\n   .pivot_table(values='val_loss', index='epoch', columns='run')\n   .plot(color='red', alpha=0.3, legend=False, title='Validation Loss', ax=axs[0,1]));\n\n  (recorder\n   .pivot_table(values='acc', index='epoch', columns='run')\n   .plot(color='blue', alpha=0.3, legend=False, title='Accuracy', ax=axs[1,0]));\n\n  (recorder\n   .pivot_table(values='rmse', index='epoch', columns='run')\n   .plot(color='orange', alpha=0.3, legend=False, title='RMSE', ax=axs[1,1]));\n\n  for ax in axs.flat:\n    ax.label_outer()\n\n\nplot_recorder(recorder)"
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#modifying-the-model",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#modifying-the-model",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Modifying the Model",
    "text": "Modifying the Model\nI think the main takeaway from this exercise so far is that the model is unstable. The training runs don’t look promising—there are sharp changes in loss and RMSE, and the accuracy plateaus quickly. I would hope to see relatively smooth curves in both losses and metrics across epochs.\nJeremy had mentioned in the lesson video that he had to fiddle with some of the constants inside the neural network as well as the learning rate to get a stable training. So with that in mind, I’ll give it a try and adjust those values and see if it makes training more stable.\nThe two things I’ll fiddle with: the arbitrary constants in init_coeffs and the learning rate.\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.5)/sizes[i+1]*6 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.5 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models()\n\n\nplot_recorder(recorder)\n\n\n\n\nIncreasing some of the constants somewhat improved the loss and RMSE and significantly improved the accuracy:\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.8370786309242249, 13.419356346130371)\n\n\n\nrecorder = train_multiple_models(lr=0.03)\n\n\nplot_recorder(recorder)\n\n\n\n\nIncreasing the learning rate significantly changed the way losses and RMSE change over time and seems to have made accuracy more chaotic. The minimum RMSE still remains at around 13:\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.7921348214149475, 13.494928359985352)\n\n\nI’ll continue to increase the constants in the model and the learning rate:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.75)/sizes[i+1]*8 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.75 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.05)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.6404494643211365, 30.712871551513672)\n\n\nI seem to have broken the model, I’ll keep increasing the constants and learning rate one more time and see it if continues to break the training:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-1.0)/sizes[i+1]*10 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*1.00 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.07)\n\n\nplot_recorder(recorder)\n\n\n\n\nOkay continuing to increase them is not helping. I’ll go in the opposite direction—-decreasing the constants and learning rate:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.1)/sizes[i+1]*2 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.05 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.005)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.40449437499046326, 23.53632354736328)\n\n\nThe trainings look MUCH smoother in terms of loss and RMSE, however the accuracy does not improve at all in any of the 100 trainings and the minimum RMSE achieved is not as low as some of the previous configurations. I’ll increase the learning rate and keep the constants as they are:\n\nrecorder = train_multiple_models(lr=0.01)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.40449437499046326, 15.42644214630127)\n\n\nMoving in the right direction for RMSE but still getting a bad accuracy. I’ll increase the learning rate a bit more:\n\nrecorder = train_multiple_models(lr=0.03)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.40449437499046326, 13.523417472839355)\n\n\nSomething about this new model favors Age and penalizes Survived. It’s hard to know what is really going on. I’ll continue to fiddle with the parameters:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.1)/sizes[i+1]*1 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.01)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.5955055952072144, 26.605520248413086)\n\n\nI’ll return to a previous configuration which yielded a pretty high accuracy (79%) and the lowest RMSE so far (13.5).\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [2]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.5)/sizes[i+1]*6 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.5 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\n\nrecorder = train_multiple_models(lr=0.1)\n\n\nplot_recorder(recorder)\n\n\n\n\n\nrecorder['acc'].max(), recorder['rmse'].min()\n\n(0.7977527976036072, 12.552786827087402)\n\n\nI could go on and on with changing arbitrary constants and the learning rate, but I’ll stop here as I’m getting a relatively stable training with a high accuracy and low RMSE. I’ll take a look at Age predictions more closely here:\n\npreds = calc_preds(coeffs, val_indep)\n\n\nage_df = pd.DataFrame({'age_actual': val_dep[:,1].numpy(), 'age_pred': preds[:,1].detach().numpy()})\n\n\nage_df.plot();\n\n\n\n\nThe model didn’t really learn anything useful on how to calculate age. It essentially just found some average value and stuck with it for each passenger."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#training-a-regression-model-for-age",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#training-a-regression-model-for-age",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Training a Regression Model for Age",
    "text": "Training a Regression Model for Age\nBefore I close out this blog post with my final thoughts, I want to see how well our model can predict Age as a single output. I’ll rewrite the training functions to handle a single regression output value:\n\ndef init_coeffs(n_coeff):\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return 100 * torch.sigmoid(res) # assuming age is between 0 and 100\n\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n\ndef calc_rmse(coeffs):\n  preds = calc_preds(coeffs, val_indep)\n  rmse = ((preds-val_dep)**2).mean().sqrt()\n  return rmse\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad():\n      update_coeffs(coeffs, lr)\n      rmse = calc_rmse(coeffs)\n    print(rmse.item(), end=\"\\n\")\n\ndef train_model(epochs=30, lr=0.01, n_coeff=11):\n    coeffs = init_coeffs(n_coeff)\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nI’ll re-prepare the data so that the appropriate variables are categorized as independent and dependent:\n\nt_dep = tensor(df.Age)\n\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\n\nval_indep.shape\n\ntorch.Size([178, 11])\n\n\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\n\nval_dep.shape\n\ntorch.Size([178, 1])\n\n\n\ncoeffs = train_model()\n\n40.933284759521484\n20.56239128112793\n17.421009063720703\n15.360074996948242\n14.128280639648438\n13.583489418029785\n13.374798774719238\n13.349427223205566\n13.402131080627441\n13.475241661071777\n13.540617942810059\n13.617292404174805\n13.704315185546875\n13.765442848205566\n13.83024787902832\n13.8984956741333\n13.92748737335205\n13.957006454467773\n13.987039566040039\n14.017572402954102\n14.045609474182129\n14.074031829833984\n14.102828979492188\n14.131990432739258\n14.161505699157715\n14.191366195678711\n13.757979393005371\n13.822358131408691\n13.89020824432373\n13.91904354095459\n\n\n\npreds = calc_preds(coeffs, val_indep)\n\n\nage_df = pd.DataFrame({'age_actual': val_dep.squeeze(1).numpy(), 'age_pred': preds.squeeze(1).detach().numpy()})\n\n\nage_df.plot();\n\n\n\n\nAgain, the model does not seem to learn anything useful about age, it just finds the mean and uses it as the value for each prediction.\n\nval_dep.mean()\n\ntensor(28.7374)\n\n\n\npreds.mode().values[0]\n\ntensor(24.7849, grad_fn=<SelectBackward0>)\n\n\nTo see if I am getting this result just by chance, or if there is something consistent about this model’s Age predictions, I’ll train this model 100 times and plot the result age distribution:\n\ndef train_multiple_models(runs=100, epochs=30, lr=4, n_coeff=11, is_seed=False):\n  recorder = pd.DataFrame()\n  for run in range(runs):\n    coeffs = train_model(epochs, lr, n_coeff)\n    preds = calc_preds(coeffs, val_indep)\n    recorder[run] = preds.squeeze(1).detach().numpy()\n  return recorder\n\n\nrecorder = train_multiple_models(lr=0.01)\n\n\nfig, ax = plt.subplots(1)\n\nrecorder.plot(legend=False, color='black', alpha=0.6, ax=ax);\n\nax.plot(val_dep.squeeze(1).numpy());\n\n\n\n\nIn many trainings (dark black lines between age values of 24 and 25) the model simply finds the mean. There are some training runs where the predicted age has slightly more variability, but it pales in comparison to the variability in actual age. I would imagine that changing the randomly instantiated coefficients’ constants and learning rate would affect this result."
  },
  {
    "objectID": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#final-thoughts",
    "href": "posts/2023-08-17-two-dependent-variables/2023-08-17-two-dependent-variables.html#final-thoughts",
    "title": "Building a From-Scratch Deep Learning Model to Predict Two Variables",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAlthough my model was not successful in correctly prediction both the age of the passenger and whether they survived, I did find this exercise to be helpful. A few takeaways:\n\nCreating a model that predicts more than one variable will require modifying the model as well as the training and validation data structure.\nChanging the arbitrary coefficients inside the deep neural net and the learning rate significantly affects the training stability and classification accuracy.\nA simple deep learning model predicts the mean for continuous variables (based on what I’ve seen here).\n\nI really enjoyed working through this example and feel more comfortable with building and modifying neural net architecture then when I started writing this blog post. I hope you enjoyed it too!"
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "",
    "text": "In this blog post I will use fastai to train a model that predicts more than one target for the Kaggle Titanic dataset.\nI’ve referenced the notebook Multi-target: Road to the Top, Part 4 by Jeremy Howard as well as a derivative notebook Small models + Multi-targets by Kaggle user Archie Tram (in which he creates a test DataLoader to get predictions from the model)."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#plan-of-attack",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#plan-of-attack",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Plan of Attack",
    "text": "Plan of Attack\n\nCreating DataLoaders\nIn Jeremy’s notebook, he is classifying images of plants with two targets: disease and variety of plant.\nHe creates his DataLoaders object as follows:\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock,CategoryBlock),\n    n_inp=1,\n    get_items=get_image_files,\n    get_y = [parent_label,get_variety],\n    splitter=RandomSplitter(0.2, seed=42),\n    item_tfms=Resize(192, method='squish'),\n    batch_tfms=aug_transforms(size=128, min_scale=0.75)\n).dataloaders(trn_path)\nThere are three blocks: 1 input ImageBlock and 2 output CategoryBlocks. The model gets the outputs with parent_label (for the disease) and a custom function get_variety (which grabs the variety column value of the given image from a DataFrame).\nIn my use case, I will have to follow a similar approach, albeit catered to tabular data.\n\n\nCalculating Losses\nJeremy calculates loss as the sum of the following:\n\nCross-Entropy loss of the disease inputs\nCross-Entropy loss of the variety inputs\n\nI’ll follow a similar approach, except if I use continuous variables as targets I’ll use MSE instead of Cross-Entropy.\n\n\nCalculating Metrics\nSimilar to the loss calculation, I’ll combine the calculation of the metric for each of the two targets. For continuous variables, I’ll use RMSE."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#training-a-multi-target-model",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#training-a-multi-target-model",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Training a Multi-Target Model",
    "text": "Training a Multi-Target Model\nWith a rough plan outlined, I’ll start the training process with loading and cleaning the Titanic dataset.\n\nLoad and Clean Data\n\nfrom fastai.tabular.all import *\n\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 6.18MB/s]\n\n\n\n\n\n\n\n\n\n# load the training data and look at it\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# feature engineering\ndef add_features(df):\n  df['LogFare'] = np.log1p(df['Fare'])\n  df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))\n  df['Family'] = df.SibSp+df.Parch\n  df['Alone'] = df.Family == 0\n  df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')\n  df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n  df['Title'] = df.Title.map(dict(Mr=\"Mr\", Miss=\"Miss\", Mrs=\"Mrs\", Master=\"Master\"))\n\n\n# add the features to our dataframe\nadd_features(df)\n\n\n# view the topmost row of the modes DataFrame\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nLogFare                   2.202765\nDeck                           ABC\nFamily                         0.0\nAlone                         True\nTicketFreq                     1.0\nTitle                           Mr\nName: 0, dtype: object\n\n\n\n# fill missing data with the column's mode\ndf.fillna(modes, inplace=True)\n\n\n# check that we no longer have missing data\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\nLogFare        0\nDeck           0\nFamily         0\nAlone          0\nTicketFreq     0\nTitle          0\ndtype: int64\n\n\n\n# create training and validation index lists\nsplits = RandomSplitter(seed=42)(df)\n\n\n\nCreate DataLoaders\nI’ll take most of the code from the Why you should use a framework notebook by Jeremy, with the following changes:\n\nRemove \"Age\" from cont_names and move it to y_names along with \"Survived\" which will be our two targets.\nSet n_out=2 for the RegressionBlock.\n\nI’ll treat both targets as a regression, as I wasn’t able to provide two DataBlocks for y_block.\nSince I’ve filled in missing values manually, I have removed the FillMissing item from procs.\n\n# create dataloaders object\ndls = TabularPandas(\n    df,\n    splits=splits,\n    procs=[Categorify, Normalize],\n    cat_names=[\"Sex\", \"Pclass\", \"Embarked\", \"Deck\", \"Title\"],\n    cont_names=[\"SibSp\", \"Parch\", \"LogFare\", \"Alone\", \"TicketFreq\", \"Family\"],\n    y_names=[\"Age\", \"Survived\"],\n    y_block=RegressionBlock(n_out=2)\n).dataloaders(path=\".\")\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      Sex\n      Pclass\n      Embarked\n      Deck\n      Title\n      SibSp\n      Parch\n      LogFare\n      Alone\n      TicketFreq\n      Family\n      Age\n      Survived\n    \n  \n  \n    \n      0\n      male\n      1\n      S\n      ABC\n      Mr\n      1.000000e+00\n      -9.897945e-09\n      3.970292\n      2.458140e-08\n      2.0\n      1.000000e+00\n      42.0\n      0.0\n    \n    \n      1\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.230014\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      18.0\n      0.0\n    \n    \n      2\n      male\n      2\n      S\n      ABC\n      Mr\n      1.000000e+00\n      2.000000e+00\n      3.358638\n      2.458140e-08\n      3.0\n      3.000000e+00\n      36.0\n      0.0\n    \n    \n      3\n      male\n      3\n      C\n      ABC\n      Mr\n      1.000000e+00\n      1.000000e+00\n      2.107689\n      2.458140e-08\n      1.0\n      2.000000e+00\n      17.0\n      0.0\n    \n    \n      4\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.351375\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      28.0\n      0.0\n    \n    \n      5\n      female\n      3\n      S\n      ABC\n      Mrs\n      1.000000e+00\n      4.000000e+00\n      3.363842\n      2.458140e-08\n      6.0\n      5.000000e+00\n      45.0\n      0.0\n    \n    \n      6\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.324836\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      23.0\n      0.0\n    \n    \n      7\n      female\n      3\n      C\n      ABC\n      Mrs\n      1.689237e-09\n      -9.897945e-09\n      2.107178\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      24.0\n      1.0\n    \n    \n      8\n      male\n      3\n      S\n      ABC\n      Mr\n      1.689237e-09\n      -9.897945e-09\n      2.188856\n      1.000000e+00\n      1.0\n      -1.856774e-08\n      39.0\n      1.0\n    \n    \n      9\n      female\n      2\n      C\n      ABC\n      Mrs\n      1.000000e+00\n      -9.897945e-09\n      3.436269\n      2.458140e-08\n      2.0\n      1.000000e+00\n      14.0\n      1.0\n    \n  \n\n\n\n\n\nCreate Loss Function\nIf I understand correctly, we will get 2 columns of predictions, and two variables of targets to compute the loss with:\n\ndef age_loss(pred, yb): return F.mse_loss(pred[:,0], yb[:,0])\ndef survived_loss(pred, yb): return F.mse_loss(pred[:,1], yb[:,1])\n\ndef combine_loss(pred, yb): return age_loss(pred, yb) + survived_loss(pred, yb)\n\n\n\nCreate Metric Function\nI’ll create an RMSE function for each target variable:\n\ndef age_rmse(pred, yb): return torch.sqrt(F.mse_loss(pred[:,0], yb[:,0]))\ndef survived_rmse(pred, yb): return torch.sqrt(F.mse_loss(pred[:,1], yb[:,1]))\n\nrmse_metrics = (age_rmse, survived_rmse)\n\n\nlearn = tabular_learner(dls, loss_func=combine_loss, metrics=rmse_metrics, layers=[10,10], n_out=2)\n\nMost times that I ran the learning rate finder, the loss was steadily increasing from the get-go. I randomly came across the following learning rate regime which looks more stable, so I’ll use the given value.\n\nlearn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=6.309573450380412e-07, valley=0.14454397559165955)\n\n\n\n\n\n\nlearn.fit(20, lr=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      age_rmse\n      survived_rmse\n      time\n    \n  \n  \n    \n      0\n      766.320923\n      554.657837\n      23.431234\n      0.721841\n      00:00\n    \n    \n      1\n      460.486603\n      170.207932\n      13.030014\n      0.590790\n      00:00\n    \n    \n      2\n      335.931213\n      132.264999\n      11.456180\n      0.649899\n      00:00\n    \n    \n      3\n      265.317535\n      116.719322\n      10.778342\n      0.477045\n      00:00\n    \n    \n      4\n      221.392242\n      121.840828\n      11.004195\n      0.441827\n      00:00\n    \n    \n      5\n      192.420349\n      132.113815\n      11.457218\n      0.472019\n      00:00\n    \n    \n      6\n      173.592255\n      120.654694\n      10.943729\n      0.462033\n      00:00\n    \n    \n      7\n      159.223709\n      113.375626\n      10.612040\n      0.519316\n      00:00\n    \n    \n      8\n      148.853653\n      114.346222\n      10.654099\n      0.484549\n      00:00\n    \n    \n      9\n      140.409439\n      109.572639\n      10.437387\n      0.467927\n      00:00\n    \n    \n      10\n      133.942352\n      114.497719\n      10.642965\n      0.590436\n      00:00\n    \n    \n      11\n      129.807709\n      110.892578\n      10.500125\n      0.455730\n      00:00\n    \n    \n      12\n      125.972458\n      112.508110\n      10.570338\n      0.451019\n      00:00\n    \n    \n      13\n      122.350586\n      126.790512\n      11.167099\n      0.512433\n      00:00\n    \n    \n      14\n      119.345764\n      112.307846\n      10.571351\n      0.579465\n      00:00\n    \n    \n      15\n      117.329689\n      113.805359\n      10.628425\n      0.484336\n      00:00\n    \n    \n      16\n      116.328194\n      115.227859\n      10.696632\n      0.475317\n      00:00\n    \n    \n      17\n      115.390640\n      115.162354\n      10.710686\n      0.500142\n      00:00\n    \n    \n      18\n      116.044281\n      125.941689\n      11.149549\n      0.558260\n      00:00\n    \n    \n      19\n      115.501900\n      116.436340\n      10.739085\n      0.500779\n      00:00\n    \n  \n\n\n\nAfter a few epochs, the RMSE values stop improving. The validation loss also fluctuates throughout the training after decreasing for the first three epochs.\n\n\nComparing Predictions to Actuals\nBased on how the training went, I’m not expecting this model to be able to predict Age and Survived very well. I’ll use the validation set to get predictions and then calculate accuracy for Survived and correlation between actuals vs. predictions for Age.\n\npreds, targ = learn.get_preds(dl=dls.valid)\n\n\n\n\n\n\n\n\n\n# Survived accuracy\n(targ[:,1] == (preds[:,1]>0.5)).float().mean()\n\ntensor(0.6348)\n\n\n\ndef corr(x,y): return np.corrcoef(x,y)[0][1]\n\n\n# Age plot\nfig, ax = plt.subplots(1)\n\nax.axis('equal')\nplt.title(f'Predicted Age vs Actual; r: {corr(preds[:,0], targ[:,0]):.2f}')\nax.scatter(preds[:,0], targ[:,0]);\n\n\n\n\nThe model achieved shoddy accuracy (63%) and an uninspiring correlation between predicted and actual age. The model did particularly poorly in predicting ages above 40."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#comparing-to-single-target-models",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#comparing-to-single-target-models",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Comparing to Single-Target Models",
    "text": "Comparing to Single-Target Models\nI’m curious to see how the model performs when I train it for single targets. I’ll train one regression model for Age, another separate regression model for Survived, and see how their results compare to the combined two-target model.\n\nSingle Target: Age\n\n# create dataloaders object\nage_dls = TabularPandas(\n    df,\n    splits=splits,\n    procs=[Categorify, Normalize],\n    cat_names=[\"Sex\", \"Pclass\", \"Embarked\", \"Deck\", \"Title\"],\n    cont_names=[\"SibSp\", \"Parch\", \"LogFare\", \"Alone\", \"TicketFreq\", \"Family\"],\n    y_names=\"Age\",\n    y_block=RegressionBlock()\n).dataloaders(path=\".\")\n\n\nage_learn = tabular_learner(age_dls, metrics=rmse, layers=[10,10])\n\nI ran the learning rate finder 10 times and got similar charts each time, which tells me that something about this model is more stable than my two-target model.\n\nage_learn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=6.309573450380412e-07, valley=0.0831763744354248)\n\n\n\n\n\n\nage_learn.fit(16, lr=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      781.124268\n      233.326263\n      15.275021\n      00:00\n    \n    \n      1\n      454.851532\n      408.981842\n      20.223301\n      00:00\n    \n    \n      2\n      328.806274\n      116.149773\n      10.777281\n      00:00\n    \n    \n      3\n      263.302643\n      119.088097\n      10.912749\n      00:00\n    \n    \n      4\n      219.239166\n      127.125175\n      11.274981\n      00:00\n    \n    \n      5\n      190.565811\n      111.707756\n      10.569189\n      00:00\n    \n    \n      6\n      171.005737\n      113.618858\n      10.659215\n      00:00\n    \n    \n      7\n      157.105713\n      109.284859\n      10.453939\n      00:00\n    \n    \n      8\n      146.396072\n      118.541183\n      10.887661\n      00:00\n    \n    \n      9\n      138.696716\n      107.435219\n      10.365096\n      00:00\n    \n    \n      10\n      132.795654\n      109.071220\n      10.443716\n      00:00\n    \n    \n      11\n      128.642639\n      112.930344\n      10.626869\n      00:00\n    \n    \n      12\n      124.508675\n      107.584816\n      10.372310\n      00:00\n    \n    \n      13\n      121.428909\n      113.099953\n      10.634846\n      00:00\n    \n    \n      14\n      119.856216\n      114.224464\n      10.687585\n      00:00\n    \n    \n      15\n      118.349365\n      109.042511\n      10.442342\n      00:00\n    \n  \n\n\n\nThe validation loss also fluctuates in this model’s training. The RMSE metric also does not really improve after the first couple of epochs. Similar to last time, I’ll plot the predicted age vs actual and calculate the correlation between the two:\n\nage_preds, age_targ = age_learn.get_preds(dl=age_dls.valid)\n\n\n\n\n\n\n\n\n\n# Age plot\nfig, ax = plt.subplots(1)\n\nax.axis('equal')\nplt.title(f'Predicted Age vs Actual; r: {corr(age_preds[:,0], age_targ[:,0]):.2f}')\nax.scatter(age_preds[:,0], age_targ[:,0]);\n\n\n\n\nSurprisingly, the single target Age model does not perform much better than my two-target model. I get a similar correlation, and this model also fails to predict ages above around 40.\n\n\nSingle Target: Survived\nIn Jeremy’s “Why you should use a framework” notebook, he achieves about an 83% accuracy. I’ll use this as a benchmark to compare my model with.\n\n# create dataloaders object\nsurvived_dls = TabularPandas(\n    df,\n    splits=splits,\n    procs=[Categorify, Normalize],\n    cat_names=[\"Sex\", \"Pclass\", \"Embarked\", \"Deck\", \"Title\"],\n    cont_names=[\"SibSp\", \"Parch\", \"LogFare\", \"Alone\", \"TicketFreq\", \"Family\"],\n    y_names=\"Survived\",\n    y_block=RegressionBlock()\n).dataloaders(path=\".\")\n\n\nsurvived_learn = tabular_learner(survived_dls, metrics=rmse, layers=[10,10])\n\n\nsurvived_learn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=0.05754399299621582, valley=0.0063095735386013985)\n\n\n\n\n\n\nsurvived_learn.fit(16, lr=0.02)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      0.250865\n      0.240620\n      0.490530\n      00:00\n    \n    \n      1\n      0.200100\n      0.214276\n      0.462899\n      00:00\n    \n    \n      2\n      0.177398\n      0.150440\n      0.387866\n      00:00\n    \n    \n      3\n      0.163052\n      0.135140\n      0.367615\n      00:00\n    \n    \n      4\n      0.153141\n      0.131269\n      0.362311\n      00:00\n    \n    \n      5\n      0.147007\n      0.133025\n      0.364726\n      00:00\n    \n    \n      6\n      0.143294\n      0.132439\n      0.363922\n      00:00\n    \n    \n      7\n      0.138928\n      0.131754\n      0.362979\n      00:00\n    \n    \n      8\n      0.135169\n      0.128147\n      0.357976\n      00:00\n    \n    \n      9\n      0.133087\n      0.125253\n      0.353910\n      00:00\n    \n    \n      10\n      0.130366\n      0.126195\n      0.355240\n      00:00\n    \n    \n      11\n      0.128971\n      0.130248\n      0.360899\n      00:00\n    \n    \n      12\n      0.127474\n      0.128108\n      0.357922\n      00:00\n    \n    \n      13\n      0.126128\n      0.124583\n      0.352963\n      00:00\n    \n    \n      14\n      0.125103\n      0.125416\n      0.354142\n      00:00\n    \n    \n      15\n      0.123530\n      0.129710\n      0.360152\n      00:00\n    \n  \n\n\n\n\nsurvived_preds, survived_targ = survived_learn.get_preds(dl=survived_dls.valid)\n\n\n\n\n\n\n\n\n\n(survived_targ == (survived_preds>0.5)).float().mean()\n\ntensor(0.8258)\n\n\nI get an accuracy of around 83% as well."
  },
  {
    "objectID": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#final-thoughts",
    "href": "posts/2023-08-19-fastai-multi-target/2023-08-19-fastai-multi-target.html#final-thoughts",
    "title": "Training a Multi-Target Regression Deep Learning Model with fastai",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are my takeaways from this experiment:\n\nA single-target regression model predicts Survived better than Age.\nA two-target regression model (Survived and Age) predicts Survived significantly worse than a single-target model (Survived only). Something about introducing an output for Age decreases the model’s performance when predicting survival rate.\nA two-target regression model (Survived and Age) predicts Age with about the same correlation as a single-target model (Age only).\n\nSomething about this dataset (and how the model learns from it) makes Age a poor target for prediction. Perhaps it’s the distribution of ages in the dataset, or the relationship with other columns, that makes it harder for the model to predict it accurately.\nI’m happy and proud that I was able to run this experiment after failing to overcome some errors the first couple of times I tried to train a two-target model earlier this week.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "",
    "text": "In this notebook I want to compare the differences between fine-tuning a pretrained model with and without using LoRA. This exercise is a fastai community study group homework assignment.\nHere is a comparison of the full-fine-tuning (Full FT) vs. LoRA fine-tuning (LoRA FT) process on the EleutherAI/pythia-70m model using the roneneldan/TinyStoriesInstruct dataset (which comes from the TinyStories paper):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nParameters\nTraining Set\nValidation Set\nPerplexity\nBatch Size\nEpochs\nTrain Steps\nTrain Time (Minutes)\n\n\n\n\nFull FT\n70.4M\n240k\n60k\n8.51\n16\n3\n22500\n100\n\n\nLoRA FT\n98k\n256k\n64k\n12.68\n16\n4\n32000\n120"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#resources",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#resources",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Resources",
    "text": "Resources\n\nI’ll use a small subset of the roneneldan/TinyStoriesInstruct dataset from HuggingFace for both trainings since when I use the full dataset I’m getting CUDA out-of-memory errors.\nI’m referencing the following to patch together the code in this notebook:\n\nJeremy Howard’s Getting started with NLP for absolute beginners for fundamental setup of data, model, and tokenizer.\nHuggingFace’s Causal language modeling tutorial for updating the tokenizer with a pad token, data_collator and training arguments.\nThis forum response that shows how to select a subset of a dataset with a given set of indexes.\nThe TinyStories author’s hyperparameters as listed in their 33M parameter model page\nHuggingFace’s LoRA Conceptual Guide for steps on how to implement LoRA using peft.\nThis blog post which walks through an example LoRA training.\nThis forum response by Sylvain Gugger which says to set save_strategy to \"no\" to avoid the Trainer creating checkpoints as I was running into errors around this."
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#plan-of-attack",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#plan-of-attack",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nIn my first iteration of this exercise (see below) I manually ran multiple different trainings with different models, dataset sizes and training arguments. The code was flexible and easy to update but I through that process I re-ran a lot of cells with different values and lost track a bit exactly the order of things I was running. In this second iteration, I’ll create a helper function get_trainer which takes various arguments (model, bs,tokz, train_ds, etc.) and returns a HuggingFace Trainer. This will help clear up some of the redundancy in my code and make it a bit cleaner to read.\n\n# all the imports\n!pip install peft accelerate evaluate -Uqq\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoModelForCausalLM, pipeline\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom evaluate import load\nimport math"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#get_trainer-helper-function",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#get_trainer-helper-function",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "get_trainer Helper Function",
    "text": "get_trainer Helper Function\nThis function prepares and returns Trainer object for a given model, tokenizer (and tokenize function), training/validation dataset, learning rate, batch size and number of epochs:\n\ndef get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr, bs, epochs):\n    # get tokenized datasets\n    train_tok_ds = train_ds.map(tok_func, batched=True)\n    eval_tok_ds = eval_ds.map(tok_func, batched=True)\n    \n    # sometimes for whatever reason the datasets are not the right size so checking it here\n    print(train_tok_ds)\n    \n    # not sure what this does but I get an error that the model didn't return a loss value without it\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokz, mlm=False)\n    \n    # define training arguments\n    training_args = TrainingArguments(\n        output_dir=\"outputs\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=lr,\n        lr_scheduler_type = \"cosine\",\n        weight_decay=0.1,\n        per_device_train_batch_size=bs, \n        per_device_eval_batch_size=bs,\n        num_train_epochs=epochs,\n        report_to='none',\n        fp16=True,\n        logging_steps=10,\n        save_strategy=\"no\"\n    )\n    \n    # define Trainer\n    trainer = Trainer(model, training_args, train_dataset=train_tok_ds, eval_dataset=eval_tok_ds,\n                  tokenizer=tokz, data_collator=data_collator)\n    \n    return trainer"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#load-the-dataset",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#load-the-dataset",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Load the Dataset",
    "text": "Load the Dataset\nAs recommended in the study group, I’ll use the TinyStoriesInstruct dataset which comes from the paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English?.\n\nds = load_dataset(\"roneneldan/TinyStoriesInstruct\")\n\n\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 21755681\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 218380\n    })\n})"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#full-fine-tuning-with-eleutheraipythia-70m",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#full-fine-tuning-with-eleutheraipythia-70m",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Full Fine-Tuning with EleutherAi/pythia-70m",
    "text": "Full Fine-Tuning with EleutherAi/pythia-70m\nFirst, I’ll fully fine-tune an existing pretrained model on a subset of the TinyStoriesInstruct dataset using the EleutherAI/pythia-70m model. I chose this model because larger models were giving me CUDA-out-of-memory errors even for small dataset and batch sizes.\n\nmodel_nm = 'EleutherAI/pythia-70m'\ntokz = AutoTokenizer.from_pretrained(model_nm)\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ndef tok_func(x): return tokz(x[\"text\"])\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_nm)\n\n\nmodel\n\nGPTNeoXForCausalLM(\n  (gpt_neox): GPTNeoXModel(\n    (embed_in): Embedding(50304, 512)\n    (layers): ModuleList(\n      (0-5): 6 x GPTNeoXLayer(\n        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (attention): GPTNeoXAttention(\n          (rotary_emb): RotaryEmbedding()\n          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n          (dense): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (mlp): GPTNeoXMLP(\n          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n          (act): GELUActivation()\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n)\n\n\n\nmodel.num_parameters()\n\n70426624\n\n\nI first trained the model on a very small subset (1000 rows) for both full-finetuning and LoRA to make sure it worked, then slowly increased the training and validation size until I got the CUDA out-of-memory error.\nFor small datasets, I noticed that the validation loss started increasing after 3 epochs so I’ve kept the number of epochs at 3. With larger datasets I could try to increase the number of epochs and see if it still overfits.\nI couldn’t figure out how to implement perplexity during training. I was getting a Sizes of tensors must match except in dimension 0. error when passing any function to compute_metrics so I calculate perplexity at the end of training instead.\nWhen I tried to train the model with 240k, 220k or 200k training samples, I got the following error after 1.60, 1.75 and 1.92 epochs respectively:\nRuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/9: file write failed\nI set the save_strategy argument in the training arguments dictionary to \"no\" and this resolved this error. However, in the future, if I wanted checkpoints during my training I would have to figure out how to resolve this error differently.\n\ntrain_ds = ds['train'].select(range(240000))\neval_ds = ds['validation'].select(range(60000))\n\n\ntrainer = get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr=5e-4, bs=16, epochs=3)\n\n\ntrainer.train()\n\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [22500/22500 1:40:16, Epoch 3/3]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n    \n  \n  \n    \n      1\n      2.385700\n      2.407521\n    \n    \n      2\n      2.098300\n      2.192903\n    \n    \n      3\n      1.841100\n      2.141196\n    \n  \n\n\n\nTrainOutput(global_step=22500, training_loss=2.1849648211161297, metrics={'train_runtime': 6016.472, 'train_samples_per_second': 119.671, 'train_steps_per_second': 3.74, 'total_flos': 1.64194783592448e+16, 'train_loss': 2.1849648211161297, 'epoch': 3.0})\n\n\n\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n\n\n    \n      \n      \n      [1875/1875 03:25]\n    \n    \n\n\nPerplexity: 8.51\n\n\nI’ll generate some text from the pretrained model and fully fine-tuned model to see how they compare:\n\nprompt = \"Once upon a time,\"\ngenerator = pipeline('text-generation', model=model_nm, tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n‘Once upon a time, thefirst two are not in agreement. The second is to be expected; and it would have been an easy task for them if they had known that he was going on their way from home as soon after leaving his house at night or when there were no other guests than himself who wanted him back with all of her belongings before returning into town again by midnight (and then later). But this one has never seen such things since I've lived here.”’\n\n\ngenerator = pipeline('text-generation', model=trainer.model.to('cpu'), tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n‘Once upon a time, there was an old man. He had a big mustache and he loved to wear it every day. One morning when the sun came out, his eyes lit up with joy! 0He wanted to go outside but couldn't find anything else. So he decided to take off his hat and coat so that no one could see him. The old man smiled at Jimmy's face and said “I'm glad you like it”. Jimmy was happy again and thanked the old man’\n\nThe pre-trained model as is does not generate text that resembles a story whatsoever. The fully fine-tuned model’s generated text is somewhat coherent and it resembles a story although elements of it still don’t make sense."
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#fine-tuning-eleutheraipythia-70m-with-lora",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#fine-tuning-eleutheraipythia-70m-with-lora",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Fine-Tuning EleutherAI/pythia-70m with LoRA",
    "text": "Fine-Tuning EleutherAI/pythia-70m with LoRA\nSince a LoRA model has less trainable parameters, I can increase the dataset size for the training. I’ll also see if I can train for more epochs without overfitting since I’m using more data.\n\ntrain_ds = ds['train'].select(range(256000))\neval_ds = ds['validation'].select(range(64000))\n\n\nlora_config = LoraConfig(task_type=TaskType.CAUSAL_LM)\nmodel = AutoModelForCausalLM.from_pretrained(model_nm)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ntrainable params: 98,304 || all params: 70,524,928 || trainable%: 0.13938901149959346\n\nmodel\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): GPTNeoXForCausalLM(\n      (gpt_neox): GPTNeoXModel(\n        (embed_in): Embedding(50304, 512)\n        (layers): ModuleList(\n          (0-5): 6 x GPTNeoXLayer(\n            (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (attention): GPTNeoXAttention(\n              (rotary_emb): RotaryEmbedding()\n              (query_key_value): Linear(\n                in_features=512, out_features=1536, bias=True\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=512, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (mlp): GPTNeoXMLP(\n              (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n              (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n              (act): GELUActivation()\n            )\n          )\n        )\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n    )\n  )\n)\n\n\n\ntrainer = get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr=5e-4, bs=16, epochs=4)\n\n\ntrainer.train()\n\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\n\n\n    \n      \n      \n      [32000/32000 2:00:46, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n    \n  \n  \n    \n      1\n      2.616000\n      2.614058\n    \n    \n      2\n      2.575500\n      2.570585\n    \n    \n      3\n      2.605000\n      2.547680\n    \n    \n      4\n      2.493900\n      2.540338\n    \n  \n\n\n\nTrainOutput(global_step=32000, training_loss=2.621225409567356, metrics={'train_runtime': 7252.3347, 'train_samples_per_second': 141.196, 'train_steps_per_second': 4.412, 'total_flos': 2.33350953959424e+16, 'train_loss': 2.621225409567356, 'epoch': 4.0})\n\n\n\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n\n\n    \n      \n      \n      [2000/2000 03:32]\n    \n    \n\n\nPerplexity: 12.68\n\n\n\nprompt = \"Once upon a time,\"\ngenerator = pipeline('text-generation', model=trainer.model.to('cpu'), tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n“Once upon a time, there was an old man who lived in the park. He had many friends and loved to play with him every day at his house all night long! One morning he decided that it would be best for everyone else because they were so happy together as each other on their own one by another’s side of town hall or doorstep…so when something unexpected happened she started playing outside - her mommy said no but could help herself out here until someone came up close enough.. She”\n\nThe generated text resembles a story and is a bit coherent for the first couple of sentences before it stops making sense in the second half.\nHere is a comparison of the full-fine-tuning (Full FT) vs. LoRA fine-tuning (LoRA FT) process on the EleutherAI/pythia-70m:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nParameters\nTraining Set\nValidation Set\nPerplexity\nBatch Size\nEpochs\nTrain Steps\nTrain Time (Minutes)\n\n\n\n\nFull FT\n70.4M\n240k\n60k\n8.51\n16\n3\n22500\n100\n\n\nLoRA FT\n98k\n256k\n64k\n12.68\n16\n4\n32000\n120"
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#generating-text-from-the-pre-trained-tinystories-model",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#generating-text-from-the-pre-trained-tinystories-model",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Generating Text from the Pre-Trained TinyStories Model",
    "text": "Generating Text from the Pre-Trained TinyStories Model\nThe authors of the paper that this dataset comes released their fine-tuned model on HuggingFace, so I’ll use it to generate text to see how a state-of-the-art TinyStories model performs:\n\nmodel_nm = \"EleutherAI/gpt-neo-125M\"\ntokz = AutoTokenizer.from_pretrained(model_nm)\ntokz.add_special_tokens({'pad_token': '[PAD]'})\ndef tok_func(x): return tokz(x[\"text\"])\n\n\ngenerator = pipeline('text-generation', model='roneneldan/TinyStories-33M', tokenizer=tokz)\ngenerator(prompt, max_length = 100, repetition_penalty=1.2)\n\nGenerated text:\n\n‘Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine and pick flowers. One day, she found an ancient book on her porch. It had lots of pictures inside that looked very old.opened the book and saw many words written around it. But then, she heard a loud noise coming from the house next door. She went to investigate and found out that someone had broken into their home. ran back to’\n\nThe model is so good! It can hold a consistent, coherent theme in story format for multiple sentences."
  },
  {
    "objectID": "posts/2023-09-01-lora-fine-tuning/index.html#final-thoughts",
    "href": "posts/2023-09-01-lora-fine-tuning/index.html#final-thoughts",
    "title": "Fine-tuning a Language Model Using LoRA",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nI’m happy to have got this all to work, as that alone was a big step in my learning process. This is the first time I have trained a causal language model using HuggingFace. One thought to close out this exercise: Would restructuring the data help? Currently the dataset has text values like “Summary:” and “Features:”, which are the prompts used by the TinyStories paper authors to generate stories using GPT-3.5 and 4. Perhaps removing these prompts from the dataset and keeping only the story text would help improve the model. I’ll explore this in a future exercise.\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "",
    "text": "In this blog post, I will work through the textbook Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari. I’ll be posting excerpts (sometimes verbatim), notes (paraphrased excerpts) and my thoughts about the content (and any other related reading that I come across while understanding the textbook topics).\nStarting in Chapter 2, I recreate the plots shown in the textbook first by following their provided code on the supplemental website, and then by recreating it using tidyverse and ggplot. This process gives me practice reading and understanding other people’s code, as well as practice applying tidyverse and ggplot syntax and principles to produce a similar result with different code.\nBefore I get into the reading, I’ll note that I have not posted my solutions to exercises to honor the request of one of the authors, Aki Vehtari:\nHowever, to learn by writing, I will write about the process of doing the exercises, the results I got, and what I learned from it all."
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html#what-you-should-be-able-to-do-after-reading-and-working-through-this-book",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html#what-you-should-be-able-to-do-after-reading-and-working-through-this-book",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "What you should be able to do after reading and working through this book",
    "text": "What you should be able to do after reading and working through this book\n\nPart 1: Review key tools and concepts in mathematics, statistics and computing\n\nChapter 1: Have a sense of the goals and challenges of regression\nChapter 2: Explore data and be aware of issues of measurement and adjustment\nChapter 3: Graph a straight line and know some basic mathematical tools and probability distributions\nChapter 4: Understand statistical estimation and uncertainty assessment, along with the problems of hypothesis testing in applied statistics\nChapter 5: Simulate probability models and uncertainty about inferences and predictions\n\nPart 2: Build linear regression models, use them in real problems, and evaluate their assumptions and fit to data\n\nChapter 6: Distinguish between descriptive and causal interpretations of regression, understanding these in historical context\nChapter 7: Understand and work with simple linear regression with one predictor\nChapter 8: Gain a conceptual understanding of least squares fitting and be able to perform these fits on the computer\nChapter 9: Perform and understand probabilistic and simple Bayesian information aggregation, and be introduced to prior distributions and Bayesian inference\nChapter 10: Build, fit, and understand linear models with multiple predictors\nChapter 11: Understand the relative importance of different assumptions of regression models and be able to check models and evaluate their fit to data\nChapter 12: Apply linear regression more effectively by transforming and combining predictors\n\nPart 3: Build and work with logistic regression and generalized linear models\n\nChapter 13: Fit, understand, and display logistic regression models for binary data\nChapter 14: Build, understand and evaluate logistic regressions with interactions and other complexities\nChapter 15: Fit, understand, and display generalized linear models, including the Poisson and negative binomial regression, ordered logistic regression, and other models\n\nPart 4: Design studies and use data more effectively in applied settings\n\nChapter 16: Use probability theory and simulation to guide data-collection decisions, without falling into the trap of demanding unrealistic levels of certainty\nChapter 17: Use poststratification to generalize from sample to population, and use regression models to impute missing data\n\nPart 5: Implement and understand basic statistical designs and analyses for causal inference\n\nChapter 18: Understand assumptions underlying causal inference with a focus on randomized experiments\nChapter 19: Perform causal inference in simple setting using regressions to estimate treatments and interactions\nChapter 20: Understand the challenges of causal inference from observational data and statistical tools for adjusting for differences between treatment and control groups\nChapter 21: Understand the assumptions underlying more advanced methods that use auxiliary variables or particular data structures to identify causal effects, and be able to fit these models to data\n\nPart 6: Become aware of more advanced regression models\n\nChapter 22: Get a sense of the directions in which linear and generalized linear models can be extended to attack various classes of applied problems\n\nAppendixes\n\nAppendix A: Get started in the statistical software R, with a focus on data manipulation, statistical graphics, and fitting using regressions\nAppendix B: Become aware of some important ideas in regression workflow\n\n\nAfter working through the book, you should be able to fit, graph, understand, and evaluate linear and generalized linear models and use these model fits to make predictions and inferences about quantities of interest, including causal effects of treatments and exposures."
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-1-overview",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-1-overview",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "Chapter 1: Overview",
    "text": "Chapter 1: Overview\nAlternate title: Prediction as a unifying theme in statistics and causal inference\n\n1.1 The three challenges of statistics\n\nGeneralizing from sample to population\nGeneralizing from treatment to support group\nGeneralizing from observed measurements to the underlying constructs of interest\n\nPrediction: for new people or new items that are not in the sample, future outcomes under differently potentially assigned treatments, and underlying constructs of interest, if they could be measured exactly.\nKey skills you should learn from this book:\n\nUnderstanding regression models: mathematical models for predicting an outcome variable from a set of predictors, starting with straight-line fits and moving to various nonlinear generalizations\nConstructing regression models: with many options involving the choice of what variables to include and how to transform and constrain them\nFitting regression models to data\nDisplaying and interpreting the results\n\nInference: using mathematical models to make general claims from particular data.\n\n\n1.2 Why learn regression?\nRegression: a method that allows researchers to summarize how predictions or average values of an outcome vary across individuals defined by a set of predictors.\n\n# load the data\ndata(hibbs)\n\n# make a scatterplot\nplot(\n  hibbs$growth, \n  hibbs$vote, \n  xlab = \"Average recent growth in personal income\",\n  ylab = \"Incumbent party's vote share\")\n\n\n\n\n\n# estimate the regression: y = a + bx + error\nM1 <- stan_glm(vote ~ growth, data=hibbs)\n\n\n# make a scatter plot\nplot(\n  hibbs$growth, \n  hibbs$vote, \n  xlab = \"Average recent growth in personal income\",\n  ylab = \"Incumbent party's vote share\")\n\n# add fitted line to the graph\nabline(coef(M1), col = 'gray')\n\n\n\n\n\n# display the fitted model\nprint(M1)\n\nstan_glm\n family:       gaussian [identity]\n formula:      vote ~ growth\n observations: 16\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 46.3    1.7  \ngrowth       3.0    0.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.9    0.7   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\ny = 46.3 + 3.0x\nMAD: Median Absolute Deviations.\nsigma: the scale of the variation in the data explained by the regression model (the scatter of points above and below the regression line). The linear model predicts vote share roughly to an accuracy of 3.9 percentage points.\nSome of the most important uses of regression:\n\nPrediction: Modeling existing observations or forecasting new data.\nExploring associations: Summarizing how well one variable, or set of variables, predicts the outcome. One can use a model to explore associations, stratifications, or structural relationships between variables.\nExtrapolation: Adjusting for known differences between the sample (observed data) and a population of interest.\nCausal inference: Estimating treatment effects. A key challenge of causal inference is ensuring that treatment and control groups are similar, on average, before exposure to treatment, or else adjusting for differences between these groups.\n\n\n\n1.3 Some examples of regression\n\nEstimating public opinion from an opt-in internet survey\n\nA characteristic problem of big data: a very large sample, relatively inexpensive to collect, but not immediately representative of the larger population\n\n\n\nA randomized experiment on the effect of an educational television program\n\n\nEstimating the effects of United Nations peacekeeping, using pre-treatment variables to adjust for differences between treatment and control groups\n\nSelection bias: perhaps peacekeepers chose the easy cases, which would explain the difference in outcomes. The treatment–peacekeeping–was not randomly assigned. They had an observational study rather than an experiment, where we must do our best to adjust for pre-treatment differences between the treatment and control groups.\nCensoring: certain ranges of data cannot be observed (e.g. countries where civil war had not yet returned by the time data collection ended)\nWhen adjusting for badness, peacekeeping was performed in tougher conditions, on average. As a result, the adjustment increases the estimated beneficial effects of peacekeeping, at least during the study.\n\n\n\nEstimating the effects of gun laws, and the difficulty of inference using regression with a large number of predictors\n\nIn this sort of regression with 50 data points and 30 predictors and no prior information to guide the inference, the coefficient estimates will be hopelessly noisy and compromised by dependence among predictors.\nThe treatments are observational and not externally applied. There is no reason to believe that the big differences in gun-related deaths between states are mostly attributable to particular policies.\n\n\n\nComparing the peacekeeping and gun-control studies\n\nIn both cases, policy conclusions have been drawn from observational data, using regression modeling to adjust for differences between treatment and control groups.\nIt is more practical to perform adjustments when there is a single goal (peacekeeping study)\n\nparticular concern that the UN might be more likely to step in when the situation on the ground was not so bad\nthe data analysis found the opposite\nthe measure of badness is constructed based on particular measured variables and so it is possible that there are important unmeasured characteristics that would cause adjustments to go the other way\n\nThe gun-control model adjusts for many potential causal variables at once\n\nThe effect of each law is estimated conditional on all others being held constant (not realistic—no particular reason for their effects to add up in a simple manner)\nThe comparison is between states, which vary in systematic ways (it is not at all clear that a simple model can hope to adjust for the relevant differences)\nRegression analysis was taken naively to be able to control for variation and give valid causal inference from observational data\n\n\n\n\n\n1.4 Challenges in building, understanding, and interpreting regressions\nTwo different ways in which regression is used for causal inference: estimating a relationship and adjusting for background variables.\n\nRegression to estimate a relationship of interest\nRandomization: a design in which people—or, more generally, experimental units—are randomly assigned to treatment or control groups.\nThere are various ways to attain approximate comparability of treatment and control groups, and to adjust for known or modeled differences between the groups.\nWe assume the comparability of the groups assigned to different treatments so that a regression analysis predicting the outcome given the treatment gives us a direct estimate of the causal effect.\nIt is always possible to estimate a linear model, even if it does not fit the data.\nInteractions: treatment effects that vary as a function of other predictors in a model.\n\nExample: the relation between cancer rate and radon is different for smokers and nonsmokers. The effect of radon is assumed to be linear but with an interaction with smoking.\n\n\n\nRegression to adjust for differences between treatment and control groups\nIn most real-world causal inference problems, there are systematic differences between experimental units that receive treatment and control. In such settings it is important to adjust for pre-treatment differences between the groups, and we can use regression to do this.\nAdjusting for background variables is particularly important when there is imbalance so that the treated and control groups differ on key pre-treatment predictors.\nThe estimated treatment effect is necessarily model based.\n\n\nInterpreting coefficients in a predictive model\nA model fit to survey data: earnings = 11000 + 1500 * (height - 60) + error, with errors in the range of mean of 0 and standard deviation of 22000.\n\nPrediction: useless for forecasting because errors are too large\nExploring an association: best fit for this example, as the estimated slope is positive, can lead to further research to study reasons taller people earn more than shorter people\nSampling inference: the regression coefficients can be interpreted directly to the extent that people in the survey are a representative sample of the population of interest\nCausal inference: height is not a randomly assigned treatment. Tall and short people may differ in many other ways.\n\n\n\nBuilding, interpreting and checking regression models\nStatistical analysis cycles through four steps:\n\nModel building\nModel fitting\nUnderstanding model fits\nCriticism\n\nAccept that we can learn from statistical analysis—we can generalize from sample to population, from treatment to control, and from observed measurements to underlying constructs of interest—even while these inferences can be flawed.\nOverinterpretation of noisy data: the gun control study took existing variations among states and too eagerly attributed it to available factors.\nNo study is perfect.\nWe should recognize challenges in extrapolation and then work to adjust for them.\n\n\n\n1.5 Classical and Bayesian Inference\nThree concerns related to fitting models to data and using models to make predictions:\n\nwhat information is being used in the estimation process\nwhat assumptions are being made\nhow estimates and predictions are interpreted, in a classical or Bayesian framework\n\n\nInformation\n\nThe starting point for any regression problem is data on an outcome variable y and one or more predictors x.\nInformation should be available on what data were observed at all\nWe typically have prior knowledge coming from sources other than the data at hand. Where local data are weak it would be foolish to draw conclusions without using prior knowledge\n\n\n\nAssumptions\n\nThree sorts of assumptions that are essential to any regression model of an outcome y given predictors x.\n\nThe functional form of the relation between x and y\nWhere the data comes from (strongest assumptions tend to be simple and easy to understand, weaker assumptions, being more general, can also be more complicated)\nReal-world relevance of the measured data\n\nThe interpretation of a regression of y on x depends also on the relation between the measured x and the underlying predictors of interest, and on the relation between the measured y and the underlying outcomes of interest\n\n\n\n\n\nClassical inference\nThe traditional approach to statistical analysis is based on summarizing the information in the data, not using prior information, but getting estimates and predictions that have well-understood statistical properties, low bias and low variance.\nUnbiasedness: estimates should be true on average\nCoverage: confidence intervals should cover the true parameter value 95% of the time\nConservatism: sometimes data are weak and we can’t make strong statements, but we’d like to be able to say, at least approximately, that our estimates are unbiased and our intervals have the advertised coverage.\nIn classical statistics there should be a clear and unambiguous (“objective”) path from data to inferences, which in turn should be checkable, at least in theory, based on their frequency properties.\n\n\nBayesian inference\nIncorporates prior information into inferences, going beyond the goal of merely summarizing existing data. The analysis gives more reasonable results and can be used to make direct predictions about future outcomes and about the results of future experiments.\nThe prior distribution represents the arena over which any predictions will be evaluated.\nWe have a choice: classical inference, leading to pure summaries of data which can have limited value as predictions; or Bayesian inference, which in theory can yield valid predictions even with weak data, but relies on additional assumptions.\nAll Bayesian inferences are probabilistic and thus can be represented by random simulations. For this reason, whenever we want to summarize uncertainty in estimation beyond simple confidence intervals, and whenever we want to use regression models for predictions, we go Bayesian.\n\n\n\n1.6 Computing least squares and Bayesian regression\nIn general, we recommend using Bayesian inference for regression\n\nIf prior information is available, you can use it\nif not, Bayesian regression with weakly informative default priors still has the advantage of yielding stable estimates and producing simulations that enable you to express inferential and predictive uncertainty (estimates with uncertainties and probabilistic predictions or forecasts)\n\nBayesian regression in R:\nfit <- stan_glm(y ~ x, data=mydata)\n\n# stan_glm can run slow for large datasets, make it faster by running in optimizing mode\nfit <- stan_glm(y ~ x, data=mydata, algorithm=\"optimizing\")\nleast squares regression (classical statistics)\nfit <- lm(y ~ x, data=mydata)\nBayesian and simulation approaches become more important when fitting regularized regression and multilevel models.\n\n\n1.8 Exercises\nIt took me 8-9 days to complete the 10 exercises at the end of this chapter. A few observations:\n\nExperiment design, even with simple paper helicopters, still requires time, effort and attention to detail. Even then, a well thought out and executed plan can still end up with uninspiring results. It is therefore the process of experimentation and not the result where I learned the most. It was important to continue experimentation (as it resulted in finding a high-performing copter) even after my initial approach didn’t seem to work as I had thought it would.\nFinding good and bad examples of research is hard on your own! I relied on others existing work where they explicitly called out (and explained) good and bad research, sometimes with great detail, in order to answer exercises 1.7 and 1.8.\nThink about the problem in real life! I don’t know if my answer to 1.9 is correct, but it helped to think about the physical phenomenon that was taking place (a helicopter falling through air, pulled by gravity) in order to come up with a better model fit than the example provided. I found that thinking of physical constraints of the real world problem helped narrow the theoretical model form.\nPick something you’re interested in and is worth your while! For exercise 1.10, the dataset I have chosen is the NFL Play-by-Play dataset, as I’m already working on a separate project where I’ll be practicing writing SQL queries to analyze the data. Play-by-play data going back to 1999 will provide me with ample opportunities to pursue different modeling choices. My interest in football will keep me dedicated through the tough times."
  },
  {
    "objectID": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-2-data-and-measurement",
    "href": "posts/2023-05-17-regression-and-other-stories/index.html#chapter-2-data-and-measurement",
    "title": "Regression and Other Stories - Notes and Excerpts",
    "section": "Chapter 2: Data and Measurement",
    "text": "Chapter 2: Data and Measurement\nIn this book we will be:\n\nfitting lines and curves to data.\nmaking comparisons and predictions and assessing our uncertainties in the resulting inferences.\ndiscuss the assumptions underlying regression models, methods for checking these assumptions, and directions for improving fitted models.\ndiscussing the challenges of extrapolating from available data to make causal inferences and predictions for new data.\nusing computer simulations to summarize the uncertainties in our estimates and predictions.\n\n\n2.1 Examining where data comes from\nI’ll recreate the charts shown in the textbook by following the code provided in the website supplement to the book, using tidyverse where it makes sense:\n\n# load the Human Development Index data\n# start using DT to produce better table outputs\nlibrary(DT)\nhdi <- read.table(\"../../../ROS_data/HDI/data/hdi.dat\", header=TRUE)\nDT::datatable(head(hdi))\n\n\n\n\n\n\n\n# load income data via the votes data\nlibrary(foreign)\nvotes <- read.dta(\"../../../ROS_data/HDI/data/state vote and income, 68-00.dta\")\nDT::datatable(head(votes), options=list(scrollX=TRUE))\n\n\n\n\n\n\n\n# preprocess\nincome2000 <- votes[votes[,\"st_year\"]==2000, \"st_income\"]\n\n# it seems like the income dataset doesn't have DC\n# whereas HDI does\n# so we're placing an NA for DC in the income list\nstate.income <- c(income2000[1:8], NA, income2000[9:50])\nstate.abb.long <- c(state.abb[1:8], \"DC\", state.abb[9:50])\nstate.name.long <- c(state.name[1:8], \"Washington, D.C.\", state.name[9:50])\nhdi.ordered <- rep(NA, 51)\ncan <- rep(NA, 51)\n\nfor (i in 1:51) {\n  ok <- hdi[,\"state\"]==state.name.long[i]\n  hdi.ordered[i] <- hdi[ok, \"hdi\"]\n  can[i] <- hdi[ok, \"canada.dist\"]\n}\nno.dc <- state.abb.long != \"DC\"\n\n\n# plot average state income and Human Development Index\npar(mar=c(3, 3, 2.5, 1), mgp=c(1.5, .2, 0), tck=-0.01, pty='s')\nplot(\n  state.income, \n  hdi.ordered, \n  xlab=\"Average state income in 2020\",\n  ylab=\"Human Development Index\",\n  type=\"n\")\n\ntext(state.income, hdi.ordered, state.abb.long)\n\n\n\n\nThe pattern between HDI numbers and average state income is strong and nonlinear. If anyone is interested in following up on this, we suggest looking into South Carolina and Kentucky, which are so close in average income and so far apart on HDI.\n\n# plot rank of average state income and Human Development Index\npar(mar=c(3, 3, 2.5, 1), mgp=c(1.5, 0.2, 0), tck=-0.01, pty='s')\nplot(\n  rank(state.income[no.dc]), \n  rank(hdi.ordered[no.dc]),\n  xlab=\"Rank of average state income in 2000\",\n  ylab=\"Rank of Human Development Index\", \n  type=\"n\")\n\ntext(rank(state.income[no.dc]), rank(hdi.ordered[no.dc]), state.abb)\n\n\n\n\nThere is a high correlation between the ranking of HDI and state income.\nI will redo the above data processing and plotting with tidyverse and ggplot for practice:\n\n# plot average state income and Human Development Index\n# using tidyverse and ggplot\nmerged_data <- hdi %>% dplyr::left_join(\n  votes %>% filter(st_year==2000),\n  by = c(\"state\" = \"st_state\")\n) \n\np <- ggplot(\n  merged_data,\n  aes(x = st_income, y = hdi, label =  st_stateabb)\n) + theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    panel.background = element_rect(fill = 'white'),\n    aspect.ratio = 1\n  ) +\n  labs(\n    x = \"Average state income in 2020\",\n    y = \"Human Development Index\"\n  )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\np + geom_text()\n\nWarning: Removed 1 rows containing missing values (`geom_text()`).\n\n\n\n\n\nNone of the tidyverse methods I found (row_number, min_rank, dense_rank, percent_rank, cume_dist, or ntile) work quite like rank (where “ties” are averaged), so I continued using rank for this plot:\n\n# plot rank of average state income and Human Development Index\np <- ggplot(\n  merged_data %>% filter(state != 'Washington, D.C.'), \n  aes(x = rank(st_income), y = rank(hdi), label = st_stateabb),\n  ) + theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    panel.background = element_rect(fill = 'white'),\n    aspect.ratio = 1\n  ) +\n  labs(\n    x = \"Rank of average state income in 2000\",\n    y = \"Rank of Human Development Index\"\n  )\n\np + geom_text()\n\n\n\n\nThe relevance of this example is that we were better able to understand the data by plotting them in different ways.\n\nDetails of measurement can be important\nIn the code chunks below, I will recreate the plots in Figure 2.3 in the text (distribution of political ideology by income and distribution of party identification by income)., using their code provided on the supplemental website.\n\npew_data_path <- file.path(data_root_path, \"Pew/data/pew_research_center_june_elect_wknd_data.dta\")\npew_pre <- read.dta(pew_data_path)\nn <- nrow(pew_pre)\nDT::datatable(head(pew_pre), options=list(scrollX=TRUE))\n\n\n\n\n\n\n\n# weight\npop.weight0 <- pew_pre$weight\n\nhead(unique(pop.weight0))\n\n[1] 1.326923 0.822000 0.493000 0.492000 2.000000 1.800000\n\n\n\n# income (1-9 scale)\ninc <- as.numeric(pew_pre$income)\n\n# remove \"dk/refused\" income level\ninc[inc==10] <- NA\n\nlevels(pew_pre$income)\n\n [1] \"less than $10,000\" \"$10,000-$19,999\"   \"$20,000-$29,999\"  \n [4] \"$30,000-$39,999\"   \"$40,000-$49,000\"   \"$50,000-$74,999\"  \n [7] \"$75,000-$99,999\"   \"$100,000-$149,999\" \"$150,000+\"        \n[10] \"dk/refused\"       \n\nunique(inc)\n\n [1]  6  8 NA  4  7  5  9  1  3  2\n\n\n\n# I believe these are the midpoints of the income levels\nvalue.inc <- c(5,15,25,35,45,62.5,87.5,125,200)\n\n# maximum income value\nn.inc <- max(inc, na.rm = TRUE)\nn.inc\n\n[1] 9\n\n\n\n# party id\npid0 <- as.numeric(pew_pre[,\"party\"])\nlean0 <- as.numeric(pew_pre[,\"partyln\"])\n\nlevels(pew_pre[,\"party\"])\n\n[1] \"missing/not asked\" \"republican\"        \"democrat\"         \n[4] \"independent\"       \"no preference\"     \"other\"            \n[7] \"dk\"               \n\nlevels(pew_pre[,\"partyln\"])\n\n[1] \"missing/not asked\" \"lean republican\"   \"lean democrat\"    \n[4] \"other/dk\"         \n\nunique(pid0)\n\n[1]  3  2  4  7  5  6 NA\n\nunique(lean0)\n\n[1] NA  3  4  2\n\n\n\n# assigning new integers for party id\npid <- ifelse(pid0==2, 5,  # Republican\n         ifelse(pid0==3, 1,  # Democrat\n         ifelse(lean0==2, 4, # Lean Republican\n         ifelse(lean0==4, 2, # Lean Democrat\n         3)))) # Independent\n\npid.label <- c(\"Democrat\", \"Lean Dem.\", \"Independent\", \"Lean Rep.\", \"Republican\")\nn.pid <- max(pid, na.rm=TRUE)\n\n\n# ideology\nideology0 <- as.numeric(pew_pre[,\"ideo\"])\n\nlevels(pew_pre[,\"ideo\"])\n\n[1] \"missing/not asked\" \"very conservative\" \"conservative\"     \n[4] \"moderate\"          \"liberal\"           \"very liberal\"     \n[7] \"dk/refused\"       \n\nunique(ideology0)\n\n[1]  5  4  3  6  2  7 NA\n\n\n\n# assign new integers for ideology\nideology <- ifelse(ideology0==2, 5, # Very conservative\n              ifelse(ideology0==3, 4, # Conservative\n              ifelse(ideology0==6, 1, # Very liberal\n              ifelse(ideology0==5, 2, # Liberal\n              3)))) # Moderate\nideology.label <- c(\"Very liberal\", \"Liberal\", \"Moderate\", \"Conservative\", \"Very conservative\")\nn.ideology <- max(ideology, na.rm=TRUE)\n\n\n# plot settings\npar(mar=c(3,2,2.5,1), mgp=c(1.5, .7, 0), tck=-0.01)\n\n# this creates an empty plot\nplot(c(1,n.inc), c(0,1), xaxs=\"i\", yaxs=\"i\", type=\"n\", xlab=\"\", ylab=\"\", xaxt=\"n\", yaxt=\"n\")\n\n# add x-axis tick marks with empty string labels\naxis(1, 1:n.inc, rep(\"\", n.inc))\n\n# add x-axis labels\naxis(1, seq(1.5, n.inc-.5, length=3), c(\"Low income\", \"Middle income\", \"High income\"), tck=0)\n\n# add y-axis tick marks and labels\naxis(2, c(0, 0.5, 1), c(0, \"50%\", \"100%\"))\n\ncenter <- floor((1 + n.inc) / 2)\n\nincprop <- array(NA, c(n.pid + 1, n.inc))\n\nincprop[n.pid + 1,] <- 1\n\n\nfor (i in 1:n.pid) {\n  for (j in 1:n.inc) {\n    incprop[i, j] <- weighted.mean((pid<i)[inc==j], pop.weight0[inc==j], na.rm = TRUE)\n  }\n}\n\nfor (i in 1:n.pid){\n  polygon(c(1:n.inc, n.inc:1), c(incprop[i,], rev(incprop[i+1,])), col=paste(\"gray\", 40+10*i, sep=\"\"))\n  lines(1:n.inc, incprop[i,])\n  text(center, mean(incprop[c(i,i+1),center]), pid.label[i])\n}\nmtext(\"Self-declared party identification, by income\", side=3, line=1, cex=1.2)\n\n\n\n\n\npar(mar=c(3,2,2.5,1), mgp=c(1.5,.7,0), tck=-.01)\nplot(c(1,n.inc), c(0,1), xaxs=\"i\", yaxs=\"i\", type=\"n\", xlab=\"\", ylab=\"\", xaxt=\"n\", yaxt=\"n\")\naxis(1, 1:n.inc, rep(\"\",n.inc))\naxis(1, seq(1.5,n.inc-.5,length=3), c(\"Low income\", \"Middle income\", \"High income\"), tck=0)\naxis(2, c(0,.5,1), c(0,\"50%\",\"100%\"))\ncenter <- floor((1+n.inc)/2)\nincprop <- array(NA, c(n.ideology+1,n.inc))\nincprop[n.ideology+1,] <- 1\nfor (i in 1:n.ideology){\n  for (j in 1:n.inc){\n    incprop[i,j] <- weighted.mean((ideology<i)[inc==j], pop.weight0[inc==j], na.rm=TRUE)\n  }\n}\nfor (i in 1:n.ideology){\n  polygon(c(1:n.inc, n.inc:1), c(incprop[i,], rev(incprop[i+1,])), col=paste(\"gray\", 40+10*i, sep=\"\"))\n  lines(1:n.inc, incprop[i,])\n  text(center, mean(incprop[c(i,i+1),center]), ideology.label[i])\n}\nmtext(\"Self-declared political ideology, by income\", side=3, line=1, cex=1.2)\n\n\n\n\nI’ll recreate the two plots using tidyverse and ggplot. I referenced this article by the Pew Research Center which shows how to calculated weighted estimates.\n\npew_pre <- pew_pre %>%\n  mutate(\n    # create an integer column for income levels\n    inc = as.numeric(income),\n    # remove \"dk/refuse\" value for income\n    inc = case_when(\n      inc == 10 ~ NA,\n      TRUE ~ inc\n    ),\n    # set political party integer column\n    pid0 = as.numeric(party),\n    # set lean integer column\n    lean0 = as.numeric(partyln),\n    # set ideology integer column\n    ideology0 = as.numeric(ideo),\n    # re-assign party values\n    pid = case_when(\n      pid0 == 2 ~ 5, # Repubican\n      pid0 == 3 ~ 1, # Democrat\n      lean0 == 2 ~ 4, # Lean Republican\n      lean0 == 4 ~ 2, # Lean Democrat\n      is.na(pid0) ~ NA,\n      TRUE ~ 3 # Independent\n    ),\n    # reassign ideology values\n    ideology = case_when(\n      ideology0 == 2 ~ 5, # Very conservative\n      ideology0 == 3 ~ 4, # Conservative\n      ideology0 == 6 ~ 1, # Very liberal\n      ideology0 == 5 ~ 2, # Liberal\n      is.na(ideology0) ~ NA,\n      TRUE ~ 3 # Moderate\n    )\n  )\n\n# constants\nn_inc <- max(pew_pre$inc, na.rm = TRUE)\nn_pid <- max(pew_pre$pid, na.rm = TRUE)\nn_ideology <- max(pew_pre$ideology, na.rm = TRUE)\n\n\n# calculate income proportions using population weight\ninc_totals <- pew_pre %>% group_by(\n  inc\n) %>% summarize(\n  total_inc = n()\n)\n\npid_weighted_estimates <- pew_pre %>% dplyr::left_join(\n  inc_totals\n) %>% group_by(\n  inc,\n  pid\n) %>% summarise(\n  weighted_n = sum(weight)\n) %>% mutate(\n  weighted_group_size = sum(weighted_n),\n  weighted_estimate = weighted_n / weighted_group_size\n) %>% arrange(\n  inc\n)\n\nJoining with `by = join_by(inc)`\n`summarise()` has grouped output by 'inc'. You can override using the `.groups`\nargument.\n\n\n\nDT::datatable(pid_weighted_estimates)\n\n\n\n\n\n\n\nggplot(pid_weighted_estimates, aes(x=inc, y=weighted_estimate)) + \n  geom_area(aes(group = pid, fill = pid), position = position_stack(reverse = TRUE), show.legend = FALSE) +\n  annotate(\"text\", x=5, y=.2, label=\"Democrat\", color=\"white\") + \n  annotate(\"text\", x=5, y=.425, label=\"Lean Dem.\", color=\"white\") + \n  annotate(\"text\", x=5, y=.55, label=\"Independent\", color=\"white\") + \n  annotate(\"text\", x=5, y=.68, label=\"Lean Rep.\") + \n  annotate(\"text\", x=5, y=.87, label=\"Republican\") + \n  scale_x_continuous(\"Average Income Level\", breaks=c(1,5,9), labels = c(\"Low Income\", \"Middle Income\", \"High Income\"), , expand = c(0, 0)) +\n  scale_y_continuous(\"Proportion of Sample\", breaks=c(0,.5, 1), labels = c(\"0%\", \"50%\", \"100%\"), expand = c(0, 0)) +\n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  ggtitle(\"Party Identification by Income Levels\")\n\nWarning: Removed 6 rows containing non-finite values (`stat_align()`).\n\n\n\n\n\n\n# ideology data prep\nideo_weighted_estimates <- pew_pre %>% dplyr::left_join(\n  inc_totals\n) %>% group_by(\n  inc,\n  ideology\n) %>% summarise(\n  weighted_n = sum(weight)\n) %>% mutate(\n  weighted_group_size = sum(weighted_n),\n  weighted_estimate = weighted_n / weighted_group_size\n) %>% arrange(\n  inc\n)\n\nJoining with `by = join_by(inc)`\n`summarise()` has grouped output by 'inc'. You can override using the `.groups`\nargument.\n\n\n\nDT::datatable(ideo_weighted_estimates)\n\n\n\n\n\n\n\nggplot(ideo_weighted_estimates, aes(x=inc, y=weighted_estimate)) + \n  geom_area(aes(group = ideology, fill = ideology), position = position_stack(reverse = TRUE), show.legend = FALSE) +\n  annotate(\"text\", x=5, y=.03, label=\"Very Liberal\", color=\"white\", size=3) + \n  annotate(\"text\", x=5, y=.13, label=\"Liberal\", color=\"white\", size=3) + \n  annotate(\"text\", x=5, y=.4, label=\"Moderate\", color=\"white\", size=3) + \n  annotate(\"text\", x=5, y=.75, label=\"Conservative\", size=3) + \n  annotate(\"text\", x=5, y=.95, label=\"Very Conservative\", size=3) + \n  scale_x_continuous(\"Average Income Level\", breaks=c(1,5,9), labels = c(\"Low Income\", \"Middle Income\", \"High Income\"), , expand = c(0, 0)) +\n  scale_y_continuous(\"Proportion of Sample\", breaks=c(0,.5, 1), labels = c(\"0%\", \"50%\", \"100%\"), expand = c(0, 0)) +\n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  ggtitle(\"Ideology by Income Levels\")\n\nWarning: Removed 6 rows containing non-finite values (`stat_align()`).\n\n\n\n\n\nOverall, I think the plots I made look pretty similar to the textbook’s. The main bottleneck was learning how to calculate proportions based on weighted estimates. Once that took shape, I was able to figure out how to present the data visually in a way that resembles the authors’ approach.\nThe figure “Ideology by Income Levels” shows that the proportion of political liberals, moderates, and conservatives is about the same for all income levels. The figure “Party Identification by Income Levels” shows a strong relation between income and Republican partisanship, at least as of 2008. The partisanship and ideology example is a reminder that even very similar measures can answer quite different questions.\n\n\n\n2.2 Validity and reliability\nData analysis reaches a dead end if we have poor data.\nTwo reasons for discussing measurement:\n\nwe need to understand what our data actually mean, otherwise we cannot extract the right information\nlearning about accuracy, reliability and validity will set a foundation for understanding variance, correlation, and error, which will be useful in setting up linear models in the forthcoming chapters\n\nThe property of being precise enough is a combination of the properties of the scale and what we are trying to use it for.\nIn social science, the way to measure what we are trying to measure is not as transparent as it is in everyday life.\nOther times, the thing we are trying to measure is pretty straightforward, but a little bit fuzzy, and the ways to tally it up aren’t obvious.\nSometimes we are trying to measure something that we all agree has meaning, but which is subjective for every person and does not correspond to a “thing” we can count or measure with a ruler.\nAttitudes are private; you can’t just weigh them or measure their widths.\nIt can be helpful to take multiple measurements on an underlying construct of interest.\n\nValidity\nA measure is valid to the degree that it represents what you are trying to measure. Asking people how satisfied they are with some government service might not be considered a valid measure of the effectiveness of that service.\nValid measures are ones in which there is general agreement that the observations are closely related to the intended construct.\nvalidity: the property of giving the right answer on average across a wide range of plausible scenarios. To study validity in an empirical way, ideally you want settings in which there is an observable true value and multiple measurements can be taken.\n\n\nReliability\nA reliable measure is one that is precise and stable. We would hope the variability in our sample is due to real differences among people or things, and not due to random error incurred during the measurement process.\n\n\nSample selection\nselection: the idea that the data you see may be a nonrepresentative sample of a larger population you will not see.\nIn addition to selection bias, there are also biases from nonresponse to particular survey items, partially observed measurements, and choices in coding and interpretation of data.\n\n\n\n2.3 All graphs are comparisons\n\nSimple scatterplots\n\nhealth_data_path = file.path(data_root_path, \"HealthExpenditure/data/healthdata.txt\")\nhealth <- read.table(health_data_path, header=TRUE)\nDT::datatable(head(health))\n\n\n\n\n\n\n\npar(mgp=c(1.7,.5,0), tck=-.01, mar=c(3,3,.1,.1))\nggplot(data=health, aes(x=spending, y=lifespan, label=country)) +\n  geom_text() + \n  labs(x=\"Health care spending (PPP US$)\",y=\"Life expectancy (years)\") + \n  theme(\n    panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n    panel.background = element_rect(fill = 'white'),\n    aspect.ratio = 0.5\n  ) \n\n\n\n\nThe graph shows the exceptional position of the United States (highest health care spending, low life expectancy) and also shows the relation between spending and lifespan in other countries.\n\n\nDisplaying more information on the graph\nAt least in theory, you can display five variables easily with a scatterplot:\n\nx position\ny position\nsymbol\nsymbol size\nsymbol color\n\nWe prefer clearly distinguishable colors or symbols such as open circles, solid circles, crosses and dots. When a graph has multiple lines, label them directly.\n\n\nMultiple plots\nLooking at data in unexpected ways can lead to discovery.\nWe can learn by putting multiple related graphs in a single display.\nThere is no single best way to display a dataset.\nIn the code chunks below I’ll recreate plots 2.6, 2.7, 2.8 and 2.9 using their code provided in the supplemental website.\n\nallnames_path = file.path(data_root_path, \"Names/data/allnames_clean.csv\")\nallnames <- read.csv(allnames_path)\nDT::datatable(head(allnames), options=list(scrollX=TRUE))\n\n\n\n\n\n\n\n# create a vector identifying which rows have Female names (TRUE) and which have Male names (FALSE)\ngirl <- as.vector(allnames$sex) == \"F\"\n\n# Since there is a `names` base R function, I'm renaming the names vector to names_vec\nnames_vec <- as.vector(allnames$name)\n\n\n# plot data\n\n# calculate the number of characters in each name\nnamelength <- nchar(names_vec)\n\n# create a vector of last letters of each name\nlastletter <- substr(names_vec, namelength, namelength)\n\n# create a vector of first letters of each name\nfirstletter <- substr(names_vec, 1, 1)\n\n# plotting function\n# i am renaming it to names_histogram since there is an `arm` package function called `discrete.histogram`\nnames_histogram <- function(x, prob, prob2=NULL, xlab=\"x\", ylab=\"Probability\", xaxs.label=NULL, yaxs.label=NULL, bar.width=NULL, ...) {\n  if (length(x) != length(prob)) stop()\n  \n  x.values <- sort (unique(x))\n  n.x.values <- length (x.values)\n  gaps <- x.values[2:n.x.values] - x.values[1:(n.x.values-1)]\n  if (is.null(bar.width)) bar.width <- min(gaps)*.2\n  par(mar=c(3,3,2,2), mgp=c(1.7,.3,0), tck=0)\n  plot(range(x)+c(-1,1)*bar.width, c(0,max(prob)),\n    xlab=xlab, ylab=ylab, xaxs=\"i\", xaxt=\"n\",  yaxs=\"i\",\n    yaxt=ifelse(is.null(yaxs.label),\"s\",\"n\"), bty=\"l\", type=\"n\", ...)\n  if (is.null(xaxs.label)){\n    axis(1, x.values)\n  }\n  else {\n    n <- length(xaxs.label[[1]])\n    even <- (1:n)[(1:n)%%2==0]\n    odd <- (1:n)[(1:n)%%2==1]\n    axis(1, xaxs.label[[1]][even], xaxs.label[[2]][even], cex.axis=.9)\n    axis(1, xaxs.label[[1]][odd], xaxs.label[[2]][odd], cex.axis=.9)\n  }\n  if (!is.null(yaxs.label)){\n    axis(2, yaxs.label[[1]], yaxs.label[[2]], tck=-.02)\n  }\n  for (i in 1:length(x)){\n    polygon(x[i] + c(-1,-1,1,1)*bar.width/2, c(0,prob[i],prob[i],0),\n      border=\"gray10\", col=\"gray10\")\n    if (!is.null(prob2))\n      polygon(x[i] + c(-1,-1,1,1)*bar.width/10, c(0,prob2[i],prob2[i],0),\n        border=\"red\", col=\"black\")\n  }\n}\n\nfor (year in c(1900,1950,2010)){\n  # get the column X1900, X1950 and X2010 from the data.frame `allnames`\n  thisyear <- allnames[,paste(\"X\",year,sep=\"\")]\n  # create 2D empty arrays to store Male and Female last and first letter of names\n  lastletter.by.sex <- array(NA, c(26,2))\n  firstletter.by.sex <- array(NA, c(26,2))\n  # construct the 2D arrays of Male/Female last/first letter of names\n  for (i in 1:26){\n    lastletter.by.sex[i,1] <- sum(thisyear[lastletter==letters[i] & girl])\n    lastletter.by.sex[i,2] <- sum(thisyear[lastletter==letters[i] & !girl])\n    firstletter.by.sex[i,1] <- sum(thisyear[firstletter==LETTERS[i] & girl])\n    firstletter.by.sex[i,2] <- sum(thisyear[firstletter==LETTERS[i] & !girl])\n  }\n  \n  # plot the histogram for the given year\n  names_histogram(1:26, 100*(lastletter.by.sex[,2])/sum(lastletter.by.sex[,2]), xaxs.label=list(1:26,letters), yaxs.label=list(seq(0,30,10),seq(0,30,10)), xlab=\"\", ylab=\"Percentage of boys born\", main=paste(\"Last letter of boys' names in\", year), cex.axis=.9, cex.main=.9, bar.width=.8)\n  for (y in c(10,20,30)) abline (y,0,col=\"gray\",lwd=.5)\n  \n  names_histogram(1:26, 100*(lastletter.by.sex[,1])/sum(lastletter.by.sex[,1]), xaxs.label=list(1:26,letters), yaxs.label=list(seq(0,30,10),seq(0,30,10)), xlab=\"\", ylab=\"Percentage of girls born\", main=paste(\"Last letter of girls' names in\", year), cex.main=.9)\n  # adding the horizontal grid lines for the girls plots\n  for (y in c(10,20,30)) abline (y,0,col=\"gray\",lwd=.5)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ll recreate similar plots using tidyverse and ggplot.\nFirst, I’ll make the data long (instead of it’s current wide shape) so that I can aggregate the data more easily for plotting:\n\n# reshape the data to make it longer\nallnames_long <- allnames %>% rename(\n  id = 'X'\n) %>% pivot_longer(\n  starts_with(\"X\"),\n  names_to = \"year\",\n  values_to = \"name_n\"\n) %>% mutate(\n  # remove X at start of the four-digit year\n  year = str_remove(year, \"X\"),\n  # extract last letter of each name\n  last_letter = str_sub(name, -1)\n) \n\nDT::datatable(head(allnames_long))\n\n\n\n\n\n\nI then aggregate the data by year, sex and last letter, and calculate the count and percentage of last letters in each year for each sex:\n\nallnames_agg <- allnames_long %>% group_by(\n  year,\n  sex,\n  last_letter\n) %>% summarise(\n  last_letter_n = sum(name_n)\n) %>% mutate(\n  total_n = sum(last_letter_n),\n  last_letter_pct = last_letter_n / total_n\n)\n\n`summarise()` has grouped output by 'year', 'sex'. You can override using the\n`.groups` argument.\n\nDT::datatable(head(allnames_agg, n=30))\n\n\n\n\n\n\nI then plot one year’s data to make sure I’m getting the same result as the textbook:\n\nggplot(allnames_agg %>% filter(year == \"1900\", sex == \"F\"), aes(x = last_letter, y = last_letter_pct)) + \n  geom_bar(stat=\"identity\", fill=\"black\") + \n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  scale_x_discrete(\"Last Letter of Name\", expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of Total\", limits=c(0,0.4), breaks=c(0,.1,.2,.3,.4), labels=c(\"0%\", \"10%\", \"20%\", \"30%\", \"40%\"), expand = c(0, 0)) +\n  ggtitle(\"Last Letter of Girls' Names in 1900\")\n\n\n\n\nFinally I’ll plots histograms to match figures 2.6 and 2.7:\n\n# plotting function \nplot_last_letter_distribution <- function(year_arg, sex_arg) {\n  # prepare string for plot title\n  title_sex <- if(sex_arg == 'F') \"Girls'\" else \"Boys'\"\n  title_str <- paste(\"Last Letter of\", title_sex, \"in\", year_arg)\n  \n  # prep data\n  plot_data <- allnames_agg %>% filter(year == year_arg, sex == sex_arg)\n  \n  # calculate y-axis limits, break points and labels\n  # calculate max y limit to 0.05 more than the nearest 0.05\n  limits_max <- round(max(plot_data$last_letter_pct)/0.05) * 0.05 + 0.05\n  \n  # calculate y-axis breaks\n  breaks_val = seq(0, limits_max, by=0.05)\n  \n  # calculate y-axis labels\n  labels_val = c()\n  \n  for (val in breaks_val) {\n    labels_val <- append(labels_val, paste0(val*100, \"%\"))\n  }\n  \n  print(ggplot(plot_data, aes(x = last_letter, y = last_letter_pct)) + \n  geom_bar(stat=\"identity\", fill=\"black\") + \n  theme(\n    plot.margin = margin(3, 3, 2.5, 1, \"lines\"),\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  scale_x_discrete(\"Last Letter of Name\", expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of Total\", limits = c(0, limits_max), breaks = breaks_val, labels = labels_val, expand = c(0, 0)) +\n  ggtitle(title_str)\n  )\n}\n\nfor (year in c(1906, 1956, 2006)) {\n  plot_last_letter_distribution(year, sex = 'M')\n}\n\n\n\n\n\n\n\n\n\n\nThe following code chunks recreate Figure 2.8 using their code provided on the supplemental website:\n\n# define full range of years in the dataset\nyrs <- 1880:2010\n\n# total number of years\nn.yrs <- length(yrs)\n\n# create empyt 3D arrays to hold last and first letter frequencies\nlastletterfreqs <- array(NA, c(n.yrs,26,2))\nfirstletterfreqs <- array(NA, c(n.yrs,26,2))\n\n# define the names of each dimension of the letter frequency 3D arrays\n# as the list of years, letters and the list c(\"girls\", \"boys\")\ndimnames(lastletterfreqs) <- list(yrs, letters, c(\"girls\",\"boys\"))\ndimnames(firstletterfreqs) <- list(yrs, letters, c(\"girls\",\"boys\"))\n\n# construct the arrays lastletterfreqs and fistletterfreqs\nfor (i in 1:n.yrs){\n  thisyear <- allnames[,paste(\"X\",yrs[i],sep=\"\")]\n  for (j in 1:26){\n    # sum of last letters for girls\n    lastletterfreqs[i,j,1] <- sum(thisyear[lastletter==letters[j] & girl])\n    \n    # sum of last letters for boys\n    lastletterfreqs[i,j,2] <- sum(thisyear[lastletter==letters[j] & !girl])\n    \n    # sum o first letters for girls\n    firstletterfreqs[i,j,1] <- sum(thisyear[firstletter==LETTERS[j] & girl])\n    \n    # sum of laster letters for boys\n    firstletterfreqs[i,j,2] <- sum(thisyear[firstletter==LETTERS[j] & !girl])\n  }\n  for (k in 1:2){\n    # percentage of each last letter (of all last letters that year)\n    lastletterfreqs[i,,k] <- lastletterfreqs[i,,k]/sum(lastletterfreqs[i,,k])\n    \n    # percentage of each first letter (of all first letters that year)\n    firstletterfreqs[i,,k] <- firstletterfreqs[i,,k]/sum(firstletterfreqs[i,,k])\n  }\n}\n\n\n# plot of percentage of last letters over time\n\n# plot settings\npar(mar=c(2,3,2,1), mgp=c(1.7,.3,0), tck=-.01)\n\n# index of letters N, Y and D\npopular <- c(14,25,4)\n\n# width and line type for all letters except N, Y and D\nwidth <- rep(.5,26)\ntype <- rep(1,26)\n\n# width and line type for N, Y and D\nwidth[popular] <- c(2,3,3)\ntype[popular] <- c(1,3,2)\n\nplot(range(yrs), c(0,41), type=\"n\", xlab=\"\", ylab=\"Percentage of all boys' names that year\", bty=\"l\", xaxt=\"n\", yaxt=\"n\", yaxs=\"i\", xaxs=\"i\")\n  axis(1, seq(1900,2000,50))\n  axis(2, seq(0,40,20), paste(seq(0,40,20), \"%\", sep=\"\"))\n  for (j in 1:26){\n    # I don't think the following two variables, `maxfreq` and `best` are used in plotting\n    maxfreq <- max(lastletterfreqs[,j,2])\n    best <- (1:n.yrs)[lastletterfreqs[,j,2]==maxfreq]\n    \n    # plotting line for each letter for all years for boys\n    lines(yrs, 100*lastletterfreqs[,j,2], col=\"black\", lwd=width[j], lty=type[j])\n  }\n  \n# plot annotations\ntext(2000, 35, \"N\")\ntext(1935, 20, \"D\")\ntext(1975, 15, \"Y\")\nmtext(\"Last letters of boys' names\", side=3, line=.5)\n\n\n\n\nNext, I’ll recreate the plot of Figure 2.8 using tidyverse and ggplot for practice:\n\n# define new linewidth vector\n\n# index of letters N, Y and D\npopular <- c(14,25,4)\n\n# width and line type for all letters except N, Y and D\nwidth <- rep(.25,26)\n\n# width and line type for N, Y and D\nwidth[popular] <- c(1,1,1)\n\nggplot(allnames_agg %>% filter(sex == \"M\"), aes(x=year, y=last_letter_pct, linetype=factor(last_letter), linewidth = factor(last_letter))) + \n  geom_line(aes(group = last_letter)) +\n  scale_linetype_manual(values = type, guide = \"none\") + \n  scale_linewidth_manual(values = width, guide = \"none\") + \n  scale_x_discrete(\"Year\", breaks = c(1900, 1950, 2000), expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of all boys' names that year\", breaks = c(0, 0.1, 0.2, 0.3, 0.4), labels = c(\"0%\", \"10%\", \"20%\", \"30%\", \"40%\"), expand = c(0, 0)) +\n  ggtitle(\"Last letters of boys' names\") + \n  theme(\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) + \n  annotate(\"text\", x = \"2000\", y = 0.35, label = \"N\") + \n  annotate(\"text\", x = \"1935\", y = 0.20, label = \"D\") + \n  annotate(\"text\", x = \"1975\", y = 0.15, label = \"Y\")\n\n\n\n\nThe following code chunks recreate Figure 2.9 using their code provided on the supplemental website:\n\n# create empty 3D array to hold yearly percentage of top ten names\ntopten_percentage <- array(NA, c(length(yrs), 2))\n\nfor (i in 1:length(yrs)){\n  # get data for the given year\n  thisyear <- allnames[,paste(\"X\",yrs[i],sep=\"\")]\n  \n  # get boys' data for this year\n  boy.totals <- thisyear[!girl]\n  \n  # calculate percentages of boys' names\n  boy.proportions <- boy.totals/sum(boy.totals)\n  \n  # get total percentage of the top ten names\n  index <- rev(order(boy.proportions))\n  popularity <- boy.proportions[index]\n  topten_percentage[i,2] <- 100*sum(popularity[1:10])\n  \n  # do the same for girls' top ten names\n  girl.totals <- thisyear[girl]\n  girl.proportions <- girl.totals/sum(girl.totals)\n  index <- rev(order(girl.proportions))\n  popularity <- girl.proportions[index]\n  topten_percentage[i,1] <- 100*sum(popularity[1:10])\n}\n\n\n# plot settings\npar(mar=c(4,2,1,0), mgp=c(1.3,.2,0), tck=-.02)\n\n# plot girls and boys top ten name percentages over time\nplot(yrs, topten_percentage[,2], type=\"l\", xaxt=\"n\", yaxt=\"n\", xaxs=\"i\", yaxs=\"i\", ylim=c(0,45), bty=\"l\", xlab=\"Year\", ylab=\"\", cex.lab=.8)\nlines(yrs, topten_percentage[,1])\naxis(1, c(1900,1950,2000), cex.axis=.8)\naxis(2, c(0,20,40), c(\"0%\",\"20%\",\"40%\"), cex.axis=.8)\ntext(1902, 35, \"Boys\", cex=.75, adj=0)\ntext(1911, 20, \"Girls\", cex=.75, adj=0)\nmtext(\"Total popularity of top ten names each year, by sex\", cex=.8)\nmtext(\"Source:  Social Security Administration, courtesy of Laura Wattenberg\", 1, 2.5, cex=.5, adj=0)\n\n\n\n\nNext, I’ll recreate the plot of Figure 2.9 using tidyverse and ggplot for practice:\n\nDT::datatable(head(allnames_long))\n\n\n\n\n\n\n\nallnames_topten_names <- allnames_long %>% group_by(\n  year,\n  sex\n) %>% mutate(\n  name_pct = name_n / sum(name_n)\n) \n\n\nDT::datatable(head(allnames_topten_names %>% arrange(year, sex, desc(name_pct))))\n\n\n\n\n\n\n\nallnames_top_ten_pct <- allnames_topten_names %>% arrange(\n  year, \n  sex, \n  desc(name_pct)\n) %>% group_by(\n  year,\n  sex\n) %>% slice(\n  1:10\n) %>% summarise(\n  top_ten_pct = sum(name_pct)\n)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\nDT::datatable(head(allnames_top_ten_pct))\n\n\n\n\n\n\n\n# plot the lines\n\nggplot(allnames_top_ten_pct, aes(x = year, y = top_ten_pct)) + \n  geom_line(aes(group = sex)) +\n  scale_x_discrete(\"Year\", breaks = c(1900, 1950, 2000), expand = c(0, 0)) +\n  scale_y_continuous(\"Percentage of top ten names that year\", breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), labels = c(\"0%\", \"10%\", \"20%\", \"30%\", \"40%\", \"50%\"), expand = c(0, 0)) +\n  ggtitle(\"Total popularity of top ten names each year, by sex\") + \n  theme(\n    panel.border = element_rect(color = \"black\", fill=NA, size=1),\n    panel.background = element_rect(color = \"white\", fill=NA),\n    plot.title = element_text(size=14,hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  annotate(\"text\", x = \"1902\", y = 0.36, label = \"Boys\") + \n  annotate(\"text\", x = \"1911\", y = 0.19, label = \"Girls\")\n\n\n\n\n\n\nGrids of plots\nRealistically, it can be difficult to read a plot with more than two colors.\nConstructing a two-way grid of plots to represent two more variables, an approach of small multiples can be more effective than trying to cram five variables onto a single plot.\nIn the following code chunks, I’ll recreate Figure 2.10 following their code provided on the supplemental website.\n\n\n\n\n# view the data\ncongress[[27]][1,]\n\n[1]   1948      1      1     -1 127802 103294\n\ncongress[[28]][1,]\n\n[1]   1950      1      1      1 134258  96251\n\ncongress[[37]][10,]\n\n[1]   1968      3      1     -1      0 140419\n\ncongress[[38]][10,]\n\n[1]   1970      3      1     -1      0 117045\n\ncongress[[47]][20,]\n\n[1]   1988      4      1     -1  86623 131824\n\ncongress[[48]][50,]\n\n[1]  1990    13    11     1 36286     0\n\n\n\n# plot settings\npar(mfrow=c(3,5), mar=c(0.1,3,0,0), mgp=c(1.7, .3, 0), tck=-.02, oma=c(1,0,2,0))\n\n# plot data for each of the selected year pairs: 1948/50, 1968/70, 1988/90\nfor (i in c(27, 37, 47)) {\n  # calculate year based on index\n  year <- 1896 + 2*(i-1)\n  \n  # get the data for the first election year\n  cong1 <- congress[[i]]\n  \n  # get data for the next election year\n  cong2 <- congress[[i+1]]\n  \n  # the second column contains the state code\n  state_code <- cong1[,2]\n  \n  # convert from state code to region index\n  region <- floor(state_code/20) + 1\n  \n  # the fourth column contains whether the election has an incumbent\n  inc <- cong1[,4]\n  \n  # the fifth and sixth columns contain democrat and republican vote counts\n  dvote1 <- cong1[,5]/(cong1[,5] + cong1[,6])\n  dvote2 <- cong2[,5]/(cong2[,5] + cong2[,6])\n  \n  # their definition of an election being \"contested\" if the \n  # proportion of dem votes is between 20 and 80 percent each election year\n  contested <- (abs(dvote1 - 0.5)) < 0.3 & (abs(dvote2 - 0.5) < 0.3)\n  \n  # plot function\n  plot(c(0, 1), c(0, 1), type=\"n\", xlab=\"\", ylab=\"\", xaxt=\"n\", yaxt=\"n\", bty=\"n\")\n  text(0.8, 0.5, paste(year,\"\\nto\\n\", year+2, sep=\"\"), cex=1.1)\n  \n  # four columns, one for each region\n  for (j in 1:4){\n    plot(c(.2, .8), c(-.4, .3), type=\"n\", xlab= \"\" , ylab=if (j==1) \"Vote swing\" else \"\", xaxt=\"n\", yaxt=\"n\", bty=\"n\", cex.lab=.9)\n    if (i==47) {\n      # x-axis labels for the final pair of elections 1988/90\n      text(c(.25, .5, .75), rep(-.4, 3), c(\"25%\", \"50%\", \"75%\"), cex=.8)\n      abline(-.35, 0, lwd=.5, col=\"gray60\")\n      segments(c(.25, .5, .75), rep(-.35, 35), c(.25, .5, .75), rep(-.37, 3), lwd=.5)\n      mtext(\"Dem. vote in election 1\", side=1, line=.2, cex=.5)\n    }\n    # y-axis for each plot\n    axis(2, c(-0.25, 0, 0.25), c(\"-25%\", \"0\", \"25%\"),  cex.axis=.8)\n    abline(0, 0)\n    # region name above the first pair of elections 1948/50\n    if (i==27) mtext(region_name[j], side=3, line=1, cex=.75)\n    \n    # plotting contested elections for the region with incumbent\n    ok <- contested & abs(inc)==1 & region==j\n    points(dvote1[ok], dvote2[ok] - dvote1[ok], pch=20, cex=.3, col=\"gray60\")\n    \n    # plotting contested elections for the region with open seats\n    ok <- contested & abs(inc)==0 & region==j\n    points(dvote1[ok], dvote2[ok] - dvote1[ok], pch=20, cex=.5, col=\"black\")\n  }\n}\n\n\n\n\nNext, I’ll recreate similar plots with tidyverse and ggplot.\n\n# get list of all .asc file paths\ncongress_data_folder <- file.path(data_root_path, \"Congress/data\")\ncongress_data_files <- list.files(congress_data_folder, full.names=TRUE, pattern = \"*.asc\")\n\n# helper function which loads data into a CSV and adds a year column\nread_congress_data <- function(fpath) {\n  year_str = str_remove(basename(fpath), \".asc\")\n  \n  out <- readr::read_table(\n    fpath,\n    col_names = c(\"state\", \"col2\", \"incumbent\", \"dem\", \"rep\"),\n    col_types = cols(state = \"i\", col2 = \"c\", incumbent = \"i\", dem = \"i\", rep = \"i\")\n  ) %>% mutate(\n    year = year_str\n  )\n  \n  return(out)\n}\n\n# read all asc files into a single data.frame\n# filter for the years in question\ncongress_data <- congress_data_files %>% map(\n  read_congress_data\n) %>% list_rbind(\n) %>% filter(\n  year %in% c(1948, 1950, 1968, 1970, 1988, 1990)\n)\n\n\n# view the data\nDT::datatable(congress_data)\n\n\n\n\n\n\nNext, I’ll create two data.frames, cong1 and cong2 with 1948, 1968, 1988 and 1950, 1970, 1990 data, respectively, then merge them row-wise in order to perform year-to-year calculations:\n\n# 1948, 1968, and 1988 data\ncong1 <- congress_data %>% filter(\n  year %in% c(1948, 1968, 1988)\n) %>% rename(\n  # rename columns to avoid duplicates later on when joining the two data.frames\n  state1 = state,\n  col21 = col2,\n  incumbent1 = incumbent,\n  dem1 = dem,\n  rep1 = rep,\n  year1 = year\n) %>% mutate(\n  # create a lookup value to join the two data.frames by\n  election = case_when(\n    year1 == 1948 ~ \"1948 to 1950\",\n    year1 == 1968 ~ \"1968 to 1970\",\n    year1 == 1988 ~ \"1988 to 1990\"\n  ),\n  lookup = paste0(state1, \"-\", col21, \"-\", election)\n)\n\n# 1950, 1970 and 1990 data\ncong2 <- congress_data %>% filter(\n  year %in% c(1950, 1970, 1990)\n) %>% rename(\n  # rename columns to avoid duplicates later on when joining the two data.frames\n  state2 = state,\n  col22 = col2,\n  incumbent2 = incumbent,\n  dem2 = dem,\n  rep2 = rep,\n  year2 = year\n) %>% mutate(\n  # create a lookup value to join the two data.frames by\n  election = case_when(\n    year2 == 1950 ~ \"1948 to 1950\",\n    year2 == 1970 ~ \"1968 to 1970\",\n    year2 == 1990 ~ \"1988 to 1990\"\n  ),\n  lookup = paste0(state2, \"-\", col22, \"-\", election)\n)\n\n\n# merge the two data.frames \ncong <- cong1 %>% dplyr::left_join(\n  cong2,\n  by = \"lookup\"\n) %>% mutate(\n  # calculate democrat vote proportion\n  dvote1 = dem1 / (dem1 + rep1),\n  dvote2 = dem2 / (dem2 + rep2),\n  # calculate difference in democrat vote proportion between election years\n  dvote_diff= dvote2 - dvote1,\n  # calculate whether an election is contested\n  # an election is contested if the dem vote in both years is \n  # between 20% and 80%\n  contested = case_when(\n    (abs(dvote1 - 0.5)) < 0.3 & (abs(dvote2 - 0.5) < 0.3) ~ 1,\n    TRUE ~ 0\n  ),\n  # standardize incumbent flag in the first election year\n  inc = abs(incumbent1),\n  # remove values that are not applicable\n  inc = na_if(inc, 9),\n  inc = na_if(inc, 3),\n  inc = as.character(inc),\n  # calculate region code\n  region = floor(state1/20) + 1\n) %>% filter(\n  # only include contested elections\n  contested == 1,\n  # only include four main regions\n  region %in% c(1,2,3,4)\n) \n\n# %>% mutate(\n#   # change region to text\n#   region = case_when(\n#     region == 1 ~ \"Northeast\",\n#     region == 2 ~ \"Midwest\",\n#     region == 3 ~ \"South\",\n#     region == 4 ~ \"West\"\n#   )\n# )\n\n\nDT::datatable(cong, options=list(scrollX=TRUE))\n\n\n\n\n\n\nFinally, I can plot the data:\n\n# labels for regions\nregion_labeller <- c(\n  \"1\" = \"Northeast\",\n  \"2\" = \"Midwest\",\n  \"3\" =  \"South\",\n  \"4\" = \"West\"\n)\n\nggplot(cong, aes(x=dvote1, y = dvote_diff)) + \n  geom_point(aes(color=inc, size=inc), show.legend = FALSE) + \n  facet_grid(rows = vars(election.x), cols = vars(region), switch = \"y\", labeller = labeller(region = region_labeller)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    strip.placement = \"outside\",\n    axis.line=element_line()\n  ) + \n  scale_color_manual(values = c(\"black\", \"grey\"), na.translate = FALSE) + \n  scale_size_manual(values = c(0.75, 0.5), na.translate = FALSE) + \n  geom_abline(slope = 0, intercept = 0) +\n  scale_y_continuous(\"Vote Swing\", breaks = c(-0.25, 0, 0.25), labels = c(\"-25%\", \"0%\", \"25%\")) +\n  scale_x_continuous(\"Dem. Vote in Election 1\", breaks = c(.25, .50, .75), labels = c(\"25%\", \"50%\", \"75%\"))\n\nWarning: Removed 38 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWhile I wasn’t able to get exactly the same output as the textbook, I’m content with the grid of plots I created with ggplot.\n\n\nApplying graphical principles to numerical displays and communication more generally\nAvoid overwhelming the reader with irrelevant material.\nDo not report numbers to too many decimal places. You should display precision in a way that respects the uncertainty and variability in the numbers being presented. It makes sense to save lots of digits for intermediate steps in computations.\nYou can often make a list or table of numbers more clear by first subtracting out the average (or for a table, row and column averages).\nA graph can almost always be made smaller than you think and still be readable. This then leaves room for more plots on a grid, which then allows more patterns to be seen at once and compared.\nNever display a graph you can’t talk about. Give a full caption for every graph (this explains to yourself and others what you are trying to show and what you have learned from each plot).\nAvoid displaying graphs that have been made simply because they are conventional.\n\n\nGraphics for understanding statistical models\nThree uses of graphs in statistical analysis:\n\nDisplays of raw data. Exploratory analysis.\nGraphs of fitted models and inferences (and simulated data).\nGraphs presenting your final results.\n\nThe goal of any graph is communication to self and others. Graphs are comparisons: to zero, to other graphs, to horizontal lines, and so forth. The unexpected is usually not an “outlier” or aberrant point but rather a systematic pattern in some part of the data.\n\n\nGraphs as comparisons\nWhen making a graph, line things up so that the most important comparisons are clearest.\n\n\nGraphs of fitted models\nIt can be helpful to graph a fitted model and data on the same plot. Another use of graphics with fitted models is to plot predicted datasets and compare them visually to actual data.\n\n\n\n2.4 Data and adjustment: trends in mortality rates\nAggregation bias: occurs when it is wrongly assumed that the trends seen in aggregated data also apply to individual data points.\nI referenced this Missouri Department of Health & Senior Services page to learn more about how to calculate age-adjusted death rates.\nIn the following code chunks I’ll recreate Figures 2.11 and 2.12 using their code provided on the supplemental website.\n\n# deaton <- read.table(root(\"AgePeriodCohort/data\",\"deaton.txt\"), header=TRUE)\nages_all <- 35:64\nages_decade <- list(35:44, 45:54, 55:64)\nyears_1 <- 1999:2013\nmort_data <- as.list(rep(NA,3))\ngroup_names <- c(\"Non-Hispanic white\", \"Hispanic white\", \"African American\")\n\nfpath_1 <- file.path(data_root_path, \"AgePeriodCohort/data\", \"white_nonhisp_death_rates_from_1999_to_2013_by_sex.txt\")\nfpath_2 <- file.path(data_root_path, \"AgePeriodCohort/data\", \"white_hisp_death_rates_from_1999_to_2013_by_sex.txt\")\nfpath_3 <- file.path(data_root_path, \"AgePeriodCohort/data\", \"black_death_rates_from_1999_to_2013_by_sex.txt\")\nmort_data[[1]] <- read.table(fpath_1, header=TRUE)\nmort_data[[2]] <- read.table(fpath_2, header=TRUE)\nmort_data[[3]] <- read.table(fpath_3, header=TRUE)\n\n\nDT::datatable(mort_data[[1]])\n\n\n\n\n\n\n\nraw_death_rate <- array(NA, c(length(years_1), 3, 3))\nmale_raw_death_rate <- array(NA, c(length(years_1), 3, 3))\nfemale_raw_death_rate <- array(NA, c(length(years_1), 3, 3))\navg_death_rate <- array(NA, c(length(years_1), 3, 3))\nmale_avg_death_rate <- array(NA, c(length(years_1), 3, 3))\nfemale_avg_death_rate <- array(NA, c(length(years_1), 3, 3))\n\n# k represents the 3 different race/ethnicity groups\nfor (k in 1:3){\n  data <- mort_data[[k]]\n  \n  # the Male column is 0 for Female, 1 for Male\n  male <- data[,\"Male\"]==1\n  \n  # j represents the 3 different age decades\n  # 35:44, 45:54, 55:64\n  for (j in 1:3){\n    # years1 is from 1999 to 2013\n    for (i in 1:length(years_1)){\n      ok <- data[,\"Year\"]==years_1[i] & data[,\"Age\"] %in% ages_decade[[j]]\n      \n      # raw death rate calculated as\n      # 100,000 * deaths / population\n      raw_death_rate[i,j,k] <- 1e5*sum(data[ok,\"Deaths\"])/sum(data[ok,\"Population\"])\n      male_raw_death_rate[i,j,k] <- 1e5*sum(data[ok&male,\"Deaths\"])/sum(data[ok&male,\"Population\"])\n      female_raw_death_rate[i,j,k] <- 1e5*sum(data[ok&!male,\"Deaths\"])/sum(data[ok&!male,\"Population\"])\n      avg_death_rate[i,j,k] <- mean(data[ok,\"Rate\"])\n      male_avg_death_rate[i,j,k] <- mean(data[ok&male,\"Rate\"])\n      female_avg_death_rate[i,j,k] <- mean(data[ok&!male,\"Rate\"])\n    }\n  }\n}\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(range(years_1), c(1, 1.1), xaxt=\"n\", yaxt=\"n\", type=\"n\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Death rate relative to 1999\", main=\"Age-adjusted death rates for non-Hispanic whites aged 45-54:\\nTrends for women and men\")\nlines(years_1, male_avg_death_rate[,2,1]/male_avg_death_rate[1,2,1], col=\"blue\")\nlines(years_1, female_avg_death_rate[,2,1]/female_avg_death_rate[1,2,1], col=\"red\")\naxis(1, seq(1990,2020,5))\naxis(2, seq(1, 1.1, .05))\ntext(2011.5, 1.075, \"Women\", col=\"red\")\ntext(2010.5, 1.02, \"Men\", col=\"blue\")\ngrid(col=\"gray\")\n\n\n\n\n\nmean_age_45_54 <- function(yr){\n  ages <- 45:54\n  ok <- births$year %in% (yr - ages)\n  return(sum(births$births[ok]*rev(ages))/sum(births$births[ok]))\n}\n\n\nnumber_of_deaths <- rep(NA, length(years_1))\nnumber_of_people <- rep(NA, length(years_1))\navg_age <- rep(NA, length(years_1))\navg_age_census <- rep(NA, length(years_1))\n\n# data for white non-hispanic death rates\ndata <- mort_data[[1]]\n\ndeath_rate_extrap_1999 <- rep(NA, length(years_1))\ndeath_rate_extrap_2013 <- rep(NA, length(years_1))\n\n# data for Males\nmale <- data[,\"Male\"]==1\n\nok_1999 <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]] \n\ndeath_rate_1999 <- (data[ok_1999 & male, \"Deaths\"] + data[ok_1999 & !male, \"Deaths\"])/(data[ok_1999 & male, \"Population\"] + data[ok_1999 & !male, \"Population\"])\n\nok_2013<- data[,\"Year\"]==2013 & data[,\"Age\"] %in% ages_decade[[2]] \n\ndeath_rate_2013 <- (data[ok_2013 & male, \"Deaths\"] + data[ok_2013 & !male, \"Deaths\"])/(data[ok_2013 & male, \"Population\"] + data[ok_2013 & !male, \"Population\"])\n\nage_adj_rate_flat <- rep(NA, length(years_1))\nage_adj_rate_1999 <- rep(NA, length(years_1))\nage_adj_rate_2013 <- rep(NA, length(years_1))\n\nok <- data[,\"Age\"] %in% ages_decade[[2]]\n\npop1999 <- data[ok & data[,\"Year\"]==1999 & male,\"Population\"] + data[ok & data[,\"Year\"]==1999 & !male,\"Population\"]\n\npop2013 <- data[ok & data[,\"Year\"]==2013 & male,\"Population\"] + data[ok & data[,\"Year\"]==2013 & !male,\"Population\"]\n\nfor (i in 1:length(years_1)){\n  ok <- data[,\"Year\"]==years_1[i] & data[,\"Age\"] %in% ages_decade[[2]]\n  \n  number_of_deaths[i] <- sum(data[ok,\"Deaths\"])\n  number_of_people[i] <- sum(data[ok,\"Population\"])\n  \n  avg_age[i] <- weighted.mean(ages_decade[[2]], data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n  \n  avg_age_census[i] <- mean_age_45_54(years_1[i])\n  \n  rates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n  \n  age_adj_rate_flat[i] <- weighted.mean(rates, rep(1,10))\n  age_adj_rate_1999[i] <- weighted.mean(rates, pop1999)\n  age_adj_rate_2013[i] <- weighted.mean(rates, pop2013)\n}\n\nfor (i in 1:length(years_1)){\n  ok <- data[,\"Year\"]==years_1[i] & data[,\"Age\"] %in% ages_decade[[2]]\n  \n  death_rate_extrap_1999[i] <- weighted.mean(death_rate_1999, data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n  \n  death_rate_extrap_2013[i] <- weighted.mean(death_rate_2013, data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n  \n}\n\n\nhead(number_of_deaths)\n\n[1] 106808 111964 117086 119812 121832 123037\n\nhead(number_of_people)\n\n[1] 27995805 28669184 29733531 29880552 30260532 30629390\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1,  number_of_deaths/number_of_people, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Mortality rate among non-Hisp whites 45-54\", main=\"So take the ratio!\", cex.main=.9)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\nAs an aside, I want to better understand how the avg_age vector is calculated:\n\n# flag for when data is from 1999 and for ages 45 to 54\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# range of ages 45 to 54\nages_decade[[2]]\n\n [1] 45 46 47 48 49 50 51 52 53 54\n\n# 1999 population for ages 45 to 54\ndata[ok & male,\"Population\"] + data[ok & !male,\"Population\"]\n\n [1] 3166393 3007083 2986252 2805975 2859406 2868751 2804957 3093631 2148382\n[10] 2254975\n\n# manually calculated weighted mean\nsum(ages_decade[[2]] * (data[ok & male,\"Population\"] + data[ok & !male,\"Population\"]) / (sum(data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])))\n\n[1] 49.25585\n\n# weighted.mean output\navg_age[1] # this equals the manually calculated weighted mean. Nice!\n\n[1] 49.25585\n\n\n\nlength(avg_age) # 15 years: 1999-2013\n\n[1] 15\n\nhead(avg_age)\n\n[1] 49.25585 49.29742 49.40278 49.34845 49.34162 49.35910\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1, avg_age, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Avg age among non-Hisp whites 45-54\", main=\"But the average age in this group is going up!\", cex.main=.9)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\n\nhead(death_rate_extrap_2013)\n\n[1] 0.003999152 0.004012133 0.004050824 0.004031180 0.004028291 0.004034446\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1, number_of_deaths/number_of_people, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Death rate for 45-54 non-Hisp whites\", main=\"Projecting backward from 2013 makes it clear that\\nall the underlying change happened between 1999 and 2005\", cex.main=.8)\nlines(years_1, death_rate_extrap_2013, col=\"green4\")\naxis(1, seq(1990,2020,5))\ntext(2003, .00395, \"Raw death rate\", cex=.8)\ntext(2001.5, .004075, \"Expected just from\\nage shift\", col=\"green4\", cex=.8)\ngrid(col=\"gray\")\n\n\n\n\n\nhead(age_adj_rate_flat)\n\n[1] 0.003908209 0.003978170 0.003964249 0.004053627 0.004072032 0.004058058\n\nhead(age_adj_rate_flat[1])\n\n[1] 0.003908209\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nplot(years_1, age_adj_rate_flat/age_adj_rate_flat[1], xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Age-adj death rate, relative to 1999\", main=\"Trend in age-adjusted death rate\\nfor 45-54-year-old non-Hisp whites\", cex.main=.8)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\n\nhead(age_adj_rate_1999)\n\n[1] 0.003815143 0.003889664 0.003891300 0.003976232 0.003999796 0.003987676\n\nhead(age_adj_rate_1999[1])\n\n[1] 0.003815143\n\nhead(age_adj_rate_2013)\n\n[1] 0.003972000 0.004040848 0.004021437 0.004111934 0.004129688 0.004115952\n\nhead(age_adj_rate_2013[1])\n\n[1] 0.003972\n\n\n\npar(mar=c(2.5, 3, 3, .2), mgp=c(2,.5,0), tck=-.01)\nrng <- range(age_adj_rate_flat/age_adj_rate_flat[1], age_adj_rate_1999/age_adj_rate_1999[1], age_adj_rate_2013/age_adj_rate_2013[1])\nplot(years_1, age_adj_rate_flat/age_adj_rate_flat[1], ylim=rng, xaxt=\"n\", type=\"l\", bty=\"l\", xaxs=\"i\", xlab=\"\", ylab=\"Age-adj death rate, relative to 1999\", main=\"It doesn't matter too much what age adjustment\\nyou use for 45-54-year-old non-Hisp whites\", cex.main=.8)\nlines(years_1, age_adj_rate_1999/age_adj_rate_1999[1], lty=2)\nlines(years_1, age_adj_rate_2013/age_adj_rate_2013[1], lty=3)\ntext(2003, 1.053, \"Using 1999\\nage dist\", cex=.8)\ntext(2004, 1.032, \"Using 2013\\nage dist\", cex=.8)\naxis(1, seq(1990,2020,5))\ngrid(col=\"gray\")\n\n\n\n\nNext, I’ll recreate the plots in Figures 2.11 and 2.12 using tidyverse and ggplot. I’ll go in the order of plots displayed in the supplemental website. They start with Figure 2.11a (Observed increase in raw mortality rates among 45-to-54-year-old non-Hispanic whites, unadjusted for age.\n\n# load death rate data for white non-hispanic individuals\nmort_data_fpath <- file.path(data_root_path, \"AgePeriodCohort/data\",\"white_nonhisp_death_rates_from_1999_to_2013_by_sex.txt\")\n\nmort_data2 <- readr::read_table(mort_data_fpath) %>% filter(\n  # get data for the age decade in question\n  Age %in% 45:54\n)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Age = col_double(),\n  Male = col_double(),\n  Year = col_double(),\n  Deaths = col_double(),\n  Population = col_double(),\n  Rate = col_double()\n)\n\nDT::datatable(mort_data2)\n\n\n\n\n\n\nI’ll look at their vector raw_death_rate to compare my recreated data.frame with.\n\nraw_death_rate[,2,1]\n\n [1] 381.5143 390.5378 393.7844 400.9698 402.6102 401.6959 408.2857 407.9969\n [9] 405.2389 412.3255 413.7259 407.1706 414.3665 410.9378 415.3878\n\n\nI’ll calculate the raw death rate for each Year. In their code, they multiple it by 100,000 to get per 100,000 deaths, but I’ll follow the textbook plot which shows it as a decimal.\n\nraw_death_rate_data <- mort_data2 %>% group_by(\n  Year\n) %>% summarize(\n  death_rate = sum(Deaths) / sum(Population)\n)\n\nDT::datatable(raw_death_rate_data)\n\n\n\n\n\n\nNext, I’ll plot the data\n\nggplot(raw_death_rate_data, aes(x = Year, y = death_rate)) + \n  geom_line() + \n  scale_y_continuous(\"Death rates among non-Hisp whites 45-54\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Raw death rates\\nfor 45-54-year-old non-Hispanic whites\")\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nNext, I’ll recreate the data and plots for Figure 2.12c (trends in age-adjusted death rates broken down by sex). I’ll start by calculating the average death rate for Males and Females:\n\navg_death_rate <- mort_data2 %>% group_by(\n  Year,\n  Male\n) %>% summarise(\n  avg_rate = mean(Rate)\n) %>% mutate(\n  age_adj_death_rate = case_when(\n    Male == 0 ~ avg_rate / 288.19,\n    Male == 1 ~ avg_rate / 495.37\n  ),\n  # convert Male to discrete value\n  Male = case_when(\n    Male == 0 ~ \"Women\",\n    Male == 1 ~ \"Men\"\n  )\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(avg_death_rate)\n\n\n\n\n\n\nNext, I’ll plot the age-adjusted death rates (relative to 1999) for Non-Hispanic White Males and Females aged 45 - 54 between 1999 and 2013:\n\nggplot(avg_death_rate, aes(x = Year, y = age_adj_death_rate, color = Male)) + \n  geom_line(aes(group = Male), show.legend = FALSE) + \n  scale_color_manual(values = c(\"blue\", \"red\")) + \n  annotate(\"text\", x = 2011.5, y = 1.075, label = \"Women\", color = \"red\") + \n  annotate(\"text\", x = 2010.5, y = 1.02, label= \"Men\", color = \"blue\") + \n  scale_y_continuous(\"Death rate relative to 1999\") + \n  scale_x_continuous(\"Year\") + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Age-adjusted death rates for non-Hispanic whites aged 45-54:\\nTrends for women and men\")\n\n\n\n\nLooks great! My grid lines are at different placements (5 years and 0.025 death rate) than the textbook but I’m happy with how the plot turned out. One hiccup that needed to be addressed was that the group aesthetic needs to be discrete (it was integer 1 for Male and integer 0 for Female) in order to use scale_color_manual.\nNext, I’ll recreate Figure 2.11b (increase in average age of this group as the baby boom generation moves through).\n\navg_age_data <- mort_data2 %>% group_by(\n  Year,\n  Age\n) %>% summarise(\n  total_pop = sum(Population)\n) %>% mutate(\n  # calculated weighted mean\n  weighted_age = Age * total_pop\n) %>% group_by(\n  # in order to calculate weighted mean age by Year, \n  # group only by Year\n  Year\n) %>% summarize(\n  weighted_mean_age = sum(weighted_age) / sum(total_pop)\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(avg_age_data)\n\n\n\n\n\n\nLet’s compare it with the actual avg_age values:\n\navg_age\n\n [1] 49.25585 49.29742 49.40278 49.34845 49.34162 49.35910 49.36995 49.39921\n [9] 49.43863 49.48060 49.51819 49.53985 49.62024 49.66772 49.70577\n\n\nNext, I’ll plot weighted mean age vs years:\n\nggplot(avg_age_data, aes(x = Year, y = weighted_mean_age)) + \n  geom_line() + \n  scale_y_continuous(\"Avg age among non-Hispanic whites 45-54\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"But the average age in this group is going up!\")\n\n\n\n\nNext, I’ll recreate Figure 2.11c (raw death rate, along with trend in death rate attributable by change in age distribution alone, had age-specific mortality rates been at the 2013 level throughout).\nAs an aside, I’m going to manually calculate the first value of death_rate_extrap_2013:\n\n# death rates in 2013 for ages 45:54\ndeath_rate_2013\n\n [1] 0.002607366 0.002898205 0.003235408 0.003428914 0.003845450 0.004221539\n [7] 0.004660622 0.004944160 0.005267291 0.005727139\n\n# weighted mean of 2013 death rates of ages 45:54 by 1999-2013 population of ages 45-54\ndeath_rate_extrap_2013 \n\n [1] 0.003999152 0.004012133 0.004050824 0.004031180 0.004028291 0.004034446\n [7] 0.004037951 0.004048038 0.004061568 0.004075922 0.004089161 0.004096481\n[13] 0.004124305 0.004140684 0.004153878\n\n# flag for when data is from 1999 and for ages 45 to 54\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# 1999 population for ages 45 to 54\ndata[ok & male,\"Population\"] + data[ok & !male,\"Population\"]\n\n [1] 3166393 3007083 2986252 2805975 2859406 2868751 2804957 3093631 2148382\n[10] 2254975\n\n# numerator of weighted mean\ndeath_rate_2013 * (data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])\n\n [1]  8255.944  8715.142  9661.744  9621.447 10995.704 12110.545 13072.845\n [8] 15295.406 11316.152 12914.555\n\n# weighted mean of death_rate_2013 and 1999 population\nsum(death_rate_2013 * (data[ok & male,\"Population\"] + data[ok & !male,\"Population\"]) / (sum(data[ok & male,\"Population\"] + data[ok & !male,\"Population\"])))\n\n[1] 0.003999152\n\n\nI’ll first calculate the total deaths, population and death rate for each year and age value.\n\nmort_data_totals <- mort_data2 %>% group_by(\n  # calculate total death rate and total population\n  # for each year and age\n  Year,\n  Age\n) %>% summarise(\n  total_deaths = sum(Deaths),\n  total_pop = sum(Population),\n  total_death_rate = total_deaths / total_pop\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(mort_data_totals %>% filter(Year == 2013)) # matches death_rate_2013\n\n\n\n\n\n\nNext, I’ll widen the data:\n\ndeath_rate_extrap_2013_data <- mort_data_totals %>% pivot_wider(\n  names_from = Year,\n  values_from = c(total_death_rate, total_pop),\n  names_vary = \"slowest\",\n  id_cols = c(Age)\n) %>% mutate(\n  # multiply each year's age population with 2013 death rate\n  across(starts_with(\"total_pop\"), ~ .x * total_death_rate_2013, .names = \"extrap_{.col}\")\n) %>% pivot_longer(\n  # return back to a long table \n  # in order to group by Age, Year and calculate weighted mean\n  !Age,\n  names_to = c(\".value\", \"Year\"),\n  names_pattern = \"^(.*)_([0-9]{4})$\"\n) %>% group_by(\n  Year\n) %>% summarise(\n  weighted_mean_extrap = sum(extrap_total_pop) / sum(total_pop)\n) \n\nDT::datatable(death_rate_extrap_2013_data, options = list(scrollX = TRUE))\n\n\n\n\n\n\nMy calculated values match death_rate_extrap_2013. Yes!\nNext, I’ll prepare the data to recreate number_of_deaths/number_of_people in the text:\n\nyearly_total_death_rate <- mort_data2 %>% group_by(\n  # calculate total death rate and total population\n  # for each year \n  Year\n) %>% summarise(\n  total_death_rate = sum(Deaths) / sum(Population)\n) %>% mutate(\n  Year = as.character(Year)\n)\n\nDT::datatable(yearly_total_death_rate)\n\n\n\n\n\n\nNext, I’ll plot the extrapolated death rate vs. years:\n\nggplot() + \n  geom_line(data = death_rate_extrap_2013_data, aes(x = Year, y = weighted_mean_extrap, group = 1), color = \"green4\") +\n  geom_line(data = yearly_total_death_rate, aes(x = Year, y = total_death_rate, group = 1), color = \"black\") +\n  scale_y_continuous(\"Death rate for 45-54 non-Hisp whites\", expand = c(0,0)) +\n  scale_x_discrete(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Projecting backward from 2013 makes it clear that\\nall the underlying change happened between 1999 and 2005\") + \n  annotate(\"text\", x = \"2003\", y = 0.00395, label = \"Raw death rate\") +\n  annotate(\"text\", x = \"2002\", y = 0.004075, label = \"Expected just from\\nage shift\", color=\"green4\")\n\n\n\n\nThe next plot I’ll recreate is Figure 2.12a (Age-adjusted death rates among 45-to-54-year-old non-Hispanic whites, showing an increase from 1999 to 2005 and a steady pattern since 2005).\nI’ll start by looking at the vector age_adj_rate_flat:\n\nage_adj_rate_flat\n\n [1] 0.003908209 0.003978170 0.003964249 0.004053627 0.004072032 0.004058058\n [7] 0.004123289 0.004112009 0.004072423 0.004130493 0.004133025 0.004059948\n[13] 0.004105064 0.004053953 0.004083609\n\n\nThen run the calculation for the first value for 1999:\n\n# flag for data in 1999 for ages 45 to 54\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# calculation of death rates\nrates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n\nprint(rates)\n\n [1] 0.002622542 0.002929417 0.003059018 0.003371734 0.003589906 0.003767493\n [7] 0.004289549 0.004447848 0.005451079 0.005553498\n\n# non-weighted mean\nmean(rates)\n\n[1] 0.003908209\n\n# calculation of age_adj_rate_flat\nweighted.mean(rates, rep(1,10))\n\n[1] 0.003908209\n\n\nThe calculation for this metric is similar to Figure 2.12c which showed the age-adjusted death rate by sex. Instead of separating by sex, I’ll combine Men and Women deaths and populations in this death rate calculation.\n\nage_adj_rate_flat_data <- mort_data2 %>% group_by(\n  Year,\n  Age\n) %>% summarise(\n  # calculate death rate for each Year and Age\n  avg_rate = sum(Deaths) / sum(Population)\n) %>% group_by(\n  Year\n) %>% summarise(\n  # calculate average death rate across ages 45-54 for each Year\n  avg_rate = mean(avg_rate)\n) %>% mutate(\n  # adjust death rate relative to 1999\n  adj_death_rate = avg_rate / 0.00390820851324269\n)\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nDT::datatable(age_adj_rate_flat_data)\n\n\n\n\n\n\nNext, I’ll plot age-adjusted death rate relative to 1999 vs. Years:\n\nggplot(age_adj_rate_flat_data, aes(x = Year, y = adj_death_rate)) + \n  geom_line() +\n  scale_y_continuous(\"Age-adj death rate, relative to 1999\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"Trend in age-adjusted death rate\\nfor 45-54-year-old non-Hisp whites\")\n\n\n\n\nThe final figure I’ll recreate is Figure 2.12b (comparison of two different age adjustments).\nI’ll start by looking at the vectors in question, age_adj_rate_1999 and age_adj_rate_2013.\n\nage_adj_rate_1999\n\n [1] 0.003815143 0.003889664 0.003891300 0.003976232 0.003999796 0.003987676\n [7] 0.004049536 0.004036497 0.003995087 0.004057049 0.004054927 0.003979230\n[13] 0.004023539 0.003970082 0.003999152\n\nage_adj_rate_2013\n\n [1] 0.003972000 0.004040848 0.004021437 0.004111934 0.004129688 0.004115952\n [7] 0.004185252 0.004174235 0.004133826 0.004193094 0.004197334 0.004125137\n[13] 0.004172716 0.004122541 0.004153878\n\n\nAs an aside, I’ll view the first values (1999 and 2013) from each of the two vectors respectively, following the their code.\n\n# flag for 1999 and 45-54 age range\nok <- data[,\"Year\"]==1999 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# average death rates\nrates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n\n# 1999 average death rates\nprint(rates)\n\n [1] 0.002622542 0.002929417 0.003059018 0.003371734 0.003589906 0.003767493\n [7] 0.004289549 0.004447848 0.005451079 0.005553498\n\n# 1999 population\nprint(pop1999)\n\n [1] 3166393 3007083 2986252 2805975 2859406 2868751 2804957 3093631 2148382\n[10] 2254975\n\n# first value of age_adj_rate_1999\nweighted.mean(rates, pop1999)\n\n[1] 0.003815143\n\n# flag for 1999 and 45-54 age range\nok <- data[,\"Year\"]==2013 & data[,\"Age\"] %in% ages_decade[[2]]\n\n# average death rates\nrates <- (data[ok&male,\"Deaths\"] + data[ok&!male,\"Deaths\"])/(data[ok&male,\"Population\"] + data[ok&!male,\"Population\"])\n\n# 2013 average death rates\nprint(rates)\n\n [1] 0.002607366 0.002898205 0.003235408 0.003428914 0.003845450 0.004221539\n [7] 0.004660622 0.004944160 0.005267291 0.005727139\n\n# 2013 population\nprint(pop2013)\n\n [1] 2576547 2629559 2692087 2905293 3017592 3054810 3099157 3182745 3199178\n[10] 3141010\n\n# first value of age_adj_rate_2013\nweighted.mean(rates, pop2013)\n\n[1] 0.004153878\n\n\nI’ll start by reusing my data.frame mort_data_totals which contains Year, Age, total_deaths, total_pop and total_death_rate.\n\nDT::datatable(mort_data_totals)\n\n\n\n\n\n\nI’ll use a similar approach to how I created death_rate_extrap_2013_data.\n\npop_1999_adj_death_rate <- mort_data_totals %>% pivot_wider(\n  names_from = Year,\n  values_from = c(total_death_rate, total_pop),\n  names_vary = \"slowest\",\n  id_cols = c(Age)\n) %>% mutate(\n  # calculate weighted mean for death rates with 1999 population\n  across(starts_with(\"total_death_rate\"), ~ sum(.x * total_pop_1999) / sum(total_pop_1999), .names = \"adj_{.col}\")\n) %>% pivot_longer(\n  # return back to a long table\n  !Age,\n  names_to = c(\".value\", \"Year\"),\n  names_pattern = \"^(.*)_([0-9]{4})$\"\n) %>% group_by(\n  Year\n) %>% dplyr::select(\n  # pull columns needed for plot\n  Year,\n  adj_total_death_rate\n) %>% distinct(\n  Year, adj_total_death_rate\n) %>% mutate(\n  # normalize death rate relative to 1999 value\n  adj_total_death_rate = adj_total_death_rate / 0.0038151430187487,\n  # make Year continuous\n  Year = as.numeric(Year)\n)\n\nDT::datatable(pop_1999_adj_death_rate, options = list(scrollX = TRUE))\n\n\n\n\n\n\nI’ll create the same data.frame for the 2013 adjusted death rates:\n\npop_2013_adj_death_rate <- mort_data_totals %>% pivot_wider(\n  names_from = Year,\n  values_from = c(total_death_rate, total_pop),\n  names_vary = \"slowest\",\n  id_cols = c(Age)\n) %>% mutate(\n  # calculate weighted mean for death rates with 1999 population\n  across(starts_with(\"total_death_rate\"), ~ sum(.x * total_pop_2013) / sum(total_pop_2013), .names = \"adj_{.col}\")\n) %>% pivot_longer(\n  # return back to a long table\n  !Age,\n  names_to = c(\".value\", \"Year\"),\n  names_pattern = \"^(.*)_([0-9]{4})$\"\n) %>% group_by(\n  Year\n) %>% dplyr::select(\n  # pull columns needed for plot\n  Year,\n  adj_total_death_rate\n) %>% distinct(\n  Year, adj_total_death_rate\n) %>% mutate(\n  # normalize death rate relative to 1999 value\n  adj_total_death_rate = adj_total_death_rate / 0.00397199977461337,\n  # make Year continuous\n  Year = as.numeric(Year)\n)\n\nDT::datatable(pop_2013_adj_death_rate, options = list(scrollX = TRUE))\n\n\n\n\n\n\nFinally, I’ll plot the three lines:\n\nggplot() + \n  geom_line(data = age_adj_rate_flat_data, aes(x = Year, y = adj_death_rate)) +\n  geom_line(data = pop_1999_adj_death_rate, aes(x = Year, y = adj_total_death_rate), linetype = \"dashed\") +\n  geom_line(data = pop_2013_adj_death_rate, aes(x = Year, y = adj_total_death_rate), linetype = \"dotted\") + \n  scale_y_continuous(\"Age-adj death rate, relative to 1999\", expand = c(0,0)) +\n  scale_x_continuous(\"Year\", breaks = c(2000, 2005, 2010), expand = c(0,0)) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line.x = element_line(color = \"black\"),\n    axis.line.y = element_line(color = \"black\"),\n    panel.grid.major = element_line(color = \"grey\", linetype = \"dotted\", size = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(size = 10)\n  ) +\n  ggtitle(\"It doesn't matter too much what age adjustment\\nyou use for 45-54-year-old non-Hisp whites\") + \n  annotate(\"text\", x = 2003, y = 1.053, label = \"Using 1999\\nage dist\") + \n  annotate(\"text\", x = 2004, y = 1.032, label = \"Using 2013\\nage dist\")"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "",
    "text": "An example image of a bear from the image dataset used in this chapter.\nIn Chapter 6, we learned to train an image recognition model for multi-label classification. In this notebook, I will apply those concepts to the bear classifier from Chapter 2.\nI’ll place the prompt of the “Further Research” section here and then answer each part.\nHere’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#setup",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Setup",
    "text": "Setup\n\nfrom fastai.vision.all import *\n\n\nimport fastai\nimport pandas as pd\n\nfastai.__version__\n\n'2.3.0'\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\nMounted at /content/gdrive\n\n\nI have three different CSVs with Google Image URLs, one each for black, brown and grizzly bears. The script below, taken from the book, creates a directory for each of the three types of bears in the bears folder, and then downloads the corresponding bear type’s images into that directory.\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\nbear_types = ['black', 'grizzly', 'teddy']\nif not path.exists():\n  path.mkdir()\n  for o in bear_types:\n    dest = path/o\n    dest.mkdir(exist_ok=True)\n    download_images(f'/content/gdrive/MyDrive/fastai-course-v4/images/bears/{o}', url_file=Path(f'/content/gdrive/MyDrive/fastai-course-v4/download_{o}.csv'))\n\n\n# confirm that `get_image_files` retrieves all images\nfns = get_image_files(path)\nfns\n\n(#535) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000009.jpg')...]\n\n\n\n# verify all images\nfailed = verify_images(fns)\nfailed\n\n(#0) []\n\n\nSince I may need to move files around if they are incorrectly labeled, I’m going to prepend the filenames with the corresponding bear type.\n\nimport os\nfor dir in os.listdir(path):\n    for f in os.listdir(path/dir):\n      os.rename(path/dir/f, path/dir/f'{dir}_{f}')\n\n\nfns = get_image_files(path)\nfns\n\n(#723) [Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000002.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000000.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000001.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000003.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000004.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000005.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000006.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000007.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000008.jpg'),Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears/black/black_00000010.jpg')...]"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#single-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Single-Label Classifier",
    "text": "Single-Label Classifier\nI’ll train the single-digit classifier as we did in Chapter 2.\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# create DataLoaders\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# verify train batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n# first training\n# use it to clean the data\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.367019\n      0.252684\n      0.080645\n      00:05\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.179421\n      0.175091\n      0.056452\n      00:04\n    \n    \n      1\n      0.155954\n      0.165824\n      0.048387\n      00:04\n    \n    \n      2\n      0.119193\n      0.173681\n      0.056452\n      00:04\n    \n    \n      3\n      0.098313\n      0.170383\n      0.048387\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\nInitial training: Clean the Dataset\n\n# plot highest loss images\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\nSome of these images are infographics containing text, illustrations and other non-photographic bear data. I’ll delete those using the cleaner\n\nfrom fastai.vision.widgets import *\n\n\n# view highest loss images\n# using ImageClassifierCleaner\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n# unlink images with \"<Delete>\" selected in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\n\n# move any images reclassified in the cleaner\nfor idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nAfter a few rounds of quickly training the model and using the cleaner, I was able to remove or change a couple dozen of the images. I’ll use lr.find() and re-train the model.\n\n\nSecond Training with Cleaned Dataset\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/bears')\n\n# create DataLoaders\ndls = bears.dataloaders(path)\n\n#verify validation batch\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n#verify training batch\ndls.train.show_batch(max_n=4, nrows=1)\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.0005754399462603033)\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\n\n# fit one cycle\nlr = 1e-3\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.405979\n      0.418305\n      0.145161\n      00:04\n    \n    \n      1\n      0.803087\n      0.214286\n      0.056452\n      00:04\n    \n    \n      2\n      0.557531\n      0.169275\n      0.048387\n      00:04\n    \n    \n      3\n      0.408410\n      0.163632\n      0.056452\n      00:04\n    \n    \n      4\n      0.321682\n      0.164792\n      0.040323\n      00:04\n    \n  \n\n\n\n\n# view confusion matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n# show results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘t’ 8=‘h’ 9=‘e’ 10=’ ’ 11=‘m’ 12=‘o’ 13=‘d’ 14=‘e’ 15=‘l’}\nlearn.export(fname=path/'single_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#multi-label-classifier",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Multi-Label Classifier",
    "text": "Multi-Label Classifier\nThere are three major differences between training a multi-label classification model and a single-label model on this dataset. I present them in a table here:\n\n\n\n\n\n\n\n\n\nClassification Model Type\nDependent Variable\nLoss Function\nget_y function\n\n\n\n\nSingle-label\nDecoded string\nCross Entropy (softmax)\nparent_label\n\n\nMulti-label\nOne-hot Encoded List\nBinary Cross Entropy (sigmoid with threshold)\n[parent_label]\n\n\n\n\n# create helper function\ndef get_y(o): return [parent_label(o)]\n\n\n# create DataBlock\nbears = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_y,\n    item_tfms=RandomResizedCrop(224, min_scale=0.5))\n\n\n# view validation batch\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# find learning rate\nlearn = cnn_learner(dls, resnet18,  metrics=partial(accuracy_multi,thresh=0.95), loss_func=BCEWithLogitsLossFlat(thresh=0.5))\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n# verify loss function\nlearn.loss_func\n\nFlattenedLoss of BCEWithLogitsLoss()\n\n\n\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.478340\n      0.436599\n      0.937695\n      00:51\n    \n    \n      1\n      0.289231\n      0.642520\n      0.887850\n      00:03\n    \n    \n      2\n      0.203213\n      0.394335\n      0.897196\n      00:03\n    \n    \n      3\n      0.159622\n      0.155405\n      0.959502\n      00:02\n    \n    \n      4\n      0.132379\n      0.090879\n      0.965732\n      00:02\n    \n  \n\n\n\n\n# verify results\nlearn.show_results()\n\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ 6=’ ’ 7=‘m’ 8=‘o’ 9=‘d’ 10=‘e’ 11=‘l’}\nlearn.export(path/'multi_label_bear_classifier.pkl')\n:::"
  },
  {
    "objectID": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "href": "posts/2021-04-25-fastai-chapter-6-bear-classifier/2021-04-25-fastai-chapter-6-bear-classifier.html#model-inference",
    "title": "fast.ai Chapter 6: Bear Classifier",
    "section": "Model Inference",
    "text": "Model Inference\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images')\n\n\nImage with a Single Bear\n\n# grizzly bear image\nimg = PILImage.create(path/'test'/'grizzly_test_1.jpg')\nimg\n\n\n\n\n\n# load learners\nsingle_learn_inf = load_learner(path/'bears'/'single_label_bear_classifier.pkl')\nmulti_learn_inf = load_learner(path/'bears'/'multi_label_bear_classifier.pkl')\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n\n\n('teddy', tensor(2), tensor([1.7475e-04, 3.7727e-04, 9.9945e-01]))\n\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\n\n((#1) ['grizzly'],\n tensor([False,  True, False]),\n tensor([6.3334e-05, 1.0000e+00, 1.4841e-04]))\n\n\n\n\nImage with Two Bears\n\n# image with grizzly and black bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with grizzly and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n# image with black and teddy bear\nimg = PILImage.create(path/'test'/'.jpg')\nimg\n\n\n# single label classification\nsingle_learn_inf.predict(img)\n\n\n# multi label classification\nmulti_learn_inf.predict(img)\n\n\n\nImages without Bears\n\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/')\nimg = PILImage.create(path/'test'/'computer.jpg')\nimg\n\n\n\n\n\nsigle_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.1316, 0.1916, 0.0004]))\n\n\n\nsingle_learn_inf.predict(img)[2].sum()\n\n\n\n\ntensor(1.)\n\n\n\n# set loss function threshold to 0.9\nmulti_learn_inf.predict(img)\n\n\n\n\n((#0) [], tensor([False, False, False]), tensor([0.0275, 0.0196, 0.8457]))"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html",
    "href": "posts/2023-09-14-rf/index.html",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "",
    "text": "In this blog post I’ll work through the first exercise given in the “Further Research” section of Chapter 9 of the fastai textbook:\n\nPick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard.\n\nI’ll use the dataset from the Zillow Prize: Zillow’s Home Value Prediction (Zestimate) competition, and work through each section in the chapter.\n\nIn this competition, Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as \\(logerror = log(Zestimate)-log(SalePrice)\\). Submissions are evaluated on Mean Absolute Error between the predicted log error and the actual log error.\n\nI get the following results by the end of the exercise:\n\n\n\nModel\nMean Absolute Error\n\n\n\n\nRandom Forest\n0.072\n\n\nNeural Net\n0.117\n\n\nEnsemble of both\n0.861\n\n\n\nThe first place entry on the private leaderboard had an error of 0.074."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#load-the-data",
    "href": "posts/2023-09-14-rf/index.html#load-the-data",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Load the Data",
    "text": "Load the Data\n\n!pip install dtreeviz\n!pip install treeinterpreter\n!pip install waterfallcharts\n\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n  s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                    special_characters=True, rotate=False, precision=precision, **kwargs)\n  return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\nimport dtreeviz\nfrom fastcore.all import *\n\n\nfrom pathlib import Path\n\ncred_path = Path(\"~/.kaggle/kaggle.json\").expanduser()\nif not cred_path.exists():\n  cred_path.parent.mkdir(exist_ok=True)\n  cred_path.write_text(creds)\n  cred_path.chmod(0o600)\n\n\nimport zipfile,kaggle\n\npath = Path('zillow-prize-1')\nif not path.exists():\n  kaggle.api.competition_download_cli(str(path))\n  zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading zillow-prize-1.zip to /content\n\n\n100%|██████████| 340M/340M [00:04<00:00, 84.9MB/s]\n\n\n\n\n\n\npath.ls(file_type='text')\n\n(#5) [Path('zillow-prize-1/train_2017.csv'),Path('zillow-prize-1/sample_submission.csv'),Path('zillow-prize-1/train_2016_v2.csv'),Path('zillow-prize-1/properties_2016.csv'),Path('zillow-prize-1/properties_2017.csv')]\n\n\n\nproperties_2016 = pd.read_csv(path/'properties_2016.csv', low_memory=False)\ndf = pd.read_csv(path/'train_2016_v2.csv', low_memory=False)\n\n\ndf.shape\n\n(90275, 3)\n\n\n\ndf = df.merge(properties_2016, how='left', left_on='parcelid', right_on='parcelid')\n\n\ndf.shape\n\n(90275, 60)\n\n\n\ndf.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      parcelid\n      logerror\n      transactiondate\n      airconditioningtypeid\n      ...\n      taxamount\n      taxdelinquencyflag\n      taxdelinquencyyear\n      censustractandblock\n    \n  \n  \n    \n      0\n      11016594\n      0.0276\n      2016-01-01\n      1.0\n      ...\n      6735.88\n      NaN\n      NaN\n      6.037107e+13\n    \n    \n      1\n      14366692\n      -0.1684\n      2016-01-01\n      NaN\n      ...\n      10153.02\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      12098116\n      -0.0040\n      2016-01-01\n      1.0\n      ...\n      11484.48\n      NaN\n      NaN\n      6.037464e+13\n    \n    \n      3\n      12643413\n      0.0218\n      2016-01-02\n      1.0\n      ...\n      3048.74\n      NaN\n      NaN\n      6.037296e+13\n    \n    \n      4\n      14432541\n      -0.0050\n      2016-01-02\n      NaN\n      ...\n      5488.96\n      NaN\n      NaN\n      6.059042e+13\n    \n  \n\n5 rows × 60 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nCurrently I’m mainly looking for situations where parcelid is NA since that would indicate an issue in my merge. Looks like all parcelids are accounted for:\n\ndf.isna().sum()\n\nparcelid                        0\nlogerror                        0\ntransactiondate                 0\nairconditioningtypeid       61494\narchitecturalstyletypeid    90014\n                            ...  \nlandtaxvaluedollarcnt           1\ntaxamount                       6\ntaxdelinquencyflag          88492\ntaxdelinquencyyear          88492\ncensustractandblock           605\nLength: 60, dtype: int64\n\n\nThe dependent variable is logerror. Most of the logerror values are between +/- 0.25.\n\ndf.logerror.hist(range=[-1, 1]);\n\n\n\n\nThe competition’s data page doesn’t highlight any particular columns, so I’ll take a look at a few of them:\n\ndf.columns\n\nIndex(['parcelid', 'logerror', 'transactiondate', 'airconditioningtypeid',\n       'architecturalstyletypeid', 'basementsqft', 'bathroomcnt', 'bedroomcnt',\n       'buildingclasstypeid', 'buildingqualitytypeid', 'calculatedbathnbr',\n       'decktypeid', 'finishedfloor1squarefeet',\n       'calculatedfinishedsquarefeet', 'finishedsquarefeet12',\n       'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50',\n       'finishedsquarefeet6', 'fips', 'fireplacecnt', 'fullbathcnt',\n       'garagecarcnt', 'garagetotalsqft', 'hashottuborspa',\n       'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet',\n       'poolcnt', 'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7',\n       'propertycountylandusecode', 'propertylandusetypeid',\n       'propertyzoningdesc', 'rawcensustractandblock', 'regionidcity',\n       'regionidcounty', 'regionidneighborhood', 'regionidzip', 'roomcnt',\n       'storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid',\n       'unitcnt', 'yardbuildingsqft17', 'yardbuildingsqft26', 'yearbuilt',\n       'numberofstories', 'fireplaceflag', 'structuretaxvaluedollarcnt',\n       'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt',\n       'taxamount', 'taxdelinquencyflag', 'taxdelinquencyyear',\n       'censustractandblock'],\n      dtype='object')\n\n\n\ndf.calculatedfinishedsquarefeet.hist(range=[0,7500]);\n\n\n\n\n\ndf.bedroomcnt.hist(range=[0,6]);\n\n\n\n\n\ndf.yearbuilt.hist();"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#data-cleaning",
    "href": "posts/2023-09-14-rf/index.html#data-cleaning",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nHandle Ordinal Columns\nI’m not displaying the output here, but I looped through all of the columns and looked at their unique values to see if there were opportunities to set ordinal values—I did not find any.\n\nfor col in df.columns:\n  print(col, df[col].unique(), len(df[col].unique()))\n  print(\" \")\n\n\n\nHandle Dates\nThe only date field is transactiondate which is currently stored in the DataFrame as a str. I’ll convert it to a datetime object.\n\ntype(df.transactiondate[0])\n\nstr\n\n\n\ndf['transactiondate'] = pd.to_datetime(df['transactiondate'])\n\n\ndf.transactiondate[0], type(df.transactiondate[0])\n\n(Timestamp('2016-01-01 00:00:00'), pandas._libs.tslibs.timestamps.Timestamp)\n\n\nI’ll use the fastai add_datepart function to add additional columns associated with the transactiondate. I want to keep transactiondate field intact to make it easier to split the training and validation sets.\n\nlen(df.columns)\n\n60\n\n\n\ntransactiondate = df['transactiondate']\ndf = add_datepart(df, 'transactiondate')\n\n\nlen(df.columns)\n\n72\n\n\n\n' '.join(o for o in df.columns if o.startswith('transaction'))\n\n'transactionYear transactionMonth transactionWeek transactionDay transactionDayofweek transactionDayofyear transactionIs_month_end transactionIs_month_start transactionIs_quarter_end transactionIs_quarter_start transactionIs_year_end transactionIs_year_start transactionElapsed'\n\n\n\ndf['transactiondate'] = transactiondate"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#define-the-training-and-validation-sets",
    "href": "posts/2023-09-14-rf/index.html#define-the-training-and-validation-sets",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Define the Training and Validation Sets",
    "text": "Define the Training and Validation Sets\nThe data page of the competition states that the training data contains transactions mostly before 10/15/2016, whereas the test data contains transactions between 10/15/2016 and 12/31/2016. I’ll use the same split for my data.\nThere are 85670 records before October 15, 2016, and 4605 records on or after. These will become the training and validation sets, respectively.\n\nlen(df[df['transactiondate'] < '2016-10-15'])\n\n85670\n\n\n\ndf[df['transactiondate'] < '2016-10-15'].transactiondate.hist();\n\n\n\n\n\nlen(df[df['transactiondate'] >= '2016-10-15'])\n\n4605\n\n\n\ndf[df['transactiondate'] >= '2016-10-15'].transactiondate.hist();\n\n\n\n\n\ndep_var = 'logerror'\n\n\ncond = df.transactiondate < '2016-10-15'\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx), list(valid_idx))\n\n\nlen(train_idx), len(valid_idx)\n\n(85670, 4605)"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#create-the-decision-tree",
    "href": "posts/2023-09-14-rf/index.html#create-the-decision-tree",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Create the Decision Tree",
    "text": "Create the Decision Tree\nI’ll setup the TabularPandas object first, as done in the text:\n\nprocs = [Categorify, FillMissing]\n\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\ncat\n\n['hashottuborspa',\n 'propertycountylandusecode',\n 'propertyzoningdesc',\n 'fireplaceflag',\n 'taxdelinquencyflag',\n 'transactionYear',\n 'transactionIs_month_end',\n 'transactionIs_month_start',\n 'transactionIs_quarter_end',\n 'transactionIs_quarter_start',\n 'transactionIs_year_end',\n 'transactionIs_year_start',\n 'transactiondate']\n\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\ntype(to)\n\nfastai.tabular.core.TabularPandas\n\n\n\nlen(to.train), len(to.valid)\n\n(85670, 4605)\n\n\n\nto.show(3)\n\n\n\n  \n    \n      \n      hashottuborspa\n      propertycountylandusecode\n      propertyzoningdesc\n      fireplaceflag\n      taxdelinquencyflag\n      transactionYear\n      transactionIs_month_end\n      transactionIs_month_start\n      transactionIs_quarter_end\n      transactionIs_quarter_start\n      transactionIs_year_end\n      transactionIs_year_start\n      transactiondate\n      airconditioningtypeid_na\n      architecturalstyletypeid_na\n      basementsqft_na\n      buildingclasstypeid_na\n      buildingqualitytypeid_na\n      calculatedbathnbr_na\n      decktypeid_na\n      finishedfloor1squarefeet_na\n      calculatedfinishedsquarefeet_na\n      finishedsquarefeet12_na\n      finishedsquarefeet13_na\n      finishedsquarefeet15_na\n      finishedsquarefeet50_na\n      finishedsquarefeet6_na\n      fireplacecnt_na\n      fullbathcnt_na\n      garagecarcnt_na\n      garagetotalsqft_na\n      heatingorsystemtypeid_na\n      lotsizesquarefeet_na\n      poolcnt_na\n      poolsizesum_na\n      pooltypeid10_na\n      pooltypeid2_na\n      pooltypeid7_na\n      regionidcity_na\n      regionidneighborhood_na\n      regionidzip_na\n      storytypeid_na\n      threequarterbathnbr_na\n      typeconstructiontypeid_na\n      unitcnt_na\n      yardbuildingsqft17_na\n      yardbuildingsqft26_na\n      yearbuilt_na\n      numberofstories_na\n      structuretaxvaluedollarcnt_na\n      taxvaluedollarcnt_na\n      landtaxvaluedollarcnt_na\n      taxamount_na\n      taxdelinquencyyear_na\n      censustractandblock_na\n      parcelid\n      airconditioningtypeid\n      architecturalstyletypeid\n      basementsqft\n      bathroomcnt\n      bedroomcnt\n      buildingclasstypeid\n      buildingqualitytypeid\n      calculatedbathnbr\n      decktypeid\n      finishedfloor1squarefeet\n      calculatedfinishedsquarefeet\n      finishedsquarefeet12\n      finishedsquarefeet13\n      finishedsquarefeet15\n      finishedsquarefeet50\n      finishedsquarefeet6\n      fips\n      fireplacecnt\n      fullbathcnt\n      garagecarcnt\n      garagetotalsqft\n      heatingorsystemtypeid\n      latitude\n      longitude\n      lotsizesquarefeet\n      poolcnt\n      poolsizesum\n      pooltypeid10\n      pooltypeid2\n      pooltypeid7\n      propertylandusetypeid\n      rawcensustractandblock\n      regionidcity\n      regionidcounty\n      regionidneighborhood\n      regionidzip\n      roomcnt\n      storytypeid\n      threequarterbathnbr\n      typeconstructiontypeid\n      unitcnt\n      yardbuildingsqft17\n      yardbuildingsqft26\n      yearbuilt\n      numberofstories\n      structuretaxvaluedollarcnt\n      taxvaluedollarcnt\n      assessmentyear\n      landtaxvaluedollarcnt\n      taxamount\n      taxdelinquencyyear\n      censustractandblock\n      transactionMonth\n      transactionWeek\n      transactionDay\n      transactionDayofweek\n      transactionDayofyear\n      transactionElapsed\n      logerror\n    \n  \n  \n    \n      0\n      #na#\n      0100\n      LARS\n      #na#\n      #na#\n      2016\n      False\n      True\n      False\n      True\n      False\n      True\n      2016-01-01\n      False\n      True\n      True\n      True\n      False\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      False\n      11016594\n      1.0\n      7.0\n      643.5\n      2.0\n      3.0\n      4.0\n      4.0\n      2.0\n      66.0\n      1247.0\n      1684.0\n      1684.0\n      1440.0\n      2101.5\n      1250.0\n      1921.0\n      6037.0\n      1.0\n      2.0\n      2.0\n      432.0\n      2.0\n      34280992.0\n      -118488536.0\n      7528.0\n      1.0\n      500.0\n      1.0\n      1.0\n      1.0\n      261.0\n      60371068.0\n      12447.0\n      3101.0\n      31817.0\n      96370.0\n      0.0\n      7.0\n      1.0\n      6.0\n      1.0\n      260.0\n      156.0\n      1959.0\n      1.0\n      122754.0\n      360170.0\n      2015.0\n      237416.0\n      6735.879883\n      14.0\n      6.037107e+13\n      1\n      53\n      1\n      4\n      1\n      1.451606e+09\n      0.0276\n    \n    \n      1\n      #na#\n      1\n      #na#\n      #na#\n      #na#\n      2016\n      False\n      True\n      False\n      True\n      False\n      True\n      2016-01-01\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n      True\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      False\n      True\n      False\n      True\n      True\n      True\n      True\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      True\n      14366692\n      1.0\n      7.0\n      643.5\n      3.5\n      4.0\n      4.0\n      7.0\n      3.5\n      66.0\n      1247.0\n      2263.0\n      2263.0\n      1440.0\n      2101.5\n      1250.0\n      1921.0\n      6059.0\n      1.0\n      3.0\n      2.0\n      468.0\n      2.0\n      33668120.0\n      -117677552.0\n      3643.0\n      1.0\n      500.0\n      1.0\n      1.0\n      1.0\n      261.0\n      60590524.0\n      32380.0\n      1286.0\n      118887.0\n      96962.0\n      0.0\n      7.0\n      1.0\n      6.0\n      1.0\n      260.0\n      156.0\n      2014.0\n      1.0\n      346458.0\n      585529.0\n      2015.0\n      239071.0\n      10153.019531\n      14.0\n      6.037620e+13\n      1\n      53\n      1\n      4\n      1\n      1.451606e+09\n      -0.1684\n    \n    \n      2\n      #na#\n      0100\n      PSR6\n      #na#\n      #na#\n      2016\n      False\n      True\n      False\n      True\n      False\n      True\n      2016-01-01\n      False\n      True\n      True\n      True\n      False\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      False\n      12098116\n      1.0\n      7.0\n      643.5\n      3.0\n      2.0\n      4.0\n      4.0\n      3.0\n      66.0\n      1247.0\n      2217.0\n      2217.0\n      1440.0\n      2101.5\n      1250.0\n      1921.0\n      6037.0\n      1.0\n      3.0\n      2.0\n      432.0\n      2.0\n      34136312.0\n      -118175032.0\n      11423.0\n      1.0\n      500.0\n      1.0\n      1.0\n      1.0\n      261.0\n      60374640.0\n      47019.0\n      3101.0\n      275411.0\n      96293.0\n      0.0\n      7.0\n      1.0\n      6.0\n      1.0\n      260.0\n      156.0\n      1940.0\n      1.0\n      61994.0\n      119906.0\n      2015.0\n      57912.0\n      11484.480469\n      14.0\n      6.037464e+13\n      1\n      53\n      1\n      4\n      1\n      1.451606e+09\n      -0.0040\n    \n  \n\n\n\nUnderlying values for categorical variables are numeric:\n\nto.items[['transactionYear', 'transactionIs_month_end', 'transactionIs_month_start']].head(3)\n\n\n\n  \n    \n\n\n  \n    \n      \n      transactionYear\n      transactionIs_month_end\n      transactionIs_month_start\n    \n  \n  \n    \n      0\n      1\n      1\n      2\n    \n    \n      1\n      1\n      1\n      2\n    \n    \n      2\n      1\n      1\n      2\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow I can build the decision tree following the steps given in the textbook:\n\n# make sure logerror looks good\nto.train.y[:5]\n\n0    0.0276\n1   -0.1684\n2   -0.0040\n3    0.0218\n4   -0.0050\nName: logerror, dtype: float32\n\n\n\nxs,y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs,y);\n\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\nThe smallest average logerror value in this model is when finishedsquarefeet12 (which according to their data dictionary is the “Finished living area”) is greater than 337, which is for 85645 rows which is basically the whole training set.\n\nView the Data for Outliers\nAs done in the book, I’ll look at a sample of the data and visualize it in more detail:\n\nsamp_idx = np.random.permutation(len(y))[:500]\n\nviz_model=dtreeviz.model(m,\n                         X_train=xs.iloc[samp_idx],\n                         y_train=y.iloc[samp_idx],\n                         feature_names=xs.columns,\n                         target_name=dep_var)\n\nviz_model.view(fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n               orientation='LR', fancy=False)\n\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n\n\n\n\n\nIn this case, since such a large portion of the data falls into the split of finishedsquarefeet12 > 337, the visualization is not helpful as there are 0 or just a few values left for the other splits even at larger sample sizes (I tried 500, 1000 and 5000).\nAs done in the text, I’ll now build a bigger tree without passing it any stopping criteria, and create a loss function (mean absolute error) to match the competition:\n\nm = DecisionTreeRegressor()\nm.fit(xs,y);\n\n\ndef mae(pred, y): return (pred - y).abs().mean()\ndef m_mae(m, xs, y): return mae(m.predict(xs), y)\n\nHere’s the mean absolute error on the training set:\n\nm_mae(m, xs, y)\n\n0.0\n\n\nHere’s the mean absolute error on the validation set:\n\nm_mae(m, valid_xs, valid_y)\n\n0.15422440832733592\n\n\nThe validation error is much larger than the training set error because the model is overfitting! As was the case in the textbook, the reason is that we have almost as many leaf nodes as we do training sample. It’s basically memorizing the training set.\n\nm.get_n_leaves(), len(xs)\n\n(83323, 85670)\n\n\nI’ll use the same stopping criteria as the text (every leaf node should contain at least 25 rows) and create a new model:\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_mae(m, xs, y), m_mae(m, valid_xs, valid_y)\n\n(0.07031707298053898, 0.08723056533912212)\n\n\nThe validation and training set errors are now similar.\n\nm.get_n_leaves(), m.get_n_leaves()/len(xs)\n\n(2688, 0.03137621104237189)\n\n\nWe now have about 3% as many leaves as we do training samples."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#creating-a-random-forest",
    "href": "posts/2023-09-14-rf/index.html#creating-a-random-forest",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Creating a Random Forest",
    "text": "Creating a Random Forest\nI’ll use mostly the same random forest function as is used in the text, except for max_samples I’ll use 40_000 which is about half of my training set (they used 200_000 which was about half of their training set).\n\ndef rf(xs, y, n_estimators=40, max_samples=40_000, max_features=0.5, min_samples_leaf=5, **kwargs):\n  return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples,\n                               max_features=max_features, min_samples_leaf=min_samples_leaf,\n                               oob_score=True).fit(xs,y)\n\n\nm = rf(xs, y);\n\n\nm_mae(m, xs, y), m_mae(m, valid_xs, valid_y)\n\n(0.05653021493284958, 0.07388558834504737)\n\n\nBoth the training and validation set errors are lower than a model with a single decision tree.\nAs done in the textbook, I’ll plot how the mean absolute error changes as the number of trees used for predictions increases.\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]);\n\n\npreds.shape\n\n(40, 4605)\n\n\nThe model’s validation error is the same as the error between the mean prediction across all trees and the validation set.\n\nmae(preds.mean(0), valid_y)\n\n0.07388558834504737\n\n\n\npreds.mean(0).shape\n\n(4605,)\n\n\n\nplt.plot([mae(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\nAs the number of trees used for prediction increases, the mean absolute error on the validation set decreases. This decrease doesn’t fully flatten out like it did in the textbook example, so I could probably use more trees to get a better result.\nNext I’ll calculate the out-of-bag error and compare it with the validation error. I use the training targets y since out-of-bag predictions are calculated using different subsets of the training data.\n\nlen(m.oob_prediction_)\n\n85670\n\n\n\nmae(m.oob_prediction_, y)\n\n0.07103692229370927\n\n\nAs is the case in the text, the OOB error is a bit smaller than the validation error."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#model-interpretation",
    "href": "posts/2023-09-14-rf/index.html#model-interpretation",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nTree Variance for Prediction Confidence\nI’ll start by observing the standard deviation of the predictions across the 40 trees for each row in the validation set.\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]);\n\n\npreds_std = preds.std(0)\npreds_std.shape\n\n(4605,)\n\n\n\nplt.hist(preds_std);\n\n\n\n\n\nmin(preds_std), max(preds_std), max(preds_std)/min(preds_std)\n\n(0.0278703429008966, 0.694632232470765, 24.923705996039914)\n\n\nAs is the case in the text, the standard deviation varies widely for each prediction, with about a 20x difference between the smallest and largest standard deviations.\n\n\nFeature Importance\n\ndef rf_feat_importance(m, df):\n  return pd.DataFrame({\n      'cols': df.columns,\n      'imp': m.feature_importances_}\n                      ).sort_values('imp', ascending=False)\n\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      101\n      structuretaxvaluedollarcnt\n      0.068990\n    \n    \n      105\n      taxamount\n      0.063532\n    \n    \n      80\n      lotsizesquarefeet\n      0.056561\n    \n    \n      102\n      taxvaluedollarcnt\n      0.054181\n    \n    \n      78\n      latitude\n      0.053154\n    \n    \n      104\n      landtaxvaluedollarcnt\n      0.052342\n    \n    \n      67\n      finishedsquarefeet12\n      0.050304\n    \n    \n      66\n      calculatedfinishedsquarefeet\n      0.050167\n    \n    \n      99\n      yearbuilt\n      0.048438\n    \n    \n      55\n      parcelid\n      0.048166\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nInteresting to see that in this case, which is different than the textbook example, the feature importance across the top-10 most important features is pretty similar. The top-10 most important features include 4 columns related to the tax value of the property, 2 columns related to the square feet of the property or land, location (lat/long) columns, the year the property was built and parcelid.\n\ndef plot_fi(fi):\n  return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\n\nRemoving Low-Importance Variables\nI’ll retrain the model after removing features with low importance (0.005), which leaves me with 28 columns instead of 73.\n\nlen(df.columns)\n\n73\n\n\n\nto_keep = fi[fi.imp>0.005].cols\nlen(to_keep)\n\n28\n\n\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\nm = rf(xs_imp, y);\n\n\nm_mae(m, xs_imp, y), m_mae(m, valid_xs_imp, valid_y)\n\n(0.05664031645342886, 0.07351868669419753)\n\n\nThe training error increased slightly and the validation decreased when compared to the model containing all 73 columns.\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\nRemoving Redundant Features\n\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n\n\ncluster_columns(xs_imp)\n\n\n\n\nThe most similar columns are the columns that are paired together at the right edge of the plot.\nAs is done in the text, I’ll create a function that quickly trains a smaller random forest (using only a quarter of the max samples used earlier) and returns the OOB score (which is 1.0 for a perfect model and 0.0 for a random model):\n\ndef get_oob(df):\n  m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n                            max_samples=10_000, max_features=0.5, n_jobs=-1, oob_score=True)\n  m.fit(df, y)\n  return m.oob_score_\n\n\n# baseline\nget_oob(xs_imp)\n\n0.011697584107195569\n\n\nRemove each of the potentially redundant variables and compare the OOB score to the baseline:\n\n{c: get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'transactionElapsed', 'transactiondate', 'transactionDayofyear', 'transactionWeek',\n    'finishedsquarefeet12', 'calculatedfinishedsquarefeet',\n    'landtaxvaluedollarcnt', 'taxvaluedollarcnt',\n    'censustractandblock', 'rawcensustractandblock')}\n\n{'transactionElapsed': 0.013909569573363978,\n 'transactiondate': 0.01231524099367498,\n 'transactionDayofyear': 0.013553857476552356,\n 'transactionWeek': 0.013361881019678945,\n 'finishedsquarefeet12': 0.013417734028419948,\n 'calculatedfinishedsquarefeet': 0.013048107145456234,\n 'landtaxvaluedollarcnt': 0.012660367356422841,\n 'taxvaluedollarcnt': 0.0128850786659368,\n 'censustractandblock': 0.012599787787639927,\n 'rawcensustractandblock': 0.012632301991060135}\n\n\nI’ll pick the variables where their removal increased the OOB score.\n\nto_drop = ['transactionElapsed',\n           'transactionDayofyear',\n           'transactionWeek',\n           'finishedsquarefeet12',\n           'taxvaluedollarcnt',\n           'rawcensustractandblock']\n\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.012975220827415757\n\n\nGreat! I now have a few less features and the OOB score increased.\nI’ll save the data and model for later so that I can maintain the previous work:\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\n# check accuracy\nm = rf(xs_final, y)\nm_mae(m, xs_final, y), m_mae(m, valid_xs_final, valid_y)\n\n(0.05687215149513172, 0.07364818858339096)\n\n\n\nsave_pickle('xs_final.pkl', xs_final)\nsave_pickle('valid_xs_final.pkl', valid_xs_final)\nsave_pickle('m.pkl', m)\n\n\n\nPartial Dependence\nAs done in the text, I’ll look at the distribution of the top 2 most important features.\n\nvalid_xs_final.structuretaxvaluedollarcnt.hist(range=[0,0.1e7]);\n\n\n\n\n\nvalid_xs_final.taxamount.hist(range=[0,30_000]);\n\n\n\n\nNext, I’ll create partial dependence plots, which observe how the dependent variable varies with respect to each of these variables.\n\nfrom sklearn.inspection import partial_dependence\n\nfig, ax = plt.subplots(figsize=(6,4))\npdp = partial_dependence(m, valid_xs_final, ['structuretaxvaluedollarcnt', 'taxamount'],\n                        grid_resolution=20)\n\nax.plot(pdp['values'][0], pdp['average'].mean(axis=1).squeeze());\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6,4))\nax.plot(pdp['values'][1], pdp['average'].mean(axis=2).squeeze());\n\n\n\n\nIn both cases, the logerror generally decreases as the value of the variable increases. Although for larger taxamount values, the logerror increases before decreasing again.\n\n\nTree Interpreter\n\nxs_final = load_pickle('xs_final.pkl')\nvalid_xs_final = load_pickle('valid_xs_final.pkl')\nm = load_pickle('m.pkl')\nxs_final.shape, valid_xs_final.shape\n\n((85670, 26), (4605, 26))\n\n\nI’ll look at the contribution of different features to the prediction of a single row:\n\nrow = valid_xs_final.iloc[:1]\nrow\n\n\n\n  \n    \n\n\n  \n    \n      \n      structuretaxvaluedollarcnt\n      taxamount\n      lotsizesquarefeet\n      latitude\n      ...\n      bedroomcnt\n      garagetotalsqft\n      propertycountylandusecode\n      bathroomcnt\n    \n  \n  \n    \n      3421\n      128318.0\n      4382.959961\n      10440.0\n      33844408.0\n      ...\n      2.0\n      400.0\n      59\n      2.5\n    \n  \n\n1 rows × 22 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\nfrom treeinterpreter import treeinterpreter\nprediction, bias, contributions = treeinterpreter.predict(m, row.values)\n\n\nprediction[0], bias[0], contributions[0].sum(), contributions[0].sum()/prediction[0]\n\n(array([0.01924753]),\n 0.011195491642025518,\n 0.00805203672769338,\n array([0.41834134]))\n\n\nThe features’ contribution to the prediction is about 40%.\n\nfrom waterfall_chart import plot as waterfall\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.5,\n          rotation_value=45, formatting='{:,.3f}');"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#finding-out-of-domain-data",
    "href": "posts/2023-09-14-rf/index.html#finding-out-of-domain-data",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Finding Out-of-Domain Data",
    "text": "Finding Out-of-Domain Data\nIn order to understand which features in the validation set are out-of-domain for the training set, we’ll train a model to predict whether a row in the data is in the training or validation set—the features with the highest importance in this model are the features that are most different between the two datasets.\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\ndf_dom.shape, len(is_valid)\n\n((90275, 22), 90275)\n\n\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n  \n    \n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      14\n      transactiondate\n      0.996253\n    \n    \n      16\n      transactionDayofweek\n      0.001673\n    \n    \n      10\n      transactionDay\n      0.001005\n    \n    \n      1\n      taxamount\n      0.000206\n    \n    \n      9\n      regionidzip\n      0.000110\n    \n    \n      3\n      latitude\n      0.000095\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nOf course transactiondate is different between the two sets—that was done intentionally to match the way the competition splits training and test data. The next two most important features are relatively unimportant, but I’ll still remove each of the three and see if it improves the model:\n\n# baseline\nm = rf(xs_final, y)\nprint('orig', m_mae(m, valid_xs_final, valid_y))\n\nfor c in ('transactiondate', 'transactionDayofweek', 'transactionDay'):\n  m = rf(xs_final.drop(c, axis=1), y)\n  print(c, m_mae(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.07414874259938188\ntransactiondate 0.07169995892420085\ntransactionDayofweek 0.07419470238304429\ntransactionDay 0.07380756091486504\n\n\nI’ll remove transactiondate and transactionDay, which should reduce the error.\n\ntime_vars = ['transactiondate', 'transactionDay']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_mae(m, valid_xs_time, valid_y)\n\n0.07191006179240521\n\n\n\n0.07191006179240521/0.07414874259938188\n\n0.9698082431542766\n\n\nGreat! My error decreased by about 3%.\nAt this point in the textbook they train the model on recent years’ data. For this dataset, all of the data comes from the same year so it doesn’t make sense to do the same."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#using-a-neural-network",
    "href": "posts/2023-09-14-rf/index.html#using-a-neural-network",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Using a Neural Network",
    "text": "Using a Neural Network\n\ndf_nn = pd.read_csv(path/'train_2016_v2.csv', low_memory=False)\ndf_nn = df_nn.merge(properties_2016, how='left', left_on='parcelid', right_on='parcelid')\ndf_nn['transactiondate'] = pd.to_datetime(df_nn['transactiondate'])\ndf_nn = add_datepart(df_nn, 'transactiondate')\n\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\n\nxs_final_time.shape, df_nn_final.shape\n\n((85670, 20), (90275, 21))\n\n\n\ncont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\n\n# look at cardinality\ndf_nn_final[cat_nn].nunique()\n\npropertyzoningdesc           1996\ntransactionDayofweek            7\npropertycountylandusecode      77\ndtype: int64\n\n\n\n# look at cardinality\ndf_nn_final[cont_nn].nunique()\n\nstructuretaxvaluedollarcnt      55450\ntaxamount                       85110\nlotsizesquarefeet               20016\nlatitude                        73312\nlandtaxvaluedollarcnt           57066\ncalculatedfinishedsquarefeet     5102\nyearbuilt                         130\nparcelid                        90150\nlongitude                       71900\nregionidzip                       388\ncensustractandblock             42398\nregionidcity                      177\nregionidneighborhood              494\nfinishedsquarefeet15             1915\nbedroomcnt                         17\ngaragetotalsqft                   870\nbathroomcnt                        23\ndtype: int64\n\n\nSome of the continuous variables are categorical in nature (latitude, censustractandblock, etc.) but have very high cardinality (tens of thousands) so instead of creating very large embeddings, I’ll keep them as continuous variables.\nSome of the other variables that cont_cat_split has determined to be continuous have a relatively small cardinality (regionidzip, regionidcity, and regionidneighborhood) so I’ll move those over to cat_nn.\n\ncont_nn.remove('regionidzip')\ncont_nn.remove('regionidcity')\ncont_nn.remove('regionidneighborhood')\n\ncat_nn.append('regionidzip')\ncat_nn.append('regionidcity')\ncat_nn.append('regionidneighborhood')\n\n\ndf_nn_final[cat_nn].nunique()\n\npropertyzoningdesc           1996\ntransactionDayofweek            7\npropertycountylandusecode      77\nregionidzip                   388\nregionidcity                  177\nregionidneighborhood          494\ndtype: int64\n\n\n\ndf_nn_final[cont_nn].nunique()\n\nstructuretaxvaluedollarcnt      55450\ntaxamount                       85110\nlotsizesquarefeet               20016\nlatitude                        73312\nlandtaxvaluedollarcnt           57066\ncalculatedfinishedsquarefeet     5102\nyearbuilt                         130\nparcelid                        90150\nlongitude                       71900\ncensustractandblock             42398\nfinishedsquarefeet15             1915\nbedroomcnt                         17\ngaragetotalsqft                   870\nbathroomcnt                        23\ndtype: int64\n\n\nI can now go ahead and build a TabularPandas object:\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)\n\nand DataLoaders:\n\ndls = to_nn.dataloaders(1024)\n\nDefine the y_range for regression:\n\ny = to_nn.train.y\ny.min(), y.max()\n\n(-4.605, 4.737)\n\n\nAnd train the neural net (I’m using the same number of layers as they did in the textbook):\n\nfrom fastai.tabular.all import *\nlearn = tabular_learner(dls, y_range=(-5,5), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00019054606673307717)\n\n\n\n\n\nThe loss curve looks stable and reasonable so I’ll go ahead and use the suggested valley learning rate.\n\nlearn.fit_one_cycle(5, 2e-4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.295388\n      0.082879\n      00:07\n    \n    \n      1\n      0.101962\n      0.052496\n      00:04\n    \n    \n      2\n      0.059026\n      0.042152\n      00:05\n    \n    \n      3\n      0.044456\n      0.038492\n      00:04\n    \n    \n      4\n      0.038725\n      0.038634\n      00:05\n    \n  \n\n\n\nThe validation loss increases between the 4th and 5th epoch so the model is starting to overfit.\n\npreds, targs = learn.get_preds()\nmae(preds, targs).item()\n\n\n\n\n\n\n\n\n0.11708459258079529\n\n\n\n0.11/0.07\n\n1.5714285714285714\n\n\nThe mean absolute error of the neural net predictions is about 60% larger than my random forest!"
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#ensembling",
    "href": "posts/2023-09-14-rf/index.html#ensembling",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Ensembling",
    "text": "Ensembling\nThe final part of this exercise is ensembling the predictions between my random forest and my neural net, and seeing how the error compares.\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) / 2\n\n\ndef mae(pred, y): return (pred - y).abs().mean()\n\n\nmae(ens_preds, valid_y)\n\n0.08611476519897165\n\n\nWhile smaller than my neural net’s error, this error is still significantly larger than my random forest."
  },
  {
    "objectID": "posts/2023-09-14-rf/index.html#final-thoughts",
    "href": "posts/2023-09-14-rf/index.html#final-thoughts",
    "title": "Building a Random Forest for the Kaggle Zillow Competition Dataset",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis was a really enjoyable exercise. I had a lot of fun working through the textbook’s process using a different dataset. There were many similarities (time-related variables were causing out-of-domain issues for the validation set) and differences (my neural net performed worse than my random forest, while in the chapter the results were flipped).\nI noticed as I was re-running the code in this notebook that there were slight differences each time I created my random forest. For example, sometimes parcelid was the 11th-most important feature, other times it was in the top 10. There were also different redundant features for each model run. Is this normal and expected? Or is there something about this data which makes the modeling process less consistent?\nI’ll end by summarizing my models’ results again, noting that my random forest beat the first place entry in the private leaderboard (which had an error of 0.074):\n\n\n\nModel\nMean Absolute Error\n\n\n\n\nRandom Forest\n0.072\n\n\nNeural Net\n0.117\n\n\nEnsemble of both\n0.861\n\n\n\nI hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "href": "posts/2023-02-19-honey-bbq-chicken-drumsticks/2023-02-19-honey-bbq-chicken-drumsticks.html",
    "title": "Making Chicken Wings",
    "section": "",
    "text": "Background\nRestaurant-bought wings are pricey. Even fast-food bought wings, which can be miserably made some times—I once bought advertised Honey BBQ Chicken Wings which were great the first time, but the second time looked like plain wings with barbeque sauce drizzled over them–were about $20 for 16 wings. I don’t mind the price, gratefully, but I do mind consistent quality and want to avoide letdowns, so I chose to learn how to make wings. I also need to fill the gap caused by the lack of NFL and College Football during the offseason, so this is a tasty project which serves that purpose as well.\n\n\nFirst Attempt\nI made my first batch of chicken wings on Saturday, February 11, 2023. They were edible and the sauce was delicious. But they were at most a 5/10. I follow some recipe I found online after a google search which went something like:\n\nPreheat oven to 400°F\nBaste oil on both sides of the wings and season with salt and pepper\nBake for 20 minutes, flipping the wings after 10 minutes\nHeat the oven to 425°F\nBaste on the sauce and bake for 7 minutes then flip. Repeat a few times.\nBroil (500°F) for 5-10 minutes.\n\nMy sauce was made of Ray’s sugar-free BBQ sauce, mustard, soy sauce, and honey.\nI did not have a basting brush and was using a spoon to lather on the sauce. It did not work well. Also, the meat did not fall off the bones and it took more effort than worthwhile to eat them.\n\n\nSecond Attempt\nI was hungrier this time I suppose because I chose to make honey bbq drumsticks instead of wings. This time I got a silicone basting brush at Safeway. I modified my approach slightly:\n\nPreheat oven to 400°F\nActually baste this time and season the drumsticks\nBake on one side for 15 minutes, flip and bake for another 15 minutes\nRemove the drumsticks and heat the oven to 425°F\nRepeat four times, twice on each side: baste on sauce, bake for 7 minutes, remove and flip.\nAdd coconut flour to the sauce (note to self: use rice flour instead)\nRepeat two times, once on each side: baste on sauce/flour mix (gravy?), bake for 7 minutes and flip.\nBroil (500°F) for 5 minutes.\n\nUsing the basting brush allowed for a more even distribution of sauce on the drumsticks. The skin still wasn’t crispy enough, although the meat was (more) easily coming off the bones and was juicy + delicious.\nWhen I make it again, probably next weekend, I’ll switch back to chicken wings and find a new recipe which emphasizes the cripsiness of the wings.\n\n\n\nclose-up of my honey bbq chicken drumsticks with a few charred spots."
  },
  {
    "objectID": "posts/2023-06-01-practical-deep-learning-for-coders-part-1/2023_06_01_practical_deep_learning_for_coders_part_1.html",
    "href": "posts/2023-06-01-practical-deep-learning-for-coders-part-1/2023_06_01_practical_deep_learning_for_coders_part_1.html",
    "title": "Practical Deep Learnings For Coders - Part 1 Notes and Examples",
    "section": "",
    "text": "Vishal Bakshi\nThis notebook contains my notes (of course videos, example notebooks and book chapters) and exercises of Part 1 of the course Practical Deep Learning for Coders.\n\n\n\n\nThe first thing I did was to run through the lesson 1 notebook from start to finish. In this notebook, they download training and validation images of birds and forests then train an image classifier with 100% accuracy in identifying images of birds.\nThe first exercise is for us to create our own image classifier with our own image searches. I’ll create a classifier which accurately predicts an image of an alligator.\nI’ll start by using their example code for getting images using DuckDuckGo image search:\n\n# It's a good idea to ensure you're running the latest version of any libraries you need.\n# `!pip install -Uqq <libraries>` upgrades to the latest version of <libraries>\n# NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities\n!pip install -Uqq fastai fastbook duckduckgo_search timm\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nThe search_images function takes a search term and max_images maximum number of images value. It prints out a line of text that it’s \"Searching for\" the term and returns an L object with the image URL.\nThe ddg_images function returns a list of JSON objects containing the title, image URL, thumbnail URL, height, width and source of the image.\n\nsearch_object = ddg_images('alligator', max_results=1)\nsearch_object\n\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n[{'title': 'The Creature Feature: 10 Fun Facts About the American Alligator | WIRED',\n  'image': 'https://www.wired.com/wp-content/uploads/2015/03/Gator-2.jpg',\n  'thumbnail': 'https://tse4.mm.bing.net/th?id=OIP.FS96VErnOXAGSWU092I_DQHaE8&pid=Api',\n  'url': 'https://www.wired.com/2015/03/creature-feature-10-fun-facts-american-alligator/',\n  'height': 3456,\n  'width': 5184,\n  'source': 'Bing'}]\n\n\nWrapping this list in L object and calling .itemgot('image') on it extracts URL value associated with the image key in the JSON object.\n\nL(search_object).itemgot('image')\n\n(#1) ['https://www.wired.com/wp-content/uploads/2015/03/Gator-2.jpg']\n\n\nNext, they provide some code to download the image to a destination filename and view the image:\n\nurls = search_images('alligator', max_images=1)\n\nfrom fastdownload import download_url\ndest = 'alligator.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\nSearching for 'alligator'\n\n\n\n\n\nFor my not-alligator images, I’ll use images of a swamp.\n\ndownload_url(search_images('swamp photos', max_images=1)[0], 'swamp.jpg', show_progress=False)\nImage.open('swamp.jpg').to_thumb(256,256)\n\nSearching for 'swamp photos'\n\n\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/usr/local/lib/python3.9/dist-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n\n\n\nIn the following code, I’ll search for both terms, alligator and swamp and store the images in alligator_or_not/alligator and alligator_or_not/swamp paths, respectively.\nThe parents=TRUE argument creates any intermediate parent directories that don’t exist (in this case, the alligator_or_not directory). The exist_ok=TRUE argument suppresses the FileExistsError and does nothing.\n\nsearches = 'swamp','alligator'\npath = Path('alligator_or_not')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'swamp photo'\nSearching for 'swamp sun photo'\nSearching for 'swamp shade photo'\nSearching for 'alligator photo'\nSearching for 'alligator sun photo'\nSearching for 'alligator shade photo'\n\n\nNext, I’ll train my model using the code they have provided.\nThe get_image_files function is a fastai function which takes a Path object and returns an L object with paths to the image files.\n\ntype(get_image_files(path))\n\nfastcore.foundation.L\n\n\n\nget_image_files(path)\n\n(#349) [Path('alligator_or_not/swamp/1b3c3a61-0f7f-4dc2-a704-38202d593207.jpg'),Path('alligator_or_not/swamp/9c9141f2-024c-4e26-b343-c1ca1672fde8.jpeg'),Path('alligator_or_not/swamp/1340dd85-5d98-428e-a861-d522c786c3d7.jpg'),Path('alligator_or_not/swamp/2d3f91dc-cc5f-499b-bec6-7fa0e938fb13.jpg'),Path('alligator_or_not/swamp/84afd585-ce46-4016-9a09-bd861a5615db.jpg'),Path('alligator_or_not/swamp/6222f0b6-1f5f-43ec-b561-8e5763a91c61.jpg'),Path('alligator_or_not/swamp/a71c8dcb-7bbb-4dba-8ae6-8a780d5c27c6.jpg'),Path('alligator_or_not/swamp/bbd1a832-a901-4e8f-8724-feac35fa8dcb.jpg'),Path('alligator_or_not/swamp/45b358b3-1a12-41d4-8972-8fa98b2baa52.jpg'),Path('alligator_or_not/swamp/cf664509-8eb6-42c8-9177-c17f48bc026b.jpg')...]\n\n\nThe fastai parent_label function takes a Path object and returns a string of the file’s parent folder name.\n\nparent_label(Path('alligator_or_not/swamp/18b55d4f-3d3b-4013-822b-724489a23f01.jpg'))\n\n'swamp'\n\n\nSome image files that are downloaded may be corrupted, so they have provided a verify_images function to find images that can’t be opened. Those images are then removed (unlinked) from the path.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\n\nfailed\n\n(#1) [Path('alligator_or_not/alligator/1eb55508-274b-4e23-a6ae-dbbf1943a9d1.jpg')]\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\nI’ll train the model using their code which uses the resnet18 image classification model, and fine_tunes it for 3 epochs.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.690250\n      0.171598\n      0.043478\n      00:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.127188\n      0.001747\n      0.000000\n      00:02\n    \n    \n      1\n      0.067970\n      0.006409\n      0.000000\n      00:02\n    \n    \n      2\n      0.056453\n      0.004981\n      0.000000\n      00:02\n    \n  \n\n\n\nThe accuracy is 100%.\nNext, I’ll test the model as they’ve done in the lesson.\n\nPILImage.create('alligator.jpg').to_thumb(256,256)\n\n\n\n\n\nis_alligator,_,probs = learn.predict(PILImage.create('alligator.jpg'))\nprint(f\"This is an: {is_alligator}.\")\nprint(f\"Probability it's an alligator: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is an: alligator.\nProbability it's an alligator: 1.0000\n\n\n\n\n\nIn this section, I’ll take notes while I watch the lesson 1 video.\n\nThis is the fifth version of the course!\nWhat seemed impossible in 2015 (image recognition of a bird) is now free and something we can build in 2 minutes.\nAll models need numbers as their inputs. Images are already stored as numbers in computers. [PixSpy] allows you to (among other things) view the color of each pixel in an image file.\nA DataBlock gives fastai all the information it needs to create a computer vision model.\nCreating really interesting, real, working programs with deep learning is something that doesn’t take a lot of code, math, or more than a laptop computer. It’s pretty accessible.\nDeep Learning models are doing things that very few, if any of us, believed would be possible to do by computers in our lifetime.\nSee the Practical Data Ethics course as well.\nMeta Learning: How To Learn Deep Learning And Thrive In The Digital World.\nBooks on learning/education:\n\nMathematician’s Lament by Paul Lockhart\nMaking Learning Whole by David Perkins\n\nWhy are we able to create a bird-recognizer in a minute or two? And why couldn’t we do it before?\n\n2012: Project looking at 5-year survival of breast cancer patients, pre-deep learning approach\n\nAssembled a team to build ideas for thousands of features that required a lot of expertise, took years.\nThey fed these features into a logistic regression model to predict survival.\nNeural networks don’t require us to build these features, they build them for us.\n\n2015: Matthew D. Zeiler and Rob Fergus looked inside a neural network to see what it had learned.\n\nWe don’t give it features, we ask it to learn features.\nThe neural net is the basic function used in deep learning.\nYou start with a random neural network, feed it examples and you have it learn to recognize things.\nThe deeper you get, the more sophisticated the features it can find are.\nWhat we’re going to learn is how neural networks do this automatically.\nThis is the key difference in why we can now do things that we couldn’t previously conceive of as possible.\n\n\nAn image recognizer can also be used to classify sounds (pictures of waveforms).\nTurning time series into pictures for image classification.\nfastai is built on top of PyTorch.\n!pip install -Uqq fastai to update.\nAlways view your data at every step of building a model.\nFor computer vision algorithms you don’t need particularly big images.\nFor big images, most of the time is taken up opening it, the neural net on the GPU is must faster.\nThe main thing you’re going to try and figure out is how do I get this data into my model?\nDataBlock\n\nblocks=(ImageBlock, CategoryBlock): ImageBlock is the type of input to the model, CategoryBlock is the type of model output\nget_image_files(path) returns a list of all image files in a path.\nIt’s critical that you put aside some data for testing the accuracy of your model (validation set) with something like RandomSplitter for the splitter parameter.\nget_y tells fastai how to get the correct label for the photo.\nMost computer vision architectures need all of your inputs to be the same size, using Resize (either crop out a piece in the middle or squish the image) for the parameter item_tfms.\nDataLoaders contains iterators that PyTorch can run through to grab batches of your data to feed the training algorithm.\nshow_batch shows you a batch of input/label pairs.\nA Learner combines a model (the actual neural network that we are training) and the data we use to train it with.\nPyTorch Image Models (timm).\nresnet has already been trained to recognize over 1 million images of over 1000 different types. fastai downloads this so you can start with a neural network that can do a lot.\nfine_tune takes those pretrained weights downloaded for you and adjusts them in a carefully controlled way to teach the model differences between your dataset and what it was originally trained for.\nYou pass .predict an image, which is how you would deploy your model, returns whether it’s a bird or not as a string, integer and probability of whether it’s a bird (in this example).\n\n\nIn the code blocks below, I’ll train the different types of models presented in the video lesson.\n\n\n\nfrom fastai.vision.all import *\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs = 8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.454409\n      3.015761\n      00:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.928762\n      1.719756\n      00:02\n    \n    \n      1\n      1.649520\n      1.394089\n      00:02\n    \n    \n      2\n      1.533350\n      1.344445\n      00:02\n    \n    \n      3\n      1.414438\n      1.279674\n      00:02\n    \n    \n      4\n      1.291168\n      1.063977\n      00:02\n    \n    \n      5\n      1.174492\n      0.980055\n      00:02\n    \n    \n      6\n      1.073124\n      0.931532\n      00:02\n    \n    \n      7\n      0.992161\n      0.922516\n      00:02\n    \n  \n\n\n\n\nlearn.show_results(max_n=3, figsize=(7,8))\n\n\n\n\n\n\n\n\n\n\n\nIt’s amazing how many it’s getting correct because this model was trained in about 24 seconds using a tiny amount of data.\nI’ll take a look at the codes out of curiousity, which is an array of string elements describing different objects in view.\n\nnp.loadtxt(path/'codes.txt', dtype=str)\n\narray(['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'Tunnel',\n       'VegetationMisc', 'Void', 'Wall'], dtype='<U17')\n\n\n\n\n\n\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ADULT_SAMPLE)\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names='salary',\n                                  cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                                               'relationship', 'race'],\n                                  cont_names = ['age', 'fnlwgt', 'education-num'],\n                                  procs = [Categorify, FillMissing, Normalize])\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      workclass\n      education\n      marital-status\n      occupation\n      relationship\n      race\n      education-num_na\n      age\n      fnlwgt\n      education-num\n      salary\n    \n  \n  \n    \n      0\n      State-gov\n      Some-college\n      Divorced\n      Adm-clerical\n      Own-child\n      White\n      False\n      42.0\n      138162.000499\n      10.0\n      <50k\n    \n    \n      1\n      Private\n      HS-grad\n      Married-civ-spouse\n      Other-service\n      Husband\n      Asian-Pac-Islander\n      False\n      40.0\n      73025.003080\n      9.0\n      <50k\n    \n    \n      2\n      Private\n      Assoc-voc\n      Married-civ-spouse\n      Prof-specialty\n      Wife\n      White\n      False\n      36.0\n      163396.000571\n      11.0\n      >=50k\n    \n    \n      3\n      Private\n      HS-grad\n      Never-married\n      Sales\n      Own-child\n      White\n      False\n      18.0\n      110141.999831\n      9.0\n      <50k\n    \n    \n      4\n      Self-emp-not-inc\n      12th\n      Divorced\n      Other-service\n      Unmarried\n      White\n      False\n      28.0\n      33035.002716\n      8.0\n      <50k\n    \n    \n      5\n      ?\n      7th-8th\n      Separated\n      ?\n      Own-child\n      White\n      False\n      50.0\n      346013.994175\n      4.0\n      <50k\n    \n    \n      6\n      Self-emp-inc\n      HS-grad\n      Never-married\n      Farming-fishing\n      Not-in-family\n      White\n      False\n      36.0\n      37018.999571\n      9.0\n      <50k\n    \n    \n      7\n      State-gov\n      Masters\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      White\n      False\n      37.0\n      239409.001471\n      14.0\n      >=50k\n    \n    \n      8\n      Self-emp-not-inc\n      Doctorate\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      White\n      False\n      50.0\n      167728.000009\n      16.0\n      >=50k\n    \n    \n      9\n      Private\n      HS-grad\n      Married-civ-spouse\n      Tech-support\n      Husband\n      White\n      False\n      38.0\n      247111.001513\n      9.0\n      >=50k\n    \n  \n\n\n\nFor tabular models, there’s not generally going to be a pretrained model that already does something like what you want because every table of data is very different, so generally it doesn’t make too much sense to fine_tune a tabular model.\n\nlearn = tabular_learner(dls, metrics=accuracy)\nlearn.fit_one_cycle(2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.373780\n      0.365976\n      0.832770\n      00:06\n    \n    \n      1\n      0.356514\n      0.358780\n      0.833999\n      00:05\n    \n  \n\n\n\n\n\n\nThe basis of most recommendation systems.\n\nfrom fastai.collab import *\npath = untar_data(URLs.ML_SAMPLE)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n    \n  \n  \n    \n      0\n      457\n      457\n      3.0\n    \n    \n      1\n      407\n      2959\n      5.0\n    \n    \n      2\n      294\n      356\n      4.0\n    \n    \n      3\n      78\n      356\n      5.0\n    \n    \n      4\n      596\n      3578\n      4.5\n    \n    \n      5\n      547\n      541\n      3.5\n    \n    \n      6\n      105\n      1193\n      4.0\n    \n    \n      7\n      176\n      4993\n      4.5\n    \n    \n      8\n      430\n      1214\n      4.0\n    \n    \n      9\n      607\n      858\n      4.5\n    \n  \n\n\n\nThere’s actually no pretrained collaborative filtering model so we could use fit_one_cycle but fine_tune works here as well.\n\nlearn = collab_learner(dls, y_range=(0.5, 5.5))\nlearn.fine_tune(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.498450\n      1.417215\n      00:00\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.375927\n      1.357755\n      00:00\n    \n    \n      1\n      1.274781\n      1.176326\n      00:00\n    \n    \n      2\n      1.033917\n      0.870168\n      00:00\n    \n    \n      3\n      0.810119\n      0.719341\n      00:00\n    \n    \n      4\n      0.704180\n      0.679201\n      00:00\n    \n    \n      5\n      0.640635\n      0.667121\n      00:00\n    \n    \n      6\n      0.623741\n      0.661391\n      00:00\n    \n    \n      7\n      0.620811\n      0.657624\n      00:00\n    \n    \n      8\n      0.606947\n      0.656678\n      00:00\n    \n    \n      9\n      0.605081\n      0.656613\n      00:00\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      15.0\n      35.0\n      4.5\n      3.886339\n    \n    \n      1\n      68.0\n      64.0\n      5.0\n      3.822170\n    \n    \n      2\n      62.0\n      33.0\n      4.0\n      3.088149\n    \n    \n      3\n      39.0\n      91.0\n      4.0\n      3.788227\n    \n    \n      4\n      37.0\n      7.0\n      5.0\n      4.434169\n    \n    \n      5\n      38.0\n      98.0\n      3.5\n      4.380877\n    \n    \n      6\n      3.0\n      25.0\n      3.0\n      3.443295\n    \n    \n      7\n      23.0\n      13.0\n      2.0\n      3.220192\n    \n    \n      8\n      15.0\n      7.0\n      4.0\n      4.306846\n    \n  \n\n\n\nNote: RISE turnes your notebook into a presentation.\nGenerally speaking, if it’s something that a human can do reasonably quickly, even an expert human (like look at a Go board and decide if it’s a good board or not) then that’s probably something that deep learning will probably be good at. If it’s something that takes logical thought process over time, particularly if it’s not based on much data, deep learning probably won’t do that well.\nThe first neural network was built in 1957. The basic ideas have not changed much at all.\nWhat’s going on in these models?\n\nArthur Samuel in late 1950s invented Machine Learning.\nNormal program: input -> program -> results.\nMachine Learning model: input and weights (parameters) -> model -> results.\n\nThe model is a mathematical function that takes the input, multiplies them with one set of weights and adds them up, then does that again for a second set of weights, and so forth.\nIt takes all of the negative numbers and replaces them with 0.\nIt takes all those numbers as inputs to the next layer.\nAnd it repeats a few times.\n\nWeights start out as being random.\nA more useful workflow: input/weights -> model -> results -> loss -> update weights.\nThe loss is a number that says how good the results were.\nWe need a way to come up with a new set of weights that are a bit better than the current weights.\n“bit better” weights means it makes the loss a bit better.\nIf we make it a little bit better a few times, it’ll eventually get good.\nNeural nets proven to solve any computable function (i.e. it’s flexible enough to update weights until the results are good).\n“Generate artwork based on someone’s twitter bio” is a computable function.\nOnce we’ve finished the training procedure we don’t the loss and the weights can be integrated into the model.\nWe end up with inputs -> model -> results which looks like our original idea of a program.\nDeploying a model will have lots of tricky details but there will be one line of code which says learn.predict which takes an input and provides results.\nThe most important thing to do is experiment.\n\n\n\n\n\nChapter 1: Your Deep Learning Journey In this section, I’ll take notes while I read Chapter 1 in the textbook.\n\n\n\nWhat you don’t need for deep learning: lots of math, lots of data, lots of expensive computers.\nDeep learning is a computer technique to extract and transform data by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task.\n\n\n\n\n\nWarren McCulloch and Walter Pitts developed a mathematical model of an artificial neuron in 1943.\nMost of Pitt’s famous work was done while he was homeless.\nPsychologist Frank Rosenblatt further developed the artificial neuron to give it the ability to learn and built the first device that used these principles, the Mark I Perceptron, which was able to recognize simple shapes.\nMarvin Minsky and Seymour Papert wrote a book about the Perceptron and showed that using multiple layers of the devices would allow the limitations of a single layer to be addressed.\nThe 1986 book Parallel Distributed Processing (PDP) by David Rumelhart, James McClelland, and the PDP Research Group defined PDP as requiring the following:\n\nA set of processing units.\nA state of activation.\nAn output function for each unit.\nA pattern of connectivity among units.\nA propogation rule for propagating patterns of activities through the network of connectivities.\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit.\nA learning rule whereby patterns of connectivity are modified by experience.\nAn environment within which the system must operate.\n\n\n\n\n\n\nThe hardest part of deep learning is artisanal: how do you know if you’ve got enough data, whether it is in the right format, if your model is training properly, and, if it’s not, what you should do about it?\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path,\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=is_cat,\n    item_tfms=Resize(224)\n)\n\ndls.show_batch()\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:11<00:00]\n    \n    \n\n\n\n\n\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n/usr/local/lib/python3.10/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 162MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.140327\n      0.019135\n      0.007442\n      01:05\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      4.17% [1/24 00:01<00:34]\n    \n    \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.070464\n      0.024966\n      0.006766\n      01:00\n    \n  \n\n\n\nThe error rate is the proportion of images that were incorrectly identified.\nCheck this model actually works with an image of a dog or cat. I’ll download a picture from google and use it for prediction:\n\nimport ipywidgets as widgets\nuploader = widgets.FileUpload()\nuploader\n\n\n\n\n\nim = PILImage.create(uploader.data[0])\nis_cat, _, probs = learn.predict(im)\nim.to_thumb(256)\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'Is this a cat?: {is_cat}.')\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\n\nIs this a cat?: True.\nProbability it's a cat: 1.000000\n\n\n\n\n\n\nA traditional program: inputs -> program -> results.\nIn 1949, IBM researcher Arthur Samuel started working on machine learning. His basic idea was this: instead of telling the computer the exact steps required to solve a problem, show it examples of the problem to solve, and let it figure out how to solve it itself.\nIn 1961 his checkers-playing program had learned so much that it beat the Connecticut state champion.\nWeights are just variables and a weight assignment is a particular choice of values for those variables.\nThe program’s inputs are values that it processes in order to produce its results (for instance, taking image pixels as inputs, and returning the classification “dog” as a result).\nBecause the weights affect the program, they are in a sense another kind of input.\nA program using weight assignment: inputs and weights -> model -> results.\nA model is a special kind of program, on that can do many different things depending on the weights.\nWeights = parameters, with the term “weights” reserved for a particulat type of model parameter.\nLearning would become entirely automatic when the adjustment of the weights was also automatic.\nTraining a maching learning model: inputs and weights -> model -> results -> performance -> update weights.\nresults are different than the performance of a model.\nUsing a trained model as a program -> inputs -> model -> results.\nmaching learning is the training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.\n\n\n\n\n\nNeural networks is a mathematical function that can solve any problem to any level of accuracy.\nStochastic Gradient Descent (SGD) is a completely general way to update the weights of a neural network, to make it improve at any given task.\nImage classification problem:\n\nOur inputs are the images.\nOur weights are the weights in the neural net.\nOur model is a neural net.\nOur results are the values that are calculated by the neural net, like “dog” or “cat”.\n\n\n\n\n\n\nThe functional form of the model is called its architecture.\nThe weights are called parameters.\nThe predictions are calculated from the independent variable, which is the data not including the labels.\nThe results or the model are called predictions.\nThe measure of performance is called the loss.\nThe loss depends not only on the predictions, but also on the correct labels (also known as targets or the dependent variable).\nDetailed training loop: inputs and parameters -> architecture -> predictions (+ labels) -> loss -> update parameters.\n\n\n\n\n\nA model cannot be created without data.\nA model can learn to operate on only the patterns seen in the input data used to train it.\nThis learning approach creates only predictions, not recommended actions.\nIt’s not enough to just have examples of input data, we need labels for that data too.\nPositive feedback loop: the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.\n\n\n\n\n\nitem_tfms are applied to each item while batch_tfms are applied to a batch of items at a time using the GPU.\nA classification model attempts to predict a class, or category.\nA regression model is one that attempts to predict one or more numeric quantities, such as temperature or location.\nThe parameter seed=42 sets the random seed to the same value every time we run this code, which means we get the same validation set every time we run it. This way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.\nWe care about how well our model works on previously unseen images.\nThe longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is overfitting.\nOverfitting is the single most important and challenging issue when training for all machine learning practitioners, and all algorithms.\nYou should only use methods to avoid overfitting after you have confirmed that overfitting is occurring (i.e., if you have observed the validation accuracy getting worse during training)\nfastai defaults to valid_pct=0.2.\nModels using architectures with more layers take longer to train and are more prone to overfitting, on the other hand, when using more data, they can be quite a bit more accurate.\nA metric is a function that measures the quality of the model’s predictions using the validation set.\nerror_rate tells you what percentage of inputs in the validation set are being classified incorrectly.\naccuracy = 1.0 - error_rate.\nThe entire purpose of loss is to define a “measure of performance” that the training system can use to update weights automatically. A good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand.\nA model that has weights that have already been trained on another dataset is called a pretrained model.\nWhen using a pretrained model, cnn_learner will remove the last layer and replace it with one or more new layers with randomized weights. This last part of the model is known as the head.\nUsing a pretrained model for a task different from what is was originally trained for is known as transfer learning.\nThe architecture only describes a template for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains.\nTo fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of epochs).\nfit will fit a model (i.e., look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels).\nFine-Tuning: a transfer learning technique that updates the parameters of a pretrained model by training for additional epochs using a different task from that used for pretraining.\nfine_tune has a few parameters you can set, but in the default form it does two steps:\n\nUse one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\nUse the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which don’t require many changes from the pretrained weights).\n\nThe head of the model is the part that is newly added to be specific to the new dataset.\nAn epoch is one complete pass through the dataset.\n\n\n\n\n\nWhen we fine tune our pretrained models, we adapt what the last layers focus on to specialize on the problem at hand.\n\n\n\n\n\nA lot of things can be represented as images.\nSound can be converted to a spectogram.\nTimes series data can be created into an image using Gramian Angular Difference Field (GADF).\nIf the human eye can recognize categories from the images, then a deep learning model should be able to do so too.\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nLabel\nThe data that we’re trying to predict\n\n\nArchitecture\nThe template of the model that we’re trying to fit; i.e., the actual mathematical function that we’re passing the input data and parameters to\n\n\nModel\nThe combination of the architecture with a particular set of parameters\n\n\nParameters\nThe values in the model that change what task it can do and that are updated through model training\n\n\nFit\nUpdate the parameters of the model such that the predictions of the model using the input data match the target labels\n\n\nTrain\nA synonym for fit\n\n\nPretrained Model\nA model that has already been trained, generally using a large dataset, and will be fine-tuned\n\n\nFine-tune\nUpdate a pretrained model for a different task\n\n\nEpoch\nOne complete pass through the input data\n\n\nLoss\nA measure of how good the model is, chosen to drive training via SGD\n\n\nMetric\nA measurement of how good the model is using the validation set, chosen for human consumption\n\n\nValidation set\nA set of data held out from training, used only for measuring how good the model is\n\n\nTraining set\nThe data used for fitting the model; does not include any data from the validation set\n\n\nOverfitting\nTraining a model in such a way that it remembers specific features of the input data, rather than generalizing wel to data not seen during training\n\n\nCNN\nConvolutional neural network; a type of neural network that works particularly well for computer vision tasks\n\n\n\n\n\n\n\nSegmentation\nNatural language processing (see below)\nTabular (see Adults income classification above)\nCollaborative filtering (see MovieLens ratings predictor above)\nStart by using one of the cut-down dataset versions and later scale up to the full-size version. This is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and use the full dataset only when they have a good understanding of what they have to do.\n\n\n\n\n\nIf the model makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by actually having seen that particular item.\nHyperparameters: various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors.\nWe, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values and we are in danger of overfitting the validation data through human trial and error and exploration.\nThe test set can be used only to evaluate the model at the very end of our efforts.\nTraining data is fully exposed to training and modeling processes, validation data is less exposed and test data is fully hidden.\nThe test and validation sets should have enough data to ensure that you get a good estimate of your accuracy.\nThe discipline of the test set helps us keep ourselves intellectually honest.\nIt’s a good idea for you to try out a simple baseline model yourself, so you know what a really simply model can achieve.\n\n\n\n\n\nA key property of the validation and test sets is that they must be representative of the new data you will see in the future.\nAs an example, for time series data, use earlier dates for training set and later more recent dates as validation set\nThe data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\n\n\nfrom fastai.text.all import *\n\n# I'm using IMDB_SAMPLE instead of the full IMDB dataset since it either takes too long or\n# I get a CUDA Out of Memory error if the batch size is more than 16 for the full dataset\n# Using a batch size of 16 with the sample dataset works fast\ndls = TextDataLoaders.from_csv(\n    path=untar_data(URLs.IMDB_SAMPLE),\n    csv_fname='texts.csv',\n    text_col=1,\n    label_col=0,\n    bs=16)\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is\n      negative\n    \n    \n      1\n      xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot … which is highly believable to xxunk . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies\n      positive\n    \n    \n      2\n      xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n\\n xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after 20 - odd years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \\n\\n xxmaj none of this excuses him this present ,\n      negative\n    \n    \n      3\n      xxbos i really wanted to love this show . i truly , honestly did . \\n\\n xxmaj for the first time , gay viewers get their own version of the \" the xxmaj bachelor \" . xxmaj with the help of his obligatory \" hag \" xxmaj xxunk , xxmaj james , a good looking , well - to - do thirty - something has the chance of love with 15 suitors ( or \" mates \" as they are referred to in the show ) . xxmaj the only problem is half of them are straight and xxmaj james does n't know this . xxmaj if xxmaj james picks a gay one , they get a trip to xxmaj new xxmaj zealand , and xxmaj if he picks a straight one , straight guy gets $ 25 , xxrep 3 0 . xxmaj how can this not be fun\n      negative\n    \n    \n      4\n      xxbos xxmaj many neglect that this is n't just a classic due to the fact that it 's the first 3d game , or even the first xxunk - up . xxmaj it 's also one of the first xxunk games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - xxunk gaming experience in general . xxmaj with graphics that are terribly dated today , the game xxunk you into the role of xxunk even * think * xxmaj i 'm going to attempt spelling his last name ! ) , an xxmaj american xxup xxunk . caught in an underground bunker . xxmaj you fight and search your way through xxunk in order to achieve different xxunk for the six xxunk , let 's face it , most of them are just an excuse to hand you a weapon\n      positive\n    \n    \n      5\n      xxbos xxmaj i 'm sure things did n't exactly go the same way in the real life of xxmaj homer xxmaj hickam as they did in the film adaptation of his book , xxmaj rocket xxmaj boys , but the movie \" october xxmaj sky \" ( an xxunk of the book 's title ) is good enough to stand alone . i have not read xxmaj hickam 's memoirs , but i am still able to enjoy and understand their film adaptation . xxmaj the film , directed by xxmaj joe xxmaj xxunk and written by xxmaj lewis xxmaj xxunk , xxunk the story of teenager xxmaj homer xxmaj hickam ( jake xxmaj xxunk ) , beginning in xxmaj october of 1957 . xxmaj it opens with the sound of a radio broadcast , bringing news of the xxmaj russian satellite xxmaj xxunk , the first artificial satellite in\n      positive\n    \n    \n      6\n      xxbos xxmaj to review this movie , i without any doubt would have to quote that memorable scene in xxmaj tarantino 's \" pulp xxmaj fiction \" ( xxunk ) when xxmaj jules and xxmaj vincent are talking about xxmaj mia xxmaj wallace and what she does for a living . xxmaj jules tells xxmaj vincent that the \" only thing she did worthwhile was pilot \" . xxmaj vincent asks \" what the hell is a pilot ? \" and xxmaj jules goes into a very well description of what a xxup tv pilot is : \" well , the way they make shows is , they make one show . xxmaj that show 's called a ' pilot ' . xxmaj then they show that show to the people who make shows , and on the strength of that one show they decide if they 're going to\n      negative\n    \n    \n      7\n      xxbos xxmaj how viewers react to this new \" adaption \" of xxmaj shirley xxmaj jackson 's book , which was promoted as xxup not being a remake of the original 1963 movie ( true enough ) , will be based , i suspect , on the following : those who were big fans of either the book or original movie are not going to think much of this one … and those who have never been exposed to either , and who are big fans of xxmaj hollywood 's current trend towards \" special effects \" being the first and last word in how \" good \" a film is , are going to love it . \\n\\n xxmaj things i did not like about this adaption : \\n\\n 1 . xxmaj it was xxup not a true adaption of the book . xxmaj from the xxunk i had\n      negative\n    \n    \n      8\n      xxbos xxmaj the trouble with the book , \" memoirs of a xxmaj geisha \" is that it had xxmaj japanese xxunk but underneath the xxunk it was all an xxmaj american man 's way of thinking . xxmaj reading the book is like watching a magnificent ballet with great music , sets , and costumes yet performed by xxunk animals dressed in those xxunk far from xxmaj japanese ways of thinking were the characters . \\n\\n xxmaj the movie is n't about xxmaj japan or real geisha . xxmaj it is a story about a few xxmaj american men 's mistaken ideas about xxmaj japan and geisha xxunk through their own ignorance and misconceptions . xxmaj so what is this movie if it is n't about xxmaj japan or geisha ? xxmaj is it pure fantasy as so many people have said ? xxmaj yes , but then why\n      negative\n    \n  \n\n\n\n\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.629276\n      0.553454\n      0.740000\n      00:19\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.466581\n      0.548400\n      0.740000\n      00:30\n    \n    \n      1\n      0.410401\n      0.418941\n      0.825000\n      00:30\n    \n    \n      2\n      0.286162\n      0.410872\n      0.830000\n      00:31\n    \n    \n      3\n      0.192047\n      0.405275\n      0.845000\n      00:31\n    \n  \n\n\n\n\n# view actual vs prediction\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n      category_\n    \n  \n  \n    \n      0\n      xxbos xxmaj this film sat on my xxmaj xxunk for weeks before i watched it . i xxunk a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj xxunk . \\n\\n xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" la xxmaj xxunk , \" based on a play by xxmaj arthur xxmaj xxunk , who is given an \" inspired by \" credit . xxmaj it starts from one person , a prostitute , standing on a street corner in xxmaj brooklyn . xxmaj she is picked up by a home contractor , who has sex with her on the hood of a car , but ca n't come . xxmaj he refuses to pay her . xxmaj when he 's off xxunk , she\n      positive\n      positive\n    \n    \n      1\n      xxbos xxmaj bonanza had a great cast of wonderful actors . xxmaj xxunk xxmaj xxunk , xxmaj pernell xxmaj whitaker , xxmaj michael xxmaj xxunk , xxmaj dan xxmaj blocker , and even xxmaj guy xxmaj williams ( as the cousin who was brought in for several episodes during 1964 to replace xxmaj adam when he was leaving the series ) . xxmaj the cast had chemistry , and they seemed to genuinely like each other . xxmaj that made many of their weakest stories work a lot better than they should have . xxmaj it also made many of their best stories into great western drama . \\n\\n xxmaj like any show that was shooting over thirty episodes every season , there are bound to be some weak ones . xxmaj however , most of the time each episode had an interesting story , some kind of conflict ,\n      positive\n      negative\n    \n    \n      2\n      xxbos i watched xxmaj grendel the other night and am compelled to put together a xxmaj public xxmaj service xxmaj announcement . \\n\\n xxmaj grendel is another version of xxmaj beowulf , the thousand - year - old xxunk - saxon epic poem . xxmaj the scifi channel has a growing catalog of xxunk and uninteresting movies , and the previews promised an xxunk low - budget mini - epic , but this one xxunk to let me switch xxunk . xxmaj it was xxunk , xxunk , bad . i watched in xxunk and horror at the train wreck you could n't tear your eyes away from . i reached for a xxunk and managed to capture part of what i was seeing . xxmaj the following may contain spoilers or might just save your xxunk . xxmaj you 've been warned . \\n\\n - xxmaj just to get\n      negative\n      negative\n    \n    \n      3\n      xxbos xxmaj this is the last of four xxunk from xxmaj france xxmaj i 've xxunk for viewing during this xxmaj christmas season : the others ( in order of viewing ) were the uninspired xxup the xxup black xxup tulip ( 1964 ; from the same director as this one but not nearly as good ) , the surprisingly effective xxup lady xxmaj oscar ( 1979 ; which had xxunk as a xxmaj japanese manga ! ) and the splendid xxup cartouche ( xxunk ) . xxmaj actually , i had watched this one not too long ago on late - night xxmaj italian xxup tv and recall not being especially xxunk over by it , so that i was genuinely surprised by how much i enjoyed it this time around ( also bearing in mind the xxunk lack of enthusiasm shown towards the film here and elsewhere when\n      positive\n      positive\n    \n    \n      4\n      xxbos xxmaj this is not really a zombie film , if we 're xxunk zombies as the dead walking around . xxmaj here the protagonist , xxmaj xxunk xxmaj louque ( played by an unbelievably young xxmaj dean xxmaj xxunk ) , xxunk control of a method to create zombies , though in fact , his ' method ' is to mentally project his thoughts and control other living people 's minds turning them into hypnotized slaves . xxmaj this is an interesting concept for a movie , and was done much more effectively by xxmaj xxunk xxmaj lang in his series of ' dr . xxmaj mabuse ' films , including ' dr . xxmaj mabuse the xxmaj xxunk ' ( 1922 ) and ' the xxmaj testament of xxmaj dr . xxmaj mabuse ' ( 1933 ) . xxmaj here it is unfortunately xxunk to his quest to\n      negative\n      positive\n    \n    \n      5\n      xxbos \" once upon a time there was a charming land called xxmaj france … . xxmaj people lived happily then . xxmaj the women were easy and the men xxunk in their favorite xxunk : war , the only xxunk of xxunk which the people could enjoy . \" xxmaj the war in question was the xxmaj seven xxmaj year 's xxmaj war , and when it was noticed that there were more xxunk of soldiers than soldiers , xxunk were sent out to xxunk the ranks . \\n\\n xxmaj and so it was that xxmaj fanfan ( gerard xxmaj philipe ) , caught xxunk a farmer 's daughter in a pile of hay , escapes marriage by xxunk in the xxmaj xxunk xxunk … but only by first believing his future as xxunk by a gypsy , that he will win fame and fortune in xxmaj his xxmaj\n      positive\n      positive\n    \n    \n      6\n      xxbos xxup ok , let me again admit that i have n't seen any other xxmaj xxunk xxmaj ivory ( the xxunk ) films . xxmaj nor have i seen more celebrated works by the director , so my capacity to xxunk xxmaj before the xxmaj rains outside of analysis of the film itself is xxunk . xxmaj with that xxunk , let me begin . \\n\\n xxmaj before the xxmaj rains is a different kind of movie that does n't know which genre it wants to be . xxmaj at first , it pretends to be a romance . xxmaj in most romances , the protagonist falls in love with a supporting character , is separated from the supporting character , and is ( sometimes ) united with his or her partner . xxmaj this movie 's hero has already won the heart of his lover but can not\n      negative\n      negative\n    \n    \n      7\n      xxbos xxmaj first off , anyone looking for meaningful \" outcome xxunk \" cinema that packs some sort of social message with meaningful performances and soul searching dialog spoken by dedicated , xxunk , heartfelt xxunk , please leave now . xxmaj you are wasting your time and life is short , go see the new xxmaj xxunk xxmaj jolie movie , have a good cry , go out & buy a xxunk car or throw away your conflict xxunk if that will make you feel better , and leave us alone . \\n\\n xxmaj do n't let the door hit you on the way out either . xxup the xxup incredible xxup melting xxup man is a grade b minus xxunk horror epic shot in the xxunk of xxmaj oklahoma by a young , xxup tv friendly cast & crew , and concerns itself with an astronaut who is\n      positive\n      negative\n    \n    \n      8\n      xxbos \" national xxmaj treasure \" ( 2004 ) is a thoroughly misguided xxunk - xxunk of plot xxunk that borrow from nearly every xxunk and dagger government conspiracy cliché that has ever been written . xxmaj the film stars xxmaj nicholas xxmaj cage as xxmaj benjamin xxmaj xxunk xxmaj xxunk ( how precious is that , i ask you ? ) ; a seemingly normal fellow who , for no other reason than being of a xxunk of like - minded misguided fortune hunters , decides to steal a ' national treasure ' that has been hidden by the xxmaj united xxmaj states xxunk fathers . xxmaj after a bit of subtext and background that plays laughably ( unintentionally ) like xxmaj indiana xxmaj jones meets xxmaj the xxmaj patriot , the film xxunk into one misguided xxunk after another  attempting to create a ' stanley xxmaj xxunk\n      negative\n      negative\n    \n  \n\n\n\n\nreview_text = \"I really liked the movie!\"\nlearn.predict(review_text)\n\n\n\n\n\n\n\n\n('positive', tensor(1), tensor([0.0174, 0.9826]))\n\n\n\n\n\n\n\nDo you need these for deep learning?\n\nLots of Math (FALSE).\nLots of Data (FALSE).\nLots of expensive computers (FALSE).\nA PhD (FALSE).\n\nName five areas where deep learning is now the best tool in the world\n\nNatural Language Processing (NLP).\nComputer vision.\nMedicine.\nImage generation.\nRecommendation systems.\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nMark I Perceptron.\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nA series of processing units.\nA state of activation.\nAn output function for each unit.\nA pattern of connectivity among units.\nA propagation rule for propagating patterns of activities through the network of connectivities.\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit.\nA learning rule whereby patterns of connectivity are modified by experience.\nAn environment within which the system must operate.\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\nUsing multiple layers of the device would allow limitations of one layer to be addressed—this was ignored.\nMore than two layers are needed to get practical, good perforamnce—only in the last decade has this been more widely appreciated and applied.\n\nWhat is a GPU?\n\nA Graphical Processing Unit, which can perform thousands of tasks at the same time.\n\nOpen a notebook and execute a cell containing: 1+1. What happens?\n\nDepending on the server, it may take some time for the output to generate, but running this cell will output 2.\n\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.\n\n(I did this for the notebook shared for Lesson 1).\n\nComplete the Jupyter Notebook online appendix.\n\nDone. Will reference some of it again.\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\n\nBecause it’s hard to instruct a computer clear instructions to recognize images.\n\nWhat did Samuel mean by “weight assignment”?\n\nA particular choice for weights (variables)\n\nWhat term do we normally use in deep learning for what Samuel called “weights”?\n\nParameters\n\nDraw a picture that summarizes Samuel’s view of a machine learning model\n\ninput and weights -> model -> results -> performance -> update weights/inputs\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\nBecause a deep learning model has many layers and connectivities and activations between neurons that are not intuitive to our understanding.\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\n\nUniversal approximation theorem.\n\nWhat do you need in order to train a model?\n\nLabeled data (Inputs and targets).\nArchitecture.\nInitial weights.\nA measure of performance (loss, accuracy).\nA way to update the model (SGD).\n\nHow could a feedback loop impact the rollout of a predictive policing model?\n\nThe model will end up predicting where arrests are made, not where crime is taking place, so more police officers will go to locations where more arrests are predicted and feed that data back to the model which will reinforce the prediction of arrests in those areas, continuing this feedback loop of predictions -> arrests -> predictions.\n\nDo we always have to use 224x224-pixel images with the cat recognition model?\n\nNo, that’s just the convention for image recognition models.\nYou can use larger images but it will slow down the training process (it takes longer to open up bigger images).\n\nWhat is the difference between classification and regression?\n\nClassification predicts discrete classes or categories.\nRegression predicts continuous values.\n\nWhat is a validation set? What is a test set? Why do we need them?\n\nA validation set is a dataset upon which a model’s accuracy (or metrics in general) is calculated during training, as well as the dataset upon which the performance of different hyperparameters (like batch size and learning rate) are measured.\nA test set is a dataset upon which a model’s final performance is measured, a truly unseen dataset for both the model and the practitioner\n\nWhat will fastai do if you don’t provide a validation set?\n\nSet aside a random 20% of the data as the validation set by default\n\nCan we always use a random sample for a validation set? Why or why not?\n\nNo, in situations where we want to ensure that the model’s accuracy is evaluated on data the model has not seen, we should not use a random validation set. Instead, we should create an intentional validation set. For example:\n\nFor time series data, use the most recent dates as the validation set\nFor human recognition data, use images of different people for training and validation sets\n\n\nWhat is overfitting? Provide an example.\n\nOverfitting is when a model memorizes features of the training dataset instead of learning generalizations of the features in the data. An example of this is when a model memorizes training data facial features but then cannot recognize different faces in the real world. Another example is when a model memorizes the handwritten digits in the training data, so it cannot then recognize digits written in different handwriting. Overfitting can be observed during training when the validation loss starts to increase as the training loss decreases.\n\nWhat is a metric? How does it differ from loss?\n\nA metric a measurement of how good a model is performing, chosen for human consumption. A loss is also a measurement of how good a model is performing, but it’s chosen to drive training using an optimizer.\n\nHow can pretrained models help?\n\nPretrained models are already good at recognizing many generalized features and so they can help by providing a set of weights in an architecture that are capable, reducing the amount of time you need to train a model specific to your task.\n\nWhat is the “head” of the model?\n\nThe last/top few neural network layers which are replaced with randomized weights in order to specialize your model via training on the task at hand (and not the task it was pretrained to perform).\n\nWhat kinds of features do the early layers of a CNN find? How about the later layers?\n\nEarly layers: simple features lie lines, color gradients\nLater layers: compelx features like dog faces, outlines of people\n\nAre image models useful only for photos?\n\nNo! Lots of things can be represented by images so if you can represent something (like a sound) as an image (spectogram) and differences between classes/categories are easily recognizable by the human eye, you can train an image classifier to recognize it.\n\nWhat is an architecture?\n\nA template, mathematical function, to which you pass input data to in order to fit/train a model\n\nWhat is segmentation?\n\nRecognizing different objects in an image based on pixel colors (each object is a different pixel color)\n\nWhat is y_range used for? When do we need it?\n\nIt’s used to specify the output range of a regression model. We need it when the target is a continuous value.\n\nWhat are hyperparameters?\n\nModeling choices such as network architecture, learning rates, data augmentation strategies and other higher level choices that govern the meaning of the weight parameters.\n\nWhat is the best way to avoid failures when using AI in an organization?\n\nMaking sure you have good validation and test sets to evaluate the performance of a model on real world data.\nTrying out a simple baseline model to know what level of performance such a model can achieve.\n\n\n\n\n\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?\n\nCPU vs GPU for Machine Learning\n\nCPUs process tasks in a sequential manner, GPUs process tasks in parallel.\nGPUs can have thousands of cores, processing tasks at the same time.\nGPUs have many cores processing at low speeds, CPUs have few cores processing at high speeds.\nSome algorithms are optimized for CPUs rather than GPUs (time series data, recommendation systems that need lots of memory).\nNeural networks are designed to process tasks in parallel.\n\nCPU vs GPU in Machine Learning Algorithms: Which is Better?\n\nMachine Learning Operations Preferred on CPUs\n\nRecommendation systems that involve huge memory for embedding layers.\nSupport vector machines, time-series data, algorithms that don’t require parallel computing.\nRecurrent neural networks because they use sequential data.\nAlgorithms with intensive branching.\n\nMachine Learning Operations Preferred on GPUs\n\nOperations that involve parallelism.\n\n\nWhy Deep Learning Uses GPUs\n\nNeural networks are specifically made for running in parallel.\n\n\nTry to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice.\n\nHidden Risks of Machine Learning Applied to Healthcare: Unintended Feedback Loops Between Models and Future Data Causing Model Degradation\n\nIf clinicians fully trust the machine learning model (100% adoption of the predicted label) the false positive rate (FPR) grows uncontrollably with the number of updates.\n\nRunaway Feedback Loops in Predictive Policing\n\nOnce police are deployed based on these predictions, data from observations in the neighborhood is then used to further update the model.\nDiscovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated.\nPredictive policing systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.\n\nPitfalls of Predictive Policing: An Ethical Analysis\n\nPredictive policing relies on a large database of previous crime data and forecasts where crime is likely to occur. Since the program relies on old data, those previous arrests need to be unbiased to generate unbiased forecasts.\nPeople of color are arrested far more often than white people for committing the same crime.\nRacially biased arrest data creates biased forecasts in neighborhoods where more people of color are arrested.\nIf the predictive policing algorithm is using biased data to divert more police forces towards less affluent neighborhoods and neighborhoods of color, then those neighborhoods are not receiving the same treatment as others.\n\nBias in Criminal Risk Scores Is Mathematically Inevitable, Researchers Say\n\nThe algorithm COMPAS which predicts whether a person is “high-risk” and deemed more likely to be arrested in the future, leads to being imprisoned (instead of sent to rehab) or longer sentences.\n\nCan bots discriminate? It’s a big question as companies use AI for hiring\n\nIf an older candidate makes it past the resume screening process but gets confused by or interacts poorly with the chatbot, that data could teach the algorithm that candidates with similar profiles should be ranked lower\n\nEcho chambers, rabbit holes, and ideological bias: How YouTube recommends content to real users\n\nWe find that YouTube’s algorithm pushes real users into (very) mild ideological echo chambers.\nWe found that 14 out of 527 (~3%) of our users ended up in rabbit holes.\nFinally, we found that, regardless of the ideology of the study participant, the algorithm pushes all users in a moderately conservative direction.\n\n\n\n\n\n\n\nI’m going to do things a bit differently than how I approached Lesson 1. Jeremy suggested that we first watch the video without pausing in order to understand what we’re going to do and then watch it a second time and follow along. I also want to be mindful of how long I’m running my Paperspace Gradient maching (at $0.51/hour) so that I don’t run the machine when I don’t need its GPU.\nSo, here’s how I’m going to approach Lesson 2: - Read the Chapter 2 Questionnaire so I know what I’ll be “tested” on at the end - Watch the video without taking notes or running code - Rewatch the video and take notes in this notebook - Add the Kaggle code cells to this notebook and run them in Paperspace - Read the Gradio tutorial without running code - Re-read the Gradio tutorial and follow along with my own code - Read Chapter 2 in the textbook and run code in this notebook in Paperspace - Read Chapter 2 in the textbook and take notes in this notebook (including answers to the Questionnaire)\nWith this approach, I’ll have a big picture understanding of each step of the lesson and I’ll minimize the time I’m spending running my Paperspace Gradient machine.\n\n\nLink to this lesson’s video.\n\nIn this lesson we’re doing things that hasn’t been in courses like this before.\nResource: aiquizzes.com—I signed up and answered a couple of questions.\nDon’t forget the FastAI Forums\n\nClick “Summarize this Topic” to get a list of the most upvoted posts\n\nHow do we go about putting a model in production?\n\nFigure out what problem you want to solve\nFigure out how to get data for it\nGather some data\n\nUse DuckDuckGo image function\nDownload data\nGet rid of images that failed to open\n\nData cleaning\n\nBefore you clean your data, train the model\nImageClassifierCleaner can be used to clean (delete or re-label) the wrongly labeled data in the dataset\n\ncleaner orders by loss so you only need to look at the first few\n\nAlways build a model to find out what things are difficult to recognize in your data and to find the things the model can help you find that are problems in the data\n\nTrain your model again\nDeploy to HuggingFace Spaces\n\nInstall Jupyter Notebook Extensions to get features like table of contents and collapsible sections (with which you can also navigate sections using arrow keys)\nType ?? followed by function name to get source code\nType ? followed by function name to get brief info\nIf you have nbdev installed doc(<fn>) will give you link to documentation\nDifferent ways to resize an image\n\nResizeMethod.Squish (to see the whole picture with different aspect ratio)\nResizeMethod.Pad (whole image in correct aspect ratio)\n\nData Augmentation\n\nRandomResizedCrop (different bit of an image everytime)\nbatch_tfms=aug_tranforms() (images get turned, squished, warped, saturated, recolored, etc.)\n\nUse if you are training for more than 5-10 epochs\nIn memory, real-time, the image is being resized/cropped/etc.\n\n\nConfusion matrix (ClassificationInterpretation)\n\nOnly meaningful for category labels\nShows what category errors your model is making (actual vs predicted)\nIn a lot of situations this will let you know what the hard categories to classify are (e.g. breeds of pets hard to identify)\n.plot_top_losses tells us where the loss is the highest (prediction/actual/loss/probability)\n\nA loss will be bad (high) if we are wrong + confident or right + unconfident\n\n\nOn your computer, normal RAM doesn’t get filled up as it saves RAM to hard disk (swapping). GPUs don’t do swapping so do only one thing at a time so you’re not using up all the memory.\nGradio + HuggingFace Spaces\n\nHere is my Hello World HuggingFace Space!\nNext, we’ll put a deep learning model in production. In the code cells below, I will train and export a dog vs cat classifier.\n\n\n\n# import all the stuff we need from fastai\nfrom fastai.vision.all import *\nfrom fastbook import *\n\n\n# download and decompress our dataset\npath = untar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:57<00:00]\n    \n    \n\n\n\n# define a function to label our images\ndef is_cat(x): return x[0].isupper()\n\n\n# create `DataLoaders`\ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path),\n    valid_pct = 0.2,\n    seed = 42,\n    label_func = is_cat,\n    item_tfms = Resize(192))\n\n\n# view batch\ndls.show_batch()\n\n\n\n\n\n# train our model using resnet18 to keep it small and fast\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.199976\n      0.072374\n      0.020298\n      00:19\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.061802\n      0.081512\n      0.020974\n      00:20\n    \n    \n      1\n      0.047748\n      0.030506\n      0.010149\n      00:18\n    \n    \n      2\n      0.021600\n      0.026245\n      0.006766\n      00:18\n    \n  \n\n\n\n\n# export our trained learner\nlearn.export('model.pkl')\n\n\nFollowing the script in the video, as well as the git-lfs and requirements.txt in Tanishq Abraham’s tutorial, I deployed a Dog and Cat Classifier on HuggingFace Spaces.\nIf you run the training for long enough (high number of epochs) the error rate will get worse. We’ll learn why in a future lesson.\nUse fastsetup to setup your local machine with Python and Jupyter.\n\nThey recommend using mamba instead of conda as it is faster.\n\n\n\n\n\nIn the cells below, I’ll run the code provided in the Chapter 2 notebook.\n\n# prepare path and subfolder names\nbear_types = 'grizzly', 'black', 'teddy'\npath = Path('bears')\n\n\n# download images of grizzly, black and teddy bears\nif not path.exists():\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok = True)\n        results = search_images_ddg(f'{o} bear')\n        download_images(dest, urls = results)\n\n\n# view file paths\nfns = get_image_files(path)\nfns\n\n(#570) [Path('bears/grizzly/ca9c20c9-e7f4-4383-b063-d00f5b3995b2.jpg'),Path('bears/grizzly/226bc60a-8e2e-4a18-8680-6b79989a8100.jpg'),Path('bears/grizzly/2e68f914-0924-42ed-9e2e-19963fa03a37.jpg'),Path('bears/grizzly/38e2d057-3eb2-4e8e-8e8c-fa409052aaad.jpg'),Path('bears/grizzly/6abc4bc4-2e88-4e28-8ce4-d2cbdb05d7b5.jpg'),Path('bears/grizzly/3c44bb93-2ac5-40a3-a023-ce85d2286846.jpg'),Path('bears/grizzly/2c7b3f99-4c8e-4feb-9342-dacdccf60509.jpg'),Path('bears/grizzly/a59f16a6-fa06-42d5-9d79-b84e130aa4e3.jpg'),Path('bears/grizzly/d1be6dc8-da42-4bee-ac31-0976b175f1e3.jpg'),Path('bears/grizzly/7bc0d3bd-a8dd-477a-aa16-449124a1afb5.jpg')...]\n\n\n\n# get list of corrupted images\nfailed = verify_images(fns)\nfailed\n\n(#24) [Path('bears/grizzly/2e68f914-0924-42ed-9e2e-19963fa03a37.jpg'),Path('bears/grizzly/f77cfeb5-bfd2-4c39-ba36-621f117a65f6.jpg'),Path('bears/grizzly/37aa7eed-5a83-489d-b8f5-54020ba41390.jpg'),Path('bears/black/90a464ad-b0a7-4cf5-86ff-72d507857007.jpg'),Path('bears/black/f03a0ceb-4983-4b8f-a001-84a0875704e8.jpg'),Path('bears/black/6193c1cf-fda4-43f9-844e-7ba7efd33044.jpg'),Path('bears/teddy/474bdbb3-de2f-49e5-8c5b-62b4f3f50548.JPG'),Path('bears/teddy/58755f3f-227f-4fad-badc-a7d644e54296.JPG'),Path('bears/teddy/eb55dc00-3d01-4385-a7da-d81ac5211696.jpg'),Path('bears/teddy/97eadc96-dc4e-4b3f-8486-88352a3b2270.jpg')...]\n\n\n\n# remove corrupted image files\nfailed.map(Path.unlink)\n\n(#24) [None,None,None,None,None,None,None,None,None,None...]\n\n\n\n# create DataBlockfor training\nbears = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct = 0.2, seed = 42),\n    get_y = parent_label,\n    item_tfms = Resize(128)\n)\n\n\n# create DataLoaders object\ndls = bears.dataloaders(path)\n\n\n# view training batch -- looks good!\ndls.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# view validation batch -- looks good!\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# observe how images react to the \"squish\" ResizeMethod\nbears = bears.new(item_tfms = Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\nNotice how the grizzlies in the third image look abnormally skinny, since the image is squished.\n\n# observe how images react to the \"pad\" ResizeMethod\nbears = bears.new(item_tfms = Resize(128, ResizeMethod.Pad, pad_mode = 'zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\nIn these images, the original aspect ratio is maintained.\n\n# observe how images react to the transform RandomResizedCrop\nbears = bears.new(item_tfms = RandomResizedCrop(128, min_scale = 0.3))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n = 4, nrows = 1)\n\n\n\n\n\n# observe how images react to data augmentation transforms\nbears = bears.new(item_tfms=Resize(128), batch_tfms = aug_transforms(mult = 2))\ndls = bears.dataloaders(path)\n# note that data augmentation occurs on training set\ndls.train.show_batch(max_n = 8, nrows = 2, unique = True)\n\n\n\n\n\n# train the model in order to clean the data\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# train the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 100MB/s] \n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.221027\n      0.206999\n      0.055046\n      00:34\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.225023\n      0.177274\n      0.036697\n      00:32\n    \n    \n      1\n      0.162711\n      0.189059\n      0.036697\n      00:31\n    \n    \n      2\n      0.144491\n      0.191644\n      0.027523\n      00:31\n    \n    \n      3\n      0.122036\n      0.188296\n      0.018349\n      00:31\n    \n  \n\n\n\n\n# view Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model confused a grizzly for a black bear and a black bear for a grizzly bear. It didn’t confuse any of the teddy bears, which makes sense given how different they look to real bears.\n\n# view images with the highest losses\ninterp.plot_top_losses(5, nrows = 1)\n\n\n\n\n\n\n\n\n\n\n\nThe fourth image has two humans in it, which is likely why the model didn’t recognize the bear. The model correctly predicted the the third and fifth images but with low confidence (57% and 69%).\n\n# clean the training and validation sets\nfrom fastai.vision.widgets import *\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI cleaned up the images (deleting an image of a cat, another of a cartoon bear, a dog, and a blank image).\n\n# delete or move images based on the dropdown selections made in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n# create new dataloaders object\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\n# retrain the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.289331\n      0.243501\n      0.074074\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.225567\n      0.256021\n      0.064815\n      00:32\n    \n    \n      1\n      0.218850\n      0.288018\n      0.055556\n      00:34\n    \n    \n      2\n      0.184954\n      0.315183\n      0.055556\n      00:31\n    \n    \n      3\n      0.141363\n      0.308634\n      0.055556\n      00:31\n    \n  \n\n\n\nWeird!! After cleaning the data, the model got worse (1.8% error rate is now 5.6%). I’ll run the cleaning routine again and retrain the model to see if it makes a difference. Perhaps there are still erroneous images in the mix.\n\n# view Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis time, the model incorrectly predicted 3 grizzlies as black bears, 2 black bears as grizzlies and 1 black bear as a teddy.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# delete or move images based on the dropdown selections made in the cleaner\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n# create new dataloaders object\nbears = bears.new(\n    item_tfms = RandomResizedCrop(224, min_scale = 0.5),\n    batch_tfms = aug_transforms())\n\ndls = bears.dataloaders(path)\n# The lower right image (cartoon bear) is one that I selected \"Delete\" for\n# in the cleaner so I'm not sure why it's still there\n# I'm wondering if there's something wrong with the cleaner or how I'm using it?\ndls.show_batch()\n\n\n\n\n\n# retrain the model\nlearn = vision_learner(dls, resnet18, metrics = error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.270627\n      0.130137\n      0.046729\n      00:31\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.183445\n      0.078030\n      0.028037\n      00:32\n    \n    \n      1\n      0.201080\n      0.053461\n      0.018692\n      00:33\n    \n    \n      2\n      0.183515\n      0.019479\n      0.009346\n      00:37\n    \n    \n      3\n      0.144900\n      0.012682\n      0.000000\n      00:31\n    \n  \n\n\n\nI’m still not confident that this is a 100% accurate model given the bad images in the training set (such as the cartoon bear) but I’m going to go with it for now.\n\n\n\n\n\n\nUnderestimating the constraints and overestimating the capabilities of deep learning may lead to frustratingly poor results, at least until you gain some experience and can solve the problems that arise.\nOverstimating the constraints and underestimating the capabilities of deep learning may mean you do not attempt a solvable problem because you talk yourself out of it.\nThe most important thing (as you learn deep learning) is to ensure that you have a project to work on.\nThe goal is not to find the “perfect” dataset or project, but just to get started and iterate from there.\nComplete every step as well as you can in a reasonable amount of time, all the way to the end.\nComputer vision\n\nObject recognition: recognize items in an image\nObject detection: recognition + highlight the location and name of each found object.\nDeep learning algorithms are generally not good at recognizing images that are significantly different in structure or style from those used to train the model.\n\nNLP\n\nDeep learning is not good at generating correct responses.\nText generation models will always be technologically a bit ahead of models for recognizing automatically generated text.\nGoogle’s online translation system is based on deep learning.\n\nCombining text and images\n\nA deep learning model can be trained on input images with output captions written in English, and can learn to generate surprisingly appropriate captions automatically for new images (with no guarantee the captions will be correct).\nDeep learning should be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely.\n\nTabular data\n\nIf you already have a system that is using random forests or gradient boosting machines then switching to or adding deep learning may not result in any dramatic improvement.\nDeep learning greatly increases the variety of columns that you can include.\nDeep learning models generally take longer to train than random forests or gradient boosting machines.\n\nRecommendation systems\n\nA special type of tabular data (a high-cardinality categorical variable representing users and another one representing products or something similar).\nDeep learning models are good at handling high cardinality categorical variables and thus recommendation systems.\nDeep learning models do well when combining these variables with other kinds of data such as natural language, images, or additional metadata represented as tables such as user information, previous transactions, and so forth.\nNearly all machine learning approaches have th downside that they tell you only which products a particular user might like, rather than what recommendations would be helpful for a user.\n\nOther data types\n\nUsing NLP deep learning methods is the current SOTA approach for many types of protein analysis since protein chains look a lot like natural language documents.\n\nThe Drivetrain Approach\n\nDefined objective\nLevers (what inputs can we control)\nData (what inputs we can collect)\nModels (how the levers influence the objective)\n\nGathering data\n\nFor most projects you can find the data online.\nUse duckduckgo_search\n\nFrom Data to DataLoaders\n\nDataLoaders is a thin class that just stores whatever DataLoader objects you pass to it and makes them available as train and valid.\nTo turn data into a DataLoaders object we need to tell fastai four things:\n\nWhat kinds of data we are working with.\nHow to get the list of items.\nHow to label these items.\nHow to create the validation set.\n\nWith the DataBlock API you can customize every stage of the creation of your DataLoaders:\n\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\nexplanation of DataBlock\n\nblocks specifies types for independent (the thing we are using to make predictions from) and dependent (our target) variables.\nComputers don’t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time–called the seed–then you will get the exact same list each time.\nImages need to be all the same size.\nA DataLoader is a class that provides batches of a few items at a time to the GPU.\nfastai default batch size is 64 items.\nResize crops the images to fit a square shape, alternatively you can pad (ResizeMethod.Pad) or squish (ResizeMethod.Squish) the images to fit the square.\nSquishing (model learns that things look differently from how they actually are), cropping (removal of features that would allow us to perform recognition) and padding (lot of empty space which is just wasted computation) are wasteful or problematic approaches. Instead, randomly select part of the image and then crop to just that part. On each epoch, we randomly select a different part of each image (RandomResizedCrop(min_scale)).\nTraining the neural network with examples of images in which objects are in slightly different places and are slightly different sizes helps it to understand the basic concept of what an object is and how it can be represented in an image.\n\nData Augmentation\n\nrefers to creating random variations of our input data, such that they appear different but do not change the meaning of the data (rotation, flipping, perspective warping, brightness changes, and contrast changes).\naug_transforms() provides a standard set of augmentations.\nUse batch_tfms to process a batch at a time on the GPU to save time.\n\nTraining your model and using it to clean your data\n\nView confusion matrix with ClassificationInterpretation.from_learner(learn). The diagonal shows images that are classified correctly. Calculated using validation set.\nSort images by loss using interp.plot_top_losses().\nLoss is high if the model is incorrect (especially if it’s also confident) or if it’s correct but not confident.\nA model can help you find data issues more quickly.\n\nUsing the model for inference\n\nlearn.export() will export a .pkl file.\nGet predictions with learn_inf.predict(<input>). This returns three things: the predicted category in the same format you originally provided, the index of the predicted category and the probabilities for each category.\nYou can access the DataLoaders as an attribute of the Learner: learn_inf.dls.\n\nDeploying your app\n\nYou almost certainly do not need a GPU to serve your model in production.\nTo classify a few users’ images at a time, you need high-volume. If you do have this scenario, use Microsoft’s ONNX Runtime or AWS SageMaker.\nRecommended wherever possible to deploy the model itself to a server and have your mobile/edge application connect to it as a web service.\nIf your application uses sensitive data, your users may be concerned about an approach that sends that data to a remote server.\n\nHow to Avoid Disaster\n\nUnderstanding and testing the behavior of a deep learning model is much more difficult than with most other code you write.\nThe kinds of photos that people are most likely to upload to the internet are the kinds of photos that do a good job of clearly and artistically displaying their subject matter, which isn’t the kind of input this system is going to be getting in real life. We may need to do a lot of our own data collection and labeling to create a useful system.\nout-of-domain data: data that our model sees in production that is very different from what it saw during training.\ndomain shift: data that our model sees changes over time.\nDeployment process\n\nManual Process: run model in parallel, humans check all predictions.\nLimited scope deployment: careful human supervision, time or geography limited.\nGradual expansion: good reporting systems needed, consider what could go wrong.\n\nUnforeseen consequences and feedback loops\n\nYour model may change the behavior of the system it’s a part of.\nfeedback loops can result in negative implications of bias getting worse.\nA helpful exercise prior to rolling out a significant machine learning system is to consider the question “What would happen if it went really, really well?”\n\n\nQuestionnaire\n\nWhere do text models currently have a major deficiency?\n\nProviding correct or accurate information.\n\nWhat are possible negative societal implications of text generation models?\n\nThe viral spread of misinformation, which can lead to real actions and harms.\n\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a god alternative to automating a process?\n\nRun the model in parallel with a human checking its predictions.\n\nWhat kind of tabular data is deep learning particularly good at?\n\nHigh-cardinality categorical data.\n\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\n\nIt will only tell you which products a particular user might like, rather than what recommendations may be helpful for a user.\n\nWhat are the steps of the Drivetrain Approach?\n\nDefine an objective\nDetermine what inputs (levers) you can control\nCollect data\nCreate models (how the levers influence the objective)\n\nHow do the steps of the Drivetrain Approach map to a recommendation system?\n\nObjective: drive additional sales due to recommendations.\nLevel: ranking of the recommendations.\nData: must be collectd to generate recommendations that will cause new sales.\nModels: two for purchasing probabilities conditional on seeing or not seeing a recommendation, the difference between these two probabilities is a utility function for a given recommendation to a customer (low in cases when algorithm recommends a familiar book that the customer has already rejected, or a book they would have bought even without the recommendation).\n\nCreate an image recognition model using data you curate, and deploy it on the web.\n\nHere.\n\nWhat is DataLoaders?\n\nA class that creates validation and training sets/batches that are fed to the GPUS\n\nWhat four things do we need to tell fastai to create DataLoaders?\n\nWhat kinds of data we are working with (independent and dependent variables).\nHow to get the list of items.\nHow to label these items.\nHow to create the validation set.\n\nWhat does the splitter parameter to DataBlock do?\n\nSet aside a percentage of the data as the validation set.\n\nHow do we ensure a random split always gives the same validation set?\n\nSet the seed parameter to the same value.\n\nWhat letters are often used to signify the independent and dependent variables?\n\nIndependent: x\nDependent: y\n\nWhat’s the difference between crop, pad and squish resize approaches? When might you choose one over the others?\n\nCrop: takes a section of the image and resizes it to the desired size. Use when it’s not necessary to have the model traing on the whole image.\nPad: keep the image aspect ratio as is, add white/black padding to make a square. Use when it’s necessary to have the model train on the whole image.\nSquish: distorts the image to fit a square. Use when it’s not necessary to have the model train on the original aspect ratio.\n\nWhat is data augmentation? Why is it needed?\n\nData augmentation is the creation of random variations of input data through techniques like rotation, flipping, brightness changes, contrast changes, perspective warping. It is needed to help the model learn to recognize objects under different lighting/perspective conditions.\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\nWhat is the difference between item_tfms and batch_tfms?\n\nitem_tfms are transforms that are applied to each item in the set.\nbatch_tfms are transforms applied to a batch of items in the set.\n\nWhat is a confusion matrix?\n\nA matrix that shows the counts of predicted (columns) vs. actual (rows) labels, with the diagonal being correctly predicted data.\n\nWhat does export save?\n\nBoth the architecture and the parameters as a .pkl file.\n\nWhat is called when we use a model for making predictions, instead of training?\n\nInference\n\nWhat are IPython widgets?\n\ninteractive browser controls for Jupyter Notebooks.\n\nWhen would you use a CPU for deployment? When might a GPU be better?\n\nCPU: low-volume, single-user inputs for prediction.\nGPU: high-volume, multiple-user inputs for predictions.\n\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n\nRequires internet connectivity (and latency).\nSensitive data transfer may not be okay with your users.\nManaging complexity and scaling the server creates additional overhead.\n\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice?\n\nout-of-domain data: the images captured of real bears may not be represented in the model’s training or validation datasets.\nNumber of bear alerts doubles or halves after rollout of the new system in some location.\nout-of-domain data: the cameras may capture low-resolution images of the bears when the training and validation set had high resolution images.\n\nWhat is out-of-domain data?\n\nData your model sees in production that it hasn’t seen during training.\n\nWhat is domain shift?\n\nChanges in the data that our model sees in production over time.\n\nWhat are the three steps in the deployment process?\n\nManual Process\nLimited scope deployment\nGradual expansion\n\n\nFurther Research\n\nConsider how the Drivetrain Approach maps to a project or problem you’re interested in.\n\nI’ll take the example of a project I will be working on to practice what I’m learning in this book: training a deep learning model which correctly classifies the typeface from a collection of single letter.\n\nThe objective: correctly classify typeface from a collection of single letters.\nLevers: observe key features of key letters that are the “tell” of a typeface.\nData: using an HTML canvas object and Adobe Fonts, generate images of single letters of multiple fonts associated with each category of typeface.\nModels: output the probabilities of each typeface a given collection of single letters is predicted as. This allows for some flexibility in how you categorize letters based on the shared characteristics of more than one typeface that the particular font may possess.\n\n\nWhen might it be best to avoid certain types of data augmentation?\n\nIn my typeface example, it’s best to avoid perspective warping because it will change key features used to recognize a typeface.\n\nFor a project you’re interested in applying deep learning to, consider the thought experiment, “What would happen if it went really, really well?”\n\nIf my typeface classifier works really well, I imagine it would be used by people to take pictures of real-world text and learn what typeface it is. This may inspire a new wave of typeface designers. If a feedback loop was possible, and the classifier went viral, the very definition of typefaces may be affected by popular opinion. Taken a step further, a generative model may be inspired by this classifier, and a new wave of AI typeface would be launched—however this last piece is highly undesirable unless the training of the model involves appropriate licensing and attribution of the typefaces used that are created by humans. Furthermore, from what I understand from reading about typefaces, the process of creating a typeface is an amazing experience and should not be replaced with AI generators. If I created such a generative model (in part 2 of the course) and it went viral (do HuggingFace Spaces go viral? Cuz that’s where I would launch it), I would take it down.\n\nStart a blog (done!)\n\n\n\n\n\n\n\n\n\nLink to this lesson’s video.\n\nHow to do a fast.ai lesson\n\nWatch lecture\nRun notebook & experiment\nReproduce results\nRepeat with different dataset\n\nfastbook repo contains “clean” folder with notebooks without markdown text.\nTwo concepts: training the model and using it for inference.\nOver 500 architectures in timm (PyTorch Image Models).\ntimm.list_models(pattern) will list models matching the pattern.\nPass string name of timm model to the Learner like: vision_learner(dls, 'timm model string', ...).\nin22 = ImageNet with 22k categories, 1k = ImageNet with 1k categories.\nlearn.predict probabilities are in the order of learn.dls.vocab.\nlearn.model contains the trained model which contains lots of nested layers.\nlearn.model.get_submodule takes a dotted string navigating through the hierarchy.\nMachine learning models fit functions to data.\nThings between dollar signs is LaTeX \"$...$\".\nGeneral form of quadratic: def quad(a,b,c,x): return a*x**2 + b*x + c\npartial from functools fixes parameters to a function.\nLoss functions tells us how good our model is.\n@interact from ipywidgets allows sliders tied to the function its above.\nMean Squared Error: def mse(preds, acts): return ((preds - acts)**2).mean()\nFor each parameter we need to know: does the loss get better when we increase or decrease the parameter?\nThe derivative is the function that tells you: if you increase the input does the output increase or decrease, and by how much?\n*params spreads out the list into its elements and passes each to the function.\n1-D (rank 1) tensor (lists of numbers), 2-D tensor (tables of numbers) 3-D tensor (layers of tables of numbers) and so on.\ntensor.requires_grad_() calculates the gradient of the values in the tensor whenever its used in calculation.\nloss.backward() calculates gradients on the inputs to the loss function.\nabc.grad attribute added after gradients are calculated.\nnegative gradient means increasing the parameter will decrease the loss.\nupdate parameters with torch.no_grad() so PyTorch doesn’t calculate the gradient (since it’s being used in a function). We don’t want the derivative of the parameter update, we only want the derivative with respect to the loss.\nAutomate the steps\n\nCalculate Mean Squared Error\nCall .backward.\nSubtract gradient * small number from the parameters\n\nAll optimizers are built on the concept of gradient descent (calculate gradients and decrease the loss).\nWe need a better function than quadratics\nRectified Linear Unit:\n\ndef rectified_linear(m,b,x):\n    y = m*x + b\n    return torch.clip(y, 0.)\n\ntorch.clip turns values less than value specified to the value specified (in this case, it turns negative values to 0.).\nAdding rectified linear functions together gives us an arbitrarily squiggly function that will match as close as we want to the data.\nReLU in 2D gives you surfaces, volumes in 3D, etc.\nWith this incredibly simple foundation you can construct an arbitrarily precise, accurate model.\nWhen you have ReLU’s getting added together, and gradient descent to optimize the parameters, and samples of inputs and outputs that you want, the computer “draws the owl” so to speak.\nDeep learning is using gradient descent to set some parameters to make a wiggly function (the addition of lots of rectified linear units or something very similar to that) that matches your data.\nWhen selecting an architecture, the biggest beginner mistake is that they jump to the highest-accuracy models.\nAt the start of the project, just use resnet18 so you can spend all of your time trying things out (data augmentation, data cleaning, different external data) as fast as possible.\nTrying better architectures is the very last thing to do.\nHow do I know if I have enough data?\n\nVast majority of projects in industry wait far too long until they train their first model.\nTrain your first model on day 1 with whatever CSV files you can hack together.\nSemi-supervised training lets you get dramatically more out of your data.\nOften it’s easy to get lots of inputs but hard to get lots of outputs (labels).\n\nUnits of parameter gradients: for each increase in parameter of 1, the gradient is the amount the loss would change by (if it stayed at that slope—which it doesn’t because it’s a curve).\nOnce you get close enough to the optimal parameter value, all loss functions look like quadratics\n\nThe slope of the loss function decreases as you approach the optimal\n\nLearning rate (a hyperparameter) is multiplied by the gradient, the product of which is subtracted from the parameters\nIf you pick a learning rate that’s too large, you will diverge; if you pick too small, it’ll take too long to train.\nhttp://matrixmultiplication.xyz/\nMatrix multiplication is the critical foundational mathematical operation in deep learning\nGPUs are good at matrix multiplication with tensor cores (multiply together two 4x4 matrices)\nUse a spreadsheet to train a deep learning model on the Kaggle Titanic dataset in which you’re trying to predict if a person survived.\n\nColumns included (convert some of them to binary categorical variables):\n\nSurvivor\nPclass\n\nConvert to Pclass_1 and Pclass_2 (both 1/0).\n\nSex\n\nConvert to Male (0/1) column.\n\nAge\n\nRemove blanks.\nNormalize (Age/Max(Age))\n\nSibSp (how many siblings they have)\nParch (# of parents/children aboard)\nFare\n\nLots of very small and very large fares, log of it has a much more even distribution. (LOG10(Fare + 1).\n\nEmbarked (which city they got on at)\n\nRemove blanks.\nConvert to Embark_S and Embark_C (both 1/0)\n\nOnes\n\nAdd a column of 1s.\n\n\nCreate random numbers for params (including Const) with =RAND() - 0.5.\nRegression\n\nUse SUMPRODUCT to calculate linear function.\nLoss of linear function is (linear function result - Survived) ^ 2.\nAverage loss = AVERAGE(individual losses).\nUser “Solver” with GRG Nonlinear Solving Method. Set Objective to minimize the cell with average loss. Change parameter variables.\n\nNeural Net\n\nTwo sets of params.\nTwo linear columns.\nTwo ReLU columns.\nAdding two linear functions together gives you a linear function, we want all those wiggles (non-linearity) so we use ReLUs.\nReLU: IF(lin1 < 0, 0, lin1)\nPreds = sum of the two ReLUs.\nLoss same as regression.\nSolver process the same as well.\n\nNeural Net (Matrix Multiplication)\n\nTranspose params into two columns.\n=MMULT(...) for Lin1 and Lin2 columns.\nKeep ReLU, Preds and Loss column the same.\nOptimize params using Solver.\nHelpful reminder to build intuition around matrix multiplication: it’s doing the same thing as the SUMPRODUCTs.\n\nDummy variables: Pclass_1, Pclass_2, etc.\n\nNext lesson: NLP\n\nIt’s about making predictions with text data which most of the time is in the form of prose.\nFirst Farsi NLP resource was created by a student of the first fastai course.\nNLP most commonly and practically used for classification.\nDocument = one or two words, a book, a wikipedia page, any length.\nClassification = figure out a category for a document.\nSentiment analysis\nAuthor identification\nLegal discovery (is this document in-scope or out-of-scope)\nOrganizing documents by topic\nTriaging inbound emails\nClassification of text looks similar to images.\nWe’re going to use a different library: HuggingFace Transformers\n\nHelpful to see how things are done in more than one library.\nHuggingFace Transformers doesn’t have the same high-level API. Have to do more stuff manually. Which is good for students at this point of the course.\nIt’s a good library.\n\nBefore the next lesson take a look at the NLP notebook and U.S. Patent to Phrase Matching data.\n\nTrying to figure out in patents whether two concepts are referring to the same thing. The document is text1, text2, and the category is similar (1) or not-similar (0).\n\nWill also talk about the two very important topics of validation sets and metrics.\n\n\n\n\n\n\n\nIn this section, I’ll train a Pets dataset classifier as done by Jeremy in this notebook.\n\nfrom fastai.vision.all import *\nimport timm\n\n\npath = untar_data(URLs.PETS)/'images'\n\n# Create DataLoaders object\ndls = ImageDataLoaders.from_name_func('.',\n                                      get_image_files(path),\n                                      valid_pct=0.2,\n                                      seed=42,\n                                      label_func=RegexLabeller(pat = r'^([^/]+)_\\d+'),\n                                      item_tfms=Resize(224))\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:00<00:00]\n    \n    \n\n\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n# train using resnet34 as architecture\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 196MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.496086\n      0.316146\n      0.100135\n      01:12\n    \n  \n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      45.65% [42/92 00:25<00:30 0.4159]\n    \n    \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.441153\n      0.315289\n      0.093369\n      01:04\n    \n    \n      1\n      0.289844\n      0.215224\n      0.069012\n      01:05\n    \n    \n      2\n      0.123374\n      0.191152\n      0.060217\n      01:03\n    \n  \n\n\n\nThe pets classifier, using resnet34 and 3 epochs, is about 94% accurate.\n\n# train using a timm architecture\n# from the convnext family of architectures\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name convnext_tiny_in22k to current convnext_tiny.fb_in22k.\n  model = create_fn(\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.130913\n      0.240275\n      0.085927\n      01:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.277886\n      0.193888\n      0.061570\n      01:08\n    \n    \n      1\n      0.196232\n      0.174544\n      0.055480\n      01:09\n    \n    \n      2\n      0.127525\n      0.156720\n      0.048038\n      01:07\n    \n  \n\n\n\nUsing convnext_tiny_in22k, the model is about 95.2% accurate, about a 20% decrease in error rate.\n\n# export to use in gradio app\nlearn.export('pets_model.pkl')\n\nYou can view my pets classifier gradio app here.\n\n\n\nIn this section, I’ll plot the timm model results as shown in Jeremy’s notebook.\n\nimport pandas as pd\n\n\n# load data\ndf_results = pd.read_csv(\"../../../fastai-course/data/results-imagenet.csv\")\ndf_results.head()\n\n\n\n\n\n  \n    \n      \n      model\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count\n      img_size\n      crop_pct\n      interpolation\n    \n  \n  \n    \n      0\n      eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n      90.052\n      9.948\n      99.048\n      0.952\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      1\n      eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n      89.966\n      10.034\n      99.012\n      0.988\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      2\n      eva_giant_patch14_560.m30m_ft_in22k_in1k\n      89.786\n      10.214\n      98.992\n      1.008\n      1,014.45\n      560\n      1.0\n      bicubic\n    \n    \n      3\n      eva02_large_patch14_448.mim_in22k_ft_in1k\n      89.624\n      10.376\n      98.950\n      1.050\n      305.08\n      448\n      1.0\n      bicubic\n    \n    \n      4\n      eva02_large_patch14_448.mim_m38m_ft_in1k\n      89.570\n      10.430\n      98.922\n      1.078\n      305.08\n      448\n      1.0\n      bicubic\n    \n  \n\n\n\n\ntop1 = what percent of the time the model predicts the correct label with the highest probability.\ntop5 = what percent of the time the model predits the correct label with the top 5 highest probabilities.\nSource\n\n# remove additional text from model name\ndf_results['model_org'] = df_results['model']\ndf_results['model'] = df_results['model'].str.split('.').str[0]\ndf_results.head()\n\n\n\n\n\n  \n    \n      \n      model\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count\n      img_size\n      crop_pct\n      interpolation\n      model_org\n    \n  \n  \n    \n      0\n      eva02_large_patch14_448\n      90.052\n      9.948\n      99.048\n      0.952\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n    \n    \n      1\n      eva02_large_patch14_448\n      89.966\n      10.034\n      99.012\n      0.988\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n    \n    \n      2\n      eva_giant_patch14_560\n      89.786\n      10.214\n      98.992\n      1.008\n      1,014.45\n      560\n      1.0\n      bicubic\n      eva_giant_patch14_560.m30m_ft_in22k_in1k\n    \n    \n      3\n      eva02_large_patch14_448\n      89.624\n      10.376\n      98.950\n      1.050\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_in22k_ft_in1k\n    \n    \n      4\n      eva02_large_patch14_448\n      89.570\n      10.430\n      98.922\n      1.078\n      305.08\n      448\n      1.0\n      bicubic\n      eva02_large_patch14_448.mim_m38m_ft_in1k\n    \n  \n\n\n\n\n\ndef get_data(part, col):\n    # get benchmark data and merge with model data\n    df = pd.read_csv(f'../../../fastai-course/data/benchmark-{part}-amp-nhwc-pt111-cu113-rtx3090.csv').merge(df_results, on='model')\n    # convert samples/sec to sec/sample\n    df['secs'] = 1. / df[col]\n    # pull out the family name from the model name\n    df['family'] = df.model.str.extract('^([a-z]+?(?:v2)?)(?:\\d|_|$)')\n    # removing `resnetv2_50d_gn` and `resnet50_gn` for some reason\n    df = df[~df.model.str.endswith('gn')]\n    # not sure why the following line is here, \"in22\" was removed in cell above\n    df.loc[df.model.str.contains('in22'),'family'] = df.loc[df.model.str.contains('in22'),'family'] + '_in22'\n    df.loc[df.model.str.contains('resnet.*d'),'family'] = df.loc[df.model.str.contains('resnet.*d'),'family'] + 'd'\n    # only returns subset of families\n    return df[df.family.str.contains('^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin')]\n\n\n# load benchmark inference data\ndf = get_data('infer', 'infer_samples_per_sec')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      model\n      infer_samples_per_sec\n      infer_step_time\n      infer_batch_size\n      infer_img_size\n      param_count_x\n      top1\n      top1_err\n      top5\n      top5_err\n      param_count_y\n      img_size\n      crop_pct\n      interpolation\n      model_org\n      secs\n      family\n    \n  \n  \n    \n      12\n      levit_128s\n      21485.80\n      47.648\n      1024\n      224\n      7.78\n      76.526\n      23.474\n      92.872\n      7.128\n      7.78\n      224\n      0.900\n      bicubic\n      levit_128s.fb_dist_in1k\n      0.000047\n      levit\n    \n    \n      13\n      regnetx_002\n      17821.98\n      57.446\n      1024\n      224\n      2.68\n      68.746\n      31.254\n      88.536\n      11.464\n      2.68\n      224\n      0.875\n      bicubic\n      regnetx_002.pycls_in1k\n      0.000056\n      regnetx\n    \n    \n      15\n      regnety_002\n      16673.08\n      61.405\n      1024\n      224\n      3.16\n      70.278\n      29.722\n      89.528\n      10.472\n      3.16\n      224\n      0.875\n      bicubic\n      regnety_002.pycls_in1k\n      0.000060\n      regnety\n    \n    \n      17\n      levit_128\n      14657.83\n      69.849\n      1024\n      224\n      9.21\n      78.490\n      21.510\n      94.012\n      5.988\n      9.21\n      224\n      0.900\n      bicubic\n      levit_128.fb_dist_in1k\n      0.000068\n      levit\n    \n    \n      18\n      regnetx_004\n      14440.03\n      70.903\n      1024\n      224\n      5.16\n      72.398\n      27.602\n      90.828\n      9.172\n      5.16\n      224\n      0.875\n      bicubic\n      regnetx_004.pycls_in1k\n      0.000069\n      regnetx\n    \n  \n\n\n\n\n\n# plot the data\nimport plotly.express as px\nw,h = 1000, 800\n\ndef show_all(df, title, size):\n    return px.scatter(df,\n                      width=w,\n                      height=h,\n                      size=df[size]**2,\n                      title=title,\n                      x='secs',\n                      y='top1',\n                      log_x=True,\n                      color='family',\n                      hover_name='model_org',\n                      hover_data=[size]\n                     )\n\nshow_all(df, 'Inference', 'infer_img_size')\n\n\n                                                \n\n\n\n# plot a subset of the data\nsubs = 'levit|resnetd?|regnetx|vgg|convnext.*|efficientnetv2|beit|swin'\n\ndef show_subs(df, title, size, subs):\n    df_subs = df[df.family.str.fullmatch(subs)]\n    return px.scatter(df_subs,\n                      width=w,\n                      height=h,\n                      size=df_subs[size]**2,\n                      title=title,\n                      trendline='ols',\n                      trendline_options={'log_x':True},\n                      x='secs',\n                      y='top1',\n                      log_x=True,\n                      color='family',\n                      hover_name='model_org',\n                      hover_data=[size])\n\nshow_subs(df, 'Inference', 'infer_img_size', subs)\n\n\n                                                \n\n\n\n# plot inference speed vs parameter count\npx.scatter(df,\n           width=w,\n           height=h,\n           x='param_count_x',\n           y='secs',\n           log_x=True,\n           log_y=True,\n           color='infer_img_size',\n           hover_name='model_org',\n           hover_data=['infer_samples_per_sec', 'family']\n)\n\n\n                                                \n\n\n\n# repeat plots for training data\ntdf = get_data('train', 'train_samples_per_sec')\nshow_all(tdf, 'Training', 'train_img_size')\n\n\n                                                \n\n\n\n# subset of training data\nshow_subs(tdf, 'Training', 'train_img_size', subs)\n\n\n                                                \n\n\n\n\n\nIn this section, I’ll recreate the content in Jeremy’s notebook here, where he walks through a quadratic example of training a function to match the data.\nA neural network layer:\n\nMultiplies each input by a number of values. These values are known as parameters.\nAdds them up for each group of values.\nReplaces the negative numbers with zeros.\n\n\n# helper functions\nfrom ipywidgets import interact\nfrom fastai.basics import *\n\n\n# helper functions\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\nIn the plot_function definition, I’ll look into why [:,None] is added after torch.linspace(min, max, 100)\n\ntorch.linspace(-1, 1, 10), torch.linspace(-1, 1, 10).shape\n\n(tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n          0.7778,  1.0000]),\n torch.Size([10]))\n\n\n\ntorch.linspace(-1, 1, 10)[:,None], torch.linspace(-1, 1, 10)[:,None].shape\n\n(tensor([[-1.0000],\n         [-0.7778],\n         [-0.5556],\n         [-0.3333],\n         [-0.1111],\n         [ 0.1111],\n         [ 0.3333],\n         [ 0.5556],\n         [ 0.7778],\n         [ 1.0000]]),\n torch.Size([10, 1]))\n\n\n[:, None] adds a dimension to the tensor.\nNext he fits a quadratic function to data:\n\ndef f(x): return 3*x**2 + 2*x + 1\n\nplot_function(f, '$3x^2 + 2x + 1$')\n\n\n\n\nIn order to simulate “finding” or “learning” the right model fit, he creates a general quadratic function:\n\ndef quad(a, b, c, x): return a*x**2 + b*x + c\n\nand uses partial to make new quadratic functions:\n\ndef mk_quad(a, b, c): return partial(quad, a, b, c)\n\n\n# recreating original quadratic with mk_quad\nf2 = mk_quad(3, 2, 1)\nplot_function(f2)\n\n\n\n\n\nf2\n\nfunctools.partial(<function quad at 0x148c6d000>, 3, 2, 1)\n\n\n\nquad\n\n<function __main__.quad(a, b, c, x)>\n\n\nNext he simulates noisy measurements of the quadratic f:\n\n# `scale` parameter is the standard deviation of the distribution\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n\n# noise function matches quadratic x + x^2 (with noise) + constant noise\ndef add_noise(x, mult, add): return x * (1+noise(x, mult)) + noise(x,add)\n\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:, None]\ny = add_noise(f(x), 0.15, 1.5)\n\n\n# values match Jeremy's\nx[:5], y[:5]\n\n(tensor([[-2.0000],\n         [-1.7895],\n         [-1.5789],\n         [-1.3684],\n         [-1.1579]]),\n tensor([[11.8690],\n         [ 6.5433],\n         [ 5.9396],\n         [ 2.6304],\n         [ 1.7947]], dtype=torch.float64))\n\n\n\nplt.scatter(x, y)\n\n<matplotlib.collections.PathCollection at 0x148e16320>\n\n\n\n\n\n\n# overlay data with variable quadratic\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x, y)\n    plot_function(mk_quad(a, b, c), ylim=(-3,13))\n\n\n\n\nImportant note changing sliders: only after changing b and c values do you realize that a also needs to be changed.\nNext, he creates a measure for how well the quadratic fits the data, mean absolute error (distance from each data point to the curve).\n\ndef mae(preds, acts): return (torch.abs(preds-acts)).mean()\n\n\n# update interactive plot\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\nIn a neural network we’ll have tens of millions or more parameters to fit and thousands or millions of data points to fit them to, which we can’t do manually with sliders. We need to automate this process.\nIf we know the gradient of our mae() function with respect to our parameters, a, b and c, then that means we know how adjusting a parameter will change the function. If, say, a has a negative gradient, then we know increasing a will decrease mae(). So we find the gradient of the parameters with respect to the loss function and adjust our parameters a bit in the opposite direction of the gradient sign.\nTo do this we need a function that will take the parameters as a single vector:\n\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n\n\n# testing it out\n# should equal 2.4219\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.4219, dtype=torch.float64)\n\n\n\n# pick an arbitrary starting point for our parameters\nabc = torch.tensor([1.1, 1.1, 1.1])\n\n# tell pytorch to calculate its gradients\nabc.requires_grad_()\n\n# calculate loss\nloss = quad_mae(abc)\nloss\n\ntensor(2.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n\n\n\n# calculate gradients\nloss.backward()\n\n# view gradients\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\n\n# increase parameters to decrease loss based on gradient sign\nwith torch.no_grad():\n    abc -= abc.grad*0.01\n    loss = quad_mae(abc)\n\nprint(f'loss={loss:.2f}')\n\nloss=2.40\n\n\nThe loss has gone down from 2.4219 to 2.40. We’re moving in the right direction.\nThe small number we multiply gradients by is called the learning rate and is the most important hyper-parameter to set when training a neural network.\n\n# use a loop to do a few more iterations\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.40\nstep=1; loss=2.36\nstep=2; loss=2.30\nstep=3; loss=2.21\nstep=4; loss=2.11\nstep=5; loss=1.98\nstep=6; loss=1.85\nstep=7; loss=1.72\nstep=8; loss=1.58\nstep=9; loss=1.46\n\n\nThe loss continues to decrease. Here are our parameters and their gradients at this stage:\n\nabc\n\ntensor([1.9634, 1.1381, 1.4100], requires_grad=True)\n\n\n\nabc.grad\n\ntensor([-13.4260,  -1.0842,  -4.5000])\n\n\nA neural network can approximate any computable function, given enough parameters using two key steps:\n\nMatrix multiplication.\nThe function \\(max(x,0)\\), which simply replaces all negative numbers with zero.\n\nThe combination of a linear function and \\(max\\) is called a rectified linear unit and can be written as:\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y, 0.)\n\n\nplot_function(partial(rectified_linear, 1, 1))\n\n\n\n\n\n# we can do the same thing using PyTorch\nimport torch.nn.functional as F\ndef rectified_linear2(m,b,x): return F.relu(m*x+b)\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\nCreate an interactive ReLU:\n\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m, b), ylim=(-1,4))\n\n\n\n\nObserve what happens when we add two ReLUs together:\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))\n\n\n\n\nCreating a triple ReLU function to fit our data:\n\ndef triple_relu(m1,b1,m2,b2,m3,b3,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x) + rectified_linear(m3,b3,x)\n\ndef mk_triple_relu(m1,b1,m2,b2,m3,b3): return partial(triple_relu, m1,b1,m2,b2,m3,b3)\n\n@interact(m1=-1.5, b1=-1.5, m2=0.5, b2=0.5, m3=1.5, b3=1.5)\ndef plot_double_relu(m1, b1, m2, b2, m3, b3):\n    f = mk_triple_relu(m1,b1,m2,b2,m3,b3)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\nThis same approach can be extended to functions with 2, 3, or more parameters. Drawing squiggly lines through some points is literally all that deep learning does. The above steps will, given enough time and enough data, create (for example) an owl recognizer if you feed it enough owls and non-owls.\nWe can could do thousands of computations on a GPU instead of the above CPU computation. We can greatly reduce the amount of computation and data needed by using a convolution instead of a matrix multiplication. We could make things much faster if, instead of starting with random parameters, we start with parameters of someone else’s model that does something similar to what we want (transfer learning).\n\n\n\nFollowing the instructions in the fastai course lesson video, I’ve created a Microsoft Excel deep learning model here for the Titanic Kaggle data.\nAs shown in the course video, I trained three different models—linear regression, neural net (using SUMPRODUCT) and neural net (using MMULT). After running Microsoft Excel’s Solver, I got the final (different than video) mean loss for each model:\n\nlinear: 0.14422715\nnnet: 0.14385956\nmmult: 0.14385956\n\nThe linear model loss in the video was about 0.10 and the neural net loss was about 0.08. So, my models didn’t do as well.\n\n\n\n\nIn this section, I’ll take notes while reading Chapter 4 in the fastai textbook.\n\n\n\nWe’ll use the MNIST dataset for our experiments, which contains handwritten digits.\nMNIST is collected by the National Institute of Standards and Technology and collated into a machine learning dataset by Yann Lecun who used MNIST in 1998 in LeNet-5, the first computer system to demonstrate practically useful recognition of handwritten digits.\nWe’ve seen that the only consisten trait among every fast.ai student who’s gone on to be a world-class practitioner is that they are all very tenacious.\nIn this chapter we’ll create a model that can classify any image as a 3 or a 7.\n\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:00<00:00]\n    \n    \n\n\n\n# ls method added by fastai\n# lists the count of items\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\n\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\n\n# 3 and 7 are the labels\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n\n(#6131) [Path('/root/.fastai/data/mnist_sample/train/3/10.png'),Path('/root/.fastai/data/mnist_sample/train/3/10000.png'),Path('/root/.fastai/data/mnist_sample/train/3/10011.png'),Path('/root/.fastai/data/mnist_sample/train/3/10031.png'),Path('/root/.fastai/data/mnist_sample/train/3/10034.png'),Path('/root/.fastai/data/mnist_sample/train/3/10042.png'),Path('/root/.fastai/data/mnist_sample/train/3/10052.png'),Path('/root/.fastai/data/mnist_sample/train/3/1007.png'),Path('/root/.fastai/data/mnist_sample/train/3/10074.png'),Path('/root/.fastai/data/mnist_sample/train/3/10091.png')...]\n\n\n\n# view one of the images\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\n\n# the image is stored as numbers\narray(im3)[4:10, 4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\n# same thing, but a PyTorch tensor\ntensor(im3)[4:10, 4:10]\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\n\n# use pandas.DataFrame to color code the array\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15, 4:22])\ndf.style.set_properties(**{'font-size': '6pt'}).background_gradient('Greys')\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      29\n      150\n      195\n      254\n      255\n      254\n      176\n      193\n      150\n      96\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      48\n      166\n      224\n      253\n      253\n      234\n      196\n      253\n      253\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      3\n      0\n      93\n      244\n      249\n      253\n      187\n      46\n      10\n      8\n      4\n      10\n      194\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      4\n      0\n      107\n      253\n      253\n      230\n      48\n      0\n      0\n      0\n      0\n      0\n      192\n      253\n      253\n      156\n      0\n      0\n      0\n    \n    \n      5\n      0\n      3\n      20\n      20\n      15\n      0\n      0\n      0\n      0\n      0\n      43\n      224\n      253\n      245\n      74\n      0\n      0\n      0\n    \n    \n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      249\n      253\n      245\n      126\n      0\n      0\n      0\n      0\n    \n    \n      7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14\n      101\n      223\n      253\n      248\n      124\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      11\n      166\n      239\n      253\n      253\n      253\n      187\n      30\n      0\n      0\n      0\n      0\n      0\n    \n    \n      9\n      0\n      0\n      0\n      0\n      0\n      16\n      248\n      250\n      253\n      253\n      253\n      253\n      232\n      213\n      111\n      2\n      0\n      0\n    \n    \n      10\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      43\n      98\n      98\n      208\n      253\n      253\n      253\n      253\n      187\n      22\n      0\n    \n  \n\n\n\nThe background white pixels are stored a the number 0, black is the number 255, and shades of grey between the two. The entire image contains 28 pixels across and 28 pixels down for a total of 768 pixels.\nHow might a computer recognize these two digits?\nIdeas:\n3s and 7s have distinct features. A seven has generally two straight lines at different angles, a three as two sets of curves stacked on each other. The point where the two curves intersect could be a recognizable feature of the the digit three. The point where the two straight-ish lines intersect could be a recognizable feature of the digit seven. One feature of confusion could be handwritten threes with a straight line at the top, similar to a seven. Another feature of confusion could be a handwritten 3 with a straight-ish ending stroke at the bottom, matching a similar stroke of a 7.\n\n\n\nIdea: find the average pixel value for every pixel of the 3s, then do the same for the 7s. To classify an image, see which of the two ideal digits the image is most similar to.\n\nBaseline: A simple model that you are confident should perform reasonably well. It should be simple to implement and easy to test, so that you can then test each of your improved ideas and make sure they are always better than your baseline. Without starting with a sensible baseline, it is difficult to know whether your super-fancy models are any good.\n\n\n# list comprehension of all digit images\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors), len(seven_tensors)\n\n(6131, 6265)\n\n\n\n# use fastai's show_image to display tensor images\nshow_image(three_tensors[1]);\n\n\n\n\nFor every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this, combine all the images in this list into a single three-dimensional tensor.\nWhen images are floats, the pixel values are expected to be between 0 and 1.\n\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\n# the length of a tensor's shape is its rank\n# rank is the number of axes and dimensions in a tensor\n# shape is the size of each axis of a tensor\nlen(stacked_threes.shape)\n\n3\n\n\n\n# rank of a tensor\nstacked_threes.ndim\n\n3\n\n\nWe calculate the mean of all the image tensors by taking the mean along dimension 0 of our stacked, rank-3 tensor. This is the dimension that indexes over all the images.\n\nmean3 = stacked_threes.mean(0)\nmean3.shape\n\ntorch.Size([28, 28])\n\n\n\nshow_image(mean3);\n\n\n\n\nThis is the ideal number 3 based on the dataset. It’s saturated where all the images agree it should be saturated (much of the background, the intersection of the two curves, and top and bottom curve), but it becomes wispy and blurry where the images disagree.\n\n# do the same for sevens\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\nHow would I calculate how similar a particular image is to each of our ideal digits?\nI would take the average of the absolute difference between each pixel’s intensity and the corresponding mean digit pixel intensity. The lower the average difference, the closer the digit is to the ideal digit.\n\n# sample 3\na_3 = stacked_threes[1]\nshow_image(a_3);\n\n\n\n\nL1 norm = Mean of the absolute value of differences.\nRoot mean squared error (RMSE) = square root of mean of the square of differences.\n\n# L1 norm\ndist_3_abs = (a_3 - mean3).abs().mean()\n\n# RMSE\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs, dist_3_sqr\n\n(tensor(0.1114), tensor(0.2021))\n\n\n\n# L1 norm\ndist_7_abs = (a_3 - mean7).abs().mean()\n\n# RMSE\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs, dist_7_sqr\n\n(tensor(0.1586), tensor(0.3021))\n\n\nFor both L1 norm and RMSE, the distance between the 3 and the “ideal” 3 is less than the distance to the ideal 7, so our simple model will give the right prediction in this case.\nBoth distances are provided in PyTorch:\n\nF.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt()\n\n(tensor(0.1586), tensor(0.3021))\n\n\nMSE = mean squared error.\nMSE will penalize bigger mistakes more heavily (and be lenient with small mistakes) than L1 norm.\n\n\n\nA NumPy array is a multidimensional table of data with all items of the same type.\njagged array: nested arrays of different sizes.\nIf the items of the array are all of simple type such as integer or float, NumPy will store them as a compact C data structure in memory.\nPyTorch tensors cannot be jagged. PyTorch tensors can live on the GPU. And can calculate their derivatives.\n\n# creating arrays and tensors\ndata = [[1,2,3], [4,5,6]]\narr = array(data)\ntns = tensor(data)\n\narr\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\ntns\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\n# select a row\ntns[1]\n\ntensor([4, 5, 6])\n\n\n\n# select a column\ntns[:,1]\n\ntensor([2, 5])\n\n\n\n# slice\ntns[1, 1:3]\n\ntensor([5, 6])\n\n\n\n# standard operators\ntns + 1\n\ntensor([[2, 3, 4],\n        [5, 6, 7]])\n\n\n\n# tensor type\ntns.type()\n\n'torch.LongTensor'\n\n\n\n# tensor changes type when needed\n(tns * 1.5).type()\n\n'torch.FloatTensor'\n\n\n\n\n\nmetric = a number that is calculated based on the predictions of our model and the correct labels in our dataset in order to tell us how good our model is.\nCalculate the metric on the validation set.\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\n\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\n\nvalid_3_tens.shape, valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\n# measure distance between image and ideal\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n\nmnist_distance(a_3, mean3)\n\ntensor(0.1114)\n\n\n\n# calculate mnist_distance for digit 3 validation images\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n(tensor([0.1109, 0.1202, 0.1276,  ..., 0.1357, 0.1262, 0.1157]),\n torch.Size([1010]))\n\n\nPyTorch broadcasts mean3 to each of the 1010 valid_3_dist tensors in order to calculate the distance. It doesn’t actually copy mean3 1010 times. It does the whole calculation in C (or CUDA for GPU).\nIn mean((-1, -2)), the tuple (-1, -2) represents a range of axes. This tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor—the horizontal and the vertical dimensions of an image.\nIf the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it’s a 3:\n\ndef is_3(x): return mnist_distance(x, mean3) < mnist_distance(x, mean7)\n\n\nis_3(a_3), is_3(a_3).float()\n\n(tensor(True), tensor(1.))\n\n\n\n# full validation set---thanks to broadcasting\nis_3(valid_3_tens)\n\ntensor([ True,  True,  True,  ..., False,  True,  True])\n\n\n\n# calculate accuracy\naccuracy_3s = is_3(valid_3_tens).float().mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s, accuracy_7s, (accuracy_3s + accuracy_7s) / 2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\nWe are getting more than 90% accuracy on both 3s and 7s. But they are very different looking digits and we’re classifying only 2 out of 10 digits, so we need to make a better model.\n\n\n\nArthur Samuel’s description of machine learning\n\nSuppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.\n\nOur pixel similarity approach doesn’t have any weight assignment, or any way of improving based on testing the effectiveness of a weight assignment. We can’t improve our pixel similarity approach.\nWe could look at each individual pixel and come up with a set of weights for each, such that the highest weights are associated with those pixels most likely to be black for a particular category. For example, pixels toward the bottom right are not very likely to be activate for a 7, so they should have a low weight for a 7, but ther are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category, for instance, the probability of being the number 8:\ndef pr_eight(x,w) = (x*w).sum()\nX is the image, represented as a vector (with all the rows stacked up end to end into a single long line) and the weights are a vector W. We need some way to update the weights to make them a little bit better. We want to find the specific values for the vector W that cause the result of our function to be high for those images that are 8s and low for those images that are not. Searching for the best vector W is a way to search for the best function for recognizing 8s.\nSteps required to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight how changing that weight would change the loss.\nStep (that is, change) all the weights based on that calculation.\nGo back to step 2 and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\nInitialize: Initialize parameters to random values.\nLoss: We need a function that will return a number that is small if the performance of the model is good (by convention).\nStep: Gradients allow us to directly figure out in which direction and by roughly how much to change each weight.\nStop: Keep training until the accuracy of the model started getting worse or we ran out of time, or once the number of epochs we decided are complete.\n\n\n\nCreate an example loss function:\n\ndef f(x): return x**2\n\nPick a tensor value at which we want gradients:\n\nxt = tensor(3.).requires_grad_()\n\n\nyt = f(xt)\nyt\n\ntensor(9., grad_fn=<PowBackward0>)\n\n\nCalculate gradients (backpropagation–during the backward pass of the network, as opposed to forward pass which is where the activations are calculated):\n\nyt.backward()\n\nView the gradients:\n\nxt.grad\n\ntensor(6.)\n\n\nThe derivative of x**2 is 2*x. When x = 3 the derivative is 6, as calculated above.\nCalculating vector gradients:\n\nxt = tensor([3., 4., 10.]).requires_grad_()\nxt\n\ntensor([ 3.,  4., 10.], requires_grad=True)\n\n\nAdd sum to our function so it takes a vector and returns a scalar:\n\ndef f(x): return (x**2).sum()\n\n\nyt = f(xt)\nyt\n\ntensor(125., grad_fn=<SumBackward0>)\n\n\n\nyt.backward()\nxt.grad\n\ntensor([ 6.,  8., 20.])\n\n\nIf the gradients are very large, that may suggest that we have more adjustments to do, whereas if they are very small, that may suggest that we are close to the optimal value.\n\n\n\nDeciding how to change our parameters based on the values of the gradients—multiplying the gradient by some small number called the learning rate (LR):\nw -= w.grad * lr\nThis is knowns as stepping your parameters using an optimization step.\nIf you pick a learning rate too low, that can mean having to do a lot of steps. If you pick a learning rate too high, that’s even worse, because it can result in the loss getting worse. If the learning rate is too high it may also “bounce” around.\n\n\n\nExample: measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, get slower as it went up the hill, and speed up again going downhill.\n\ntime = torch.arange(0,20).float(); time\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n        14., 15., 16., 17., 18., 19.])\n\n\n\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nspeed\n\ntensor([72.1328, 55.1778, 39.8417, 33.9289, 21.9506, 18.0992, 11.3346,  0.3637,\n         7.3242,  4.0297,  3.9236,  4.1486,  1.9496,  6.1447, 12.7890, 23.8966,\n        30.6053, 45.6052, 53.5180, 71.2243])\n\n\n\nplt.scatter(time, speed);\n\n\n\n\nWe added a bit of random noise since measuring things manually isn’t precise.\nWhat was the roller coaster’s speed? Using SGD, we can try to find a function that matches our observations. Guess that it will be a quadratic of the form a*(time**2) + (b*t) + c.\nWe want to distinguish clearly between the function’s input (the time when we are measuring the coaster’s speed) and its parameters (the values that define which quadratic we’re trying).\nCollect parameters in one argument and separate t and params in the function’s signature:\n\ndef f(t, params):\n  a,b,c = params\n  return a*(t**2) + (b*t) + c\n\nDefine a loss function:\n\ndef mse(preds, targets): return ((preds-targets)**2).mean()\n\nStep 1: Initialize the parameters\n\nparams = torch.randn(3).requires_grad_()\n\nStep 2: Calculate the predictions\n\npreds = f(time, params)\n\nCreate a little function to see how close our predictions are to our targets:\n\ndef show_preds(preds, ax=None):\n  if ax is None: ax=plt.subplots()[1]\n  ax.scatter(time, speed)\n  ax.scatter(time, to_np(preds), color='red')\n  ax.set_ylim(-300,100)\n\nshow_preds(preds)\n\n\n\n\nStep 3: Calculate the loss\n\nloss = mse(preds, speed)\nloss\n\ntensor(11895.1143, grad_fn=<MeanBackward0>)\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nparams.grad\n\ntensor([-35554.0117,  -2266.8909,   -171.8540])\n\n\n\nparams\n\ntensor([-0.5364,  0.6043,  0.4822], requires_grad=True)\n\n\nStep 5: Step the weights\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\nLet’s see if the loss has improved (it has) and take a look at the plot:\n\npreds = f(time, params)\nmse(preds, speed)\n\ntensor(2788.1594, grad_fn=<MeanBackward0>)\n\n\n\nshow_preds(preds)\n\n\n\n\nStep 6: Repeat the process\n\ndef apply_step(params, prn=True):\n  preds = f(time, params)\n  loss = mse(preds, speed)\n  loss.backward()\n  params.data -= lr * params.grad.data\n  params.grad = None\n  if prn: print(loss.item())\n  return preds\n\n\nfor i in range(10): apply_step(params)\n\n2788.159423828125\n1064.841552734375\n738.7333984375\n677.02001953125\n665.3380737304688\n663.1239013671875\n662.7010498046875\n662.6172485351562\n662.59765625\n662.5902709960938\n\n\n\n_, axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\nStep 7: Stop\nWe decided to stop after 10 epochs arbitrarily. In practice, we would watch the training and validation losses and our metrics to decide when to stop.\n\n\n\n\nAt the beginning, the weights of our model can be random (training from scratch) or come from a pretrained model (transfer learning).\nIn both cases the model will need to learn better weights.\nUse a loss function to compare model outputs to targets.\nChange the weights to make the loss a bit lower by multiple gradients by the learning rate and subtracting from the parameters.\nIterate until you have reached the lowest loss and then stop.\n\n\n\n\nConcatenate the images into a single tensor. view changes the shape of a tensor without changing its contents. -1 is a special parameter to view that means “make this axis as big as necessary to fit all the data”.\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\nUse the label 1 for 3s and 0 for 7s. Unsqueeze adds a dimension of size one.\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nPyTorch Dataset is required to return a tuple of (x,y) when indexed.\n\ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor([1]))\n\n\nPrepare the validation dataset:\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x, valid_y))\nx,y = valid_dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([1]))\n\n\nStep 1: Initialize the parameters\nWe need an initially random weight for every pixel.\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28,1))\nweights.shape\n\ntorch.Size([784, 1])\n\n\n\\(y = wx + b\\).\nWe created w (weights) now we need to create b (intercept or bias):\n\nbias = init_params(1)\nbias\n\ntensor([-0.0313], requires_grad=True)\n\n\nStep 2: Calculate the predictions\nPrediction for one image\n\n(train_x[0] * weights.T).sum() + bias\n\ntensor([0.5128], grad_fn=<AddBackward0>)\n\n\nIn Python, matrix multiplication is represetend with the @ operator:\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[ 0.5128],\n        [-3.8324],\n        [ 4.9791],\n        ...,\n        [ 3.0790],\n        [ 4.1521],\n        [ 0.3523]], grad_fn=<AddBackward0>)\n\n\nTo decide if an output represents a 3 or a 7, we can just check whether it’s greater than 0:\n\ncorrects = (preds>0.0).float() == train_y\ncorrects\n\ntensor([[ True],\n        [False],\n        [ True],\n        ...,\n        [False],\n        [False],\n        [False]])\n\n\n\ncorrects.float().mean().item()\n\n0.38964182138442993\n\n\nStep 3: Calculate the loss\nA very small change in the value of a weight will often not change the accuracy at all, and thus the gradient is 0 almost everywhere. It’s not useful to use accuracy as a loss function.\nWe need a loss function that when our weights result in slightly better predictions, gives us a slightly better loss.\nIn this case, what does “slightly better prediction mean”: if the correct answer is 3 (1), the score is a little higher, or if the correct answer is a 7 (0), the score is a little lower.\nThe loss function receives not the images themselves, but the predictions from the model.\nThe loss function will measure how distant each prediction is from 1 (if it should be 1) and how distant it is from 0 (if it should be 0) and then it will take the mean of all those distances.\n\ndef mnist_loss(predictions, targets):\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\nTry it out with sample predictions and targets:\n\ntrgts = tensor([1,0,1])\nprds = tensor([0.9, 0.4, 0.2])\ntorch.where(trgts==1, 1-prds, prds)\n\ntensor([0.1000, 0.4000, 0.8000])\n\n\nThis function returns a lower number when predictions are more accurate, when accurate predictions are more confident and when inaccurate predictions are less confident.\nSince we need a scalar for the final loss, mnist_loss takes the mean of the previous tensor:\n\nmnist_loss(prds, trgts)\n\ntensor(0.4333)\n\n\nmnist_loss assumes that predictions are between 0 and 1. We need to ensure that, using sigmoid, which always outputs a number between 0 and 1:\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\nIt’s also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. Update mnist+loss to first apply sigmoid to the inputs:\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\nWe already had a metric, which was overall accuracy. So why did we define a loss?\nTo drive automated learning, the loss must be a function that has a meaningful derivative. It can’t have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level.\nThe loss function is calculated for each item in our dataset, and then at the end of an epoch, the loss values are all averaged and the overall mean is reported for the epoch.\nIt is important that we focus on metrics, rather than the loss, when judging the performance of a model.\n\n\nThe optimization step: change or update the weights based on the gradients.\nTo take an optimization step, we need to calculate the loss over one or more data items. Calculating the loss for the whole dataset would take a long time, calculating it for a single item would not use much information so it would result in an imprecise and unstable gradient.\nCalculate the average loss for a few data items at a time (mini-batch). The number of data items in the mini-batch is called the batch-size.\nA larger batch size means you will get a more accurate and stable estimate of your dataset’s gradients from the loss function, but it will take longer and you will process fewer mini-batches per epoch. Using batches of data works well for GPUs, but give the GPU too many items at once and it will run out of memory.\nWe get better generalization if we can vary things during training (like performing data augmentation). One simple and effective thing we can vary is what data items we put in each mini-batch. Randomly shuffly the dataset before we create mini-batches. The DataLoader will do the shuffling and mini-batch collation for you:\n\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n\n[tensor([10,  3,  8, 11,  0]),\n tensor([6, 1, 7, 9, 4]),\n tensor([12, 13,  5,  2, 14])]\n\n\nFor training, we want a collection containing independent and dependent variables. A Dataset in PyTorch is a collection containing tuples of independent and dependent variables.\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\n\nlist(enumerate(string.ascii_lowercase))[:5]\n\n[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]\n\n\nWhen we pass a Dataset to a Dataloader we will get back many batches that are themselves tuples of tensors representing batches of independent and dependent variables:\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n\n[(tensor([24,  2,  4,  8,  9, 13]), ('y', 'c', 'e', 'i', 'j', 'n')),\n (tensor([23, 17,  6, 14, 25, 18]), ('x', 'r', 'g', 'o', 'z', 's')),\n (tensor([22,  5,  7, 20,  3, 19]), ('w', 'f', 'h', 'u', 'd', 't')),\n (tensor([ 0, 21, 12,  1, 16, 10]), ('a', 'v', 'm', 'b', 'q', 'k')),\n (tensor([11, 15]), ('l', 'p'))]\n\n\n\n\n\n\nIn code, the process will be implemented something like this for each epoch:\nfor x,y in dl:\n  # calculate predictions\n  pred = model(x)\n  # calculate the loss\n  loss = loss_func(pred, y)\n  # calculate the gradients\n  loss.backward()\n  # step the weights\n  parameters -= parameters.grad * lr\nStep 1: Initialize the parameters\n\nweights = init_params((28*28, 1))\nbias = init_params(1)\n\nA DataLoader can be created from a Dataset:\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\nDo the same for the validation set:\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nCreate a mini-batch of size 4 for testing:\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\npreds = linear1(batch)\npreds\n\ntensor([[10.4546],\n        [ 9.4603],\n        [-0.2426],\n        [ 6.7868]], grad_fn=<AddBackward0>)\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(0.1404, grad_fn=<MeanBackward0>)\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nweights.grad.shape, weights.grad.mean(), bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0089), tensor([-0.0619]))\n\n\nCreate a function to calculate gradients:\n\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  loss = mnist_loss(preds, yb)\n  loss.backward()\n\nTest it:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0178), tensor([-0.1238]))\n\n\nLook what happens when we call it again:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(), bias.grad\n\n(tensor(-0.0267), tensor([-0.1857]))\n\n\nThe gradients have changed. loss.backward adds the gradients of loss to any gradients that are currently stored. So we have to set the current gradients to 0 first:\n\nweights.grad.zero_()\nbias.grad.zero_();\n\nMethods in PyTorch whose names end in an underscore modify their objects in place.\nStep 5: Step the weights\nWhen we update the weights and biases based on the gradient and learning rate, we have to tell PyTorch not to take the gradient of this step. If we assign to the data attribute of a tensor, PyTorch will not take the gradient of that step. Here’s our basic training loop for an epoch:\n\ndef train_epoch(model, lr, params):\n  for xb,yb in dl:\n    calc_grad(xb, yb, model)\n    for p in params:\n      p.data -= p.grad*lr\n      p.grad.zero_()\n\nWe want to check how we’re doing by looking at the accuracy of the validation set. To decide if an output represents a 3 (1) or a 7 (0) we can just check whether the prediction is greater than 0.\n\npreds, train_y[:4]\n\n(tensor([[10.4546],\n         [ 9.4603],\n         [-0.2426],\n         [ 6.7868]], grad_fn=<AddBackward0>),\n tensor([[1],\n         [1],\n         [1],\n         [1]]))\n\n\n\n(preds>0.0).float() == train_y[:4]\n\ntensor([[ True],\n        [ True],\n        [False],\n        [ True]])\n\n\n\n# if preds is greater than 0 and the label is 1 -> correct 3 prediction\n# if preds is not greater than 0 and the label is 0 -> correct 7 prediction\nTrue == 1, False == 0\n\n(True, True)\n\n\nCreate a function to calculate validation accuracy:\n\ndef batch_accuracy(xb, yb):\n  preds = xb.sigmoid()\n  correct = (preds>0.5) == yb\n  return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.7500)\n\n\nPut the batches back together:\n\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n  return round(torch.stack(accs).mean().item(), 4)\n\nStarting point accuracy:\n\nvalidate_epoch(linear1)\n\n0.5703\n\n\nLet’s train for 1 epoch and see if the accuracy improves:\n\nlr = 1.\nparams = weights, bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.6928\n\n\nStep 6: Repeat the process\nThen do a few more:\n\nfor i in range(20):\n  train_epoch(linear1, lr, params)\n  print(validate_epoch(linear1), end = ' ')\n\n0.852 0.9061 0.931 0.9418 0.9477 0.9569 0.9584 0.9594 0.9599 0.9633 0.9647 0.9652 0.9657 0.9662 0.9672 0.9677 0.9687 0.9696 0.9701 0.9696 \n\n\nWe’re already about at the same accuracy as our “pixel similarity” approach.\n\n\nReplace our linear function with PyTorch’s nn.Lienar module. A module is an object of a class that inherits from the PyTorch nn.Module class, and behaves identically to standard Python functions in that you can call them using parentheses and they will return the activations of a model.\nnn.Linear does the same thing as our init_params and linear together. It contains both weights and biases in a single class:\n\nlinear_model = nn.Linear(28*28, 1)\n\nEvery PyTorch module knows what parameters it has that can be trained; they are available through the parameters method:\n\nw,b = linear_model.parameters()\nw.shape, b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\nWe can use this information to create an optimizer:\n\nclass BasicOptim:\n  def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n  def step(self, *args, **kwargs):\n    for p in self.params: p.data -= p.grad.data * self.lr\n\n  def zero_grad(self, *args, **kwargs):\n    for p in self.params: p.grad = None\n\nWe can create our optimizer by passing in the model’s parameters:\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\nSimplify our training loop:\n\ndef train_epoch(model):\n  for xb,yb in dl:\n    # calculate the gradients\n    calc_grad(xb,yb,model)\n    # step the weights\n    opt.step()\n    opt.zero_grad()\n\nOur validation function doesn’t need to change at all:\n\nvalidate_epoch(linear_model)\n\n0.3985\n\n\nPut our training loop in a function:\n\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')\n\nSimilar results as the previous training:\n\ntrain_model(linear_model, 20)\n\n0.4932 0.7959 0.8506 0.9136 0.9341 0.9492 0.9556 0.9629 0.9658 0.9683 0.9702 0.9717 0.9741 0.9746 0.9761 0.9766 0.9775 0.978 0.9785 0.979 \n\n\nfastai provides the SGD class that by default does the same thing as our BasicOptim:\n\nlinear_model = nn.Linear(28*28, 1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n0.4932 0.8735 0.8174 0.9082 0.9331 0.9468 0.9546 0.9614 0.9653 0.9668 0.9692 0.9727 0.9736 0.9751 0.9756 0.9761 0.9775 0.978 0.978 0.9785 \n\n\nfastai provides Learner.fit which we can use instead of train_model. To create a Learner we first need to create a DataLoaders, by passing our training and validation DataLoaders:\n\ndls = DataLoaders(dl, valid_dl)\n\nTo create a Learner without using an application such as cnn_learner we need to pass in all the elements that we’ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:\n\nlearn = Learner(dls, nn.Linear(28*28, 1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.636474\n      0.503518\n      0.495584\n      00:00\n    \n    \n      1\n      0.550751\n      0.189374\n      0.840530\n      00:00\n    \n    \n      2\n      0.201501\n      0.178350\n      0.839549\n      00:00\n    \n    \n      3\n      0.087588\n      0.105257\n      0.912659\n      00:00\n    \n    \n      4\n      0.045719\n      0.076968\n      0.933759\n      00:00\n    \n    \n      5\n      0.029454\n      0.061683\n      0.947498\n      00:00\n    \n    \n      6\n      0.022817\n      0.052156\n      0.954367\n      00:00\n    \n    \n      7\n      0.019893\n      0.045825\n      0.962709\n      00:00\n    \n    \n      8\n      0.018424\n      0.041383\n      0.965653\n      00:00\n    \n    \n      9\n      0.017549\n      0.038113\n      0.967125\n      00:00\n    \n  \n\n\n\n\n\n\n\nAdding a nonlinearity between two linear classifiers givs us a neural network.\n\ndef simple_net(xb):\n  res = xb@w1 + b1\n  res = res.max(tensor(0.0))\n  res = res@w2 + b2\n  return res\n\n\n# initialize weights\nw1 = init_params((28*28, 30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nw1 has 30 output activations which means w2 must have 30 input activations so that they match. 30 output activations means that the first layer can construct 30 different features, each representing a different mix of pixels. You can change that 30 to anything you like to make the model more or less complex.\nres.max(tensor(0.0)) is called a rectified linear unit or ReLU. It replaces every negative number with a zero.\n\nplot_function(F.relu)\n\n\n\n\nWe need a nonlinearity becauase a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\nThe neural net can solve any computable problem to an arbitrarily high level of accuracy if you can find the right parameters w1 and w2 and if you make the matrices big enough.\nWe can replace our function with PyTorch:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nnn.Sequential create a modeule that will call each of the listed layers or functions in turn. When using nn.Sequential PyTorch requires us to use the module version (nn.ReLU) and not the function version (F.relu). Modules are classes so you have to instantiate them.\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.363529\n      0.409795\n      0.505888\n      00:00\n    \n    \n      1\n      0.165949\n      0.239534\n      0.792934\n      00:00\n    \n    \n      2\n      0.089140\n      0.117148\n      0.913150\n      00:00\n    \n    \n      3\n      0.056798\n      0.078107\n      0.941119\n      00:00\n    \n    \n      4\n      0.042071\n      0.060734\n      0.957311\n      00:00\n    \n    \n      5\n      0.034718\n      0.051121\n      0.962218\n      00:00\n    \n    \n      6\n      0.030605\n      0.045103\n      0.964181\n      00:00\n    \n    \n      7\n      0.027994\n      0.040995\n      0.966143\n      00:00\n    \n    \n      8\n      0.026145\n      0.037990\n      0.969087\n      00:00\n    \n    \n      9\n      0.024728\n      0.035686\n      0.970559\n      00:00\n    \n    \n      10\n      0.023585\n      0.033853\n      0.972522\n      00:00\n    \n    \n      11\n      0.022634\n      0.032346\n      0.973994\n      00:00\n    \n    \n      12\n      0.021826\n      0.031080\n      0.975466\n      00:00\n    \n    \n      13\n      0.021127\n      0.029996\n      0.976448\n      00:00\n    \n    \n      14\n      0.020514\n      0.029053\n      0.975957\n      00:00\n    \n    \n      15\n      0.019972\n      0.028221\n      0.976448\n      00:00\n    \n    \n      16\n      0.019488\n      0.027481\n      0.977920\n      00:00\n    \n    \n      17\n      0.019051\n      0.026818\n      0.978410\n      00:00\n    \n    \n      18\n      0.018654\n      0.026219\n      0.978410\n      00:00\n    \n    \n      19\n      0.018291\n      0.025677\n      0.978901\n      00:00\n    \n    \n      20\n      0.017958\n      0.025181\n      0.978901\n      00:00\n    \n    \n      21\n      0.017650\n      0.024727\n      0.980373\n      00:00\n    \n    \n      22\n      0.017363\n      0.024310\n      0.980864\n      00:00\n    \n    \n      23\n      0.017096\n      0.023925\n      0.980864\n      00:00\n    \n    \n      24\n      0.016846\n      0.023570\n      0.981845\n      00:00\n    \n    \n      25\n      0.016610\n      0.023241\n      0.982336\n      00:00\n    \n    \n      26\n      0.016389\n      0.022935\n      0.982336\n      00:00\n    \n    \n      27\n      0.016179\n      0.022652\n      0.982826\n      00:00\n    \n    \n      28\n      0.015980\n      0.022388\n      0.982826\n      00:00\n    \n    \n      29\n      0.015791\n      0.022142\n      0.982826\n      00:00\n    \n    \n      30\n      0.015611\n      0.021913\n      0.983317\n      00:00\n    \n    \n      31\n      0.015440\n      0.021700\n      0.983317\n      00:00\n    \n    \n      32\n      0.015276\n      0.021500\n      0.983317\n      00:00\n    \n    \n      33\n      0.015120\n      0.021313\n      0.983317\n      00:00\n    \n    \n      34\n      0.014969\n      0.021137\n      0.983317\n      00:00\n    \n    \n      35\n      0.014825\n      0.020972\n      0.983317\n      00:00\n    \n    \n      36\n      0.014686\n      0.020817\n      0.982826\n      00:00\n    \n    \n      37\n      0.014553\n      0.020671\n      0.982826\n      00:00\n    \n    \n      38\n      0.014424\n      0.020532\n      0.982826\n      00:00\n    \n    \n      39\n      0.014300\n      0.020401\n      0.982826\n      00:00\n    \n  \n\n\n\nYou can view the training process in learn.recorder:\n\nplt.plot(L(learn.recorder.values).itemgot(2))\n\n\n\n\nView the final accuracy:\n\nlearn.recorder.values[-1][2]\n\n0.982826292514801\n\n\nAt this point we have:\n\nA function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters.\nA way to find the best set of parameters for any function (stochastic gradient descent).\n\n\n\nWe can add as many layers in our neural network as we want, as long as we add a nonlinearity between each pair of linear layers.\nThe deeper the model gets, the harder it is to optimize the parameters.\nWith a deeper model (one with more layers) we do not need to use as many parameters. We can use smaller matrices with more layers and get better results than we would get with larger matrices and few layers.\nIn the 1990s what held back the field for years was that so few researchers were experimenting with more than one nonlinearity.\nTraining an 18-layer model:\n\ndls = ImageDataLoaders.from_folder(path)\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n/usr/local/lib/python3.10/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.098852\n      0.014919\n      0.996075\n      02:01\n    \n  \n\n\n\n\n\n\n\nActivations: Numbers that are calculated (both by linear and nonlinear layers)\nParameters: Numbers that are randomly initialized and optimized (that is, the numbers that define the model).\nPart of becoming a good deep learning practitioner is getting used to the idea of looking at your activations and parameters, and plotting the and testing whether they are behaving correctly.\nActivations and parameters are all contained in tensors. The number of dimensions of a tensor is its rank.\nA neural network contains a number of layers. Each layer is either linear or nonlinear. We generally alternate between these two kinds of layers in a neural network. Sometimes a nonlinearity is referred to as an activation function.\nKey concepts related to SGD:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nReLU\nFunction that returns 0 for negative numbers and doesn’t change positive numbers.\n\n\nMini-batch\nA small group of inputs and labels gathered together in two arrays. A gradient descent is updated on this batch (rather than a whole epoch).\n\n\nForward pass\nApplying the model to some input and computing the predictions.\n\n\nLoss\nA value that represents how well or badly our model is doing.\n\n\nGradient\nThe derivative of the loss with respect to some parameter of the model.\n\n\nBackward pass\nComputing the gradients of the loss with respect to all model parameters.\n\n\nGradient descent\nTaking a step in the direction opposite to the gradients to make the model parameters a little bit better.\n\n\nLearning rate\nThe size of the step we take when applying SGD to update the parameters of the model.\n\n\n\n\n\n\n1. How is a grayscale image represented on a computer? How about a color image?\nGrayscale image pixels can be 0 (black) to 255 (white). Color image pixels have three values (Red, Green, Blue) where each value can be from 0 to 255.\n2. How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid')]\n\n\nMNIST_SAMPLE path has a labels.csv file, a train folder, and a valid folder.\n\n(path/'train').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/train/3'),Path('/root/.fastai/data/mnist_sample/train/7')]\n\n\nThe train folder has a 3 and a 7 folder, each which contains training images.\n\n(path/'valid').ls()\n\n(#2) [Path('/root/.fastai/data/mnist_sample/valid/3'),Path('/root/.fastai/data/mnist_sample/valid/7')]\n\n\nThe valid folder contains a 3 and a 7 folder, each containing validation set images.\n3. Explain how the “pixel similarity” approach to classifying digits works.\nPixel similarity works by calculating the absolute mean difference (L1 norm) between each image and the mean digit 3, and averaging the classification (if the absolute mean difference between the image and the ideal 3 is less than the absolute mean difference between the image and the ideal 7, it’s classified as a 3) across all images of each digit’s validation set as the accuracy of the model.\n4. What is list comprehension? Create one now that selects odd numbers from a list and doubles them.\nList comprehension is syntax for creating a new list based on another sequence or iterable (docs)\n\n# for each element in range(10)\n# if the modulo of the element and 2 is not 0\n# double the element's value and store in this new list\ndoubled_odds = [2*elem for elem in range(10) if elem % 2 != 0]\ndoubled_odds\n\n[2, 6, 10, 14, 18]\n\n\n5. What is a rank-3 tensor?\nA rank-3 tensor is a “cube” (3-dimensional tensor).\n6. What is the difference between tensor rank and shape? How do you get the rank from the shape?\nTensor rank is the number of dimensions of the tensor. Tensor shape is the number of elements in each dimension. The following tensor is a 2-dimensional tensor with rank 2, the shape of which is 3 elements by 2 elements.\n\na_tensor = tensor([[1,3], [4,5], [5,6]])\n# dim == rank\na_tensor.dim(), a_tensor.shape\n\n(2, torch.Size([3, 2]))\n\n\n7. What are RMSE and L1 norm?\nRMSE = Root Mean Squared Error: The square root of the mean of squared differences between two sets of values.\nL1 norm = mean absolute difference: the mean of the absolute value of differences between two sets of values.\n8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\nYou can do so by using tensors on a GPU.\n9. Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom four numbers.\n\na_tensor = tensor([[1,2,3], [4,5,6], [7,8,9]])\na_tensor\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\n\na_tensor = 2 * a_tensor\na_tensor\n\ntensor([[ 2,  4,  6],\n        [ 8, 10, 12],\n        [14, 16, 18]])\n\n\n\na_tensor.view(-1, 9)[0,-4:]\n\ntensor([12, 14, 16, 18])\n\n\n10. What is broadcasting? Broadcasting is when a tensor of smaller rank (or a scalar) is expanded so that you can perform an operation between it and a tensor of larger rank. Broadcasting makes it so that the two operands have the same rank.\n\na_tensor + tensor([1,2,3])\n\ntensor([[ 3,  6,  9],\n        [ 9, 12, 15],\n        [15, 18, 21]])\n\n\n\nAre metrics generally calculated using the training set or the validation set? Why?\n\nMetrics are calculated on the validation set because since that is the data the model does not see during training, the metric tells you how your model performs on data it hasn’t seen before.\n12. What is SGD?\nSGD is Stochastic Gradient Descent, an automated process where a model learns the right parameters needed to solve problems like image classification. The randomly (from scratch) or pretrained (transfer learning) parameters are updated using their gradients with respect to the loss and the learning rate. Metrics like the accuracy measure how well the model is performing.\n13. Why does SGD use mini-batches?\nOne reason is to utilize the ability of a GPU to process a lot of data at once.\nAnother reason is that calculating the loss one image at a time leads to an unstable loss function whereas calculating the loss on the entire dataset takes too long. Mini-batches fall in between these two extremes.\n14. What are the seven steps in SGD for machine learning?\n\nInitialize the weights.\nCalculate the predictions.\nCalculate the loss.\nCalculate gradients.\nStep the weights.\nRepeat the process.\nStop.\n\n15. How do we initialize the weights in a model?\nEither randomly (if training from scratch) or using pretrained weights (if transfer learning from an existing model like resnet18).\n16. What is loss?\nA machine-friendly way to measure how well (or badly) the model is performing. The model is learning to step the weights in order to decrease the loss.\n17. Why can’t we always use a high learning rate?\nBecause we risk overshooting the minimum loss (getting stuck back and forth between the two sides of the parabola) or diverging (resulting in larger losses each step).\n18. What is a gradient?\nThe rate of change or derivative of one variable with respect to another variable. In our case, gradients are the ratio of change in loss to change in parameter at one point.\n19. Do you need to know how to calculate gradients yourself?\nNope! Although you should understand the basic concept of derivatives. PyTorch calculates gradients with the .backward method.\n20. Why can’t we use accuracy as a loss function?\nBecause small changes in predictions do not result in small changes in accuracy. Accuracy drastically jumps (from 0 to 1 in our MNIST_SAMPLE example) at one point, with 0 slope elsewhere. We want a smooth function where you can calculate non-zero and non-infinite derivatives everywhere.\n21. Draw the sigmoid function. What is special about its shape?\nThe sigmoid function outputs between 0 and 1 for input values going from -inf to +inf. It also has a smooth positive slope everywhere so it’s easy to take the derivate.\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n22. What is the difference between a loss function and a metric?\nThe loss function is a machine-friendly way to measure the performance of the model while a metric is a human-friendly way to do the same.\nThe purpose of the loss function is to provide a smooth function to take derivates over so the training system can change the weights little by little towards the optimum.\nThe purpose of the metric is to inform the human how well or badly the model is learning during training.\n23. What is the function to calculate new weights using a learning rate?\nIn code, the function is:\nparameters.data -= parameters.grad * lr\nThe new weights are stepped incrementally in the opposite direction of the gradients. If the gradient is negative, the weights will be increased. If the gradient is positive, the weights will be decreased.\n24. What does the DataLoader class do?\nThe DataLoader class prepares training and validation batches and feeds them to the GPU during training. It also performs any necessary item_tfms or batch_tfms to the data.\n25. Write pseudocode showing the basic steps taken in each epoch for SGD.\ndef train_epoch(model):\n  # calculate predictions\n  preds = model(xb)\n  # calculate the loss\n  loss = loss_func(preds, targets)\n  # calculate gradients\n  loss.backward()\n  # step the weights\n  params.data -= params.grad * lr\n  # reset the gradients\n  params.zero_grad_()\n  # calculate accuracy\n  acc = tensor([accuracy for each batch]).mean()\n\nCreate a function that, if passed two arguments [1, 2, 3, 4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n\n\ndef zipped_tuples(x, y): return list(zip(x,y))\n\n\nzipped_tuples([1,2,3,4], 'abcd')\n\n[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n\n\nThe output data structure is the same structure as the PyTorch Dataset.\n27. What does view do in PyTorch?\nview changes the rank and shape of the tensor.\n\ntensor([1,2,3],[4,5,6]).view(3,2)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\ntensor([1,2,3],[4,5,6]).view(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n28. What are the bias parameters in a neural network? Why do we need them?\nThe bias parameters are the intercept \\(b\\) in the function \\(y = wx + b\\). We need them for situations where the inputs are 0 (since \\(w*0 = 0\\)). Bias also helps to create a more flexible function (source).\n29. What does the @ operator do in Python?\nMatrix multiplication.\n\nv1 = tensor([1,2,3])\nv2 = tensor([4,5,6])\nv1 @ v2\n\ntensor(32)\n\n\n30. What does the backward method do?\nCalculate the gradients of the loss function with respect to the parameters.\n31. Why do we have to zero the gradients?\nEach time you call .backward PyTorch will add the new gradients to the current gradients, so we need to zero the gradients to prevent them from accumulating.\n32. What information do we have to pass to Learner?\nReference:\nLearner(dls, simple_net, opt_func=SGD,\n            loss_func=mnist_loss, metrics=batch_accuracy)\nWe pass to the Learner:\n\nDataLoaders containing training and validation sets.\nThe model we want to train.\nAn optimizer function.\nA loss function.\nAny metrics we want calculated.\n\n33. Show Python or pseudocode for the basic steps of a training loop.\nSee #25.\n34. What is ReLU? Draw a plot for it for values from -2 to +2.\nReLU is Rectified Linear Unit. It’s a function where if the inputs are negative, they are set to zero, and if the inputs are positive, they are kept as is.\n\nplot_function(F.relu, min=-2, max=2)\n\n\n\n\n35. What is an activation function?\nAn activation function is the function that produces our predictions (in our case, a neural net with linear and nonlinear layers). Sometimes the ReLU is referred to as the activation function.\n36. What’s the difference between F.relu and nn.ReLU?\nF.relu is a function whereas nn.ReLU is a class that needs to be instantiated.\n37. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why wo we normally use more?\nUsing more layers results in more accurate models.\n\n\n\nSince this lesson’s Further Research was so intensive, I decided to create separate blog posts for each one:\n\nImplementing a fastai Learner from Scratch\nImplementing an MNIST Classifier from Scratch\n\n\n\n\n\n\nAs recommended at the end of the lesson 3 video, I will read + run through the code from Jeremy’s notebook Getting started with NLP for absolute beginners before starting lesson 4.\n\nIn this notebook we’ll see how to solve the Patent Phrase Matching problem by treating it as a classification task, by representing it in a very similar way to that shown above.\n\n\n\n\n\n\n!pip install kaggle\n\n\n! pip install -q datasets\n\n\n! pip install transformers[sentencepiece]\n\n\n!pip install accelerate -U\n\n\n# for working with paths in Python, I recommend using `pathlib.Path`\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\npath = Path('us-patent-phrase-to-phrase-matching')\n\n\nimport zipfile,kaggle\nkaggle.api.competition_download_cli(str(path))\nzipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading us-patent-phrase-to-phrase-matching.zip to /content\n\n\n100%|██████████| 682k/682k [00:00<00:00, 750kB/s]\n\n\n\n\n\n\n\n\n\n!ls {path}\n\nsample_submission.csv  test.csv  train.csv\n\n\n\n\n\n\nimport pandas as pd\n\n\ndf = pd.read_csv(path/'train.csv')\n\n\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n      score\n    \n  \n  \n    \n      0\n      37d61fd2272659b1\n      abatement\n      abatement of pollution\n      A47\n      0.50\n    \n    \n      1\n      7b9652b17b68b7a4\n      abatement\n      act of abating\n      A47\n      0.75\n    \n    \n      2\n      36d72442aefd8232\n      abatement\n      active catalyst\n      A47\n      0.25\n    \n    \n      3\n      5296b0c19e1ce60e\n      abatement\n      eliminating process\n      A47\n      0.50\n    \n    \n      4\n      54c1e3b9184cb5b6\n      abatement\n      forest region\n      A47\n      0.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      36468\n      8e1386cbefd7f245\n      wood article\n      wooden article\n      B44\n      1.00\n    \n    \n      36469\n      42d9e032d1cd3242\n      wood article\n      wooden box\n      B44\n      0.50\n    \n    \n      36470\n      208654ccb9e14fa3\n      wood article\n      wooden handle\n      B44\n      0.50\n    \n    \n      36471\n      756ec035e694722b\n      wood article\n      wooden material\n      B44\n      0.75\n    \n    \n      36472\n      8d135da0b55b8c88\n      wood article\n      wooden substrate\n      B44\n      0.50\n    \n  \n\n36473 rows × 5 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nDataset description\n\ndf.describe(include='object')\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n    \n  \n  \n    \n      count\n      36473\n      36473\n      36473\n      36473\n    \n    \n      unique\n      36473\n      733\n      29340\n      106\n    \n    \n      top\n      37d61fd2272659b1\n      component composite coating\n      composition\n      H01\n    \n    \n      freq\n      1\n      152\n      24\n      2186\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nIn the describe output, freq is the number of rows with the top value in a given column.\n\ndf.query('anchor == \"component composite coating\"').shape\n\n(152, 5)\n\n\nStructure the input data:\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\ndf.input.head()\n\n0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\nName: input, dtype: object\n\n\n\n\n\nTransformers use a Dataset object for storing a dataset. We can create one like so:\n\nfrom datasets import Dataset, DatasetDict\n\nds = Dataset.from_pandas(df)\n\n\nds\n\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n    num_rows: 36473\n})\n\n\nA deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n\nTokenization: Split each text up into words (tokens).\nNumericalization: Convert each word (or token) into a number.\n\nThe details on how this is done depends on the model. So pick a model first:\n\nmodel_nm = 'microsoft/deberta-v3-small'\n\nAutoTokenizer will create a tokenizer appropriate for a given model:\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\nHere’s an example of how the tokenizer splits a text into “tokens” (which are like words, but can be sub-word pieces):\n\ntokz.tokenize(\"G'day folks, I'm Jeremy from fast.ai!\")\n\n['▁G',\n \"'\",\n 'day',\n '▁folks',\n ',',\n '▁I',\n \"'\",\n 'm',\n '▁Jeremy',\n '▁from',\n '▁fast',\n '.',\n 'ai',\n '!']\n\n\nUncommon words will be split into pieces. The start of a new word is represented by _.\n\ntokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")\n\n['▁A',\n '▁platypus',\n '▁is',\n '▁an',\n '▁or',\n 'ni',\n 'tho',\n 'rhynch',\n 'us',\n '▁an',\n 'at',\n 'inus',\n '.']\n\n\nHere’s a simple function which tokenizes our inputs:\n\ndef tok_func(x): return tokz(x[\"input\"])\n\nTo run this quickly in parallel on every row in our dataset, use map:\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\nThis adds a new item to our dataset called input_ids. For instance, here is the input and IDs for the first row of our data:\n\nrow = tok_ds[0]\nrow['input'], row['input_ids']\n\n('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n [1,\n  54453,\n  435,\n  294,\n  336,\n  5753,\n  346,\n  54453,\n  445,\n  294,\n  47284,\n  265,\n  6435,\n  346,\n  23702,\n  435,\n  294,\n  47284,\n  2])\n\n\nThere’s a list called vocab in the tokenizer which contains a unique integer for every possible token string. We can look them up like this, for instance to find the token for the word “of”:\n\ntokz.vocab['▁of']\n\n265\n\n\n265 is present in our input_ids for the first row of data.\n\ntokz.vocab['of']\n\n1580\n\n\nFinally, we need to prepare our labels. Transformers always assumes that your labels has the column name labels, but in our dataset it’s currently score. Therefore, we need to rename it:\n\ntok_ds = tok_ds.rename_columns({'score':'labels'})\n\n\n\n\n\neval_df = pd.read_csv(path/'test.csv')\neval_df.describe()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n    \n  \n  \n    \n      count\n      36\n      36\n      36\n      36\n    \n    \n      unique\n      36\n      34\n      36\n      29\n    \n    \n      top\n      4112d61851461f60\n      el display\n      inorganic photoconductor drum\n      G02\n    \n    \n      freq\n      1\n      2\n      1\n      3\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nThis is the test set. Possibly the most important idea in machine learning is that of having separate training, validation, and test data sets.\n\n\nTo explain the motivation, let’s start simple, and imagine we’re trying to fit a model where the true relationship is this quadratic:\n\ndef f(x): return -3*x**2 + 2*x + 20\n\nUnfortunately matplotlib (the most common library for plotting in Python) doesn’t come with a way to visualize a function, so we’ll write something to do this ourselves:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_function(f, min=-2.1, max=2.1, color='r'):\n    x = np.linspace(min,max, 100)[:,None]\n    plt.plot(x, f(x), color)\n\n\nplot_function(f)\n\n\n\n\nFor instance, perhaps we’ve measured the height above ground of an object before and after some event. The measurements will have some random error. We can use numpy’s random number generator to simulate that. I like to use seed when writing about simulations like this so that I know you’ll see the same thing I do:\n\nfrom numpy.random import normal,seed,uniform\nnp.random.seed(42)\n\n\ndef noise(x, scale): return normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\n\nx = np.linspace(-2, 2, num=20)[:,None]\ny = add_noise(f(x), 0.2, 1.3)\nplt.scatter(x,y);\n\n\n\n\nNow let’s see what happens if we underfit or overfit these predictions. To do that, we’ll create a function that fits a polynomial of some degree (e.g. a line is degree 1, quadratic is degree 2, cubic is degree 3, etc). The details of how this function works don’t matter too much so feel free to skip over it if you like!\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\ndef plot_poly(degree):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(x, y)\n    plt.scatter(x,y)\n    plot_function(model.predict)\n\n\nplot_poly(1)\n\n\n\n\nAs you see, the points on the red line (the line we fitted) aren’t very close at all. This is under-fit – there’s not enough detail in our function to match our data.\nAnd what happens if we fit a degree 10 polynomial to our measurements?\n\nplot_poly(10)\n\n\n\n\nWell now it fits our data better, but it doesn’t look like it’ll do a great job predicting points other than those we measured – especially those in earlier or later time periods. This is over-fit – there’s too much detail such that the model fits our points, but not the underlying process we really care about.\nLet’s try a degree 2 polynomial (a quadratic), and compare it to our “true” function (in blue):\n\nplot_poly(2)\nplot_function(f, color='b')\n\n\n\n\nThat’s not bad at all!\nSo, how do we recognise whether our models are under-fit, over-fit, or “just right”? We use a validation set. This is a set of data that we “hold out” from training – we don’t let our model see it at all. If you use the fastai library, it automatically creates a validation set for you if you don’t have one, and will always report metrics (measurements of the accuracy of a model) using the validation set.\nThe validation set is only ever used to see how we’re doing. It’s never used as inputs to training the model.\nTransformers uses a DatasetDict for holding your training and validation sets. To create one that contains 25% of our data for the validation set, and 75% for the training set, use train_test_split:\n\ndds = tok_ds.train_test_split(0.25, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 27354\n    })\n    test: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9119\n    })\n})\n\n\nAs you see above, the validation set here is called test and not validate, so be careful!\nIn practice, a random split like we’ve used here might not be a good idea – here’s what Dr Rachel Thomas has to say about it:\n\n“One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a train_test_split method, this method takes a random subset of the data, which is a poor choice for many real-world problems.”\n\n\n\n\nSo that’s the validation set explained, and created. What about the “test set” then – what’s that for?\nThe test set is yet another dataset that’s held out from training. But it’s held out from reporting metrics too! The accuracy of your model on the test set is only ever checked after you’ve completed your entire training process, including trying different models, training methods, data processing, etc.\nYou see, as you try all these different things, to see their impact on the metrics on the validation set, you might just accidentally find a few things that entirely coincidentally improve your validation set metrics, but aren’t really better in practice. Given enough time and experiments, you’ll find lots of these coincidental improvements. That means you’re actually over-fitting to your validation set!\nThat’s why we keep a test set held back. Kaggle’s public leaderboard is like a test set that you can check from time to time. But don’t check too often, or you’ll be even over-fitting to the test set!\nKaggle has a second test set, which is yet another held-out dataset that’s only used at the end of the competition to assess your predictions. That’s called the “private leaderboard”.\nWe’ll use eval as our name for the test set, to avoid confusion with the test dataset that was created above.\n\neval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\neval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n\n\n\n\n\n\n\n\nWhen we’re training a model, there will be one or more metrics that we’re interested in maximising or minimising. These are the measurements that should, hopefully, represent how well our model will works for us.\nIn real life, outside of Kaggle, things not easy… As my partner Dr Rachel Thomas notes in The problem with metrics is a big problem for AI:\n\nAt their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so. This is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.\n\nIn Kaggle, however, it’s very straightforward to know what metric to use: Kaggle will tell you! According to this competition’s evaluation page, “submissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores.” This coefficient is usually abbreviated using the single letter r. It is the most widely used measure of the degree of relationship between two variables.\nr can vary between -1, which means perfect inverse correlation, and +1, which means perfect positive correlation. The mathematical formula for it is much less important than getting a good intuition for what the different values look like. To start to get that intuition, let’s look at some examples using the California Housing dataset, which shows “is the median house value for California districts, expressed in hundreds of thousands of dollars”. This dataset is provided by the excellent scikit-learn library, which is the most widely used library for machine learning outside of deep learning.\n\nfrom sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing(as_frame=True)\nhousing = housing['data'].join(housing['target']).sample(1000, random_state=52)\nhousing.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      MedInc\n      HouseAge\n      AveRooms\n      AveBedrms\n      Population\n      AveOccup\n      Latitude\n      Longitude\n      MedHouseVal\n    \n  \n  \n    \n      7506\n      3.0550\n      37.0\n      5.152778\n      1.048611\n      729.0\n      5.062500\n      33.92\n      -118.28\n      1.054\n    \n    \n      4720\n      3.0862\n      35.0\n      4.697897\n      1.055449\n      1159.0\n      2.216061\n      34.05\n      -118.37\n      3.453\n    \n    \n      12888\n      2.5556\n      24.0\n      4.864905\n      1.129222\n      1631.0\n      2.395007\n      38.66\n      -121.35\n      1.057\n    \n    \n      13344\n      3.0057\n      32.0\n      4.212687\n      0.936567\n      1378.0\n      5.141791\n      34.05\n      -117.64\n      0.969\n    \n    \n      7173\n      1.9083\n      42.0\n      3.888554\n      1.039157\n      1535.0\n      4.623494\n      34.05\n      -118.19\n      1.192\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nWe can see all the correlation coefficients for every combination of columns in this dataset by calling np.corrcoef:\n\nnp.set_printoptions(precision=2, suppress=True)\n\nnp.corrcoef(housing, rowvar=False)\n\narray([[ 1.  , -0.12,  0.43, -0.08,  0.01, -0.07, -0.12,  0.04,  0.68],\n       [-0.12,  1.  , -0.17, -0.06, -0.31,  0.  ,  0.03, -0.13,  0.12],\n       [ 0.43, -0.17,  1.  ,  0.76, -0.09, -0.07,  0.12, -0.03,  0.21],\n       [-0.08, -0.06,  0.76,  1.  , -0.08, -0.07,  0.09,  0.  , -0.04],\n       [ 0.01, -0.31, -0.09, -0.08,  1.  ,  0.16, -0.15,  0.13,  0.  ],\n       [-0.07,  0.  , -0.07, -0.07,  0.16,  1.  , -0.16,  0.17, -0.27],\n       [-0.12,  0.03,  0.12,  0.09, -0.15, -0.16,  1.  , -0.93, -0.16],\n       [ 0.04, -0.13, -0.03,  0.  ,  0.13,  0.17, -0.93,  1.  , -0.03],\n       [ 0.68,  0.12,  0.21, -0.04,  0.  , -0.27, -0.16, -0.03,  1.  ]])\n\n\nThis works well when we’re getting a bunch of values at once, but it’s overkill when we want a single coefficient:\n\nnp.corrcoef(housing.MedInc, housing.MedHouseVal)\n\narray([[1.  , 0.68],\n       [0.68, 1.  ]])\n\n\nTherefore, we’ll create this little function to just return the single number we need given a pair of variables:\n\ndef corr(x,y): return np.corrcoef(x,y)[0][1]\n\ncorr(housing.MedInc, housing.MedHouseVal)\n\n0.6760250732906\n\n\nNow we’ll look at a few examples of correlations, using this function (the details of the function don’t matter too much):\n\ndef show_corr(df, a, b):\n    x,y = df[a],df[b]\n    plt.scatter(x,y, alpha=0.5, s=4)\n    plt.title(f'{a} vs {b}; r: {corr(x, y):.2f}')\n\n\nshow_corr(housing, 'MedInc', 'MedHouseVal')\n\n\n\n\nSo that’s what a correlation of 0.68 looks like. It’s quite a close relationship, but there’s still a lot of variation. (Incidentally, this also shows why looking at your data is so important – we can see clearly in this plot that house prices above $500,000 seem to have been truncated to that maximum value).\nLet’s take a look at another pair:\n\nshow_corr(housing, 'MedInc', 'AveRooms')\n\n\n\n\nThe relationship looks like it is similarly close to the previous example, but r is much lower than the income vs valuation case. Why is that? The reason is that there are a lot of outliers – values of AveRooms well outside the mean.\nr is very sensitive to outliers. If there’s outliers in your data, then the relationship between them will dominate the metric. In this case, the houses with a very high number of rooms don’t tend to be that valuable, so it’s decreasing r from where it would otherwise be.\nLet’s remove the outliers and try again:\n\nsubset = housing[housing.AveRooms<15]\nshow_corr(subset, 'MedInc', 'AveRooms')\n\n\n\n\nAs we expected, now the correlation is very similar to our first comparison.\nHere’s another relationship using AveRooms on the subset:\n\nshow_corr(subset, 'MedHouseVal', 'AveRooms')\n\n\n\n\nAt this level, with r of 0.34, the relationship is becoming quite weak.\nLet’s look at one more:\n\nshow_corr(subset, 'HouseAge', 'AveRooms')\n\n\n\n\nAs you see here, a correlation of -0.2 shows a very weak negative trend.\nWe’ve seen now examples of a variety of levels of correlation coefficient, so hopefully you’re getting a good sense of what this metric means.\nTransformers expects metrics to be returned as a dict, since that way the trainer knows what label to use, so let’s create a function to do that:\n\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}\n\n\n\n\nTo train a model in Transformers we’ll need this:\n\nfrom transformers import TrainingArguments,Trainer\n\nWe pick a batch size that fits our GPU, and small number of epochs so we can run experiments quickly:\n\nbs = 128\nepochs = 4\n\nThe most important hyperparameter is the learning rate. fastai provides a learning rate finder to help you figure this out, but Transformers doesn’t, so you’ll just have to use trial and error. The idea is to find the largest value you can, but which doesn’t result in training failing.\n\nlr = 8e-5\n\nTransformers uses the TrainingArguments class to set up arguments. Don’t worry too much about the values we’re using here – they should generally work fine in most cases. It’s just the 3 parameters above that you may need to change for different models.\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n\nWe can now create our model, and Trainer, which is a class which combines the data and model together (just like Learner in fastai):\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=corr_d)\n\n\n\n\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nLet’s train our model!\n\ntrainer.train();\n\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n\n    \n      \n      \n      [856/856 03:28, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.032255\n      0.790911\n    \n    \n      2\n      No log\n      0.023222\n      0.814958\n    \n    \n      3\n      0.040500\n      0.022491\n      0.828246\n    \n    \n      4\n      0.040500\n      0.023501\n      0.828109\n    \n  \n\n\n\nThe key thing to look at is the “Pearson” value in table above. As you see, it’s increasing, and is already above 0.8. That’s great news! We can now submit our predictions to Kaggle if we want them to be scored on the official leaderboard. Let’s get some predictions on the test set:\n\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds\n\n\n\n\narray([[ 0.58],\n       [ 0.69],\n       [ 0.57],\n       [ 0.33],\n       [-0.01],\n       [ 0.5 ],\n       [ 0.55],\n       [-0.01],\n       [ 0.31],\n       [ 1.15],\n       [ 0.29],\n       [ 0.24],\n       [ 0.76],\n       [ 0.91],\n       [ 0.75],\n       [ 0.43],\n       [ 0.33],\n       [-0.01],\n       [ 0.66],\n       [ 0.33],\n       [ 0.46],\n       [ 0.26],\n       [ 0.18],\n       [ 0.22],\n       [ 0.59],\n       [-0.04],\n       [-0.02],\n       [ 0.01],\n       [-0.03],\n       [ 0.59],\n       [ 0.3 ],\n       [-0.  ],\n       [ 0.68],\n       [ 0.52],\n       [ 0.47],\n       [ 0.23]])\n\n\nLook out - some of our predictions are <0, or >1! This once again shows the value of remember to actually look at your data. Let’s fix those out-of-bounds predictions:\n\npreds = np.clip(preds, 0, 1)\n\n\npreds\n\narray([[0.58],\n       [0.69],\n       [0.57],\n       [0.33],\n       [0.  ],\n       [0.5 ],\n       [0.55],\n       [0.  ],\n       [0.31],\n       [1.  ],\n       [0.29],\n       [0.24],\n       [0.76],\n       [0.91],\n       [0.75],\n       [0.43],\n       [0.33],\n       [0.  ],\n       [0.66],\n       [0.33],\n       [0.46],\n       [0.26],\n       [0.18],\n       [0.22],\n       [0.59],\n       [0.  ],\n       [0.  ],\n       [0.01],\n       [0.  ],\n       [0.59],\n       [0.3 ],\n       [0.  ],\n       [0.68],\n       [0.52],\n       [0.47],\n       [0.23]])\n\n\n\n\n\n\nIn this section I’ll run through the explanation and code provided in Jeremy’s notebook here.\nIn this notebook I’ll try to give a taste of how a competitions grandmaster might tackle the U.S. Patent Phrase to Phrase Matching competition. The focus generally should be two things:\n\nCreating an effective validation set\nIterating rapidly to find changes which improve results on the validation set.\n\nIf you can do these two things, then you can try out lots of experiments and find what works, and what doesn’t. Without these two things, it will be nearly impossible to do well in a Kaggle competition (and, indeed, to create highly accurate models in real life!)\nThe more code you have, the more you have to maintain, and the more chances there are to make mistakes. So keep it simple!\n\nfrom pathlib import Path\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle:\n    !pip install -Uqq fastai\nelse:\n    import zipfile,kaggle\n    path = Path('us-patent-phrase-to-phrase-matching')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading us-patent-phrase-to-phrase-matching.zip to /content\n\n\n100%|██████████| 682k/682k [00:00<00:00, 1.49MB/s]\n\n\n\n\n\n\n\n\n\nfrom fastai.imports import *\n\n\nif iskaggle: path = Path('../input/us-patent-phrase-to-phrase-matching')\npath.ls()\n\n(#3) [Path('us-patent-phrase-to-phrase-matching/sample_submission.csv'),Path('us-patent-phrase-to-phrase-matching/test.csv'),Path('us-patent-phrase-to-phrase-matching/train.csv')]\n\n\nLet’s look at the training set:\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n      score\n    \n  \n  \n    \n      0\n      37d61fd2272659b1\n      abatement\n      abatement of pollution\n      A47\n      0.50\n    \n    \n      1\n      7b9652b17b68b7a4\n      abatement\n      act of abating\n      A47\n      0.75\n    \n    \n      2\n      36d72442aefd8232\n      abatement\n      active catalyst\n      A47\n      0.25\n    \n    \n      3\n      5296b0c19e1ce60e\n      abatement\n      eliminating process\n      A47\n      0.50\n    \n    \n      4\n      54c1e3b9184cb5b6\n      abatement\n      forest region\n      A47\n      0.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      36468\n      8e1386cbefd7f245\n      wood article\n      wooden article\n      B44\n      1.00\n    \n    \n      36469\n      42d9e032d1cd3242\n      wood article\n      wooden box\n      B44\n      0.50\n    \n    \n      36470\n      208654ccb9e14fa3\n      wood article\n      wooden handle\n      B44\n      0.50\n    \n    \n      36471\n      756ec035e694722b\n      wood article\n      wooden material\n      B44\n      0.75\n    \n    \n      36472\n      8d135da0b55b8c88\n      wood article\n      wooden substrate\n      B44\n      0.50\n    \n  \n\n36473 rows × 5 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nAnd the test set:\n\neval_df = pd.read_csv(path/'test.csv')\nlen(eval_df)\n\n36\n\n\n\neval_df.head()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n    \n  \n  \n    \n      0\n      4112d61851461f60\n      opc drum\n      inorganic photoconductor drum\n      G02\n    \n    \n      1\n      09e418c93a776564\n      adjust gas flow\n      altering gas flow\n      F23\n    \n    \n      2\n      36baf228038e314b\n      lower trunnion\n      lower locating\n      B60\n    \n    \n      3\n      1f37ead645e7f0c8\n      cap component\n      upper portion\n      D06\n    \n    \n      4\n      71a5b6ad068d531f\n      neural stimulation\n      artificial neural network\n      H04\n    \n  \n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ndf.target.value_counts()\n\ncomposition                    24\ndata                           22\nmetal                          22\nmotor                          22\nassembly                       21\n                               ..\nswitching switch over valve     1\nswitching switch off valve      1\nswitching over valve            1\nswitching off valve             1\nwooden substrate                1\nName: target, Length: 29340, dtype: int64\n\n\nWe see that there’s nearly as many unique targets as items in the training set, so they’re nearly but not quite unique. Most importantly, we can see that these generally contain very few words (1-4 words in the above sample).\n\ndf.anchor.value_counts()\n\ncomponent composite coating              152\nsheet supply roller                      150\nsource voltage                           140\nperfluoroalkyl group                     136\nel display                               135\n                                        ... \nplug nozzle                                2\nshannon                                    2\ndry coating composition1                   2\nperipheral nervous system stimulation      1\nconduct conducting material                1\nName: anchor, Length: 733, dtype: int64\n\n\nWe can see here that there’s far fewer unique values (just 733) and that again they’re very short (2-4 words in this sample).\n\ndf.context.value_counts()\n\nH01    2186\nH04    2177\nG01    1812\nA61    1477\nF16    1091\n       ... \nB03      47\nF17      33\nB31      24\nA62      23\nF26      18\nName: context, Length: 106, dtype: int64\n\n\nThe first character is the section the patent was filed under – let’s create a column for that and look at the distribution:\n\ndf['section'] = df.context.str[0]\ndf.section.value_counts()\n\nB    8019\nH    6195\nG    6013\nC    5288\nA    4094\nF    4054\nE    1531\nD    1279\nName: section, dtype: int64\n\n\nFinally, we’ll take a look at a histogram of the scores:\n\ndf.score.hist();\n\n\n\n\nThere’s a small number that are scored 1.0 - here’s a sample:\n\ndf[df.score==1]\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      anchor\n      target\n      context\n      score\n      section\n    \n  \n  \n    \n      28\n      473137168ebf7484\n      abatement\n      abating\n      F24\n      1.0\n      F\n    \n    \n      158\n      621b048d70aa8867\n      absorbent properties\n      absorbent characteristics\n      D01\n      1.0\n      D\n    \n    \n      161\n      bc20a1c961cb073a\n      absorbent properties\n      absorption properties\n      D01\n      1.0\n      D\n    \n    \n      311\n      e955700dffd68624\n      acid absorption\n      absorption of acid\n      B08\n      1.0\n      B\n    \n    \n      315\n      3a09aba546aac675\n      acid absorption\n      acid absorption\n      B08\n      1.0\n      B\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      36398\n      913141526432f1d6\n      wiring trough\n      wiring troughs\n      F16\n      1.0\n      F\n    \n    \n      36435\n      ee0746f2a8ecef97\n      wood article\n      wood articles\n      B05\n      1.0\n      B\n    \n    \n      36440\n      ecaf479135cf0dfd\n      wood article\n      wooden article\n      B05\n      1.0\n      B\n    \n    \n      36464\n      8ceaa2b5c2d56250\n      wood article\n      wood article\n      B44\n      1.0\n      B\n    \n    \n      36468\n      8e1386cbefd7f245\n      wood article\n      wooden article\n      B44\n      1.0\n      B\n    \n  \n\n1154 rows × 6 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nWe can see from this that these are just minor rewordings of the same concept, and isn’t likely to be specific to context. Any pretrained model should be pretty good at finding these already.\n\n\n\n! pip install transformers[sentencepiece] datasets accelerate\n\n\nfrom torch.utils.data import DataLoader\nimport warnings,transformers,logging,torch\nfrom transformers import TrainingArguments,Trainer\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\n\n\nif iskaggle:\n    !pip install -q datasets\nimport datasets\nfrom datasets import load_dataset, Dataset, DatasetDict\n\n\n# quiet huggingface warnings\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\n\n\n# specify which model we are going to be using\nmodel_nm = 'microsoft/deberta-v3-small'\n\nWe can now create a tokenizer for this model. Note that pretrained models assume that text is tokenized in a particular way. In order to ensure that your tokenizer matches your model, use the AutoTokenizer, passing in your model name.\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n\n\n\n\n\n\n\n\n\nWe’ll need to combine the context, anchor, and target together somehow. There’s not much research as to the best way to do this, so we may need to iterate a bit. To start with, we’ll just combine them all into a single string. The model will need to know where each section starts, so we can use the special separator token to tell it:\n\nsep = tokz.sep_token\nsep\n\n'[SEP]'\n\n\n\ndf['inputs'] = df.context + sep + df.anchor + sep + df.target\n\nGenerally we’ll get best performance if we convert pandas DataFrames into HuggingFace Datasets, so we’ll convert them over, and also rename the score column to what Transformers expects for the dependent variable, which is label:\n\nds = Dataset.from_pandas(df).rename_column('score', 'label')\neval_ds = Dataset.from_pandas(eval_df)\n\nTo tokenize the data, we’ll create a function (since that’s what Dataset.map will need):\n\ndef tok_func(x): return tokz(x[\"inputs\"])\n\n\ntok_func(ds[0])\n\n{'input_ids': [1, 336, 5753, 2, 47284, 2, 47284, 265, 6435, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nThe only bit we care about at the moment is input_ids. We can see in the tokens that it starts with a special token 1 (which represents the start of text), and then has our three fields separated by the separator token 2. We can check the indices of the special token IDs like so:\n\ntokz.all_special_tokens\n\n['[CLS]', '[SEP]', '[UNK]', '[PAD]', '[MASK]']\n\n\nWe can now tokenize the input. We’ll use batching to speed it up, and remove the columns we no longer need:\n\ninps = \"anchor\",\"target\",\"context\"\ntok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','id','section'))\n\n\n\n\nLooking at the first item of the dataset we should see the same information as when we checked tok_func above:\n\ntok_ds[0]\n\n{'label': 0.5,\n 'input_ids': [1, 336, 5753, 2, 47284, 2, 47284, 265, 6435, 2],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\n\n\nAccording to this post, the private test anchors do not overlap with the training set. So let’s do the same thing for our validation set.\nFirst, create a randomly shuffled list of anchors:\n\nanchors = df.anchor.unique()\nnp.random.seed(42)\nnp.random.shuffle(anchors)\nanchors[:5]\n\narray(['time digital signal', 'antiatherosclerotic', 'filled interior',\n       'dispersed powder', 'locking formation'], dtype=object)\n\n\nNow we can pick some proportion (e.g 25%) of these anchors to go in the validation set:\n\nval_prop = 0.25\nval_sz = int(len(anchors)*val_prop)\nval_anchors = anchors[:val_sz]\n\nNow we can get a list of which rows match val_anchors, and get their indices:\n\n# is_val is a boolean array\nis_val = np.isin(df.anchor, val_anchors)\nidxs = np.arange(len(df))\nval_idxs = idxs[ is_val]\ntrn_idxs = idxs[~is_val]\nlen(val_idxs),len(trn_idxs)\n\n(9116, 27357)\n\n\nOur training and validation Datasets can now be selected, and put into a DatasetDict ready for training:\n\ndds = DatasetDict({\"train\":tok_ds.select(trn_idxs),\n             \"test\": tok_ds.select(val_idxs)})\n\nBTW, a lot of people do more complex stuff for creating their validation set, but with a dataset this large there’s not much point. As you can see, the mean scores in the two groups are very similar despite just doing a random shuffle:\n\ndf.iloc[trn_idxs].score.mean(),df.iloc[val_idxs].score.mean()\n\n(0.3623021530138539, 0.3613426941641071)\n\n\n\n\n\nLet’s now train our model! We’ll need to specify a metric, which is the correlation coefficient provided by numpy (we need to return a dictionary since that’s how Transformers knows what label to use):\n\ndef corr(eval_pred): return {'pearson': np.corrcoef(*eval_pred)[0][1]}\n\nWe pick a learning rate and batch size that fits our GPU, and pick a reasonable weight decay and small number of epochs:\n\nlr,bs = 8e-5,128\nwd,epochs = 0.01,4\n\nTransformers uses the TrainingArguments class to set up arguments. We’ll use a cosine scheduler with warmup, since at fast.ai we’ve found that’s pretty reliable. We’ll use fp16 since it’s much faster on modern GPUs, and saves some memory. We evaluate using double-sized batches, since no gradients are stored so we can do twice as many rows at a time.\n\ndef get_trainer(dds):\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                   tokenizer=tokz, compute_metrics=corr)\n\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=wd, report_to='none')\n\nWe can now create our model, and Trainer, which is a class which combines the data and model together (just like Learner in fastai):\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n               tokenizer=tokz, compute_metrics=corr)\n\n\n\n\n\ntrainer.train();\n\n\n\n    \n      \n      \n      [856/856 03:02, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.027171\n      0.794542\n    \n    \n      2\n      No log\n      0.026872\n      0.811033\n    \n    \n      3\n      0.035300\n      0.024633\n      0.816882\n    \n    \n      4\n      0.035300\n      0.024581\n      0.817413\n    \n  \n\n\n\n\n\n\nWe now want to start iterating to improve this. To do that, we need to know whether the model gives stable results. I tried training it 3 times from scratch, and got a range of outcomes from 0.808-0.810. This is stable enough to make a start - if we’re not finding improvements that are visible within this range, then they’re not very significant! Later on, if and when we feel confident that we’ve got the basics right, we can use cross validation and more epochs of training.\nIteration speed is critical, so we need to quickly be able to try different data processing and trainer parameters. So let’s create a function to quickly apply tokenization and create our DatasetDict:\n\ndef get_dds(df):\n    ds = Dataset.from_pandas(df).rename_column('score', 'label')\n    tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','id','section'))\n    return DatasetDict({\"train\":tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})\n\n\ndef get_model(): return AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n\n\ndef get_trainer(dds, model=None):\n    if model is None: model = get_model()\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                   tokenizer=tokz, compute_metrics=corr)\n\nPerhaps using the special separator character isn’t a good idea, and we should use something we create instead. Let’s see if that makes things better. First we’ll change the separator and create the DatasetDict:\n\nsep = \" [s] \"\ndf['inputs'] = df.context + sep + df.anchor + sep + df.target\ndds = get_dds(df)\n\n\n\n\n\nget_trainer(dds).train()\n\n\n\n    \n      \n      \n      [856/856 03:27, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.027216\n      0.799765\n    \n    \n      2\n      No log\n      0.025568\n      0.814325\n    \n    \n      3\n      0.031000\n      0.023474\n      0.817759\n    \n    \n      4\n      0.031000\n      0.024206\n      0.817377\n    \n  \n\n\n\nTrainOutput(global_step=856, training_loss=0.023552694610346144, metrics={'train_runtime': 207.9058, 'train_samples_per_second': 526.335, 'train_steps_per_second': 4.117, 'total_flos': 582121520370810.0, 'train_loss': 0.023552694610346144, 'epoch': 4.0})\n\n\nThat’s looking quite a bit better, so we’ll keep that change.\n(Vishal note: I trained it a few times but couldn’t get the pearson coefficient past 0.8174)\nOften changing to lowercase is helpful. Let’s see if that helps too:\n\ndf['inputs'] = df.inputs.str.lower()\ndds = get_dds(df)\nget_trainer(dds).train()\n\n\n\n\n\n\n    \n      \n      \n      [755/856 02:53 < 00:23, 4.33 it/s, Epoch 3.52/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.025207\n      0.798847\n    \n    \n      2\n      No log\n      0.024926\n      0.813183\n    \n    \n      3\n      0.031800\n      0.023556\n      0.815640\n    \n  \n\n\n\n\n\n    \n      \n      \n      [856/856 03:17, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.025207\n      0.798847\n    \n    \n      2\n      No log\n      0.024926\n      0.813183\n    \n    \n      3\n      0.031800\n      0.023556\n      0.815640\n    \n    \n      4\n      0.031800\n      0.024359\n      0.815295\n    \n  \n\n\n\nTrainOutput(global_step=856, training_loss=0.024133934595874536, metrics={'train_runtime': 197.3858, 'train_samples_per_second': 554.386, 'train_steps_per_second': 4.337, 'total_flos': 582121520370810.0, 'train_loss': 0.024133934595874536, 'epoch': 4.0})\n\n\n\n\n\nWhat if we made the patent section a special token? Then potentially the model might learn to recognize that different sections need to be handled in different ways. To do that, we’ll use, e.g. [A] for section A. We’ll then add those as special tokens:\n\ndf['sectok'] = '[' + df.section + ']'\nsectoks = list(df.sectok.unique())\ntokz.add_special_tokens({'additional_special_tokens': sectoks})\n\n8\n\n\n\ndf['inputs'] = df.sectok + sep + df.context + sep + df.anchor.str.lower() + sep + df.target\ndds = get_dds(df)\n\n\n\n\nSince we’ve added more tokens, we need to resize the embedding matrix in the model:\n\nmodel = get_model()\nmodel.resize_token_embeddings(len(tokz))\n\nEmbedding(128009, 768)\n\n\n\ntrainer = get_trainer(dds, model=model)\ntrainer.train()\n\n\n\n    \n      \n      \n      [856/856 03:41, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      No log\n      0.025942\n      0.810038\n    \n    \n      2\n      No log\n      0.025694\n      0.814332\n    \n    \n      3\n      0.010500\n      0.023547\n      0.816508\n    \n    \n      4\n      0.010500\n      0.024562\n      0.817200\n    \n  \n\n\n\nTrainOutput(global_step=856, training_loss=0.009868621826171875, metrics={'train_runtime': 221.7169, 'train_samples_per_second': 493.548, 'train_steps_per_second': 3.861, 'total_flos': 695370741753690.0, 'train_loss': 0.009868621826171875, 'epoch': 4.0})\n\n\nBefore submitting a model, retrain it on the full dataset, rather than just the 75% training subset we’ve used here. Create a function like the ones above to make that easy for you!\n\n\n\n\nIn this section, I’ll take notes while watching this lesson’s video.\n\nIntroduction\n\nIn the book, we do NLP using Recurrent Neural Networks (RNNs).\nIn the video, we’re going to be fine-tuning a pretrained NLP model using a library called HuggingFace Transformers.\nIt’s useful to have experience in using more than one library. See the same concepts applied in different ways. Great for understanding the concepts.\nHuggingFace libraries are SOTA in NLP.\nTransformers library in process of being integrated into fastai library.\nHuggingFace Transformers doesn’t have the same layered API as fastai.\n\nFine-Tuning a Pretrained Model\n\nIn the quadratic/sliders example, a pretrained model is like someone telling you that they are confident what parameter a should be, are somewhat confident what b should be, and have no idea what c should be. Then, we would train c until it firts our model, adjust b and keep a as is. That’s what it’s like fine-tuning a pretrained model.\nA pretrained model is a bunch of parameters have already been fit, where for some of them we’re pretty confident of what they should be, and for some of them we really have no idea at all.\nFine-tuning is the process of taking those ones where we have no idea at all what they should be and trying to get them right, and then moving the other ones a little bit.\n\nULMFiT\n\nThe idea of fine-tuning a pretrained NLP model was pioneered by ULMFiT which was first introduced in a fastai course, later turned into an academic paper by Jeremy and Sebastian Ruder which inspired a huge change in NLP capabilities around the world.\nStep 1\n\nBuild a language model using all of Wikipedia that tried to predict the next word of a Wikipedia article. Filling in these kinds of things requires understanding a lot about how language is structured and about the world. Getting good at fitting a language model requires a neural net getting good at a lot of things. It needs to understand language at a reasonably good level, what is true, what is not true, different ways in which things are expressed and so on. Started with random weights. At the end was a model that could predict more than 30% of the time correctly what the next word in a Wikipedia article would be.\n\nStep 2\n\nCreate a second language model, that predicts the next word of a sentence. Took the pretrained model and ran a few more epochs using IMDb movie reviews. So it got very good at predicting the next work of an IMDb movie review.\n\nStep 3\n\nTook those weights and fine-tuned them for the task of predicting whether or not a movie review was positive or negative sentiment.\n\n\nThe first two models don’t require labels. The labels was what’s the next word of the sentence.\nULMFiT built with RNNs.\nTransformers developed at the same time of ULMFiT’s release.\nTransformers can take advantage of modern accelerators like Google’s TPUs.\nTransformers don’t allow you to predict the next word of a sentence, it’s just not how they are structured. Instead they deleted at random a few words and asked the model to predict what words were deleted. The basic concept similar to ULMFiT ,replaced RNN with Transformer. Replaced language model with masked language model.\nHow do you go from a model that’s trained to predict the next word to a model that does classification?\n\nThe first layer of ImageNet classification model finds basic features like diagonal edges, gradients, etc. Layer two combined those (ReLUs added together, activations from sets of ReLUs matrix multipled, etc.)\nLayer 5 had bird and lizard eyeball detectors, dog face detectors, flowers detectors, etc.\nLater layers do things much more specific to the training task.\nPretty unlikely that you need to change the early layers.\nThe layer that says “what is this” is deleted in fine-tuning (the layer that has one output per category). The model is then spitting out a few hundred activations. We stick a new random matrix on top of that and train it, so it can predict what you’re trying to predict. Then we gradually train the rest of the layers.\n\nGetting started with NLP for absolute beginners\n\nUS Patent Phrase to Phrase Matching Competition.\nClassification is probably the most widely use case for NLP.\nDocument = an input to an NLP model that contains text.\nClassifying a document is a rich thing to do: sentiment analysis, author identifiation, legal discovery, organizing documents by topic, triaging inbound emails.\nThe Kaggle competition on US Patents does not immediately look like a classification problem.\nColumns: Anchor, target, context, score\nGoal: come up with a model that automatically determines which anchor and target pairs are talking about the same thing. score = 1.0 means the anchor and target mean the same thing, 0.0 means they are not.\nWhether the anchor and target are determined to be similar or not depends on the context.\nRepresent the problem as <constant string><anchor><seperator><constant string><target> and choose category 0.0, 0.25, 0.50, 0.75 or 1.00.\nKaggle data is already on Kaggle.\nAlways look through the competition’s Data page and read through it before jumping into the data.\nUse DataFrame.describe(include='object') to see stats about the fields (count, unique, top, frequency of top).\nThis dataset contains very small documents (3-4 words) that are not very unique. There’s not a lot of unique data to work with.\nCreate a single string of anchor, target, and context with separators and store as the input column.\nNeural networks work with numbers: We’re going to take the numbers, multiply by matrices, replace negatives with zeros, add them up, and do this a few times.\n\nTokenization: Split each document into tokens (words).\nThe list of unique words is called the vocabulary.\nNumericalization: Each word in the vocabulrary gets a number. The bigger the vocab, the more memory gets used, the more data we need to train. We don’t want a large vocabulary.\nTokenize into sub-words (pieces of words).\n\nWe can turn a pandas DataFrame into a Huggingface dataset’s Dataset using Dataset.from_pandas.\nWhatever pretrained model you used comes with a tokenizer. Before you start tokenizing, you have to decide on which model to use.\nHugginface Model Hub has pretrained models trained on specific corpuses.\nThere are some generally good models, deberta-v3 is one of those.\nNLP has been practically effective for general users for only a year or two, a lot of this stuff we’re figuring out as a community.\nAlways start with a small model, it’s faster to train, we’re going to be able to do more iterations.\nAutoTokenizer.from_pretrained(<model name>) will download the vocab and details about how this particular model tokenized the dataset.\n_ represents the start of a word.\ndef tok_func(x): return tokx(x['input']) takes a document x, and tokenizes it’s input.\nDataset.map will parallelize the process of calling the function on each value. batched=True will do a bunch at a time. Tokenizer library is an optimized Rust library.\ninput_ids will contain numbers in the position of each of the tokens.\nHow do you choose the keywords and the order of the fields when creating input?\n\nIt’s arbitrary, try a few things. We just want something it can learn from that separates one field from another.\n\nIf one of the fields was long (1000 characters) is there any special handling required there?\n\nLong documents in ULMFiT require no special consideration. ULMFiT is the best approach for large documents. It will split large documents into pieces.\nLarge documents are challening for Transformers. It does the whole document at once.\nDocuments over 2000 words: look at ULMFiT.\nUnder 2000 words: Transformers should be fine unless you have a laptop GPU with not much memory.\n\nHuggingFace transformers expect that your target is a column called labels.\ntest.csv doesn’t have a score field.\nPerhaps the most important idea in machine learning is having separate training, validation and test datasets.\nTest and validation sets are all about identifying and controlling for overfitting.\nUnderfit: not enough complexity in the model fit to match the data that’s there. It’s systematically biased.\nCommon misunderstanding is that simpler models are more reliable in some way, but models that are too simple will be systematically incorrect.\nOverfit: it’s done a good job of fitting our data points, but if we sample some more data points from our distribution the model won’t be close to them.\nUnderfitting is easy to recognize (we can look at training data and see that it’s not very close).\nOverfitting is harder to recognize because the training data is very close.\nHow do we tell if we have a good fit that’s not overfitting? We measure how good our model is by looking ONLY at the points we set aside as the validation set.\nfast.ai won’t let you train a model without a validation set and shows metrics only on the validation set.\nCreating a good validation set is not generally as simple as just randomly pulling some of your data out of the data that you train your model on.\nKaggle is a great place to learn how to create a good validation set.\nA test set is another validation set that you don’t use for metrics. Helps you see if you overfit using the validation set.\nKaggle has two test sets: leaderboard feedback during competition and second test set that is private until after competition is finished.\nDon’t accidentally find a model that is good by coincidence. Only if you have a test set that you hold out will you know if you’ve done this.\nIf your model is terrible on the test set—go back to square one.\nYou don’t want functions with gradient of 0 of inf (like accuracy) you want something smooth.\nOne metric is not enough to capture all of the real world dynamics involved in a model’s use.\nGoodhart’s law: when a measure becomes a target, it’s ceases to be a good measure.\nAI is really good at optimizing metrics so you have to be careful what metrics you choose for models that are used in real life (impacting people’s lives).\nPearson correlation coefficient is the most widely used measure of how similar two variables are\n\n-1.0 to +1.0.\nAbbreviated as r.\n\nHow do I plot datasets with far too many points? The answer is: get less points (sample).\nnp.corrcoef gives a diagonally symmetric matrix of r values.\nVisualizing your data is important so you can see things like how data is truncated.\nalpha=0.5 for scatter plots creates darker areas where there’s lots of dots.\nr relies on the square of the difference, big outliers increase that by a lot.\nr is very sensitive to outliers.\nIf you’re trying to win a Kaggle competition that uses r and even a couple of your rows are really wrong, it will be a disaster.\nYou almost can’t see the relationship for \\(r=0.34\\)\nTransformers expects metric to be returned as a dict.\ntok_ds.train_test_split() returns a DatasetDict({train: Dataset, test: Dataset}).\nTransformers calls it validation set test, on which is calculates metrics.\nThe fastai equivalent of Learner is the HuggingFace Transformer’s Trainer.\nThe larger the batch size, the more you can do in parallel and the faster it’ll be, but if it’s too large you’ll get an out-of-memory error on the GPU.\nIf you’re using a framework that doesn’t have a learning rate finder like fastai, you can just start with a really low learning rate and then keep doubling it until it falls apart.\nTrainingArguments is a class that takes all of the configuration (like learning rate, warmup ratio, scheduler type, weight decay, etc.).\nYou always want fp16=True as it will be faster.\nAutoModelForSequenceClassification will create an model for classification, .from_pretrained will use a pretrained model which has a num_labels param which is the number of output columns we have, which in this case is 1 (the score).\nTrainer takes the model, the training and validation data, TrainingArguments(), tokenizer and metrics).\nTrainer.train() will train the model.\nHuggingFace is very verbose, the warnings which you can ignore.\nThe only reason we get a high r value after 4 epochs is because we used a pretrained model.\nThe pretrained model already knows a lot about language and has a good sense of whether two phrases have the same meaning or not.\nHow do you decide when it’s okay to remove outliers?\n\nOutliers should never just be removed for modelling.\nInstead we would observe that clearly from looking at this dataset, these two groups can’t be treated the same way (low income/high # of rooms vs. high income/high # of rooms). Split them into two separate analyses.\nOutlier exists in a statistical sense, it doesn’t exist in a real sense (i.e. things that we should ignore or throw away). Some of the most useful insights in data projects are digging into outliers and understanding what are they? and where did they come from? It’s in those edge cases where you discover really important things like when processes go wrong, labelling problems. Never delete outliers. Investigate them, have a strategy about what you’re going to do with them.\n\nTraining with HuggingFace’s Transformer is similar to the things we’ve seen before with fastai.\ntrainer.predict(eval_ds).predictions.astype(float) to get predictions from Trainer object.\nAlways look at your outputs. So you can see things like having negative predictions or predictions over 1, which are outside the range of the patent phrase matching score. For now, we can at least round these off up to 0 and down to 1, respectively, better ways to do this but this is better than nothing.\nKaggle expects submissions to generally be in a CSV file.\nNLP is probably where the biggest opportunities are for big wins in research and commercialization.\n\nIt’s worth thinking about both use and misuse of modern NLP.\nYou can create bots to generate context appropriate conversation and scale it up to 99% of Twitter and nobody would know. This is worrying because a lot of how people see the world is coming out of social media conversation, which at this point are contrallable. It would not be that hard to create something that’s optimized towards moving a point of view amongst a billion people in a very subtle way, very gradually over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth.\nWhat GPT is used for we may not know for decades, if ever.\n2017: millions of submissions to the FTC about Net Neutrality very heavily biased against it. An analysis showed that something like 99% of them were auto-generated. We don’t know for sure but this seems successful because repealing Net Neutrality went through, the comments were factored into this decision.\nYou can always create a generative model that beats bot classifiers designed to classify its content as auto-generated. Similar problem with spam prevention.\nIf you pass num_labels=1 to AutoModelForSequenceClassification it treats it as a regression problem.\n\n\n\n\nIn this section, I’ll take notes and run code examples from Chapter 10: NLP Deep Dive: RNNs in the textbook.\n\nIn general, in NLP the pretrained model is trained on a different task.\nlanguage model: a model that has been trained to guess the next word in a text (having read the ones before).\nself-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels.\nTo properly guess the next word in a sentence, the model will have to develop an understanding of the natural language.\nSelf-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning.\nSelf-supervised learning and computer vision\nEven if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targeting.\nYou get even better results if you fine-tune the sequence-based language model prior to fine-tuning the classification model.\nThe IMDb dataset contains 100k movie reviews (50k unlabeled, 25k labeled training set reviews, 25k labeled validation set reviews). We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles, this will result in a language model that is particularly good at predicting the next word of a movie review. This is known as Universal Language Model Fine-tuning (ULMFiT).\nThe extra stage of fine-tuning the language model, prior to transfer learning to classification task, resulted in significantly better predictions.\n\n\n\n\nUsing categorical variables as independent variables for a neural network:\n\nMake a list of all possible levels of that categorical variable (the vocab).\nReplace each level with its index in the vocab.\nCreate an embedding matrix for this containing a row for each level (i.e., for each item of the vocab).\nUse this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to, but faster and more efficient than, a matrix that takes as input one-hot-encoded vectors representing the indexes).\n\nWe can do nearly the same thing with text:\n\nFirst we concatenate all of the documents in our dataset into one big long string and split it into words (or tokens), giving us a very long list of words.\nOur independent variable will be the sequence of words starting with the first word in our very long list and ending with the second to last, and our dependent variable will be the sequence of words starting with the second word and ending with the last word.\nOur vocab will consist of a mix of common words that are already in the vocabulary of our pretrained model and new words specific to our corpus.\nOur embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words, we won’t have anything, so we will just initialize the corresponding row with a random vector.\n\nSteps for creating a language model:\n\nTokenization: convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\nNumericalization: List all of the unique words that appear (vocab) and convert each word into a number by looking up its index in the vocab.\nLanguage model data loader creation: fastai’s LMDataLoader automatically handles creating a dependent variable that is offset from the independent variable by one token, and handles important details liks shuffling the training data so that the dependent and independent variables maintain their structure as required.\nLanguage model creation: we need a model that handles input lists that could be arbitrarily big or small. We use a Recurrent Neural Network (RNN).\n\n\n\n\nThere is no one approach to tokenization. There are three main approaches:\n\nWord-based: Split a sentence on spaces and separate parts of meaning even when there are no spaces (“don’t” -> “do n’t”). Punctuation marks are generally split into separate tokens.\nSubword based: Split words into smaller parts, based on the most commonly occurring substrings (“occasion” -> “o c ca sion”).\nCharacter-based: Split a sentence into its individual characters.\n\n\n\n\nRather than providing its own tokenizers, fastai provides a consistent interface to a range of tokenizers in external libraries.\nLet’s try it out with the IMDb dataset:\n\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n\n\n\n\n\n\n    \n      \n      100.00% [144441344/144440600 00:02<00:00]\n    \n    \n\n\n\npath.ls()\n\n(#7) [Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/imdb.vocab'),Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/README')]\n\n\nget_text_files gets all the text files in a path\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\n\nfiles[:10]\n\n(#10) [Path('/root/.fastai/data/imdb/unsup/42765_0.txt'),Path('/root/.fastai/data/imdb/unsup/19120_0.txt'),Path('/root/.fastai/data/imdb/unsup/8649_0.txt'),Path('/root/.fastai/data/imdb/unsup/32022_0.txt'),Path('/root/.fastai/data/imdb/unsup/30143_0.txt'),Path('/root/.fastai/data/imdb/unsup/14876_0.txt'),Path('/root/.fastai/data/imdb/unsup/28162_0.txt'),Path('/root/.fastai/data/imdb/unsup/32133_0.txt'),Path('/root/.fastai/data/imdb/unsup/21844_0.txt'),Path('/root/.fastai/data/imdb/unsup/830_0.txt')]\n\n\nHere’s a review that we will tokenize:\n\ntxt = files[0].open().read(); txt[:75]\n\n\"Despite some humorous banter and a decent supporting cast, I can't really r\"\n\n\nWordTokenizer will always point to fastai’s current default word tokenizer.\nfastai’s coll_repr(collection, n) displays the first n items of collection, along with the full size.\n\ntokz = WordTokenizer()\ntoks = first(tokz([txt]))\nprint(coll_repr(toks, 30))\n\n(#243) ['Despite','some','humorous','banter','and','a','decent','supporting','cast',',','I','ca',\"n't\",'really','recommend','this','movie','.','The','leads','are',\"n't\",'very','likable','and','I','did',\"n't\",'particularly','care'...]\n\n\nTokenization is a surprisingly subtle task. “.” is separated when it terminates a sentence but not in an acronym or number:\n\nfirst(tokz(['The U.S. dollar $1 is $1.00.']))\n\n(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']\n\n\nfastai adds some functionality to the tokenization process with the Tokenizer class:\n\ntkn = Tokenizer(tokz)\nprint(coll_repr(tkn(txt), 31))\n\n(#264) ['xxbos','xxmaj','despite','some','humorous','banter','and','a','decent','supporting','cast',',','i','ca',\"n't\",'really','recommend','this','movie','.','xxmaj','the','leads','are',\"n't\",'very','likable','and','i','did',\"n't\"...]\n\n\nTokens that start with xx are special tokens.\nxxbos is a special token that indicates the start of a new text (“BOS” is a standard NLP acronym that means “beginning of stream”). By recognizing this start token, the model will be able to learn it needs to “forget” what was said previously and focus on upcoming words. These special tokens don’t come from the external tokenizer. fastai adds them by default by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. We are translating the original English language sequence into a simplified tokenized language that is designed to be easy for a model to learn.\nFor example, the rules will replace a sequence of four exclamation points with a single exclamation point follow by a special repeated character token and then the number four.\n\ntkn('!!!!')\n\n(#4) ['xxbos','xxrep','4','!']\n\n\nIn this way, the model’s embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repititions of every punctuation mark. A capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word so the embedding matrix needs only the lowercase version of the words saving compute and memory resources but can still learn the concept of capitalization.\nHere are some of the main special tokens:\nxxbos: Indicates the beginning of a text (in this case, a review).\nxxmaj: Indicates the next word begins with a capital.\nxxunk: Indicates the next word is unknown.\n\ndefaults.text_proc_rules\n\n[<function fastai.text.core.fix_html(x)>,\n <function fastai.text.core.replace_rep(t)>,\n <function fastai.text.core.replace_wrep(t)>,\n <function fastai.text.core.spec_add_spaces(t)>,\n <function fastai.text.core.rm_useless_spaces(t)>,\n <function fastai.text.core.replace_all_caps(t)>,\n <function fastai.text.core.replace_maj(t)>,\n <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]\n\n\nfix_html: replaces special HTML characters with a readable version.\nreplace_rep: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character.\nreplace_wrep: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the character.\nspec_add_spaces: adds spaces around / and #.\nrm_useless_spaces: Removes all repetitions of the space character.\nreplace_all_caps: Lowercases a word written in all caps and adds a special token for all caps (xxcap) in front of it.\nreplace_maj: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it.\nlowercase: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\n\ncoll_repr(tkn(\"&copy;    Fast.ai www.fast.ai/INDEX\"), 31)\n\n\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\"\n\n\n\n\n\nWord tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a sentence. However this assumption is not always appropriate. Languages like Chinese and Japanese don’t use spaces. Turkish and Hungarian can add many subwords together without spaces.\nTwo steps of subword tokenization:\n\nAnalyze a corpus of documents to find the most commonly occuring groups of letters. These becomes the vocab.\nTokenize the corpus string using this vocab of subword units.\n\n\ntxts = L(o.open().read() for o in files[:2000])\n\n\n! pip install sentencepiece\ndef subword(sz):\n  sp = SubwordTokenizer(vocab_sz=sz)\n  sp.setup(txts)\n  return ' '.join(first(sp([txt]))[:40])\n\nsetup reads the documents and finds the common sequences of characters to create the vocab.\n\nsubword(1000)\n\n\n\n\n\n\n\n\n\"▁De s p ite ▁some ▁humor ous ▁b ant er ▁and ▁a ▁de cent ▁support ing ▁cast , ▁I ▁can ' t ▁really ▁recommend ▁this ▁movie . ▁The ▁lead s ▁are n ' t ▁very ▁li k able ▁and ▁I\"\n\n\nWhen using fastai’s subword tokenizer, _ represents a space character in the original text.\nIf we use a smaller vocab, each token will represent fewer characters and it will take more tokens to represent a sentence.\n\nsubword(200)\n\n\n\n\n\n\n\n\n'▁ D es p it e ▁ s o m e ▁h u m or o us ▁b an ter ▁and ▁a ▁ d e c ent ▁ s u p p or t ing ▁ c a s t'\n\n\nIf we use a larger vocab, most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence:\n\nsubword(10000)\n\n\n\n\n\n\n\n\n\"▁Des pite ▁some ▁humorous ▁ban ter ▁and ▁a ▁decent ▁support ing ▁cast , ▁I ▁can ' t ▁really ▁recommend ▁this ▁movie . ▁The ▁leads ▁are n ' t ▁very ▁likable ▁and ▁I ▁didn ' t ▁particular ly ▁care ▁if ▁they\"\n\n\nA larger vocab means fewer tokens per sentence, which means faster training, less memory and less state for the model to remember; but on the downside, it means larger embedding matricces, which require more data to learn.\nSubword tokenization provides a way to easily scale between character tokenization (using a small subword vocab) and word tokenization (using a large subword vocab) and handles every human language. It can even handle genomic sequences or MIDI music notation. It’s likely to become (or has already) the most common tokenization approach.\n\n\n\nNumericalization is the process of mapping tokens to integers.\n\nMake a list of all possible levels of the categorical variable (the vocab).\nReplace each level with its index in the vocab.\n\n\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n\n(#264) ['xxbos','xxmaj','despite','some','humorous','banter','and','a','decent','supporting','cast',',','i','ca',\"n't\",'really','recommend','this','movie','.','xxmaj','the','leads','are',\"n't\",'very','likable','and','i','did',\"n't\"...]\n\n\nJust like with SubwordTokenizer, we need to call setup on Numericalize to create the vocab. That means we’ll need our tokenized corpus first:\n\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n\n(#264) ['xxbos','xxmaj','despite','some','humorous','banter','and','a','decent','supporting'...]\n\n\n\nnum = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab, 20)\n\n\"(#2200) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','of','to','is','in','i','it'...]\"\n\n\nOur special rules tokens appear first, and then every word appears once in frequency order.\nThe defaults to Numericalize are min_freq=3 and max_vocab=60000. max_vocab results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn’t enough data to train useful representations for rare words (better handles by setting min_freq, any word appearing fewer than it is replaced with xxunk).\nfastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter.\nThe Numericalizer object is used like a function:\n\nnums = num(toks)[:20]; nums\n\nTensorText([  2,   8, 418,  68,   0,   0,  12,  13, 618, 419, 190,  11,  18,\n            259,  38,  93, 445,  21,  28,  10])\n\n\nWe can check that the integers map back to the original text:\n\n' '.join(num.vocab[o] for o in nums)\n\n\"xxbos xxmaj despite some xxunk xxunk and a decent supporting cast , i ca n't really recommend this movie .\"\n\n\n\n\n\nWe want our language model to read text in order, so that it can efficiently predict what the next word is, this means each new batch should begin precisely where the previous one left off.\nAt the beginning of each epoch we will shuffle the order of the documents to make a new stream.\nWe then cut this stream into a certain number of batches (which is our batch size). For example, if the stream has 50,000 tokens as we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (1 to 5,000 for the first mini-stream, then from 5,001 to 10,000…) because we want the model to read continuous rows of text. An xxbos token is added at the start of each text during preprocessing, so that the model knowns when it reads the stream when a new entry is beginning.\nFirst apply our Numericalize object to the tokenized texts:\n\nnums200 = toks200.map(num)\n\nThen pass it to the LMDataLoader:\n\ndl = LMDataLoader(nums200)\n\n\nx,y = first(dl)\nx.shape, y.shape\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\n\nx[:1], y[:1]\n\n(LMTensorText([[   2,    8,  418,   68,    0,    0,   12,   13,  618,  419,  190,\n                  11,   18,  259,   38,   93,  445,   21,   28,   10,    8,    9,\n                 693,   42,   38,   72, 1274,   12,   18,   81,   38,  479,  420,\n                  58,   47,  305,  274,   17,    9,  135,   10,   18,  619,   81,\n                  38,   49,    9,  221,  120,  221,   47,  305,  274,   11,   29,\n                   8,    0,    8, 1275,  783,   74,   59,  446,   15,   43,    9,\n                   0,  285,  114,    0,   24,    0]]),\n TensorText([[   8,  418,   68,    0,    0,   12,   13,  618,  419,  190,   11,\n                18,  259,   38,   93,  445,   21,   28,   10,    8,    9,  693,\n                42,   38,   72, 1274,   12,   18,   81,   38,  479,  420,   58,\n                47,  305,  274,   17,    9,  135,   10,   18,  619,   81,   38,\n                49,    9,  221,  120,  221,   47,  305,  274,   11,   29,    8,\n                 0,    8, 1275,  783,   74,   59,  446,   15,   43,    9,    0,\n               285,  114,    0,   24,    0,   30]]))\n\n\nLooking at the first row of the independent variable:\n\n' '.join(num.vocab[o] for o in x[0][:20])\n\n\"xxbos xxmaj despite some xxunk xxunk and a decent supporting cast , i ca n't really recommend this movie .\"\n\n\nWhich is the start of the text.\nThe dependent variable is the same thing offset by one token:\n\n' '.join(num.vocab[o] for o in y[0][:20])\n\n\"xxmaj despite some xxunk xxunk and a decent supporting cast , i ca n't really recommend this movie . xxmaj\"\n\n\nWe are now ready to train our text classifier.\n\n\n\n\nTwo steps to training a state-of-the-art text classifier using transfer learning:\n\nFine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews.\nUse that model to train a classifier.\n\n\n\nfastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock.\n\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb,\n    splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n\n\n\n\n\n\n\n\nfrom_folder tells TextBlock how to access the texts so that it can do initial preprocessing. fastai performs a few optmizations:\n\nIt saves the tokenized documents in a temporary folder, so it doesn’t have to tokenize them more than once.\nIt runs multiple tokenization processes in parallel, to take advantage of your computer’s CPUs.\n\n\ndls_lm.show_batch(max_n=2)\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj caught this at xxmaj cinequest . xxmaj it was well attended , but the crowd seemed disappointed . xxmaj in my humble opinion , \" charlie the xxmaj ox \" was very amateurish and overrated ( it pales in comparison with other cinequest pics i saw ) . xxmaj acting ( with the exception of xxmaj polito ) seemed self - conscious and \" stagey . \" xxmaj photography , despite originating on high - end xxup hd\n      xxmaj caught this at xxmaj cinequest . xxmaj it was well attended , but the crowd seemed disappointed . xxmaj in my humble opinion , \" charlie the xxmaj ox \" was very amateurish and overrated ( it pales in comparison with other cinequest pics i saw ) . xxmaj acting ( with the exception of xxmaj polito ) seemed self - conscious and \" stagey . \" xxmaj photography , despite originating on high - end xxup hd ,\n    \n    \n      1\n      career , seemed to specialize in patriarch roles , such as in \" all the xxmaj president 's xxmaj men \" , \" max xxmaj dugan xxmaj returns \" , and \" you xxmaj ca n't xxmaj take it xxmaj with xxmaj you \" . xxmaj and in this case , those of us who never saw him on the stage get a big treat , because this was a taped xxmaj broadway production . xxmaj he dominates every scene\n      , seemed to specialize in patriarch roles , such as in \" all the xxmaj president 's xxmaj men \" , \" max xxmaj dugan xxmaj returns \" , and \" you xxmaj ca n't xxmaj take it xxmaj with xxmaj you \" . xxmaj and in this case , those of us who never saw him on the stage get a big treat , because this was a taped xxmaj broadway production . xxmaj he dominates every scene ,\n    \n  \n\n\n\nEach item in the training dataset is a document:\n\n' '.join(dls_lm.vocab[o] for o in dls_lm.train.dataset[0][0])\n\n\"xxbos xxmaj it is a delight to watch xxmaj laurence xxmaj harvey as a neurotic chess player , who schemes to murder the opponent he can not defeat at the chessboard . xxmaj this movie has wonderful pacing and several cliffhanger moments , as xxmaj harvey 's plot several times seems on the point of failure or exposure , but he manages to beat the odds yet again . xxmaj columbo wages a skilful war of nerves against this high - strung genius , and the scene where he manages to rattle him enough to cause him to make a mistake while playing chess is one of the highlights of the movie , as xxmaj harvey looks down in disbelief at the board , where he has just allowed himself to be xxunk . xxmaj the climax is almost as strong , and watching xxmaj laurence xxmaj harvey collapse completely as his scheme is exposed brings the movie to a satisfying finish . xxmaj highly recommended .\"\n\n\n\n' '.join(dls_lm.vocab[o] for o in dls_lm.train.dataset[2][0])\n\n\"xxbos xxmaj eyeliner was worn nearly 6 xxrep 3 0 years ago in xxmaj egypt . xxmaj really not that much of a stretch for it to be around in the 12th century . i also did n't realize the series flopped . xxmaj there is a second season airing now is n't there ? xxmaj it is amazing to me when commentaries are made by those who are either ill - informed or do n't watch a show at all . xxmaj it is a waste of space on the boards and of other 's time . xxmaj the first show of the series was maybe a bit painful as the cast began to fall into place , but that is to be expected from any show . xxmaj the remainder of the first season is excellent . i can hardly wait for the second season to begin in the xxmaj united xxmaj states .\"\n\n\nTo confirm my understanding, that the first item in each batch is continuing the mini-stream, I’ll take a look at the first mini-stream of the first two batches:\n\ncounter = 0\nfor xb, yb in dls_lm.train:\n  output = ' '.join(dls_lm.vocab[o] for o in xb[0])\n  print(output)\n  counter += 1\n  if counter == 2: break\n\nxxbos xxmaj just got this in the mail and i was positively surprised . xxmaj as a big fan of 70 's cinema it does n't take much to satisfy me when it comes to these kind of flicks . xxmaj despite the obvious low budget on this movie , the acting is overall good and you can already see why xxmaj pesci was to become on of the greatest actors ever . xxmaj i 'm not sure how authentic\nthis movie is , but it sure is a good contribution to the mob genre … .. xxbos xxmaj why on earth should you explore the mesmerizing nature documentary \" earth \" ? xxmaj how much time do you have on earth so i can explain this to you ? xxup ok , i will not xxunk my review exploration on \" earth \" to infinity , but i must stand my ground on why this is a \" must\n\n\nConfirmed! The second batch’s first mini-stream is a continuation of the first batch’s first mini-stream. In this case, the first mini-stream of the second batch also contains the start of the next movie review (document) as indicated by the xxbos special token.\n\n\n\nTo convert the integer word indices into activations that we can use for our neural network, we will use embeddings. We feed those embeddings into a recurrent neural network (RNN) using an architecture called AWS-LSTM.\nThe embeddings in the pretrained model are merged with random embeddings added for words that weren’t in the pretraining vocabulary.\n\nlearn = language_model_learner(\n    dls_lm,\n    AWD_LSTM,\n    drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]\n).to_fp16()\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:00<00:00]\n    \n    \n\n\nThe loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab).\nPerplexity is a metric often used in NLP for language models. It is the exponential of loss (i.e., torch.exp(cross_entropy)).\nlanguage_model_learner automatically calls freeze when using a pretrained model (which is the default) so this will train only the embeddings (the part of the model that contains randomly initialized weights—embeddings for the words that are in our IMDb vocab, but aren’t in the pretrained model vocab).\nI wasn’t able to train my model on Google Colab (I got a ran out of memory error even for small batches) so I trained the IMDb language model on Paperspace and wrote a separate blog post about it.\n\n\n\n\n\nEven simple algorithms could be used to create fraudulent accounts and try to influence policymakers (99% of the 2017 Net Neutrality public comments were likely faked).\nMany people assume or hope that algorithms will come to our defense here, the problem is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms.\n\n\n\n\n1. What is self-supervised learning?\nSelf-supervised learning is when you train a model on data that does not contain any external labels. Instead, the labels are embedded in the independent variable.\n2. What is a language model?\nA language model is a model that predicts the next word based on the previous words in a text.\n3. Why is a language model considered self-supervised?\nBecause we do not train the model with external labels. The dependent variable is the next token in a sequence of previous tokens (independent variable).\n4. What are self-supervised models usually used for?\nPretraining a model that will be used for transfer learning.\n5. Why do we fine-tune language models?\nIn order for it to learn the style of language used in our specific corpus.\n6. What are the three steps to create a state-of-the-art text classifier?\n\nTrain a language model on a large general corpus like Wikipedia.\nFine-tune a language model using your task-specific corpus.\nFine-tune a classifier using the encoder of the twice-pretrained language model.\n\n7. How do the 50,000 unlabeled movie reviews help create a better text classifier for the IMDb dataset?\nThe 50k unlabeled movie reviews help create a better text classifier for the IMDb dataset because when you fine-tune the pretrained Wikipedia language model using this data, the model learns the particular style and content of IMDb movie reviews, which helps it better understand what the language used in the reviews means when classifying it as positive or negative.\n8. What are the three steps to prepare your data for a language model?\n\nTokenization: convert the text into a list of words (or characters or substrings).\nNumericalization: List all of the words that appear (the vocab) and convert each word into a number by looking up its index in the vocab.\nLanguage model data loader creation: combine the documents into one string and split it into fixed sequence length batches while preserving the order of the tokens, create a dependent variable that is offset from the independent variable by one token, and shuffle the training data (maintaining independent/dependent variable structure).\n\n9. What is tokenization? Why do we need it?\nTokenization is the conversion of text into smaller parts (like words, subwords or characters). In order to convert our documents into numbers (categories) that the language model can learn something about, we first tokenize them (break them into smaller parts) so that we can generate a list of unique tokens (unique levels of a categorical variable) contained in the corpus (categorical variable).\n10. Name three approaches to tokenization.\n\nword-based: split a sentence based on spaces.\nsubword based: split words into commonly occurring substrings.\ncharacter-based: split a sentence into its individual characters.\n\n11. What is xxbos?\nA special token that tells the language model that we are at the start of a new stream (document).\n12. List four rules that fastai applies to text during tokenization.\nI’ll list them all:\n\nfix_html: replace special HTML characters (like &copy—the copyright symbol) with a readable version.\nreplace_rep: replace repeated characters with a special token for repetition (xxrep), the number of times it’s repeated, and then the character.\nreplace_wrep: do the same as replace_rep but for repeated words (using the special token xxwrep).\nspec_add_spaces: add spaces around / and #.\nrm_useless_spaces: remove all repetitions of the space character.\nreplace_all_caps: lowercase all-caps words and place a special token xxcap in front of it.\nreplace_maj: lowercase a capitalized word and place a special token xxmaj in front of it.\nlowercase: lowercase all text and place a special token at the beginning (xxbos) and/or at the end (xxeos).\n\n13. Why are repeated characters replaced with a token showing the number of repetitions and the character that’s repeated?\nSo that the model’s embedding matrix can encode information about general concepts such as repeated punctuation without requiring a unique token for every number of repetitions of a character.\n14. What is numericalization?\nConverting a token to a number by looking up its index in the vocab (unique list of all tokens).\n15. Why might there be words that are replaced with the “unknown word” token?\nIn order to avoid having an overly large embedding matrix, fastai’s numericalization replaces two types of words with with the unknown word token xxunk:\n\nWords that appear less than min_freq times.\nWords that are not in the max_vocab most frequent words.\n\nFor example, if min_freq = 3 then all words that appear once or twice are replaced with xxunk.\nIf max_vocab = 60000 then words the appear less frequently than the 60000th most frequent word are replaced with xxunk.\n16. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain?\nThe second row contains 64 tokens of the (n/b/s+1)th group of tokens where n is the number of tokens, divided by the number of batches b divided by the sequence length s. So, if we have 90 tokens divided into 6 batches (rows) with a sequence length (columns) of 5, then the second row of the first batch contains the 4th (i.e., 3 + 1) group of tokens.\nPutting Tanishq’s answer here as well:\n\nThe dataset is split into 64 mini-streams (batch size).\nEach batch has 64 rows (batch size) and 64 columns (sequence length).\nThe first row of the first batch contains the beginning of the first mini-stream (tokens 1-64).\nThe second row of the first batch contains the beginning of the second mini-stream.\nThe first row of the second batch contains the second chunk of the first mini-stream (tokens 65 - 128).\n\n17. Why do we need padding for text classification? Why don’t we need it for language modeling?\nWhen the data is prepared for language modeling, the documents are concatenated into a single string and broken up into equally-sized batches, so there is no need to pad any batches—they’re already the right size.\nIn the case of text classification, each document is maintained in full length in a batch, and documents will very likely have a varying number of tokens (i.e., everyone is not writing the same length of movie reviews with the same number of special tokens) so in each batch, all of the documents (except the largest) will need to be padded to the batch’s largest document’s size. fastai sorts the data by length each epoch and groups together documents of similar lengths for each batch before applying the padding.\nSomething that I would like to understand however is:\nWhat if the number of tokens in the training dataset is not divisible by the selected batch size and sequence length? Does fastai use padding in that case? Suppose you have 1000 tokens in total, a batch size of 16 and sequence length of 20. 320 goes into 1000 3 times with a remainder. Does fastai create a 4th batch with padding? Or remove the tokens so there’s only 3 batches? I’ll see if I can figure out what it does with some sample code:\n\nbs,sl = 5, 2\nints = L([[0,1,2,3,4,5,6,7,8,9,10,11,12,13]]).map(tensor)\n\n\ndl = LMDataLoader(ints, bs=bs, seq_len=sl)\n\nlist(dl)\n\n[(LMTensorText([[0, 1],\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]]),\n  tensor([[ 1,  2],\n          [ 3,  4],\n          [ 5,  6],\n          [ 7,  8],\n          [ 9, 10]]))]\n\n\n\nlist(LMDataLoader(ints, bs=bs, seq_len=sl, drop_last=False))\n\n[(LMTensorText([[0, 1],\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]]),\n  tensor([[ 1,  2],\n          [ 3,  4],\n          [ 5,  6],\n          [ 7,  8],\n          [ 9, 10]]))]\n\n\nLooks like fastai drops the last batch if it’s not full. I’ve posted this question in the fastai forums to get a confirmation on my understanding.\n18. What does an embedding matrix for NLP contain? What is its shape?\nIt contains the parameters that are trained by the neural net, with each parameter corresponding to each token in the vocab.\nFrom Tanishq’s solutions:\n\nThe embedding matrix has the size (vocab_size x embedding_size) where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens.\n\n19. What is perplexity?\nA metric used in NLP. It is the exponential of the loss.\n20. Why do we have to pass the vocabulary of the language model to the classifier data block?\nThe indexes corresponding to the tokens have to be maintained because we are fine-tuning the language model.\n21. What is gradual unfreezing?\nWhen we train one layer at a time for one epoch before we unfreeze and train the full model (including all layers of the encoder).\n22. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\nBecause text generation models can be trained to beat automatic identification algorithms.\n\n\n\n1. See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\n\nHere is a tweet thread by Arvind Narayan talking about how the danger of ChatGPT is that “you can’t tell when it’s wrong unless you already know the answer”.\nThis New York Times article walks through different examples of ChatGPT responding to prompts with disinformation.\nThis NewsGuard article, which was referenced in the NYT article, discusses how ChatGPT-4 is more prone to perpetuating misinformation than its predecessor GPT-3.5. GPT-3.5 generated 80 of 100 false narratives given as prompts while GPT-4 generated 100 of 100 false narratives. Also, “ChatGPT-4’s responses that contained false and misleading claims were less likely to include disclaimers about the falsity of those claims (23% of the time) [than ChatGPT-3.5 (51% of the time)].\nThis NBC New York article walks through an example of how a ChatGPT written story on Michael Bloomberg was full of made-up quotes and sources. It also talks about how some educators are embracing ChatGPT in the classroom, and while ineffective, there are machine-generated text identification algorithms available. Although it’s important to note, as disussed in the fastai course, that text generation models will always be ahead of automatic identification models (generative models can be trained to beat identification models).\nIn this Harvard Business School Working Knowledge article Scott Van Voorhiss and Tsedal Neeley summarise the story of how Dr. Timnit Gebru went from Ethiopia, to Boston, to a PhD at Stanford, and co-lead of Google AI Ethics, later to be fired when because she co-authored a paper that asked for companies to hold off on building large language models until we figured out how to handle the bias perpetuated by these models.\n\nThe article’s authors use these events as a case study to learn from when handling issues of ethics in AI.\n\n“The biggest message I want to convey is that AI can scale bias in ways that we can barely understand today”.\n“in failing to give Gebru the independence to do her job, might have sacrificed an opportunity to become a global leader in responsible AI development”.\nFinally, in this paper the authors test detection tools for AI-generated text in academic settings. “The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AI-generated text”. Across the 14 tools, the highest average accuracy was less than 80%, with 50% for AI-generated/human-edited text and 26% for machine-paraphrased AI-generated text.\n\n2. Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?\nThe first thing that comes to mind is Glaze by the University of Chicago which “works by understanding the AI models that are training on human art, and using machine learning algorithms, computing a set of minimal changes to artworks, such that it appears unchanged to human eyes, but appears to AI models like a dramatically different art style…So when someone then prompts the model to generate art mimicking the charcoal artist, they will get something quite different from what they expected.”\nI can’t imagine how something analogous to Glaze can be created for language, since plain text is just plain text, but conceptually, if human-written language is altered in a similar way, then it will be prevented from being generated similarly by LLMs like GPT. This would effect not just LLMs but anyone training their model on such altered data, but perhaps that is a cost worth having to prevent the perpetuation of copyrighted or disinformation content.\nAnother idea is that disinformation detection may benefit from a human-in-the-loop. AI-generated content that is not identified automatically may be identified by a human as disinformation. A big enough sample of accounts spreading this misinformation may lead to identifying broader trends in which accounts are fake."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "",
    "text": "In this notebook, I’ll work through the first “Further Research” exercise at the end of Chapter 4 of the Practical Deep Learning for Coders textbook:\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter.\n\nI’ve emphasized that this Learner implementation is basic, based on what we’ve learned in Chapter 4. I’ll call my implementation BasicLearner, as it corresponds to the BasicOptim optimizer created in the chapter. I’ll use my BasicLearner implementation to train a simple neural net on the MNIST_SAMPLE dataset."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#mnist_sample-training-loop",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#mnist_sample-training-loop",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "MNIST_SAMPLE Training Loop",
    "text": "MNIST_SAMPLE Training Loop\nI’ll start by recreating the training loop in Chapter 4 to train a simple neural net to classify the handwritten digits 3s and 7s.\n\nLoad and Prepare the Data\n\nfrom fastai.vision.all import *\n\nThe MNIST_SAMPLE dataset is available through fastai’s URLs which I download using untar_data.\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\npath.ls()\n\n(#3) [Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid'),Path('/root/.fastai/data/mnist_sample/labels.csv')]\n\n\nThen stack the list of training set and validation set tensor images into 3-dimensional tensors.\n\nstacked_threes = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'3').ls().sorted()]).float()/255\nstacked_sevens = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'7').ls().sorted()]).float()/255\nstacked_threes.shape, stacked_sevens.shape\n\n(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))\n\n\n\nshow_image(stacked_threes[0]);\n\n\n\n\n\nshow_image(stacked_sevens[0]);\n\n\n\n\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255\nvalid_3_tens.shape, valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\n\nshow_image(valid_3_tens[0]);\n\n\n\n\n\nshow_image(valid_7_tens[0]);\n\n\n\n\nWe then combine the training sets for 3s and 7s and “flatten” (not sure if that’s the right term) the tensors so that each image’s pixels are in a one-dimensional row.\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_y = tensor([1]*stacked_threes.shape[0] + [0]*stacked_sevens.shape[0]).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nThen do the same for the validation sets:\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_x.shape, valid_y.shape\n\n(torch.Size([2038, 784]), torch.Size([2038, 1]))\n\n\nWe create training and validation datasets with the same structure as PyTorch’s Dataset:\n\ndset = list(zip(train_x, train_y))\nx,y = dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([1]))\n\n\n\nvalid_dset = list(zip(valid_x, valid_y))\nx,y = valid_dset[0]\nx.shape, y\n\n(torch.Size([784]), tensor([1]))\n\n\nThen feed those datasets into fastai’s DataLoaders:\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\nvalid_xb, valid_yb = first(valid_dl)\nvalid_xb.shape, valid_yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\n\nCreate Our Model\nFor this exercise they have us create a simple neural net with a ReLU sandwiched between two linear functions. I have kept the number of intermediate activations (30) the same as the text\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n\n\nCreate a Loss Function\nThe loss function we will use does the following:\n\nPass the model’s activations through a sigmoid function so that they are between 0 and 1.\nWhen the target is 1 (the digit 3), take the difference between 1 and the activation. When the target is 0 (the digit 7), take the difference between 0 and the activation.\nTake the mean of the distance between activations and targets.\n\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\nCreate a Function to Calculate Predictions, Loss and Gradients\nThe calc_grad function takes as inputs the independent and dependent data batches, passes them through the model to get the activations (predictions), calculates the batch’s loss, and calls backward on the loss to calculate the weights’ gradients:\n\ndef calc_grad(xb, yb, model):\n  preds = model(xb)\n  loss = mnist_loss(preds, yb)\n  loss.backward()\n\n\n\nCreate an Optimizer\nThe optimizer handles the calculation to step the weights and reset the gradients. When stepping the weights, the .data attribute of the parameters is used since PyTorch doesn’t calculate gradients on it. The zero_grad method sets the gradients to 0 (None) so that they don’t accumulate additively when the next epoch’s gradients are calculated:\n\nclass BasicOptim:\n  def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n  def step(self, *args, **kwargs):\n    for p in self.params: p.data -= p.grad.data * self.lr\n\n  def zero_grad(self, *args, **kwargs):\n    for p in self.params: p.grad = None\n\n\nlr = 0.1\n\n\nopt = BasicOptim(simple_net.parameters(), lr)\n\n\n\nCreate a Function to Train One Epoch\nFor each training epoch:\n\nGet a batch from the training DataLoader.\nCalculate the activations, loss, and gradients.\nStep the weights in the direction opposite of the gradients.\nReset the gradients to zero.\n\n\ndef train_epoch(model):\n  for xb,yb in dl:\n    calc_grad(xb, yb, model)\n    opt.step()\n    opt.zero_grad()\n\n\n\nCreate a Function to Calculate a Metric for One Batch\nThe metric of choice in the chapter is accuracy, which is the mean of correctly predicted digits across the batch:\n\ndef batch_accuracy(xb, yb):\n  preds = xb.sigmoid()\n  correct = (preds>0.5) == yb\n  return correct.float().mean()\n\n\n\nCreate a Function to Calculate the Metric for One Epoch\nFor each batch in the validation DataLoader, calculate the accuracy. Then, take the mean of all batch accuracy values as the accuracy for the epoch:\n\ndef validate_epoch(model):\n  accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n  return round(torch.stack(accs).mean().item(), 4)\n\n\n\nCreate a Function for the Training Loop\ntrain_model takes a model, and number of epochs that you want to train the model for as inputs. For each epoch, it trains the model on the training set batches, and outputs the epoch’s metric on the validation set batches:\n\ndef train_model(model, epochs):\n  for i in range(epochs):\n    train_epoch(model)\n    print(validate_epoch(model), end=' ')\n\n\n\nTrain the Model\nAs is done in the text, I’ll train the model for 40 epochs.\n\ntrain_model(simple_net, 40)\n\n0.5127 0.8013 0.9175 0.9419 0.957 0.9653 0.9672 0.9677 0.9687 0.9702 0.9726 0.9736 0.9745 0.9755 0.9755 0.9765 0.977 0.9785 0.9785 0.9785 0.9795 0.9799 0.9804 0.9809 0.9809 0.9814 0.9819 0.9819 0.9819 0.9824 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 0.9829 \n\n\nI get a similar starting and final accuracy as the example from the text."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#basiclearner-class",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#basiclearner-class",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "BasicLearner Class",
    "text": "BasicLearner Class\nMy BasicLearner should recreate the training process performed in the above sections. I’ll start by defining the inputs and outputs for an instance of this class:\n\nInputs and Outputs\nThe fastai Learner requires the following inputs:\n\nDataLoaders with training and validation sets.\nThe model we want to train with.\nAn optimizer function.\nA loss function.\nAny metrics we want calculated.\n\nThe Learner outputs a table with the following information when a fit(epochs, lr) method is called. I’ve bolded the items that I’m going to show in the first iteration of my Learner:\n\nEpoch #.\nTraining Loss.\nValidation Loss.\nMetric.\nTime.\n\nWith these inputs and outputs in mind, I’ll write the BasicLearner class:\n\nclass BasicLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      self.opt.step()\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(\"Epoch\", self.metric.__name__, sep=\"\\t\")\n    for i in range(self.epochs):\n      self.train_epoch()\n      print(i, self.validate_epoch(), sep=\"\\t\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    self.train_model(self.model, self.epochs)\n\nI’ll combine my training and validation DataLoaders and confirm that they contain the correct number of tuples in their datasets:\n\ndls = DataLoaders(dl, valid_dl)\n\n\nlen(dls.train.dataset)\n\n12396\n\n\n\nlen(dls.valid.dataset)\n\n2038\n\n\nI’ll create a fresh neural net to use as a from-scratch model during training:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\nI’ll instantiate my BasicLearner class:\n\nlearn = BasicLearner(dls=dls,\n                     model=simple_net,\n                     opt_func=BasicOptim,\n                     loss_func=mnist_loss,\n                     metric=batch_accuracy)\n\nAnd train the model:\n\nlearn.fit(40, 0.1)\n\nEpoch   batch_accuracy\n0   0.5068\n1   0.814\n2   0.9184\n3   0.9419\n4   0.9575\n5   0.9648\n6   0.9663\n7   0.9677\n8   0.9692\n9   0.9707\n10  0.9736\n11  0.9736\n12  0.9741\n13  0.9755\n14  0.9765\n15  0.9775\n16  0.978\n17  0.9785\n18  0.979\n19  0.979\n20  0.979\n21  0.9795\n22  0.9795\n23  0.9804\n24  0.9804\n25  0.9809\n26  0.9814\n27  0.9819\n28  0.9819\n29  0.9819\n30  0.9814\n31  0.9819\n32  0.9819\n33  0.9824\n34  0.9824\n35  0.9829\n36  0.9829\n37  0.9829\n38  0.9829\n39  0.9829\n\n\nLooks good! I’m getting similar starting and ending accuracy values as before."
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#improving-the-basiclearner-class",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#improving-the-basiclearner-class",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "Improving the BasicLearner Class",
    "text": "Improving the BasicLearner Class\nNow that I’ve confirmed that my BasicLearner is able to train a neural net to get 98% accuracy classifying 3s and 7s, I would like to add a bit more functionality to the class.\nFirst, I’d like to add a predict method to the learner which will take as input a tensor image, and then output the prediction, so that I can test if my model has truly learned how to classify 3s and 7s.\n\nclass BasicLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      self.opt.step()\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(\"Epoch\", self.metric.__name__, sep=\"\\t\")\n    for i in range(self.epochs):\n      self.train_epoch()\n      print(i, self.validate_epoch(), sep=\"\\t\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    self.train_model(self.model, self.epochs)\n\n  def predict(self, x):\n    prediction = self.model(x)\n    prediction = prediction.sigmoid()\n    label = \"3\" if prediction > 0.5 else \"7\"\n    return prediction, label\n\nI’ll instantiate a new model and BasicLearner and train it again:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n\nlearn = BasicLearner(dls=dls,\n                     model=simple_net,\n                     opt_func=BasicOptim,\n                     loss_func=mnist_loss,\n                     metric=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\nEpoch   batch_accuracy\n0   0.5073\n1   0.8184\n2   0.9194\n3   0.9419\n4   0.957\n5   0.9638\n6   0.9658\n7   0.9672\n8   0.9697\n9   0.9706\n10  0.9726\n11  0.9741\n12  0.9741\n13  0.9755\n14  0.976\n15  0.9765\n16  0.9765\n17  0.978\n18  0.978\n19  0.978\n20  0.9795\n21  0.9795\n22  0.9799\n23  0.9809\n24  0.9809\n25  0.9814\n26  0.9814\n27  0.9814\n28  0.9819\n29  0.9814\n30  0.9814\n31  0.9824\n32  0.9829\n33  0.9829\n34  0.9829\n35  0.9829\n36  0.9824\n37  0.9824\n38  0.9824\n39  0.9824\n\n\nWith the model trained, I can see if it predicts an image of a 3 correctly:\n\nshow_image(dls.valid.dataset[1][0].view(-1,28,28));\n\n\n\n\n\nlearn.predict(dls.valid.dataset[1][0])\n\n(tensor([1.0000], grad_fn=<SigmoidBackward0>), '3')\n\n\nThe final piece that I’ll add is a “training loss” column in the fit method’s output during training. The training loss of each batch will be stored in a tensor, at the end of each epoch I’ll calculate the mean loss value, print it out, and reset the loss tensor to 0.\n\nclass BasicLearner:\n  def __init__(self, dls, model, opt_func, loss_func, metric):\n    self.dls = dls\n    self.model = model\n    self.opt_func = opt_func\n    self.loss_func = loss_func\n    self.metric = metric\n\n  def calc_grad(self, xb, yb, model):\n    preds = self.model(xb)\n    loss = self.loss_func(preds, yb)\n    # store the loss of each batch\n    # later to be averaged across the epoch\n    self.loss = torch.cat((self.loss, tensor([loss])))\n    loss.backward()\n\n  def train_epoch(self):\n    for xb,yb in self.dls.train:\n      self.calc_grad(xb, yb, self.model)\n      self.opt.step()\n      self.opt.zero_grad()\n\n  def validate_epoch(self):\n    accs = [self.metric(self.model(xb), yb) for xb,yb in self.dls.valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\n  def train_model(self, model, epochs):\n    print(\"Epoch\", \"Train Loss\", self.metric.__name__, sep=\"\\t\")\n    for i in range(self.epochs):\n      self.loss = tensor([])\n      self.train_epoch()\n      print(i, round(self.loss.mean().item(), 4), self.validate_epoch(), sep=\"\\t\\t\")\n\n  def fit(self, epochs, lr):\n    self.lr = lr\n    self.epochs = epochs\n    self.opt = self.opt_func(self.model.parameters(), self.lr)\n    self.train_model(self.model, self.epochs)\n\n  def predict(self, x):\n    prediction = self.model(x)\n    prediction = prediction.sigmoid()\n    label = \"3\" if prediction > 0.5 else \"7\"\n    return prediction, label\n\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n\nlearn = BasicLearner(dls=dls,\n                     model=simple_net,\n                     opt_func=BasicOptim,\n                     loss_func=mnist_loss,\n                     metric=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\nEpoch   Train Loss  batch_accuracy\n0       0.3627      0.5229\n1       0.1088      0.7715\n2       0.0593      0.9111\n3       0.0439      0.9389\n4       0.0375      0.9516\n5       0.0337      0.9629\n6       0.0311      0.9653\n7       0.0291      0.9667\n8       0.0275      0.9672\n9       0.0261      0.9687\n10      0.025       0.9721\n11      0.0241      0.9736\n12      0.0233      0.9746\n13      0.0225      0.9755\n14      0.0219      0.9755\n15      0.0213      0.976\n16      0.0208      0.9765\n17      0.0204      0.978\n18      0.02        0.9785\n19      0.0196      0.9785\n20      0.0193      0.979\n21      0.0189      0.979\n22      0.0186      0.979\n23      0.0184      0.9799\n24      0.0181      0.9804\n25      0.0178      0.9804\n26      0.0176      0.9804\n27      0.0174      0.9804\n28      0.0172      0.9804\n29      0.017       0.9814\n30      0.0168      0.9824\n31      0.0166      0.9824\n32      0.0164      0.9829\n33      0.0163      0.9829\n34      0.0161      0.9829\n35      0.016       0.9824\n36      0.0158      0.9829\n37      0.0157      0.9829\n38      0.0155      0.9829\n39      0.0154      0.9829\n\n\n\n# check prediction again\nlearn.predict(dls.valid.dataset[1][0])\n\n(tensor([1.0000], grad_fn=<SigmoidBackward0>), '3')"
  },
  {
    "objectID": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#further-improvements",
    "href": "posts/2023-07-24-basiclearner/2023_07_23_basiclearner.html#further-improvements",
    "title": "Implementing a fastai Learner from Scratch",
    "section": "Further Improvements",
    "text": "Further Improvements\nMy BasicLearner is able to train a neural net classifying two digits to an accuracy of 98%. During training, it prints out the epoch number, training loss and metric. It also has a predict method to test its classification on new tensor images. While I’m happy with the result of this exercise, there are certainly numerous improvements and additions that can be made to expand this learner to match the functionality of the fastai Learner class.\nI hope you enjoyed reading this blog post!"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "",
    "text": "An example image from the image dataset used in this lesson. The image has a train going over a bridge with skyscrapers in the background.\nThis chapter introduced two more classification models:\nIn this chapter the authors walk us through in the chapter is the PASCAL dataset.\nHere’s my video walkthrough for this notebook:"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#setup",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#the-data",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "The Data",
    "text": "The Data\nfastai comes with datasets available for download using the URLs object. We will use the PASCAL_2007 dataset.\n\n# download the dataset\npath = untar_data(URLs.PASCAL_2007)\n\n#read label CSV into a DataFrame\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\n  \n    \n      \n      fname\n      labels\n      is_valid\n    \n  \n  \n    \n      0\n      000005.jpg\n      chair\n      True\n    \n    \n      1\n      000007.jpg\n      car\n      True\n    \n    \n      2\n      000009.jpg\n      horse person\n      True\n    \n    \n      3\n      000012.jpg\n      car\n      False\n    \n    \n      4\n      000016.jpg\n      bicycle\n      True\n    \n  \n\n\n\n\nNext, they have us go through some pandas fundamentals for accessing data in a DataFrame\n\n# accessing all rows and the 0th column\ndf.iloc[:,0]\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# accessing all columns for the 0th row\ndf.iloc[0,:]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# trailing :s are not needed\ndf.iloc[0]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\n\n# accessing a column by its name\ndf['fname']\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\n# creating a new DataFrame and performing operations on it\ndf1 = pd.DataFrame()\n\n# adding a new column\ndf1['a'] = [1,2,3,4]\ndf1\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n    \n      3\n      4\n    \n  \n\n\n\n\n\n# adding a new column\ndf1['b'] = [10, 20, 30, 40]\n\n# adding two columns\ndf1['a'] + df1['b']\n\n0    11\n1    22\n2    33\n3    44\ndtype: int64"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#constructing-a-datablock",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Constructing a DataBlock",
    "text": "Constructing a DataBlock\nA DataBlock can be used to create Datasets from which DataLoaders can be created to use during training. A DataBlock is an object which contains the data and has helper functions which can access and transform the data.\nThey start by creating an empty DataBlock\n\ndblock = DataBlock()\ndblock\n\n<fastai.data.block.DataBlock at 0x7efe5c559d90>\n\n\nThe DataFrame with filenames and labels can be fed to the DataBlock to create a Datasets object, which is > an iterator that contains a training Dataset and validation Dataset\nEach dataset is\n\na collection that returns a tuple of your independent and dependent variable for a single item\n\nA Dataet created from an empty DataBlock (meaning, a DataBlock with no helper functions to tell it how the data is structured and accessed) will contain a tuple for each row of the DataFrame, where both values of the tuple are the same row.\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object, fname                   005618.jpg\n labels      tvmonitor chair person\n is_valid                      True\n Name: 2820, dtype: object)\n\n\nWhat we want is for the DataBlock to create Datasets of (independent, dependent) values. In this case, the independent variable is the image and the dependent variable is a list of labels.\nIn order to parse the DataFrame rows, we need to provide two helper functions to the DataBlock: get_x and get_y. In ordert to convert them to the appropriate objects that will be used in training, we need to provide two more arguments: ImageBlock and MultiCategoryBlock. In order for the DataBlock to correctly split the data into training and validation datasets, we need to define a splitter function and pass it as an argument as well.\nget_x will access the filename from each row of the DataFrame and convert it to a file path.\nget_y will access the labels from each row and split them into a list.\nImageBlock will take the file path and convert it to a PILImage object.\nMultiCategoryBlock will convert the list of labels to a one-hot encoded tensor using the Dataset’s vocab.\nsplitter will explicitly choose for the validation set the rows where is_valid is True.\nRandomResizedCrop will ensure that each image is the same size, which is a requirement for creating a tensor with all images.\n\ndef get_x(row): return path/'train'/row['fname']\ndef get_y(row): return row['labels'].split(' ')\ndef splitter(df):\n  train = df.index[~df['is_valid']].tolist()\n  valid = df.index[df['is_valid']].tolist()\n  return train, valid\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    splitter=splitter,\n    get_x=get_x,\n    get_y=get_y,\n    item_tfms = RandomResizedCrop(128, min_scale=0.35))\n\ndsets = dblock.datasets(df)\ndls = dblock.dataloaders(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0.]))\n\n\nThe Datasets vocab is a list of alphabetically ordered unique labels:\n\ndsets.train.vocab\n\n['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\n\nLet me breakdown the tuple returned by dsets.train[0]. The first value is a PILImageobject which can be viewed by calling its show() method:\n\ndsets.train[0][0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7efe5c3764d0>\n\n\n\n\n\nThe second value is a one-hot encoded list, where 1s are in the location of the labels in the corresponding vocab list. I’ll use the torch.where method to access the indices where there are 1s:\n\ntorch.where(dsets.train[0][1]==1)\n\n(TensorMultiCategory([6]),)\n\n\n\ndsets.train.vocab[torch.where(dsets.train[0][1]==1)[0]]\n\n(#1) ['car']\n\n\n\ndls.show_batch(nrows=1, ncols=3)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-4-two-digit-mnist-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 4: Two-Digit MNIST Classifier",
    "text": "Chapter 4: Two-Digit MNIST Classifier\nI’ll first review the loss function used in the single-label classification models created in Chapters 4 and 5 before reviewing Binary Cross Entropy Loss introduced in this chapter.\nIn this chapter, we built a image classifier which would predict if an input image was an of the digit 3 or the digit 7.\nThe target (or expected outcome) is a list of 0s (for 7) and 1s (for 3). If we gave a batch of images of a 3, a 7 and a 3, the target would be [1, 0, 1].\nSuppose the model predicted the following values: [0.9, 0.4, 0.2] where each value represented the probability or confidence it had that each image was a 3.\nLoss represents the positive difference between the target and the prediction: - 1 - prediction when target == 1 - prediction when target == 0\nFor the first image, the model had 90% confidence it was a 3, and it was indeed a 3. The loss is 1 - 0.9 = 0.1.\nFor the second second image, the model had a 40% confidence it was a three, and the image was of a 7. The loss is 0.4.\nFor the last image, the model had a 20% confidence it was a 3, and the image was a 3. The loss is 1 - 0.2 = 0.8.\nThe average of these three losses is 1.3/3 or 0.433.\nThe following cell illustrates this with code:\n\ndef mnist_loss(predictions, targets):\n  return torch.where(targets==1, 1-predictions, predictions).mean()\n\ntargets = tensor([1,0,1])\npredictions = tensor([0.9, 0.4, 0.2])\nmnist_loss(predictions=predictions, targets=targets)\n\ntensor(0.4333)\n\n\nThe assumption that this loss function makes is that the predictions are always between 0 and 1. That may not always be true! In order to make this assumption explicit, we take the sigmoid of the prediction before calculating the loss. The sigmoid function outputs a value between 0 and 1 for any input value.\n\ntensor([0.4,-100,200]).sigmoid()\n\ntensor([0.5987, 0.0000, 1.0000])\n\n\n\ndef mnist_loss(predictions, targets):\n  predictions = predictions.sigmoid()\n  return torch.where(targets==1, 1-predictions, predictions).mean()"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#chapter-5-37-breed-pet-classifier",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Chapter 5: 37 Breed Pet Classifier",
    "text": "Chapter 5: 37 Breed Pet Classifier\nIn this chapter, we train an image classifier that when given an input image, predicts which of the 37 pet breeds the image shows. The loss function needs to handle 37 activations for each image. In order to ensure the sum of those activations equals 1.0—so that the highest activation represents the model’s highest confidence—the softmax function is used. In order to increase the separation between probabilities, the softmax function’s output is passed through the logarithm function, and the negative value is taken. The combination of softmax and (negative) logarithm is called cross entropy loss.\nSuppose we had 4 images in a batch. The model would output activations something like this:\n\n# create a pseudo-random 4 x 37 tensor \n# with values from -2 to 2\nacts = (-2 - 2) * torch.rand(4, 37) + 2\nacts\n\ntensor([[-1.9994e+00,  7.0629e-01, -1.8230e+00,  8.6118e-02,  8.8579e-01,\n         -9.7763e-01,  9.7619e-01,  5.4613e-01,  9.2020e-01,  8.2653e-01,\n         -1.3831e+00,  1.2236e+00, -4.2582e-01,  1.1371e+00,  1.2409e+00,\n          1.4403e-02, -9.2988e-01, -1.1939e+00, -9.9743e-01, -1.9572e+00,\n         -6.8404e-02,  6.2455e-01,  8.6748e-01, -1.4574e+00, -1.4451e+00,\n          1.1349e-01,  1.7424e+00,  6.5414e-02, -1.2517e+00, -1.9933e+00,\n         -1.5570e+00,  1.3880e+00,  1.5099e+00,  6.2576e-01, -1.4279e-03,\n          1.7448e+00,  1.9862e+00],\n        [ 4.5219e-02,  4.6843e-01, -1.1474e+00, -1.8876e+00, -5.7879e-01,\n          6.9787e-01, -7.2457e-02, -1.7235e+00, -9.9028e-01,  1.2248e+00,\n          6.4889e-01,  5.0363e-01,  1.8472e-01, -1.0468e+00, -1.0113e+00,\n         -1.0628e+00,  1.9783e+00, -1.8394e+00, -8.0410e-02, -5.9383e-01,\n         -1.6868e+00, -2.6366e-01, -8.3354e-01,  6.8552e-01, -8.6600e-02,\n          1.6034e+00,  7.3355e-01,  1.3205e+00,  1.4004e+00, -5.2889e-01,\n          5.6740e-01, -9.6958e-01, -1.4997e+00,  4.6890e-01, -1.7328e+00,\n          1.0302e+00, -5.7672e-01],\n        [-2.0183e-01,  9.5745e-01, -6.7022e-01, -1.4942e+00, -1.7716e+00,\n         -1.5369e+00,  5.3614e-01,  2.1942e-01, -4.8692e-01, -1.0483e+00,\n         -1.3250e+00, -2.7229e-01,  7.0113e-01,  6.7435e-01,  1.3605e+00,\n         -5.5024e-01, -8.2829e-01, -3.0993e-01, -2.9132e-02, -6.5741e-01,\n         -1.8838e+00, -1.5611e+00,  1.3386e+00, -9.3677e-01,  9.4050e-01,\n          1.6461e+00, -1.7923e+00, -1.2952e+00, -1.4606e+00,  1.9617e+00,\n          1.8974e+00, -3.5640e-01, -5.1258e-01,  1.3049e+00,  9.6022e-01,\n          1.8340e+00, -1.6090e+00],\n        [ 3.3658e-01, -1.9117e+00,  1.3840e+00,  1.4359e+00,  3.0289e-01,\n         -1.9664e+00, -1.8941e+00,  4.2836e-02,  1.6804e+00,  1.5752e+00,\n         -4.4672e-01,  1.0409e+00, -2.8504e-01, -1.3567e+00,  3.1620e-01,\n         -1.9444e+00,  1.5615e+00, -5.0563e-01, -1.8748e+00, -1.1123e+00,\n         -1.9222e+00,  1.3545e+00, -2.9159e-01, -4.6669e-01,  1.2639e+00,\n         -1.4171e+00, -2.7517e-01, -1.2380e+00, -1.5908e+00,  1.4929e+00,\n          1.0642e+00, -3.4285e-01, -1.8219e+00,  1.6329e+00, -1.2953e+00,\n          1.7803e+00,  3.6970e-01]])\n\n\nPassing these through softmax will normalize them from 0 to 1:\n\nsm_acts = acts.softmax(dim=1)\nsm_acts[0], sm_acts[0].sum()\n\n(tensor([0.0020, 0.0302, 0.0024, 0.0162, 0.0361, 0.0056, 0.0396, 0.0257, 0.0374,\n         0.0341, 0.0037, 0.0507, 0.0097, 0.0465, 0.0516, 0.0151, 0.0059, 0.0045,\n         0.0055, 0.0021, 0.0139, 0.0278, 0.0355, 0.0035, 0.0035, 0.0167, 0.0851,\n         0.0159, 0.0043, 0.0020, 0.0031, 0.0597, 0.0675, 0.0279, 0.0149, 0.0853,\n         0.1086]), tensor(1.0000))\n\n\nTaking the negative log of this tensor will give us the final loss:\n\nnll_loss = -1. * torch.log(sm_acts)\nnll_loss\n\ntensor([[6.2054, 3.4997, 6.0290, 4.1199, 3.3202, 5.1836, 3.2298, 3.6599, 3.2858,\n         3.3795, 5.5891, 2.9825, 4.6318, 3.0690, 2.9651, 4.1916, 5.1359, 5.3999,\n         5.2035, 6.1632, 4.2744, 3.5815, 3.3385, 5.6635, 5.6511, 4.0925, 2.4636,\n         4.1406, 5.4577, 6.1994, 5.7630, 2.8180, 2.6961, 3.5803, 4.2074, 2.4612,\n         2.2198],\n        [3.9156, 3.4924, 5.1082, 5.8484, 4.5396, 3.2629, 4.0333, 5.6843, 4.9511,\n         2.7360, 3.3119, 3.4572, 3.7761, 5.0076, 4.9721, 5.0235, 1.9825, 5.8002,\n         4.0412, 4.5546, 5.6476, 4.2245, 4.7943, 3.2753, 4.0474, 2.3574, 3.2273,\n         2.6403, 2.5604, 4.4897, 3.3934, 4.9304, 5.4605, 3.4919, 5.6936, 2.9306,\n         4.5375],\n        [4.3197, 3.1604, 4.7881, 5.6121, 5.8895, 5.6548, 3.5817, 3.8985, 4.6048,\n         5.1662, 5.4429, 4.3902, 3.4167, 3.4435, 2.7574, 4.6681, 4.9462, 4.4278,\n         4.1470, 4.7753, 6.0016, 5.6790, 2.7793, 5.0546, 3.1774, 2.4718, 5.9102,\n         5.4131, 5.5785, 2.1562, 2.2205, 4.4743, 4.6305, 2.8130, 3.1577, 2.2839,\n         5.7269],\n        [3.8515, 6.0998, 2.8041, 2.7522, 3.8852, 6.1545, 6.0822, 4.1453, 2.5077,\n         2.6129, 4.6348, 3.1472, 4.4732, 5.5448, 3.8719, 6.1325, 2.6266, 4.6937,\n         6.0629, 5.3004, 6.1103, 2.8336, 4.4797, 4.6548, 2.9243, 5.6052, 4.4633,\n         5.4261, 5.7790, 2.6952, 3.1239, 4.5310, 6.0101, 2.5552, 5.4834, 2.4078,\n         3.8184]])\n\n\nSuppose the target for each image was given by the following tensor, where the target is an integer from 0 to 36 representing one of the pet breeds:\n\ntargs = tensor([3, 0, 34, 10])\nidx = range(4)\nnll_loss[idx, targs]\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\ndef cross_entropy(acts, targs):\n  idx = range(len(targs))\n  sm_acts = acts.softmax(dim=1)\n  nll_loss = -1. * torch.log(sm_acts)\n  return nll_loss[idx, targs].mean()\n\nI compare this with the built-in F.cross_entropy and nn.CrossEntropyLoss functions:\n\nF.cross_entropy(acts, targs,reduction='none')\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targs)\n\ntensor([4.1199, 3.9156, 3.1577, 4.6348])\n\n\nNote that the nn version of the loss function returns an instantiation of that function which then must be called with the activations and targets as its inputs.\n\ntype(nn.CrossEntropyLoss(reduction='none'))\n\ntorch.nn.modules.loss.CrossEntropyLoss"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#binary-cross-entropy-loss",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Binary Cross Entropy Loss",
    "text": "Binary Cross Entropy Loss\nThe authors begin the discussion of explaining the multi-label classification model loss function by observing the activations from the trained model. I’ll do the same—I love that approach since it grounds the concepts involved in the construction of loss function in the actual model outputs.\n\nlearn = cnn_learner(dls, resnet18)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\nx, y = dls.train.one_batch()\nif torch.cuda.is_available():\n    learn.model.cuda()\nactivs = learn.model(x)\nactivs.shape\n\ntorch.Size([64, 20])\n\n\nEach batch has 64 images and each of those images has 20 activations, one for each label in .vocab. Currently, they are not restricted to values between 0 and 1.\nNote: the activations tensor has to first be placed on the cpu and then detached from the graph (which is used to track and calculate gradients of the weights with respect to the loss function) before it can be converted to a numpy array used for the plot.\n\nys = activs[0].cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nPassing them through a sigmoid function achieves that:\n\nys = activs[0].sigmoid().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe negative log of the activations is taken in order to push the differences between loss values. For vocab where the target is 1, -log(inputs) is calculated. For vocab where the target is 0, -log(1-inputs) is calculated. This seems counterintuitive at first, but let’s take a look at the plot of these functions:\n\nys = -activs[0].sigmoid().log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nThe sigmoid activations that were very close to 0 (Vocab Index = 0, 2, 5, and 16) are now much larger than those that were very close to 1 (Vocab Index = 6, 14, and 18). Since the target is 1, this correctly assigns a larger loss to the inaccurate predictions and the smaller loss to the accurate ones. We can say the same (but opposite) for -log(1-inputs), which is used when the target is 0.:\n\nys = -(1- activs[0].sigmoid()).log().cpu().detach().numpy()\n\nplt.xlabel(\"Vocab Index\")\nplt.ylabel(\"Activation\")\nplt.xticks(np.arange(20), np.arange(20))\nplt.bar(range(20), ys)\n\n<BarContainer object of 20 artists>\n\n\n\n\n\nFinally, the mean of all image loss values is taken for the batch. The Binary Cross Entropy Function look likes this:\n\ndef binary_cross_entropy(inputs, targets):\n  inputs = inputs.sigmoid()\n  return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\nThe inputs (the activations for each vocab value)) is the first value and the targets of each image are the second value of the dls.train.one_batch() tuple.\n\nbinary_cross_entropy(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\nI will compare this with the built-in function F.binary_cross_entropy_with_logits and function class nn.BCEWithLogitsLoss to make sure I receive the same result.\n\nF.binary_cross_entropy_with_logits(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)\n\n\n\nnn.BCEWithLogitsLoss()(activs,y)\n\nTensorMultiCategory(1.0472, device='cuda:0', grad_fn=<AliasBackward>)"
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#mult-label-classification-accuracy",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Mult-Label Classification Accuracy",
    "text": "Mult-Label Classification Accuracy\nFor single-label classification, the accuracy function compared whether the index of the highest activation matched the index of the target vocab. A single index for a single label.\n\ndef accuracy(inputs, targets, axis=-1):\n  predictions = inputs.argmax(dim=axis)\n  return (predictions==targets).float().mean()\n\nFor multi-label classification, each image can have more than one correct corresponding vocab index and the corresponding activations may not be the maximum of the inputs tensor. So instead of using the maximum, a threshold is used to identify predictions. If the activation is above that threshold, it’s considered to be a prediction.\n\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n  if sigmoid: inp = inp.sigmoid()\n  return ((inp > thresh)==targ.bool()).float().mean()\n\ntarg is a one-hot encoded Tensor, so 1s are converted to True and 0s are converted to False using the .bool method."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#training-the-model",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Training the Model",
    "text": "Training the Model\nAt last! I can now train the model, setting a different accuracy threshold as needed using the built-in partial function.\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.942256\n      0.698276\n      0.239323\n      00:29\n    \n    \n      1\n      0.821279\n      0.566598\n      0.281633\n      00:28\n    \n    \n      2\n      0.602543\n      0.208145\n      0.805498\n      00:28\n    \n    \n      3\n      0.359614\n      0.125162\n      0.939801\n      00:28\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.133149\n      0.112483\n      0.947072\n      00:29\n    \n    \n      1\n      0.115643\n      0.105032\n      0.953028\n      00:29\n    \n    \n      2\n      0.096643\n      0.103564\n      0.952769\n      00:29\n    \n  \n\n\n\nIn about three and a half minutes, this model was able to achieve more than 95% accuracy. I’ll look at its predictions on the validation images:\n\nlearn.show_results(max_n=18)\n\n\n\n\n\n\n\nVarying the threshold will vary the accuracy of the model. The metrics of the learner can be changed after training, and calling the validate method will recalculate the accuracy:\n\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.930816650390625]\n\n\nA threshold of 0.1 decreases the accuracy of the model, as does a threshold of 0.99. A 0.1 threshold includes labels for which the model was not confident, and a 0.99 threshold exclused labels for which the model was not very confident. I can calculate and plot the accuracy for a range of thresholds, as they did in the book:\n\npreds, targs = learn.get_preds()\nxs = torch.linspace(0.05, 0.95, 29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\n\n\n\n\nbest_threshold = xs[np.argmax(accs)]\nbest_threshold\n\ntensor(0.4679)\n\n\n\nlearn.metrics = partial(accuracy_multi, thresh=best_threshold)\nlearn.validate()\n\n\n\n\n(#2) [0.1035640612244606,0.9636053442955017]\n\n\nThe highest accuracy (96.36%) is achieved when the threshold is 0.4679."
  },
  {
    "objectID": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "href": "posts/2021-04-12-fastai-chapter-6/2021-04-12-fastai-chapter-6.html#regression",
    "title": "fast.ai Chapter 6: Classification Models",
    "section": "Regression",
    "text": "Regression\nThe authors provide some context here which, while I can appreciate, judge I won’t fully understand until I experience the next 5 or 6 chapters.\n\nA model is defined by its independent and dependent variables, along with its loss function. The means that there’s really a far wider array of models than just the simple domain-based split\n\nThe “domain-based split” is a reference to the distinction between computer vision, NLP and other different types of problems.\nTo illustrate their point, they have us work through an image regression problem with much of the same process (and model) as an image classification problem.\n\n# download data\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n\n\n\n# helper functions to retrieve images\n# and to retrieve text files\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\n\n\n# check that `img2pose` converts file name correctly\nimg_files[0], img2pose(img_files[0])\n\n(Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_rgb.jpg'),\n Path('/root/.fastai/data/biwi_head_pose/03/frame_00457_pose.txt'))\n\n\n\n# check image size\nim = PILImage.create(img_files[0])\nim.shape\n\n(480, 640)\n\n\n\n# view the image\nim.to_thumb(160)\n\n\n\n\n\n# helper function to extract coordinates\n# of the subject's center of head\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n  ctr = np.genfromtxt(img2pose(f), skip_header=3)\n  c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n  c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n  return tensor([c1,c2])\n\n\n# check coordinates of the first file\nget_ctr(img_files[0])\n\ntensor([444.7946, 261.7657])\n\n\n\n# create the DataBlock\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)]\n)\n\n\n# confirm that the data looks OK\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n# view tensors\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\n\nEach batch has 64 images. Each image has 3 channels (rgb) and is 240x320 pixels in size. Each image has 1 pair of coordinates.\n\n# view a single coordinate pair\nyb[0]\n\nTensorPoint([[0.0170, 0.3403]], device='cuda:0')\n\n\n\n# create Learner object\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\nThe y_range argument shifts the final layer’s sigmoid output to a coordinate between -1 and 1. The sigmoid function is transformed using the following function.\n\ndef plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = torch.linspace(min,max)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)\n\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\nplot_function(partial(sigmoid_range, lo=-1, hi=1), min=-4, max=4)\n\n\n\n\n\n# confirm loss function\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nfastai has chosen MSE as the loss function, which is appropriate for a regression problem.\n\n# pick a learning rate\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.033113110810518265)\n\n\n\n\n\n\n# use lr = 2e-2\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.047852\n      0.011552\n      01:55\n    \n    \n      1\n      0.007220\n      0.002150\n      01:56\n    \n    \n      2\n      0.003190\n      0.001313\n      01:56\n    \n    \n      3\n      0.002376\n      0.000295\n      01:56\n    \n    \n      4\n      0.001650\n      0.000106\n      01:54\n    \n  \n\n\n\nA loss of 0.000106 is an accuracy of:\n\nmath.sqrt(0.000106)\n\n0.010295630140987\n\n\nThe conclusion to this (what has felt like a marathon of a) chapter is profound:\n\nIn problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different number of outputs. The loss function is the one thing that changes, which is why it’s important to double-check that you are using the right loss function for your problem…make sure you think hard about your loss function, and remember that you most probably want the following:\n\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "",
    "text": "A chloropleth map of Minnesota Census data\nIn this blog post, I’ll walk through my process of creating an ArcGIS geodatabase and a set of layouts visualizing U.S. Census Data. The data used for this app is from table B20005 (Sex By Work Experience In The Past 12 Months By Earnings In The Past 12 Months).\nYou can view the final layout PDFs at the following links:"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#table-of-contents",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nGet the Data\n\nTract Boundaries\nACS 5-Year Estimates\nUsing data.census.gov\nUsing the censusapi R package\n\nConnect Data to Geodatabase\n\nTract Boundaries\nACS 5-Year Estimates\n\nVisualize Data\n\nCreate a Map\nCreate a Symbology\nCreate a Layout\n\nNormalize the Data\n\nCreate Additional Layouts"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#get-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Get the Data",
    "text": "Get the Data\n\nTract Boundaries\n\nDownload and unzip 2019 TIGER Shapefile for MN (tl_2019_27_tract.zip) (corresponds to the final year, 2019, in the ACS 5-year estimates). These will contain the Census Tract geographies needed to create a map in ArcGIS.\n\n\n\nACS 5-Year Estimates\n\nUsing data.census.gov\n\nOn data.census.gov, search for B20005\n\n\n\nSelect the link to the Table B20005 with “2019 inflation-adjusted dollars”\n\n\n\nClick the dropdown at the top next to the label Product and select 2015: ACS 5-Year Estimates Detailed Tables\n\n\n\nClick Customize Table at the top right of the page\n\n\n\nIn the Geo* section, click Tract > Minnesota > All Census Tracts within Minnesota\n\n\n\nOnce it’s finished loading, click Close and then Download Table\n\n\n\nOnce downloaded, extract the zip folder and open the file ACSDT52015.B20005_data_with_overlays….xslx_ in Excel any tool that can handle tabular data\nSlice the last 11 characters of the GEO_ID (using the RIGHT function in a new column) to replace the existing GEO_ID column values. For example, a GEO_ID of 1400000US27029000100 should be replaced with 27029000100. This will later on be matched with the GEOID field in the tl_2019_27_tract shapefile\nSave/export the file as .XLSX\n\n\n\nUsing the censusapi R package\nPass the following arguments to the censusapi::listCensusMetadata function and assign its return value to B20005_vars:\n\nB20005_vars <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  type=\"variables\",\n  group=\"B20005\"\n)\n\n\nPass the following arguments to censusapi::getCensus and assign its return value to B20005:\n\n\nB20005 <- censusapi::listCensusMetadata(\n  name=\"acs/acs5\",\n  vintage=\"2015\",\n  region=\"tract:*\",\n  regionin=\"state:27\", # 27 = Minnesota state FIPS code\n  vars=c(\"GEO_ID\", \"NAME\", B20005_vars$name)\n)\n\n\nReplace GEO_ID (or create a new column) with the last 11 characters\n\n\nB20005 <- substr(B20005$GEO_ID, 10, 20)\n\n\nExport to an .XLSX file\n\n\nwrite.xlsx(B20005, “acs5_b20005_minnesota.xlsx”, row.names = FALSE)"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#connect-data-to-geodatabase",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Connect Data to Geodatabase",
    "text": "Connect Data to Geodatabase\nOpen ArcGIS Pro and start a new project.\n\nTract Boundaries\n\nRight click Folders in the Contents pane and click Add folder connection\n\n\n\nSelect the downloaded (and extracted) tl_2019_27_tract folder and click OK\n\n\n\nClick on tl_2019_27_tract folder in the Contents pane\nIn the Catalog pane, right-click tl_2019_27.shp and then click Export > Feature Class to Geodatabase\n\n\n\nConfirm Input Features (tl_2019_27_tract.shp) and Output Geodatabase (Default.gdb or whatever geodatabase you are connected to) and then click the green Run button\nRefresh the Geodatabase and click on it in the Contents pane to view the added shapefile\n\n\n\n\nACS 5-Year Estimates\n\nUnder the View ribbon click on Geoprocessing to open that pane\nIn the Geoprocessing pane, search for Join Field and click on it\n\n\n\nNext to Input Table click on the folder icon to Browse. Select the tl_2019_27_tract table in your geodatabase\n\n\n\nClick the Input Join Field dropdown and select GEOID\nNext to Join Table click on the folder icon to Browse. Select the acs5_b20005_minnesota$ Excel table and click OK (note: the Excel table is inside the XLSX file)\n\n\n\nType GEO_ID under Join Table Field\nClick on the down arrow next to Transfer Fields and select B20005_002E, B20005_003E, B20005_049E, and B20005_050E\n\n\n\nClick on Validate Join\n\n\n\nClick on Run\nA success message should be displayed at the bottom of the Geoprocessing pane"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#visualize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Visualize the Data",
    "text": "Visualize the Data\nIn this section, I’ll create maps and layouts to visualize the population estimates using Census Tract spatial data.\n\nCreate a Map\n\nIn the Catalog pane, right-click tl_2019_27_tract > Add to New > Map\n\n\n\nTo reference the raw data: from the Feature Layer ribbon, click Attribute Table\n\n\n\n\n\nCreate a Symbology\n\nSelect the tl_2019_27_tract layer in Contents pane\nClick Appearance under the Feature Layer ribbon\nClick the down arrow on Symbology and select Graduated Colors\n\n\n\nSelect B20005_002E in the Field dropdown and Natural Breaks (Jenks) for the Method\n\n\n\nThe class breaks created by this method do not reliably classify the data, which is determined using the City of New York Department of Planning Map Reliability Calculator. There’s a 10.1% chance that a tract is erroneously classified.\n\n\n\nAfter adjusting the class breaks, the following result in a reliable result (less than 10% chance of misclassifying any geography on the map and less than 20% of misclassifying estimates within a class due to sampling error)\n\n\n\nApply these breaks in the Classes tab in the Symbology pane\n\n\n\nThe Map pane displays the updated choropleth\n\n\n\n\nCreate a Layout\nUnder the Insert ribbon click on New Layout and Letter (8.5” x 11”)\n\n\n\nOn the Insert ribbon, click Map Frame and Default Extent under the Map category\n\n\n\nClick and drag the cursor to draw the Map Frame. Under the Layout ribbon select Activate and zoom/pan until the full choropleth is visible. Click Deactivate when you’re finished.\n\n\n\nAdd guides to create 0.5 inch margins by right-clicking on rulers clicking Add Guide\n\n\n\nUnder the Insert ribbon click on Legend and draw a rectangle underneath the map\n\n\n\nRight-click the legend and click Properties to format the font size, text visibility (under Legend Item in the dropdown next to Legend in the Format Legend panel) and more\n\n\n\nOn the Ribbon tab in the Graphics and Text panel, you can choose different text types to add text to your layout. I’ve added titles and explanatory text.\n\n\n\nThe census tracts for the city of Minneapolis are too small to be clearly visible. Under the Insert ribbon click Map Frame, select the map and draw a small rectangle over Wisconsin.\nWith the new Map Frame selected, click Reshape > Circle under the Insert ribbon. Draw a circle over the rectangular map.\n\n\n\nRight-click on the circular map and click Properties to add a border. Add a textbox to label it as the City of Minneapolis.\n\n\n\nFrom the Graphics and Text panel on the Insert ribbon use the straight line and circle tool to add some visual cues indicating that the map frame is a detail view of the city\n\n\n\nUnder the Share ribbon, select Export Layout and export it to a PDF file"
  },
  {
    "objectID": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "href": "posts/2021-09-26-arcgis-census/2021-09-26-arcgis-census.html#normalize-the-data",
    "title": "Visualize U.S. Census Data with ArcGIS",
    "section": "Normalize the Data",
    "text": "Normalize the Data\nWhile the worker population estimates gives us a sense of how workers are distributed across the state, they are a proxy for population density. Census Tracts in Urban areas, like the Minneapolis, will likely have more workers than Rural areas, because they have a higher population. To supplement this layout, I’ll create layouts that show the percentage of the total sex population who are full time workers.\n\nTo duplicate the Male Full TIme Estimates layout, right-click it in the Catalog pane, click Copy and then right click in the gray area underneath it and click Paste\n\n\n\n\nRename the layout to Male Full Time Percentages and open it\nRename the two maps in the Contents pane\n\n\n\nRight-click tl_2019_27_tract under Main Map and click Symbology to open the Symbology pane\n\n\n\nSelect B20005_002E (Total Male Estimate) in the Normalization dropdown. This will be the value that divides a Census Tract’s population estimate\n\n\n\nCalculate the Margin of Error (MOE) for the percentage of total male workers who are full time employed using equation 6 from the “Calculating Measures of Error for Derived Estimates” in the Understanding and Using American Community Survey Data: What All Data Users Need to Know handbook in order to determine the class break reliability. In the equation below, P = X/Y is the percentage of full time workers in the tract (X= B20005_003E and Y = B20005_002E)\n\n\n\nOne reliable set of class breaks, which were few and far between, was the following:\n\n\n\nApply those class breaks in the Symbology pane and update the text to match\n\n\n\nCreate Additional Layouts\n\nRepeat the process to create the following Layouts given the following class breaks\n\nFemale Full Time Estimates\n\n\n\n\nFemale Full Time Percentages\n\n\n\nI hope you enjoyed this tutorial."
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "",
    "text": "Here’s a video walkthrough of this notebook:"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#introduction",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, I work through the first of four “Further Research” problems assigned at the end of Chapter 7 in the textbook “Deep Learning for Coders with fastai and PyTorch”.\nThe prompt for this exercise is:\n\nUse the fastai documentation to build a function that crops an image to a square in each of the four corners; then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai?"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#what-is-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "What is Test Time Augmentation?",
    "text": "What is Test Time Augmentation?\nI’ll quote directly from the text:\n\nDuring inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n\nTTA is data augmentation during validation, in hopes that objects located outside the center of the image (which is the default fastai validation image crop) can be recognized by the model in order to increase the model’s accuracy.\nThe default Learner.tta method averages the predictions on the center crop and four randomly generated crops. The method I’ll create will average the predictions between the center crop and four corner crops.\n\n\n\ntta.png"
  },
  {
    "objectID": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "href": "posts/2021-05-29-chapter-07-tta/2021-05-29-chapter-07-tta.html#user-defined-test-time-augmentation",
    "title": "fast.ai Chapter 7:Test Time Augmentation",
    "section": "User-defined Test Time Augmentation",
    "text": "User-defined Test Time Augmentation\n\nRead and understand the Learner.tta and RandomCrop source code\ndef tta(self:Learner, ds_idx=1, dl=None, n=4, item_tfms=None, batch_tfms=None, beta=0.25, use_max=False):\n    \"Return predictions on the `ds_idx` dataset or `dl` using Test Time Augmentation\"\n    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n    if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n    try:\n        self(_before_epoch)\n        with dl.dataset.set_split_idx(0), self.no_mbar():\n            if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n            aug_preds = []\n            for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n        aug_preds = torch.cat(aug_preds)\n        aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n        self.epoch = n\n        with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n    finally: self(event.after_fit)\n\n    if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n    preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n    return preds,targs\nclass RandomCrop(RandTransform):\n    \"Randomly crop an image to `size`\"\n    split_idx,order = None,1\n    def __init__(self, size, **kwargs):\n        size = _process_sz(size)\n        store_attr()\n        super().__init__(**kwargs)\n\n    def before_call(self, b, split_idx):\n        self.orig_sz = _get_sz(b)\n        if split_idx: self.tl = (self.orig_sz-self.size)//2\n        else:\n            wd = self.orig_sz[0] - self.size[0]\n            hd = self.orig_sz[1] - self.size[1]\n            w_rand = (wd, -1) if wd < 0 else (0, wd)\n            h_rand = (hd, -1) if hd < 0 else (0, hd)\n            self.tl = fastuple(random.randint(*w_rand), random.randint(*h_rand))\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n        return x.crop_pad(self.size, self.tl, orig_sz=self.orig_sz)\n\n\nPractice cropping images using the .crop method on a PILImage object\nA PIL Image has a method called crop which takes a crop rectangle tuple, (left, upper, right, lower) and crops the image within those pixel bounds.\nHere’s an image with a grizzly bear at the top and a black bear on the bottom. There are four coordinates of interest: left, upper, right and bottom. The leftmost points on the image are assigned a pixel value of 0. The rightmost points are located at the image width pixel pixel value. The uppermost points are at pixel 0, and the bottommost points are at the image height pixel value.\n\nf = \"/content/gdrive/MyDrive/fastai-course-v4/images/test/grizzly_black.png\"\nimg = PILImage.create(f)\nimg.to_thumb(320)\n\n\n\n\n\nTop-Left Corner Crop\nA top-left corner crop the corresponds to a left pixel of 0, upper pixel 0, right pixel of 224, and bottom pixel of 224. The order in the tuple is left, upper, right, bottom, so 0, 0, 224, 224. You can see that this crop is taken from the top left corner of the original image.\n\nimg.crop((0,0,224,224))\n\n\n\n\n\n\nTop Right Corner Crop\nFor the top right corner, I get the image width since the left end of the crop will be 224 pixels from the right end of the image. That translates to w-224. The upper pixel is 0, and the rightmost pixel is at w, and the bottom pixel is 224. You can see that this crop is at the top right corner of the original.\n\nw = img.width\nh = img.height\nimg.crop((w-224, 0, w, 224))\n\n\n\n\n\n\nBottom Right Corner Crop\nFor the bottom right corner the left pixel is 224 from the right end, w-224, the upper pixel is 224 from the bottom, h-224, the right pixel is at w, and the bottom is at h.\n\nimg.crop((w-224, h-224, w, h))\n\n\n\n\n\n\nBottom Left Corner Crop\nThe bottom left corner’s leftmost pixel is 0, uppermost pixel is 224 pixels from the bottom of the whole image, h - 224, the rightmost pixel is 224, and bottommost pixel is the bottom of the whole image, at h.\n\nimg.crop((0, h-224, 224, h))\n\n\n\n\n\n\nCenter Crop\nFinally, for the center crop, the leftmost pixel is 112 left of the image center, w/2 - 112, the upper pixel is 112 above the image center, h/2 - 112, the rightmost pixel is 112 right of the center, w/2 + 112, and the bottom pixel is 112 below the center, h/2 + 112.\n\nimg.crop((w/2-112, h/2-112, w/2+112,h/2+112))\n\n\n\nSummary\nTo better visualize this, here are a couple of images which show the left, upper, right and bottom coordinates for the corner and center crops.\nSummary of corner crop arguments (left, upper, right, bottom)\n\n\n\ncrop_dimensions-01.png\n\n\nSummary of center crop arguments (left, upper, right, bottom)\n\n\n\ncenter_crop_dimensions-01.png\n\n\n\n\n\nDefine a function which takes an image and returns a stacked Tensor with four corner crops and a center crop\nI wrap those five lines of code into a function called corner_crop, which takes a PILImage img, and a square side length size (defaulted to 224) as its arguments. It first grabs the width and height of the image. And then goes on to save the crops of the four corners and center as TensorImages, returning them all in a single stacked Tensor.\n\ndef corner_crop(img, size=224):\n  \"\"\"Returns a Tensor with 5 cropped square images\n  img: PILImage\n  size: int\n  \"\"\"\n  w,h = img.width, img.height\n  top_left = TensorImage(img.crop((0,0,size,size)))\n  top_right = TensorImage(img.crop((w-size, 0, w, size)))\n  bottom_right = TensorImage(img.crop((w-size, h-size, w, h)))\n  bottom_left = TensorImage(img.crop((0, h-size, size, h)))\n  center = TensorImage(img.crop((w/2-size/2, h/2-size/2, w/2+size/2,h/2+size/2)))\n  return torch.stack([top_left, top_right, bottom_right, bottom_left, center])\n\nI’ll test the corner_crop function and make sure that the five images are cropped correctly.\nHere’s the top left corner.\n\nimgs = corner_crop(img)\n\n# Top Left Corner Crop\nimgs[0].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1a177d0>\n\n\n\n\n\nTop right corner:\n\n# Top Right Corner Crop\nimgs[1].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e197da50>\n\n\n\n\n\nBottom right:\n\n# Bottom Right Corner Crop\nimgs[2].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e146ed50>\n\n\n\n\n\nBottom left:\n\n# Bottom Left Corner Crop\nimgs[3].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424dd0>\n\n\n\n\n\nAnd center:\n\n# Center Crop\nimgs[4].show()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f12e1424450>\n\n\n\n\n\n\n\nDefine a new CornerCrop transform by extending the Transform class definition\nThe main purpose for all of that was for me to wrap my head around how the crop behavior functions so that I can wrap that into a transform.\nTransforms are any function that you want to apply to your data. I’ll extend the base Transform class and add in the functionality I need for these crops. When an object of the CornerCrop class is constructed, the constructor takes size and corner_type arguments. Since I’ll use this within a for-loop, the corner_type argument is an integer from 0 to 3, corresponding to the loop counter. The transform is applied to the data during the .encodes method. I grab the original image width and height, and create a list of cropped images using the left, upper, right, bottom coordinates we saw above. Finally, based on the corner_type, the corresponding crop is returned.\n\nclass CornerCrop(Transform):\n    \"Create 4 corner and 1 center crop of `size`\"\n    def __init__(self, size, corner_type=0, **kwargs):\n      self.size = size\n      self.corner_type = corner_type\n\n    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n      self.w, self.h = x.size\n      self.crops = [\n                    x.crop((0,0,self.size, self.size)),\n                    x.crop((self.w - self.size, 0, self.w, self.size)),\n                    x.crop((self.w-self.size, self.h-self.size, self.w, self.h)),\n                    x.crop((0, self.h-self.size, self.size, self.h))\n                    ]\n      return self.crops[self.corner_type]\n\nTo test this transform, I created an image with top left, top right, bottom right and bottom left identified. I created multiple copies so that I can create batches.\n\n# test image for CornerCrop\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\nImage.open((path/'01.jpg'))\n\n\n\n\nI create a DataBlock and pass my CornerCrop to the item_tfms parameter. I’ll cycle through the different corner types. 0 corresponds to top left, 1 is top right, 2 is bottom right and 3 is bottom left. All images in my batch should be cropped to the same corner.\nI set corner_type to 0, build the DataBlock and DataLoaders and the batch shows top left.\n\n# get the data\n# path = untar_data(URLs.IMAGENETTE)\npath = Path('/content/gdrive/MyDrive/fastai-course-v4/images/test/corner_crop_images')\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,0))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 1, build the DataBlock and DataLoaders and the batch shows top right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,1))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 2, build the DataBlock and DataLoaders and the batch shows bottom right.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,2))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nI set corner_type to 3, build the DataBlock and DataLoaders and the batch shows bottom left.\n\n# build the DataBlock and DataLoaders using CornerCrop\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=CornerCrop(224,3))\n\ndls = dblock.dataloaders(path, bs=4)\n\n# view a batch\ndls.show_batch()\n\n\n\n\nNow, I can implement this transform into a new TTA method.\n\n\nDefine a new Learner.corner_crop_tta method by repurposing the existing Learner.tta definition\nI’ll largely rely on the definition of tta in the built-in Learner class. In this method, predictions are calculated on four sets of augmented data (images) and then averaged along with predictions on a center-crop dataset.\nIn the existing for-loop, four sets of predictions on randomly generated crops are appended into a list.\nfor i in self.progress.mbar if hasattr(self,'progress') else range(n):\n  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\nIn my loop, I create a new DataLoader each time, passing a different corner_type argument to the CornerCrop transform. I also have to pass the ToTensor transform, so that the PIL Image is converted to a Tensor. In the first iteration, it will append predictions on the top left corner crops. In the next one, it will append predictions on the top right, then the bottom right, and finally on the fourth loop, the bottom left.\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nSince I am to average these with the center-crop image predictions, I’ll create a new DataLoader without the CornerCrop transform and calculate the predictions on those images:\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\nFinally, I’ll append the center crop preds to aug_preds list, concatenate them into a single tensor and take the mean of the predictions:\naug_preds.append(preds[None])\npreds = torch.cat(aug_preds).mean(0)\nI decided to create a new Learner2 class which extends the built-in the Learner, and added the corner_crop_tta method by copying over the tta method, commenting out the lines I won’t need and adding the lines and changes I’ve written above.\n\nclass Learner2(Learner):\n  def corner_crop_tta(self:Learner, ds_idx=1, dl=None, n=4, beta=0.25, use_max=False):\n      \"Return predictions on the `ds_idx` dataset or `dl` using Corner Crop Test Time Augmentation\"\n      if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n      # if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n      try:\n          #self(_before_epoch)\n          with dl.dataset.set_split_idx(0), self.no_mbar():\n              if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n              aug_preds = []\n              # Crop image from four corners\n              for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n                  dl = dl.new(after_item=[CornerCrop(224,i), ToTensor])\n                  self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n                  aug_preds.append(self.get_preds(dl=dl, inner=True)[0][None])\n         # aug_preds = torch.cat(aug_preds)\n         # aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n          self.epoch = n\n          dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n          # Crop image from center\n          with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n          aug_preds.append(preds[None])\n      finally: self(event.after_fit)\n\n     # if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n     # preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n     # preds = torch.cat([aug_preds, preds]).mean(0)\n      preds = torch.cat(aug_preds).mean(0)\n      return preds,targs\n\n\n\nImplement this new TTA method on the Imagenette classification model\nIn the last section of this notebook, I train a model on the Imagenette dataset, which a subset of the larger ImageNet dataset. Imagenette has 10 distinct classes.\n\n# get the data\npath = untar_data(URLs.IMAGENETTE)\n\n# build the DataBlock and DataLoaders \n# for a single-label classification\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label, # image folder names are the class names\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\ndls = dblock.dataloaders(path, bs=64)\n\n# view a batch\ndls.show_batch()\n\n\n\n\n\n\n\n\n# Try `CornerCrop` on a new DataLoader\n# add `ToTensor` transform to conver PILImage to TensorImage\nnew_dl = dls[1].new(after_item=[CornerCrop(224,3), ToTensor])\nnew_dl.show_batch()\n\n\n\n\n\n# baseline training\nmodel = xresnet50()\nlearn = Learner2(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.628959\n      2.382344\n      0.450336\n      02:39\n    \n    \n      1\n      1.258259\n      3.365233\n      0.386482\n      02:45\n    \n    \n      2\n      0.992097\n      1.129573\n      0.653473\n      02:49\n    \n    \n      3\n      0.709120\n      0.643617\n      0.802091\n      02:47\n    \n    \n      4\n      0.571318\n      0.571139\n      0.824122\n      02:45\n    \n  \n\n\n\nI run the default tta method, pass the predictions and targets to the accuracy function and calculate an accuracy of about 83.5% percent. Which is higher than the default center crop validation accuracy.\n\n# built-in TTA method\npreds_tta, targs_tta = learn.tta()\naccuracy(preds_tta, targs_tta).item()\n\n\n    \n        \n      \n      \n    \n    \n\n\n\n\n\n0.8345780372619629\n\n\nFinally, I run my new corner_crop_tta method, pass the predictions and targets to the accuracy function, and calculate an accuracy of about 70.9% percent. Which is lower than the default center crop validation accuracy.\n\n# user-defined TTA method\npreds, targs = learn.corner_crop_tta()\naccuracy(preds, targs).item()\n\n\n\n\n0.7098581194877625\n\n\nI’ll walk through the corner_crop_tta code to verify the accuracy calculated above.\nI first create an empty list for my augmented image predictions.\nThen I loop through a range of 4, each time creating a new DataLoader which applies the CornerCrop transform for each corner type and append the predictions onto the list.\n\n# get predictions on corner cropped validation images\naug_preds = []\nfor i in range(4):\n  dl = dls[1].new(after_item=[CornerCrop(224,i), ToTensor])\n  #self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n  aug_preds.append(learn.get_preds(dl=dl, inner=True)[0][None])\nlen(aug_preds), aug_preds[0].shape\n\n\n\n\n\n\n\n\n\n\n\n\n\n(4, torch.Size([1, 2678, 1000]))\n\n\nI then create a new DataLoader without my transform, and get those predictions.\n\n# get predictions on center crop validation images\ndl = dls[1].new(shuffled=False, drop_last=False)\nwith dl.dataset.set_split_idx(1): preds,targs = learn.get_preds(dl=dl, inner=True)\npreds.shape\n\n\n\n\ntorch.Size([2678, 1000])\n\n\nThe shape of these predictions is missing an axis, so I pass None as a Key and it adds on a new axis.\n\n# add an axis to match augmented prediction tensor shape\npreds = preds[None]\npreds.shape\n\ntorch.Size([1, 2678, 1000])\n\n\nI append the center crop predictions onto the augmented predictions and concatenate all five sets of predictions into a Tensor and calculate the mean.\n\n# average all 5 sets of predictions\naug_preds.append(preds)\npreds = torch.cat(aug_preds).mean(0)\n\nI then pass those average predictions and the targets to the accuracy function calculate the accuracy which is slightly higher than above. I ran these five cells multiple times and got the same accuracy value. When I ran the corner_crop_tta method multiple times, I got different accuracy values each time. Something in the corner_crop_tta definition is incorrect. I’ll go with this value since it was consistent.\n\n# calculate validation set accuracy\naccuracy(preds, targs).item()\n\n0.7311426401138306\n\n\nThe following table summarize the results from this training:\n\n\n\nValidation\nAccuracy\n\n\n\n\nCenter Crop\n82.4%\n\n\nCenter Crop + 4 Random Crops: Linearly Interpolated\n83.5%\n\n\nCenter Crop + 4 Random Crops: Averaged\n73.1%\n\n\n\nThere are a few further research items I should pursue in the future:\n\nFix the corner_crop_tta method so that it returns the same accuracy each time it’s run on the same trained model\nTry corner_crop_tta on a multi-label classification dataset such as PASCAL\nTry linear interpolation (between center crop and corner crop maximum) instead of mean"
  },
  {
    "objectID": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html",
    "href": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html",
    "title": "Fine-Tuning a Language Model as a Text Classifier",
    "section": "",
    "text": "In this notebook, I’ll fine-tune a lanaguage model on the IMDb reviews dataset, grab the encoder, create a new classification model with it and then fine-tune it to classify IMDb reviews as positive or negative. The code (and prose) below is taken from Chapter 10 of the fastai textbook.\nThe data is stored in three folders: train (25k labeled reviews), test (25k labeled reviews) and unsup (50k unlabeled reviews). The language model is trained on all 100k reviews and the classification model is trained using the train dataset (its accuracy calculated on the test validation set)."
  },
  {
    "objectID": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tuning-the-pretrained-language-model",
    "href": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tuning-the-pretrained-language-model",
    "title": "Fine-Tuning a Language Model as a Text Classifier",
    "section": "Fine-Tuning the Pretrained Language Model",
    "text": "Fine-Tuning the Pretrained Language Model\nFirst, we fine-tune the pretrained language model (which was trained on all of Wikipedia) using 100k movie reviews. This fine-tuned model will learn to predict the next word of an IMDb movie review.\nNote that fastai’s TextBlock sets up its numericalizer’s vocab automatically.\n\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb,\n    splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_ln=80)\n\nThe dependent variable is the independent variable shifted over by one token:\n\ndls_lm.show_batch(max_n=2)\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj this movie is my favorite of all time . xxmaj the dialogue is spectacular , and is delivered with such rapid - fire speed that one viewing is not enough . xxmaj the film comedy was elevated to new heights with xxmaj howard xxmaj hawks outstanding direction . xxmaj based on the classic play \" the xxmaj front xxmaj page \" , xxmaj hawks gives it a delightful twist by\n      xxmaj this movie is my favorite of all time . xxmaj the dialogue is spectacular , and is delivered with such rapid - fire speed that one viewing is not enough . xxmaj the film comedy was elevated to new heights with xxmaj howard xxmaj hawks outstanding direction . xxmaj based on the classic play \" the xxmaj front xxmaj page \" , xxmaj hawks gives it a delightful twist by presenting\n    \n    \n      1\n      xxmaj woody xxmaj woodpecker , \" duck xxmaj amuck \" and especially \" one xxmaj froggy xxmaj evening \" show up how weak this movie is in comparison . xxmaj plus the movie fits in shambolic slapstick alongside strained sentiment ( the underlying theme of the story is family ; our hero is n't ready to have a son , and his nemesis - xxmaj alan xxmaj cumming as the xxmaj norse\n      woody xxmaj woodpecker , \" duck xxmaj amuck \" and especially \" one xxmaj froggy xxmaj evening \" show up how weak this movie is in comparison . xxmaj plus the movie fits in shambolic slapstick alongside strained sentiment ( the underlying theme of the story is family ; our hero is n't ready to have a son , and his nemesis - xxmaj alan xxmaj cumming as the xxmaj norse god\n    \n  \n\n\n\n\nlearn = language_model_learner(\n    dls_lm,\n    AWD_LSTM,\n    drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]\n).to_fp16()\n\nI fine-tuned the model for one epoch and saved it to load and use later. language_model_learner automatically freezes the pretrained model so it trains only the randomly instantiated embeddings representing the IMDb vocab.\n\nlearn.fit_one_cycle(1, 2e-2)\n\nPaperspace’s file browser is located at /notebooks so I change the learn.path to that location:\n\nlearn.path = Path('/notebooks')\n\nI then save the learner so that it saves the trained embeddings.\n\nlearn.save('1epoch')\n\nPath('/notebooks/models/1epoch.pth')\n\n\nLater on, I load the saved model, unfreeze the layers of the pretrained language model and fine-tune it for 10 epochs on the IMDb reviews dataset at a smaller learning rate (as shown in the fastai text):\n\nlearn = learn.load('1epoch')\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      4.214371\n      4.114542\n      0.300169\n      61.224136\n      41:36\n    \n    \n      1\n      3.917021\n      3.850335\n      0.316820\n      47.008827\n      42:00\n    \n    \n      2\n      3.752428\n      3.724050\n      0.326502\n      41.431866\n      42:13\n    \n    \n      3\n      3.660530\n      3.660284\n      0.331666\n      38.872364\n      42:32\n    \n    \n      4\n      3.560096\n      3.620281\n      0.335297\n      37.348042\n      42:36\n    \n    \n      5\n      3.507077\n      3.592660\n      0.338347\n      36.330578\n      42:44\n    \n    \n      6\n      3.430038\n      3.575986\n      0.340261\n      35.729839\n      42:39\n    \n    \n      7\n      3.360812\n      3.566898\n      0.341806\n      35.406578\n      42:53\n    \n    \n      8\n      3.310551\n      3.567138\n      0.342046\n      35.415089\n      43:28\n    \n    \n      9\n      3.297931\n      3.570799\n      0.341944\n      35.544979\n      44:01\n    \n  \n\n\n\nIOPub message rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_msg_rate_limit`.\n\nCurrent values:\nServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\n\n\nWe save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder.\n\nlearn.save_encoder('imdb_finetuned')\n\nBefore we fine-tune the model to be a classifier, the textbook has us generate random reviews:\n\nTEXT = 'I liked this movie because'\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)]\n\nprint(\"\\n\".join(preds))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni liked this movie because it showed a lot of normal people in America about who we belong and what they say and do . \n\n The acting was great , the story was fun and enjoyable and the movie was very well\ni liked this movie because my family and i are great Canadians and also Canadians , especially the Canadians . This is not a Canadian and American movie , but instead of being a \" mockumentary \" about the\n\n\nThe reviews are certainly not polished, but it’s still fascinating to see how the model predicts the next word to create a somewhat sensical review."
  },
  {
    "objectID": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tune-the-text-classifier",
    "href": "posts/2023-08-05-imdb-classifier/2023-08-05-imdb-classifier.html#fine-tune-the-text-classifier",
    "title": "Fine-Tuning a Language Model as a Text Classifier",
    "section": "Fine-tune the Text Classifier",
    "text": "Fine-tune the Text Classifier\nFor the final piece of this lesson, we move from language model to classifier, starting with creating the classifier DataLoaders.\nWe pass it the vocab of the language model to make sure we use the same correspondence of token to index, so that the embeddings learned in the fine-tuned language model can be applied to the classifier.\nThe dependent variable in this classifier is the label of the parent folder, pos for positive and neg for negative.\nFinally, we don’t pass is_lm=True to the TextBlock since it’s False by default (which we want in this case because we have labeled data, and don’t want to use next token as the label).\n\n(path/'train').ls()\n\n(#4) [Path('/root/.fastai/data/imdb/train/pos'),Path('/root/.fastai/data/imdb/train/unsupBow.feat'),Path('/root/.fastai/data/imdb/train/neg'),Path('/root/.fastai/data/imdb/train/labeledBow.feat')]\n\n\n\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n    get_y = parent_label,\n    get_items = partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\nThe independent variable is the movie review and the dependent variable is the sentiment (positive, pos, or negative, neg):\n\ndls_clas.show_batch(max_n=3)\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero\n      pos\n    \n    \n      1\n      xxbos xxmaj by now you 've probably heard a bit about the new xxmaj disney dub of xxmaj miyazaki 's classic film , xxmaj laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky . xxmaj during late summer of 1998 , xxmaj disney released \" kiki 's xxmaj delivery xxmaj service \" on video which included a preview of the xxmaj laputa dub saying it was due out in \" 1 xxrep 3 9 \" . xxmaj it 's obviously way past that year now , but the dub has been finally completed . xxmaj and it 's not \" laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky \" , just \" castle xxmaj in xxmaj the xxmaj sky \" for the dub , since xxmaj laputa is not such a nice word in xxmaj spanish ( even though they use the word xxmaj laputa many times\n      pos\n    \n    \n      2\n      xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it\n      pos\n    \n  \n\n\n\nEach batch has to have tensors of the same size, so fastai does the following (when using a TextBlock with is_lm=False):\n\nBatch together texts that are roughly the same lengths (by sorting the documents by length prior to each epoch).\nExpand the shortest texts to make them all the same size (as the largest document in the batch) by padding them with a special padding token that will be ignored by the model.\n\nLet’s create the model to classify texts:\n\nlearn = text_classifier_learner(\n    dls_clas, \n    AWD_LSTM, \n    drop_mult=0.5, \n    metrics=accuracy\n).to_fp16()\n\nLoad the encoder from our fine-tuned language model:\n\nlearn.path = Path('/notebooks')\n\n\nlearn = learn.load_encoder('imdb_finetuned')\n\nThe last step is to train with discriminative learning rates and gradual unfreezing. For NLP classifiers the text recommends unfreezing a few layers at a time to achieve the best performance:\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.245777\n      0.174727\n      0.934000\n      01:48\n    \n  \n\n\n\nWe get a similar accuracy as the textbook value (0.929320).\nNext, train the model with all layers except the last two parameter groups frozen:\n\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.226701\n      0.161235\n      0.938800\n      01:59\n    \n  \n\n\n\nThe accuracy improved a bit!\nUnfreeze the third parameter group and keep training:\n\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.188972\n      0.147045\n      0.946440\n      02:43\n    \n  \n\n\n\nThe accuracy continues to improve.\nFinally, train the whole model:\n\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.163849\n      0.143639\n      0.947600\n      03:18\n    \n    \n      1\n      0.149648\n      0.144494\n      0.947840\n      03:19\n    \n  \n\n\n\nWe’ll test the model with a few low-hanging-fruit inputs:\n\nlearn.predict(\"I really like this movie!\")\n\n\n\n\n\n\n\n\n('pos', tensor(1), tensor([0.0034, 0.9966]))\n\n\n\nlearn.predict(\"I really did not like this movie!\")\n\n\n\n\n\n\n\n\n('neg', tensor(0), tensor([0.9985, 0.0015]))\n\n\n\nlearn.predict(\"I'm not sure if I loved or hated this movie\")\n\n\n\n\n\n\n\n\n('neg', tensor(0), tensor([0.6997, 0.3003]))\n\n\nTo recap, here are the three steps that were involved in creating the IMDb movie review classifier:\n\nA language model was pretrained on all of Wikipedia.\nWe then fine-tuned that model on 100k IMDb movie reviews (documents).\nUsing the encoder from the fine-tuned language model, we created a classification model and fine-tuned it for a few epochs, gradually unfreezing layers for consecutive epochs. This model accurately classifies movie review as positive or negative.\n\nThat’s a wrap for this exercise. I hope you enjoyed this blog post!"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "",
    "text": "In this blog post, I’ll run a fun little experiment which uses the code Jeremy Howard wrote in Getting started with NLP for absolute beginners to train an NLP classifier to predict whether or not a passenger on the titanic survived.\nI’ll start by acknowledging the obvious—that training an NLP model for tabular data that doesn’t contain much natural language is probably not going to give great results. However, it gives me an opportunity to use a simple dataset (that I’ve worked with before and am familiar with) to train a model following a process that is new to me (using the HuggingFace library). With that disclaimer out of the way, let’s jump in!"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#plan-of-attack",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#plan-of-attack",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nJeremy’s example uses tabular data with columns containing natural language and some additional data to predict values between 0 and 1 (0 means the two phrases are not similar in meaning, 1 means they are similar). Fundamentally, my dataset works in the same way—I have a bunch of columns describing features of the passengers and then a value of 0 (died) or 1 (survived) that I’m trying to predict.\n\nPreparing the Data\nThe data preparation step will be similar—I will concatenate multiple columns with a separator between each term.\n\n\nTraining Process\nI’ll use the same model (and thus tokenizer) as Jeremy did, so the training setup will be much of the same.\n\n\nMetrics\nJeremy used Pearson’s correlation coefficient (as specified by the Kaggle competition the dataset came from). In my case, I’ll need to figure out how to pass accuracy to the HuggingFace Trainer."
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#load-and-prep-the-data",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#load-and-prep-the-data",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Load and Prep the Data",
    "text": "Load and Prep the Data\nI’ll start by using the boilerplate code Jeremy has provided to get data from Kaggle.\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nimport os\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path(\"../input/titanic\")\nelse:\n  path = Path('titanic')\n  if not path.exists():\n    import zipfile, kaggle\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to /content\n\n\n100%|██████████| 34.1k/34.1k [00:00<00:00, 1.45MB/s]\n\n\n\n\n\n\n\n\n\n# load the training data and look at it\nimport torch, numpy as np, pandas as pd\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe only data cleaning I’ll do is fill missing values with the mode of each column:\n\nmodes = df.mode().iloc[0]\ndf.fillna(modes, inplace=True)\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\nI’ll also set my independent variable as a float to resolve an error I got during training (\"mse_cuda\" not implemented for 'Long').\n\ndf['Survived'] = df['Survived'].astype(float)\n\nI’ll next create an input column which creates the input to the model:\n\ndf['input'] = 'Pclass: ' + df.Pclass.apply(str) +\\\n '; Name: ' + df.Name + '; Sex: ' + df.Sex + '; Age: ' + df.Age.apply(str) +\\\n  '; SibSp: ' + df.SibSp.apply(str) + '; Parch: ' + df.Parch.apply(str) +\\\n  '; Ticket: ' + df.Ticket + '; Fare: ' + df.Fare.apply(str) + \\\n  '; Cabin: ' + df.Cabin + '; Embarked: ' + df.Embarked\n\n\ndf['input'][0]\n\n'Pclass: 3; Name: Braund, Mr. Owen Harris; Sex: male; Age: 22.0; SibSp: 1; Parch: 0; Ticket: A/5 21171; Fare: 7.25; Cabin: B96 B98; Embarked: S'"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#tokenization",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#tokenization",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Tokenization",
    "text": "Tokenization\n\n! pip install datasets transformers[sentencepiece] accelerate -U\n\n\nfrom datasets import Dataset,DatasetDict\n\nI’ll remove 100 rows of data to serve as a test set for final predictions after the model is trained.\n\n# create a random sample of 100 passengers\neval_df = df.sample(100)\n\n\neval_df.head()\n\n\n\n  \n    \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      input\n    \n  \n  \n    \n      709\n      710\n      1.0\n      3\n      Moubarek, Master. Halim Gonios (\"William George\")\n      male\n      24.0\n      1\n      1\n      2661\n      15.2458\n      B96 B98\n      C\n      Pclass: 3; Name: Moubarek, Master. Halim Gonio...\n    \n    \n      439\n      440\n      0.0\n      2\n      Kvillner, Mr. Johan Henrik Johannesson\n      male\n      31.0\n      0\n      0\n      C.A. 18723\n      10.5000\n      B96 B98\n      S\n      Pclass: 2; Name: Kvillner, Mr. Johan Henrik Jo...\n    \n    \n      840\n      841\n      0.0\n      3\n      Alhomaki, Mr. Ilmari Rudolf\n      male\n      20.0\n      0\n      0\n      SOTON/O2 3101287\n      7.9250\n      B96 B98\n      S\n      Pclass: 3; Name: Alhomaki, Mr. Ilmari Rudolf; ...\n    \n    \n      720\n      721\n      1.0\n      2\n      Harper, Miss. Annie Jessie \"Nina\"\n      female\n      6.0\n      0\n      1\n      248727\n      33.0000\n      B96 B98\n      S\n      Pclass: 2; Name: Harper, Miss. Annie Jessie \"N...\n    \n    \n      39\n      40\n      1.0\n      3\n      Nicola-Yarred, Miss. Jamila\n      female\n      14.0\n      1\n      0\n      2651\n      11.2417\n      B96 B98\n      C\n      Pclass: 3; Name: Nicola-Yarred, Miss. Jamila; ...\n    \n  \n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nI’ll remove these 100 rows from the original DataFrame which I will use for training and validation sets.\n\ndf = df.drop(eval_df.index)\n\n\ndf.shape\n\n(791, 13)\n\n\n\nds = Dataset.from_pandas(df)\n\n\neval_ds = Dataset.from_pandas(eval_df)\n\n\nds\n\nDataset({\n    features: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__'],\n    num_rows: 791\n})\n\n\n\neval_ds\n\nDataset({\n    features: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__'],\n    num_rows: 100\n})\n\n\nI’ll use the same model as in Jeremy’s example:\n\nmodel_nm = 'microsoft/deberta-v3-small'\n\n\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\nI’ll check the tokenizer:\n\ntokz.tokenize(\"We are about to tokenize this dataset!\")\n\n['▁We', '▁are', '▁about', '▁to', '▁token', 'ize', '▁this', '▁dataset', '!']\n\n\n\n# function to tokenize inputs\ndef tok_func(x): return tokz(x[\"input\"])\n\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\n\neval_ds = eval_ds.map(tok_func, batched=True)\n\n\n\n\n\nrow = tok_ds[0]\nrow['input'], row['input_ids']\n\n('Pclass: 3; Name: Braund, Mr. Owen Harris; Sex: male; Age: 22.0; SibSp: 1; Parch: 0; Ticket: A/5 21171; Fare: 7.25; Cabin: B96 B98; Embarked: S',\n [1,\n  916,\n  4478,\n  294,\n  404,\n  346,\n  5445,\n  294,\n  24448,\n  407,\n  261,\n  945,\n  260,\n  12980,\n  6452,\n  346,\n  23165,\n  294,\n  2844,\n  346,\n  5166,\n  294,\n  1460,\n  260,\n  693,\n  346,\n  42209,\n  32154,\n  294,\n  376,\n  346,\n  916,\n  22702,\n  294,\n  767,\n  346,\n  14169,\n  294,\n  336,\n  320,\n  524,\n  1259,\n  30877,\n  346,\n  40557,\n  294,\n  574,\n  260,\n  1883,\n  346,\n  22936,\n  294,\n  736,\n  8971,\n  736,\n  8454,\n  346,\n  77030,\n  569,\n  294,\n  662,\n  2])\n\n\nI’ll look at the index for some of the words in the input to check that they are present in the input_ids column:\n\ntokz.vocab['▁P']\n\n916\n\n\n\ntokz.vocab['▁3']\n\n404\n\n\n\ntokz.vocab['▁Name']\n\n5445\n\n\nTransformers expects the independent variable to be named labels:\n\ntok_ds = tok_ds.rename_columns({'Survived':'labels'})\n\n\ntok_ds\n\nDataset({\n    features: ['PassengerId', 'labels', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 791\n})"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#preparing-training-and-validation-sets",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#preparing-training-and-validation-sets",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Preparing Training and Validation Sets",
    "text": "Preparing Training and Validation Sets\nSince I cut into my training and validation set by pulling out a test set, I’ll use a smaller split for the validation set.\n\ndds = tok_ds.train_test_split(0.15, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['PassengerId', 'labels', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 672\n    })\n    test: Dataset({\n        features: ['PassengerId', 'labels', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 119\n    })\n})"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#creating-an-accuracy-function",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#creating-an-accuracy-function",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Creating an Accuracy Function",
    "text": "Creating an Accuracy Function\nSince my independent variable is binary (0 or 1), I’ll create an accuracy function with the following:\n\nIf predictions are greater than 0.5, classify them as 1, if less than 0.5, classify they as 0.\nCompare predictions to the labels and take the mean value of the boolean array which will be the % of correctly predicted values.\n\n\ndef calculate_accuracy(preds, labels):\n  return torch.tensor(((preds>0.5)==labels)).float().mean().item()\n\n# Transformers want a dictionary for the metric\ndef acc_d(eval_pred): return {'accuracy': calculate_accuracy(*eval_pred) }"
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#training-the-model",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#training-the-model",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Training the Model",
    "text": "Training the Model\nI’ll use the same code as is shown in Jeremy’s notebook for preparing the Trainer:\n\nfrom transformers import TrainingArguments,Trainer\n\n\nbs = 128\nepochs = 4\n\nI’ll use the same learning rate as the example to start with:\n\nlr = 8e-5\n\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=acc_d)\n\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ntrainer.train();\n\n\n\n    \n      \n      \n      [24/24 00:08, Epoch 4/4]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Accuracy\n    \n  \n  \n    \n      1\n      No log\n      0.253301\n      0.462185\n    \n    \n      2\n      No log\n      0.246423\n      0.537815\n    \n    \n      3\n      No log\n      0.223734\n      0.537815\n    \n    \n      4\n      No log\n      0.216874\n      0.747899\n    \n  \n\n\n\nI trained the model a few times and noticed that the accuracy varied significantly. For some trainings, it was stuck at around 0.56, for others, it went from 0.4 to 0.5 to 0.6. In this final training, it jumped from 0.54 to 0.75 in the final epoch. I think this means that the combination of data and hyperparameters is causing an unstable training regime for this model.\nLet’s look at some of the predictions on the test set:\n\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds[:10], preds.shape\n\n\n\n\n(array([[0.53808594],\n        [0.16943359],\n        [0.14575195],\n        [0.52392578],\n        [0.50390625],\n        [0.52539062],\n        [0.52734375],\n        [0.14221191],\n        [0.52001953],\n        [0.53320312]]),\n (100, 1))\n\n\nI’ll calculate the accuracy for the test set:\n\ntorch.tensor((preds.squeeze(1)>0.5) == eval_df['Survived'].values).float().mean().item()\n\n0.8100000023841858\n\n\nNot bad! I get an 81% accuracy on my test set. The linear, neural net, deep neural net and fastai tabular_learner model achieved an accuracy of about 83% on their validation sets."
  },
  {
    "objectID": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#final-thoughts",
    "href": "posts/2023-08-23-titanic-nlp/2023-08-23-titanic-nlp.html#final-thoughts",
    "title": "Using HuggingFace Transformers for Tabular Titanic Data",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nOverall I found this exercise enjoyable. I learned a little bit more about using HuggingFace Transfomers, and better understand what Jeremy did in his example notebook. I am not confident in this model or approach as I did notice the training was unstable (highly varying accuracy across different trainings), and this dataset is not really meant for an NLP model. I also had a relatively smaller number of rows than the example that Jeremy showed. That being said, my model wasn’t a complete dud as it mostly accurately predicted who survived in my test set."
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html",
    "title": "Transcribing Sherlock into Spanish",
    "section": "",
    "text": "Sherlock Holmes kneeling next to Toby the bloodhound and pointing, likely towards where he thinks Toby should go next\nTranscription Progress (00:06:07 out of 17:40:32 transcribed)"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#background",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Background",
    "text": "Background\nI have watched all four seasons of BBC’s Sherlock probably 5 times. I learn something new about it each time.\nI have tried to learn Spanish using Duolingo, stopping and re-starting every year or so, without much success.\nI don’t really recall how the thought came about but I decided to combine my love of the show with my desire to learn Spanish into one project—this one!"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#setup",
    "title": "Transcribing Sherlock into Spanish",
    "section": "Setup",
    "text": "Setup\nUsing the embedded Google Translate UI and my partner’s translator-level knowledge of the language, I am transcribing every word of the show into Spanish.\n\n\n\nA screenshot of my translation setup: Google Translate embedded underneath the search bar—the result of googling “Google Translate”. I’ve typed “Sherlock Holmes” in the “English” textbox on the left and it has translated to “Sherlock Holmes” in the Spanish output on the right.\n\n\nIn a second tab, I have the show open (with subtitles on).\n\n\n\nA screenshot of Sherlock playing in the Amazon Prime Video player\n\n\nI transcribe in a .txt file titled transcript.txt, documenting the following fields:\n\nseason number\nepisode number\ntimestamp (hours::minutes:seconds)\nwho is the speaker?\nthe english transcription of what they say\nthe spanish translation of that\nnotes which usually documents specific word translations\n\nAs an example, the first bit of dialogue in the series is John Watson’s therapist Ella asking him “How’s your blog going?” which translates to “Cómo va tu blog?” Where va = goes.\nHow goes your blog? I would say quite well heheheh.\nseason,episode,time,speaker,english,spanish,notes\n1,1,00:01:30,ella,how's your blog going?,cómo va tu blog?, va = goes"
  },
  {
    "objectID": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "href": "posts/2023-02-20-sherlock-holmes-spanish-transcription/index.html#what-im-learning",
    "title": "Transcribing Sherlock into Spanish",
    "section": "What I’m Learning",
    "text": "What I’m Learning\nI’ll write in this blog post some examples of the translations and how I’m thinking through the process, as well as what I’m learning from discussions with my partner.\nThree main themes I’m seeing so far about translating from English to Spanish:\n\nwhich words to use depends a lot on context.\nwords that sound the same but mean different things will sometimes have different emphasis.\na word that is technically correct may not be used frequently in conversation.\n\nI’m not quite sure how to best document what I’m learning so I’ll just start writing.\n\nElla: “You haven’t written a word, have you?”\nSomething I enjoy doing is translating the Spanish back into English without changing word positions. The benefit of this exercise of translating and translating back is that it reveals (or focuses my attention on) nuances I wouldn’t otherwise be aware of.\nEnglish: You haven’t written a word, have you?\nSpanish: No has escrito una palabra verdad?\nBack to English: Not you have written a word true?\nI asked my partner how she would translate it and she said: No has escrito ni una palabra, verdad?\nWhich translates to: You haven’t written not even a word, true?\nIt bothers me that I don’t know why in English the question ends in have you? but in Spanish it ends with true?. Of course this may just be how Spanish works or how conversational Spanish works.\nI asked my partner how you would say just have you? in Spanish and it’s lo has?\nGoogle Translate aligns with this when it translates from Spanish to English:\nSpanish: No has escrito ni una palabra lo has?\nEnglish: You haven’t written a word, have you?\nBut recommends ending with verdad? when I translate from English to Spanish.\n\n\nSpeaker: “You can share mine”\nHere are the Google Translate forward and backward translations:\n\nEnglish: You can share mine.\nSpanish: Puedes compartir el mio.\n\nSpanish: Puedes compartir el mio.\nEnglish: Can you share mine.\n\nHowever, if I start the Spanish translation with tu the English translation matches my original prompt:\n\nSpanish: Tu puedes compartir el mio.\nEnglish: You can share mine.\n\nI think this is a good example of how what is technically correct may or may not be what’s used in conversation—saying tu may not be strictly required for conversation and may be implicitly understood because of the form used—puedes (you can).\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npuedes\nyou can\n\n\npuedo\nI can\n\n\npuedemos\nwe can\n\n\npueden\nthey can\n\n\n\n\n\nLestrade: “Well, they all took the same poison.”\nSomething else I’ve enjoyed and learned from is watching how a translation changes as you type the full sentence in Google Translate.\nFor example when translating from Spanish (pues, todos tomaron el mismo veneno) to English (well, they all took the same poison):\n\n\n\n\n\n\n\nSpanish\nEnglish\n\n\n\n\npues\nwell\n\n\npues, todos\nwell, everyone\n\n\npues, todos tomaron\nwell, they all took\n\n\npues, todos tomaron el\nwell, everyone took\n\n\npues, todos tomaron el mismo\nwell, they all took the same\n\n\npues, todos tomaron el mismo veneno\nwell, they all took the same poison\n\n\n\nWhat I’m observing might have less to do with how Spanish works and more to do with how Google Translate works. Although some words seem interchangeable (todos seems to mean everyone or they all)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "",
    "text": "In this blog post, I have written excerpts from the book Visualization Analysis & Design by Tamara Munzner."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#structure-whats-in-this-book",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#structure-whats-in-this-book",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "Structure: What’s in This Book",
    "text": "Structure: What’s in This Book\n\nChapter 1: high-level introduction to an analysis framework of breaking down vis design to what-why-how questions that have data-task-idiom answers\nChapter 2: addresses the what question with answers about data abstraction\nChapter 3: addresses the why question with task abstractions\nChapter 4: extends the analysis framework to two additional levels: the domain situation level on top and the algorithm level on the bottom\nChapter 5: the principles of marks and channels for encoding information\nChapter 6: eight rules of thumb for design\nChapter 7: how to visually encode data by arranging space for tables\nChapter 8: for spatial data\nChapter 9: for networks\nChapter 10: choices for mapping color and other channels in visual encoding\nChapter 11: ways to manipulate and change a view\nChapter 12: ways to facet data between multiple views\nChapter 13: how to reduce the amount of data shown in each view\nChapter 14: embedding information about a focus set within the context of overview data\nChapter 15: six case studies\n\nAccompanying web page"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-human-in-the-loop",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-human-in-the-loop",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.2 Why Have a Human in the Loop?",
    "text": "1.2 Why Have a Human in the Loop?\nVis allows people to analyze data when they don’t know exactly what questions they need to ask in advance.\nIf a fully automatic solution has been deemed to be acceptable, then there is no need for human judgment, and thus no need for you to design a vis tool.\nThe outcome of designing vis tools targeted at specific real-world domain problems is often a much crisper understanding of the user’s task, in addition to the tool itself."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-computer-in-the-loop",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-have-a-computer-in-the-loop",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.3 Why Have a Computer in the Loop?",
    "text": "1.3 Why Have a Computer in the Loop?\nBy enlisting computation, you can build tools that allow people to explore or present large datasets that would be completely unfeasible to draw by hand, thus opening up the possibility of seeing how datasets change over time.\nAs a designer, you can think about what aspects of hand-drawn diagrams are important in order to automatically create drawings that retain the hand-drawn spirit."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-an-external-representation",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-an-external-representation",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.4 Why Use an External Representation?",
    "text": "1.4 Why Use an External Representation?\nVis allows people to offload internal cognition and memory usage to the perceptual system, using carefully designed images as a form of external representations, sometimes called external memory."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-depend-on-vision",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-depend-on-vision",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.5 Why Depend on Vision?",
    "text": "1.5 Why Depend on Vision?\nThe visual system provides a very high-bandwidth channel to our brains. A significant amount of visual information processing occurs in parallel at the preconscious level.\nSound is poorly suited for providing overviews of large information spaces compared with vision. We experience the perceptual channel of sound as a sequential stream, rather than as a simultaneous experience where what we hear over a long period of time is automatically merged together."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-show-the-data-in-detail",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-show-the-data-in-detail",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.6 Why Show the Data in Detail?",
    "text": "1.6 Why Show the Data in Detail?\nStatistical characterization of datasets is a very powerful approach but it has the intrinsic limitation of losing information through summarization.\nAnscombe’s Quartet illustrates how datasets that have identical descriptive statistics can have very different structures that are immediately obvious when the dataset is shown graphically."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-interactivity",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-use-interactivity",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.7 Why Use Interactivity?",
    "text": "1.7 Why Use Interactivity?\nWhen datasets are large enough, the limitations of both people and display preclude just showing everything at once."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-the-vis-idiom-design-space-huge",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-the-vis-idiom-design-space-huge",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.8 Why is the Vis Idiom Design Space Huge?",
    "text": "1.8 Why is the Vis Idiom Design Space Huge?\nidiom: a distinct approach to creating and manipulating visual representations."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-tasks",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-tasks",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.9 Why Focus on Tasks?",
    "text": "1.9 Why Focus on Tasks?\nA tool that serves well for one task can be poorly suited for another, for exactly the same dataset.\nReframing the users’ task from domain-specific form into abstract form allows you to consider the similarities and differences between what people need across many real-world usage contexts."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-effectiveness",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-focus-on-effectiveness",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.10 Why Focus on Effectiveness?",
    "text": "1.10 Why Focus on Effectiveness?\nThe goals of the designer are not met if the result is beautiful but not effective.\nAny depiction of data is an abstraction where choices are made about which aspects to emphasize."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-most-designs-ineffective",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-most-designs-ineffective",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.11 Why Are Most Designs Ineffective?",
    "text": "1.11 Why Are Most Designs Ineffective?\nThe vast majority of the possibilities in the design space will be ineffective for any specific usage context.\nIn addressing design problems, it’s not a very useful goal to optimize or find the very best choice. A more appropriate goal when you design is to satisfy or find one of the many possible good solutions rather than one of the even larger number of bad ones.\nProgressively smaller search spaces:\n\nSpace of possible solutions\nSpace of solutions known to the designer\nSpace of solutions you actively consider\nSpace of solutions you investigate in detail\nSelected solution\n\nThe problem of a small consideration space is the higher probability of only considering OK or poor solutions and missing a good one.\nOne way to ensure that more than one possibility is considered is to explicitly generate multiple ideas in parallel."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-validation-difficult",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-is-validation-difficult",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.12 Why Is Validation Difficult?",
    "text": "1.12 Why Is Validation Difficult?\nHow do you know it works? How do you argue that one design is better or worse than another for the intended users? What does better mean? Do users get something done faster? Do they have more fun doing it? Can they work more effectively? What does effectively mean? How do you measure insight or engagement? What is the design better than? Is it better than another vis system? Is it better than doing the same things manually, without visual support? Is it better than doing the same things completely automatically? And what sort of thing does it do better? How do you decide what sort of task the users should do when testing the system? And who is the user? An expert who has done this task for decades, or a novice who needs the task to be explained before they begin? Are they familiar with how the system works from using it for a long time, or are they seeing it for the first time? Are the users limited by the speed of their own thought process, or their ability to move the mouse, or simply the speed of the computer in drawing each picture?\nHow do you decide what sort of benchmark data you should use when testing the system? Can you characterize what classes of data the system is suitable for? How might you measure the quality of an image generated by a vis tool? How well do any of the automatically computed quantitative metrics of quality match up with human judgments? Does the complexity of the algorithm depend on the number of data items to show or the number of pixels to draw? Is there a trade-off between computer speed and computer memory usage?"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-there-resource-limitations",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-are-there-resource-limitations",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.13 Why Are There Resource Limitations?",
    "text": "1.13 Why Are There Resource Limitations?\nThree different kinds of limitations:\n\nComputational capacity\nHuman perceptual and cognitive capacity\nDisplay capacity\n\nscalability: design systems to handle large amounts of data gracefully.\nDesigning systems that gracefully handle larger datasets that do not fit into core memory requires significantly more complex algorithms.\nHuman memory for things that are not directly visible is notoriously limited.\nchange blindness: when even very large changes are not noticed if we are attending to something else in our view.\ninformation density: a measure of the amount of information encoded versus the amount of unused space.\nThere is a trade-off between the benefits of showing as much as possible at once (to minimize the need for navigation and exploration) and the costs of showing too much at once (where the user is overwhelmed by visual clutter)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "1.14 Why Analyze?",
    "text": "1.14 Why Analyze?\nAnalyzing existing systems is a good stepping stone to designing new ones.\nHigh-level framework for analyzing vis use according to three questions:\n\nwhat data the user sees (data)\nwhy the user intends to use a vis tool (task)\nhow the visual encoding and interaction idioms are constructed in terms of design choices (idiom)\n\none of these analysis trios is called an instance.\nComplex vis tool usage often requires analysis in terms of a sequence of instances that are chained together. (sort > finding outliers)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.1 The Big Picture",
    "text": "2.1 The Big Picture\n\nWhat?\n\nDatasets\n\nData Types\n\nItems\nAttributes\nLinks\nPositions\nGrids\n\nData and Dataset Types\n\nTables\nNetworks & Trees\nFields\nGeometry\nClusters, Sets, Lists\n\nDataset Availability\n\nStatic\nDynamic\n\n\nAttributes\n\nAttribute Types\n\nCategorical\nOrdered\n\nOrdinal\nQuantitative\n\n\nOrdering Direction\n\nSequential\nDiverging\nCyclic"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-do-data-semantics-and-types-matter",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-do-data-semantics-and-types-matter",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.2 Why Do Data Semantics and Types Matter?",
    "text": "2.2 Why Do Data Semantics and Types Matter?\nMany aspects of vis design are driven by the kind of data that you have at your disposal.\nsemantics: the real-world meaning of the data.\ntype: the structural or mathematical interpretation of the data."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#data-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#data-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.3 Data Types",
    "text": "2.3 Data Types\nFive basic data types discussed in this book:\n\nItems\n\nIndividual entity that is discrete (such as a row in a simple table or a node in a network)\n\nAttributes\n\nSome specific property that can be measured, observed, or logged\n\nLinks\n\nA relationship between items, typically within a network\n\nPositions\n\nspatial data\n\nGrids"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#dataset-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#dataset-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.4 Dataset Types",
    "text": "2.4 Dataset Types\ndataset: any collection of information that is the target of analysis\nDataset types:\n\ntables\n\nitems\nattributes\n\nnetworks\n\nitems (nodes)\nlinks\nattributes\n\nfields\n\ngrids\npositions\nattributes\n\ngeometry\n\nitems\npositions\n\nclusters, sets and lists\n\nitems\n\n\n\n2.4.1 Tables\nflat table: each row represents and item of data, each column is an attribute of the dataset\ncell: fully specific by the combination of a row and a column (item and attribute) and contains a value for that pair.\nmultidimensional table: more complex structure for indexing into a cell, with multiple keys\n\n\n2.4.2 Networks and Trees\nnetworks: well suited for specifying that there is some kind of relationship (link) between two or more items (nodes)\nA synonym for networks is graphs.\nA synonym for node is vertex.\nA synonym for link is edge.\n\n2.4.2.1 Trees\nTrees: networks with hierarchical structure. Each child node has only one parent node pointing to it.\n\n\n\n2.4.3 Fields\nContains attribute values associated with cells. Each cell in a field contains measurements or calculations from a continuous domain.\nsampling: how frequently to take the measurements (of continuous data).\ninterpolation: how to show values in between the sampled points in a way that does not mislead. Interpolating appropriately between the measurements allows you to reconstruct a new view of the data from an arbitrary viewpoint that’s faithful to what you measured.\ndiscrete: data where a finite number of individual items exist where interpolation between them is not a meaningful concept.\nTechnically all data stored within a computer is discrete rather than continuous; however, the interesting question is whether the underlying semantics of the bits that are stored represents samples of a continuous phenomenon or intrinsically discrete data.\n\n2.4.3.1 Spatial Fields\nCell structure of the field is based on sapling at spatial positions.\nA synonym for nonspatial data is abstract data.\nscientific visualization (scivis): concerned with situations where spatial position is given with the dataset. A central concern in scivis is handling continuous data appropriately within the mathematical framework of signal processing.\ninformation visualization (infovis): concerned with situations where the use of space in a visual encoding is chosen by the designer. A central concern of infovis is determining whether the chosen idiom is suitable for the combination of data and task, leading to the use of methods from human-computer interaction and design.\n\n\n2.4.3.2 Grid Types\nWhen a field contains data created by sampling at completely regular intervals, the calls form a uniform grid.\ngrid geometry: location in space.\ngrid topology: how each cell connects with its neighboring cells.\nrectilinear grid: supports nonuniform sampling, allowing efficient storage of information that has high complexity in some areas and low complexity in others, at the cost of storing some information about the geometric location of each row.\nstructured grid: allows curvilinear shapes, where the geometric location of each cell needs to be specified.\nunstructured grid: provides complete flexibility, but the topological information about how cells connect to each other must be stored explicitly in addition to their spatial positions.\n\n\n\n2.4.4 Geometry\nSpecifies information about the shape of items with explicit spatial positions. Geometry datasets do not necessarily have attributes.\nGeometric data is sometimes shown alone, particularly when shape understanding is the primary task. In other cases, it is the backdrop against which additional information is overlaid.\n\n\n2.4.5 Other Combinations\nset: unordered group of items.\nlist: a group of items with a specified ordering.\ncluster: a grouping based on attribute similarity.\npath: an ordered set of segments formed by links connecting nodes.\ncompound network: a network with an associated tree (all the nodes in the network are the leaves of the tree, and interior nodes in the tree provide a hierarchical structure for the nodes that is different from network links between them).\ndata abstraction: describing the what part of an analysis instance that pertains to data.\n\n\n2.4.6 Dataset Availability\nstatic file (offline): the entire dataset is available all at once.\ndynamic streams (online): the dataset information trickles in over the course of the vis session."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#attribute-types",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#attribute-types",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.5 Attribute Types",
    "text": "2.5 Attribute Types\nThe major distinction is between categorical versus ordered.\nOrdered type contains further differentiation between ordinal versus quantitative.\nOrdered data might range sequentially from a minimum to a maximum value, or it might diverge in both directions from a zero point in the middle of a range, or the values may wrap around in a cycle.\nAttributes may have a hierarchical structure.\n\n2.5.1 Categorical\nDoes not have implicit ordering, but if often has hierarchical structure.\nA synonym for categorical is nominal.\nAny arbitrary external ordering can be imposed upon categorical data but these orderings are not implicit in the attribute itself.\n\n\n2.5.2 Ordered: Ordinal and Quantitative\nordered data: does have an implicit ordering.\nordinal data: we cannot do full-fledged arithmetic with, but there is a well defined ordering (shirt sizes, rankings).\nquantitative data: a subset of ordered data. A measurement of magnitude that supports arithmetic comparison (height, weight, temperature, stock price, etc). Both integers and real numbers are quantitative data.\n\n2.5.2.1 Sequential versus Diverging\nsequential: a homogeneous range from a minimum to a maximum value.\ndiverging: two sequences pointing in opposite directions that meet at a common zero point.\n\n\n2.5.2.2 Cyclic\ncyclic: where the values wrap around back to a starting point rather than continuing to increase indefinitely.\n\n\n\n2.5.3 Hierarchical Attributes\nThe attribute of time can be aggregated hierarchically from days up to weeks, months and years.\nThe geographic attribute of a postal code can be aggregated up to the level of cities or states or entire countries."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#semantics",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#semantics",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "2.6 Semantics",
    "text": "2.6 Semantics\nKnowing the type of an attribute does not tell us about its semantics.\n\n2.6.1 Key versus Value Semantics\nkey attribute: acts as an index that is used to look up value attributes.\nA synonym for key attribute is independent attribute or dimension.\nA synonym for value attribute is dependent attribute or measure.\n\n2.6.1.1 Flat Tables\nflat table: has only one key, where each item corresponds to a row in the table and any number of value attributes. Key may be categorical or ordinal attributes but quantitative attributes are typically unsuitable as keys because there is nothing to prevent them from having the same values for multiple items.\n\n\n2.6.1.2 Multidimensional Tables\nwhere multiple keys are required to look up an item. The combination of all keys must be unique for each item, even though an individual key attribute may contain duplicates.\n\n\n2.6.1.3 Fields\nIn spatial fields, spatial position acts as a quantitative key.\nmultivariate structure of fields depends on the number of value attributes.\nmultidimensional structure of fields depends on the number of keys.\na scalar field has one attribute per cell.\na vector field has two or more attributes per cell.\na tensor field has many attributes per cell.\n\n\n2.6.1.4 Scalar Fields\nare univariate, with a single attribute at each point in space.\n\n\n2.6.1.5 Vector Fields\nare multivariate with a list of multiple attributes at each point. The dimensionality of the field determines the number of components in the direction vector.\n\n\n2.6.1.6 Tensor Fields\nhave an array of attributes at each point, representing a more complex multivariate mathematical structure than the list of numbers in a vector. The full information at each point in a tensor field cannot be represented by just an arrow and would require a more complex shape such as an ellipsoid.\n\n\n2.6.1.7 Field Semantics\nCategorization of spatial fields requires knowledge of the attribute semantics and cannot be determined from type information alone.\n\n\n\n2.6.2 Temporal Semantics\ntemporal attribute: any kind of information that relates to time.\nTemporal analysis tasks often involve finding or verifying periodicity either at a predetermined scale or at some scale not known in advance.\nA temporal key attribute is usually considered to have a quantitative type, although it’s possible to consider it as ordinal data if the duration between events is not interesting.\n\n2.6.2.1 Time-Varying Data\nwhen time is one of the key attributes, as opposed to when the temporal attribute is a value rather than a key.\nThe question of whether time has key or value semantics requires external knowledge about the nature of the dataset and cannot be made purely from type information.\ntime-series dataset: an ordered sequence of time-value pairs. A special case of tables where time is the key.\ndynamic can mean a dataset has time-varying semantics or a dataset has stream type."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-1",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-1",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.1 The Big Picture",
    "text": "3.1 The Big Picture\n\nDiscovery may involve generating or verifying a hypothesis\nSearch can be classified according to whether the identity and location of targets are known or not\n\nboth are known with lookup\nthe target is known but its location is not for locate\nthe location is known but the target is not for browse\nneither the target nor the location are known for explore\n\nQueries can have three scopes:\n\nidentify one target\ncompare some targets\nsummarize all targets\n\nTargets for all kinds of data are finding trends and outliers\nFor one attribute, the target can be:\n\none value,\nthe extremes of minimum and maximum values or\nthe distribution of all values across the entire attribute\n\nFor multiple attributes the target can be:\n\ndependencies\ncorrelations or\nsimilarities between them"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze-tasks-abstractly",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-analyze-tasks-abstractly",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.2 Why Analyze Tasks Abstractly?",
    "text": "3.2 Why Analyze Tasks Abstractly?\nTransforming task descriptions from domain-specific language into abstract form allows you to reason about similarities and differences between them.\nIf you don’t to this kind of translation then everything just appears to be different. The apparent difference is misleading: there are lots of similarities in what people want to do once you strip away the surface language differences.\nThe analysis framework has verbs describing actions and nouns describing targets.\nIt is often useful to consider only one of the user’s goals at a time, in order to more easily consider the question of how a particular idiom supports that goal. To describe complex activities, you can specify a chained sequence of tasks, where the output of one becomes the input to the next.\nTask abstraction can and should guide the data abstraction.\n\n3.3 Who: Designer or User\nOn the specific side, tools are narrow: the designer has built many choices into the design of the tool itself in a way that the user cannot override.\nOn the general side, tools are flexible and users have many choices to make.\nSpecialized vis tools are designed for specific contexts with a narrow range of data configurations, especially those created through a problem-driven process.\n\n\n3.4 Actions\nThree levels of actions that define user goals:\n\nhow the vis is being used to analyze (consume or produce data)\nwhat kind of search is involved (whether target and location are known)\nwhat kind query (identify one target, compare targets, or summarize all targets)\n\n\n3.4.1 Analyze\nTwo possible goals of people who want to analyze data: consume or actively produce new information.\n\nConsume information that has already been generated as data stored in a format amenable to computation\n\nDiscover something new\nPresent something that the user already understands\nEnjoy a vis to indulge their casual interests in a topic\n\n\n\n\n3.4.1.1 Discover\nUsing a vis to find new knowledge that was not previously known, by the serendipitous observation of unexpected phenomena or motivated by existing theories, models, hypotheses or hunches.\ngenerate a new hypothesis: finding completely new things\nverify or disconfirm an existing hypothesis.\nThe discover goal is often discussed as the classic motivation for sophisticated interactive idioms, because the vis designer doesn’t know in advance what the user will need to see.\n(discover = explore, present = explain).\nWhy the vis is being used doesn’t dictate how the vis idiom is designed to achieve those goals.\n\n\n3.4.1.2 Present\nThe use of vis for the succinct communication of information, for telling a story with data, or guiding an audience through a series of cognitive operations.\nThe crucial point about the present goal is that vis is being used by somebody to communicate something specific and already understood to an audience. The knowledge communicated is already known to the presenter in advance. The output of a discover session becomes the input to a present session.\nThe decision about why is separable from how the idiom is designed: presentation can be supported through a wide variety of idiom design choices.\n\n\n3.4.1.3 Enjoy\nCasual encounters with vis.\nA vis tool may have been intended by the designer for the goal of discovery with a particular audience, but it might be used for pure enjoyment by a different group of people.\n\n\n\n3.4.2 Produce\nThe intent of the user is to generate new material.\nThere are three kinds of produce goals:\n\nannotate\nrecord\nderive\n\n\n3.4.2.1 Annotate\nthe addition of graphical or textual annotations associated with one or more preexisting visualization elements, typically as a manual action by the user. Annotation for data items could be thought of as a new attribute for them.\n\n\n3.4.2.2 Record\nSaves or captures visualization elements as persistent artifacts (screenshots, lists of bookmarked elements or locations, parameter settings, interaction logs, or annotations). An annotation made by a user can subsequently be recorded.\n\n\n3.4.2.3 Derive\nProduce new data elements based on existing data elements. There is a strong relationship between the form of the data (the attribute and dataset types) and what kinds of vis idioms are effective at displaying it.\nDon’t just draw what you’re given; decide what the right thing to show is, create it with a series of transformations from the original dataset, and draw that.\nA synonym for derive is transform.\nderived attributes extend the dataset beyond the original set of attributes that it contains.\n\n\n\n3.4.3 Search\nThe classification of search into four alternatives is broken down according to whether the identity and location of the search target is already known. The verb find is often used as a synonym in descriptions of search tasks, implying a successful outcome.\n\n3.4.3.1 Lookup\nUsers already know both what they’re looking for and where it is.\n\n\n3.4.3.2 Locate\nTo find a known target at an unknown location.\n\n\n3.4.3.3 Browse\nWhen users don’t know exactly what they’re looking for but they do have a location in mind of where to look for it.\n\n\n3.4.3.4 Explore\nWhen users don’t know what they’re looking for and are not even sure of the location.\n\n\n\n3.4.4 Query\nOnce a target or set of targets for a search has been found, a low-level user goal is to query these targets at one of three scopes: identify (single target), compare (multiple targets) or summarize (all targets).\n\n3.4.4.1 Identify\nIf a search returns known targets either by lookup or locate then identify returns their characteristics.\nIf a search returns targets matching particular characteristics either by browse or explore, then identify returns specific references.\n\n\n3.4.4.2\nComparison tasks are typically more difficult than identify tasks and require more sophisticated idioms to support the user.\n\n\n3.4.4.3 Summarize\nA synonym for summarize is overview: to provide a comprehensive view of everything (verb) and a summary display of everything (noun)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#targets",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#targets",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.5 Targets",
    "text": "3.5 Targets\nTarget: some aspect of the data that is of interest to the user.\nTargets are nouns whereas actions are verbs.\nThree high-level targets are very broadly relevant for all kinds of data:\n\na trend: high-level characterization of a pattern in the data. (A synonym for trend is pattern)\noutliers: data that don’t fit the trend, synonyms for outliers are anomalies, novelties, deviants and surprises.\nfeatures: definition dependent on the task, any particular structures of interest\n\nThe lowest-level target for an attribute is to find an individual value. Another target is to find the extremes (min/max across a range). Another target is the distribution of all values for an attribute.\nSome targets encompass the scope of multiple attributes:\n\ndependency: the values for the first attribute directly depend on those of the second.\ncorrelation: a tendency for the values of the second attribute to be tied to those of the first.\nsimilarity: a quantitative measurement calculated on all values of two attributes, allowing attributes to be ranked with respect to how similar, or different, they are from each other.\n\nNetwork targets:\n\ntopology: the structure of interconnections in a network.\npath: of one or more links that connects two nodes.\nshape: of spatial data."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#how-a-preview",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#how-a-preview",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "3.6 How: A Preview",
    "text": "3.6 How: A Preview\n\nEncode\n\nArrange\n\nExpress\nSeparate\nOrder\nAlign\nUse (spatial data)\n\nMap\n\nColor\nSize, Angle, Curvature, …\nShape\nMotion\n\n\nManipulate\n\nChange\nSelect\nNavigate\n\nFacet\n\nJuxtapose\nPartition\nSuperimpose\n\nReduce\n\nFilter\nAggregate\nEmbed\n\n\nThe rest of this book defines, describes and discusses these choices in depth.\nThe Strahler number is a measure of node importance. Very central nodes have large Strahler numbers, whereas peripheral nodes have low values."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-2",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-2",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.1 The Big Picture",
    "text": "4.1 The Big Picture\nFour nested levels of design:\n\nDomain situation\n\nTask and data abstraction\n\nVisual encoding and interaction idiom\n\nAlgorithm"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-validate",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-validate",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.2 Why Validate?",
    "text": "4.2 Why Validate?\nThe vis design space is huge, and most designs are ineffective."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#four-levels-of-design",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#four-levels-of-design",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.3 Four Levels of Design",
    "text": "4.3 Four Levels of Design\n\nDomain situation: where you consider the details of a particular application domain for vis\n\nWhy-why abstraction level (Data-task): where you map those domain-specific problems and data into forms that are independent of the domain\n\nHow level (visual encoding/interaction idiom): specify the approach to visual encoding and interaction\n\nAlgorithm level: instantiate idioms computationally\n\n\n\n\nThe four levels are nested, the output from an upstream level above is input to the downstream level below. A block is the outcome of the design process at that level. Choosing the wrong block at an upstream level inevitable cascades to all downstream levels.\nVis design is usually a highly iterative refinement process, where a better understanding of the blocks at one level will feed back and forward into refining the blocks at the other levels.\n\n4.3.1 Domain Situation\ndomain situation: a group of target users, their domain interest, their questions, and their data.\ndomain: a particular field of interest of the target users of a vis tool.\nSituation blocks are identified.\nThe outcome of the design process is an understanding that the designer reaches about the needs of the user. The outcome of identifying a situation block is a detailed set of questions asked about or actions carried out by the target users, about a possible heterogeneous collection of data that’s also understood in detail.\nMethods include: interviews, observations, or careful research about target users within a specific domain.\nWorking closely with a specific target audience to iteratively refine a design is called user-centered design or human-centered design.\nWhat users say they do when reflecting on their past behavior gives you an incomplete picture compared with what they actually do if you observe them.\n\n\n4.3.2 Task and Data Abstraction\nAbstracting into the domain-independent vocabulary allows you to realize how domain situation blocks that are described using very different language might have similar reasons why the user needs the vis tool and what data it shows.\nTask blocks are identified by the designer as being suitable for a particular domain situation block, just as the situation blocks themselves are identified at the level above.\nAbstract data blocks are designed.\nThe data abstraction level requires you to consider whether and how the same dataset provided by a user should be transformed into another form.\nYour goal is to determine which data type would support a visual representation of it that addresses the user’s problem.\nExplicitly considering the choices made in abstracting from domain-specific to generic tasks and data can be very useful in the vis design process.\n\n\n4.3.3 Visual Encoding and Interaction Idiom\nidiom: a distinct way to create and manipulate the visual representation of the abstract data block that you chose at the previous level, guided by the abstract tasks that you also identified at that level.\nthe visual encoding idiom controls exactly what users see.\nthe interaction idiom controls how users change what they see.\nIdiom blocks are designed.\nThe nested model emphasizes identifying task abstractions and deciding on data abstractions in the previous level exactly so that you can use them to rule out many of the options as being a bad match for the goals of the users. You should make decisions about good and bad matches based on understanding human abilities, especially in terms of visual perception and memory.\n\n\n4.3.4 Algorithm\nalgorithm: a detailed procedure that allows a computer to automatically carry out the desired goal.\nAlgorithm blocks are designed."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#angles-of-attack",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#angles-of-attack",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.4 Angles of Attack",
    "text": "4.4 Angles of Attack\nWith problem-driven work, you start at the top domain situation level and work your way down through abstraction, idiom, and algorithm decisions.\nIn technique-driven work, you work at one of the bottom two levels, idiom or algorithm design, where your goal is to invent new idioms that better support existing abstractions, or new algorithms that better support existing idioms."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#threats-the-validity",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#threats-the-validity",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.5 Threats the Validity",
    "text": "4.5 Threats the Validity\nthreats to validity: different fundamental reasons why you might have made the wrong choices.\n\nWrong problem: You (designer) misunderstood their (target users) needs.\nWrong abstraction: You’re showing them the wrong thing.\nWrong idiom: The way you show it doesn’t work.\nWrong algorithm: Your code is too slow."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#validation-approaches",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#validation-approaches",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "4.6 Validation Approaches",
    "text": "4.6 Validation Approaches\n\n4.6.1 Domain Validation\nThe primary threat is that the problem is mischaracterized; the target users do not in fact have these problems (that the designer asserts would benefit from vis tool support).\nfield study: where the investigator observes how people act in real-world settings, rather than by bringing them into a laboratory setting. Field studies for domain situation assessment often involve gathering qualitative data through semi-structured interviews.\nOne downstream form of validation is adoption rates of the vis tool.\n\n\n4.6.2 Abstraction Validation\nThe threat at this level is that the identified task abstraction blocks and designed data abstraction blocks do not solve the characterized problems of the target audience. The key aspect of validation against this threat is that the system must be tested by target users doing their own work, rather than doing an abstract task specified by the designers of the vis system.\n\n\n4.6.3 Idiom Validation\nThe threat at this level is that the chosen idioms are not effective at communicating the desired abstraction to the person using the system. One immediate validation approach is to carefully justify the design of the idiom with respect to known perceptual and cognitive principles.\nA downstream approach to validate against this threat is to carry out a lab study: a controlled experiment in a laboratory setting.\n\n\n4.6.4 Algorithm Validation\nThe primary threat at this level is that the algorithm is suboptimal in terms of time or memory performance, either to a theoretical minimum or in comparison with previously proposed algorithms.\nAn immediate form of validation is to analyze the computational complexity of the algorithm, using the standard approaches from the computer science literature.\nThe downstream form of validation is to measures the wall-clock time and memory performance of the implemented algorithm.\n\n\n4.6.5 Mismatches\nA common problem in weak vis projects is a mismatch between the level at which the benefit is claimed (for example, visual encoding idiom) and the validation methodologies chosen (for example, wall-clock timings of the algorithm)."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-3",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#the-big-picture-3",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.1 The Big Picture",
    "text": "5.1 The Big Picture\nMarks are basic geometric elements that depict items or links, and channels control their appearance. Channels that perceptually convey magnitude information are a good match for ordered data, and those that convey identity information are a good match for categorical data.\n\nMagnitude Channels: Ordered Attributes (Most effective to least):\n\nPosition on common scale\nPosition on unaligned scale\nLength (1D size)\nTilt/angle\nArea (2D size)\nDepth (3D position)\nColor luminance\nColor saturation\nCurvature\nVolume (3D size)\n\nIdentity Channels: Categorical Attributes\n\nSpatial Region\nColor hue\nMotion\nShape"
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-marks-and-channels",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#why-marks-and-channels",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.2 Why Marks and Channels?",
    "text": "5.2 Why Marks and Channels?\nThe core of the design space of visual encodings can be described as an orthogonal combination of two aspects: graphical elements called marks and visual channels to control their appearance."
  },
  {
    "objectID": "posts/2023-05-20-visualization-analysis-and-design/index.html#defining-marks-and-channels",
    "href": "posts/2023-05-20-visualization-analysis-and-design/index.html#defining-marks-and-channels",
    "title": "Visualization Analysis & Design - Excerpts",
    "section": "5.3 Defining Marks and Channels",
    "text": "5.3 Defining Marks and Channels\nmark: a basic graphical element in an image\n\nPoints (0 dimensional)\nLines (1D)\nArea (2D)\nVolume (3D)\n\nchannel: is a way to control the appearance of marks, independent of the dimensionality of the geometric primative.\n\nPosition\n\nHorizontal\nVertical\nBoth\n\nShape\nSize\n\nLength\nArea\nVolume\n\nColor\nTilt (or Angle)\n\nA single quantitative attribute can be encoded with vertical spatial position. Bar charts show this and the horizontal spatial position channel for the categorical attribute.\nScatterplots encode two quantitative attributes using point marks and both vertial and horizontal spatial position. A third categorical attribute is encoded by adding color to the scatterplot. Adding the visual channel of size encodes a fourth quantitative attribute as well.\nHigher-dimensional mark types usually have built-in constraints (on size and shape) that arise from the way that they are defined. An area or line mark cannot be size or shape coded, but a point can.\n\n5.3.1 Channel Types"
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "",
    "text": "Side view of Jalen Hurts walking on the Eagles sideline with Kansas City Chiefs-colored confetti falling around him"
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#background-and-goals",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "Background and Goals",
    "text": "Background and Goals\nIt’s been 5 years since I last explored NFL’s play-by-play data. It’s also been 5 years since my Eagles won the Super Bowl, which will be played in less than 24 hours from now. Go Birds.\nIt’s been so long since I’ve blogged that fastpages, the blogging library I use, has been deprecated.\nI have thoroughly enjoyed some of the statistical analyses put forth by fans of the NFL this year. My favorite analyst is Deniz Selman, a fellow Eagles fan who makes these beautiful data presentations.\nI also appreciate Deniz’ critique of analysis-without-context that often negates the brilliance of Jalen Hurts:\n\n\nAs I’ve been trying to say all year, EPA/dropback is not nearly as valuable a metric when the offense lets the QB decide whether it’s a “dropback” or not during the play by reading the defense, and that QB is the absolute best at making that decision. #FlyEaglesFly\n\n— Deniz Selman (@denizselman33) February 11, 2023\n\n\nMy second favorite analyst is Ben Baldwin, AKA Computer Cowboy especially his 4th down analysis realtime during games.\nThere has been an onslaught of statistical advances in the NFL since I last explored play-by-play data and I’m excited to learn as much as I can. In particular, I’d like to get a hang of the metrics EPA (Expected Points Added) and DVOA (Defense-adjusted Value Over Average), which may not necessarily intersect with my play-by-play analysis (I believe Football Outsiders is the proprietor of that formula).\nI’d also like to use this project to practice more advanced SQL queries than I’m used to. Given the complexity of the play-by-play dataset (by team, down, field position, etc.) I’m hoping I can get those reps in.\nLastly, I’d like to explore data presentation with these statistics using R, python, Adobe Illustrator and Photoshop. I’ve been inspired by simple, elegant graphics like those made by Peter Gorman in Barely Maps and bold, picturesque statistics posted by PFF on twitter:\n\n\nThe most clutch pass rushers face off in the Super Bowl pic.twitter.com/o50lV9Bkgk\n\n— PFF (@PFF) February 12, 2023\n\n\nI’ll work on this project in this post throughout this year–and maybe beyond if it fuels me with enough material–or it’ll fork off into something entirely new or different.\nI’ll start off by next exploring the schema of the play-by-play dataset."
  },
  {
    "objectID": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#documenting-the-nfl-play-by-play-dataset-fields",
    "href": "posts/2023-02-11-nflverse/2023-02-11-nflverse.html#documenting-the-nfl-play-by-play-dataset-fields",
    "title": "Exploring NFL Play-by-Play Data with SQL and Python",
    "section": "Documenting the NFL Play-by-Play Dataset Fields",
    "text": "Documenting the NFL Play-by-Play Dataset Fields\nIn this section, I describe the fields in the 2022 NFL Play-by-Play Dataset. Not all of the fields are intuitive or immediately useful, so not all 372 column descriptions will be listed.\n\nimport pandas as pd\nimport numpy as np\n\n\n# load the data\nfpath = \"../../../nfl_pbp_data/play_by_play_2022.csv\"\npbp_2022 = pd.read_csv(fpath, low_memory=False)\n\npbp_2022.head()\n\n\n\n\n\n  \n    \n      \n      play_id\n      game_id\n      old_game_id\n      home_team\n      away_team\n      season_type\n      week\n      posteam\n      posteam_type\n      defteam\n      ...\n      out_of_bounds\n      home_opening_kickoff\n      qb_epa\n      xyac_epa\n      xyac_mean_yardage\n      xyac_median_yardage\n      xyac_success\n      xyac_fd\n      xpass\n      pass_oe\n    \n  \n  \n    \n      0\n      1\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NaN\n      NaN\n      NaN\n      ...\n      0\n      1\n      0.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      43\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.443521\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      68\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      1.468819\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.440373\n      -44.037291\n    \n    \n      3\n      89\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.492192\n      0.727261\n      6.988125\n      6.0\n      0.60693\n      0.227598\n      0.389904\n      61.009598\n    \n    \n      4\n      115\n      2022_01_BAL_NYJ\n      2022091107\n      NYJ\n      BAL\n      REG\n      1\n      NYJ\n      home\n      BAL\n      ...\n      0\n      1\n      -0.325931\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.443575\n      -44.357494\n    \n  \n\n5 rows × 372 columns\n\n\n\nThe 2022 NFL Play-by-Play dataset has 50147 rows (plays) and 372 columns.\n\npbp_2022.shape\n\n(50147, 372)\n\n\nplay_id is an identifier for each play in each game. It not a unique identifier as there are many duplicates. There are 4597 unique play_id values in this dataset.\n\nlen(pbp_2022.play_id.unique())\n\n4597\n\n\ngame_id is an identifier for each game in the dataset in the format of {year}_{week}_{away_team}_{home_team}. There are 284 unique games in this dataset.\n\nlen(pbp_2022.game_id.unique()), pbp_2022.game_id[1]\n\n(284, '2022_01_BAL_NYJ')\n\n\nThere are 32 unique home_teams and away_teams.\n\nlen(pbp_2022.home_team.unique()), len(pbp_2022.away_team.unique())\n\n(32, 32)\n\n\nThere are two season_type values: 'REG' for regular season and 'POST' for postseason.\n\npbp_2022.season_type.unique()\n\narray(['REG', 'POST'], dtype=object)\n\n\nThere are 22 week values: - 18 regular season weeks (17 games + 1 bye) - 4 postseason weeks - Wild Card Weekend - Divisional Playoffs - Conference Championships - Super Bowl\n\npbp_2022.week.unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22])\n\n\nI believe posteam stands for the team that has possession of the ball. There are 32 unique teams that can have possession of the ball in a game, and in some cases the posteam is nan.\n\nlen(pbp_2022.posteam.unique()), pbp_2022.posteam.unique()\n\n(33,\n array([nan, 'NYJ', 'BAL', 'BUF', 'LA', 'CAR', 'CLE', 'SEA', 'DEN', 'MIN',\n        'GB', 'IND', 'HOU', 'JAX', 'WAS', 'KC', 'ARI', 'LAC', 'LV', 'NE',\n        'MIA', 'ATL', 'NO', 'NYG', 'TEN', 'DET', 'PHI', 'PIT', 'CIN',\n        'CHI', 'SF', 'DAL', 'TB'], dtype=object))\n\n\nposteam_type has values 'home', 'away' and nan.\n\nlen(pbp_2022.posteam_type.unique()), pbp_2022.posteam_type.unique()\n\n(3, array([nan, 'home', 'away'], dtype=object))\n\n\ndefteam lists any of the 32 teams on defense on a given play. It can also have the value nan.\n\nlen(pbp_2022.defteam.unique()), pbp_2022.defteam.unique()\n\n(33,\n array([nan, 'BAL', 'NYJ', 'LA', 'BUF', 'CLE', 'CAR', 'DEN', 'SEA', 'GB',\n        'MIN', 'HOU', 'IND', 'WAS', 'JAX', 'ARI', 'KC', 'LV', 'LAC', 'MIA',\n        'NE', 'NO', 'ATL', 'TEN', 'NYG', 'PHI', 'DET', 'CIN', 'PIT', 'SF',\n        'CHI', 'TB', 'DAL'], dtype=object))\n\n\nside_of_field can be nan, any of the 32 team abbreviations, or 50 (midfield).\n\nlen(pbp_2022.side_of_field.unique()), pbp_2022.side_of_field.unique()\n\n(34,\n array([nan, 'BAL', 'NYJ', 'LA', 'BUF', '50', 'CLE', 'CAR', 'DEN', 'SEA',\n        'GB', 'MIN', 'HOU', 'IND', 'WAS', 'JAX', 'ARI', 'KC', 'LV', 'LAC',\n        'MIA', 'NE', 'NO', 'ATL', 'TEN', 'NYG', 'PHI', 'DET', 'CIN', 'PIT',\n        'SF', 'CHI', 'TB', 'DAL'], dtype=object))\n\n\nyardline_100 can be nan or between 1 and 99.\n\nlen(pbp_2022.yardline_100.unique()), np.nanmin(pbp_2022.yardline_100), np.nanmax(pbp_2022.yardline_100)\n\n(100, 1.0, 99.0)\n\n\nThere are 61 game_date values.\n\nlen(pbp_2022.game_date.unique()), pbp_2022.game_date[0]\n\n(61, '2022-09-11')\n\n\nquarter_seconds_remaining is between 0 and 900 (15 minutes).\n\npbp_2022.quarter_seconds_remaining.min(), pbp_2022.quarter_seconds_remaining.max()\n\n(0, 900)\n\n\nhalf_seconds_remaining is between 0 and 1800 (30 minutes).\n\npbp_2022.half_seconds_remaining.min(), pbp_2022.half_seconds_remaining.max()\n\n(0, 1800)\n\n\ngame_seconds_remaining is between 0 and 3600 (60 minutes).\n\npbp_2022.game_seconds_remaining.min(), pbp_2022.game_seconds_remaining.max()\n\n(0, 3600)\n\n\ngame_half is either Half1 (first half), Half2 (second half), or Overtime.\n\npbp_2022.game_half.unique()\n\narray(['Half1', 'Half2', 'Overtime'], dtype=object)\n\n\nquarter_end is either 1 (True) or 0 (False).\n\npbp_2022.quarter_end.unique(), pbp_2022.query('quarter_end == 1').desc[41]\n\n(array([0, 1]), 'END QUARTER 1')\n\n\ndrive is the current number of drives in the game (including both teams) as well as nan values.\n\npbp_2022.drive.unique()\n\narray([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n       26., 27., 28., 29., 30., 31., 32., 33., 34., 35.])\n\n\nsp teams seems to indicate whether the play involves the Special Teams unit, either 1 (True) or 0 (False).\n\npbp_2022.sp.unique(), pbp_2022.query('sp == 1').desc[32]\n\n(array([0, 1]),\n '(3:19) 9-J.Tucker 24 yard field goal is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\nquarter indicates the current quarter of the play. quarter == 5 represents Overtime.\n\npbp_2022.qtr.unique()\n\narray([1, 2, 3, 4, 5])\n\n\ndown represents the current down of the play (nan, 1st, 2nd, 3rd or 4th).\n\npbp_2022.down.unique()\n\narray([nan,  1.,  2.,  3.,  4.])\n\n\ngoal_to_go indicates whether this play is 1st & Goal, 2nd & Goal, 3rd & Goal or 4th & Goal, either 1 (True) or 0 (False).\n\npbp_2022.goal_to_go.unique()\n\narray([0, 1])\n\n\ntime is the minutes:seconds formatted time left in the current quarter.\n\npbp_2022.head().time.unique()\n\narray(['15:00', '14:56', '14:29', '14:25'], dtype=object)\n\n\nyrdln is a formatted string of team abbreviation and yard number.\n\npbp_2022.yrdln.unique()\n\narray(['BAL 35', 'NYJ 22', 'NYJ 41', ..., 'NYJ 3', 'CIN 6', 'MIN 12'],\n      dtype=object)\n\n\nydstogo is the number of yards before the next first down.\n\npbp_2022.ydstogo.unique()\n\narray([ 0, 10,  5, 15,  6,  2,  1, 12,  9, 19, 11,  3,  8,  4, 16, 17,  7,\n       20, 14, 18, 13, 22, 26, 24, 21, 25, 23, 28, 30, 27, 31, 38, 36, 29,\n       34, 35, 32, 33])\n\n\nydsnet is the net yards (yards gained - yards lost) of the current drive.\n\npbp_2022.ydsnet.unique()\n\narray([ nan,  14.,  21.,   7.,   1.,  15.,   9.,  16.,  44.,  18.,  62.,\n        48.,   3.,  11.,   4.,  88.,  75.,  23.,  43.,  -2.,  38.,   0.,\n        45.,  60.,  13.,   6.,  -1.,  58.,  25.,  89.,  59.,  19.,  66.,\n        29.,  -4.,  24.,   2.,  12.,  42.,  78.,  52.,  57.,  64.,  35.,\n        -3.,  70.,  77.,  72.,  50.,  37.,  31.,  -6.,  32.,  -5.,  20.,\n        79.,  74.,  34.,  65.,   8.,  47.,   5.,  69.,  53.,  33.,  76.,\n        80., -16.,  71.,  68.,  55.,  27.,  90.,  86.,  17.,  30.,  67.,\n        63.,  73.,  61., -13.,  92.,  40.,  22.,  -7.,  39.,  41.,  28.,\n        82.,  49.,  10.,  36.,  46.,  84.,  54., -23., -11.,  83.,  26.,\n        94.,  87., -10.,  85.,  51., -14.,  56.,  -8.,  81.,  -9.,  93.,\n       -12., -15., -17.,  91.,  99.,  98., -19.,  96.,  95.,  97., -20.,\n       -25.])\n\n\ndesc is a narrative description of the current play.\n\npbp_2022.head().desc[1]\n\n'9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).'\n\n\nplay_type is either nan or one of 9 different play types, including no_play.\n\nlen(pbp_2022.play_type.unique()), pbp_2022.play_type.unique()\n\n(10,\n array([nan, 'kickoff', 'run', 'pass', 'punt', 'no_play', 'field_goal',\n        'extra_point', 'qb_kneel', 'qb_spike'], dtype=object))\n\n\nyards_gained is the number of yards gained (positive) or lost (negative) on the current play. It does not capture yards gained or lost due to a penalty.\n\npbp_2022.head().yards_gained, pbp_2022.yards_gained.min()\n\n(0     NaN\n 1     0.0\n 2    19.0\n 3     0.0\n 4     5.0\n Name: yards_gained, dtype: float64,\n -26.0)\n\n\nshotgun indicates whether the quarterback was in shotgun position, either 1 (True) or 0 (False).\n\npbp_2022.shotgun.unique(), pbp_2022.query('shotgun == 1').desc[3]\n\n(array([0, 1]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nno_huddle indicates whether the team huddled before the snap, either 1 (True) or 0 (False).\n\npbp_2022.no_huddle.unique(), pbp_2022.query('no_huddle == 1').desc[3]\n\n(array([0, 1]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nqb_dropback indicates whether the quarterback drops back on the play, either 1 (True), 0 (False) or nan.\n\npbp_2022.qb_dropback.unique(), pbp_2022.query('qb_dropback == 1').desc[3]\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nqb_kneel indicates whether the quarterback kneels on the play, either 1 (True) or 0 (False).\n\npbp_2022.qb_kneel.unique(), pbp_2022.query('qb_kneel == 1').desc[176]\n\n(array([0, 1]), '(:59) 8-L.Jackson kneels to NYJ 43 for -1 yards.')\n\n\nqb_spike indicates whether the quarterback spikes the ball on the play, either 1 (True) or 0 (False).\n\npbp_2022.qb_spike.unique(), pbp_2022.query('qb_spike == 1').desc[520]\n\n(array([0, 1]),\n '(:29) (No Huddle) 7-J.Brissett spiked the ball to stop the clock.')\n\n\nqb_scramble indicates whether the quarterback scrambles on the play, either 1 (True) or 0 (False). It looks like a scramble is not the same as a designed quarterback run, so I’ll dig deeper into this before using this field in analyses.\n\npbp_2022.qb_scramble.unique()\n\narray([0, 1])\n\n\npass_length is either nan, 'short' or 'deep'. I’ll first understand what distance (in yards) corresponds to these designations before I use this field in analyses.\n\npbp_2022.pass_length.unique()\n\narray([nan, 'short', 'deep'], dtype=object)\n\n\npass_location is either nan, 'left', 'right', or 'middle'.\n\npbp_2022.pass_location.unique()\n\narray([nan, 'left', 'right', 'middle'], dtype=object)\n\n\nair_yards is the number of yards a quarterback’s pass traveled in the air. It can be positive, zero or negative.\n\npbp_2022.air_yards.unique()\n\narray([ nan,   0.,  -4.,   3.,   2.,  16.,  11.,   5.,  21.,  14.,  -1.,\n         1.,   7.,   6.,  15.,  -3.,   8.,  10.,  50.,  27.,  25.,  -5.,\n        31.,  -6.,  17.,  51.,  13.,   4.,  12.,  36.,   9.,  32.,  18.,\n        22.,  -2.,  23.,  45.,  40.,  52.,  -7.,  26.,  29.,  20.,  47.,\n        24.,  30.,  28.,  37.,  39.,  -8.,  19.,  41.,  38., -12.,  42.,\n       -10.,  46.,  35.,  33.,  -9.,  34.,  44.,  43.,  53.,  57.,  48.,\n        49.,  54.,  58.,  56.,  59.,  55.,  61., -18., -54., -13.,  62.,\n        65., -20., -16.])\n\n\nyards_after_catch is the number of yards the receiver gains or loses after catching the ball.\n\npbp_2022.yards_after_catch.unique()\n\narray([ nan,   8.,   1.,   6.,   0.,   3.,   5.,   4.,  12.,   9.,  10.,\n        -4.,  18.,   7.,  15.,   2.,  11.,  13.,  -1.,  29.,  30.,  27.,\n        28.,  16.,  26.,  24.,  25.,  -5.,  41.,  14.,  22.,  19.,  17.,\n        21.,  32.,  20.,  -2.,  35.,  -3.,  51.,  66.,  38.,  46.,  23.,\n        31.,  37.,  68.,  -6.,  33.,  52.,  75.,  34.,  71.,  44.,  61.,\n        60.,  58.,  48.,  50.,  53.,  39.,  62.,  47.,  -7.,  42.,  40.,\n        36.,  49.,  70.,  45.,  65.,  43.,  74., -10.,  -9.])\n\n\nrun_location is either nan, 'left', 'right', or 'middle'.\n\npbp_2022.run_location.unique()\n\narray([nan, 'left', 'right', 'middle'], dtype=object)\n\n\nrun_gap represents which offensive line gap the runner ran through. It is either nan, 'end', 'tackle' or 'guard'. I’ll have to dig a bit deeper (look at some video corresponding to the run plays) to understand if 'guard' represents the A (gap between center and guard) or B gap (gap between guard and tackle), if 'tackle' represents the B or C gap (gap between tackle and end), and if 'end' represents the C or D (gap outside the end) gap.\n\npbp_2022.run_gap.unique()\n\narray([nan, 'end', 'tackle', 'guard'], dtype=object)\n\n\nfield_goal_result is either nan, 'made', 'missed', or 'blocked'.\n\npbp_2022.field_goal_result.unique()\n\narray([nan, 'made', 'missed', 'blocked'], dtype=object)\n\n\nkick_distance is the distance of the kick in yards for the following play_type values: 'punt', 'field_goal', 'extra_point', and 'kickoff'. Looking through the data, not all 'kickoff's have a kick_distance value.\n\npbp_2022.kick_distance.unique(), pbp_2022.query('kick_distance.notnull()').play_type.unique()\n\n(array([nan, 45., 40., 48., 24., 50., 56., 41., 33., 20., 49., 43.,  7.,\n        36., 57., 25., 39., 60., 62., 61., 44., 46., 58., 26., 34., 64.,\n        30., 47., 54., 28., 53., 38., 29., 70., 37., 27., 52., 42., 63.,\n        51., 23., 55., 59., 69., 66., 14., 32., 35.,  0., 31., 67., 74.,\n        19., 10., 22., 12.,  8.,  5., -1., 73., 65.,  3., 21.,  9., 16.,\n        15., 13., 18., 17.,  6., 77., 68., 11., 71., 79.]),\n array(['punt', 'field_goal', 'extra_point', 'kickoff'], dtype=object))\n\n\nextra_point_result is either nan, 'good', 'failed' or 'blocked'.\n\npbp_2022.extra_point_result.unique()\n\narray([nan, 'good', 'failed', 'blocked'], dtype=object)\n\n\ntwo_point_conv_result, the result of a two-point conversion is either nan, 'failure' or 'success'.\n\npbp_2022.two_point_conv_result.unique()\n\narray([nan, 'failure', 'success'], dtype=object)\n\n\nhome_timeouts_remaining is the number of timeouts the home team has left. It is either 3, 2, 1, or 0.\n\npbp_2022.home_timeouts_remaining.unique()\n\narray([3, 2, 1, 0])\n\n\naway_timeouts_remaining is the number of timeouts the away team has left. It is either 3, 2, 1, or 0.\n\npbp_2022.away_timeouts_remaining.unique()\n\narray([3, 2, 1, 0])\n\n\ntimeout indicates if a team calls a timeout, either 1 (True) or 0 (False).\n\npbp_2022.timeout.unique(), pbp_2022.query('timeout == 1').desc[13]\n\n(array([nan,  0.,  1.]), 'Timeout #1 by BAL at 09:56.')\n\n\ntimeout_team indicates which team called the timeout, and has 33 unique values—1 nan and 32 team abbreviations.\n\n(pbp_2022.timeout_team.unique(), \npbp_2022.query('timeout == 1').desc[13], \npbp_2022.query('timeout == 1').timeout_team[13])\n\n(array([nan, 'BAL', 'NYJ', 'LA', 'BUF', 'CLE', 'CAR', 'DEN', 'SEA', 'GB',\n        'MIN', 'IND', 'HOU', 'WAS', 'JAX', 'KC', 'ARI', 'LAC', 'LV', 'NE',\n        'MIA', 'ATL', 'NO', 'NYG', 'TEN', 'PHI', 'DET', 'PIT', 'CIN', 'SF',\n        'CHI', 'DAL', 'TB'], dtype=object),\n 'Timeout #1 by BAL at 09:56.',\n 'BAL')\n\n\ntd_team indicates which team scored the touchdown. It is nan or one of 32 team abbreviations.\n\n(pbp_2022.td_team.unique(),\npbp_2022.query('td_team.notnull()').td_team[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, 'BAL', 'NYJ', 'BUF', 'LA', 'CLE', 'CAR', 'SEA', 'DEN', 'MIN',\n        'GB', 'HOU', 'IND', 'WAS', 'JAX', 'KC', 'ARI', 'LAC', 'LV', 'MIA',\n        'NE', 'NO', 'ATL', 'TEN', 'NYG', 'DET', 'PHI', 'PIT', 'CIN', 'SF',\n        'CHI', 'TB', 'DAL'], dtype=object),\n 'BAL',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\ntd_player_name indicates which player scored the touchdown. It is nan or one of 416 players who scored a touchdown in the 2022 season.\n\n(pbp_2022.td_player_name.unique()[:5],\nlen(pbp_2022.td_player_name.unique()),\npbp_2022.query('td_team.notnull()').td_player_name[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, 'D.Duvernay', 'R.Bateman', 'T.Conklin', 'G.Davis'],\n       dtype=object),\n 417,\n 'D.Duvernay',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\ntd_player_id indicates the id of the player who scored the touchdown. There are 422 unique player IDs. Later on, I’ll look into why there are 5 fewer player IDs than player names.\n\n(pbp_2022.td_player_id.unique()[:5],\nlen(pbp_2022.td_player_id.unique()),\npbp_2022.query('td_team.notnull()').td_player_name[68],\n pbp_2022.query('td_team.notnull()').td_player_id[68],\npbp_2022.query('td_team.notnull()').desc[68])\n\n(array([nan, '00-0036331', '00-0036550', '00-0034270', '00-0036196'],\n       dtype=object),\n 423,\n 'D.Duvernay',\n '00-0036331',\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\nposteam_timeouts_remaining is the number of timeouts remaining for the team with ball possession. It can be nan, 3, 2, 1, or 0.\n\npbp_2022.posteam_timeouts_remaining.unique()\n\narray([nan,  3.,  2.,  0.,  1.])\n\n\ndefteam_timeouts_remaining is the number of timeouts remaining for the team on defense. It can be nan, 3, 2, 1, or 0.\n\npbp_2022.defteam_timeouts_remaining.unique()\n\narray([nan,  3.,  2.,  1.,  0.])\n\n\ntotal_home_score is the total number of points scored by the home team.\n\npbp_2022.total_home_score.unique()[:5]\n\narray([0, 3, 9, 6, 7])\n\n\ntotal_away_score is the total number of points scored by the away team.\n\npbp_2022.total_away_score.unique()[:5]\n\narray([ 0,  3,  9, 10, 16])\n\n\nposteam_score is the total number of points scored by the team with ball possession on the current play.\n\npbp_2022.posteam_score.unique()[:5]\n\narray([nan,  0.,  3.,  9., 10.])\n\n\ndefteam_score is the total number of points scored by the team on defense on the current play.\n\npbp_2022.defteam_score.unique()[:5]\n\narray([nan,  0.,  3., 10., 17.])\n\n\nscore_differential is the difference between posteam_score and defteam_score.\n\npbp_2022.score_differential.unique()[:5]\n\narray([nan,  0., -3.,  3.,  9.])\n\n\npunt_blocked indicates if the punt was blocked. It is either nan, 1 (True) or 0 (False).\n\npbp_2022.punt_blocked.unique(),pbp_2022.query('punt_blocked == 1').desc[3236]\n\n(array([nan,  0.,  1.]),\n '(5:06) 11-R.Dixon punt is BLOCKED by 44-T.Andersen, Center-42-M.Orzech, RECOVERED by ATL-9-L.Carter at LA 26. 9-L.Carter for 26 yards, TOUCHDOWN.')\n\n\nfirst_down_rush indicates whether a first down was achieved by a rushing play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_rush.unique(), \n pbp_2022.query('first_down_rush == 1').desc[2],\n pbp_2022.query('first_down_rush == 1').play_type[2])\n\n(array([nan,  0.,  1.]),\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).',\n 'run',\n nan)\n\n\nfirst_down_pass indicates whether a first down was achieved by a passing play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_pass.unique(), \n pbp_2022.query('first_down_pass == 1').desc[26],\n pbp_2022.query('first_down_pass == 1').play_type[26])\n\n(array([nan,  0.,  1.]),\n '(6:01) 19-J.Flacco pass deep left to 8-E.Moore to NYJ 41 for 24 yards (32-M.Williams).',\n 'pass')\n\n\nfirst_down_penalty indicates whether a first down was achieved by a penalty. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.first_down_penalty.unique(), \n pbp_2022.query('first_down_penalty == 1').desc[17],\n pbp_2022.query('first_down_penalty == 1').play_type[17])\n\n(array([nan,  0.,  1.]),\n '(8:31) (Shotgun) 19-J.Flacco pass incomplete deep left to 8-E.Moore. PENALTY on BAL-44-M.Humphrey, Illegal Contact, 5 yards, enforced at NYJ 12 - No Play.',\n 'no_play')\n\n\nthird_down_converted indicates if the team with ball possession on third down got a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.third_down_converted.unique(), \n pbp_2022.query('third_down_converted == 1').down[9],\n pbp_2022.query('third_down_converted == 1').ydstogo[9],\n pbp_2022.query('third_down_converted == 1').desc[9],\npbp_2022.query('third_down_converted == 1').yards_gained[9])\n\n(array([nan,  0.,  1.]),\n 3.0,\n 2,\n '(12:41) (Shotgun) 8-L.Jackson right tackle to BAL 40 for 4 yards (57-C.Mosley, 3-J.Whitehead).',\n 4.0)\n\n\nthird_down_failed indicates if the team with ball possession on third down did not get a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.third_down_failed.unique(), \n pbp_2022.query('third_down_failed == 1').down[5],\n pbp_2022.query('third_down_failed == 1').ydstogo[5],\n pbp_2022.query('third_down_failed == 1').desc[5],\npbp_2022.query('third_down_failed == 1').yards_gained[5])\n\n(array([nan,  0.,  1.]),\n 3.0,\n 5,\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.',\n 0.0)\n\n\nfourth_down_converted indicates if the team with ball possession on fourth down got a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.fourth_down_converted.unique(), \n pbp_2022.query('fourth_down_converted == 1').down[145],\n pbp_2022.query('fourth_down_converted == 1').ydstogo[145],\n pbp_2022.query('fourth_down_converted == 1').desc[145],\npbp_2022.query('fourth_down_converted == 1').yards_gained[145])\n\n(array([nan,  0.,  1.]),\n 4.0,\n 1,\n '(7:32) 19-J.Flacco pass short right to 84-C.Davis to BAL 21 for 7 yards (23-K.Fuller).',\n 7.0)\n\n\nfourth_down_failed indicates if the team with ball possession on fourth down did not get a first down on the play. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.fourth_down_failed.unique(), \n pbp_2022.query('fourth_down_failed == 1').down[154],\n pbp_2022.query('fourth_down_failed == 1').ydstogo[154],\n pbp_2022.query('fourth_down_failed == 1').desc[154],\npbp_2022.query('fourth_down_failed == 1').yards_gained[154])\n\n(array([nan,  0.,  1.]),\n 4.0,\n 6,\n '(4:22) (Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.',\n 0.0)\n\n\nincomplete_pass indicates if the pass was incomplete. It is either nan, 1 (True) or 0 (False).\n\n(pbp_2022.incomplete_pass.unique(),\n pbp_2022.query('incomplete_pass == 1').desc[3])\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\ntouchback indicates if the kickoff or punt either went past the back of the endzone or was fair-caught in the end zone.\n\n(pbp_2022.touchback.unique(),\n pbp_2022.query('touchback == 1').desc[33])\n\n(array([0, 1]),\n '9-J.Tucker kicks 65 yards from BAL 35 to end zone, Touchback.')\n\n\ninterception indicates if the quarterback’s pass was intercepted by a defender. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.interception.unique(),\n pbp_2022.query('interception == 1').desc[28])\n\n(array([nan,  0.,  1.]),\n '(5:07) (Shotgun) 19-J.Flacco pass short middle intended for 81-L.Cager INTERCEPTED by 32-M.Williams at NYJ 46. 32-M.Williams to NYJ 13 for 33 yards (19-J.Flacco).')\n\n\nfumble_forced indicates if a fumble was forced on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_forced.unique(),\n pbp_2022.query('fumble_forced == 1').desc[80])\n\n(array([nan,  0.,  1.]),\n '(1:16) (Shotgun) 19-J.Flacco pass short right to 83-T.Conklin to BAL 21 for 6 yards (32-M.Williams, 58-M.Pierce). FUMBLES (58-M.Pierce), touched at BAL 25, recovered by NYJ-17-G.Wilson at BAL 27. 17-G.Wilson to BAL 27 for no gain (14-K.Hamilton).')\n\n\nfumble_not_forced indicates if a fumble occurred on the play but was not forced by another player. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_not_forced.unique(),\n pbp_2022.query('fumble_not_forced == 1').desc[264])\n\n(array([nan,  0.,  1.]),\n '(13:46) (Shotgun) 9-M.Stafford to LA 11 for -6 yards. FUMBLES, and recovers at LA 11. 9-M.Stafford sacked at LA 10 for -7 yards (50-G.Rousseau).')\n\n\nfumble_out_of_bounds indicates if a fumbled ball went out of bounds. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_out_of_bounds.unique(),\n pbp_2022.query('fumble_out_of_bounds == 1').desc[1160])\n\n(array([nan,  0.,  1.]),\n '(:32) (Shotgun) 16-T.Lawrence pass short right to 1-T.Etienne to WAS 11 for 3 yards (22-D.Forrest). FUMBLES (22-D.Forrest), ball out of bounds at WAS 19. The Replay Official reviewed the pass completion ruling, and the play was Upheld. The ruling on the field stands.')\n\n\nsolo_tackle indicates if a player made a solo tackle on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.solo_tackle.unique(),\n pbp_2022.query('solo_tackle == 1').desc[1])\n\n(array([nan,  1.,  0.]),\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\nsafety indicates if a defensive player scored a safety on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.safety.unique(),\n pbp_2022.query('safety == 1').desc[3255])\n\n(array([nan,  0.,  1.]),\n '(:13) (Run formation) 19-B.Powell right end ran ob in End Zone for -26 yards, SAFETY (37-D.Alford).')\n\n\npenalty indicates if there was a penalty on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.penalty.unique(),\n pbp_2022.query('penalty == 1').desc[5])\n\n(array([nan,  0.,  1.]),\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\ntackled_for_loss indicates if a player was tackled for a loss of yards. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.tackled_for_loss.unique(),\n pbp_2022.query('tackled_for_loss == 1').desc[15])\n\n(array([nan,  0.,  1.]),\n '(9:49) 20-Br.Hall right end to NYJ 9 for -2 yards (92-J.Madubuike).')\n\n\nfumble_lost indicates if a player lost a fumble to the other team. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble_lost.unique(),\n pbp_2022.query('fumble_lost == 1').desc[129])\n\n(array([nan,  0.,  1.]),\n '(14:13) (No Huddle, Shotgun) 19-J.Flacco pass short middle to 20-Br.Hall to BAL 16 for 6 yards (36-C.Clark). FUMBLES (36-C.Clark), RECOVERED by BAL-44-M.Humphrey at BAL 15.')\n\n\nqb_hit indicates if the quarterback was hit on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.qb_hit.unique(),\n pbp_2022.query('qb_hit == 1').desc[5])\n\n(array([nan,  0.,  1.]),\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\nrush_attempt indicates if the play was a rushing play. It is either nan, 1 (True), or 0 (False). A QB scramble is considered a rush attempt.\n\n(pbp_2022.rush_attempt.unique(),\n pbp_2022.query('rush_attempt == 1').desc[9],\n pbp_2022.query('rush_attempt == 1 and qb_scramble == 1').desc[89])\n\n(array([nan,  0.,  1.]),\n '(12:41) (Shotgun) 8-L.Jackson right tackle to BAL 40 for 4 yards (57-C.Mosley, 3-J.Whitehead).',\n '(14:15) (Shotgun) 8-L.Jackson scrambles left end ran ob at BAL 35 for 8 yards (3-J.Whitehead).')\n\n\npass_attempt indicates if the play was a passing play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.pass_attempt.unique(),\n pbp_2022.query('pass_attempt == 1').desc[3])\n\n(array([nan,  0.,  1.]),\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\nsack indicates if the quarterback was sacked on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.sack.unique(),\n pbp_2022.query('sack == 1').desc[54])\n\n(array([nan,  0.,  1.]),\n '(9:43) (Shotgun) 8-L.Jackson sacked ob at NYJ 49 for 0 yards (56-Qu.Williams).')\n\n\ntouchdown indicates if a player scored a touchdown on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.touchdown.unique(),\n pbp_2022.query('touchdown == 1').desc[68])\n\n(array([nan,  0.,  1.]),\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\npass_touchdown, rush_touchdown, and return_touchdown indicate if the touchdown was a result of a pass, rush or kickoff/punt/fumble/interception return play, respectively. Their value is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.pass_touchdown.unique(),\n pbp_2022.query('pass_touchdown == 1').desc[68])\n\n(array([nan,  0.,  1.]),\n '(3:51) (Shotgun) 8-L.Jackson pass deep right to 13-D.Duvernay for 25 yards, TOUCHDOWN.')\n\n\n\n(pbp_2022.rush_touchdown.unique(),\n pbp_2022.query('rush_touchdown == 1').desc[298])\n\n(array([nan,  0.,  1.]),\n '(13:34) (Shotgun) 17-J.Allen scrambles right end for 4 yards, TOUCHDOWN.')\n\n\n\n(pbp_2022.return_touchdown.unique(),\n pbp_2022.query('return_touchdown == 1').desc[1651],\n pbp_2022.query('return_touchdown == 1').desc[2197],\n pbp_2022.query('return_touchdown == 1').desc[47094])\n\n(array([nan,  0.,  1.]),\n '(7:40) (Shotgun) 10-M.Jones sacked at NE 6 for -9 yards (29-Br.Jones). FUMBLES (29-Br.Jones) [29-Br.Jones], RECOVERED by MIA-6-M.Ingram at NE 2. 6-M.Ingram for 2 yards, TOUCHDOWN.',\n '(6:36) (Shotgun) 16-J.Goff pass short left intended for 88-T.Hockenson INTERCEPTED by 24-J.Bradberry (43-K.White) [95-M.Tuipulotu] at DET 27. 24-J.Bradberry for 27 yards, TOUCHDOWN.',\n '6-N.Folk kicks 66 yards from NE 35 to BUF -1. 20-N.Hines for 101 yards, TOUCHDOWN.')\n\n\nThe following fields indicate if the play involved an attempt at an Extra Point, Two Point Conversion, Field Goal, Kickoff, or Punt, respectively:\n\nextra_point_attempt\ntwo_point_attempt\nfield_goal_attempt\nkickoff_attempt\npunt_attempt\n\nTheir value is either nan, 1 (True), or 0 (False).empt\n\n(pbp_2022.extra_point_attempt.unique(),\n pbp_2022.query('extra_point_attempt == 1').desc[69])\n\n(array([nan,  0.,  1.]),\n '9-J.Tucker extra point is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\n\n(pbp_2022.two_point_attempt.unique(),\n pbp_2022.query('two_point_attempt == 1').desc[1179])\n\n(array([nan,  0.,  1.]),\n 'TWO-POINT CONVERSION ATTEMPT. 16-T.Lawrence pass to 17-E.Engram is incomplete. ATTEMPT FAILS.')\n\n\n\n(pbp_2022.field_goal_attempt.unique(),\n pbp_2022.query('field_goal_attempt == 1').desc[32])\n\n(array([nan,  0.,  1.]),\n '(3:19) 9-J.Tucker 24 yard field goal is GOOD, Center-46-N.Moore, Holder-11-J.Stout.')\n\n\n\n(pbp_2022.kickoff_attempt.unique(),\n pbp_2022.query('kickoff_attempt == 1').desc[1])\n\n(array([nan,  1.,  0.]),\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.punt_attempt.unique(),\n pbp_2022.query('punt_attempt == 1').desc[6])\n\n(array([nan,  0.,  1.]),\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\nfumble indicates if a player fumbled the ball on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.fumble.unique(),\n pbp_2022.query('fumble == 1').desc[80])\n\n(array([nan,  0.,  1.]),\n '(1:16) (Shotgun) 19-J.Flacco pass short right to 83-T.Conklin to BAL 21 for 6 yards (32-M.Williams, 58-M.Pierce). FUMBLES (58-M.Pierce), touched at BAL 25, recovered by NYJ-17-G.Wilson at BAL 27. 17-G.Wilson to BAL 27 for no gain (14-K.Hamilton).')\n\n\ncomplete_pass indicates if a player completed a pass on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.complete_pass.unique(),\n pbp_2022.query('complete_pass == 1').desc[7])\n\n(array([nan,  0.,  1.]),\n '(13:42) 8-L.Jackson pass short right to 7-R.Bateman pushed ob at BAL 32 for 4 yards (3-J.Whitehead).')\n\n\nassist_tackle indicates if a player assisted on the tackle on the play. It is either nan, 1 (True), or 0 (False).\n\n(pbp_2022.assist_tackle.unique(),\n pbp_2022.query('assist_tackle == 1').desc[2])\n\n(array([nan,  0.,  1.]),\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThe following fields provide the player_id (string), player_name (string) and yards gained (integer) for the passer, receiver or rusher on the play, respectively.\n\npasser_player_id\npasser_player_name\npassing_yards\nreceiver_player_id\nreceiver_player_name\nreceiving_yards\nrusher_player_id\nrusher_player_name\nrushing_yards\n\n\n(pbp_2022.passer_player_id[3],\n pbp_2022.passer_player_name[3],\n pbp_2022.passing_yards[3],\n pbp_2022.desc[3])\n\n('00-0026158',\n 'J.Flacco',\n nan,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\n(pbp_2022.receiver_player_id[3],\n pbp_2022.receiver_player_name[3],\n pbp_2022.receiving_yards[3],\n pbp_2022.desc[3])\n\n('00-0036924',\n 'Mi.Carter',\n nan,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\n(pbp_2022.rusher_player_id[2],\n pbp_2022.rusher_player_name[2],\n pbp_2022.rushing_yards[2],\n pbp_2022.desc[2])\n\n('00-0036924',\n 'Mi.Carter',\n 19.0,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThe following fields provide the player_id (string) and player_name (string) for players who intercepted the ball, returned a punt, returned a kickoff, punted the ball, kicked off the ball, recovered their own kickoff, or blocked the kick, respectively:\n\ninterception_player_id\ninterception_player_name\npunt_returner_player_id\npunt_returner_player_name\nkickoff_returner_player_name\nkickoff_returner_player_id\npunter_player_id\npunter_player_name\nkicker_player_name\nkicker_player_id\nown_kickoff_recovery_player_id\nown_kickoff_recovery_player_name\nblocked_player_id\nblocked_player_name\n\n\n(pbp_2022.interception_player_id[28],\n pbp_2022.interception_player_name[28],\n pbp_2022.desc[28])\n\n('00-0033894',\n 'M.Williams',\n '(5:07) (Shotgun) 19-J.Flacco pass short middle intended for 81-L.Cager INTERCEPTED by 32-M.Williams at NYJ 46. 32-M.Williams to NYJ 13 for 33 yards (19-J.Flacco).')\n\n\n\n(pbp_2022.punt_returner_player_id[6],\n pbp_2022.punt_returner_player_name[6],\n pbp_2022.desc[6])\n\n('00-0036331',\n 'D.Duvernay',\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\n\n(pbp_2022.kickoff_returner_player_id[1],\n pbp_2022.kickoff_returner_player_name[1],\n pbp_2022.desc[1])\n\n('00-0034419',\n 'B.Berrios',\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.punter_player_id[6],\n pbp_2022.punter_player_name[6],\n pbp_2022.desc[6])\n\n('00-0036313',\n 'B.Mann',\n '(13:53) 7-B.Mann punts 45 yards to BAL 19, Center-42-T.Hennessy. 13-D.Duvernay pushed ob at BAL 28 for 9 yards (42-T.Hennessy).')\n\n\n\n(pbp_2022.kicker_player_id[1],\n pbp_2022.kicker_player_name[1],\n pbp_2022.desc[1])\n\n('00-0029597',\n 'J.Tucker',\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\n\n(pbp_2022.own_kickoff_recovery_player_id[4964],\n pbp_2022.own_kickoff_recovery_player_name[4964],\n pbp_2022.desc[4964])\n\n('00-0033770',\n 'J.Hardee',\n '7-B.Mann kicks onside 12 yards from NYJ 35 to NYJ 47. RECOVERED by NYJ-34-J.Hardee.')\n\n\n\n(pbp_2022.blocked_player_id[1947],\n pbp_2022.blocked_player_name[1947],\n pbp_2022.desc[1947])\n\n('00-0036926',\n 'P.Turner',\n '(:02) 7-Y.Koo 63 yard field goal is BLOCKED (98-P.Turner), Center-48-L.McCullough, Holder-13-B.Pinion, recovered by ATL-13-B.Pinion at ATL 49. 13-B.Pinion to 50 for 1 yard (53-Z.Baun, 48-J.Gray).')\n\n\nThe following fields show player_id (string), player_name (string) or team (string) for a variety of defensive plays such as tackle for loss, quarterback hit, solo tackle, assist tackle and so on.\n\ntackle_for_loss_1_player_id\ntackle_for_loss_1_player_name\ntackle_for_loss_2_player_id\ntackle_for_loss_2_player_name\nqb_hit_1_player_id\nqb_hit_1_player_name\nqb_hit_2_player_id\nqb_hit_2_player_name\nsolo_tackle_1_team\nsolo_tackle_2_team\nsolo_tackle_1_player_id\nsolo_tackle_2_player_id\nsolo_tackle_1_player_name\nsolo_tackle_2_player_name\nassist_tackle_1_player_id\nassist_tackle_1_player_name\nassist_tackle_1_team\nassist_tackle_2_player_id\nassist_tackle_2_player_name\nassist_tackle_2_team\nassist_tackle_3_player_id\nassist_tackle_3_player_name\nassist_tackle_3_team\nassist_tackle_4_player_id\nassist_tackle_4_player_name\nassist_tackle_4_team\ntackle_with_assist\ntackle_with_assist_1_player_id\ntackle_with_assist_1_player_name\ntackle_with_assist_1_team\ntackle_with_assist_2_player_id\ntackle_with_assist_2_player_name\ntackle_with_assist_2_team\npass_defense_1_player_id\npass_defense_1_player_name\npass_defense_2_player_id\npass_defense_2_player_name\nsack_player_id\nsack_player_name\nhalf_sack_1_player_id\nhalf_sack_1_player_name\nhalf_sack_2_player_id\nhalf_sack_2_player_name\n\n\n(pbp_2022.tackled_for_loss[15],\n pbp_2022.tackle_for_loss_1_player_id[15],\n pbp_2022.tackle_for_loss_1_player_name[15],\n pbp_2022.desc[15])\n\n(1.0,\n '00-0036130',\n 'J.Madubuike',\n '(9:49) 20-Br.Hall right end to NYJ 9 for -2 yards (92-J.Madubuike).')\n\n\nThere are no plays where tackle_for_loss_2_player_id has a value.\n\npbp_2022.tackle_for_loss_2_player_id.unique()\n\narray([nan])\n\n\n\n(pbp_2022.qb_hit[5],\n pbp_2022.qb_hit_1_player_id[5],\n pbp_2022.qb_hit_1_player_name[5],\n pbp_2022.desc[5])\n\n(1.0,\n '00-0026190',\n 'C.Campbell',\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\n\n(pbp_2022.qb_hit[55],\n pbp_2022.qb_hit_1_player_id[55],\n pbp_2022.qb_hit_1_player_name[55],\n pbp_2022.qb_hit_2_player_id[55],\n pbp_2022.qb_hit_2_player_name[55],\n pbp_2022.desc[55])\n\n(1.0,\n '00-0034163',\n 'J.Johnson',\n '00-0034163',\n 'J.Martin',\n '(8:59) (Shotgun) 8-L.Jackson sacked at BAL 49 for -2 yards (sack split by 52-J.Johnson and 54-J.Martin).')\n\n\n\n(pbp_2022.solo_tackle[777],\n pbp_2022.solo_tackle_1_team[777],\n pbp_2022.solo_tackle_1_player_id[777],\n pbp_2022.solo_tackle_1_player_name[777],\n pbp_2022.solo_tackle_2_team[777],\n pbp_2022.solo_tackle_2_player_id[777],\n pbp_2022.solo_tackle_2_player_name[777],\n pbp_2022.desc[777])\n\n(1.0,\n 'MIN',\n '00-0032129',\n 'J.Hicks',\n 'GB',\n '00-0036631',\n 'R.Newman',\n '(12:21) 12-A.Rodgers sacked at GB 35 for -9 yards (58-J.Hicks). FUMBLES (58-J.Hicks) [58-J.Hicks], RECOVERED by MIN-94-D.Tomlinson at GB 33. 94-D.Tomlinson to GB 33 for no gain (70-R.Newman).')\n\n\n\n(pbp_2022.assist_tackle[2],\n pbp_2022.assist_tackle_1_team[2],\n pbp_2022.assist_tackle_1_player_id[2],\n pbp_2022.assist_tackle_1_player_name[2],\n pbp_2022.assist_tackle_2_team[2],\n pbp_2022.assist_tackle_2_player_id[2],\n pbp_2022.assist_tackle_2_player_name[2],\n pbp_2022.desc[2])\n\n(1.0,\n 'BAL',\n '00-0033894',\n 'M.Williams',\n 'BAL',\n '00-0033294',\n 'C.Clark',\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\nThere are no plays where assist_tackle_3_player_id or assist_tackle_4_player_id have a value.\n\npbp_2022.assist_tackle_3_player_id.unique(), pbp_2022.assist_tackle_4_player_id.unique()\n\n(array([nan]), array([nan]))\n\n\ntackle_with_assist is not the same as assist_tackle.\n\n(pbp_2022.tackle_with_assist[2],\n pbp_2022.tackle_with_assist_1_team[2],\n pbp_2022.tackle_with_assist_1_player_id[2],\n pbp_2022.tackle_with_assist_1_player_name[2],\n pbp_2022.tackle_with_assist_2_team[2],\n pbp_2022.tackle_with_assist_2_player_id[2],\n pbp_2022.tackle_with_assist_2_player_name[2],\n pbp_2022.desc[2])\n\n(0.0,\n nan,\n nan,\n nan,\n nan,\n nan,\n nan,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\n(pbp_2022.tackle_with_assist[22659],\n pbp_2022.tackle_with_assist_1_team[22659],\n pbp_2022.tackle_with_assist_1_player_id[22659],\n pbp_2022.tackle_with_assist_1_player_name[22659],\n pbp_2022.tackle_with_assist_2_team[22659],\n pbp_2022.tackle_with_assist_2_player_id[22659],\n pbp_2022.tackle_with_assist_2_player_name[22659],\n pbp_2022.desc[22659])\n\n(1.0,\n 'LAC',\n '00-0031040',\n 'K.Mack',\n 'ATL',\n '00-0035208',\n 'O.Zaccheaus',\n '(9:31) (No Huddle, Shotgun) 1-M.Mariota pass short left to 5-D.London to LAC 6 for 5 yards (52-K.Mack, 43-M.Davis). FUMBLES (52-K.Mack), RECOVERED by LAC-52-K.Mack at LAC 6. 52-K.Mack pushed ob at 50 for 44 yards (17-O.Zaccheaus, 5-D.London).')\n\n\nI’ll explore this more later before using these fields in analyses, but it seems like the assist_tackle fields provide information on players who assisted with the tackle, while tackle_with_assist lists information of the “main” player who was assisted on the tackle.\n\n(pbp_2022.assist_tackle[22659],\n pbp_2022.assist_tackle_1_team[22659],\n pbp_2022.assist_tackle_1_player_id[22659],\n pbp_2022.assist_tackle_1_player_name[22659],\n pbp_2022.assist_tackle_2_team[22659],\n pbp_2022.assist_tackle_2_player_id[22659],\n pbp_2022.assist_tackle_2_player_name[22659],\n pbp_2022.desc[22659])\n\n(1.0,\n 'LAC',\n '00-0033697',\n 'M.Davis',\n 'ATL',\n '00-0037238',\n 'D.London',\n '(9:31) (No Huddle, Shotgun) 1-M.Mariota pass short left to 5-D.London to LAC 6 for 5 yards (52-K.Mack, 43-M.Davis). FUMBLES (52-K.Mack), RECOVERED by LAC-52-K.Mack at LAC 6. 52-K.Mack pushed ob at 50 for 44 yards (17-O.Zaccheaus, 5-D.London).')\n\n\n\n(pbp_2022.pass_defense_1_player_id[1613],\n pbp_2022.pass_defense_1_player_name[1613],\n pbp_2022.pass_defense_2_player_id[1613],\n pbp_2022.pass_defense_2_player_name[1613],\n pbp_2022.desc[1613])\n\n('00-0033050',\n 'X.Howard',\n '00-0036998',\n 'J.Holland',\n '(10:05) (Shotgun) 10-M.Jones pass deep right intended for 1-D.Parker INTERCEPTED by 8-J.Holland (25-X.Howard) at MIA -3. 8-J.Holland to MIA 28 for 31 yards (76-I.Wynn).')\n\n\nThe following fields show player_id (string), player_name (string) or team (string) for a variety of fumble-related plays:\n\nforced_fumble_player_1_team\nforced_fumble_player_1_player_id\nforced_fumble_player_1_player_name\nforced_fumble_player_2_team\nforced_fumble_player_2_player_id\nforced_fumble_player_2_player_name\nfumbled_1_team\nfumbled_1_player_id\nfumbled_1_player_name\nfumbled_2_player_id\nfumbled_2_player_name\nfumbled_2_team\nfumble_recovery_1_team\nfumble_recovery_1_yards\nfumble_recovery_1_player_id\nfumble_recovery_1_player_name\nfumble_recovery_2_team\nfumble_recovery_2_yards\nfumble_recovery_2_player_id\nfumble_recovery_2_player_name\n\n\n(pbp_2022.fumble_forced[9041],\n pbp_2022.forced_fumble_player_1_team[9041],\n pbp_2022.forced_fumble_player_1_player_id[9041],\n pbp_2022.forced_fumble_player_1_player_name[9041],\n pbp_2022.forced_fumble_player_2_team[9041],\n pbp_2022.forced_fumble_player_2_player_id[9041],\n pbp_2022.forced_fumble_player_2_player_name[9041],\n pbp_2022.desc[9041])\n\n(1.0,\n 'NYG',\n '00-0033046',\n 'J.Ward',\n 'NYG',\n '00-0036167',\n 'T.Crowder',\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.fumbled_1_team[9041],\n pbp_2022.fumbled_1_player_id[9041],\n pbp_2022.fumbled_1_player_name[9041],\n pbp_2022.fumbled_2_team[9041],\n pbp_2022.fumbled_2_player_id[9041],\n pbp_2022.fumbled_2_player_name[9041],\n pbp_2022.desc[9041])\n\n('CHI',\n '00-0034279',\n 'E.St. Brown',\n 'CHI',\n '00-0036953',\n 'T.Ebner',\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.fumble_recovery_1_team[9041],\n pbp_2022.fumble_recovery_1_player_id[9041],\n pbp_2022.fumble_recovery_1_player_name[9041],\n pbp_2022.fumble_recovery_1_yards[9041],\n pbp_2022.fumble_recovery_2_team[9041],\n pbp_2022.fumble_recovery_2_player_id[9041],\n pbp_2022.fumble_recovery_2_player_name[9041],\n pbp_2022.fumble_recovery_2_yards[9041],\n pbp_2022.desc[9041])\n\n('CHI',\n '00-0036945',\n 'J.Fields',\n -3.0,\n 'CHI',\n '00-0033082',\n 'L.Patrick',\n -6.0,\n '(:03) (Shotgun) 1-J.Fields pass short right to 25-T.Ebner to CHI 35 for 2 yards. Lateral to 19-E.St. Brown to CHI 44 for 9 yards. FUMBLES, touched at CHI 44, recovered by CHI-1-J.Fields at CHI 39. 1-J.Fields to CHI 36 for -3 yards. Lateral to 19-E.St. Brown to CHI 44 for 8 yards. Lateral to 25-T.Ebner to NYG 44 for 12 yards (55-J.Ward). FUMBLES (55-J.Ward), recovered by CHI-62-L.Patrick at NYG 46. 62-L.Patrick to CHI 48 for -6 yards. Lateral to 1-J.Fields to CHI 49 for 1 yard. Lateral to 76-T.Jenkins to CHI 46 for -3 yards (48-T.Crowder). FUMBLES (48-T.Crowder), touched at CHI 45, recovered by CHI-25-T.Ebner at CHI 41. 25-T.Ebner to CHI 32 for -9 yards. FUMBLES, touched at CHI 32, RECOVERED by NYG-24-D.Belton at CHI 28.')\n\n\n\n(pbp_2022.sack[54],\n pbp_2022.sack_player_name[54],\n pbp_2022.sack_player_id[54],\n pbp_2022.desc[54])\n\n(1.0,\n 'Qu.Williams',\n '00-0035680',\n '(9:43) (Shotgun) 8-L.Jackson sacked ob at NYJ 49 for 0 yards (56-Qu.Williams).')\n\n\nWhen a sack is split, sack == 1 but sack_player_name and id are nan.\n\n(pbp_2022.sack[55],\n pbp_2022.sack_player_name[55],\n pbp_2022.sack_player_id[55],\n pbp_2022.half_sack_1_player_id[55],\n pbp_2022.half_sack_1_player_name[55],\n pbp_2022.half_sack_2_player_id[55],\n pbp_2022.half_sack_2_player_name[55],\n pbp_2022.desc[55])\n\n(1.0,\n nan,\n nan,\n '00-0034163',\n 'J.Johnson',\n '00-0034163',\n 'J.Martin',\n '(8:59) (Shotgun) 8-L.Jackson sacked at BAL 49 for -2 yards (sack split by 52-J.Johnson and 54-J.Martin).')\n\n\nreturn_team (string) and return_yards (integer) are the abbreviation and yardage of the team that returned the kickoff or punt. I’ll look into if fumble returns are included before I use this field for analyses.\n\n(pbp_2022.return_team[1], \n pbp_2022.return_yards[1],\n pbp_2022.desc[1])\n\n('NYJ',\n 25.0,\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')\n\n\nThe following fields hold information about penalties.\n\npenalty_team (string)\npenalty_player_id (string)\npenalty_player_name (string)\npenalty_yards (integer)\npenalty_type (string)\n\n\n(pbp_2022.penalty[5],\n pbp_2022.penalty_team[5],\n pbp_2022.penalty_player_id[5],\n pbp_2022.penalty_player_name[5],\n pbp_2022.penalty_yards[5],\n pbp_2022.penalty_type[5],\n pbp_2022.desc[5])\n\n(1.0,\n 'NYJ',\n '00-0026158',\n 'J.Flacco',\n 10.0,\n 'Intentional Grounding',\n '(14:01) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short right [93-C.Campbell]. PENALTY on NYJ-19-J.Flacco, Intentional Grounding, 10 yards, enforced at NYJ 46.')\n\n\n\npbp_2022.penalty_type.unique()\n\narray([nan, 'Intentional Grounding', 'Illegal Contact',\n       'Offensive Holding', 'Defensive Pass Interference',\n       'Defensive Holding', 'Offensive Pass Interference', 'False Start',\n       'Horse Collar Tackle', 'Defensive Too Many Men on Field',\n       'Taunting', 'Delay of Game', 'Roughing the Passer',\n       'Unsportsmanlike Conduct', 'Low Block', 'Illegal Formation',\n       'Ineligible Downfield Pass', 'Unnecessary Roughness',\n       'Neutral Zone Infraction', 'Running Into the Kicker',\n       'Illegal Shift', 'Defensive Offside', 'Illegal Use of Hands',\n       'Illegal Block Above the Waist', 'Offensive Too Many Men on Field',\n       'Encroachment', 'Disqualification', 'Ineligible Downfield Kick',\n       'Face Mask', 'Player Out of Bounds on Kick',\n       'Illegal Forward Pass', 'Chop Block', 'Delay of Kickoff',\n       'Tripping', 'Illegal Substitution', 'Offensive Offside',\n       'Illegal Blindside Block', 'Illegal Touch Pass',\n       'Offside on Free Kick', 'Roughing the Kicker',\n       'Fair Catch Interference', 'Leverage', 'Illegal Motion',\n       'Defensive Delay of Game', 'Illegal Bat', 'Illegal Touch Kick',\n       'Illegal Double-Team Block', 'Invalid Fair Catch Signal',\n       'Illegal Crackback', 'Illegal Kick/Kicking Loose Ball'],\n      dtype=object)\n\n\nreplay_or_challenge (1 for True and 0 for False) and replay_or_challenge_result (nan, 'upheld', or 'reversed') show information about whether a replay or challenge occurred on the play.\n\n(pbp_2022.replay_or_challenge[621],\n pbp_2022.replay_or_challenge_result[621],\n pbp_2022.desc[621])\n\n(1,\n 'upheld',\n '(7:42) (Shotgun) 25-M.Gordon right tackle to SEA 1 for no gain (6-Q.Diggs, 10-U.Nwosu). FUMBLES (6-Q.Diggs), RECOVERED by SEA-30-M.Jackson at SEA 2. 30-M.Jackson to SEA 10 for 8 yards (14-C.Sutton). The Replay Official reviewed the fumble ruling, and the play was Upheld. The ruling on the field stands.')\n\n\nsafety_player_name and safety_player_id have information about the player who caused the safety.\n\n(pbp_2022.safety[3255],\n pbp_2022.safety_player_name[3255],\n pbp_2022.safety_player_id[3255],\n pbp_2022.desc[3255])\n\n(1.0,\n 'D.Alford',\n '00-0037034',\n '(:13) (Run formation) 19-B.Powell right end ran ob in End Zone for -26 yards, SAFETY (37-D.Alford).')\n\n\nseries_result is the result of the offensive series.\n\npbp_2022.series_result.unique()\n\narray(['First down', 'Punt', 'Turnover', 'Field goal',\n       'Missed field goal', 'Touchdown', 'End of half',\n       'Turnover on downs', 'QB kneel', 'Opp touchdown', 'Safety', nan],\n      dtype=object)\n\n\nplay_type_nfl shows slightly different play type categories.\n\npbp_2022.play_type_nfl.unique()\n\narray(['GAME_START', 'KICK_OFF', 'RUSH', 'PASS', 'PUNT', 'TIMEOUT',\n       'PENALTY', 'FIELD_GOAL', 'END_QUARTER', 'SACK', 'XP_KICK',\n       'END_GAME', 'PAT2', nan, 'FREE_KICK'], dtype=object)\n\n\ndrive_play_count shows how many plays the drive had. I’ll look into it more before using it for analyses. It doesn’t always match the number of plays on the drive, or at least seems not to, so I need to understand how they calculate this value.\n\npbp_2022.drive_play_count.unique()\n\narray([nan,  4.,  6.,  5.,  3.,  8.,  1.,  9., 16., 11.,  2., 13.,  7.,\n       14., 10., 15., 12.,  0., 18., 19., 20., 17., 21.])\n\n\ndrive_time_of_possession is a formatted string of minutes:seconds the drive took.\n\npbp_2022.drive_time_of_possession.unique()[:5]\n\narray([nan, '1:18', '3:53', '2:44', '1:04'], dtype=object)\n\n\ndrive_first_downs is the number of first downs achieved on the drive.\n\npbp_2022.drive_first_downs.unique()[:5]\n\narray([nan,  1.,  0.,  3.,  2.])\n\n\ndrive_inside20 is either nan, 1 (True) or 0 (False) and indicates if a drive ended inside of the red zone (20 yards from the end zone).\n\npbp_2022.drive_inside20.unique()\n\narray([nan,  0.,  1.])\n\n\ndrive_ended_with_score indicates if a drive ended with the offensive team scoring. It is either nan, 1 (True) or 0 (False).\n\npbp_2022.drive_ended_with_score.unique()\n\narray([nan,  0.,  1.])\n\n\nI’ll have to look into it more before using it for analyses, but I believe drive_yards_penalized is the total number of offensive penalty yards on the drive.\n\npbp_2022.drive_yards_penalized.unique()[:5]\n\narray([ nan, -10.,   0.,   5.,  32.])\n\n\ndrive_play_id_started and drive_play_id_ended indicate the start and end play_id of the drive. Note that play_id are not consecutive and doesn’t start at 1.\n\n(pbp_2022.drive_play_id_started[1],\npbp_2022.drive_play_id_ended[1])\n\n(43.0, 172.0)\n\n\naway_score and home_score are the final scores of the away team and home team.\n\n(pbp_2022.away_team[1],\n pbp_2022.away_score[1],\n pbp_2022.home_team[1],\n pbp_2022.home_score[1])\n\n('BAL', 24, 'NYJ', 9)\n\n\nresult is the difference between the home and the away team (I think—will look into it more).\n\npbp_2022.result[1]\n\n-15\n\n\ntotal is the total number of points scored by both teams.\n\npbp_2022.total[1]\n\n33\n\n\ndiv_game indicates if the game is between teams in the same division. It is either 1 (True) or 0 (False).\n\npbp_2022.div_game.unique(), pbp_2022.div_game[1]\n\n(array([0, 1]), 0)\n\n\naway_coach and home_coach are the names of the away team and home team coaches, respectively.\n\npbp_2022.away_coach[1], pbp_2022.home_coach[1]\n\n('John Harbaugh', 'Robert Saleh')\n\n\nThe following fields give the name and jersey number of the passer, rusher or receiver on the play:\n\npasser\npasser_id\npasser_jersey_number\nrusher\nrusher_id\nrusher_jersey_number\nreceiver\nreceiver_id\nreceiver_jersey_number\n\n\n(pbp_2022.passer[3], \n pbp_2022.passer_id[3],\n pbp_2022.passer_jersey_number[3])\n\n('J.Flacco', '00-0026158', 19.0)\n\n\n\n(pbp_2022.rusher[2], \n pbp_2022.rusher_id[2],\n pbp_2022.rusher_jersey_number[2])\n\n('Mi.Carter', '00-0036924', 32.0)\n\n\n\n(pbp_2022.receiver[3], \n pbp_2022.receiver_id[3],\n pbp_2022.receiver_jersey_number[3])\n\n('Mi.Carter', '00-0036924', 32.0)\n\n\nThe following fields indicate if the play is a pass, rush, first down, or special teams, respectively. Their value is nan, 1 (True) or 0 (False):\n\npass\nrush\nfirst_down\nspecial\n\n\npbp_2022['pass'][3], pbp_2022.desc[3]\n\n(1,\n '(14:29) (No Huddle, Shotgun) 19-J.Flacco pass incomplete short left to 32-Mi.Carter.')\n\n\n\npbp_2022.rush[2], pbp_2022.desc[2]\n\n(1,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\npbp_2022.first_down[2], pbp_2022.desc[2]\n\n(1.0,\n '(14:56) 32-Mi.Carter left end to NYJ 41 for 19 yards (32-M.Williams; 36-C.Clark).')\n\n\n\npbp_2022.special[1], pbp_2022.desc[1]\n\n(1,\n '9-J.Tucker kicks 68 yards from BAL 35 to NYJ -3. 10-B.Berrios to NYJ 22 for 25 yards (51-J.Ross).')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "vishal bakshi",
    "section": "",
    "text": "welcome to my blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Random Forest for the Kaggle Zillow Competition Dataset\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I follow the techniques presented in the fastai textbook to train a leaderboard-beating random forest.\n\n\n\n\n\n\nVishal Bakshi\n\n\nWednesday, September 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning a Language Model Using LoRA\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I want to compare fine-tuning a pretrained model with and without using LoRA.\n\n\n\n\n\n\nVishal Bakshi\n\n\nFriday, September 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing HuggingFace Transformers for Tabular Titanic Data\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I run a fun experiment to see how well a NLP model can predict tabular data.\n\n\n\n\n\n\nVishal Bakshi\n\n\nWednesday, August 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining a Multi-Target Regression Deep Learning Model with fastai\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I train single and multi-target regression tabular deep learning models using fastai, and compare the results.\n\n\n\n\n\n\nVishal Bakshi\n\n\nSaturday, August 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a From-Scratch Deep Learning Model to Predict Two Variables\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I build a deep learning model from scratch which can predict two dependent variables from a tabular dataset.\n\n\n\n\n\n\nVishal Bakshi\n\n\nFriday, August 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Losses and Accuracy for 100 Deep Neural Net Training Runs\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I modify existing code to record and plot training values for a simple deep neural net model.\n\n\n\n\n\n\nVishal Bakshi\n\n\nTuesday, August 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Deep Learnings For Coders - Part 1 Notes and Examples\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nAn update on my progress through part 1 of the fastai course.\n\n\n\n\n\n\nVishal Bakshi\n\n\nSunday, August 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tuning a Language Model as a Text Classifier\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this notebook I work through the chapter exercise presented in Chapter 10 of the fastai textbook.\n\n\n\n\n\n\nVishal Bakshi\n\n\nSaturday, August 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing an MNIST Classifier From Scratch\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this blog post, I write a MNISTLearner class which trains a neural net to classify the full MNIST dataset of 10 handwritten digits.\n\n\n\n\n\n\nVishal Bakshi\n\n\nWednesday, July 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a fastai Learner from Scratch\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\nIn this blog post, I write a BasicLearner class which trains a neural net to classify handwritten digits.\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, July 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Pandas - Notes and Exercises\n\n\n\n\n\n\n\ndata analysis\n\n\npython\n\n\n\n\nAn update on my progress in the book “Effective Pandas” by Matt Harrison.\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, July 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\nExploring NFL Play-by-Play Data with SQL and Python\n\n\n\n\n\n\n\ndata analysis\n\n\nSQL\n\n\npython\n\n\n\n\nAn update on my analysis and visualization of NFL play-by-play data.\n\n\n\n\n\n\nVishal Bakshi\n\n\nThursday, July 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nVisualize U.S. Census Data with ArcGIS\n\n\n\n\n\n\n\nArcGIS\n\n\ndata analysis\n\n\n\n\nA tutorial to create a geodatabase, maps and layouts to visualize U.S. Census Data in ArcGIS Pro\n\n\n\n\n\n\nVishal Bakshi\n\n\nFriday, June 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRegression and Other Stories - Notes and Excerpts\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nSunday, June 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization Analysis & Design - Excerpts\n\n\n\n\n\n\n\n\n\n\n\n\nThursday, June 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTranscribing Sherlock into Spanish\n\n\n\n\n\n\n\nspanish\n\n\n\n\nAn update on my goal to transcribe BBC’s Sherlock into Spanish.\n\n\n\n\n\n\nVishal Bakshi\n\n\nTuesday, February 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMaking Chicken Wings\n\n\n\n\n\n\n\nfood\n\n\n\n\nAn update on my goal to make 10/10 chicken wings by the 2023 NFL season.\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Classification Models\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 6: Bear Classifier\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 7:Test Time Augmentation\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfast.ai Chapter 8: Collaborative Filter Deep Dive\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR Shiny Census App\n\n\n\n\n\n\n\nR\n\n\ndata analysis\n\n\nSQL\n\n\n\n\nAn explanation of my development process for a census data shiny app\n\n\n\n\n\n\nVishal Bakshi\n\n\nMonday, February 20, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "my name is vishal bakshi. i’m a data analyst at the City of Portland. i haven’t blogged in a couple of years and recently have had the desire to do so again so here i am."
  },
  {
    "objectID": "about.html#things-i-like-consistently",
    "href": "about.html#things-i-like-consistently",
    "title": "About",
    "section": "things i like consistently",
    "text": "things i like consistently\nhip hop, yoga, lifting free weights, resistance bands, coding, writing, reading, cold weather, philadelphia eagles, la lakers."
  },
  {
    "objectID": "about.html#what-im-currently-reading",
    "href": "about.html#what-im-currently-reading",
    "title": "About",
    "section": "what i’m currently reading",
    "text": "what i’m currently reading\ncheck out my currently reading list on goodreads."
  },
  {
    "objectID": "about.html#what-im-working-on-consistently",
    "href": "about.html#what-im-working-on-consistently",
    "title": "About",
    "section": "what i’m working on consistently",
    "text": "what i’m working on consistently\n\nobserving my thoughts, emotions and behaviors.\nmaking and practicing hip hop music.\nlearning about how and why our society is structured the way it is.\ndata analysis/science/visualization."
  }
]