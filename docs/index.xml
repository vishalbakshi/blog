<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Vishal Bakshi&#39;s Blog</title>
<link>https://vishalbakshi.github.io/blog/</link>
<atom:link href="https://vishalbakshi.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Machine Learning blog by Vishal Bakshi</description>
<generator>quarto-1.7.32</generator>
<lastBuildDate>Sun, 17 Aug 2025 07:00:00 GMT</lastBuildDate>
<item>
  <title>TIL: Launching Jupyter with a Custom Modal Image and Volume</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-17-modal-jupyter/</link>
  <description><![CDATA[ 




<p>Yesterday I learned of the Modal docs example showing how to start <a href="https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py">a jupyter server via a Modal tunnel</a>. I was elated to see this because it solved my problem of not being able to specify a custom image when using <code>modal launch jupyter</code>.</p>
<p>I have a Dockerfile which installs <code>colbert-ai</code> from the <code>main</code> branch of the stanford-futuredata/ColBERT repo with a specific PyTorch and Transformers version:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode dockerfile code-with-copy"><code class="sourceCode dockerfile"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">FROM</span> mambaorg/micromamba:latest</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">USER</span> root</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> update <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span> git nano curl wget build-essential <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> clean <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rm</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-rf</span> /var/lib/apt/lists/<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> clone https://github.com/stanford-futuredata/ColBERT.git /ColBERT <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> /ColBERT <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-9">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert python=3.11 cuda <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> nvidia/label/11.7.1 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-10">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert faiss-gpu <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> pytorch <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-11">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert pip install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-e</span> . <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-12">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert pip install torch==1.13.1 transformers==4.38.2 pandas</span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">ENV</span> CONDA_DEFAULT_ENV=colbert</span>
<span id="cb1-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">ENV</span> PATH=/opt/conda/envs/colbert/bin:$PATH</span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">WORKDIR</span> /</span>
<span id="cb1-18"></span>
<span id="cb1-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval </span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\"\$</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(micromamba shell hook --shell bash)</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\"</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;</span> ~/.bashrc <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-20">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"micromamba activate colbert"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;</span> ~/.bashrc</span>
<span id="cb1-21"></span>
<span id="cb1-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">CMD</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/bin/bash"</span>]</span></code></pre></div>
<p>I then modified the Modal documentation example as follows (<code>jupyter_inside_modal.py</code>) to use my Dockerfile to create an image and use an existing Modal Volume:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> subprocess</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> modal</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> modal <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image, App, Secret, Volume</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datetime</span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb2-7"></span>
<span id="cb2-8">SOURCE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> os.environ.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SOURCE"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb2-9">VOLUME <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Volume.from_name(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"colbert-maintenance"</span>, create_if_missing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-10">MOUNT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/colbert-maintenance"</span></span>
<span id="cb2-11">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.from_dockerfile(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Dockerfile.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>SOURCE<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, gpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"L4"</span>)</span>
<span id="cb2-12"></span>
<span id="cb2-13">app <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> App(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jupyter-tunnel"</span>, image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image.pip_install(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jupyter"</span>))</span>
<span id="cb2-14">JUPYTER_TOKEN <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># some list of characters you'll enter when accessing the Modal tunnel</span></span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@app.function</span>(max_containers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, volumes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{MOUNT: VOLUME}, timeout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span>, gpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"L4"</span>)</span>
<span id="cb2-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> run_jupyter(timeout: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>):</span>
<span id="cb2-18">    jupyter_port <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8888</span></span>
<span id="cb2-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> modal.forward(jupyter_port) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> tunnel:</span>
<span id="cb2-20">        jupyter_process <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subprocess.Popen(</span>
<span id="cb2-21">            [</span>
<span id="cb2-22">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jupyter"</span>,</span>
<span id="cb2-23">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"notebook"</span>,</span>
<span id="cb2-24">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--no-browser"</span>,</span>
<span id="cb2-25">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--allow-root"</span>,</span>
<span id="cb2-26">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--ip=0.0.0.0"</span>,</span>
<span id="cb2-27">                <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"--port=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>jupyter_port<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,</span>
<span id="cb2-28">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--NotebookApp.allow_origin='*'"</span>,</span>
<span id="cb2-29">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--NotebookApp.allow_remote_access=1"</span>,</span>
<span id="cb2-30">            ],</span>
<span id="cb2-31">            env<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>os.environ, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"JUPYTER_TOKEN"</span>: JUPYTER_TOKEN},</span>
<span id="cb2-32">        )</span>
<span id="cb2-33"></span>
<span id="cb2-34">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Jupyter available at =&gt; </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tunnel<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>url<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-35"></span>
<span id="cb2-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb2-37">            end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> timeout</span>
<span id="cb2-38">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> end_time:</span>
<span id="cb2-39">                time.sleep(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb2-40">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Reached end of </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>timeout<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> second timeout period. Exiting..."</span>)</span>
<span id="cb2-41">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">KeyboardInterrupt</span>:</span>
<span id="cb2-42">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Exiting..."</span>)</span>
<span id="cb2-43">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">finally</span>:</span>
<span id="cb2-44">            jupyter_process.kill()</span>
<span id="cb2-45"></span>
<span id="cb2-46"></span>
<span id="cb2-47"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@app.local_entrypoint</span>()</span>
<span id="cb2-48"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> main(timeout: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span>):</span>
<span id="cb2-49">    run_jupyter.remote(timeout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>timeout)</span></code></pre></div>
<p>I then run the following locally form my terminal:</p>
<pre><code>SOURCE="0.2.22.main.torch.1.13.1" modal run jupyter_inside_modal.py</code></pre>
<p>Where my Dockerfile is in the same folder as <code>jupyter_inside_modal.py</code> and titled <code>Dockerfile.0.2.22.main.torch.1.13.1</code>. I can then access the cloned repo as well as my mounted volume and use a Jupyter Notebook to explore data, iterate on function definitions, compare model weights, add hooks to ColBERT models, and so on. This unlocks a ton of productivity and iteration velocity that I was scratching my head on how to obtain without the use of notebooks.</p>



 ]]></description>
  <category>ColBERT</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-17-modal-jupyter/</guid>
  <pubDate>Sun, 17 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Reflections After Completing the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-17-ai-evals/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>This blog post is part journal, part reflection, and part planning around the topics of AI engineering, AI evals, applied AI, and my career path into machine learning. When I decided to take <a href="https://t.co/Zrmp6LRd9c">Hamel and Shreya’s AI evals course</a>, I had recently watched a very short talk on LLM reliability by Featherless AI CEO Eugene Cheah. I had also learned about the <a href="https://www.realevals.xyz/">realevals.xyz benchmark</a>, which shows just how bad LLMs are at reliably completing “boring” tasks that are necessary in commerce. I had also watched and <a href="https://youtu.be/9s88C8XBBiQ">deeply reflected</a> on a discussion between Jason Liu, Andy Walters, and Vignesh Mohankumar about the different routes into AI industry work. I had also reflected on my own fear of applied AI: the risk of automating tasks for which failures would have real, concrete consequences. The amalgamation of these experiences led me to adopt the “narrow fence/long leash” framework as discussed in the AnswerAI launch blog post, an idea that came from the old GE research and development lab. In this framework, the manager would provide the researchers a “long leash” to explore whatever problems they wanted to, at whatever depth they needed to, as long as they tied back to an applied AI project; as long as they stayed within the bounds of this “narrow fence” of an applied AI project.</p>
<p>A couple of weeks before the AI evals course started, I was chosen to be a maintainer of the stanford_futuredata/ColBERT (<code>colbert-ai</code> on PyPI) library. I had “suddenly” gone from “hobby ML researcher” to maintainer of a foundational library in the information retrieval ML space. I say “suddenly” because while everything I had done up to that point led to me landing that role, I hadn’t done everything with the goal of landing it. I had done what I did because it was fun, it was exciting, it was challenging, and I was learning a lot. Suddenly my priorities shifted. I put ColBERT maintenance (gratefully) at #1, and everything else I was working on (both long Leashes and narrow fences) became a distant #2. I had a number of goals of what I wanted to prepare before the AI evals course started, but I was not able to bring that energy and bring that preparation as I planned. Like anything, you get out what you put in. While I wasn’t able to put in my maximum in the AI Evals course, I was able to adapt to my circumstances and put in and get out something extremely valuable. I hope to distill those experiences in this blog post.</p>
</section>
<section id="applied-ai-is-different" class="level2">
<h2 class="anchored" data-anchor-id="applied-ai-is-different">Applied AI is Different</h2>
<p>Taking the AI Evals course was a humbling experience. I thought that because of how much time I had spent learning about deep learning through the fast.ai course, reading research papers, and diving into research codebases would give me some kind of running start for the course. I was completely wrong. Of course, there are transferable skills between any data-related fields, but successful applied AI folks are just built different: I can think of no better example than <a href="https://www.youtube.com/watch?v=N-qAOv_PNPc">the talk by Teresa Torres</a>.</p>
<p>Now, Teresa has a very impressive background. She studied <a href="https://symsys.stanford.edu/">symbolic systems at Stanford</a>, which was a cross-functional, interdisciplinary approach to what seems like human-software interaction and related systems. She also had experience as a technical product manager. However, her background does not make it any less impressive what she was able to accomplish in the given time frame: in a couple of weeks she created an Applied AI product that was set to be integrated with existing software in beta with robust evals. She applied data science skills because it just made sense to, because she needed to, because she had to, because she was terrified of creating a product that was not validated and tested. She was terrified of using AI-generated code that she didn’t understand, so she had to learn Python. She wanted to cleanly and quickly inspect data, so she learned data visualization. She wanted to be cost-effective when using AI, so she recreated the MapReduce methodology for LLMs from first principles. And she did all this in a few weeks. While her velocity and problem-solving abilities are likely unique to her, I don’t think her overall behavior is unique to folks in Applied AI. After my sense of awe and feeling inspired settled down, I couldn’t help but think that people like her just operate differently than I do. I’m a believer that almost anything can be learned, but there’s just something about applied AI that seems mutually exclusive from AI research, data science or data analysis. I can’t quite put my finger on it because I do believe I’m on the outside of that world looking in. But I felt it when I watched Teresa’s talk.</p>
<p>I also felt it when I was listening to other students ask questions and talk about their projects during office hours. I won’t go into specifics because office hours are meant to be private, but student after student had specific, applied, real-production-level questions and problems that they were trying to solve. I think what I was inspired by the most was that they didn’t wait to take the course to solve these problems. They were already figuring things out with whatever tools and skills they currently had, and were taking this course because they (correctly) bet it was going to provide them a system they needed to get the results they desired. You can also see this “production-ready” nature of the cohorts based on <a href="https://x.com/sh_reya/status/1957139727322411291">the hundreds of testimonials of the course</a>.</p>
<p>I also felt the unique nature of applied AI by a consistent theme taught in the course: the quality of error analysis and the quality of your AI evals depend on your product sense and knowledge. “Looking at data” is just as much of a data science skill as it is a product skill. Data science teaches you how, but product sense teaches you why (and where to look). I think it’s why someone like Teresa, who seems to have fantastic product sense, is able to pick up necessary data science skills to execute on her ability “follow the smell” of failure modes.</p>
<p>I think there certainly are unicorns where ML researchers or data scientists also have good product sense. The <a href="https://www.youtube.com/watch?v=DgPr3HVp0eg">folks at AnswerAI</a> certainly seem to fit this description as was evident in the SolveIt walkthrough during the course. Another example is Omar Khattab, who gave a guest talk on DSPy during the course, who seems to see problems and solutions (and systems) differently (and earlier) than the industry at-large. As a fast.ai student/community member and late interaction enjoyer, I am of course biased.</p>
</section>
<section id="the-value-of-mundane-tasks" class="level2">
<h2 class="anchored" data-anchor-id="the-value-of-mundane-tasks">The Value of Mundane Tasks</h2>
<p>A topic that I’ve been meaning to write about but haven’t yet found the time or right opportunity to do so is the skill of looking at data. I think many different paths in data science and ML (and elsewhere) provide opportunities to build this skill. Personally I built this skill by working in low-tech or even no-tech data environments. I think when you’re a lowly analyst cleaning data entry errors or doing manual data entry yourself, you learn viscerally about the pain points in data collection and how those pain points can find their way to inaccuracies in downstream analyses. When you’re going through binders of handwritten notes and printed PDF tables, cross-referencing aggregate numbers with poorly formatted Excel workbooks containing missing data, you build the resilience and patience necessary to thoroughly “look at data” going in and out of LLMs. I think something that has kept me from becoming a better programmer is I’m not lazy and I enjoy a moderate dose (and see the value) of mundane tasks (such as reading hundreds or thousands of LLM outputs). I think those are two characteristics that have helped me become a better data and LLM wrangler. I think it’s also what will help me become a good maintainer.</p>
<p>A more abstract “skill” is the urge to figure out: why are two things that are supposed to be equal, not equal? And then stubbornly resolving that discrepancy, encountering all sorts of roadblocks (and learning opportunities along the way). Prioritizing this urge has accounted for most of my professional development in ML.</p>
</section>
<section id="my-approach-to-the-course" class="level2">
<h2 class="anchored" data-anchor-id="my-approach-to-the-course">My Approach to the Course</h2>
<p>I somewhat organically found my rhythm in this course. I held myself to the following non-negotiable standard:</p>
<ol type="1">
<li>I would attend every lecture live.</li>
<li>I would attend every office hours live.</li>
<li>I would write a blog post with standout ideas <a href="https://vishalbakshi.github.io/blog/index.html#category=AI%20Evals">from each lesson and corresponding course reader chapter</a>.</li>
</ol>
<p>I didn’t do most of the homeworks, I didn’t apply what I learned to the (personal) projects I was working on (that were now back burner projects).</p>
</section>
<section id="applying-applied-ai-skills" class="level2">
<h2 class="anchored" data-anchor-id="applying-applied-ai-skills">Applying Applied AI Skills</h2>
<p>While I can’t commit to a daily or weekly allotment of hours that I’ll spend applying the learnings from this course, there are two concrete tasks that I can commit to finishing before the end of the year:</p>
<ol type="1">
<li>Perform error analysis on each pipeline step in my <a href="https://youtu.be/NwPKy1rqXT8">AgentFasbook project</a> for one chapter of fastbook.</li>
<li>Perform error analysis (and LLM Judge prompt error estimation “in production”) for my <a href="https://youtu.be/FXOXoaGjntc">TinyScaleLab project</a> for one training run.</li>
</ol>
<p>My AgentFastbook project involves expanding my manually curated fastbook-benchmark IR dataset using an LLM pipeline (decomposing a gold answer into atomic facts –&gt; retrieving chapter passages relevant to those facts –&gt; extracting only relevant text from those passages). I learned from Q+A in the course discord that I should perform error analysis on each step first and then on the full “trace” from end-to-end (because the quality of retrieval can for example effect the quality of relevant text extraction).</p>
<p>My TinyScaleLab project aims to train tiny models to 1) generate coherent english (as shown in <a href="https://arxiv.org/abs/2305.07759">the TinyStories paper</a>) and 2) perform small tasks (like gold answer decomposition or text extraction) reliably after multi-stage finetuning.</p>
</section>
<section id="closing-thoughtstestimonial" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughtstestimonial">Closing Thoughts/Testimonial</h2>
<p>I still have a lingering feeling that I didn’t maximize what I put into the course. I think that’s totally on me. The best I can do now is use that as motivation to apply what I’ve learned to my projects.</p>
<p>One unexpected benefit from this course is that my mind is more open to AI applications. Watching the examples in the course, listening to problems that students are solving in real life, really made me question my relationship with LLMs. I don’t think I fully see the potential and myriad of the problems LLMs can solve. I’m not even sure I see the “shapes” of those problems and solutions. I think the only way to bridge the gap between my understanding of applied AI and the understanding of so many people in the course that I witnessed first-hand is to actually engage in that work. Jeremy Howard and Jonathan Whitaker’s SolveIt talk, even though I was a student in their first cohort, made me question my relationship with problem-solving. Teresa’s talk made me question if I am moving with enough courage in this space. Omar’s talk made me question whether I’m investing enough time in systems thinking.</p>
<p>Engaging in this course with a lot of things going on in my life was a grounding and stabilizing experience. Shreya and Hamel, as knowledgeable and brilliant as they are, are equally welcoming and inclusive. I learned a lot about how to hold space for people to ask challenging or vulnerable questions and what it means to actively encourage community and belonging in a distributed, semi-asynchronous, remote setting. There was a very strong resonance amongst everyone in the cohort. You could tell that everyone was on the same frequency, thinking about the same problems, trying to figure out similar solutions. Everyone had a different angle, background, or story to share, whether it was the guest speakers or the students participating in the office hours. I strongly recommend that everyone using LLMs or building LLM applications take this course.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-17-ai-evals/</guid>
  <pubDate>Sun, 17 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Logit Divergence Between Models Differently Converted to torch.bfloat16</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-17-hf-torch-dtype/</link>
  <description><![CDATA[ 




<div id="cell-1" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span></code></pre></div>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post I’ll illustrate a recent head-scratcher I came across—how to convert a model to <code>torch.bfloat16</code> changes the intermediate and final outputs. I don’t know why this happens and not sure of a path to figure that out.</p>
<p>In <code>model1</code> I specify <code>torch_dtype</code> in <code>AutoModelForCausalLM.from_pretrained</code>. In <code>model2</code>, I don’t, and instead use <code>to(torch.bfloat16)</code> after the model is loaded.</p>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HuggingFaceTB/SmolLM2-135M"</span></span>
<span id="cb2-2">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span></span>
<span id="cb2-3">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb2-4">model1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16).to(device)</span>
<span id="cb2-5">model2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint).to(device).to(torch.bfloat16)</span></code></pre></div>
</div>
</section>
<section id="comparing-logits" class="level2">
<h2 class="anchored" data-anchor-id="comparing-logits">Comparing Logits</h2>
<p>Given a set of input tokens, the output logits of the two models are not identical.</p>
<div id="cell-8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="80502c56-d418-4fba-dde2-a134dcd209db">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Gravity is"</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>).to(device)</span>
<span id="cb3-2">inputs</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>tensor([[22007,  6463,   314]], device='cuda:0')</code></pre>
</div>
</div>
<div id="cell-9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="782d67bf-c79b-45cc-fb92-9981b1528788">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">model1.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb5-2">logits1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model1(inputs).logits</span>
<span id="cb5-3">logits1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>tensor([[[18.0000, 14.5625, 14.6875,  ..., 16.2500, 16.2500, 22.1250],
         [15.6875, -0.4180, -0.3477,  ...,  8.2500, 12.1250,  7.3438],
         [12.1875, -2.2812, -2.2031,  ...,  7.3750, 10.6875,  8.1875]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;UnsafeViewBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-10" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cf487415-91c9-4464-c064-a506c4a090cb">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">model2.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb7-2">logits2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model2(inputs).logits</span>
<span id="cb7-3">logits2</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>tensor([[[18.0000, 14.5625, 14.6875,  ..., 16.2500, 16.2500, 22.1250],
         [15.7500, -0.2715, -0.2002,  ...,  8.4375, 12.2500,  7.5000],
         [12.3125, -2.2188, -2.1406,  ...,  7.5000, 10.6875,  8.3125]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;UnsafeViewBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9c6a1f1e-6ce9-4a20-ee2d-ef92a3a9f82e">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">torch.allclose(logits1, logits2)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>False</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1377752c-96df-4195-9d6f-052bbbe3cf7d">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">(logits1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> logits2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor(0.3457, device='cuda:0')</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="41d1e426-3945-40e1-c97b-f2e20debb62e">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(logits1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> logits2).mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor(0.0762, device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;MeanBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="comparing-weights" class="level2">
<h2 class="anchored" data-anchor-id="comparing-weights">Comparing Weights</h2>
<p>A helper function to inspect a particular submodule in a particular layer.</p>
<div id="cell-16" class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _print(model1, model2, module, submodule, layer_idx):</span>
<span id="cb15-2">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model1.model.layers[layer_idx], module), submodule).weight</span>
<span id="cb15-3">    w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model2.model.layers[layer_idx], module), submodule).weight</span>
<span id="cb15-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> torch.allclose: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>allclose(w1, w2)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<div id="cell-17" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2c8feb33-9f2e-4511-9518-0d313209b7d1">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">_print(model1, model2, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"self_attn"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"q_proj"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>self_attn.q_proj torch.allclose: True</code></pre>
</div>
</div>
<p>Looping through all weight matrices in state dicts, they are all identical—why are output logits not identical then? I would assume that something in the matrix ops is causing the divergence.</p>
<div id="cell-19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6f87344e-7033-4bee-e12c-ff02b8bb7a89">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb18-2">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb18-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> model1.state_dict().keys():</span>
<span id="cb18-4">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model1.state_dict()[k]</span>
<span id="cb18-5">    w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model2.state_dict()[k]</span>
<span id="cb18-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> torch.allclose(w1, w2): n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb18-7">    d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb18-8">n, d, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>d</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(273, 273, 1.0)</code></pre>
</div>
</div>
</section>
<section id="forward-hooks" class="level2">
<h2 class="anchored" data-anchor-id="forward-hooks">Forward Hooks</h2>
<p>Hooking the two models to track intermediate layer outputs.</p>
<div id="cell-22" class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">model1.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb20-2">model2.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb20-3">outputs_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span></code></pre></div>
</div>
<div id="cell-23" class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> capture_output(name):</span>
<span id="cb21-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> hook_fn(module, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, output):</span>
<span id="cb21-3">        outputs_dict[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].detach()</span>
<span id="cb21-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> hook_fn</span></code></pre></div>
</div>
<div id="cell-24" class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">hooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb22-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>):</span>
<span id="cb22-3">    hooks.append(model1.model.layers[i].register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb22-4">    hooks.append(model2.model.layers[i].register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span></code></pre></div>
</div>
<div id="cell-25" class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb23-2">    model1(inputs)</span>
<span id="cb23-3">    model2(inputs)</span></code></pre></div>
</div>
<div id="cell-26" class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> h <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hooks: h.remove()</span></code></pre></div>
</div>
<p>The difference in intermediate outputs diverges as you pass through the model. That smells of typical floating point precision error.</p>
<div id="cell-28" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6c7066d4-d24a-4f37-da3c-c396f78e69e0">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mean"</span></span>
<span id="cb25-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>):</span>
<span id="cb25-3">    o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb25-4">    o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb25-5"></span>
<span id="cb25-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> torch.allclose(o1, o2):</span>
<span id="cb25-7">        max_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>().item()</span>
<span id="cb25-8">        mean_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb25-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: max diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>max_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb25-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mean"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: mean diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mean_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0: mean diff = 0.0017547607421875
Layer 1: mean diff = 0.005035400390625
Layer 2: mean diff = 0.00830078125
Layer 3: mean diff = 0.010986328125
Layer 4: mean diff = 0.011962890625
Layer 5: mean diff = 0.01251220703125
Layer 6: mean diff = 0.01312255859375
Layer 7: mean diff = 0.0137939453125
Layer 8: mean diff = 0.015380859375
Layer 9: mean diff = 0.0172119140625
Layer 10: mean diff = 0.0189208984375
Layer 11: mean diff = 0.0185546875
Layer 12: mean diff = 0.01953125
Layer 13: mean diff = 0.020751953125
Layer 14: mean diff = 0.021728515625
Layer 15: mean diff = 0.0234375
Layer 16: mean diff = 0.026123046875
Layer 17: mean diff = 0.0263671875
Layer 18: mean diff = 0.0269775390625
Layer 19: mean diff = 0.0301513671875
Layer 20: mean diff = 0.03271484375
Layer 21: mean diff = 0.036376953125
Layer 22: mean diff = 0.044921875
Layer 23: mean diff = 0.05322265625
Layer 24: mean diff = 0.05810546875
Layer 25: mean diff = 0.06689453125
Layer 26: mean diff = 0.0771484375
Layer 27: mean diff = 0.091796875
Layer 28: mean diff = 0.1005859375
Layer 29: mean diff = 0.1484375</code></pre>
</div>
</div>
<p>The max difference in outputs reaches <code>6.0</code> by the 30th layer!</p>
<div id="cell-30" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d9b03a21-de97-48f9-919c-9c252f976f5c">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span></span>
<span id="cb27-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>):</span>
<span id="cb27-3">    o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb27-4">    o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb27-5"></span>
<span id="cb27-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> torch.allclose(o1, o2):</span>
<span id="cb27-7">        max_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>().item()</span>
<span id="cb27-8">        mean_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb27-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: max diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>max_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb27-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mean"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: mean diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mean_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0: max diff = 0.0625
Layer 1: max diff = 0.25
Layer 2: max diff = 0.25
Layer 3: max diff = 0.25
Layer 4: max diff = 0.25
Layer 5: max diff = 0.25
Layer 6: max diff = 0.25
Layer 7: max diff = 0.5
Layer 8: max diff = 0.5
Layer 9: max diff = 0.5
Layer 10: max diff = 1.0
Layer 11: max diff = 1.0
Layer 12: max diff = 1.0
Layer 13: max diff = 0.5
Layer 14: max diff = 0.5
Layer 15: max diff = 0.5
Layer 16: max diff = 0.5
Layer 17: max diff = 0.5
Layer 18: max diff = 0.5
Layer 19: max diff = 0.75
Layer 20: max diff = 0.5
Layer 21: max diff = 0.5
Layer 22: max diff = 0.5
Layer 23: max diff = 0.75
Layer 24: max diff = 1.0
Layer 25: max diff = 2.0
Layer 26: max diff = 2.0
Layer 27: max diff = 2.0
Layer 28: max diff = 1.0
Layer 29: max diff = 6.0</code></pre>
</div>
</div>
<p>Reloading the models and inspecting the outputs of intermediate modules like <code>self_attn</code> and <code>mlp</code>.</p>
<div id="cell-32" class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">model1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16).to(device)</span>
<span id="cb29-2">model2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint).to(device).to(torch.bfloat16)</span></code></pre></div>
</div>
<div id="cell-33" class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">modules <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb30-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"self_attn"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"q_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"v_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"o_proj"</span>],</span>
<span id="cb30-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mlp"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gate_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"up_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"down_proj"</span>],</span>
<span id="cb30-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layernorm"</span>: [],</span>
<span id="cb30-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"post_attention_layernorm"</span>: []</span>
<span id="cb30-6">    }</span></code></pre></div>
</div>
<div id="cell-34" class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">outputs_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span></code></pre></div>
</div>
<div id="cell-35" class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> module <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules.keys():</span>
<span id="cb32-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layernorm"</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"post_attention_layernorm"</span>:</span>
<span id="cb32-3">        hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model1.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb32-4">        hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model2.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb32-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb32-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> submodule <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules[module]:</span>
<span id="cb32-7">            hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model1.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module), submodule).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb32-8">            hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model2.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module), submodule).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span></code></pre></div>
</div>
<div id="cell-36" class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb33-2">    model1(inputs)</span>
<span id="cb33-3">    model2(inputs)</span></code></pre></div>
</div>
<div id="cell-37" class="cell">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> h <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hooks: h.remove()</span></code></pre></div>
</div>
<p>Interestingly, the intermediate attention outputs are identical but there’s divergence in the outputs of the attention mechanism as it passes through <code>o_proj</code>.</p>
<div id="cell-39" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ce0830e3-31bc-4554-ce6e-ba3d91c133fb">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> module <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules.keys():</span>
<span id="cb35-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layernorm"</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"post_attention_layernorm"</span>:</span>
<span id="cb35-3">        o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-4">        o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-5">        diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb35-6">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb35-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb35-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> submodule <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules[module]:</span>
<span id="cb35-9">            o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-10">            o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-11">            diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb35-12">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>self_attn.q_proj: 0.0
self_attn.k_proj: 0.0
self_attn.v_proj: 0.0
self_attn.o_proj: 2.1457672119140625e-05
mlp.gate_proj: 0.000476837158203125
mlp.up_proj: 0.0003833770751953125
mlp.down_proj: 0.00177764892578125
input_layernorm: 0.0
post_attention_layernorm: 1.8715858459472656e-05</code></pre>
</div>
</div>
<p>Again I haven’t dug into why these differences exist, but wanted to document that they do.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>deep learning</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-17-hf-torch-dtype/</guid>
  <pubDate>Sun, 17 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 8 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-14-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-8-course-review-and-live-coding-an-annotation-app" class="level2">
<h2 class="anchored" data-anchor-id="lesson-8-course-review-and-live-coding-an-annotation-app">Lesson 8: Course Review and Live Coding an Annotation App</h2>
<p><strong>Idea 1: You should have AI-in-the-loop,</strong> as opposed to being the “human-in-the-loop”. You should drive the AI. You should understand every step of the AI pipeline (as is learned through the process of doing error analysis). As we’ve seen throughout the course, there are strategic opportunities to use LLMs to <em>supplement</em> your analysis, potentially evolving to full automation with routine human validation (e.g.&nbsp;production LLM Judge outputs sampled and reviewed every week).</p>
<p><strong>Idea 2: Provide a detailed prompt when vibe coding an annotation app.</strong> Shreya provided the direct path to the trace CSV, an explanation of the CSV and message column structures (content/roles), and the goals/key characteristics of the app and its UI/UX (such as open coding, a progress bar, navigation buttons and a dropdown of previously used annotations). Shreya asks the LLM to create a <code>plan.md</code> when coding something for the first time and reviews/provides feedback before finalizing it. You can always follow up with more requirements as you review the plan before executing on it.</p>
<p><strong>Idea 3: Implement heuristics for where the user’s attention should go.</strong> One example is semantic and/or keyword highlighting in the displayed trace based on previous annotations. This will help the user more easily identify common failure modes (which supports the core goal: reduce friction! Ease cognitive load!).</p>
<p><strong>Idea 4: Common preferred annotation app UI characteristics</strong> <mark>(note: these are just examples, you should think about your own app requirements when building it)</mark>: expanding/collapse the system prompt, “keywords from user query” at the top which are highlighted in the messages listed below it, annotation should always be visible (shouldn’t require scrolling to view it), highlight domain-specific details (like dates/times for scheduling requests), visually flagged duplicate assistant messages, cleanly render raw JSON messages/tool calls.</p>
<p><strong>Idea 5: Think step by step.</strong> That means you too, not just the LLM! To improve accuracy, start with low hanging fruit (disambiguate your prompt/instructions), then tackle more involved tasks (decomposing the task into smaller subtasks that are easier for the LLM to handle), and finally approach advanced strategies (e.g.&nbsp;fine-tuning, prompt optimization, human review loops to generate more labeled examples).</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-14-ai-evals/</guid>
  <pubDate>Thu, 14 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 7 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-12-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-7-interfaces-for-human-review" class="level2">
<h2 class="anchored" data-anchor-id="lesson-7-interfaces-for-human-review">Lesson 7: Interfaces for Human Review</h2>
<p><strong>Idea 1: Custom UIs = 10x review throughput</strong> compared to reviewing in a spreadsheet. This because custom UIs allow a domain-aware view (emails structured like your inbox instead of a string of text) and hotkeys for navigation or one-click tags, <em>and</em> takes only 1 hr to prototype nowadays. A middle-ground between spreadsheets and custom UIs: jupyter notebook (a pseudo-interface) especially with the <a href="https://ipython.readthedocs.io/en/stable/config/integrating.html#rich-display:~:text=_repr_html_%20should%20return%20HTML%20as%20a%20str"><code>_repr_html_</code> method</a>.</p>
<p><strong>Idea 2: HCI Principles for UIs (Nielsen, 1994).</strong> Visibility of status (let your user know where they are), recognition over recall (assign tags instead of free-form text in second round of error analysis and beyond), match the real world (native end user display form; results in catching errors only apparent in this form), user control (pass/fail 1-key press, undo, tag select w/number keys, “defer” for uncertainty, goal: <em>get the user into a flow state</em>), minimalist first (expand on demand). Add a progress bar whenever you’re making a user wait for something. Overall principle: <mark>reduce friction</mark>.</p>
<p><strong>Idea 3: Nerd-snipe your features.</strong> Shreya implemented a highlight feature where on the backend their app looks for semantic or keyword similarities with previous failed samples and highlights those words in the current example display to flag common issues for easier user identification. Super cool. Another similar example: batch-label similar traces after clustering to wipeout repeat bugs. I never considered integrating machine intelligence into error analysis before this!</p>
<p><strong>Idea 4: Criteria drift happens!</strong> Reviewers’ definitions change over time so keep rubrics and labels editable. What you think was acceptable/unacceptable changes as you review real traces. Additionally, humans’ understanding of LLM capabilities also evolves over time (i.e.&nbsp;humans align with LLMs as LLMs align with humans).</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-12-ai-evals/</guid>
  <pubDate>Tue, 12 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lessons 5 + 6 and Chapter 7 from the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-07-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-5-architecture-specific-evaluation-strategies" class="level2">
<h2 class="anchored" data-anchor-id="lesson-5-architecture-specific-evaluation-strategies">Lesson 5: Architecture-Specific Evaluation Strategies</h2>
<p><strong>Idea 1: Include edge cases in few shot examples</strong>. Show the LLM examples that you might struggle with, give it a lot of information in each example. I imagine that you will gain a better understanding of what truly contributes to Pass or Fail judgments as you curate difficult few shot examples. Don’t just randomly pick examples, cherry pick them based on how well they complement the rest of your prompt. Give examples that are the most instructive.</p>
<p><strong>Idea 2: Use automation strategically</strong>. We don’t want to not look at our data, but we also want to use the reasoning power of LLMs. Shreya fed an LLM their open codes, axial codes and traces and asked it to label true/false (if LLM responses in trace are substantiated with tool outputs) for each trace and provide a rationale. Shreya trusted the LLM’s true/false labels because they provided it with open codes.</p>
<p><strong>Idea 3: Don’t provide open/axial codes in LLM Judge Prompt few shot examples.</strong> We’re using this judge in production on unlabeled traces which will not have open/axial codes so we don’t want the LLM Judge to learn/expect these codes to be present.</p>
<p><strong>Idea 4: Aim for 80-85% LLM Judge TNR and TPR.</strong> 50% is random chance, 100% probably means something’s wrong in your judge prompt.</p>
<p><strong>Idea 5: The fastest way for you to fail in an AI project is for people to lose trust in what you’re doing.</strong> There’s a human bias that people have in trusting what a computer says. Don’t take the judge at face value. Run some tests to evaluate the confidence interval of the Judge, unbiasing its success rate (bias = Judge labels “Pass” more than “Fail” by default or vice vesa). The eval should align with the product experience.</p>
</section>
<section id="lesson-6-rag-cicd" class="level2">
<h2 class="anchored" data-anchor-id="lesson-6-rag-cicd">Lesson 6: RAG, CI/CD</h2>
<p><strong>Idea 1 Do error analysis on the whole system, but do evals on retrieval and LLM generation separately.</strong> Make sure your retriever’s Recall@k is 80%+, then perform error analysis, otherwise you’re evaluating generation errors based on flawed context. Don’t use popular metrics to evaluate generation—measure what’s relevant to your product, which you will uncover during your analysis.</p>
<p><strong>Idea 2 Bring domain knowledge to chunk size.</strong> Is there a natural breaking point in your document? What is a meaningful chunk in the context of your domain? The chunks ultimately represent the document during search. It’s okay to have variable chunk sizes.</p>
<p><strong>Idea 3 Likert scales have a use!</strong> Shreya asks an LLM to score synthetic queries (when creating an evaluation dataset) on a Likert scale and filters out queries with scores of 1 or 2 (out of 5). These scores are discarded after this filtering use.</p>
<p><strong>Idea 4 Ground synthetic queries in realism:</strong> User queries are often confusing to interpret, incomplete, and contain typos/grammatical errors. The queries in your evaluation dataset should reflect such nuances to provide meaningful use cases for retrieval.</p>
<p><strong>Idea 5 Optimize for Recall@k first.</strong> Shreya has rarely seen utility in optimizing for Precision@k first (how many of the top-k retrieved chunks are relevant?) because the consumer of these chunks in a RAG pipeline is an LLM, which cares more about how many of the total relevant chunks are present in the top-k retrieved chunks (Recall@k) to generate a relevant response. LLMs are getting better at reasoning over the retrieved chunks to determine relevance. Use MRR@k (how high up in the ranking is the first relevant chunk?) after optimizing for Recall@k as MRR@k measures how quickly the LLM finds <em>an</em> answer.</p>
<p><strong>Idea 6: Focus on process, not tools.</strong> Which goes against how most people think about building AI systems. If something’s not working, your first instinct should be to actually understand what is going wrong, not to plug-in a different tool in hopes for improvement, or sweep different hyperparameters. Additionally, don’t get lost in the vector DB sauce—start with basic BM25 keyword search first.</p>
</section>
<section id="lesson-7-evaluating-retrieval-augmented-generation-rag" class="level2">
<h2 class="anchored" data-anchor-id="lesson-7-evaluating-retrieval-augmented-generation-rag">Lesson 7: Evaluating Retrieval-Augmented Generation (RAG)</h2>
<p><strong>Idea 1: Multi-stage retrieval and the Recall/Precision trade-off</strong>. LLMs can handle many passages so we want to make sure it’s provided as many relevant passages as possible, this means increasing the number of passages provided. However, long contexts cost more and are limited by the LLM’s context window. We use a cheaper retriever to do a first pass on retrieving relevant passages (Recall@k) and a more powerful retriever to then re-rank them (Precision@k, MRR or NDCG@k). We take the top-k (where k is smaller than the first pass retrieved passages) and pass that to the LLM.</p>
<blockquote class="blockquote">
<p>Modern LLM attend more strongly to salient tokens, so they can often ignore irrelevant content if the key information is present. But if that information is missing altogether–low recall–then the generator has no way to produce a correct answer.</p>
</blockquote>
<p><strong>Idea 2: The ARES framework complements error analysis.</strong> Precisely evaluating “Answer Faithfulness” (failures include hallucinations, omissions and misinterpretations) and “Answer Relevance” (failure = factually correct based on context but irrelevant to the query) requires error analysis to identify where and how specific failure modes occur in our product.</p>
<p><strong>Idea 3: Be wary of synthentically generated queries</strong> as they often are not representative of the messy queries encountered in production. Regularly validate these queries, referncing real queries from logs or human-curated examples.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-07-ai-evals/</guid>
  <pubDate>Thu, 07 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 4 and Chapter 5 of the Course Reader from the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-03-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-4-automated-evaluators" class="level2">
<h2 class="anchored" data-anchor-id="lesson-4-automated-evaluators">Lesson 4: Automated Evaluators</h2>
<p><strong>Idea 1: You can’t measure what you don’t ask for.</strong> If your prompt didn’t include an instruction on providing links to X, and the LLM doesn’t provide those links, that’s a specification failure (fix the prompt!). If the prompt did include it but the LLM failed to apply the instruction, that’s a generalization failure and should be tracked by an automated evaluator.</p>
<p><strong>Idea 2: Use code-based evaluators if you can,</strong> as they are deterministic. They take as input the trace and a failure mode and return <code>True</code> or <code>False</code> or some score for objective rule-based checks (e.g.&nbsp;parsing structure, regex/string matching for keywords, structural constraints, tool execution errors).</p>
<p><strong>Idea 3: Just because you can ask an LLM Judge anything you want, doesn’t mean you should.</strong> Use LLM Judges to do specific, well-defined, binary failure mode classification (Pass/Fail) tasks. A Pass/Fail LLM Judge score is easier to assess and leads to easy-to-interpret Judge accuracy. Also, don’t pack multiple criteria into one prompt, create a prompt for each criterion.</p>
<p><strong>Idea 4: Don’t leak test instances into your process of building an LLM Judge</strong> Use 10-20% of labeled axial coding data to curate Judge prompt few shot examples (training set), ~40% to iteratively improve the prompt (dev set), and ~40% for final unbiased Judge evalation after prompt tuning is done (test set). The last thing you want in your prompt is a few shot example that’s in the test set. <mark>Low dev set performance (TPR and TNR) tell us that the few shot examples from the train set do not generalize.</mark></p>
<p><strong>Idea 5: Don’t show your LLM Judge what it’s already good at.</strong> Your few shot examples should show difficult/tricky situations for evaluation. To do this, manually iterate the examples. I would imagine that like open coding, you would build a more nuanced intuition about your data (and your product!) through this process. Other tips: write your examples like you are explaining it to a human; try to include the best example of a pass or fail; your examples can also contain reasoning to provide richer “grounding” to your LLM.</p>
</section>
<section id="chapter-5-implementing-automated-evaluators" class="level2">
<h2 class="anchored" data-anchor-id="chapter-5-implementing-automated-evaluators">Chapter 5: Implementing Automated Evaluators</h2>
<p><strong>Idea A: Reference-based and reference-free metrics serve different purposes.</strong> A “reference” here means “reference LLM output”. Reference-based metrics allow iterative development with holistic checks (whether the LLM output match the golden reference). Reference-free metrics better adapt at scale on new, unlabeled data (as they measure intrinsic properties or rules related to failure modes). Reference-based metric: LLM output matches a “golden” trace with a specific sequence of tool calls. Reference-free metric: LLM output contains valid tool call names.</p>
<p><strong>Idea B: Test your judge on unlabeled data.</strong> Even the test set is biased as it represents a portion of our labeled data, which may not be representative of broader out-of-domain situations your Judge will inevitably encounter. We use the Judge’s “raw success rate” (number of Pass labels/number of unlabeled traces) and a series of calculations on random test set samples to estimate within a confidence interval the Judge’s “true success rate.”</p>
<p><strong>Idea C: Judges don’t come pretrained on a product’s values—we have to teach them.</strong> This is why we validate the judge’s response and calculate metrics around alignment (like TNR and TPR). This is also why we provide few shot examples so the Judge can evaluate on the specific desired characteristics of our product (especially for vibe-y dimensions like tone).</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-03-ai-evals/</guid>
  <pubDate>Sun, 03 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>TIL: How LLM-Foundry Ignores Prompt Tokens in labels for Instruction Finetuning.</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-31-llm-foundry/</link>
  <description><![CDATA[ 




<p>I recently realized (after a conversation with a colleague) that I didn’t know how LLM-Foundry replaces prompt tokens with <code>-100</code> to exclude them from the loss calculation. I fed Claude project knowledge the LLM-Foundry GitHub repo and asked it to walk me through this process line-by-line. Since “Chat using integrations can’t be shared” I’m pasting Claude’s walkthrough here (with a couple of my notes). All the referenced functions are inside <a href="https://github.com/mosaicml/llm-foundry/blob/main/llmfoundry/data/finetuning/collator.py"><code>llmfoundry/data/finetuning/collator.py</code></a></p>
<section id="setup-input-example" class="level2">
<h2 class="anchored" data-anchor-id="setup-input-example">Setup: Input Example</h2>
<p>Let’s say we have this raw training example:</p>
<pre><code>{
    'prompt': 'What is photosynthesis?',
    'response': 'Photosynthesis is the process by which plants convert sunlight into energy.'
}</code></pre>
<p>After tokenization, this becomes:</p>
<pre><code>example = {
    'turns': [
        {
            'input_ids': [1127, 318, 2825, 43943, 30],      # "What is photosynthesis?"
            'labels': [21197, 43943, 318, 262, 1429, 416, 543, 6134, 10385, 4252, 1657, 656, 2568, 13]  # "Photosynthesis is the process..."
        }
    ]
}</code></pre>
</section>
<section id="line-by-line-processing" class="level2">
<h2 class="anchored" data-anchor-id="line-by-line-processing">Line-by-Line Processing</h2>
<section id="collator-entry-point-seq2seqfinetuningcollator.__call__" class="level3">
<h3 class="anchored" data-anchor-id="collator-entry-point-seq2seqfinetuningcollator.__call__">Collator Entry Point (<code>Seq2SeqFinetuningCollator.__call__</code>)</h3>
<pre><code>def __call__(self, examples: list[TokenizedExample]) -&gt; dict[str, torch.Tensor]:
    # examples is a list of TokenizedExample objects (batch)
    
    if self.decoder_only_format:
        batch = self._process_and_batch_decoder_only(examples)  # Most common path</code></pre>
</section>
<section id="batch-processing-_process_and_batch_decoder_only" class="level3">
<h3 class="anchored" data-anchor-id="batch-processing-_process_and_batch_decoder_only">Batch Processing (<code>_process_and_batch_decoder_only</code>)</h3>
<pre><code>def _process_and_batch_decoder_only(self, examples: list[TokenizedExample]):
    processed_examples = []
    
    # Step 1: Process each example in the batch through stitch_turns_decoder_only
    input_ids_and_labels = [
        stitch_turns_decoder_only(
            example_turns=example['turns'],           # The prompt/response turns
            target_prompts=self.target_prompts,      # 'none' 
            target_responses=self.target_responses,   # 'last'
            eos_token_id=self.tokenizer.eos_token_id,
        ) for example in examples
    ]</code></pre>
</section>
<section id="core-processing-stitch_turns_decoder_only" class="level3">
<h3 class="anchored" data-anchor-id="core-processing-stitch_turns_decoder_only">Core Processing (<code>stitch_turns_decoder_only</code>)</h3>
<p>This is where the magic happens:</p>
<pre><code>def stitch_turns_decoder_only(example_turns, target_prompts, target_responses, eos_token_id):
    target_prompts = target_prompts.lower()  # 'none'
    target_responses = target_responses.lower()  # 'last'
    
    # Line 1: Look up the target policy functions
    prompt_to_target = _TARGET_POLICY_LOOKUP[target_prompts]  # Gets _sequence_to_labels_none
    response_to_target = _TARGET_POLICY_LOOKUP[target_responses]  # Gets _sequence_to_labels_last
    
    # Line 2: Initialize output sequences
    input_ids = []
    labels = []
    
    # Line 3: Process each turn in the conversation
    for idx, turn in enumerate(example_turns):
        is_last_turn = idx + 1 == len(example_turns)  # True for our single-turn example
        
        # Line 4: Extract the prompt and response token sequences
        context = ensure_list(turn['input_ids'])   # [1127, 318, 2825, 43943, 30] - "What is photosynthesis?"  
        target = ensure_list(turn['labels'])       # [21197, 43943, 318, ...] - "Photosynthesis is..."
        
        # Line 5: Add EOS token to final response if needed
        if is_last_turn and eos_token_id is not None:
            if target[-1] != eos_token_id:
                target = target + [eos_token_id]   # Append &lt;|endoftext|&gt; token
        
        # Line 6: Build the concatenated input_ids sequence
        input_ids += context  # Add prompt tokens: [1127, 318, 2825, 43943, 30]
        input_ids += target   # Add response tokens: [1127, 318, 2825, 43943, 30, 21197, 43943, 318, ...]
        
        # Line 7: Build the labels sequence using target policies
        labels += prompt_to_target(context, is_last_turn, prompt_cutoff)  # Calls _sequence_to_labels_none
        labels += response_to_target(target, is_last_turn)                # Calls _sequence_to_labels_last</code></pre>
<blockquote class="blockquote">
<p>Vishal note: in Line 7, <code>labels</code> starts out as an empty list <code>[]</code> so the prompt is added first (via <code>+= prompt_to_target(...)</code>), after which the response is added (via <code>+= response_to_target</code>).</p>
</blockquote>
</section>
<section id="target-policy-functions" class="level3">
<h3 class="anchored" data-anchor-id="target-policy-functions">Target Policy Functions</h3>
<pre><code>def _sequence_to_labels_none(sequence, is_last_turn, cutoff):
    # Line 1: Convert ALL prompt tokens to -100 (ignore during loss)
    return [CROSS_ENTROPY_IGNORE_INDEX] * len(sequence)  # [-100, -100, -100, -100, -100]

def _sequence_to_labels_last(sequence, is_last_turn, cutoff):  
    # Line 1: Check if this is the final turn
    if is_last_turn:  # True in our case
        return sequence  # Keep actual token IDs: [21197, 43943, 318, ...]
    else:
        return [CROSS_ENTROPY_IGNORE_INDEX] * len(sequence)  # Would be [-100, -100, ...] for non-final turns</code></pre>
</section>
<section id="result-after-stitch_turns_decoder_only" class="level3">
<h3 class="anchored" data-anchor-id="result-after-stitch_turns_decoder_only">Result After <code>stitch_turns_decoder_only</code></h3>
<pre><code># Returned values:
input_ids = [1127, 318, 2825, 43943, 30, 21197, 43943, 318, 262, 1429, 416, 543, 6134, 10385, 4252, 1657, 656, 2568, 13, 50256]
#            |----------prompt----------|  |-------------------response---------------------------|  |EOS|

labels =    [-100, -100, -100, -100, -100, 21197, 43943, 318, 262, 1429, 416, 543, 6134, 10385, 4252, 1657, 656, 2568, 13, 50256]
#            |------ignore prompt-------|  |-------------------keep response--------------------|  |EOS|</code></pre>
<blockquote class="blockquote">
<p>Vishal note: we added <code>prompt_to_target</code> first to the empty <code>labels</code> list so the <code>-100</code>s come first.</p>
</blockquote>
</section>
<section id="back-to-batch-processing-continued" class="level3">
<h3 class="anchored" data-anchor-id="back-to-batch-processing-continued">Back to Batch Processing (continued)</h3>
<pre><code># Step 2: Handle sequence length limits
for input_ids, labels in input_ids_and_labels:
    orig_size = len(input_ids)
    
    # Truncate if too long
    if orig_size &gt; max_seq_len:
        input_ids = input_ids[:max_seq_len]
        labels = labels[:max_seq_len]
    
    # Create attention mask
    attention_mask = [1] * len(input_ids)  # All 1s for actual tokens
    
    # Step 3: Pad sequences to max_seq_len
    n_total = len(input_ids)
    i_pad = [CROSS_ENTROPY_IGNORE_INDEX] * (max_seq_len - n_total)  # Padding with -100
    
    if self.tokenizer.padding_side == 'left':
        labels = i_pad + labels  # Left pad labels with -100
    else:
        labels = labels + i_pad  # Right pad labels with -100 (more common)
    
    processed_example = {
        'input_ids': input_ids,      # Will be padded by tokenizer.pad()
        'labels': labels,            # Already padded with -100
        'attention_mask': attention_mask,  # Will be padded by tokenizer.pad()
    }
    processed_examples.append(processed_example)

# Step 4: Final padding using HuggingFace tokenizer
batch = self.tokenizer.pad(
    processed_examples,
    padding='max_length',
    max_length=max_seq_len,
    return_tensors='pt',
)</code></pre>
</section>
<section id="final-batch-output" class="level3">
<h3 class="anchored" data-anchor-id="final-batch-output">Final Batch Output</h3>
<pre><code>batch = {
    'input_ids': torch.tensor([
        [1127, 318, 2825, 43943, 30, 21197, 43943, 318, ..., 0, 0, 0]  # Padded with pad_token_id (0)
    ]),
    'labels': torch.tensor([
        [-100, -100, -100, -100, -100, 21197, 43943, 318, ..., -100, -100, -100]  # Padded with -100
    ]),
    'attention_mask': torch.tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0]  # 1 for real tokens, 0 for padding
    ])
}</code></pre>
</section>
<section id="key-points" class="level3">
<h3 class="anchored" data-anchor-id="key-points">Key Points</h3>
<ol type="1">
<li>Prompt tokens → Labels = -100: The _sequence_to_labels_none function converts all prompt tokens to -100</li>
<li>Response tokens → Labels = actual token IDs: The _sequence_to_labels_last function keeps response tokens as-is</li>
<li>Padding tokens → Labels = -100: All padding positions also get -100</li>
<li>Loss calculation: PyTorch’s CrossEntropyLoss(ignore_index=-100) will only compute loss on response tokens</li>
</ol>
<p>This ensures the model learns to generate responses but never tries to “predict” the input prompt!</p>


</section>
</section>

 ]]></description>
  <category>LLM-Foundry</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-31-llm-foundry/</guid>
  <pubDate>Thu, 31 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 3 and Chapter 4 of the Course Reader from the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-31-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-3-error-analysis-contd." class="level2">
<h2 class="anchored" data-anchor-id="lesson-3-error-analysis-contd.">Lesson 3: Error Analysis (cont’d.)</h2>
<p><strong>Idea 1: Open code at the trace-level</strong>. Many errors only emerge in the context of the whole trace and mistakes often cascade across turns (i.e.&nbsp;label only the <em>first</em> failure).</p>
<p><strong>Idea 2: Manually classify open codes using AI-generated axial codes.</strong> This verifies whether the AI-generated codes are applicable (accurate and relevant). During the lesson ChatGPT missed “Did not invoke tool” in its initial axial code generation based on open codes. This type of mistake is common and is why we manually apply LLM-generated axial codes.</p>
<p><strong>Idea 3: If possible, reproduce a multi-turn error with a simpler single-turn test case</strong>. For example if a multi-turn conversation fails when the LLM tries to retrieve some information, (e.g.&nbsp;return the correct price for product X), create a new single-turn conversation targeting just that task. This “minimal reproducible error” is analogous to software engineering’s “minimal reproducible bug”. In both cases, you’re cutting through the noise and targeting the single task that fails; this help find the root cause.</p>
</section>
<section id="chapter-4-collaborative-evaluation-practices" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4-collaborative-evaluation-practices">Chapter 4: Collaborative Evaluation Practices</h2>
<p><strong>Idea A: When possible, have only one person make the final judgment call on AI evaluation</strong>. You want a single person making decisions about the success or failure of the AI outputs to reduce noisy cooks in the kitchen. You need that person to have deep domain knowledge or be someone who represents the target users. Hamel and Shreya call this person the Principal Domain Expert.</p>
<p><strong>Idea B: Annotator disagreements inform rubric improvements, not retroactive label updates</strong>. You’re not trying to win an argument. This idea reminds me of being on an interview panel: when you discuss rubric scores with the highest disagreement, the goal is not for panelists to change their scores, the goal is to come to a common understanding of what rubric item was measuring, and if needed, update your understanding to reach concensus for future candidate assessments. If your process is flawed, try to correct it as soon as you can.</p>
<p><strong>Idea C: Battle-test your artifacts manually</strong>. The iteratively improved human annotator rubric and the concensus labeled dataset becomes the gold standard used for automated evaluators. The rubric becomes the specification passed to the LLM-Judge. A recurring theme in this course: don’t build something in the abstract, build it while grounded in real data. Just as we don’t predefine axial codes for failure modes before we look at the data and document open codes, we don’t predefine a rubric for the LLM Judge before we look at the data. There’s a feedback loop between error analysis artifacts (open codes, axial codes, annotator rubrics, annotation scores) and looking at your data. Looking at data builds your intuition which then informs the error analysis artifacts.</p>
<p><strong>Idea D: Even with multiple annotators, there’s an escalation path.</strong> A benevolent dictator may need to intervene and make the final call if the annotators can’t come to concensus. This underscores the importance of identifying a single Principal Domain Expert <em>even during collaborative evaluation</em>.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-31-ai-evals/</guid>
  <pubDate>Thu, 31 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 2 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-28-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-2-error-analysis" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-error-analysis">Lesson 2: Error Analysis</h2>
<p><strong>Idea 1: Error analysis is how we close the gulf of comprehension.</strong> The gulf of comprehension is the gap between the information the data contains and your understanding of it. While it’s either impossible or unreasonable to look at every single piece of data collected from your users, Hamel and Shreya recommend looking at at least 100 traces.</p>
<p><strong>Idea 2: You need ~100 diverse traces to get a good representation of failure modes.</strong> A trace is a full record of the pipeline’s interaction (initil user query, all LLM inputs/outs, intermediate reasoning or tool calls, and final user-facing result). The number 100 is not a hard and fast rule, but signifies that 10-15 traces is not going to be enough. You want to look at enough traces such that you reach a theoretical saturation of failure modes. What that means is you’ve seen enough traces that are diverse enough that looking at any more traces will not introduce any new failure modes that you haven’t seen yet.</p>
<p><strong>Idea 3: Open coding is Hamel’s favorite subject in evals.</strong> Open codes are brief, descriptive notes about any observed problems, surprising actions or where behavior feels wrong or unexpected in the trace. As you write open codes, categories of errors will emerge.</p>
<p><strong>Idea 4: During open coding, don’t find the root cause, just observe and note.</strong> As you come across failed traces, it’s tempting to make note of why you think this failure occurred. For example, if you see that the LLM tool call return value did not contain the correct information, it’s tempting to step back and think about why that took place. Maybe the input to the tool was incorrect because your voice agent converted speech to text incorrectly. Maybe there’s something wrong in the database which resulted in leading to missed matches. As you can see, this type of pontification can potentially be endless and is not fruitful for the focused process of identifying failure modes. Just note the failure and move on to the next trace. We’ll think about root causes later.</p>
<p><strong>Idea 5: Pull the main failure mode from each trace. don’t get embroiled in the details.</strong> Relatedly, it’s tempting to zoom in as deep as you can, putting each word of the trace under a microscope. For folks who like analyzing data, this is satisfying. But it introduces noise in identifying high-priority failure modes. These open codes are later going to be clustered by theme (Axial Coding) and higher-level themes will be extracted; too granular of an analysis is a waste of time.</p>
<p><strong>Idea 6: The quality of open coding is going to hinge on your product sense.</strong> Successful open coding depends on your ability to skillfully look at data. This involves two broad skills: 1) you have to know what to look for, 2) you have to be detail-oriented enough to find the failure by looking at a sequence of messages. Knowing what to look for requires a deep understanding of your product from the user’s perspective.</p>
<p><strong>Idea 7: Start with a simple approach to get value fast.</strong> Fatigue from cognitive load is a real thing, especially when you’re looking at a hundred traces, many of which can involve multiple messages between the user and the assistant and multiple tool calls. The success of error analysis is going to depend on how fresh your mind is during the process. To achieve this, keep your open coding and axial coding heuristics simple. You can always do a second, third, and fourth pass through future traces after you fix the most pressing failure modes.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-28-ai-evals/</guid>
  <pubDate>Mon, 28 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Analyzing PyTorch 2.0 Release Notes for ColBERT Dependency Impact</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-27-torch-colbert/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post, I walk through the <a href="https://github.com/pytorch/pytorch/releases/tag/v2.0.0">PyTorch 2.0 Release Note</a> PRs where I’m estimating there will be some kind of impact to ColBERT as I update the torch dependency to 2.0 (ColBERT is currently dependent on torch==1.13.1). The level of detail in my analysis of a PyTorch PR is not necessarily signifying its importance. In some cases, I am using this analysis as an opportunity to get more familiar with details about the ColBERT codebase (such as the number of instances where <code>torch.cat</code> is used).</p>
</section>
<section id="full-release-notes-analysis" class="level2">
<h2 class="anchored" data-anchor-id="full-release-notes-analysis">Full Release Notes Analysis</h2>
<p>You can find my item-by-item PyTorch 2.0 release notes analysis for ColBERT in <a href="https://docs.google.com/spreadsheets/d/1sUEN7xo5-hLVoxF9NL_ibGxPaKlPzxmnMU46zf3wd-U/edit?usp=sharing">this Google Sheet</a>.</p>
<p>Overall, across 508 PyTorch PRs, I have estimated that 455 of them are not applicable to ColBERT and 42 (8.7%) have a potential impact. I was unclear if or how 11 of the PyTorch 2.0 PRs would affect ColBERT (2.6%).</p>
<p>There are 5 sections in the PyTorch 2.0 Release Notes, here’s a break down of PRs by section that will have a potential (or unclear) impact on ColBERT:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Section</th>
<th style="text-align: center;"># of PRs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Improvements</td>
<td style="text-align: center;">22</td>
</tr>
<tr class="even">
<td style="text-align: center;">Bug Fixes</td>
<td style="text-align: center;">21</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;">Backwards Incompatible Changes</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Deprecations</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>In my estimation, the improvements and bug fixes PRs in PyTorch 2.0 will only improve the performance of ColBERT. That being said, there still may be noticeable differences in indexing, search, and training artifacts which may break tests I write for before/after comparisons.</p>
<p>Fortunately, only two backward-compatible changes may affect ColBERT.</p>
<p>There are 11 subsections in the PyTorch 2.0 Release Notes, here’s a break down of PRs that will have a potential (or unclear) impact on ColBERT:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Subsection</th>
<th style="text-align: center;"># of PRs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">MPS</td>
<td style="text-align: center;">12</td>
</tr>
<tr class="even">
<td style="text-align: center;">Python API</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cuda</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td style="text-align: center;">Releng</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Distributed</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">Build</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ONNX</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td style="text-align: center;">Cpu</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cpp API</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">torch.nn API</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>I am including MPS-related PRs in this analysis just in case we consider making ColBERT compatible with MPS in the future.</p>
<p>I’ll start by analyzing breaking changes, which are likely going to be the most impactful.</p>
</section>
<section id="backwards-incompatible-changes" class="level2">
<h2 class="anchored" data-anchor-id="backwards-incompatible-changes">Backwards Incompatible Changes</h2>
<section id="pr-92731" class="level3">
<h3 class="anchored" data-anchor-id="pr-92731">PR <a href="https://github.com/pytorch/pytorch/pull/92731">#92731</a></h3>
<blockquote class="blockquote">
<p>Gradients are now set to None instead of zeros by default in <code>torch.optim.*.zero_grad()</code> and <code>torch.nn.Module.zero_grad()</code> (#92731)</p>
</blockquote>
<p>There are two lines in ColBERT where<code>zero_grad</code> is called: in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L37">colbert/utils/amp.py</a> and in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L61">colbert/training/training.py</a>. I’m not sure how this change would affect ColBERT behavior, but flagging it as something to keep in mind.</p>
</section>
<section id="pr-92306" class="level3">
<h3 class="anchored" data-anchor-id="pr-92306">PR <a href="https://github.com/pytorch/pytorch/pull/92306">#92306</a></h3>
<blockquote class="blockquote">
<p>Algorithms <code>{Adadelta, Adagrad, Adam, Adamax, AdamW, ASGD, NAdam, RAdam, RMSProp, RProp, SGD}</code> default to faster <code>foreach</code> implementation when on CUDA + differentiable=<code>False</code></p>
</blockquote>
<p>This PR adds the following lines to <code>AdamW</code>, which is used in ColBERT’s <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L60">`training.py</a>:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> foreach <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb1-2">    foreach <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _default_to_foreach(</span>
<span id="cb1-3">        [params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps],</span>
<span id="cb1-4">        differentiable<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>differentiable)</span></code></pre></div>
<p><code>foreach</code> is <code>None</code> in ColBERT, as it’s not specified and that’s what it defaults to (<code>foreach: Optional[bool] = None</code>):</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AdamW(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> p: p.requires_grad, colbert.parameters()), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>config.lr, eps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-8</span>)</span></code></pre></div>
<p>Since this is described as a “faster implementation”, I would expect the training time to decrease. <mark>I’ll be on the lookout for this when comparing training time benchmarks before/after upgrading to PyTorch 2.0</mark>.</p>
</section>
<section id="pr-88913" class="level3">
<h3 class="anchored" data-anchor-id="pr-88913">PR <a href="https://github.com/pytorch/pytorch/pull/88913">#88913</a></h3>
<blockquote class="blockquote">
<p>Update <code>torch.tensor</code> and <code>nn.Parameter</code> to serialize all their attributes (#88913)</p>
</blockquote>
<p><mark>It’s unclear what this PR is doing but since it’s touching the <code>nn.Parameter</code> definition, I’m flagging it.</mark></p>
</section>
</section>
<section id="bug-fixes" class="level2">
<h2 class="anchored" data-anchor-id="bug-fixes">Bug Fixes</h2>
<p>I would expect PyTorch PRs that introduce bug fixes to only positively affect ColBERT. That being said, a positive effect is still a change and can potentially impact concrete artifacts during indexing, search and training. I am planning on curating a baseline set of these artifacts before I test the upgrade to PyTorch 2.0.</p>
<section id="pr-92810" class="level3">
<h3 class="anchored" data-anchor-id="pr-92810">PR <a href="https://github.com/pytorch/pytorch/pull/92810">#92810</a></h3>
<blockquote class="blockquote">
<p>Fix SIGSEGV on a big-endian machine when reading pickle data (#92810)</p>
</blockquote>
<p>The PR states:</p>
<blockquote class="blockquote">
<p>This PR fixes SIGSEGV on a big-endian machine when reading pickle data.</p>
</blockquote>
<p>I’m not familiar with the term “big-endian” so had to look it up:</p>
<blockquote class="blockquote">
<p>A big-endian system stores the most significant byte of a word at the smallest memory address and the least significant byte at the largest. A little-endian system, in contrast, stores the least-significant byte at the smallest address. (<a href="https://en.wikipedia.org/wiki/Endianness">source</a>)</p>
</blockquote>
<p>Claude’s understanding of the cpp method affected by this PR is that it affects the <code>torch.load</code> method. There are a number of ColBERT files that use <code>torch.load</code>:</p>
<ul>
<li>colbert/utils/coalesce.py uses it to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L47">load <code>codes.pt</code></a> (centroid id for each embedding in chunk) and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L66">load <code>residuals.pt</code></a> (16-bits residual for each embedding in chunk).</li>
<li>colbert/search/index_loader.py uses it to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_loader.py#L33">load <code>ivf.pid.pt</code></a> or <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_loader.py#L36"><code>ivf.pt</code></a>.</li>
<li>colbert/utils/utils.py uses it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L44">in <code>torch_load_dnn</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L91"><code>load_checkpoint_raw</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L205"><code>load_ranking</code></a>.</li>
<li>colbert/indexing/index_manager.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/index_manager.py#L20"><code>load_index_part</code></a>.</li>
<li>colbert/indexing/codecs/residual_embeddings.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual_embeddings.py#L86"><code>load_codes</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual_embeddings.py#L93"><code>load_residuals</code></a>.</li>
<li>colbert/indexing/codecs/residual.py uses it in <code>load</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L141">centroids</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L142"><code>avg_residual</code></a>, and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L143"><code>bucket_cutoffs, bucket_weights</code></a>.</li>
<li>colbert/indexing/collection_indexer.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L256"><code>_concatenate_and_split_sample</code></a>.</li>
<li>colbert/index_updater.py uses it in <code>_load_disk_ivf</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L281"><code>ivf.pid.pt</code></a> or <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L286"><code>ivf.pt</code></a>, in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L312"><code>load_chunk_codes</code></a>, and in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L316"><code>_load_chunk_residuals</code></a>.</li>
<li>colbert/tests/index_coalesce_test.py uses it to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L57">multi-file <code>codes.pt</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L66">single-file <code>codes.pt</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L83">multi-file <code>residuals.pt</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L92">single-file <code>residuals.pt</code></a>.</li>
</ul>
</section>
<section id="pr-92315" class="level3">
<h3 class="anchored" data-anchor-id="pr-92315">PR <a href="https://github.com/pytorch/pytorch/pull/92315">#92315</a></h3>
<blockquote class="blockquote">
<p>Fix NVML visible device parsing (#92315)</p>
</blockquote>
<blockquote class="blockquote">
<p>CUDA_VISIBLE_DEVICES can contain either ordinals or UUIDs Extend the logic to be able to parse it by UUID</p>
</blockquote>
<p><mark>I don’t think this would affect any artifacts created during indexing/searching/training but would make it easier for PyTorch to identify GPUs.</mark></p>
</section>
<section id="pr-93095" class="level3">
<h3 class="anchored" data-anchor-id="pr-93095">PR <a href="https://github.com/pytorch/pytorch/pull/93095">#93095</a></h3>
<p>This PR fixes an error in <a href="https://github.com/pytorch/pytorch/issues/93006">#93006</a> when using <code>topk</code>, which is used in the following places in ColBERT:</p>
<ul>
<li><code>get_cells</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L17">colbert/search/candidate_generation.py</a></li>
<li><code>score_pids</code> in colbert/search/index_storage.py to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L127">filter centroids by the threshold</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L152">filter <code>pids</code> using pruned centroid scores</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L161">filter <code>pids</code> using full centroid scores</a></li>
<li>filter scores in <code>colbert_score_reduce</code> for the <code>"flipr"</code> interaction method: <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L146">link1</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L150">link 2</a></li>
</ul>
<p>Claude recommended also consider uses of <code>max</code> and <code>argmax</code> to be potentially impacted:</p>
<ul>
<li><code>get_cells</code> if <code>ncells==1</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L15">colbert/search/candidate_generation.py</a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L121"><code>ColBERT.score</code></a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L135"><code>colbert_score_reduce</code></a></li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCodec.compress_into_codes</code> on <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">GPU</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">CPU</a></li>
<li>colbert/search/strided_tensor.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L74">`StridedTensor.lookup</a></li>
<li>colbert/search/strided_tensor_core.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L27"><code>StridedTensorCore.__init__</code></a></li>
<li>colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L215">`Checkpoint.score</a></li>
<li>colbert/indexing/utils.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L48"><code>optimize_ivf</code></a></li>
</ul>
<p><mark>I would assume that the only impact this PR would have on PyTorch is avoiding any errors during the use of <code>topk</code> (no such errors have been reported on in the open issues).</mark></p>
</section>
<section id="pr-85596" class="level3">
<h3 class="anchored" data-anchor-id="pr-85596">PR <a href="https://github.com/pytorch/pytorch/pull/85596">#85596</a></h3>
<blockquote class="blockquote">
<p>Fix: half reduction with multiple sub-iterators (#85596)</p>
</blockquote>
<p>Fixes <a href="https://github.com/pytorch/pytorch/issues/74438">cuda low-precision reductions on large tensors produce wrong results #74438</a>:</p>
<blockquote class="blockquote">
<p>Reductions with low precision inputs (half, bfloat16) that need sub-iterators accumulate directly in output and thus truncate intermediate results</p>
</blockquote>
<p>This would fix any issues related to the use of <code>half</code> in the following ColBERT files:</p>
<ul>
<li>in <code>ResidualCodec</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L27"><code>centroids</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L35"><code>avg_residual</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L40"><code>bucket_weights</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L159">saving <code>centroids</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L172">compressing token embeddings</a>, calculating cosine similarity between token embeddings and centroids <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">on GPU</a>.</li>
<li>colbert/search/candidate_generation.py in <code>CandidateGeneration.generate_candidates</code> for queries when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L52">using the GPU</a>.</li>
<li>colbert/indexing/collection_indexer.py in <code>CollectionIndexer._sample_embeddings</code> when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L181">saving <code>local_sample_embs</code></a>, in <code>CollectionIndexer.train_kmeans</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L308">centroids on the GPU</a>, and in <code>CollectionIndexer.index</code> when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L370">saving token embeddings</a>.</li>
<li>colbert/modeling/colbert.py in <code>ColBERT.doc</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L106">document token embeddings on the GPU</a></li>
</ul>
<p><mark>If this PR fix is relevant to the use of <code>half</code> in the above files I would expect there to be numeric differences in indexing/search artifacts</mark>.</p>
</section>
<section id="pr-86492" class="level3">
<h3 class="anchored" data-anchor-id="pr-86492">PR <a href="https://github.com/pytorch/pytorch/pull/86492">#86492</a></h3>
<blockquote class="blockquote">
<p>Fixes a memory leak by making autocast cache global instead of thread-local (#86492)</p>
</blockquote>
<p>This PR adds a PyTorch test which:</p>
<blockquote class="blockquote">
<p>Verifies that the autocast cache is global. This is done by mocking out cache clearing at the end of the forward pass, running forward+backward with an explicit call to autocast in the backward, and verifying that the weight only get cast to float16 once.</p>
</blockquote>
<p>Claude’s analysis:</p>
<blockquote class="blockquote">
<p>This PyTorch enhancement directly benefits ColBERT. By making the autocast cache global, this PR provides a performance improvement when training ColBERT with mixed precision. It reduces redundant computations during the backward pass, leading to faster and more efficient training without changing the model’s functionality.</p>
</blockquote>
<p>ColBERT uses <code>torch.cuda.amp.autocast</code> in the following files:</p>
<ul>
<li>colbert/utils/amp.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L15"><code>MixedPrecisionManager.context</code></a> which is used in colbert/training/training.py during <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L96"><code>train</code></a>, and in colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L87"><code>Checkpoint.query</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L93"><code>Checkpoint.doc</code></a> to calculate query and document token embeddings, respectively.</li>
<li>colbert/distillation/scorer in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/distillation/scorer.py#L48"><code>Scorer._score_pairs</code></a>.</li>
</ul>
</section>
<section id="pr-88898" class="level3">
<h3 class="anchored" data-anchor-id="pr-88898">PR <a href="https://github.com/pytorch/pytorch/pull/88898">#88898</a></h3>
<p>Fixes PyTorch <a href="https://github.com/pytorch/pytorch/issues/88873">#88873</a>:</p>
<blockquote class="blockquote">
<p>torch_extension.py should be fixed or ninja compile will fail.</p>
</blockquote>
<p>Gemini’s analysis: Because ColBERT uses the very feature this PR is fixing (torch.utils.cpp_extension.py), the change is directly relevant. This bug fix is important for any developer or user who needs to compile and run ColBERT on a Windows machine. It ensures that ColBERT’s performance-critical custom CUDA code can be built correctly, preventing potential compilation errors.</p>
<p>ColBERT uses <code>torch.utils.cpp_extension</code> in the following files:</p>
<ul>
<li>colbert/modeling/colbert.py in <code>ColBERT.try_load_torch_extensions</code> to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L12">load <code>segmented_lookup.cpp</code></a> on CPU.</li>
<li>colbert/search/index_storage.py in <code>IndexScorer.try_load_torch_extensions</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L38">filter_pids.cpp</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L51">decompress_residuals.cpp</a>.</li>
<li>colbert/search/strided_tensor.py in <code>StridedTensor.try_load_torch_extensions</code> to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L26">load <code>segmented_lookup.cpp</code></a> on CPU.</li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCode.try_load_torch_extensions</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L103">decompress_residuals.cpp</a></li>
</ul>
<p><mark>This might be related to ColBERT <a href="https://github.com/stanford-futuredata/ColBERT/issues/371">#317</a></mark></p>
</section>
<section id="pr-90149" class="level3">
<h3 class="anchored" data-anchor-id="pr-90149">PR <a href="https://github.com/pytorch/pytorch/pull/90149">#90149</a></h3>
<blockquote class="blockquote">
<p>Fix a static initialization order fiasco in c10d (#90149)</p>
</blockquote>
<p>Gemini’s analysis: Because ColBERT’s multi-GPU functionality is built directly on the PyTorch library that this PR is fixing, this change is highly relevant. This is a crucial stability improvement that makes ColBERT’s distributed training and inference more reliable by preventing potential crashes at startup.</p>
<p><mark>If Gemini’s analysis is correct, this will make ColBERT’s multi-GPU functionality more reliable and might address related open issues.</mark></p>
</section>
<section id="prs-86956-86958" class="level3">
<h3 class="anchored" data-anchor-id="prs-86956-86958">PRs <a href="https://github.com/pytorch/pytorch/pull/86956">#86956</a>, <a href="https://github.com/pytorch/pytorch/pull/86958">#86958</a></h3>
<blockquote class="blockquote">
<p>Fix issues with non-contiguous Tensor handling (#86956, #86958)</p>
</blockquote>
<p><mark>These are both MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS</mark></p>
</section>
<section id="prs-94119-86240-91520-94442-94386" class="level3">
<h3 class="anchored" data-anchor-id="prs-94119-86240-91520-94442-94386">PRs <a href="https://github.com/pytorch/pytorch/pull/94119">#94119</a>, <a href="https://github.com/pytorch/pytorch/pull/86240">#86240</a>, <a href="https://github.com/pytorch/pytorch/pull/91520">#91520</a>, <a href="https://github.com/pytorch/pytorch/pull/94442">#94442</a>, <a href="https://github.com/pytorch/pytorch/pull/94386">#94386</a></h3>
<blockquote class="blockquote">
<p>Fix issues with ops implementation torch.median (#90326, #88807), torch.{std,var} correction argument (#91203), torch.index_select (#94117, #91064), torch.cumsum (#94119), torch.where (#86240), torch.nn.Embedding (#82809), torch.nn.Softplus (#88555), torch.nn.functional.pad (#89864), torch.max (#91520), padding functions (#91522), torch.nn.functional.upsample (#91669), pooling functions (#91519, #94348), torch.nn.{NLLLoss,SmoothL1Loss} (#94226), torch.nn.SoftPlus (#94256), torch.masked_fill (#94263), torch.fill_ (#94479), torch.median (#94489), torch.nonzero (#94442), torch.nn.BatchNorm (#94351), torch.{min,max} (#94386), torch.nn.GELU (#94529), torch.nn.LSTM (#94889), #95137),torch.nn.Conv2d(#95078),torch.nn.functional.bilinear(#94892),torch.copy_ (#95272),torch.max_pool2d(#94963),torch.div (#95769)</p>
</blockquote>
<p>ColBERT uses topk in the following files:</p>
<ul>
<li><code>get_cells</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L17">colbert/search/candidate_generation.py</a></li>
<li><code>score_pids</code> in colbert/search/index_storage.py to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L127">filter centroids by the threshold</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L152">filter <code>pids</code> using pruned centroid scores</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L161">filter <code>pids</code> using full centroid scores</a></li>
<li>filter scores in <code>colbert_score_reduce</code> for the <code>"flipr"</code> interaction method: <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L146">link1</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L150">link 2</a></li>
</ul>
<p>ColBERT uses max/argmax in the following files:</p>
<ul>
<li><code>get_cells</code> if <code>ncells==1</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L15">colbert/search/candidate_generation.py</a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L121"><code>ColBERT.score</code></a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L135"><code>colbert_score_reduce</code></a></li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCodec.compress_into_codes</code> on <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">GPU</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">CPU</a></li>
<li>colbert/search/strided_tensor.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L74">`StridedTensor.lookup</a></li>
<li>colbert/search/strided_tensor_core.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L27"><code>StridedTensorCore.__init__</code></a></li>
<li>colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L215">`Checkpoint.score</a></li>
<li>colbert/indexing/utils.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L48"><code>optimize_ivf</code></a></li>
</ul>
<p>ColBERT uses <code>torch.cumsum</code> in the following files to calculate <code>offsets</code>.:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L50">colbert/indexing/utils.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L31">colbert/search/strided_tensor_core.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L48">colbert/search/strided_tensor.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L68">colbert/search/index_storage.py</a></li>
</ul>
<p>ColBERT uses <code>torch.where</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L49">colbert/modeling/checkpoint.py</a> to pool embeddings within each cluster.</li>
</ul>
<p>ColBERT uses <code>torch.nonzero</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L351">colbert/index_updater.py</a> to construct mask of where pids to be removed appear in ivf.</li>
</ul>
<p><mark>These are all MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91120-94464" class="level3">
<h3 class="anchored" data-anchor-id="prs-91120-94464">PRs <a href="https://github.com/pytorch/pytorch/pull/91120">#91120</a>, <a href="https://github.com/pytorch/pytorch/pull/94464">#94464</a></h3>
<blockquote class="blockquote">
<p>Fix issues with torch.bool for Unary ops (#91120), scatter ops (#94464)</p>
</blockquote>
<p>Claude’s analysis: Claude: The PR fixes compatibility issues where boolean tensors needed to be cast to int8 on older macOS versions, then cast back. This would be important for ColBERT’s masking operations which rely heavily on boolean tensors for attention and padding masks.</p>
<p><mark>These are MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-94484" class="level3">
<h3 class="anchored" data-anchor-id="pr-94484">PR <a href="https://github.com/pytorch/pytorch/pull/94484">#94484</a></h3>
<blockquote class="blockquote">
<p>Properly cast torch.int64 to torch.int32 for reduction ops and raise warning. (#94484)</p>
</blockquote>
<p>Claude’s analysiss: The PR changes TORCH_CHECK (which throws an error) to TORCH_WARN_ONCE (which just warns) and automatically casts int64 to int32 for min/max operations. This would allow ColBERT to run on MPS with int64 tensors instead of failing, though with potential precision loss.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91120-94464-1" class="level3">
<h3 class="anchored" data-anchor-id="prs-91120-94464-1">PRs <a href="https://github.com/pytorch/pytorch/pull/91197">#91120</a>, <a href="https://github.com/pytorch/pytorch/pull/91514">#94464</a></h3>
<blockquote class="blockquote">
<p>Fix handling of ops taking multiple dtypes as input (#91197, #91514)</p>
</blockquote>
<p>Claude’s analysis: The PR fixes MPS scatter to handle type mismatches between source and destination tensors automatically.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91786-94662" class="level3">
<h3 class="anchored" data-anchor-id="prs-91786-94662">PRs <a href="https://github.com/pytorch/pytorch/pull/91786">#91786</a>, <a href="https://github.com/pytorch/pytorch/pull/94662">#94662</a></h3>
<blockquote class="blockquote">
<p>Fix handling of channels last for torch.cat (#91786, #94662), torch.Conv2d (#91822, #94384), torch.nn.{ELU,ReLU,Hardswish} (#94664), torch.nn.BatchNorm (#94760), torch.nn.MaxPool2d (#94877)</p>
</blockquote>
<p>ColBERT uses <code>.cat</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L45">colbert/indexing/utils.py</a> to concatenate a list of tensors (<code>unique_pids_per_centroid</code>) into a single tensor (<code>ivf</code>).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/index_manager.py#L23">colbert/indexing/index_manager.py</a> to concatenate multiple path names.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L31"></a> to calculate <code>offsets</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L39"></a> to add padding to a tensor.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/distillation/scorer.py#L60"></a> to concatenate <code>scores</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_encoder.py#L38"></a> to concatenate document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L181"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L182"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L220"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L237"></a> to concatenate <code>centroids</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L276"></a> to concatenate document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L51"></a> to concatenate <code>packed_tensor</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L152"></a> to concatenate <code>all_orders</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L155"></a> to concatenate <code>all_lengths</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L101"></a> to concatenate <code>compressed_embs.codes</code> (centroid IDs corresponding to document token embeddings).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L108"></a> to concatenate <code>compressed_embs.residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L117"></a> to concatenate <code>doclens</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L431"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L434"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L506"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L507"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L115"></a> to concatenate <code>batches</code> (of queries).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L168"></a> to concatenate document token embeddings (in order).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L169"></a> to concatenate <code>mask</code> for document token embeddings (in order).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L48"></a> to concatenate <code>code</code> chunks.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L69"></a> to concatenate <code>offsets</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L149"></a> to concatenate <code>approx_scores</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/query_tokenization.py#L88"></a> to concatenate <code>ids</code> (for query tokens).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/query_tokenization.py#L89"></a> to concatenate <code>masks</code> (for query tokens).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/utils.py#L72"></a> to concatenate prefix token.</li>
</ul>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-94259-94278-95145-95762-95905" class="level3">
<h3 class="anchored" data-anchor-id="prs-94259-94278-95145-95762-95905">PRs <a href="https://github.com/pytorch/pytorch/pull/94259">#94259</a>, <a href="https://github.com/pytorch/pytorch/pull/94278">#94278</a>, <a href="https://github.com/pytorch/pytorch/pull/95145">#95145</a>, <a href="https://github.com/pytorch/pytorch/pull/95762">#95762</a>, <a href="https://github.com/pytorch/pytorch/pull/95905">#95905</a></h3>
<blockquote class="blockquote">
<p>Fix view operations handling (#94259, #94278,#95145, #95762, #95905)</p>
</blockquote>
<p>Claude’s analysis: This PR fixes crashes in view operations when slicing with incorrect lengths, which ColBERT uses for tensor reshaping and indexing operations.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-87853" class="level3">
<h3 class="anchored" data-anchor-id="pr-87853">PR <a href="87853">#87853</a></h3>
<blockquote class="blockquote">
<p>Move incorrectly placed closing curly brace of extern “C” block (#87853)</p>
</blockquote>
<p>Gemini’s analysis: This pull request is a foundational C++ correctness fix for the PyTorch framework. Because ColBERT compiles its own C++ extensions that depend on these core headers, this change is directly beneficial. It ensures the stability and reliability of ColBERT’s own build process, preventing potential compilation failures.</p>
<p>ColBERT’s C++ extensions:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/modeling/segmented_maxsim.cpp">segmented_maxsim.cpp</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/search/filter_pids.cpp">filter_pids.cpp</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/search/decompress_residuals.cpp">decompress_residuals.cpp</a></li>
</ul>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-93322" class="level3">
<h3 class="anchored" data-anchor-id="pr-93322">PR <a href="https://github.com/pytorch/pytorch/pull/93322">#93322</a></h3>
<blockquote class="blockquote">
<p>Fix MSVC compiler error in basic_ops.h (#93322)</p>
</blockquote>
<p>Gemini’s take: This pull request is a crucial build-system and correctness fix. It directly impacts ColBERT by ensuring that its custom C++ code can be compiled successfully on Windows machines that use the affected MSVC compiler. Without this fix, users in that environment would be unable to run ColBERT. This change makes ColBERT’s build process more robust and widens its platform compatibility.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
<section id="pr-89310" class="level3">
<h3 class="anchored" data-anchor-id="pr-89310">PR <a href="https://github.com/pytorch/pytorch/pull/89310">#89310</a></h3>
<blockquote class="blockquote">
<p>Fix a bug that redefines __STDC_FORMAT_MACROS (#89310)</p>
</blockquote>
<p>Gemini’s take: This pull request provides a stability and correctness fix to the underlying PyTorch framework. Because ColBERT compiles its own C++ code that depends on these core PyTorch headers, this change is directly beneficial. It makes ColBERT’s own compilation process more reliable and prevents a potential class of build failures.</p>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-90411" class="level3">
<h3 class="anchored" data-anchor-id="pr-90411">PR <a href="https://github.com/pytorch/pytorch/pull/90411">#90411</a></h3>
<blockquote class="blockquote">
<p>Add manual cuda deps search logic (#90411)</p>
</blockquote>
<p>Gemini’s take: This PyTorch pull request adds a new mechanism to help PyTorch find its essential CUDA libraries (cuBLAS and cuDNN) on Linux systems.</p>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-89759" class="level3">
<h3 class="anchored" data-anchor-id="pr-89759">PR <a href="https://github.com/pytorch/pytorch/pull/89759">#89759</a></h3>
<blockquote class="blockquote">
<p>Workaround for NumPy builds that ship with a broken Dlpack deleter (#89759)</p>
</blockquote>
<p><mark>TBD if this improves reliability as ColBERT uses NumPy</mark>/.</p>
</section>
<section id="pr-86288" class="level3">
<h3 class="anchored" data-anchor-id="pr-86288">PR <a href="https://github.com/pytorch/pytorch/pull/86288">#86288</a></h3>
<blockquote class="blockquote">
<p>Workaround MSVC ICE due to constexpr char* template argument (#86288)</p>
</blockquote>
<p>Gemini’s take: It directly impacts ColBERT by ensuring that its custom C++ code can be compiled successfully on Windows machines that use an affected MSVC compiler.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
<section id="pr-85408" class="level3">
<h3 class="anchored" data-anchor-id="pr-85408">PR <a href="https://github.com/pytorch/pytorch/pull/85408">#85408</a></h3>
<blockquote class="blockquote">
<p>Add define to fix issue with compatibility with latest Windows SDK (#85408)</p>
</blockquote>
<p>Gemini’s take: It directly impacts ColBERT by ensuring that the underlying PyTorch framework can be successfully built on modern Windows environments.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
</section>
<section id="improvements" class="level2">
<h2 class="anchored" data-anchor-id="improvements">Improvements</h2>
<p>These PyTorch PRs are related to improvements, which could affect ColBERT by speeding things up (and therefore seeing a speed up in indexing/search/training time) or changing baseline indexing/search/training artifacts if improvements impact numeric precision.</p>
<section id="pr-56398" class="level3">
<h3 class="anchored" data-anchor-id="pr-56398">PR <a href="https://github.com/pytorch/pytorch/pull/56398">#56398</a></h3>
<blockquote class="blockquote">
<p>Set std/var correction overloads default value to None (#56398)</p>
</blockquote>
<p><mark>Unclear if and how this affects ColBERT but highlighting it since it changes code in PyTorch’s aten/src/ATen/native.</mark></p>
</section>
<section id="pr-86309" class="level3">
<h3 class="anchored" data-anchor-id="pr-86309">PR <a href="https://github.com/pytorch/pytorch/pull/86309">#86309</a></h3>
<blockquote class="blockquote">
<p>Add support for int32 indices in index/index_put ops (#86309)</p>
</blockquote>
<p><mark>I think this PR is related to <a href="https://github.com/stanford-futuredata/ColBERT/pull/180">this ColBERT PR</a> which I think is related to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L163">this line of code in <code>IndexScorer</code></a></mark>.</p>
</section>
<section id="pr-87022" class="level3">
<h3 class="anchored" data-anchor-id="pr-87022">PR <a href="https://github.com/pytorch/pytorch/pull/87022">#87022</a></h3>
<blockquote class="blockquote">
<p>Enable where to have cpu scalar args (#87022)</p>
</blockquote>
<p>ColBERT uses <code>torch.where</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L49">colbert/modeling/checkpoint.py</a> to pool embeddings within each cluster.</li>
</ul>
<p><mark>Unclear if this will affect ColBERT but there are currently no open issues related to <code>torch.where</code></mark>.</p>
</section>
<section id="pr-90914" class="level3">
<h3 class="anchored" data-anchor-id="pr-90914">PR <a href="https://github.com/pytorch/pytorch/pull/90914">#90914</a></h3>
<blockquote class="blockquote">
<p>Add support for NumPy scalars to torch.tensor.asarray (#90914)</p>
</blockquote>
<p><mark>Found <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L198">1 use of <code>asarray</code></a> but it doesn’t deal with a scalar so probably won’t be affected</mark>.</p>
</section>
<section id="pr-85926" class="level3">
<h3 class="anchored" data-anchor-id="pr-85926">PR <a href="https://github.com/pytorch/pytorch/pull/85926">#85926</a></h3>
<blockquote class="blockquote">
<p>Enable out variant of torch.max(#85926)</p>
</blockquote>
<p><mark>Unclear what this PR does but highlighting it since ColBERT uses <code>torch.max</code></mark>.</p>
</section>
<section id="pr-91846" class="level3">
<h3 class="anchored" data-anchor-id="pr-91846">PR <a href="https://github.com/pytorch/pytorch/pull/91846">#91846</a></h3>
<blockquote class="blockquote">
<p>Implement faster gradient clipping using foreach function (#91846)</p>
</blockquote>
<p>ColBERT uses <code>torch.nn.utils.clip_grad_norm_</code> in two lines:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">colbert/utils/amp.py#L26</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L31">colbert/utils/amp.py#L31</a></li>
</ul>
<p><mark>IIUC this won’t affect ColBERT since it doesn’t set <code>foreach</code> in <code>torch.nn.utils.clip_grad_norm_</code>.</mark></p>
</section>
<section id="pr-92334" class="level3">
<h3 class="anchored" data-anchor-id="pr-92334">PR <a href="https://github.com/pytorch/pytorch/pull/92334">#92334</a></h3>
<blockquote class="blockquote">
<p>Enable DDP to handle custom dataclass forward outputs (#92334)</p>
</blockquote>
<p><mark>ColBERT does use DistributedDataParallel (<a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L56">in <code>train</code></a>) butit’s not being passed a custom dataclass, it’s being passed a <code>colbert</code> model so I don’t think this PR applies.</mark></p>
</section>
<section id="pr-89137" class="level3">
<h3 class="anchored" data-anchor-id="pr-89137">PR <a href="https://github.com/pytorch/pytorch/pull/89137">#89137</a></h3>
<blockquote class="blockquote">
<p>Skip collective communications for NO_SHARD in clip_grad_norm_ (#89137)</p>
</blockquote>
<p><mark>ColBERT doesn’t use FullyShardedDataParallel but it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">does use <code>torch.nn.utils.clip_grad_norm</code></a> so not sure if this PyTorch PR affects it</mark>.</p>
</section>
<section id="pr-90028" class="level3">
<h3 class="anchored" data-anchor-id="pr-90028">PR <a href="https://github.com/pytorch/pytorch/pull/90028">#90028</a></h3>
<blockquote class="blockquote">
<p>Apply the “largest” dtype across all parameters/gradients as defined by PyTorch’s type promotion semantics for the total norm returned in clip_grad_norm_ for low prec grads (#90028)</p>
</blockquote>
<p><mark>ColBERT doesn’t use FullyShardedDataParallel but it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">does use <code>torch.nn.utils.clip_grad_norm</code></a> so not sure if this PyTorch PR affects it</mark>.</p>
</section>
<section id="pr-85692" class="level3">
<h3 class="anchored" data-anchor-id="pr-85692">PR <a href="https://github.com/pytorch/pytorch/pull/85692">#85692</a></h3>
<blockquote class="blockquote">
<p>Set CUDA_MODULE_LOADING to LAZY when not set by the user (#85692)</p>
</blockquote>
<p><mark>Unclear exactly what this does but it relates to the CUDA_MODULE_LOADING env var which is not set in ColBERT</mark></p>
</section>
<section id="pr-89172" class="level3">
<h3 class="anchored" data-anchor-id="pr-89172">PR <a href="https://github.com/pytorch/pytorch/pull/89172">#89172</a></h3>
<blockquote class="blockquote">
<p>Add an option to disable reduced precision reductions for BF16 GEMM (#89172)</p>
</blockquote>
<p><mark>Unclear exactly what this does, but in the PR they mentioned it improves H100 usage, so I’ll keep that in mind.</mark></p>
</section>
<section id="pr-91436" class="level3">
<h3 class="anchored" data-anchor-id="pr-91436">PR <a href="https://github.com/pytorch/pytorch/pull/91436">#91436</a></h3>
<blockquote class="blockquote">
<p>Add an env variable to disable addmm_cuda_lt kernel (#91436)</p>
</blockquote>
<p><mark>Unclear what this does, but it’s adding a variable, so it’s a new feature.</mark></p>
</section>
<section id="pr-86041-93022" class="level3">
<h3 class="anchored" data-anchor-id="pr-86041-93022">PR <a href="https://github.com/pytorch/pytorch/pull/86041">#86041</a>, <a href="https://github.com/pytorch/pytorch/pull/93022">#93022</a></h3>
<blockquote class="blockquote">
<p>Clean up flatbuffer lib dependency and fixed its test to match pkl models (#86041, #93022)</p>
</blockquote>
<p><mark>I am not sure what these PRs are doing. The title refers “pkl models” which ColBERT doesn’t use to my knowledge.</mark></p>
</section>
<section id="pr-93898" class="level3">
<h3 class="anchored" data-anchor-id="pr-93898">PR <a href="https://github.com/pytorch/pytorch/pull/93898">#93898</a></h3>
<blockquote class="blockquote">
<p>Type corrections to avoid unnecessary static_casts (#93898)</p>
</blockquote>
<p><mark>Unclear what this PR does but it touches a lot of what seem to be core files so I’m flagging it</mark>.</p>
</section>
<section id="pr-87245" class="level3">
<h3 class="anchored" data-anchor-id="pr-87245">PR <a href="https://github.com/pytorch/pytorch/pull/87245">#87245</a></h3>
<blockquote class="blockquote">
<p>Integrate all ONNX operators with a new JitScalarType API (#87245)</p>
</blockquote>
<p><mark>It’s onnx related, which ColBERT doesn’t use, but it also says: “this PR addresses not only the issue above, but the entire family of issues related to torch._C.Value.type() parsing when scalarType() or dtype() is not available.”</mark></p>
</section>
<section id="pr-87343" class="level3">
<h3 class="anchored" data-anchor-id="pr-87343">PR <a href="https://github.com/pytorch/pytorch/pull/87343">#87343</a></h3>
<blockquote class="blockquote">
<p>Add share_from_this to torch::jit::Graph (#87343)</p>
</blockquote>
<p><mark>Is ONNX related, but unclear if it affects anything else?</mark></p>
</section>
<section id="pr-84789" class="level3">
<h3 class="anchored" data-anchor-id="pr-84789">PR <a href="https://github.com/pytorch/pytorch/pull/84789">#84789</a></h3>
<blockquote class="blockquote">
<p>Use optional op to keep None in results for ONNX internal tests (#84789)</p>
</blockquote>
<p><mark>Is ONNX related, but unclear if it affects anything else?</mark></p>
</section>
<section id="pr-86218" class="level3">
<h3 class="anchored" data-anchor-id="pr-86218">PR <a href="https://github.com/pytorch/pytorch/pull/86218">#86218</a></h3>
<blockquote class="blockquote">
<p>Add fp16 support for torch.nn.Linear (#89774), torch.nn.GELU (#86218)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91884" class="level3">
<h3 class="anchored" data-anchor-id="pr-91884">PR <a href="https://github.com/pytorch/pytorch/pull/91884">#91884</a></h3>
<blockquote class="blockquote">
<p>Add support for empty Tensors in torch.bitwise_not (#87286), torch.nn.LayerNorm (#94212), many backward functions (#94343), torch.nn.functional.hardswish (#94342), torch.topk (#91884), torch.arange (#94485), torch.linal.inv (#94551),</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91734" class="level3">
<h3 class="anchored" data-anchor-id="pr-91734">PR <a href="https://github.com/pytorch/pytorch/pull/91734">#91734</a></h3>
<blockquote class="blockquote">
<p>Add support for reduction ops on multiple axis at a time (#91734)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-94639" class="level3">
<h3 class="anchored" data-anchor-id="pr-94639">PR <a href="https://github.com/pytorch/pytorch/pull/94639">#94639</a></h3>
<blockquote class="blockquote">
<p>Add support for k greater than 16 for torch.topk (#94639)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91576" class="level3">
<h3 class="anchored" data-anchor-id="pr-91576">PR <a href="https://github.com/pytorch/pytorch/pull/91576">#91576</a></h3>
<blockquote class="blockquote">
<p>Simplify OpenMP detection in CMake (#91576)</p>
</blockquote>
<p>Claude’s take: While ColBERT should continue working, there could be subtle performance or compilation differences depending on how PyTorch’s simplified OpenMP detection affects the runtime compilation of ColBERT’s C++ extensions, particularly in multi-threaded scenarios.</p>
<p><mark>Unclear what this PR is doing but flagging it as it might improve performance as Claude states.</mark></p>
</section>
</section>
<section id="deprecations" class="level2">
<h2 class="anchored" data-anchor-id="deprecations">Deprecations</h2>
<p>These are PRs I would think would have a significant impact if applicable.</p>
<section id="pr-92143" class="level3">
<h3 class="anchored" data-anchor-id="pr-92143">PR <a href="https://github.com/pytorch/pytorch/pull/92143">#92143</a></h3>
<blockquote class="blockquote">
<p>Deprecate tensor.mT,tensor.T,tensor.mH,tensor.H on 0D-tensors (#92143)</p>
</blockquote>
<p><mark>There are five instances where .T is used, but pretty sure none of these are 0-D tensors, will confirm</mark>:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L13">colbert/search/candidate_generation.py#L13</a>: cosine similarity between centroids and query token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L43">colbert/search/candidate_generation.py#L43</a>: used in <code>generate_candidate_scores</code> which uses <code>lookup_eids</code> which I can’t find anywhere else in the codebase.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L195">colbert/modeling/colbert.py#L195</a>: Cosine similarity between query and document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">colbert/indexing/codecs/residual.py#L215</a>: Cosine similarity between centroids and document token embeddings (GPU).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">colbert/indexing/codecs/residual.py#L217</a>: Cosine similarity between centroids and document token embeddings (CPU).</li>
</ul>
</section>
</section>
<section id="performance" class="level2">
<h2 class="anchored" data-anchor-id="performance">Performance</h2>
<p>Similar to improvements, I would only expect this set of PRs to improve ColBERT performance, keeping an eye on how different artifacts changed because of that.</p>
<section id="pr-93234" class="level3">
<h3 class="anchored" data-anchor-id="pr-93234">PR <a href="https://github.com/pytorch/pytorch/pull/93234">#93234</a></h3>
<blockquote class="blockquote">
<p>Improve performance for functional.multi_head_attention_forward() (#93234, #89847)</p>
</blockquote>
<p><mark>ColBERT uses BERT, which has its own attention implementation, so this likely wouldn’t impact it unless the BERT model specifically uses <code>torch.nn.functional.multi_head_attention_forward</code> or <code>torch.nn.MultiheadAttention</code>.</mark></p>
</section>
<section id="pr-84981" class="level3">
<h3 class="anchored" data-anchor-id="pr-84981">PR <a href="https://github.com/pytorch/pytorch/pull/84981">#84981</a></h3>
<blockquote class="blockquote">
<p>Use atomicAdd for bfloat16 in Ampere and above (#84981)</p>
</blockquote>
<p>Gemini’s take: This pull request directly accelerates a fundamental operation used during the training of ColBERT. By replacing a slow, emulated function with a fast, hardware-native instruction, this change leads to a noticeable increase in training speed for anyone training ColBERT with bfloat16 mixed precision on an Ampere or newer GPU.</p>
<p><mark>If Gemini is correct, then I will see a speedup in training.</mark></p>
</section>
<section id="pr-94034" class="level3">
<h3 class="anchored" data-anchor-id="pr-94034">PR <a href="https://github.com/pytorch/pytorch/pull/94034">#94034</a></h3>
<blockquote class="blockquote">
<p>Add various performance fixes to c++ STL usage (#94034)</p>
</blockquote>
<p>Gemini’s take: The changes in this PR touch several core PyTorch components that are critical to ColBERT’s operation:</p>
<ul>
<li>Autograd Engine (function.h): Every gradient calculation during training will benefit from these optimizations.</li>
<li>CUDA Communication (comm.cpp): The code that handles broadcasting and gathering tensors across GPUs for multi-GPU training and inference is made more efficient.</li>
<li>Mixed Precision (autocast_mode.h): The logic for automatic mixed precision, which is key for training ColBERT efficiently, is also slightly optimized.</li>
</ul>
<p><mark>If Gemini is correct, then I will see a speed up in all aspects of ColBERT. </mark></p>
</section>
<section id="pr-86568" class="level3">
<h3 class="anchored" data-anchor-id="pr-86568">PR <a href="https://github.com/pytorch/pytorch/pull/86568">#86568</a></h3>
<blockquote class="blockquote">
<p>Add fmsub to vectorization primitives (#86568)</p>
</blockquote>
<p>Gemini’s take: This pull request is a CPU-specific performance optimization. It adds support for the fmsub (fused multiply-subtract) instruction to PyTorch’s CPU vectorization library. This allows PyTorch to perform the operation (a * b) - c in a single, faster instruction on modern CPUs that support it (e.g., via AVX or NEON).</p>
<p><mark>I’m pretty sure ColBERT doesn’t use multiply-subtract, but keeping it in here just in case it comes up. </mark></p>
</section>
<section id="pr-92300" class="level3">
<h3 class="anchored" data-anchor-id="pr-92300">PR <a href="https://github.com/pytorch/pytorch/pull/92300">#92300</a></h3>
<blockquote class="blockquote">
<p>Fix biasadd OMP perf issue for the packed MKL SGEMM (#92300)</p>
</blockquote>
<p>Gemini’s take: This pull request is a CPU-specific performance optimization. It fixes a parallelization issue within the Intel MKL (Math Kernel Library) backend for linear layers. This change improves the efficiency of adding a bias term to the output of a matrix multiplication when running on a CPU.</p>
<p><mark>If Gemini is correct, I would expect a speedup on CPU. </mark></p>
</section>
<section id="pr-91114" class="level3">
<h3 class="anchored" data-anchor-id="pr-91114">PR <a href="https://github.com/pytorch/pytorch/pull/91114">#91114</a></h3>
<blockquote class="blockquote">
<p>Increase performance of torch.add{cmul,cdiv,mm}(#94214, #94534)torch.multinomial (#86342), faster op launch time (#86437), torch.linear (#91114), view handling (#91743, #94218), convolutions(#94661), scatter/gather (#94663)</p>
</blockquote>
<p>Gemini’s take: While the Adam optimizer used by ColBERT does use the addcdiv operation, this is executed on the GPU via CUDA, not MPS. This pull request is a performance optimization for the torch.nn.Linear layer, but it is exclusively for the MPS (Metal Performance Shaders) backend.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>Based on my analysis, I’m optimistic about upgrading ColBERT from torch==1.13.1 to 2.0. The upgrade should deliver concrete benefits with reasonable testing overhead. Performance-wise, I’ll be watching for training time improvements from the faster <code>foreach</code> optimizer implementations and expect speedups across all aspects of ColBERT from C++ optimizations and CUDA improvements. For validation, I’ll need to check for numeric differences in indexing/search artifacts from half-precision bug fixes and benchmark retrieval quality metrics after reindexing to avoid regressions. The reliability improvements should make ColBERT’s multi-GPU functionality more reliable and might address related open issues. Plus there are fixes for operations like <code>topk</code> and <code>torch.load</code> that ColBERT uses extensively. Most MPS-related changes will only affect ColBERT if we choose future compatibility, so they’re not immediate concerns but good to have.</p>
<p>My next step will be to establish training time benchmarks and indexing/retrieval/training baseline artifacts so that I can concretely monitor even subtle performance/behavior changes when using <code>torch==2.0</code> in my development branch.</p>


</section>

 ]]></description>
  <category>ColBERT</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-27-torch-colbert/</guid>
  <pubDate>Sun, 27 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>TIL: Custom Composer Callback to Push Checkpoints to HuggingFace Hub During Training.</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-26-push-to-hub/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this short TIL blog post, I’m going to share the code I wrote with Claude’s help for a custom Composer callback which pushes the model to Hugging Face Hub every specified number of steps. The purpose of doing so is so that you can run evaluation after training so it doesn’t slow down training.</p>
</section>
<section id="custom-composer-callback" class="level2">
<h2 class="anchored" data-anchor-id="custom-composer-callback">Custom Composer Callback</h2>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> HFPushCallback(Callback):</span>
<span id="cb1-2">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, repo_id: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, push_every_n_steps: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb1-3">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.repo_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> repo_id</span>
<span id="cb1-4">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.push_every_n_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> push_every_n_steps</span>
<span id="cb1-5">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> os.getenv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HF_TOKEN"</span>)</span>
<span id="cb1-6">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hf_api <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> HfApi(token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.token)</span>
<span id="cb1-7">  </span>
<span id="cb1-8">      create_repo(</span>
<span id="cb1-9">          repo_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.repo_id,</span>
<span id="cb1-10">          token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.token,</span>
<span id="cb1-11">          private<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-12">          exist_ok<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb1-13">      )</span>
<span id="cb1-14">  </span>
<span id="cb1-15">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> batch_end(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, state: State, logger: Logger) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:            </span>
<span id="cb1-16">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> state.timestamp.batch.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.push_every_n_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb1-17">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._push_model(state)</span>
<span id="cb1-18">  </span>
<span id="cb1-19">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _push_model(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, state: State):</span>
<span id="cb1-20">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> tempfile.TemporaryDirectory() <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> temp_dir:</span>
<span id="cb1-21">          state.model.model.save_pretrained(temp_dir)</span>
<span id="cb1-22">          </span>
<span id="cb1-23">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hf_api.upload_folder(</span>
<span id="cb1-24">              folder_path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>temp_dir,</span>
<span id="cb1-25">              repo_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.repo_id,</span>
<span id="cb1-26">              commit_message<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>state<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>timestamp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>batch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>value<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb1-27">          )</span></code></pre></div>
<p>Important to note, <code>state.model</code> is the <code>ComposerHFCausalLM</code> wrapper around the HuggingFace model, so you have to access <code>state.model.model</code> to use the attribute <code>save_pretrained</code>.</p>
</section>
<section id="running-inference" class="level2">
<h2 class="anchored" data-anchor-id="running-inference">Running Inference</h2>
<p>You can use the following code to run inference on the model, just as you would any set of PEFT adapters from Hugging Face Hub.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> peft <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PeftModel</span>
<span id="cb2-3"></span>
<span id="cb2-4"></span>
<span id="cb2-5">model_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HuggingFaceTB/SmolLM2-135M"</span></span>
<span id="cb2-6">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(model_id)</span>
<span id="cb2-7">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(model_id)</span>
<span id="cb2-8"></span>
<span id="cb2-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PeftModel.from_pretrained(</span>
<span id="cb2-10">    model,</span>
<span id="cb2-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;repo_id&gt;"</span>,</span>
<span id="cb2-12">    revision <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;revision&gt;"</span></span>
<span id="cb2-13">)</span>
<span id="cb2-14"></span>
<span id="cb2-15">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The best thing about artificial intelligence is "</span></span>
<span id="cb2-16">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(prompt, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb2-17">attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inputs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"attention_mask"</span>]</span>
<span id="cb2-18"></span>
<span id="cb2-19">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.generate(</span>
<span id="cb2-20">  inputs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>],</span>
<span id="cb2-21">  attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask,</span>
<span id="cb2-22">  pad_token_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tokenizer.eos_token_id</span>
<span id="cb2-23">)</span>
<span id="cb2-24"></span>
<span id="cb2-25"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tokenizer.decode(outputs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], skip_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))      </span></code></pre></div>
<p>The <code>revision</code> parameter is the commit ID in your Hugging Face repo. In this way, if you, say, push your model every 100 steps, then you can use the <code>revision</code> argument for each of those checkpoints and run your evaluations. Then you can log those evaluations to your W&amp;B project so that your evaluation log is comparable with other training logs.</p>


</section>

 ]]></description>
  <category>LLM</category>
  <category>Custom Composer Callback</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-26-push-to-hub/</guid>
  <pubDate>Sat, 26 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Takeaways from Lesson 1 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-22-evals-course/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post, I’m going highlight ideas that stood out to me from the first lesson and first three chapters of the course reader from the AI evals course by Hamel Husain and Shreya Shankar.</p>
</section>
<section id="ideas-that-stood-out-from-the-first-three-chapters-of-the-course-reader" class="level2">
<h2 class="anchored" data-anchor-id="ideas-that-stood-out-from-the-first-three-chapters-of-the-course-reader">Ideas that Stood Out from the First Three Chapters of the Course Reader</h2>
<p><strong>Idea 1: LLMs used for intermediate tasks still need evals.</strong> I think this is an important reminder because I often think of LLM tasks as complex or mission-critical or user-facing. However, there are smaller subtasks that happen “behind the scenes”, assisting the rest of the pipeline. These tasks can be overlooked. For example you could have an LLM-assisted task of curating few-shot examples that are used for a larger prompt downstream. This task needs evals.</p>
<p><strong>Idea 2: looking at data, whether it’s curating few-shot examples manually or reviewing LLM traces to classify failures, deepens our understanding of what the user wants and how the LLM fails to deliver.</strong> What makes a good example? We have to think about objectives, content, format, tone, instruction, and output. What type of failure are we witnessing in a trace? We have to think about those dimensions, and identify specifically what the LLM failed to do and categorize it precisely.</p>
<p><strong>Idea 3: We allow ourselves to adjust annotations or revise failure mode definitions as needed as we organize our thoughts and observations on LLM traces.</strong> It’s normal for annotation schemas to evolve after reviewing more data. I fully agree and relate to this idea. <a href="https://youtu.be/FXOXoaGjntc">After evaluating 1350 LLM judge scores (for LM-generated tiny stories)</a> I improved my understanding of what makes a high quality story (in terms of creativity, grammar, context-tracking, plot, factual knowledge, and reasoning) and modified the criteria I used to evaluate those dimensions.</p>
<p><strong>Idea 4: Forcing binary decisions about whether a failure mode occurs or not produces more reproducible annotations than the Likert scale.</strong> As someone who has advised, designed and administered a dozen or more surveys, I plead the case for binary decisions and groan at the sight of a Likert scale. What’s the difference between “Good” and “Very Good”? What about “Mostly Satisfied” and “Somewhat Satisfied”? If you’re asking someone to evaluate even 10s of LLM traces using a Likert scale, the cognitive load quickly fatigues the evaluator. It’s much simpler to answer “Did the LLM fail to use an appropriate tone?” (Yes/No) than “How well did the LLM use an appropriate tone?” (Exceptionally Well, Very Well, Somewhat Well, Not Well at All).</p>
<p>Note: Allen Downey has an excellent blog post, <a href="https://www.allendowney.com/blog/2024/05/03/the-mean-of-a-likert-scale/"><em>The Mean of a Likert Scale</em></a> which tackles the challenges of summarizing Likert scale data.</p>
</section>
<section id="ideas-from-lesson-1" class="level2">
<h2 class="anchored" data-anchor-id="ideas-from-lesson-1">Ideas from Lesson 1</h2>
<p><strong>Idea A: LLM Evaluation is the systematic measurement of LLM pipeline quality.</strong> This is the first (and best) concise definition of evals I’ve come across. We want to create a system (that’s reproducible and reliable) to measure the quality of an LLM pipeline. What to measure in the pipeline and how to define quality is what this course will teach us.</p>
<p><strong>Idea B: Pay attention to your prompt.</strong> Many folks don’t read the prompt they’re sending to the LLM! Think about the prompt first before writing an AI-assisted prompt. Specifying your problem involves writing your prompts with care. Use an LLM where it makes sense to be used (e.g.&nbsp;to improve the clarity of your already specific prompt).</p>
<p><strong>Idea C: You have to wear all these hats.</strong> To cross the gulf of comprehension (i.e.&nbsp;what is my data and what stories does it tell?) you must wear your “data scientist” hat. For specification, your “product” hat, and for generalization your “engineer” hat. Shreya and Hamel encourage us to move slowly as we navigate these gulfs. As Kawhi Leonard says: <a href="https://x.com/patbev21/status/1884687382412132558">slow is pro</a>.</p>
<p><strong>Idea D: Collecting representative samples and analyzing failure modes is the most important step in AI evals.</strong> This is where you learn the most, is what most people skip, what has the least guidance out there in the industry. Shreya spends 75-80% of her time on error analysis. Asking the question “what would make a user unhappy?” uncovers your failure modes.</p>
<p><strong>Idea E: The process of writing a good prompt sets the foundation for your evaluation success.</strong> If your prompt fails to specify, in no uncertain terms, what the LLM should and shouldn’t do, how can you expect to evaluate the LLM’s output? “The LLM didn’t structure its response as JSON”—did you ask it to? “The LLM didn’t provide measurements for the recipe”—did you ask it to?</p>
<p><strong>Idea F: The process of removing errors from the LLM pipeline is a process of improving your communication to the LLM,</strong> and this requires empathy! You have to put yourself in the user’s shoes and ask: what do I want from this interaction? What will make me happy? What will make me frustrated? And then you have to relay those user needs to the LLM via the prompt. And finally you have to evaluate whether the desired experience was delivered by the LLM. This dance between playing data scientist/product manager/user/engineer/communicator makes applied AI such a fascinating field.</p>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>I’m swimming in tasks right now (including a new puppy!) so it’s important for me to move slowly and deliberately. Taking this course at this juncture in my life is itself a process of evaluation: what do I focus my time on? How deep do I go into which assignments? What questions should I ask during office hours? The excellent structure and resources that Hamel and Shreya provided us will help ease my cognitive load, allowing me to focus on high value tasks. I don’t know if I’ll be able to write a blog post or make a video for each lesson, but I’ll try to publish at least once a week (on average). Our HW is to write a system prompt for a recipe chatbot, so expect some of my musings on that soon!</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-22-evals-course/</guid>
  <pubDate>Tue, 22 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Revisiting ColBERTv1 : A Return to First Principles</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/</link>
  <description><![CDATA[ 




<div id="cell-1" class="cell">
<details class="code-fold">
<summary>pip installs</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install transformers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.49.0</span></span>
<span id="cb1-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install ragatouille</span></code></pre></div>
</details>
</div>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>imports</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.indexing.collection_encoder <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CollectionEncoder</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.infra <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ColBERTConfig</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.modeling.checkpoint <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Checkpoint</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.modeling.tokenization.utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> _split_into_batches, _sort_by_length, _insert_prefix_token</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb2-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> getsource</span>
<span id="cb2-8"></span>
<span id="cb2-9">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Checkpoint(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"answerdotai/answerai-colbert-small-v1"</span>, ColBERTConfig())</span>
<span id="cb2-10">ce <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CollectionEncoder(config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ColBERTConfig(), checkpoint<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>checkpoint)</span></code></pre></div>
</details>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<ul>
<li>Main takeaways (Omar’s thread)</li>
<li>Abstract</li>
<li>Background: Neural Rankers</li>
<li>How does ColBERT compare to previous architectures?</li>
<li>The ColBERT architecture</li>
<li>Encoding Queries and Documents</li>
<li>MaxSim</li>
<li>Offline Indexing</li>
<li>Experimental Evaluation</li>
<li>Results</li>
<li>Ablation Studies</li>
<li>Indexing Throughput &amp; Footprint</li>
<li>Conclusion</li>
</ul>
</section>
<section id="main-takeaways-omars-thread" class="level2">
<h2 class="anchored" data-anchor-id="main-takeaways-omars-thread">Main Takeaways (Omar’s Thread)</h2>
<p>Instead of coming up with my own takeaways, I’m going to do something different this time, where I’m going to walk through Omar’s thread from 2023, where he himself summarizes the main takeaways from the ColBERT v1 paper.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Progress on dense retrievers is saturating.<br><br>The best retrievers in 2024 will apply new forms of late interaction, i.e.&nbsp;scalable attention-like scoring for multi-vector embeddings.<br><br>A🧵on late interaction, how it works efficiently, and why/where it's been shown to improve quality <a href="https://t.co/2XG33TtM9R">pic.twitter.com/2XG33TtM9R</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804963760976092?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I think it’s important to highlight that he talks about new forms of late interaction as “scalable attention-like scoring for multi-vector embeddings”. From my understanding of ColBERT, the attention-like scoring mechanism refers to the BERT contextualization of meaning of tokens. As the query or the document goes through BERT, it passes through the attention mechanism, and tokens attend to each other. So no single query token or document token is isolated; it exists in the context of the entire query or the entire document that it’s in, respectively. The “multi-vector embeddings” part of his tweet is referring to this idea: we don’t compress an entire document or an entire query into a single vector, but instead have more than one vector representing different dimensions of meaning of the text.</p>
<p><mark><strong>Update</strong>: Omar clarified on Twitter that by “attention-like scoring mechanism” he was actually referring specifically to MaxSim as fast attention-like scoring, rather than the BERT contextualization during encoding that I initially described above. Which in hindsight makes sense—emphasis on “attention-like <strong>scoring mechanism</strong>”</mark>.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Say you have 1M documents. With infinite GPUs, what would your retriever look like?<br><br>Maybe a cross-encoder? Finetune a large LM to take &lt;query,doc&gt; pairs. Run it 1M times to get a score for all docs.<br><br>Expressive! Given a query, the LM can pay attention to every detail in the doc! <a href="https://t.co/P4t7bYe9dT">pic.twitter.com/P4t7bYe9dT</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804965942013978?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Something I want to highlight is that you still have to fine-tune a language model to become a retriever because the language model itself has this general knowledge about language and the relationship between words but it doesn’t have explicitly the capability of accurately producing a score that measures the relevancy of one body of text to another. So fine-tuning brings out that implicit skill that is in the latent space of the model into an actionable task. If we had infinite GPUs, we would want to get the relationship of every query token to every document token encoded. We would want to do this during training and we would want to do this during inference. That’s why he says “expressive” - this is the ultimate expressiveness or maximum expressiveness that you can achieve between query and document tokens. Your query tokens are no longer just contextualized within the query; they are contextualized within the query and every single document. That’s a really powerful expressive way of capturing meaning between two bodies of text to determine if they are related to each other to some level.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
But cross-encoders are expensive: must re-run each doc thru the LM <em>for every new query</em>.<br><br>Most retrievers are <em>single-vector</em> encoders: Cram each doc into a vector in advance; match queries/docs with a dot-product.<br><br>Scalable! We can apply dot-product search at the billion scale! <a href="https://t.co/lvLp6vSd8K">pic.twitter.com/lvLp6vSd8K</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804967942697408?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>On the other side of the spectrum, we have the least expressive encoding but it is also more scalable: single vector encoders. You’re cramming each document into a vector and then you’re matching the queries and documents with a dot product.</p>
<p>You’re getting limited contextualization because of the limited expressiveness of what the tokens mean because everything is expressed by a single vector. But this is the most scalable.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
A huge burden on these bi-encoders: They must create <em>one</em> vector that captures <em>every</em> question you may ask about <em>any</em> content in the doc<br><br>Repeatedly been shown to be really fragile (especially OOD) and data-inefficient.<br><br>Can we build far more effective, yet scalable, encoders?
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804970144710981?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I think the key here is that the motivation is not just to build more effective encoders but effective <em>and</em> scalable encoders.</p>
<p>A brief aside on the limitations of single vector representations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/1.png" class="img-fluid figure-img" alt="The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video"></a></p>
<figcaption>The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video</figcaption>
</figure>
</div>
<p>There is a great talk by Antoine Chaffin published recently called <a href="https://youtu.be/1x3k0V2IITo?feature=shared">“Late Interaction Beats Single Vectors for RAG Introduction to PyLate”</a> which is part of Hamel Husain’s AI Evals course. He talks about how pooling is the intrinsic flaw of dense models: the pooling operation compresses <code>n</code> tokens into a single one, because of this selective behavior is learned during training through data, it gets more extreme with longer context because you have to compress more, and the compressed representation learns one notion of similarity. This is what Omar means by the “huge burden” on the bi-encoders. They have to compress a lot of information into a single representation.</p>
<p>If you think about it, during training, which Antoine is talking about here (and he also had a good thread on Twitter, which I can’t find because Twitter’s search is horrible) is that as you’re training a single vector representation, small changes in the query will result in wholesale changes of the single vector that represents the entire document.</p>
<p>So for example if you have a query about actors the document embedding will be trained on expressing that one notion of movies. If you have a separate query about the plot now the document encoding has to represent that different notion of similarity with one vector.</p>
<p>Antoine says in his thread that because of this you get a very noisy training experience for the document encodings because they’re constantly being tossed around left and right to match different notions of similarity with each query in the training step. They can’t match all of the notions because they are a single vector representation or compression of multiple tokens.</p>
<p>Contrast this with the late interaction setup where you have one representation for each token. Now, when you are training, the token embedding in the document that corresponds to the query about the actors gets modified and adjusted to result in a better relevance score. Later on in training, a query about the plot is going to activate the token in the document about the plot, and the query about the visual effects will activate the visual effect tokens in the document, and so on. So, you get this fine-grained, nuanced representation aligning with fine-grained, nuanced meaning in queries during training. The benefit of this is that n each step of training you can have a new, nuanced gradient update to the weights that produce these granular representations.</p>
<p>Back to Omar’s thread.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Late interaction (ColBERT) is simple.<br><br>Let's <em>not</em> cram docs into a vector. Instead, let's make the attention (interaction) scalable.<br><br>How? Build a <em>late</em> interaction fn.<br><br>(1) Applied after, not during, encoding.<br>(2) Pruning-capable, i.e.&nbsp;scales better than linear. This is key! <a href="https://t.co/m7fS15SdIE">pic.twitter.com/m7fS15SdIE</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804972455768167?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>If we want efficiency, we can’t get the full effect of attention. We can’t get all query tokens and all document tokens attending to each other. And we can get an interaction between query token embeddings and document token embeddings that is done efficiently if it is applied after, not during, encoding. The “late” in “late interaction” is what allows the pruning capability, which we’ll talk about later in the presentation.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Is there a function like this that preserves the retrieval precision of BERT attention?<br><br>Oddly, yes and it's incredibly simple: just aggregate (sum) the MaxSim from query tokens to doc tokens.<br><br>Basically, softly locate each query vector in the doc (w dot product), and aggregate.
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804974485811685?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The key aspect of MaxSim is it takes the maximum similarity. Unlike something like average similarity, it doesn’t care about all tokens; it cares about the document token that has the maximum similarity with the given query token. This eliminates all but one document token from final consideration, which is what allows pruning capability to unlock. Because the interaction between query and document token embeddings happens after they’re encoded, you can encode queries and documents separately.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
It was a last-ditch run on a Sunday night (3 Nov'19) after complex scoring failed.<br><br>I spent weeks looking for a "bug". ColBERT w cheap scoring rivaled BERT-large cross-encoders with 10,000x more FLOPs?!<br><br>Called it ColBERT as a pun: late show / late interaction<br><br>log scale latency: <a href="https://t.co/4MGmzAYHYG">pic.twitter.com/4MGmzAYHYG</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804976406802703?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Incredible that events like these happen, and I think happen quite frequently.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
There's a little-known trick that was essential for ColBERT's results: Query Augmentation.<br><br>ColBERT appends [MASK] tokens to the query encoder to allow BERT to create more query vectors that aren't there!<br><br>Is it the earliest form of a scratchpad / chain of thought? From Nov 2019! <a href="https://t.co/2rIoMn1jxP">pic.twitter.com/2rIoMn1jxP</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804978667434048?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This is a super interesting concept that we’ll look at in detail later on.</p>
<p>As another aside—one of the reasons I was motivated to re-read the ColBERT V1 paper was this thread below by Antoine.</p>
<blockquote class="twitter-tweet blockquote" data-theme="dark">
<p lang="en" dir="ltr">
I am starting to be more and more convinced that MaxSim generalize very well to long documents but struggles on longer query, most probably due to the asymmetry<br>Larger documents are bound by the number of query tokens, but larger queries might get noisy<br>Either it is a query…
</p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1942909502723883381?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>If you have a document with 1k tokens and a query with 32 tokens, at most 32 document tokens will pass the maximum similarity threshold, hence “larger documents are bound by the number of query tokens.” As your query gets larger the meaning of tokens becomes diverse, this may create noise in the meaning expressed in the query. Query tokens with vastly different meanings will have maximum similarity with document tokens that are vastly different in meaning as well.</p>
<p>The conversation continues with:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Yes, that is also why it was not a big issue with query expansion because all the queries had the same number of tokens<br>but with longer queries and no query exp, meh
</p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1943002350274179544?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>slm tokens makes a really interesting point. Let’s assume that for each of the 32 query tokens, you find a document token that has a cosine similarity of 1. You add them up, and the maximum similarity is 32, the length of the query. That’s what they’re saying by the maximum MaxSim is the length of the query. Assuming that as your query gets longer, the meaning of the tokens starts to vary, and potentially the maximum similarity between the query and document tokens starts to vary.</p>
<p>You can imagine that you could have a very long query where one or more tokens are kind of obscure and may be on the fringes of the intent and meaning of the whole query, and potentially dissimilar to any document in the collection. These will potentially have a very small maximum similarity with some document token. In this situation, you can imagine that you have a very high variance because some query tokens will have high maximum similarities, some will have low, and you get this kind of noisy distribution of similarities across the query.</p>
<p>They go on to say that normalization can’t be something like the mean over query tokens, but has to be related to the distribution. Why does Antoine say that there is no query expansion? We’ll see that in the next tweet.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
btw let's rule out query expansion because I am training models using flash attention and so there isn't any query expansion (I keep forgetting ffs 😭)
</p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1942969244183834786?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This will make more sense after we look at query expansion/augmentation later in this blog post. Basically, query expansion relies on masked tokens containing semantic meaning, but, IIUC, Flash Attention negates masked tokens, so it nullifies the interaction between masked tokens and other tokens.</p>
<p>Here’s a tweet from Antoine 9 months ago that explains this:</p>
<blockquote class="twitter-tweet blockquote" data-theme="dark">
<p lang="en" dir="ltr">
</p><p>Funny insight:<br><br>ColBERT query expansion works by adding tokens that are not attended but attend to the others <br><br>This works in the OG attention implementation with attention mask (as the attention values are computed for these tokens, their contributions are just masked out for the other tokens, but their representations are computed w.r.t the unmasked tokens)</p>
<p>However, with Flash Attention, masked tokens embeddings are just zeros, meaning the contribution to MaxSim is always zeros and these tokens are not used, as if there was no query expansion at all</p>
This might explain Jina-ColBERT results of attending vs not attending to those (which seems contrary to our results): if they activated FA during the tests, the comparaison is actually no query expansion vs query expansion with attending, not not attending vs attending
<p></p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1862059400271389138?ref_src=twsrc%5Etfw">November 28, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Here is a slide from a previous video that I made titled <a href="https://youtu.be/u_v6HHyv4No?feature=shared">“Understanding Eager Bidirectional Attention via the Attention Mask”</a>. In this case, we have 16 tokens, including four masked tokens. The large 16x16 tensor at the bottom is the attention scores tensor. The masked tokens are correctly not being attended to, but they do attend to other tokens. The last four columns are set to negative infinity because they are masked tokens, so their attention scores will be zero. But the last four rows do contain some 1s. The attention scores will not be zero for those rows and columns where we have 1s. Masked tokens are not being attended to, but they do attend to other tokens, and therefore they do have an attention score, and therefore they will have hidden states in the embedding space.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="16x16 attention mask"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/2.png" class="img-fluid figure-img" alt="16x16 attention mask"></a></p>
<figcaption>16x16 attention mask</figcaption>
</figure>
</div>
<p>Back to Omar’s thread.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
OK, but how can ColBERT search 100M docs in ~100 milliseconds?<br><br>Late interaction is pruning-capable: it only needs to "touch" &lt; 0.1% of the documents to find the top-K.<br><br>This is by design: it's composed of monotonic functions (Max/Sum), which enable some neat algorithmic tricks.
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804980840182079?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
We can decompose late interaction into dozens of tiny nearest-neighbor searches, at the token level.<br><br>We'll only fetch &amp; score docs in which at least one token in close to (at least one token in) the query.<br><br>Otherwise, we can prove the score will be too small, and we can skip it! <a href="https://t.co/6vBZp1U0Ku">pic.twitter.com/6vBZp1U0Ku</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804982949917032?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>As shown in the diagram, we have clusters of document token embeddings, clustered by some vector-similarity indexing process. For each query token embedding, we find the closest few clusters to it. We perform our MaxSim operation between the query tokens and all those clustered documents’ tokens. With this initial clustering step, we’re filtering out low-relevance documents using nearest neighbor search from the start.</p>
<p>With these main takeaways under our belt, in the following sections we’ll walk through each part of the ColBERTv1 paper in detail. I’ll provide excerpts from the paper in block quotes (highlighted emphasis mine) and then my thoughts after that.</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a <mark>late interaction architecture that independently encodes the query and the document using BERT</mark> and then employs a <mark>cheap yet powerful interaction step that models their fine-grained similarity.</mark> By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to <mark>pre-compute document representations offine</mark>, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT’s <mark>pruning-friendly interaction mechanism</mark> enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT’s effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing <mark>two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query</mark>.</p>
</blockquote>
<p>The key part about late interaction is that the architecture independently encodes the query and document. This allows you to index document representations offline which allows you to delay the query-document interaction until the end of the architecture.</p>
<p>What took me some unpacking is the line: &gt; ColBERT’s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection.</p>
<p>IIUC, the pruning-friendliness of ColBERT is unlocked by the fact that the interaction mechanism uses <strong>maximum</strong> similarity, and because of the nature of maximum (i.e.&nbsp;only 1 token can satisfy maximum) you can ignore low-similarity documents. Vector similarity indexes group together documents by similarity, so if a cluster of documents is not close to the query token embedding in question, it can be ignored completely.</p>
</section>
<section id="background-neural-rankers" class="level2">
<h2 class="anchored" data-anchor-id="background-neural-rankers">Background: Neural Rankers</h2>
<p>On the terms “ranker” vs.&nbsp;“retriever” and what that brings up or me:</p>
<p>The terms Ranker and Retriever give me different mental images.</p>
<p>When I think of Ranker, I think of you already having some passages that are deemed relevant and you’re ranking them, bringing the best ones to the top.</p>
<p>When I think of Retriever, the mental image I have is that you have this collection of data documents, a corpus of text where you have irrelevant and relevant passages all mixed together. The retriever then goes in, sifts through this text, and finds the relevant passages.</p>
<p>It’s been a bit of an adjustment for me using these two as synonyms. So that’s something I just want to keep in mind as I’m reading literature is that Ranker and Retriever should give me the same mental image, but they don’t.</p>
<blockquote class="blockquote">
<p><mark>By computing deeply-contextualized semantic representations of query-document pairs, these LMs help bridge the pervasive vocabulary mismatch [21, 42] between documents and queries [30].</mark></p>
</blockquote>
<p>I wanted to highlight this sentence from the background section because I thought it was getting to the core of something about BERT that I didn’t really know. We’ll see in a bit. But first–</p>
<p>The following excerpt is from the paper Modeling and Solving Term Mismatch for Full-Text Retrieval Which is reference [42], written in 2012:</p>
<blockquote class="blockquote">
<p>Even though modern retrieval systems typically use a multitude of features to rank documents, the backbone for search ranking is usually the standard tf.idf retrieval models.</p>
<p>This thesis addresses a limitation of the fundamental retrieval models, the term mismatch problem, which happens when query terms fail to appear in the documents that are relevant to the query. The term mismatch problem is a long standing problem in information retrieval.</p>
</blockquote>
<p>I haven’t read the full thesis, but it does make sense that for keyword-based search, the query term failing to appear in the document that is relevant to the query would be a major problem as it’s frequency in the document would be 0.</p>
<p>Another paper referenced on this vocabulary mismatch problem is “Understanding the Behaviors of BERT and Ranking” where they say:</p>
<blockquote class="blockquote">
<p>The observations suggest that, BERT’s pre-training on surrounding contexts favors text sequence pairs that are closer in their semantic meaning.</p>
</blockquote>
<p>So, it seems like even in the embedding space, the term mismatch problem is present. Another excerpt from the same paper:</p>
<blockquote class="blockquote">
<p>[BERT] prefers semantic matches between paraphrase tokens</p>
</blockquote>
<p>Here’s Figure 2 from the same paper where each point on the chart corresponds to one query-passage pair with a random regular term removed from the passage:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure 2 from “Understanding the Behaviors of BERT and Ranking”"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/3.png" class="img-fluid figure-img" alt="Figure 2 from “Understanding the Behaviors of BERT and Ranking”"></a></p>
<figcaption>Figure 2 from “Understanding the Behaviors of BERT and Ranking”</figcaption>
</figure>
</div>
<p>The x-axis is the original ranking score, and the y-axis is the score after the term is removed. One takeaway they had in the paper is that BERT in general has extreme scores. It either scores 1 or 0. But that’s not the main take away here when it comes to the concept of query-document-term mismatch. In the bottom right corner of the BERT chart, we can see that there are query-passage pairs with a high orginal ranking score and a low score after a term is removed. The original ranking of 1.0 drops to a ranking of 0.0. This is evidence that the query document term mismatch problem occurs in semantic space as well. If you remove a term that’s semantically similar in the query to the document, then BERT will not recognize the similarity between the two and will give the pair a low ranking score.</p>
<p>Let’s continue a little bit more into the background of neural rankers, but now in the context of how does ColBERT compare to these previous neural architectures?</p>
</section>
<section id="how-does-colbert-compare-to-previous-architectures" class="level2">
<h2 class="anchored" data-anchor-id="how-does-colbert-compare-to-previous-architectures">How does ColBERT compare to previous architectures?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure 2 from ColBERTv1 paper"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/4.png" class="img-fluid figure-img" alt="Figure 2 from ColBERTv1 paper"></a></p>
<figcaption>Figure 2 from ColBERTv1 paper</figcaption>
</figure>
</div>
<p>The small rectangles in this graphic represent words, subwords or tokens. The wider rectangle represent large dimension vectors or representations.</p>
<p>Representation-based Similarity (figure 2a) calculates a single cosine similarity score between a single query embedding and a single document embedding. Query-Document interaction (2b) feeds an interaction matrix with similarity scores between every pair of query-document tokens to a neural net which produces a single final similarity score. BERT (2c), all-to-all interaction, attends each token in the query to all other tokens in the query, and each token in the document to all other tokens in the document, contextualizing each token with all other query/document tokens. From the <a href="https://arxiv.org/pdf/1901.04085">Passage Re-Ranking with BERT paper</a>:</p>
<blockquote class="blockquote">
<p>We use a BERT_LARGE model as a binary classification model, that is, we use the [CLS] vector as input to a single layer neural network to obtain the probability of the passage being relevant</p>
</blockquote>
<p>Late interaction (2d), ColBERT, combines the best of both worlds: the offline computation of representation-based similarity and the richness/granularity of interaction-based similarity. Query tokens attend to each other during encoding, document tokens attend to each other during (offline) encoding; during interaction, each query token interacts with all document tokens and the document token with the maximum similarity is selected; these maximum similarities are summed across all query tokens, giving you one score per document. Not all documents in the collection need to be considered; vector similarity indexes naturally group relevant documents together. Searching for document token embeddings in clusters close to the query token embeddings reduces the number of candidates considered.</p>
<p>These architectural differences of ColBERT give it a ton of advantages:</p>
<blockquote class="blockquote">
<p>As Figure 1 illustrates, ColBERT can serve queries in tens or few hundreds of milliseconds. For instance, when used for reranking as in “ColBERT (re-rank)”, it delivers over 170× speedup (and requires 14,000× fewer FLOPs) relative to existing BERT-based models, while being more effective than every non-BERT baseline (§4.2 &amp; 4.3). ColBERT’s indexing—the only time it needs to feed documents through BERT—is also practical: it can index the MS MARCO collection of 9M passages in about 3 hours using a single server with four GPUs (§4.5), retaining its effectiveness with a space footprint of as little as few tens of GiBs.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="5.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure 1 from the ColBERTv1 paper"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/5.png" class="img-fluid figure-img" alt="Figure 1 from the ColBERTv1 paper"></a></p>
<figcaption>Figure 1 from the ColBERTv1 paper</figcaption>
</figure>
</div>
<p>Figure 1 shows that ColBERT has comparable performance to BERT Large and BERT Base but a 100x faster query latency. This is not an insignificant difference, as they say that “raising query response times by as little as 100ms is known to impact user experience and even measurably diminish revenue”. So, it follows that a decrease from 10,000 milliseconds to 100s of milliseconds is going to measurably increase revenue.</p>
<p>One related paper I briefly want to focus on is this 2018 paper by Zamani et al, which introduces SNRM (emphasis mine):</p>
<blockquote class="blockquote">
<p>In 2018, Zamani et al.&nbsp;[41] introduced SNRM, a representationfocused IR model that encodes each query and each document as a single, sparse high-dimensional vector of “latent terms”. By producing a <mark>sparse-vector representation for each document</mark>, SNRM is able to use a traditional IR inverted index for representing documents, allowing fast end-to-end retrieval. Despite highly promising results and insights, SNRM’s effectiveness is substantially outperformed by the state of the art on the datasets with which it was evaluated (e.g., see [18, 38]). <mark>While SNRM employs sparsity to allow using inverted indexes</mark>, we relax this assumption and compare a (dense) BERT-based representation-focused model against our late-interaction ColBERT in our ablation experiments</p>
</blockquote>
<p>I fed the SNRM paper to Claude Sonnet 4 and asked it to give me an example of sparsity allows the use of an inverted file index:</p>
<blockquote class="blockquote">
<p><strong>Simple SNRM example:</strong></p>
<p><strong>Training produces latent terms like:</strong> - Latent term 47: “sports concepts” - Latent term 203: “technology concepts”</p>
<p><strong>Document representations:</strong> - Doc A (about basketball): [0, 0, …, 0.8, 0, …, 0, 0, …] (high weight at position 47) - Doc B (about AI): [0, 0, …, 0, 0, …, 0.9, 0, …] (high weight at position 203)</p>
<p><strong>Inverted index:</strong> - Latent term 47 → [Doc A: 0.8, Doc C: 0.3] - Latent term 203 → [Doc B: 0.9, Doc D: 0.5]</p>
<p><strong>Query time:</strong> Sports query activates latent term 47 → quickly finds Doc A and C without checking every document.</p>
</blockquote>
<p>Let’s say we have two document representations: document A which is about basketball and document B which is about AI. SNRM generates sparse representations, so a number of values are zero. Let’s say that we have a latent term 47 that’s about sports concepts and a latent term 203 that’s about technology concepts. The inverted index for latent term 47 is going to store the value of that term in doc A (which is 0.8) and in doc D (let’s say that’s 0.3). For the latent term 203 (which is technology concepts), the inverted index will store 0.9 for doc B (which is the highest position value) and 0.5 for doc D (which would be some relatively low position). At query time, a sports query activates the latent term for 47, and because that’s efficiently stored in the inverted index, it’s a quick lookup and you don’t have to check every document.</p>
<p>So now that we have a sense of where the ColBERT architecture falls in the context of previous work, we can now dive into the ColBERT architecture itself.</p>
</section>
<section id="the-colbert-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-colbert-architecture">The ColBERT Architecture</h2>
<blockquote class="blockquote">
<p>delaying the query–document interaction can facilitate cheap neural re-ranking (i.e., through pre-computation) and even support practical end-to-end neural retrieval (i.e., through pruning via vector-similarity search)</p>
</blockquote>
<p>ColBERT balances neural retrieval quality and cost, benefiting both re-ranking and end-to-end retrieval. The delayed query-document interaction enables offline document indexing. At query time, you only encode the query and run MaxSim operations. For end-to-end retrieval, this same offline indexing allows vector similarity clustering—instead of searching all documents, you query the closest clusters, dramatically reducing candidates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="6.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/6.png" class="img-fluid figure-img" alt="Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d"></a></p>
<figcaption>Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d</figcaption>
</figure>
</div>
<p>The general architecture of ColBERT, comprises of: a query encoder fQ (shown in green), a document encoder fD (shown in blue), and the late interaction mechanism S(shown in gray). Given a query q and document d, fQ encodes q into a bag of embeddings Eq while fD encodes d into another bag Ed. Each embeddings in Eq and Ed is contextualized based on the other terms in q or d, respectively.</p>
<p>Before we look at the Late Interaction Mechanism, let’s look closer at what is involved during the encoding of queries and documents.</p>
</section>
<section id="encoding-queries-and-documents" class="level2">
<h2 class="anchored" data-anchor-id="encoding-queries-and-documents">Encoding Queries and Documents</h2>
<blockquote class="blockquote">
<p>We share a single BERT model among our query and document encoders but distinguish input sequences that correspond to queries and documents by prepending a special token [Q] to queries and another token [D] to documents.</p>
</blockquote>
<blockquote class="blockquote">
<p>Given BERT’s representation of each token, our encoder passes the contextualized output representations through a linear layer with no activations. This layer serves to control the dimension of ColBERT’s embeddings, producing m-dimensional embeddings for the layer’s output size m. As we discuss later in more detail, we typically x m to be much smaller than BERT’s xed hidden dimension.</p>
</blockquote>
<blockquote class="blockquote">
<p>While ColBERT’s embedding dimension has limited impact on the efficiency of query encoding, this step is crucial for controlling the space footprint of documents</p>
</blockquote>
<p>A quick note about embedding dimension: there are models such as <a href="https://huggingface.co/answerdotai/answerai-colbert-small-v1">answerai-colbert-small-v1</a> where the embedding dimension is as small as 96.</p>
<p>Here’s the desription of the query encoder:</p>
<blockquote class="blockquote">
<p><strong>Query Encoder.</strong> Given a textual query q, we tokenize it into its BERT-based WordPiece [35] tokens q1, q2…ql . We prepend the token [Q] to the query. We place this token right after BERT’s sequence-start token [CLS]. If the query has fewer than a pre-defined number of tokens Nq , <mark>we pad it with BERT’s special [mask] tokens up to length Nq</mark> (otherwise, we truncate it to the first Nq tokens). This padded sequence of input tokens is then passed into BERT’s deep transformer architecture, which computes a contextualized representation of each token.</p>
</blockquote>
<p>Here is a key contribution of this paper, that I am going to do a dive into next:</p>
<blockquote class="blockquote">
<p>We denote the padding with masked tokens as <strong>query augmentation</strong>, a step that allows BERT to produce query-based embeddings at the positions corresponding to these masks. Query augmentation is intended to serve as a <mark>soft, differentiable mechanism for learning to expand queries with new terms or to re-weigh existing terms based on their importance for matching the query</mark>. As we show in §4.4, this operation is essential for ColBERT’s effectiveness.</p>
</blockquote>
<section id="query-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="query-augmentation">Query Augmentation</h3>
<p>Query augmentation is the idea that mask tokens carry semantic meaning, so padding short queries up to some fixed length expands the queries with these new semantically relevant terms, adding more nuance to help match similar terms in documents. In this side quest, I want to understand just how semantically similar these mask token embeddings are to the non-mask query tokens. I’ll start by digging into the code in the repo which takes text and converts it to embeddings.</p>
<p>Here’s how the <code>Searcher</code> encodes the query, where it uses <code>queryFromText</code></p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encode(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, text: TextQueries, full_length_search<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb3-2">    queries <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(text) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> [text]</span>
<span id="cb3-3">    bsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(queries) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-4"></span>
<span id="cb3-5">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.checkpoint.query_tokenizer.query_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.query_maxlen</span>
<span id="cb3-6">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.checkpoint.queryFromText(queries, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>bsize, to_cpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, full_length_search<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>full_length_search)</span>
<span id="cb3-7"></span>
<span id="cb3-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> Q</span></code></pre></div>
<div id="cell-88" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="37d9aab6-94cb-4de6-fcf1-f5a8e2827de4" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">checkpoint.query_tokenizer.query_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb4-2">Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.queryFromText([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"this is a short query"</span>], bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, to_cpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, full_length_search<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast() if self.activated else NullContextManager()</code></pre>
</div>
</div>
<div id="cell-89" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f66d2856-acc2-454d-911b-3f3b98324759" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">Q.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>torch.Size([1, 32, 96])</code></pre>
</div>
</div>
<p>Note that even though there are less than 32 tokens in <code>"this is a short query"</code>, the norm of all <code>Q</code> embeddings is <code>1.0</code>. This is because ColBERT adds <code>[MASK]</code> tokens to pad the query to a 32-token length, which we’ll see next.</p>
<div id="cell-91" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b21a9f43-389e-4837-bb58-ef047821bddb" data-execution_count="199">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">Q.norm(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="199">
<pre><code>tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])</code></pre>
</div>
</div>
<p><code>checkpoint.queryFromText</code> uses <code>QueryTokenizer.tokenizer</code> which does the following:</p>
<p>It first tokenizes the text</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># tokenize with max_length - 1 to add the marker id afterwards</span></span>
<span id="cb10-2">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tok(batch_text, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_length'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb10-3">                return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(max_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).to(DEVICE)</span>
<span id="cb10-4"></span>
<span id="cb10-5">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Q_marker_token_id)</span>
<span id="cb10-6">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p>And then replaces the padding token with the mask token.</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># postprocess for the [MASK] augmentation</span></span>
<span id="cb11-2">ids[ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.pad_token_id] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask_token_id</span></code></pre></div>
<p>Looking at that concretely:</p>
<div id="cell-94" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.query_tokenizer.tok([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"this is a short query"</span>], padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_length'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb12-2">                return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb12-3"></span>
<span id="cb12-4">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], checkpoint.query_tokenizer.Q_marker_token_id)</span>
<span id="cb12-5">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-95" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6d429500-8377-4845-c844-08099e97adcc" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]])</code></pre>
</div>
</div>
<div id="cell-96" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="14e03a64-ea80-4a01-db59-1d2dc156355a" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">ids[ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> checkpoint.query_tokenizer.pad_token_id] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.query_tokenizer.mask_token_id</span>
<span id="cb15-2">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103]])</code></pre>
</div>
</div>
<div id="cell-97" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="7d24afd2-b0ea-4f04-a1bf-ad826c24ce00" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">checkpoint.query_tokenizer.tok.decode([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">103</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>'[MASK]'</code></pre>
</div>
</div>
<div id="cell-98" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:71}}" data-outputid="900df54d-3ce7-428a-d888-c7cd3023214d" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">checkpoint.query_tokenizer.tok.decode(ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>'[CLS] [unused0] this is a short query [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'</code></pre>
</div>
</div>
<p>This replacement of pad tokens with mask tokens is critical because <code>queryFromText</code> calls <code>query</code> which is defined as:</p>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> query(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, input_ids, attention_mask):</span>
<span id="cb21-2">    input_ids, attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> input_ids.to(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device), attention_mask.to(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device)</span>
<span id="cb21-3">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.bert(input_ids, attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb21-4">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear(Q)</span>
<span id="cb21-5"></span>
<span id="cb21-6">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask(input_ids, skiplist<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]), device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb21-7">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask</span>
<span id="cb21-8"></span>
<span id="cb21-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.nn.functional.normalize(Q, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<p>It actually creates its own <code>mask</code> to multiply <code>Q</code> by—it doesn’t use <code>attention_mask</code>.</p>
<p>Looking at <code>mask</code>:</p>
<div id="cell-102" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f57d4111-5908-4afa-8094-80164a8f80af" data-execution_count="20">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(getsource(checkpoint.mask))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    def mask(self, input_ids, skiplist):
        mask = [[(x not in skiplist) and (x != self.pad_token) for x in d] for d in input_ids.cpu().tolist()]
        return mask
</code></pre>
</div>
</div>
<p>If the token is <code>not in skiplist</code> and <code>!= self.pad_token</code> it gets a <code>1</code> in the <code>mask</code>. Since we swapped pad tokens with MASK tokens, they get a <code>1</code>.</p>
<div id="cell-104" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c8d1cd3a-ee95-43f9-f43f-0ceffc6c2aaa" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103]])</code></pre>
</div>
</div>
<div id="cell-105" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="99bb7805-01f9-43be-f484-4bfd3e7a088c" data-execution_count="24">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">torch.tensor(checkpoint.mask(ids, skiplist<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[])).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</code></pre>
</div>
</div>
<p>As we can see, the <code>mask</code> is all <code>1</code>s, so <code>Q</code> remains unchanged.</p>
<p>As an aside, I’ve been thinking about how RAGatouille sets the maximum query length based on the full query length (instead of fixing to 32):</p>
<div class="sourceCode" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(query, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>):</span>
<span id="cb28-2">    query_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(query.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.35</span>)</span>
<span id="cb28-3">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._upgrade_searcher_maxlen(query_length, base_model_max_tokens)</span>
<span id="cb28-4">    results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._search(query, k, pids)]</span>
<span id="cb28-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb28-6">    longest_query_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>([<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.35</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> query])</span>
<span id="cb28-7">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._upgrade_searcher_maxlen(longest_query_length, base_model_max_tokens)</span>
<span id="cb28-8">    results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._batch_search(query, k)</span></code></pre></div>
<p>I think the following note about <code>full_length_search</code> in the ColBERT repo is related but I’m not currently sure:</p>
<div class="sourceCode" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Full length search is only available for single inference (for now)</span></span>
<span id="cb29-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Batched full length search requires far deeper changes to the code base</span></span>
<span id="cb29-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span>(full_length_search <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(batch_text) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(batch_text) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb29-4"></span>
<span id="cb29-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> full_length_search:</span>
<span id="cb29-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Tokenize each string in the batch</span></span>
<span id="cb29-7">    un_truncated_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tok(batch_text, add_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).to(DEVICE)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>]</span>
<span id="cb29-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the longest length in the batch</span></span>
<span id="cb29-9">    max_length_in_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> un_truncated_ids)</span>
<span id="cb29-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set the max length</span></span>
<span id="cb29-11">    max_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_len(max_length_in_batch)</span>
<span id="cb29-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb29-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Max length is the default max length from the config</span></span>
<span id="cb29-14">    max_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.query_maxlen</span></code></pre></div>
</section>
<section id="mask-token-embeddings-are-not-meaningless" class="level3">
<h3 class="anchored" data-anchor-id="mask-token-embeddings-are-not-meaningless"><code>MASK</code> token embeddings are not meaningless</h3>
<p>So what meaning is embedded for the MASK token in the semantic space? To (lightly) explore this, I’ll calculate the cosine similarity between the non-MASK and MASK tokens.</p>
<div id="cell-110" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1d97c78d-80f4-426d-9feb-7aaa3f8dcd15" data-execution_count="25">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>tensor([  101,     1,  2023,  2003,  1037,  2460, 23032])</code></pre>
</div>
</div>
<div id="cell-111" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="55585a26-af0a-4b64-8453-102cc78da962" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">checkpoint.query_tokenizer.tok.decode(ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>'[CLS] [unused0] this is a short query'</code></pre>
</div>
</div>
<div id="cell-112" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="9e6e1ed8-eca4-4a74-b2da-f98270723158" data-execution_count="32">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">checkpoint.query_tokenizer.tok.decode([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2460</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">23032</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>'short query'</code></pre>
</div>
</div>
<div id="cell-113" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="620147a3-0f6b-4a41-cd88-120864057e16" data-execution_count="27">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>:]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor([103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103,
        103, 103, 103, 103, 103, 103, 103, 103, 103, 103])</code></pre>
</div>
</div>
<div id="cell-114" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:53}}" data-outputid="91f261b2-0a15-4150-cf9d-58c6933c3069" data-execution_count="28">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">checkpoint.query_tokenizer.tok.decode(ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>:])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>'[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'</code></pre>
</div>
</div>
<p>Gathering the 96-dimensional embeddings for my non-MASK tokens.</p>
<div id="cell-116" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="249a17b0-4f81-4171-84a9-dc10719f7f0a" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">Qnm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]</span>
<span id="cb40-2">Qnm.shape, Qnm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(torch.Size([7, 96]), torch.Size([1, 7, 96]))</code></pre>
</div>
</div>
<p>Gathering the 96-dimensional embeddings for my MASK tokens.</p>
<div id="cell-118" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f68d9c1b-3b29-4da5-e522-dcd4e2a19cee" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">Qm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>:]</span>
<span id="cb42-2">Qm.shape, Qm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(torch.Size([25, 96]), torch.Size([25, 1, 96]))</code></pre>
</div>
</div>
<p>Taking the cosine similarity between the non-MASK and MASK tokens, we see that the MASK tokens (rows) are considerably similar in meaning (and in one case exactly the same) to the non-MASK tokens (columns)!</p>
<div id="cell-120" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fbf9c790-8b74-49db-e9a9-248b313bf574" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">torch.nn.functional.cosine_similarity(Qnm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), Qm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>tensor([[1.0000, 0.9709, 0.9150, 0.9461, 0.9672, 0.7417, 0.6532],
        [0.9793, 0.9566, 0.9038, 0.9301, 0.9495, 0.7177, 0.6400],
        [0.9804, 0.9585, 0.9053, 0.9320, 0.9517, 0.7195, 0.6380],
        [0.9814, 0.9604, 0.9059, 0.9340, 0.9534, 0.7198, 0.6394],
        [0.9812, 0.9598, 0.9065, 0.9342, 0.9533, 0.7220, 0.6390],
        [0.9814, 0.9601, 0.9073, 0.9355, 0.9537, 0.7223, 0.6341],
        [0.9816, 0.9612, 0.9068, 0.9367, 0.9549, 0.7222, 0.6354],
        [0.9809, 0.9604, 0.9050, 0.9357, 0.9535, 0.7212, 0.6333],
        [0.9823, 0.9623, 0.9064, 0.9385, 0.9562, 0.7228, 0.6365],
        [0.9826, 0.9629, 0.9070, 0.9389, 0.9567, 0.7249, 0.6373],
        [0.9818, 0.9614, 0.9062, 0.9387, 0.9554, 0.7248, 0.6390],
        [0.9819, 0.9608, 0.9044, 0.9387, 0.9550, 0.7241, 0.6388],
        [0.9821, 0.9620, 0.9032, 0.9394, 0.9561, 0.7252, 0.6372],
        [0.9830, 0.9655, 0.9048, 0.9428, 0.9601, 0.7278, 0.6390],
        [0.9836, 0.9690, 0.9081, 0.9459, 0.9632, 0.7273, 0.6392],
        [0.9860, 0.9761, 0.9165, 0.9517, 0.9717, 0.7343, 0.6475],
        [0.9850, 0.9848, 0.9299, 0.9542, 0.9805, 0.7550, 0.6576],
        [0.9805, 0.9872, 0.9360, 0.9545, 0.9827, 0.7667, 0.6658],
        [0.9780, 0.9878, 0.9385, 0.9544, 0.9833, 0.7705, 0.6693],
        [0.9774, 0.9879, 0.9389, 0.9543, 0.9832, 0.7701, 0.6699],
        [0.9764, 0.9887, 0.9397, 0.9549, 0.9839, 0.7709, 0.6731],
        [0.9762, 0.9887, 0.9394, 0.9548, 0.9838, 0.7714, 0.6736],
        [0.9763, 0.9885, 0.9396, 0.9548, 0.9837, 0.7715, 0.6738],
        [0.9776, 0.9881, 0.9392, 0.9551, 0.9837, 0.7709, 0.6713],
        [0.9772, 0.9887, 0.9400, 0.9555, 0.9840, 0.7714, 0.6736]])</code></pre>
</div>
</div>
<p>It’s interesting to note that the first MASK token (first row) has a cosine similarity of 1 with the first non-MASK token (the <code>[CLS]</code> tokens, first column). Other interesting observations:</p>
<ul>
<li>the second non-MASK token (<code>[unused0]</code>, second column) is more similar to the last MASK token than most of the other MASK tokens.</li>
<li>In general, the MASK tokens are much less similar to the last two non-MASK token (<code>short query</code>, 6th and 7th columns) than they are to the first five non-MASK tokens.</li>
</ul>
<p>I think this is enough evidence to show that the MASK tokens carry semantic meaning important to the query.</p>
<p>Returning to the paper, let’s see what they have to say about the document encoder:</p>
<blockquote class="blockquote">
<p><strong>Document Encoder</strong>. Our document encoder has a very similar architecture. We first segment a document d into its constituent tokens d1, d2…dm, to which we prepend BERT’s start token [CLS] followed by our special token [D] that indicates a document sequence. <mark>Unlike queries, we do not append [mask] tokens to documents</mark>. After passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list. This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness. In summary, given q = q0, q1…ql and d = d0, d1…dn , we compute the bags of embeddings Eq and Ed in the following manner, where # refers to the [mask] tokens:</p>
<p>Eq := Normalize( CNN( BERT(“[Q]q0q1…ql ##…#”) ) ) (1)</p>
<p>Ed := Filter( Normalize( CNN( BERT(“[D]d0d1…dn”) ) ) ) (2)</p>
</blockquote>
<p>I want to highlight something they say about how they encode their documents:</p>
<blockquote class="blockquote">
<p>When batching, <mark>we pad all documents to the maximum length of a document within the batch</mark>. To make capping the sequence length on a per-batch basis more effective, our indexer proceeds through documents in groups of B (e.g., B = 100,000) documents. <mark>It sorts these documents by length</mark> and then feeds batches of b (e.g., b = 128) <mark>documents of comparable length</mark> through our encoder.</p>
</blockquote>
<p>So let’s look at some of the code. In the <code>CollectionEncoder</code> class, which is what’s used to encode documents and queries, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_encoder.py#L26">they call <code>docFromText</code></a> and they pass to it the passages which are currently strings:</p>
<div class="sourceCode" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> passages_batch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> batch(passages, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.index_bsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>):</span>
<span id="cb46-2">    embs_, doclens_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.checkpoint.docFromText(</span>
<span id="cb46-3">        passages_batch,</span>
<span id="cb46-4">        ...)</span></code></pre></div>
<p>Inside <code>docFromText</code>, the document tokenizer’s <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L138"><code>tensorize</code> method is called</a>, and you pass to it the documents which are still strings:</p>
<div class="sourceCode" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> bsize:</span>
<span id="cb47-2">    text_batches, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.doc_tokenizer.tensorize(</span>
<span id="cb47-3">        docs, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>bsize</span>
<span id="cb47-4">    )</span></code></pre></div>
<p>And then <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/doc_tokenization.py#L48">inside <code>DocTokenizer.tensorize</code></a>, you first convert the text into tokens. And then you pass those tokens into the <code>_sort_by_length</code> helper method:</p>
<div class="sourceCode" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> tensorize(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, batch_text, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb48-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(batch_text) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>], (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(batch_text))</span>
<span id="cb48-3"></span>
<span id="cb48-4">    obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tok(batch_text, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb48-5">                    return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).to(DEVICE)</span>
<span id="cb48-6"></span>
<span id="cb48-7">    ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.D_marker_token_id)</span>
<span id="cb48-8">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb48-9"></span>
<span id="cb48-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> bsize:</span>
<span id="cb48-11">        ids, mask, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _sort_by_length(ids, mask, bsize)</span>
<span id="cb48-12">        batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _split_into_batches(ids, mask, bsize)</span>
<span id="cb48-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> batches, reverse_indices</span>
<span id="cb48-14"></span>
<span id="cb48-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ids, mask</span></code></pre></div>
<p>And finally, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/utils.py#L40">inside the <code>_sort_by_length</code> method</a>, it sums the mask in the last dimension, which is the sequence length dimension. Then it sorts it, grabs those indices, and returns the tokens of the passages in order. Using those indices:</p>
<div class="sourceCode" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _sort_by_length(ids, mask, bsize):</span>
<span id="cb49-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> ids.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> bsize:</span>
<span id="cb49-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ids, mask, torch.arange(ids.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb49-4"></span>
<span id="cb49-5">    indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).sort().indices</span>
<span id="cb49-6">    reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> indices.sort().indices</span>
<span id="cb49-7"></span>
<span id="cb49-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ids[indices], mask[indices], reverse_indices</span></code></pre></div>
<p>They’re summing the mask across the sequence length dimension (<code>mask.sum(-1)</code>). The mask contains 1s where you have non-padding tokens and 0s where you have padding tokens. So the sum of the mask across a sequence is the number of non-padding tokens in it. Sorting by this sum sorts the sequences by non-padding token length in ascending order.</p>
<p>Let’s look at this concretely through code.</p>
</section>
<section id="sorting-documents-by-length-for-batching" class="level3">
<h3 class="anchored" data-anchor-id="sorting-documents-by-length-for-batching">Sorting Documents by Length for Batching</h3>
<p>To better understand how ColBERT sorts documents by length for batching, I’m going to walk through a toy example using the internal methods provided in the repo.</p>
<p>I’ll start by intentionally creating a list of passages of four different lengths: 40, 60, 80, and 100</p>
<div id="cell-133" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">passages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb50-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>):</span>
<span id="cb50-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb50-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>)</span>
<span id="cb50-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>)</span>
<span id="cb50-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>)</span></code></pre></div>
</div>
<p>I now pass the passages with a bat size of 32 into the Checkpoints.DocFromText method, and as a result, I get encoded documents where the bat size is 128, the maximum document length is 103, and the embedding dimension is 96 because I’m using answerai-colbert-small-v1.</p>
<div id="cell-135" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="68ad41b5-0de8-47c5-a676-37ecdf8bc6db" data-execution_count="5">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.docFromText(docs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>passages, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast() if self.activated else NullContextManager()</code></pre>
</div>
</div>
<div id="cell-136" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1414b4c2-4f36-407c-b617-2b1e08a96169" data-execution_count="6">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">res[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>torch.Size([128, 103, 96])</code></pre>
</div>
</div>
<p>If we look one level deeper, inside docfromtext it calls the docTokenizer’s tensorize method which converts the string of text into tokens</p>
<div id="cell-138" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">text_batches, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tensorize(passages, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span></code></pre></div>
</div>
<div id="cell-139" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1880f14a-a499-48af-a232-1866041700f1" data-execution_count="8">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">text_batches[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>torch.Size([32, 103])</code></pre>
</div>
</div>
<div id="cell-140" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="eb4ab00f-7d27-4e08-f589-29937da66e0c" data-execution_count="10">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">text_batches[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([ 101,    2, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037,  102,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0], device='cuda:0')</code></pre>
</div>
</div>
<p>Taking a look at the number of tokens in each of the batch items.</p>
<div id="cell-142" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1c30db69-0973-428c-8334-db7e12190cce" data-execution_count="11">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>]:</span>
<span id="cb60-2">    obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tok(passages[i], padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb60-3">                       return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb60-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>].shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1, 102])
torch.Size([1, 82])
torch.Size([1, 62])
torch.Size([1, 42])</code></pre>
</div>
</div>
<p><code>tensorize</code> adds the <code>[Q]</code> or <code>[D]</code> token, so that’s why the first batch item only has 102 tokens, whereas after Tensorize, it has 103 tokens.</p>
<p>Before I run the rest of the sorting code, I’m going to shuffle the passages so that we can see if sorting actually takes place.</p>
<div id="cell-145" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1">random.shuffle(passages)</span></code></pre></div>
</div>
<div id="cell-146" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1d2981e8-e3eb-4cf4-9e2d-477bc4d8b4f4" data-execution_count="14">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> passages[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(o))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>80
120
80
80
160
80
160
160
120
200</code></pre>
</div>
</div>
<p>When passing all of the passages to the <code>.tok</code> method, the tokenized batch has a number of tokens equal to the largest, longest passage. All 128 passages are tokenized up to a length of 102.</p>
<div id="cell-148" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="91d5a4ed-20e5-4e49-c275-63186d596371" data-execution_count="15">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tok(passages, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb65-2">                       return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb65-3">obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>torch.Size([128, 102])</code></pre>
</div>
</div>
<p>Here’s the step where we add the prefixed tokens for the documents, which is <code>[D]</code>.</p>
<div id="cell-150" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], ce.checkpoint.doc_tokenizer.D_marker_token_id)</span>
<span id="cb67-2">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-151" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e9702bae-2d9f-4243-d67e-6493a571f295" data-execution_count="17">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">ids.shape, mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Looking at the number of non-zero tokens in the batch (i.e., the non-padding tokens), we can see that our batch is still currently unsorted</p>
<div id="cell-153" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fbe6f4d0-b7ee-4ccb-a64a-91cba295e447" data-execution_count="18">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb70-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(63)
tensor(43)
tensor(43)
tensor(83)
tensor(43)
tensor(83)
tensor(83)
tensor(63)
tensor(103)</code></pre>
</div>
</div>
<p>Looking at the sum of the masks, we can see that the sum of masks is equal to the number of non-padding tokens in the batch item.</p>
<div id="cell-155" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="51eeac0c-0838-41fa-8d40-413f43fc1f82" data-execution_count="19">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>): <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(mask[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(63)
tensor(43)
tensor(43)
tensor(83)
tensor(43)
tensor(83)
tensor(83)
tensor(63)
tensor(103)</code></pre>
</div>
</div>
<p>Alright, here’s the main part where it sorts by length the batches.</p>
<div id="cell-157" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="54ca1995-bd89-4a39-955d-bc69bdc07331" data-execution_count="20">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">ids, mask, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _sort_by_length(ids, mask, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span>
<span id="cb74-2">ids.shape, mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Looking at the lengths of non-zero values in ids and mask. The items are now sorted by token length in increasing order.</p>
<div id="cell-159" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="942cc8ed-af36-4b59-838a-ccf99c63b13f" data-execution_count="21">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb76-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[i]), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(mask[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)</code></pre>
</div>
</div>
<p>It then splits it into batches of 32. Note that all batches are padded up to the maximum document length.</p>
<div id="cell-161" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _split_into_batches(ids, mask, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span></code></pre></div>
</div>
<div id="cell-162" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5388ec21-f3f5-4c15-c667-582abdd8b2f1" data-execution_count="23">
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>): <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(batches[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 103])
torch.Size([32, 103])
torch.Size([32, 103])
torch.Size([32, 103])</code></pre>
</div>
</div>
<p>Looking at the lengths of non-zero values in each batch we can see that the batches are now sorted by length of passage.</p>
<div id="cell-164" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1424fa48-3507-4b53-8f82-52ccbd0ad231" data-execution_count="24">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>):</span>
<span id="cb81-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"------ Batch </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb81-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb81-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(batches[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][j]), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(batches[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][j]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>------ Batch 0
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
------ Batch 1
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
------ Batch 2
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
------ Batch 3
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)</code></pre>
</div>
</div>
<p>Now let’s go a layer deeper and look at <code>_sort_by_length</code> to see how the sorting actually happens. I’ll reinstantiate the passages and shuffle them to make sure the sorting actually happens.</p>
<div id="cell-166" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1">passages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb83-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>):</span>
<span id="cb83-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb83-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>)</span>
<span id="cb83-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>)</span>
<span id="cb83-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>)</span></code></pre></div>
</div>
<div id="cell-167" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">random.shuffle(passages)</span></code></pre></div>
</div>
<p>I’ll also tokenize the passages and insert the prefix tokens.</p>
<div id="cell-169" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b456ac02-a0ec-4535-e5c3-35adcb8a43e7" data-execution_count="27">
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tok(passages, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb85-2">                       return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb85-3">obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>torch.Size([128, 102])</code></pre>
</div>
</div>
<div id="cell-170" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], ce.checkpoint.doc_tokenizer.D_marker_token_id)</span>
<span id="cb87-2">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-171" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d8ef3186-088e-46b3-c0fe-2fb711f56713" data-execution_count="29">
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">ids.shape, mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Checking to make sure that my batch is shuffled</p>
<div id="cell-173" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9d2b5d99-f4b1-40a3-f4f4-6610581b7912" data-execution_count="30">
<div class="sourceCode cell-code" id="cb90" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb90-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(103)
tensor(103)
tensor(43)
tensor(83)
tensor(83)
tensor(103)
tensor(63)
tensor(83)
tensor(43)
tensor(43)</code></pre>
</div>
</div>
<p>To sort the batch items by length of non-padding tokens, they sum the mask across the last dimension, which is the number of tokens.</p>
<div id="cell-175" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="351136c3-ea11-4339-ddd8-297058bc2d4d" data-execution_count="31">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>torch.Size([128, 103])</code></pre>
</div>
</div>
<p>So, what you get here is basically the number of non-padding tokens in each of the 128 items.</p>
<div id="cell-177" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9b928051-0462-461c-907c-d1f39e828bd9" data-execution_count="32">
<div class="sourceCode cell-code" id="cb94" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1">mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>tensor([103, 103,  43,  83,  83, 103,  63,  83,  43,  43, 103,  83,  43,  83,
         63,  83,  43, 103,  43,  63, 103, 103,  43,  43, 103,  63,  83, 103,
        103, 103, 103,  63,  43,  83,  63,  83,  63, 103,  43,  43,  63,  63,
        103,  43, 103,  43,  63, 103,  43,  43,  63,  63,  83,  63, 103, 103,
         83,  63, 103,  83,  43, 103, 103,  63,  83,  43, 103,  83,  83, 103,
        103,  63,  63,  83,  43,  83,  83,  43,  83,  43, 103,  83, 103,  43,
         83,  83,  43,  63,  63,  63, 103,  63,  43, 103,  83,  63,  63,  43,
         63, 103,  83,  83,  83, 103,  63,  63,  63,  83,  83,  43,  43,  63,
        103,  83,  63,  83,  43,  83,  63,  63,  43,  43,  43,  63, 103,  43,
         83,  43])</code></pre>
</div>
</div>
<p>Then they sort it and get the indices.</p>
<div id="cell-179" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c32564d4-1263-41dd-fa9e-a2e6326b6039" data-execution_count="33">
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1">indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).sort().indices</span>
<span id="cb96-2">indices</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>tensor([ 49,  22,  23, 110, 109,  32,  38,  43,  45,  48,  39,  97,  60,  92,
         65,  86,  83,  74,  79,  77, 121, 127,   2, 125, 122,   8,   9, 116,
        120,  12,  18,  16,  63,  51,  53,  98,  96,  57,  95,  25, 114,  91,
         31,  89,   6,  88,  87, 123,  71,  72,  19, 119,  34, 106,  36, 105,
         14,  50,  40,  41, 104, 118,  46, 111,  64, 107, 126,  81, 108, 115,
         84,  85,  94, 102, 101, 113, 117, 100,  15,  33,  35,  26,  52,  56,
         59,  13,  11,   7,  67,  76,  78,  75,   3,  73,   4,  68,  17,   1,
         20,  21,  24, 124, 112,  27,  28,  29,  10,   5,  93,  80,  82,  70,
         69,  66,   0,  90,  62,  61,  30,  58,  55,  54,  99,  47,  44, 103,
         42,  37])</code></pre>
</div>
</div>
<div id="cell-180" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="110c7559-8e92-4cda-b255-2b6360b377d5" data-execution_count="37">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">mask[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">49</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(), mask[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">37</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>(tensor(43), tensor(103))</code></pre>
</div>
</div>
<p>If we look at the first index, the corresponding mask sum is 43, which is the smallest non-padding token length. And if we look at the last index, the sum of the mask is 103, which is the largest.</p>
<p>And then they sort these indices and then get the indices of that sort</p>
<div id="cell-183" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4d611ab4-29ea-4cbd-e22a-4833bcd1c8b5" data-execution_count="38">
<div class="sourceCode cell-code" id="cb100" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1">reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> indices.sort().indices</span>
<span id="cb100-2">reverse_indices</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor([114,  97,  22,  92,  94, 107,  44,  87,  25,  26, 106,  86,  29,  85,
         56,  78,  31,  96,  30,  50,  98,  99,   1,   2, 100,  39,  81, 103,
        104, 105, 118,  42,   5,  79,  52,  80,  54, 127,   6,  10,  58,  59,
        126,   7, 124,   8,  62, 123,   9,   0,  57,  33,  82,  34, 121, 120,
         83,  37, 119,  84,  12, 117, 116,  32,  64,  14, 113,  88,  95, 112,
        111,  48,  49,  93,  17,  91,  89,  19,  90,  18, 109,  67, 110,  16,
         70,  71,  15,  46,  45,  43, 115,  41,  13, 108,  72,  38,  36,  11,
         35, 122,  77,  74,  73, 125,  60,  55,  53,  65,  68,   4,   3,  63,
        102,  75,  40,  69,  27,  76,  61,  51,  28,  20,  24,  47, 101,  23,
         66,  21])</code></pre>
</div>
</div>
<div id="cell-184" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="8b07ff83-1783-4921-e11e-131109581342" data-execution_count="39">
<div class="sourceCode cell-code" id="cb102" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1">indices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">114</span>], indices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">97</span>], indices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">22</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(tensor(0), tensor(1), tensor(2))</code></pre>
</div>
</div>
<p>Looking at the first three values of <code>reverse_indices</code>: the reverse indices’ first value corresponds to the original index of 0, the reverse indices’ second value corresponds to the original index of 1, and the reverse indices’ third value corresponds to the original index of 2.</p>
<p>Finally, using <code>indices</code> to index into <code>ids</code></p>
<div id="cell-187" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2208b96c-fa28-4691-82b7-da4f0c41477a" data-execution_count="40">
<div class="sourceCode cell-code" id="cb104" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1">ids[indices]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([[ 101,    2, 1037,  ...,    0,    0,    0],
        [ 101,    2, 1037,  ...,    0,    0,    0],
        [ 101,    2, 1037,  ...,    0,    0,    0],
        ...,
        [ 101,    2, 1037,  ..., 1037, 1037,  102],
        [ 101,    2, 1037,  ..., 1037, 1037,  102],
        [ 101,    2, 1037,  ..., 1037, 1037,  102]])</code></pre>
</div>
</div>
<p>We can see that <code>ids[indices]</code> is sorted.</p>
<div id="cell-189" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5f05b8f3-8f3a-4a69-fa8a-0b19dfbf5175" data-execution_count="43">
<div class="sourceCode cell-code" id="cb106" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>):</span>
<span id="cb106-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb106-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[indices][i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>j]))</span>
<span id="cb106-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(43)
tensor(43)
tensor(43)
tensor(43)
------------------------------
tensor(63)
tensor(63)
tensor(63)
tensor(63)
tensor(63)
------------------------------
tensor(83)
tensor(83)
tensor(83)
tensor(83)
tensor(83)
------------------------------
tensor(103)
tensor(103)
tensor(103)
tensor(103)
tensor(103)
------------------------------</code></pre>
</div>
</div>
<p>And that is what they mean by “sorting documents by length and feeding the batches with documents of comparable length to the encoder”.</p>
<p>So that’s how queries and documents are encoded. We can now look at the interaction between the two at query time, which is the MaxSim operator.</p>
</section>
</section>
<section id="maxsim" class="level2">
<h2 class="anchored" data-anchor-id="maxsim">MaxSim</h2>
<blockquote class="blockquote">
<p>Using Eq and Ed , ColBERT computes the relevance score between q and d via late interaction, which we define as a summation of maximum similarity (MaxSim) operators. In particular, we find the maximum cosine similarity of each v ∈ Eq with vectors in Ed , and combine the outputs via summation.</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?S_%7Bq,d%7D%20:=%20%5Csum_%7Bi%20%5Cin%20%5B%5C%7CE_q%5C%7C%5D%7D%20%5Cmax_%7Bj%20%5Cin%20%5B%5C%7CE_d%5C%7C%5D%7D%20E_%7Bqi%7D%20%5Ccdot%20E_%7Bdj%7D%5ET"></p>
<p>Looking at the equation—we iterate through the queries, for each query we iterate through the document tokens and calculate the cosine similarity. we keep the maximum and sum it to S. Note that cosine similarity can be implemented as dot product because the embeddings are normalized. Another way to put it, taken from <a href="https://www.mixedbread.com/blog/maxsim-cpu#:~:text=For%20each%20candidate%20document%2C%20MaxSim%20iterates%20through%20every%20token%20within%20the%20query%2C%20and%20compares%20its%20similarity%20to%20every%20token%20within%20the%20document%2C%20before%20keeping%20the%20maximum%20value%20for%20each%20query%20token%20(hence%20the%20Max)%20and%20summing%20them%20up%20to%20produce%20a%20document%2Dlevel%20score.">Ben Clavie’s recent maxsim-cpu release blog post</a>:</p>
<blockquote class="blockquote">
<p>For each candidate document, MaxSim iterates through every token within the query, and compares its similarity to every token within the document, before keeping the maximum value for each query token (hence the Max) and summing them up to produce a document-level score.</p>
</blockquote>
<p>Beautiful.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="7.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Example of a MaxSim calculation between a query and a document"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/7.png" class="img-fluid figure-img" alt="Example of a MaxSim calculation between a query and a document"></a></p>
<figcaption>Example of a MaxSim calculation between a query and a document</figcaption>
</figure>
</div>
<p>Here is an example of the summation of MaxSim between query and document token embeddings. In this example, we have four query tokens and eight document tokens. For the first query token q1, the highest cosine similarity is with the fifth document token d5. d1 has the maximum cosine similarity for that q2, d2 for q3, and d8 for q4. Adding up these maximum cosine similarities, we get a final relevance score of 3.1. Since there are four tokens, the maximum possible MaxSim value is 4.0.</p>
<blockquote class="blockquote">
<p>this interaction mechanism softly searches for each query term tq —in a manner that reflects its context in the query—against the document’s embeddings, quantifying the strength of the “match” via the largest similarity score between tq and a document term td . Given these term scores, it then estimates the document relevance by summing the matching evidence across all query terms.</p>
</blockquote>
<p>The query tokens have passed through a transformer model and as such have passed through an attention mechanism so that all tokens attend to all other tokens. So the query itself now has interdependent relationships across tokens. When we’re searching for one token and looking to find the closest document, we’re not just looking to find the closest document to that token in isolation, we’re trying to find the closest document to that token within the context of the entire query. Some contextualized query token embeddings will find strong matches in certain documents, but what we’re looking for is the document for which the total maximum similarity for all query tokens is the largest. You can imagine that as a query gets very long, and the words in the query drift farther apart in meaning, the MaxSim values (before summation) for a document will have high variance.</p>
<blockquote class="blockquote">
<p>more sophisticated matching is possible with other choices such as deep convolution and attention layers (i.e., as in typical interaction-focused models),</p>
</blockquote>
<p>This reminds me of the <a href="https://arxiv.org/abs/2502.05364">Hypencoder paper</a> where they use a neural net for each query that takes as input a document embeddings and outputs a scalar relevance score. This is motivated by the fact that inner product (which is what cosine similarity is) is a linear operation and can thus only linearly separate two groups of vectors (such as embeddings). When your embedding dimension is much smaller than the number of vectors that you have, you can’t separate two groups linearly. In our case, our embedding dimension may be 96 and the number of vectors could be in the millions. Mathematically, you cannot linearly separate such a high number of vectors when they’re in a relatively low-dimensional space.</p>
<p>So you need a complex function because a line doesn’t work, and anytime you need a complex function where it’s more squiggly than a line, a neural net is a good choice!</p>
<p>However, the simplicity of MaxSim has two benefits:</p>
<blockquote class="blockquote">
<p>First, it stands out as a particularly cheap interaction mechanism, as we examine its FLOPs in §4.2. Second, and more importantly, it is amenable to highly-efficient pruning for top-k retrieval, as we evaluate in §4.3. This enables using vector-similarity algorithms for skipping documents without materializing the full interaction matrix or even considering each document in isolation. Other cheap choices (e.g., a summation of average similarity scores, instead of maximum) are possible; however, many are less amenable to pruning.</p>
</blockquote>
<p>ColBERT’s MaxSim mechanism enables efficient pruning: document tokens are clustered by similarity in vector indexes. At query time, each query token searches only the nearest clusters, skipping irrelevant documents. The “maximum” aggregation makes this possible—you only need the best matches, not exhaustive comparison across all documents.</p>
<p>We’re going to take a look at this paragraph, and then we’re going to look at the code that corresponds to it. Note that this is for a single query, which is what I’m going to focus on.</p>
<blockquote class="blockquote">
<p>Given a query q, we compute its bag of contextualized embeddings Eq (Equation 1) and, concurrently, gather the document representations into a 3-dimensional tensor D consisting of k document matrices. We pad the k documents to their maximum length to facilitate batched operations, and move the tensor D to the GPU’s memory. On the GPU, <mark>we compute a batch dot-product of Eq and D</mark>, possibly over multiple mini-batches. e output materializes a 3-dimensional tensor that is a collection of cross-match matrices between q and each document. To compute the score of each document, <mark>we reduce its matrix across document terms via a max-pool (i.e., representing an exhaustive implementation of our MaxSim computation) and reduce across query terms via a summation.</mark> Finally, we sort the k documents by their total scores.</p>
</blockquote>
<p>So let’s first look at the higher level class which is the <code>IndexScorer</code>. <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L188C9-L189C75">In <code>score_pids</code></a>, if the query size is 1 (which it is in our case), it’s going to pass the query and the documents to <code>colbert_score_packed</code>.</p>
<div class="sourceCode" id="cb108" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> Q.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb108-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> colbert_score_packed(Q, D_packed, D_mask, config), pids</span></code></pre></div>
<p>Inside <code>colbert_score_packed</code>, it removes the unit batch axis of the queries, and makes sure that q and d both have two dimensions. Then it performs the dot product between the two, and we can do this instead of explicitly calling cosine similarity because q and d are both normalized embeddings. The dot product results in a <code>scores</code> tensor that has size <code>number of document tokens x number of query tokens</code>. It then passes these scores into the <code>StridedTensor</code> and it gets back <code>scores_padded</code> and <code>scores_mask</code> which are then passed to <code>colbert_score_reduce</code>.</p>
<div class="sourceCode" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> colbert_score_packed(Q, D_packed, D_lengths, config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ColBERTConfig()):</span>
<span id="cb109-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb109-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Works with a single query only.</span></span>
<span id="cb109-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb109-5"></span>
<span id="cb109-6">    use_gpu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> config.total_visible_gpus <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb109-7"></span>
<span id="cb109-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> use_gpu:</span>
<span id="cb109-9">        Q, D_packed, D_lengths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q.cuda(), D_packed.cuda(), D_lengths.cuda()</span>
<span id="cb109-10"></span>
<span id="cb109-11">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q.squeeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># removes the unit batch axis</span></span>
<span id="cb109-12"></span>
<span id="cb109-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> Q.dim() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, Q.size()                     <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># num query tokens x emb dim</span></span>
<span id="cb109-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> D_packed.dim() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, D_packed.size()       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># num doc tokens   x emb dim</span></span>
<span id="cb109-15"></span>
<span id="cb109-16">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D_packed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Q.to(dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>D_packed.dtype).T  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># num doc tokens x num query tokens</span></span>
<span id="cb109-17"></span>
<span id="cb109-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> use_gpu <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> config.interaction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"flipr"</span>:</span>
<span id="cb109-19">        scores_padded, scores_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StridedTensor(scores, D_lengths, use_gpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>use_gpu).as_padded_tensor()</span>
<span id="cb109-20"></span>
<span id="cb109-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> colbert_score_reduce(scores_padded, scores_mask, config)</span>
<span id="cb109-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb109-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ColBERT.segmented_maxsim(scores, D_lengths)</span></code></pre></div>
<p><code>scores_padded</code> has shape <code>number of documents x maximum number of tokens in the documents x number of query tokens</code>. So if we have three documents, a maximum of 13 document tokens, and 32 query tokens, <code>scores_padded</code> has shape 3 x 13 x 32.</p>
<p>Finally, <code>colbert_score_reduce</code> is called which takes the maximum of <code>scores_padded</code> across the second dimension (number of document tokens) to leave us with one score for each query token per document.</p>
<div class="sourceCode" id="cb110" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> colbert_score_reduce(scores_padded, D_mask, config: ColBERTConfig):</span>
<span id="cb110-2">    D_padding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>D_mask.view(scores_padded.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), scores_padded.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>()</span>
<span id="cb110-3">    scores_padded[D_padding] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9999</span></span>
<span id="cb110-4">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores_padded.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).values</span>
<span id="cb110-5"></span>
<span id="cb110-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># flipr code removed for brevity</span></span>
<span id="cb110-7"></span>
<span id="cb110-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> scores.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p>Taking <code>scores.sum(-1)</code>, the summation across the query token dimension, leaves us with one score per document, our desired result.</p>
<blockquote class="blockquote">
<p>Relative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in fact, its cost is dominated by the cost of gathering and transferring the pre-computed embeddings. To illustrate, ranking k documents via typical BERT rankers requires feeding BERT k different inputs each of length l = |q| + |di | for query q and documents di , where attention has quadratic cost in the length of the sequence. In contrast, ColBERT feeds BERT only a single, much shorter sequence of length l = |q|. Consequently, ColBERT is not only cheaper, it also scales much better with k as we examine in §4.2.</p>
</blockquote>
<p>So, what is involved in that cost of gathering and transferring the pre-computed embeddings? We will look at what they say about offline indexing next.</p>
</section>
<section id="offline-indexing" class="level2">
<h2 class="anchored" data-anchor-id="offline-indexing">Offline Indexing</h2>
<blockquote class="blockquote">
<p>Instead of applying MaxSim between one of the query embeddings and all of one document’s embeddings, we can use fast vector-similarity data structures to efficiently conduct this search between the query embedding and all document embeddings across the full collection. For this, we employ an off-the-shelf library for large-scale vector-similarity search, namely faiss [15] from Facebook. In particular, at the end of offline indexing (§3.4), we maintain a mapping from each embedding to its document of origin and then index all document embeddings into faiss.</p>
</blockquote>
<p>The current implementation in the repo uses a more efficient indexing system, the PLAID index, as opposed to what is written here (indexing “all document embeddings into faiss” and “mantain a mapping from each embeddings to its document of origin”). Instead, the PLAID index uses residual compression with centroids, maintains an Inverted File (IVF) structure that maps centroids to passage IDs, and stores embeddings as compressed residuals relative to centroids.</p>
<p>There’s a lot to unpack in the following section:</p>
<blockquote class="blockquote">
<p>Subsequently, when serving queries, we use a two-stage procedure to retrieve the top-k documents from the entire collection. Both stages rely on ColBERT’s scoring: the first is an approximate stage aimed at filtering while the second is a refinement stage. For the first stage, we concurrently issue Nq vector-similarity queries (corresponding to each of the embeddings in Eq ) onto our faiss index. This retrieves the top-k’ (e.g., k’ = k/2) matches for that vector over all document embeddings. We map each of those to its document of origin, producing Nq × k’ document IDs, only K ≤ Nq × k’ of which are unique. These K documents likely contain one or more embeddings that are highly similar to the query embeddings. For the second stage, we refine this set by exhaustively re-ranking only those K documents in the usual manner described in §3.5. In our faiss-based implementation, we use an IVFPQ index (“inverted file with product quantization”). This index partitions the embedding space into P (e.g., P = 1000) cells based on k-means clustering and then assigns each document embedding to its nearest cell based on the selected vector-similarity metric. For serving queries, when searching for the top-k’ matches for a single query embedding, only the nearest p (e.g., p = 10) partitions are searched. To improve memory efficiency, every embedding is divided into s (e.g., s = 16) sub-vectors, each represented using one byte. Moreover, the index conducts the similarity computations in this compressed domain, leading to cheaper computations and thus faster search.</p>
</blockquote>
<ul>
<li>There are NQ query token embeddings, and for each one, we find the top K’ document IDs.</li>
<li>NQ x k’ (say 32 x 500) documents will include some duplicates, Meaning that some documents will contain document token embeddings that are close to more than one query token. Removing those duplicates will give us a K number of documents, which is less than the number of query embeddings x k’.</li>
<li>This first stage has greatly reduced the number of documents in consideration. You go from all documents in consideration, which could be tens of millions, down to just NQ x k’ or fewer documents in consideration. This K documents are then re-ranked according to the MaxSim computation across query token embeddings that we saw earlier.</li>
<li>The late interaction architecture allows for pruning, as exhibited by the IVFPQ index. That index starts by partitioning the embedding space into some number of clusters, where that number of clusters is much much less than the number of token embeddings. It assigns each document token embedding to its nearest cluster based on whatever similarity metric is being used. This reduces the number of potential candidates that are close to a query token. When we search for a document similar to a given query, you only search the nearest p partitions, and p is small, say 10. And so think about this: you started out with maybe tens of millions of documents, you’ve narrowed that down to a thousand clusters of documents, and then you’re now narrowing that even further down to only 10 clusters that are considered for a single query token. That we can index these documents offline before the interaction takes place and organize them into clusters is what allows this pruning to take place.</li>
<li>The last thing I’ll say about this section is that the compression that they’re explaining here, where they divide every embedding into sub-vectors each represented using one byte, is now replaced with the PLAID compression where they quantize the residual embeddings into n bits.</li>
</ul>
<p>So with that, we have pretty much covered all of the conceptual foundations of ColBERT. To understand the impact of those foundations, we’ll now look at the experimental evaluation section from the paper.</p>
</section>
<section id="experimental-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="experimental-evaluation">Experimental Evaluation</h2>
<p>And we’ll explore the four research questions that the ColBERT authors have put forth in this section.</p>
<ul>
<li>RQ1: In a typical re-ranking setup, how well can ColBERT bridge the existing gap between highly-efficient and highly-effective neural models?</li>
<li>RQ2: Beyond re-ranking, can ColBERT effectively support end-to-end retrieval directly from a large collection?</li>
<li>RQ3: What does each component of ColBERT (e.g., late interaction, query augmentation) contribute to its quality?</li>
<li>RQ4: What are ColBERT’s indexing-related costs in terms of offline computation and memory overhead?</li>
</ul>
<p>Some training details to prepare the ColBERT retriever: they fine-tune ColBERT models on the MS MARCO and TREC CAR datasets with a learning rate of 3e-6 and a batch size of 32. They fix the number of embeddings per query at 32, meaning that they have 32 tokens per query, and the embedding dimension is 128. The model is trained on a triple of query, positive document and negative document. ColBERT is used to produce a score for each document individually, and is optimized via pairwise softmax cross-entropy loss over the computed scores of the positive and negative document.</p>
<p>Here are two lines from the <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L74-L119C21"><code>train</code> function</a> where you can see the loss method, which is cross-entropy loss, and that the labels are just zeros because the document that is positive is first in the batch item (the zero-eth index):</p>
<div class="sourceCode" id="cb111" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1">...</span>
<span id="cb111-2"></span>
<span id="cb111-3">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(config.bsize, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">long</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>DEVICE)</span>
<span id="cb111-4"></span>
<span id="cb111-5">...</span>
<span id="cb111-6"></span>
<span id="cb111-7">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.CrossEntropyLoss()(scores, labels[:scores.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)])</span></code></pre></div>
<p>We’ll now dig into the results of their evaluation of the ColBERT architecture vs.&nbsp;existing methods to address the four research questions listed.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="8.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Table 1"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/8.png" class="img-fluid figure-img" alt="Table 1"></a></p>
<figcaption>Table 1</figcaption>
</figure>
</div>
<p>Here are the results for the first type of evaluation where Colbert and other architectures are used to re-rank The top 1000 results produced by BM25, which is full text search. There are three notable takeaways from this table:</p>
<ol type="1">
<li>ColBERT beats non-BERT-based models in terms of retrieval metric MRR@10 and is comparable to BERT models.</li>
<li>ColBERT is three orders of magnitude faster than the more performant BERT models and is comparable in latency to the non-BERT neural rankers.</li>
<li>Except for KNRM, ColBERT requires 11x to 48600x fewer FLOPs per query.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="9.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure 4"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/9.png" class="img-fluid figure-img" alt="Figure 4"></a></p>
<figcaption>Figure 4</figcaption>
</figure>
</div>
<p>They also compared ColBERT to BERT-base trained on retrieval. In this comparison, they increased the number of documents considered for re-ranking, calculated the FLOPs required to perform the re-ranking and then calculated the retrieval performance. The purple line at the top shows BERT-base, the green line at the bottom shows ColBERT. ColBERT for each value of k (number of documents reranked) requires fewer FLOPs and is comparable in retrieval performance. Most importantly, ColBERT scales much better than BERT-base as the number of document candidates considered increases from 10 to 2000. ColBERT stays within the same order of magnitude for FLOPs whereas BERT-base FLOPs increase by two orders of magnitude.</p>
<p>Next, we’ll look at their full retrieval results where they retrieve the top 1000 documents from the 8.8 million document MS Marco Corpus.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Table 2"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/10.png" class="img-fluid figure-img" alt="Table 2"></a></p>
<figcaption>Table 2</figcaption>
</figure>
</div>
<p>What immediately jumps off the page here is the MRR improvement that ColBERT provides, twice that of the Anserini BM25 method, which is an excellent baseline. However, ColBERT has about 5-8 times the latency of these other methods–that could be justifiable given the increased improvement. ColBERT (end-to-end) has the best Recall across all methods.</p>
<p>Next, we’ll look at some of the ablation studies that they performed, which to me are the most exciting results.</p>
</section>
<section id="ablation-studies" class="level2">
<h2 class="anchored" data-anchor-id="ablation-studies">Ablation Studies</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="11.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure 5"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/11.png" class="img-fluid figure-img" alt="Figure 5"></a></p>
<figcaption>Figure 5</figcaption>
</figure>
</div>
<p>This figure is really interesting, and there’s a lot to unpack here. I think it serves as a really comprehensive summary of the main architectural decisions that they’ve made in this work.</p>
<p>Models A through E are used in a re-ranking setting. The first comparison we’ll look at is between model A and model D. Model A is a BERT model, and it takes the CLS token embedding representation for query and document and performs an inner product between them to calculate similarity for re-ranking, which achieves an MRR@10 of about 0.26 which is 6 points fewer than a 5-layer ColBERT model. So, fine grained, token-wise embedding interaction with ColBERT is yielding better results than a single vector interaction with BERT. This is a confirmation of the fundamental concept behind late interaction.</p>
<p>The second comparison is between model B and model D. Model B is using average similarity, and model D is using the MaxSim operator. Model D again has about a 6 point increase in MRR. This validates the second fundamental concept behind late interaction: the MaxSim operator.</p>
<p>The third comparison is between Model C and Model D. In Model C, the query is <strong>not</strong> padded to 32 tokens with <code>[MASK]</code> tokens. Model D uses query augmentation (it pads to 32 with <code>[MASK]</code> tokens) and has a 2 point increase in MRR showing that these <code>[MASK]</code> tokens, which carry semantic meaning in embedding space, improve the model’s ability to find relevant documents given a query.</p>
<p>The final comparison is between Model E and Model F. Model E is ColBERT used as a re-ranker for the top 1000 documents retrieved by full-text search. Model F is ColBERT used for end-to-end retrieval using a vector similarity index to cluster documents before retrieval. Using ColBERT end-to-end gives another boost to performance.</p>
<p>Something to keep in mind is that BERT requires you to pass in the query and document embedding one pair at a time. For one query, you have to do a thousand forward passes if you have a thousand documents that you want to compare it to. Whereas ColBERT, because of late interaction, can utilize vector similarity indexes because documents are indexed offline, and the interaction calculation is much quicker because you are considering fewer candidate documents—the documents that are close to the query token embeddings via the clusters created by the indexing process.</p>
</section>
<section id="indexing-throughpout-footprint" class="level2">
<h2 class="anchored" data-anchor-id="indexing-throughpout-footprint">Indexing Throughpout &amp; Footprint</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="12.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure 6"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/12.png" class="img-fluid figure-img" alt="Figure 6"></a></p>
<figcaption>Figure 6</figcaption>
</figure>
</div>
<p>I’ll start by showing figure 6, where it shows that on top of basic ColBERT indexing, adding these optimizations increases the throughput, which means it increases the number of documents that are processed each minute. The two that I’ll highlight here is that length-based bucketing, which we saw in detail, and per-batch maximum sequence length, where they pad all items in the batch to the maximum document length, both improve the throughput.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="13.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Table 4"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/13.png" class="img-fluid figure-img" alt="Table 4"></a></p>
<figcaption>Table 4</figcaption>
</figure>
</div>
<p>This table shows the space footprint and MRR@10 for different settings, dimensions, and bytes per dimension. The most space-effcient setting, re-ranking with cosine similarity with 24-dimensional vectors stored as 2-byte floats, which takes up 27 GB, is only 1% worse in MRR@10 than the most space-consuming one which takes up 286 GB.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The body of the ColBERTv1 paper is only about 9 pages long, but it is incredibly information dense. What I thought would be a 1-day foray turned into a 6-day deep dive. I found it helpful to interleave twitter conversations with core concepts from the paper, as casual conversations are often more accessible, and equally impressive as formal work.</p>
<p>As a new canonical ColBERT maintainer I wanted to ground myself in the first principles of late interaction. There are three key elements involved:</p>
<ol type="1">
<li>Independent encoding of queries and documents.</li>
<li>Offline document indexing.</li>
<li>the MaxSim operation.</li>
</ol>
<p>Encoding queries and documents separately allows for offline document indexing, and delays the interaction to the end of the architecture. Offline indexing and MaxSim both unlock pruning in their own ways. Vector-similarity indexes, through clustering, eliminate low-relevance documents from consideration before the interaction takes place. MaxSim eliminates low-relevance tokens during the interaction.</p>
<p>MaxSim is further enhanced by query augmentation, as meaningful <code>[MASK]</code> tokens are introduced in the query to improve the chance of matching relevant terms in the document.</p>
<p>That the most space-efficient setting is only 1% less performant than the most space-consuming setting foreshadows the compression opportunities realized in the PLAID paper.</p>
<p>With these foundations reinforced, I’ll revisit the ColBERTv2 and PLAID papers next, and will continue to concretely witness the concepts at play in the repo’s codebase.</p>


</section>

 ]]></description>
  <category>ColBERT</category>
  <category>information retrieval</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/</guid>
  <pubDate>Wed, 16 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Debugging Flash Attention in LLM-Foundry (and a 20% Slow Down!)</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-06-30-debug-fa2-llm-foundry/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I’m learning a lot about LLM-Foundry while working on a group research project. In this blog post I’ll walk through how we figured out two things:</p>
<ol type="1">
<li>LLM-Foundry, by default when using a HuggingFace LlamaModel, does not use <code>flash_attn_varlen_func</code> and instead uses <code>flash_attn_func</code>. In other words, it doesn’t unpad the batch by default.</li>
<li>When forcing LLM-Foundry to use <code>flash_attn_varlen_func</code>, it slows down training time.</li>
</ol>
<p>I’ll start by walking through the forward pass of the HuggingFace <code>LlamaModel</code> down to the attention mechanism which uses the <code>_flash_attention_forward</code> utility function which decides which Flash Attention interface is being used.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/BWO5guW7Kl4?si=l30jswKSmduhzd2g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
</section>
<section id="what-is-the-value-of-the-attention_mask" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-value-of-the-attention_mask">What is the value of the <code>attention_mask</code>?</h2>
<p>The model we’re using is SmolLM2-135M which uses the now-deprecated <a href="https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src/transformers/models/llama/modeling_llama.py#L324"><code>LlamaFlashAttention2</code> module</a>.</p>
<p>Inspecting the <a href="https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src/transformers/models/llama/modeling_llama.py#L828"><code>LlamaModel</code> forward pass</a>, the first instance of where the <code>attention_mask</code> is used:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">causal_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._update_causal_mask(</span>
<span id="cb1-2">    attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions</span>
<span id="cb1-3">)</span></code></pre></div>
<p>Looking at <code>_update_causal_mask</code>:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config._attn_implementation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"flash_attention_2"</span>:</span>
<span id="cb2-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> attention_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> attention_mask:</span>
<span id="cb2-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> attention_mask</span>
<span id="cb2-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span></code></pre></div>
<p>If <code>0.0 in attention_mask</code> then the <code>attention_mask</code> will be returned, other <code>None</code> is returned.</p>
<p>We’ll come back to this point later on.</p>
</section>
<section id="when-is-flash_attn_varlen_func-called" class="level2">
<h2 class="anchored" data-anchor-id="when-is-flash_attn_varlen_func-called">When is <code>flash_attn_varlen_func</code> called?</h2>
<p>Let’s assume <code>0.0 in attention_mask</code> is <code>True</code>, so the <code>attention_mask</code> is kept as is and is passed onto the <code>LlamaDecoderLayer</code> and eventually the attention mechanism <a href="https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src/transformers/models/llama/modeling_llama.py#L414">which calls <code>_flash_attention_forward</code></a>. <code>flash_attention_forward</code> is <a href="https://github.com/huggingface/transformers/blob/ea0ea392e57f8816f9ab8e5f740577a0343a1594/src/transformers/modeling_flash_attention_utils.py#L409">defined in transformers/modeling_flash_utils.py</a>, and triggers the use of <code>flash_attn_varlen_func</code> if one of two conditions are true:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> attention_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span></code></pre></div>
<p>or</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> (</span>
<span id="cb4-2">    position_ids <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb4-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> query_states.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb4-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> (max_length_q <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> (query_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> (torch.diff(position_ids, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span>()))</span>
<span id="cb4-5">)</span></code></pre></div>
<p>The <code>elif</code> condition is <code>True</code> if <code>position_ids is not None</code> and <code>query_states.shape[0] == 1</code> and either <code>max_length_q is not None</code> or <code>(query_length != 1 and not (torch.diff(position_ids, dim=-1) &gt;= 0).all())</code>. The <code>torch.diff</code> expression is <code>False</code> if the difference in consecutive values in <code>position_ids</code> are not greater than <code>0</code>. For example, the following <code>position_ids</code> would yield <code>False</code>:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span></code></pre></div>
<p><code>torch.diff</code> for the 4th and 5th position (<code>3</code> to <code>0</code>) is <code>-3</code>. We would expect such a <code>position_ids</code> sequence when you have packed sequences.</p>
</section>
<section id="how-do-we-check-the-value-of-attention_mask-during-training" class="level2">
<h2 class="anchored" data-anchor-id="how-do-we-check-the-value-of-attention_mask-during-training">How do we check the value of <code>attention_mask</code> during training?</h2>
<p>To do so, I wrote the following Composer callback:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> FlashAttentionDebug(Callback):</span>
<span id="cb6-2">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> before_forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, state: State, logger: Logger) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-3">      model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> state.model</span>
<span id="cb6-4">      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.config._attn_implementation)</span>
<span id="cb6-5">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb6-6"></span>
<span id="cb6-7">      <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> create_hook_fn(name):</span>
<span id="cb6-8">          <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> hook_fn(module, args, kwargs, output):</span>
<span id="cb6-9">              <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kwargs:</span>
<span id="cb6-10">                  <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> FlashAttentionDebug: attention_mask is None:"</span>, kwargs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>] <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb6-11">                  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kwargs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>] <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-12">                      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> FlashAttentionDebug: attention_mask:"</span>, kwargs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>])</span>
<span id="cb6-13">                      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> FlashAttentionDebug: 0.0 in attention_mask:"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kwargs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>])</span>
<span id="cb6-14">                      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> FlashAttentionDebug: attention_mask.shape:"</span>, kwargs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>].shape)</span>
<span id="cb6-15">                      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> FlashAttentionDebug: attention_mask.sum():"</span>, kwargs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"attention_mask"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>())</span>
<span id="cb6-16">          <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> hook_fn</span>
<span id="cb6-17"></span>
<span id="cb6-18">      attn_layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.model.base_model.model.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].self_attn</span>
<span id="cb6-19">      hook_handle <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attn_layer.register_forward_hook(create_hook_fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"attn_layer"</span>), with_kwargs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb6-20">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hooks.append(hook_handle)</span>
<span id="cb6-21"></span>
<span id="cb6-22">      decoder_layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.model.base_model.model.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb6-23">      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(decoder_layer))</span>
<span id="cb6-24">      hook_handle <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoder_layer.register_forward_hook(create_hook_fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"decoder_layer"</span>), with_kwargs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb6-25">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hooks.append(hook_handle)</span>
<span id="cb6-26"></span>
<span id="cb6-27">      _model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.model.base_model.model.model</span>
<span id="cb6-28">      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(_model))</span>
<span id="cb6-29">      hook_handle <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _model.register_forward_hook(create_hook_fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"model"</span>), with_kwargs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb6-30">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hooks.append(hook_handle)</span>
<span id="cb6-31"></span>
<span id="cb6-32">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> after_forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, state: State, logger: Logger) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-33">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> hook <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hooks:</span>
<span id="cb6-34">          hook.remove()</span>
<span id="cb6-35">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span></code></pre></div>
<p><code>create_hook_fn</code> is a closure which returns <code>hook_fn</code>. I used this pattern so I could log the name of the module the hook is attached to. Note that when using <code>register_forward_hook</code> you must specify <code>with_kwargs=True</code> to pass kwargs to the hook function.</p>
<p>Here are the outputs when using the default LLM-Foundry pretraining setup:</p>
<pre><code>flash_attention_2
&lt;class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'&gt;
&lt;class 'transformers.models.llama.modeling_llama.LlamaModel'&gt;
attn_layer FlashAttentionDebug: attention_mask is None: True
decoder_layer FlashAttentionDebug: attention_mask is None: True
model FlashAttentionDebug: attention_mask is None: False
model FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
model FlashAttentionDebug: 0.0 in attention_mask: False
model FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])
model FlashAttentionDebug: attention_mask.sum(): tensor(2048, device='cuda:0')</code></pre>
<p>Note that in the attention layer, <code>attention_mask</code> is <code>None</code> because as we can see in the <code>model</code> forward output, <code>0.0</code> is not in <code>attention_mask</code> (it’s full of <code>1</code>s).</p>
</section>
<section id="how-do-we-create-an-attention_mask-with-0.0s" class="level2">
<h2 class="anchored" data-anchor-id="how-do-we-create-an-attention_mask-with-0.0s">How do we create an <code>attention_mask</code> with <code>0.0</code>s?</h2>
<p>With the help of Cursor (my first time using it!) I was able to add one simple line to the <code>__call__</code> method of the default pretraining collator <code>ConcatenatedSequenceCollatorWrapper</code>:</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">batch[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (batch[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">long</span>()</span></code></pre></div>
<p>Where <code>input_ids</code> are not <code>0</code> (the EOS token id used for padding) <code>attention_mask</code> will be <code>1</code>; it will be <code>0</code> where there are padding tokens.</p>
<p>Since I’m using Modal for training, and since the image brings down our LLM-Foundry fork, and since I need to modify the <code>ConcatenatedSequenceCollatorWrapper.__call__</code> method (which lives in <code>llmfoundry/data/text_data.py</code>) I add the following line after my Modal is built:</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image.add_local_file(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text_data.py"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/llm-foundry/llmfoundry/data/text_data.py"</span>)</span></code></pre></div>
<p>Running training with this modified collator the <code>FlashAttentionDebug</code> callback logs the following:</p>
<pre><code>flash_attention_2
&lt;class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'&gt;
&lt;class 'transformers.models.llama.modeling_llama.LlamaModel'&gt;
attn_layer FlashAttentionDebug: attention_mask is None: False
attn_layer FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
attn_layer FlashAttentionDebug: 0.0 in attention_mask: True
attn_layer FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])
attn_layer FlashAttentionDebug: attention_mask.sum(): tensor(201, device='cuda:0')
decoder_layer FlashAttentionDebug: attention_mask is None: False
decoder_layer FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
decoder_layer FlashAttentionDebug: 0.0 in attention_mask: True
decoder_layer FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])
decoder_layer FlashAttentionDebug: attention_mask.sum(): tensor(201, device='cuda:0')
model FlashAttentionDebug: attention_mask is None: False
model FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
model FlashAttentionDebug: 0.0 in attention_mask: True
model FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])
model FlashAttentionDebug: attention_mask.sum(): tensor(201, device='cuda:0')</code></pre>
<p>Now we can see that in the attention layer, the <code>attention_mask</code> is not <code>None</code>. It contains <code>0.0</code> values (note how the <code>sum</code>, 201, is less than the sequence length of 2048) which is why the <code>_update_causal_mask</code> method returned attention_mask as is. We can also visually inspect the <code>attention_mask</code> tensor in the model, decoder layer and attention mechanism forward pass and see both <code>1</code>s and <code>0</code>s.</p>
</section>
<section id="how-do-we-know-if-flash_attn_varlen_func-is-being-used" class="level2">
<h2 class="anchored" data-anchor-id="how-do-we-know-if-flash_attn_varlen_func-is-being-used">How do we know if <code>flash_attn_varlen_func</code> is being used?</h2>
<p>Now that we know that introducing <code>0</code>s in the <code>attention_mask</code> allows it to be passed through the model, including the attention mechanism, we should confirm that <code>flash_attn_varlen_func</code> is called. If you recall, one of the conditions for it being called was that <code>attention_mask is not None</code>. To check this, we can monkey-patch <code>_upad_input</code> which is the method called to unpad the batch if <code>attention_mask is not None</code>:</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> transformers.modeling_flash_attention_utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> flash_utils</span>
<span id="cb11-2">original_upad_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> flash_utils._upad_input</span>
<span id="cb11-3">original_prepare_fa2_from_position_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> flash_utils.prepare_fa2_from_position_ids</span>
<span id="cb11-4"></span>
<span id="cb11-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> debug_upad_input(query_states, key_states, value_states, attention_mask, query_length):</span>
<span id="cb11-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"DEBUG: Using _upad_input"</span>)</span>
<span id="cb11-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  query_states: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>query_states<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  key_states: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>key_states<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  value_states: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>value_states<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-10">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  attention_mask: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>attention_mask<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  query_length: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>query_length<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-12">    query_layer, key_layer, value_layer, indices_q, (cu_seqlens_q, cu_seqlens_k),(max_seqlen_in_batch_q, max_seqlen_in_batch_k) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> original_upad_input(query_states, key_states, value_states, attention_mask, query_length)</span>
<span id="cb11-13">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"query_layer.shape: "</span>, query_layer.shape)</span>
<span id="cb11-14">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"key_layer.shape: "</span>, key_layer.shape)</span>
<span id="cb11-15">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"value_layer.shape: "</span>, value_layer.shape)</span>
<span id="cb11-16">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"indices_q.shape: "</span>, indices_q.shape)</span>
<span id="cb11-17">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"cu_seqlens_q.shape: "</span>, cu_seqlens_q.shape)</span>
<span id="cb11-18">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"cu_seqlens_q: "</span>, cu_seqlens_q.tolist())</span>
<span id="cb11-19">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"cu_seqlens_k.shape: "</span>, cu_seqlens_k.shape)</span>
<span id="cb11-20">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"cu_seqlens_k: "</span>, cu_seqlens_k.tolist())</span>
<span id="cb11-21">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"max_seqlen_in_batch_q: "</span>, max_seqlen_in_batch_q)</span>
<span id="cb11-22">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"max_seqlen_in_batch_k: "</span>, max_seqlen_in_batch_k)</span>
<span id="cb11-23">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"indices_q: "</span>, indices_q.tolist())</span>
<span id="cb11-24">    seqlens_in_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention_mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.int32)</span>
<span id="cb11-25">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(seqlens_in_batch.tolist())</span>
<span id="cb11-26">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(attention_mask[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].tolist())</span>
<span id="cb11-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> original_upad_input(query_states, key_states, value_states, attention_mask, query_length)</span>
<span id="cb11-28"></span>
<span id="cb11-29">flash_utils._upad_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> debug_upad_input</span></code></pre></div>
<p>I added a <code>original_upad_input</code> call and stored the output so I could see what gets passed on to <code>flash_attn_varlen_func</code>.</p>
<p>During the training run, with our modified collator, we see the following output (this was for a run with a batch size of 6):</p>
<pre><code>DEBUG: Using _upad_input
  query_states: torch.Size([6, 2048, 9, 64])
  key_states: torch.Size([6, 2048, 3, 64])
  value_states: torch.Size([6, 2048, 3, 64])
  attention_mask: torch.Size([6, 2048])
  query_length: 2048
query_layer.shape:  torch.Size([1395, 9, 64])
key_layer.shape:  torch.Size([1395, 3, 64])
value_layer.shape:  torch.Size([1395, 3, 64])
indices_q.shape:  torch.Size([1395])
cu_seqlens_q.shape:  torch.Size([7])
cu_seqlens_q:  [0, 220, 437, 732, 915, 1045, 1395]
cu_seqlens_k.shape:  torch.Size([7])
cu_seqlens_k:  [0, 220, 437, 732, 915, 1045, 1395]
max_seqlen_in_batch_q:  350
max_seqlen_in_batch_k:  350
</code></pre>
<p>Some key observations:</p>
<p><code>query_states</code> has size 6 (batch size) x 2048 (sequence length) x 9 (num heads) x 64 (head dim).</p>
<p><code>query_layer</code> (one of the <code>_upad_input</code> outputs and <code>flash_attn_varlen_func</code> inputs) has size 1395 (total sequence length) x 9 (num heads) x 64 (head dim).</p>
<p>The <code>cu_seqlens_q</code> that is a critical input to <code>flash_attn_varlen_func</code> show us that there are 6 sequences packed together and the “boundaries” of the sequences are <code>[0, 220, 437, 732, 915, 1045, 1395]</code>. Using my <a href="https://vishalbakshi.github.io/blog/posts/2025-05-13-DataInspector-BinPackCollator/index.html#datainspector"><code>DataInspector</code> callback</a> I confirmed the number of non-padding tokens in the batch: 220, 217, 295, 183, 130, 350. The sum of these counts is 1395, the total sequence length passed to <code>flash_attn_varlen_func</code>.</p>
</section>
<section id="wait-flash_attn_varlen_func-slows-down-training" class="level2">
<h2 class="anchored" data-anchor-id="wait-flash_attn_varlen_func-slows-down-training">Wait, <code>flash_attn_varlen_func</code> slows down training?</h2>
<p>When using my modified collator, and therefore utilizing <code>flash_attn_varlen_func</code> the training time slows down by <em>over 20%</em>. This was certainly a surprise for me! After discussing this with our research advisor, we learned that this is likely because the HuggingFace implementation of the model unpads and re-pads the batch for each layer.</p>
<p>We can see this <a href="https://github.com/huggingface/transformers/blob/03db2700abf84971351c7374a548a9d4fc156916/src/transformers/modeling_flash_attention_utils.py#L532">in the <code>_flash_attention_forward</code> method, after <code>flash_attn_varlen_func</code> is called</a>:</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">attn_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _pad_input(attn_output_unpad, indices_q, batch_size, query_length)</span></code></pre></div>
<p>The solution to mitigating this slow-down is to implement our own custom model where it unpads the batch only once. This will be our next task!</p>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>The biggest takeaway from this experience, as has been the case for all practical training experiments I’ve run (whether for LMs or vision models) is that there’s a difference between what is theoretically efficient and whether that is practically efficient. In theory, <code>flash_attn_varlen_func</code> should be faster because you are not wasting the quadratic attention compute on padding tokens. In practice, unpadding and re-padding the batch for each layer for each forward pass adds an overhead which not only cancels out that attention computation speedup, but slows down the training compared to a fully-padded forward pass. This is a critical lesson I experience again and again, and it helps me understand the value of choosing the right implementation to actualize theoretical efficiencies.</p>
<p>I’m growing my YouTube channel this year, so if you like this type of content <a href="https://www.youtube.com/@vishal_learner">please subscribe</a>!</p>


</section>

 ]]></description>
  <category>python</category>
  <category>deep learning</category>
  <category>LLM-Foundry</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-06-30-debug-fa2-llm-foundry/</guid>
  <pubDate>Mon, 30 Jun 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Introducing portfolio-llm: A Professional Portfolio You Can Chat With</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-06-26-portfolio-llm/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>For my latest project, I wanted to solve a problem I was facing: I have published a large volume of machine learning blog posts and videos, and I was not sure how to unify all of my work.</p>
<p>I’ve also been interested in the <a href="https://llmstxt.org/">llms.txt standard created by Jeremy Howard</a>, so I had an idea: what if I wrote an llms_ctx.txt for my own professional portfolio to make it interactive and queryable?</p>
<p>In this post, I’m going to share the result of that experiment: a professional portfolio you can chat with. I’ll walk through the entire process, from concept and design to the rigorous evaluation framework I built to test the system. I’ll also share my thoughts on how this approach changes the job search paradigm.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Qhax3JerFP0?si=BNFnHmw6cNfW5SlN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
</section>
<section id="the-goal-an-interactive-conversation" class="level2">
<h2 class="anchored" data-anchor-id="the-goal-an-interactive-conversation">The Goal: An Interactive Conversation</h2>
<p>You can see a demo of my llms_ctx.txt on Claude.ai in the video embedded above. Here are a couple screenshot examples:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Claude.ai chat screenshot"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-26-portfolio-llm/1.png" class="img-fluid figure-img" alt="Claude.ai chat screenshot"></a></p>
<figcaption>Claude.ai chat screenshot</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="A not very concise response"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-26-portfolio-llm/2.png" class="img-fluid figure-img" alt="A not very concise response"></a></p>
<figcaption>A not very concise response</figcaption>
</figure>
</div>
</section>
<section id="how-it-works-the-llms_ctx.txt-file" class="level2">
<h2 class="anchored" data-anchor-id="how-it-works-the-llms_ctx.txt-file">How It Works: The <code>llms_ctx.txt</code> File</h2>
<p>The <code>llms_ctx.txt</code> file contains 5 sections (following the llms.txt standard):</p>
<ul>
<li>A header: <code># Vishal Bakshi: Building Reliable Systems</code></li>
<li>A blockquote with a high-level summary</li>
</ul>
<blockquote class="blockquote">
<p>Vishal Bakshi has a background in engineering, data analytics, and education, now focusing on Applied AI and ML research. He specializes in building reliable systems, with an emphasis on resource-constrained research, deep evaluation, and a first-principles understanding of machine learning.</p>
</blockquote>
<ul>
<li>A <code>## High-Level Overview &amp; FAQ</code> section with QA pairs.</li>
<li>A <code>## Portfolio Deep Dive Q&amp;A</code> section with QA pairs.</li>
<li>A <code>## Out-of-Scope Questions</code> section with QA pairs.</li>
</ul>
<p>The creation of my <code>llms_ctx.txt</code> file was an iterative process heavily collaborated with Gemini 2.5 Pro.</p>
<p>I first create two txt files: one containing metadata for my videos, and one containing metadata for my blog posts. I chose to keep the metadata simple: the existing title and description. A potential improvement on this could be producing an AI-generated summary of each content piece and adding that as a third field.</p>
<p>I provided Gemini these two files and asked it to generate 5-8 themes across my body of work, and citing which blog posts and videos best represented each theme. It provided by 8 themes. I then iterated on this list manually, adding and removing content pieces and adjusting theme names as needed. You can view the full list of themes and corresponding content pieces <a href="https://github.com/vishalbakshi/portfolio-llm?tab=readme-ov-file#for-a-deeper-dive">on the GitHub repo for this project</a>.</p>
<p>I then asked Gemini to produce three sets of questions:</p>
<ul>
<li>High-Level Overview &amp; FAQ</li>
<li>Portfolio Deep Dive Q&amp;A</li>
<li>Out-of-Scope Questions</li>
</ul>
<p>I then wrote a first draft set of answers for each of the 40 or so questions Gemini created across these three categories. I iterated on my answers with Gemini to make them more readable and effective.</p>
</section>
<section id="evaluation" class="level2">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<p>I wanted to test for three users:</p>
<ul>
<li>First interaction (has not read my list of themes, blog posts or wactched any of my vidoes)</li>
<li>Portfolio deep dive (has read my list of themes, and a few blog posts/videos)</li>
<li>Out of scope (a user who asks unrelated or unspecified questions in my llms_ctx.txt)</li>
</ul>
<p>I wanted to test two types of questions:</p>
<ul>
<li>Unit Test questions (verbatim to the questions in the llms_ctx.txt, tests if the LLM can retrieve answers explicitly listed)</li>
<li>Paraphrased/Follow-Up questions (questions that are similar in meaning but different in phrasing than the questions listed in llms_ctx.txt, or questions that cover multiple QA pairs)</li>
</ul>
<p>For each question, across all users and question types, I created <a href="https://github.com/vishalbakshi/portfolio-llm/blob/main/evals.csv">an evaluation spreadsheet</a> with the following columns:</p>
<ul>
<li>Model</li>
<li>Prompt Type (Unit Test, Paraphrased)</li>
<li>User</li>
<li>Prompt (the question)</li>
<li>Response</li>
<li>Gold Answer (either verbatim from llms_ctx.txt or a bulleted list of content the answer should cover)</li>
<li>Completeness (Full, Partial, None)</li>
<li>Accuracy (Perfect, Minor Error, Major Error)</li>
<li>Conciseness (Concise, Verbose)</li>
<li>Hallucination (None, Speculation, Factual Fabrication)</li>
<li>Format Adherence (Followed, Ignored)</li>
</ul>
<p>I generate an <a href="https://github.com/vishalbakshi/portfolio-llm/blob/main/evals.txt">XML-style version of my evals</a> with <a href="https://github.com/vishalbakshi/portfolio-llm/blob/main/Evals%20XML%20Generation.ipynb">a notebook</a> so I could feed it to an LLM for feedback.</p>
<p>For my first round of evaluation I used only two metrics: Fidelity (High, Medium, Low) and Hallucinations (High, Medium, Low). Upon conversing with Gemini, I learned that I was conflating verbosity with Hallucinations and should decompose Fidelity into Completeness and Accuracy for more clarity in evaluation.</p>
<p>My evaluation results were almost perfect (118/120 “Full” Completeness, 120/120 “Perfect” Accuracy and 120/120 “None” Hallucinations). I’m always wary of such high evaluation results:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
If your eval system says you are achieving 100% w/AI accuracy then your product is actually deeply broken, or you are tracking an irrelevant set of metrics
</p>
— Hamel Husain (<span class="citation" data-cites="HamelHusain">@HamelHusain</span>) <a href="https://twitter.com/HamelHusain/status/1843066772003733783?ref_src=twsrc%5Etfw">October 6, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>However, Claude Sonnet 4 (the model I used via Claude.ai) is highly capable and I provided it well-structured (question-answer pairs) context so it’s a relatively easy Q&amp;A task. I’m essentially providing it 120 few-shot examples of QA pairs which it can also use as a knowledge base.</p>
</section>
<section id="what-i-learned-in-the-process" class="level2">
<h2 class="anchored" data-anchor-id="what-i-learned-in-the-process">What I Learned in the Process</h2>
<ul>
<li>I realized that the style of the response is also dictated by the user prompt which is not in my control. So I’m focused more on the content delivered. It’s up to the user of Claude/Gemini/ChatGPT/etc. to dictate the style (verbose, explanatory, etc.)</li>
<li>I noticed that Claude doesn’t always include the URLs. Not sure how to improve that.</li>
<li>Realizing the benefit of having a chat ui interface as it would allow me to better control the model’s responses by injecting additional instructions or analyzing the response before it’s sent to the reader.</li>
<li>I realized long-term patterns in my work that I don’t see when I’m in the weeds. For example, for fastbook-benchmark, AgentFastbook, and TinyScaleLab I have focused on evals first while I develop other foundational skills.</li>
<li>Evaluating the model responses helped me identify gaps in my answers that I iterate upon.</li>
</ul>
</section>
<section id="what-this-means-for-job-seekers-and-employers" class="level2">
<h2 class="anchored" data-anchor-id="what-this-means-for-job-seekers-and-employers">What this Means for Job Seekers and Employers</h2>
<p>I might just be uninformed, but I think this is a novel approach to sharing a portfolio. I think this llms_ctx.txt approach could benefit both the job seeker and the employer. The job seeker benefits from the creation of the llms_ctx.txt file as they are forced to deeply think about common themes in their work and answer both broad and targeted questions about their projects and experience. The job seeker can also get a “sanity check” on what roles they are well suited for by posing as a hiring manager in a conversation with an LLM they have provided this context to. I also think that this portfolio style could benefit the hiring manager. The hiring manager can have a deep conversation with the LLM to evaluate the candidate across different desired skills/job requirements/experience requirements, especially those that are not explicitly mentioned but need to be inferred from th candidate’s body of work.</p>
<p>There’s also an opportunity to operationalize and systematize my end-to-end process for generating the llms_ctx.txt. You could imagine a UI which guides the user across the pipeline that I followed:</p>
<ul>
<li>Construct metadata around their existing work (blogs, videos, repos, resume, CV)</li>
<li>Generate and iterate on themes underpinning their work.</li>
<li>Generate and iterate on questions and answers related to those themes.</li>
<li>Construct the llms_txt.ctx</li>
<li>Evaluate model responses across 3 dimensions (Completeness, Accuracy, Hallucinations).</li>
</ul>
<p>I believe this end-to-end process could be operationalized into a valuable app, and I’m open-sourcing the methodology in the hopes that others will build upon it.</p>
</section>
<section id="try-it-yourself" class="level2">
<h2 class="anchored" data-anchor-id="try-it-yourself">Try It Yourself</h2>
<p>I’ll end with a call to action: try this for your own work/experience and let me know how it goes! I’m happy to be a test user for your llms_ctx.txt. Just ping me <a href="https://x.com/vishal_learner">on Twitter</a>.</p>
<p>I hope this project unlocks a new universe of opportunities in the job search space. I also hope that this approach allows folks from unconventional or non-traditional backgrounds (like myself) who rely on self-taught and boostrapped methods of experience (taking courses, writing blog posts, publishing YouTube videos) to synthesize their work in a cohesive way. I will continue to iterate on my llms_ctx.txt file, and have conversations with an LLM using it at each step of my professional journey, improving both the file and my professional development along the way.</p>


</section>

 ]]></description>
  <category>Career</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-06-26-portfolio-llm/</guid>
  <pubDate>Thu, 26 Jun 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Takeaways from Gemini Deep Research Report on Small Batch Training Challenges</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-06-18-small-batch-report/</link>
  <description><![CDATA[ 




<div id="cell-1" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tensor</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CrossEntropyLoss</span></code></pre></div>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I’ve recently been training models on the fastai <a href="https://github.com/fastai/imagenette?tab=readme-ov-file">Imagenette dataset</a> to gain some intuition on what improves downstream performance (accuracy). <a href="https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/">My latest experiment</a> was to train three models on different batch sizes and learning rates (for a fixed 5 epochs) to understand the relationship between the two.</p>
<p>I posted my initial musing (after I had analyzed results for batch sizes 32 to 2048) that lowering the batch size might continue to yield higher accuracy. Jeremy validated this approach:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
That’s a great question! Stable low bs training isn’t easy, but it’s a good plan :)
</p>
— Jeremy Howard (<span class="citation" data-cites="jeremyphoward">@jeremyphoward</span>) <a href="https://twitter.com/jeremyphoward/status/1930486029607416069?ref_src=twsrc%5Etfw">June 5, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I also prompted Allen AI’s <a href="https://paperfinder.allen.ai/chat/606ffc67-5539-47a9-8c8a-ec5820666ccc?profile=paper-finder-only">Paper Finder</a> with:</p>
<blockquote class="blockquote">
<p>strategies to make low batch size training more stable and improve accuracy for image recognition</p>
</blockquote>
<p>Imagenette is unlocking an entire new lane of research and experimentation for me, helping me towards my <a href="https://vishalbakshi.github.io/blog/posts/2025-04-26-TinyScale-Lab-Kickoff/">TinyScaleLab project goal</a> of training and analyzing high performant tiny language models.</p>
<p>In this blog post, I’ll walk through the Gemini Deep Research and Ai2 Paper Finder findings. The full Deep Research report is in <a href="https://docs.google.com/document/d/1AWoW4sOQ_iR_3pxsTRzYOT0cql60ux0Sjrt4rbLilZM/edit?usp=sharing">this Google Doc</a>.</p>
</section>
<section id="geminis-deep-research-report" class="level2">
<h2 class="anchored" data-anchor-id="geminis-deep-research-report">Gemini’s Deep Research Report</h2>
<p>Here was my initial prompt:</p>
<blockquote class="blockquote">
<p>I recently have been training xresnet18, xresnet34 and xse_resnext50 on the Imagenette dataset by fast.ai (which is a 10k subset of ImageNet with 10 easily classified classes). I have trained on batch sizes from 1 to 2048. Generally speaking, the highest accuracy achieved increases from a batch size of 1 to a batch size of 8, 16 or 32 (depending on the model/learning rate) and then decreases as batch size increases to 2048. This makes me want to explore small batch size training for this project. Jeremy Howard tweeted that “Stable low bs training isn’t easy, but it’s a good plan”. I want you to help me answer two questions in this chat:</p>
<ol type="1">
<li><p>Why is stable low batch size training difficult?</p></li>
<li><p>What techniques are there (either in literature/arxiv, blog posts or forums) to make low batch size training stable? I’m most interested in improving the accuracy of low batch size trainings.</p></li>
</ol>
</blockquote>
<p>I’ll paste the entire executive summary it generated, as its a good one (emphasis mine):</p>
<blockquote class="blockquote">
<p>Training deep learning models with small batch sizes presents a unique set of challenges, primarily stemming from high gradient variance during optimization and the inherent limitations of standard Batch Normalization with few samples. However, this training regime also offers the potential for <mark>improved model generalization</mark>. This report investigates the difficulties associated with stable low batch size training and explores techniques to mitigate these issues, with a particular focus on enhancing model accuracy. For architectures like xresnet, overcoming the instability of Batch Normalization with small batches is a critical first step, often addressed by substituting it with alternatives like <mark>Group Normalization</mark>. Stability and accuracy can be further improved through <mark>careful management of learning rates</mark>, including the use of adaptive schedulers and awareness of optimizer-specific phenomena such as the “surge” in optimal learning rates for Adam-family optimizers. <mark>The choice of optimizer itself</mark>, typically between SGD with momentum and adaptive methods like AdamW, also plays a significant role, alongside appropriate <mark>regularization strategies tailored to the small-batch context</mark>.</p>
</blockquote>
<p>The report starts by outlining why small batch sizes lead to unstable training. Gradient updates using a small batch size “may not accurately reflect the gradient of the true loss function that would be computed over the entire dataset” and “is an estimate derived from a very limited subset of the training data”.</p>
</section>
<section id="small-batch-sizes-the-double-edged-sword" class="level2">
<h2 class="anchored" data-anchor-id="small-batch-sizes-the-double-edged-sword">Small Batch Sizes: The Double Edged Sword</h2>
<p>On one hand, small batch size for a fixed number epochs provide more gradient updates and thus lower the loss. In the plot below (xresnet18) the ideal batch size (16-32) performs better than larger batch sizes. On the other hand, the larger number of updates will take longer and the GPU will be less utilized.</p>
<p><strong>xresnet18</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="lr_acc_18.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="xresnet18 batch size vs accuracy for different learning rates"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-small-batch-report/lr_acc_18.png" class="img-fluid figure-img" alt="xresnet18 batch size vs accuracy for different learning rates"></a></p>
<figcaption>xresnet18 batch size vs accuracy for different learning rates</figcaption>
</figure>
</div>
<blockquote class="twitter-tweet blockquote" data-dnt="true" align="center" data-conversation="none">
<p lang="en" dir="ltr">
We covered this in some of our earlier courses - lower batch sizes provide more updates, which should give better results for a fixed # epochs.
</p>
— Jeremy Howard (<span class="citation" data-cites="jeremyphoward">@jeremyphoward</span>) <a href="https://twitter.com/jeremyphoward/status/1927192030335132090?ref_src=twsrc%5Etfw">May 27, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Additionally, as the Gemini report goes on to say:</p>
<blockquote class="blockquote">
<p>Small batches can lead to gradients that are “bigger and chaotic” because an incorrect prediction on a single data point within a tiny batch can result in a disproportionately large loss and, subsequently, a large gradient update compared to its effect in a larger batch…high gradient noise, stemming from the limited data used for each estimation, leads to more volatile and less precise parameter updates</p>
</blockquote>
<p>This was a good opportunity to do a quick refresher on cross entropy loss so I worked through <a href="https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb">Chapter 5 of fastbook</a> again and wrote up <a href="https://vishalbakshi.github.io/blog/posts/2025-06-11-CELoss/">a blog post</a> and <a href="https://www.youtube.com/watch?v=swHhoP53jq4">video walkthrough</a> on that.</p>
<p>Suppose we have a batch size of two for a binary classification task:</p>
<div id="cell-17" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="90fce2ca-d659-41a1-9e29-457566223b40" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor([[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">9.5</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]], requires_grad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-2">acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>tensor([[0.4000, 9.5000],
        [4.0000, 5.0000]], requires_grad=True)</code></pre>
</div>
</div>
<p>With the following targets—the first batch item target is the first class (index <code>0</code>), which has a very wrong small activation.</p>
<div id="cell-19" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">targ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span></code></pre></div>
</div>
<div id="cell-20" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="14e2581f-1023-4b2d-e27b-4abdfd25ab17" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CrossEntropyLoss()(acts, targ)</span>
<span id="cb5-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>tensor(4.7067, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-21" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a9c7c33f-a43e-49d1-a45b-adb6bf7a6fca" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">loss.backward()</span>
<span id="cb7-2">acts.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor([[-0.4999,  0.4999],
        [ 0.1345, -0.1345]])</code></pre>
</div>
</div>
<p>Now suppose we had the same two items but in a batch size of 8.</p>
<div id="cell-23" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor(</span>
<span id="cb9-2">    [[ <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4000</span>,  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">9.5000</span>], <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># same item as before</span></span>
<span id="cb9-3">     [ <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.0000</span>,  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.0000</span>], <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># same item as before</span></span>
<span id="cb9-4">     [ <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3367</span>,  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1288</span>],</span>
<span id="cb9-5">     [ <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2345</span>,  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2303</span>],</span>
<span id="cb9-6">     [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.1229</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1863</span>],</span>
<span id="cb9-7">     [ <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.2082</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6380</span>],</span>
<span id="cb9-8">     [ <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4617</span>,  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2674</span>],</span>
<span id="cb9-9">     [ <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5349</span>,  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8094</span>]], requires_grad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>Adding 6 items to the targets.</p>
<div id="cell-25" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">targ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span></code></pre></div>
</div>
<div id="cell-26" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c1f16f85-df02-4d87-f5c2-2ca579425447" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CrossEntropyLoss()(acts, targ)</span>
<span id="cb11-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor(1.5563, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<p>Note that the loss is smaller.</p>
<div id="cell-28" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="01eb068c-1918-4dad-c0f4-594b01c14ee5" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">loss.backward()</span>
<span id="cb13-2">acts.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>tensor([[-0.1250,  0.1250],
        [ 0.0336, -0.0336],
        [-0.0560,  0.0560],
        [ 0.0626, -0.0626],
        [ 0.0352, -0.0352],
        [-0.0069,  0.0069],
        [ 0.0686, -0.0686],
        [ 0.0540, -0.0540]])</code></pre>
</div>
</div>
<p>The loss and gradients are much smaller. The impact of the one confidently wrong prediction has decreased with larger batch size.</p>
<p>There are also two other factors at play, as the Gemini report states:</p>
<blockquote class="blockquote">
<p>On one hand, the noisy gradients can cause the optimization process to oscillate significantly around an optimal solution, making it challenging for the model to settle into a good minimum and potentially slowing down overall convergence</p>
</blockquote>
<blockquote class="blockquote">
<p>On the other hand, this very noise and the resultant exploratory behavior can be beneficial. The stochasticity introduced by small batches can act as a form of implicit regularization, helping the model to escape sharp, narrow local minima in the loss landscape and instead find flatter, broader minima</p>
</blockquote>
<p>The push and pull between the pros (faster updates, implicit regularization) and cons (longer training time, GPU underutilization, oscillating around minima, chaotic gradients/high gradient noise) makes small batch training a fascinating topic. The very characteristics that provide benefit to the training process also damage it.</p>
</section>
<section id="small-batch-size-and-batch-normalization" class="level2">
<h2 class="anchored" data-anchor-id="small-batch-size-and-batch-normalization">Small Batch Size and Batch Normalization</h2>
<p>In addition to making the gradient noisy, small batch sizes make Batch Normalization statistics noisy.</p>
<blockquote class="blockquote">
<p>BN standardizes the activations within a network by calculating mean and variance statistics per batch. When batch sizes are very small (e.g., 1, 2, or 4 samples), these batch-wise statistics become extremely noisy and unreliable estimators of the true population statistics across the entire dataset</p>
</blockquote>
<p>From the <a href="https://arxiv.org/abs/1803.08494">Group Normalization paper</a> (for which I have done a <a href="https://www.youtube.com/watch?v=ZCTcxNEGens">video walkthrough</a>):</p>
<blockquote class="blockquote">
<p>normalizing along the batch dimension introduces problems — BN’s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation.</p>
</blockquote>
<p>The three models I’m using all have a considerable number of Batch Normalization layers.</p>
<div id="cell-37" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span></code></pre></div>
</div>
<div id="cell-38" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bbe244fe-90e6-455f-fdb1-f14334b8c350" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">bn_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb16-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> module <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> xresnet18().modules():</span>
<span id="cb16-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(module, nn.BatchNorm2d): bn_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb16-4">bn_count</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>22</code></pre>
</div>
</div>
<div id="cell-39" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9b07f053-6422-4605-ea78-61677ea481f9" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">bn_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb18-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> module <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> xresnet34().modules():</span>
<span id="cb18-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(module, nn.BatchNorm2d): bn_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb18-4">bn_count</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>38</code></pre>
</div>
</div>
<div id="cell-40" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="176953d5-7ca7-4866-bd9c-695e5105e9a5" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">bn_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb20-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> module <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> xse_resnext50().modules():</span>
<span id="cb20-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(module, nn.BatchNorm2d): bn_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb20-4">bn_count</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>55</code></pre>
</div>
</div>
</section>
<section id="techniques-for-stable-and-accurate-low-batch-size-training" class="level2">
<h2 class="anchored" data-anchor-id="techniques-for-stable-and-accurate-low-batch-size-training">Techniques for Stable and Accurate Low Batch Size Training</h2>
<p>The fascinating complexity of stable small batch training is summarized in the report with the following:</p>
<blockquote class="blockquote">
<p>Addressing the challenges of small batch training requires a multi-pronged approach, focusing on adapting learning rate strategies, rethinking normalization layers, selecting appropriate optimizers, and employing advanced gradient management and regularization techniques.</p>
</blockquote>
<section id="learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="learning-rate">Learning Rate</h3>
<blockquote class="blockquote">
<p>for small batches, the high gradient noise often necessitates smaller learning rates to prevent divergence and ensure stability</p>
</blockquote>
<p>We can see that there’s evidence for that from my <a href="https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/">Imagenette experiments</a>. For the xresnet18, xresnet34 and xse_resnext50 models (top to bottom charts, respectively), 1e-3 yields higher accuracy for lower batch sizes than 1e-2.</p>
<p><strong>xresnet18</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="lr_acc_18.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="xresnet18 batch size vs accuracy for different learning rates"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-small-batch-report/lr_acc_18.png" class="img-fluid figure-img" alt="xresnet18 batch size vs accuracy for different learning rates"></a></p>
<figcaption>xresnet18 batch size vs accuracy for different learning rates</figcaption>
</figure>
</div>
<p><strong>xresnet34</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="lr_acc_34.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="xresnet34 batch size vs accuracy for different learning rates"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-small-batch-report/lr_acc_34.png" class="img-fluid figure-img" alt="xresnet34 batch size vs accuracy for different learning rates"></a></p>
<figcaption>xresnet34 batch size vs accuracy for different learning rates</figcaption>
</figure>
</div>
<p><strong>xse_resnext50</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="lr_acc_50.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="xse_resnext50 batch size vs accuracy for different learning rates"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-small-batch-report/lr_acc_50.png" class="img-fluid figure-img" alt="xse_resnext50 batch size vs accuracy for different learning rates"></a></p>
<figcaption>xse_resnext50 batch size vs accuracy for different learning rates</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>small batch sizes (e.g., 2 to 32) can be more robust to learning rate choices</p>
</blockquote>
<p>I witnessed the second point during my training runs as well. For xresnet34 and a batch size of 8 or 16, the difference in accuracy between learning rates of 1e-4, 1e-3 and 1e-2 was 10%. For a batch size of 1024, the difference in accuracy was 40%. xresnet18 had a similar trend while xse_resnext50 was more robust to changes in learning rates (1e-4, 1e-3, 1e-2) as the accuracy varied about 10-20% across batch sizes of 2 to 1024. In the charts above, this is visualized by the diverging LR curves as batch size increases.</p>
<blockquote class="blockquote">
<p>A learning rate warm-up strategy, where training begins with a very small learning rate that is gradually increased to its target value over a few initial epochs, can significantly stabilize the early phases of training.</p>
</blockquote>
<p>My training runs already did this by default (using fastai’s <code>Learner</code> and <code>vision_learner</code>), though I haven’t experimented with any available parameters related to this (e.g.&nbsp;number of warmup steps).</p>
<blockquote class="blockquote">
<p>The optimal learning rate strategy is non-linear, optimizer-dependent, and may require empirical tuning guided by these more nuanced theoretical understandings.</p>
</blockquote>
</section>
<section id="alternative-forms-of-normalization" class="level3">
<h3 class="anchored" data-anchor-id="alternative-forms-of-normalization">Alternative Forms of Normalization</h3>
<p>Gemini found in its research that alternative forms of normalization (other than Batch Normalization) can improve small batch size performance. Most notably, in the Group Normalization paper they found that Group Normalization resulted in a 10% lower error rate than Batch Normalization for small batch sizes. Group Normalization calculates mean and variance for a group of channels for a single image, thereby being <em>batch independent</em>, so a small batch size doesn’t make the statistics any noisier (as is the case for Batch Normalization where the mean and variance are calculated across all images in the batch). However, there’s two sides to this trade-off. As the paper states:</p>
<blockquote class="blockquote">
<p>BN’s mean and variance computation introduces uncertainty caused by the stochastic batch sampling, which helps regularization. This uncertainty is missing in GN (and LN/IN). But it is possible that GN combined with a suitable regularizer will improve results. This can be a future research topic.</p>
</blockquote>
<p>Gemini also suggests Layer Normalization (calculating mean and variance across all channels for a single image) and Instance Normalization (across a single channel for a single image) as these are both also batch independent. However, the Group Normalization paper finds that these two perform worse than Group Normalization.</p>
</section>
<section id="optimizers-and-regularization" class="level3">
<h3 class="anchored" data-anchor-id="optimizers-and-regularization">Optimizers and Regularization</h3>
<p>Finally, Gemini suggested to try out different optimizers (and tuning their hyperparameters) and regularization techniques (weight decay, data augmentation), both things I wanted to experiment with going into this project.</p>
</section>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>Based on Gemini’s research and my reading of the Group Normalization paper I plan on experimenting on the following:</p>
<ul>
<li>Replace Batch Normalization layers with Group Normalization.</li>
<li>Add a regularization method like weight decay.</li>
<li>Try out different optimizers using Benjamin Warner’s <a href="https://optimi.benjaminwarner.dev/">optimi</a> library.</li>
<li>Try out different data augmentation techniques from Benjamin Warner’s <a href="https://fastxtend.benjaminwarner.dev/vision.augment.batch.html">fastxtend</a> library.</li>
</ul>
<p>At each step, I use <code>lr_find</code> and sample three stable LRs, and focus on small batch sizes of {2, 4, 8, 16, 32, 64}. I’m particularly interested in seeing how these experiments affect the three models I’m using (xresnet18, xresnet34, xse_resnext50) as xse_resnext50 was more robust to larger LRs than the other two.</p>
<p>I’ll be documenting my findings in blog posts as well as on my <a href="https://www.youtube.com/@vishal_learner">YouTube channel</a> so please subscribe to follow along!</p>


</section>

 ]]></description>
  <category>python</category>
  <category>fastai</category>
  <category>imagenette</category>
  <category>TinyScaleLab</category>
  <category>deep learning</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-06-18-small-batch-report/</guid>
  <pubDate>Wed, 18 Jun 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>An Analysis of Batch Size vs. Learning Rate on Imagenette</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/</link>
  <description><![CDATA[ 




<div id="cell-1" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.callback.wandb <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> WandbCallback</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> wandb</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span></code></pre></div>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In <a href="https://vishalbakshi.github.io/blog/posts/2025-06-01-imagenette/">my previous blog post</a> and <a href="https://youtu.be/5xCUEaowcTE">video</a> I worked through my initial imagenette experiments.</p>
<blockquote class="blockquote">
<p>Imagenette is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).</p>
</blockquote>
<p>With a batch size of 64 and three different LRs (1e-4, 1e-3, 1e-2) I achieved the following results using three models (xresnet34, xse_resnext50, xresnet18):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Model/Learning Rate</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">xresnet34/1e-2</td>
<td style="text-align: center;">0.7929</td>
</tr>
<tr class="even">
<td style="text-align: center;">xse_resnext50/1e-2</td>
<td style="text-align: center;">0.7926</td>
</tr>
<tr class="odd">
<td style="text-align: center;">xresnet18/1e-2</td>
<td style="text-align: center;">0.7870</td>
</tr>
<tr class="even">
<td style="text-align: center;">xresnet34/1e-3</td>
<td style="text-align: center;">0.7776</td>
</tr>
<tr class="odd">
<td style="text-align: center;">xresnet18/1e-3</td>
<td style="text-align: center;">0.7743</td>
</tr>
<tr class="even">
<td style="text-align: center;">xse_resnext50/1e-3</td>
<td style="text-align: center;">0.7526</td>
</tr>
<tr class="odd">
<td style="text-align: center;">xresnet18/1e-1</td>
<td style="text-align: center;">0.7373</td>
</tr>
<tr class="even">
<td style="text-align: center;">xse_resnext50/1e-4</td>
<td style="text-align: center;">0.6532</td>
</tr>
<tr class="odd">
<td style="text-align: center;">xresnet34/1e-4</td>
<td style="text-align: center;">0.6446</td>
</tr>
<tr class="even">
<td style="text-align: center;">xresnet18/1e-4</td>
<td style="text-align: center;">0.6171</td>
</tr>
</tbody>
</table>
<p>There was a hierarchy of LRs: 1e-2 performed best, followed by 1e-3 and 1e-4.</p>
<p>In this notebook I’m going to expand my training runs to more batch sizes: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]. In each case, I’ll use <code>lr_find</code> to determine three stable learning rates.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<div id="cell-7" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/content/2025-06-04-imagenette-runs.csv"</span>)</span>
<span id="cb2-4">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"model"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"run"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: x.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-"</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb2-5">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"run"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: x.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-"</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _lr(x):</span>
<span id="cb2-8">    _map <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"05"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-5"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"06"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-6"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"07"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-7"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"0.0001"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-4"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"0.001"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-3"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"0.01"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-2"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"0.1"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-1"</span>}</span>
<span id="cb2-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> _map[x.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-"</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]]</span>
<span id="cb2-10"></span>
<span id="cb2-11">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"run"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: _lr(x))</span>
<span id="cb2-12">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Categorical(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], categories<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-7"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-6"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-5"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-4"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-3"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-2"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1e-1"</span>])</span></code></pre></div>
</div>
<p>In total there were 123 training runs.</p>
<div id="cell-9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="8d4136e7-853b-4711-a504-aeb14b3ff108" data-execution_count="10">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(123, 5)</code></pre>
</div>
</div>
<p>The best training run goes to xresnet34 with a batch size of 16 and a learning rate of 1e-3.</p>
<div id="cell-11" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:81}}" data-outputid="2e30e554-1c1a-495f-9be6-aa7b0fe29193" data-execution_count="11">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>()]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<div id="df-bd697f8f-dc53-42ec-9e0e-ea34025e8e15" class="colab-df-container">
    <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">run</th>
<th data-quarto-table-cell-role="th">acc</th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">bs</th>
<th data-quarto-table-cell-role="th">lr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">9</td>
<td>imagenette-xresnet34-bs-16-lr-0.001</td>
<td>0.800764</td>
<td>xresnet34</td>
<td>16</td>
<td>1e-3</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-bd697f8f-dc53-42ec-9e0e-ea34025e8e15')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-bd697f8f-dc53-42ec-9e0e-ea34025e8e15 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-bd697f8f-dc53-42ec-9e0e-ea34025e8e15');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


    </div>
  </div>
</div>
</div>
<p>The top 10 training runs were for a LR of 1e-3 or 1e-2.</p>
<div id="cell-13" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:363}}" data-outputid="c8fa0fa4-c586-4b1c-c1cd-c541e1b8a267" data-execution_count="12">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">df.sort_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<div id="df-77f8aa8d-c052-47a9-9bb5-915ebc8e4364" class="colab-df-container">
    <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">run</th>
<th data-quarto-table-cell-role="th">acc</th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">bs</th>
<th data-quarto-table-cell-role="th">lr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">9</td>
<td>imagenette-xresnet34-bs-16-lr-0.001</td>
<td>0.800764</td>
<td>xresnet34</td>
<td>16</td>
<td>1e-3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">66</td>
<td>imagenette-xresnet18-bs-32-lr-0.01</td>
<td>0.799745</td>
<td>xresnet18</td>
<td>32</td>
<td>1e-2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75</td>
<td>imagenette-xresnet18-bs-64-lr-0.01</td>
<td>0.797452</td>
<td>xresnet18</td>
<td>64</td>
<td>1e-2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>imagenette-xresnet18-bs-16-lr-0.01</td>
<td>0.796433</td>
<td>xresnet18</td>
<td>16</td>
<td>1e-2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>imagenette-xresnet18-bs-8-lr-0.001</td>
<td>0.795414</td>
<td>xresnet18</td>
<td>8</td>
<td>1e-3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">25</td>
<td>imagenette-xresnet34-bs-8-lr-0.001</td>
<td>0.795414</td>
<td>xresnet34</td>
<td>8</td>
<td>1e-3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>imagenette-xresnet34-bs-16-lr-0.01</td>
<td>0.795159</td>
<td>xresnet34</td>
<td>16</td>
<td>1e-2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">84</td>
<td>imagenette-xresnet18-bs-128-lr-0.01</td>
<td>0.791338</td>
<td>xresnet18</td>
<td>128</td>
<td>1e-2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">76</td>
<td>imagenette-xresnet34-bs-64-lr-0.01</td>
<td>0.790573</td>
<td>xresnet34</td>
<td>64</td>
<td>1e-2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">67</td>
<td>imagenette-xresnet34-bs-32-lr-0.01</td>
<td>0.789554</td>
<td>xresnet34</td>
<td>32</td>
<td>1e-2</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-77f8aa8d-c052-47a9-9bb5-915ebc8e4364')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-77f8aa8d-c052-47a9-9bb5-915ebc8e4364 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-77f8aa8d-c052-47a9-9bb5-915ebc8e4364');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


    <div id="df-cea70ab5-c2c5-42c0-a9aa-2d37f96325b0">
      <button class="colab-df-quickchart" onclick="quickchart('df-cea70ab5-c2c5-42c0-a9aa-2d37f96325b0')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
      </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

      <script>
        async function quickchart(key) {
          const quickchartButtonEl =
            document.querySelector('#' + key + ' button');
          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
          quickchartButtonEl.classList.add('colab-df-spinner');
          try {
            const charts = await google.colab.kernel.invokeFunction(
                'suggestCharts', [key], {});
          } catch (error) {
            console.error('Error during call to suggestCharts:', error);
          }
          quickchartButtonEl.classList.remove('colab-df-spinner');
          quickchartButtonEl.classList.add('colab-df-quickchart-complete');
        }
        (() => {
          let quickchartButtonEl =
            document.querySelector('#df-cea70ab5-c2c5-42c0-a9aa-2d37f96325b0 button');
          quickchartButtonEl.style.display =
            google.colab.kernel.accessAllowed ? 'block' : 'none';
        })();
      </script>
    </div>

    </div>
  </div>
</div>
</div>
<p>All three models achieved a max accuracy within 1.2% of each other.</p>
<div id="cell-15" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:210}}" data-outputid="1bff0dea-551a-4909-d258-99917c965ff5" data-execution_count="14">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df.groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"model"</span>])[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>().sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">acc</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">xresnet34</td>
<td>0.800764</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">xresnet18</td>
<td>0.799745</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">xse_resnext50</td>
<td>0.788280</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> float64</label>
</div>
</div>
<p>A batch size of 16 yielded the highest accuracy, with 8, 32, 64 and 128 within 1%. Accuracy drops significantly for a batch size of 1, 1024 and 2048 showing that too large or too small a batch size is detrimental to training.</p>
<div id="cell-17" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:492}}" data-outputid="aca37107-cb01-462f-d674-f85c9ce8b482" data-execution_count="16">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">df.groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>])[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>().sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">acc</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">bs</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>0.800764</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">32</td>
<td>0.799745</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">64</td>
<td>0.797452</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">8</td>
<td>0.795414</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">128</td>
<td>0.791338</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">4</td>
<td>0.784968</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">256</td>
<td>0.764841</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">512</td>
<td>0.731975</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.719745</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1024</td>
<td>0.662166</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2048</td>
<td>0.485350</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.452229</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> float64</label>
</div>
</div>
<p>As we saw earlier, the highest accuracy was achieved with a learning rate of 1e-3, with 1e-2 right on its tail.</p>
<div id="cell-19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:304}}" data-outputid="4ed7d42b-5d78-43c1-bade-052f414da0ed" data-execution_count="18">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">df.groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], observed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>().sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">acc</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">lr</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-3</td>
<td>0.800764</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-2</td>
<td>0.799745</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-4</td>
<td>0.715414</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-5</td>
<td>0.526624</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-6</td>
<td>0.304713</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-7</td>
<td>0.150828</td>
</tr>
</tbody>
</table>

</div><br><label><b>dtype:</b> float64</label>
</div>
</div>
<p>Finally, I’ll look at the maximum accuracy achieved for each model/learning rate/batch size combination.</p>
<p>For xresnet18, some interesting patterns:</p>
<ul>
<li>For each learning rate, the maximum accuracy increases up to a point, then decreases, illustrating how batch sizes too small or too large lead to worse performance
<ul>
<li>1e-4: acc increases up to bs=8 then decreases</li>
<li>1e-3: acc increases up to bs=8 then decreases</li>
<li>1e-2: acc increases up to bs=32 then decreases</li>
</ul></li>
</ul>
<div id="cell-22" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:175}}" data-outputid="53e68ba5-db55-4fcf-f64e-2de3b94683f5" data-execution_count="21">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"model == 'xresnet18'"</span>)</span>
<span id="cb10-2">pd.crosstab(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], _df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>], values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>], aggfunc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>).sort_index(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div id="df-a8a72bd5-679d-4766-8624-a87fadf17809" class="colab-df-container">
    <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">bs</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">4</th>
<th data-quarto-table-cell-role="th">8</th>
<th data-quarto-table-cell-role="th">16</th>
<th data-quarto-table-cell-role="th">32</th>
<th data-quarto-table-cell-role="th">64</th>
<th data-quarto-table-cell-role="th">128</th>
<th data-quarto-table-cell-role="th">256</th>
<th data-quarto-table-cell-role="th">512</th>
<th data-quarto-table-cell-role="th">1024</th>
<th data-quarto-table-cell-role="th">2048</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">lr</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-4</td>
<td>0.388535</td>
<td>0.608153</td>
<td>0.693503</td>
<td>0.704968</td>
<td>0.698089</td>
<td>0.665478</td>
<td>0.624713</td>
<td>0.512102</td>
<td>0.400000</td>
<td>0.310318</td>
<td>0.146497</td>
<td>0.000255</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-3</td>
<td>0.452229</td>
<td>0.709809</td>
<td>0.778344</td>
<td>0.795414</td>
<td>0.789299</td>
<td>0.784968</td>
<td>0.769427</td>
<td>0.744713</td>
<td>0.719236</td>
<td>0.647389</td>
<td>0.544459</td>
<td>0.336306</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-2</td>
<td>0.285605</td>
<td>0.649682</td>
<td>0.738089</td>
<td>0.778089</td>
<td>0.796433</td>
<td>0.799745</td>
<td>0.797452</td>
<td>0.791338</td>
<td>0.747006</td>
<td>0.731975</td>
<td>0.661147</td>
<td>0.485350</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-a8a72bd5-679d-4766-8624-a87fadf17809')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-a8a72bd5-679d-4766-8624-a87fadf17809 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-a8a72bd5-679d-4766-8624-a87fadf17809');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


    <div id="df-196b94d7-b776-402c-85a0-8001c541b87f">
      <button class="colab-df-quickchart" onclick="quickchart('df-196b94d7-b776-402c-85a0-8001c541b87f')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
      </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

      <script>
        async function quickchart(key) {
          const quickchartButtonEl =
            document.querySelector('#' + key + ' button');
          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
          quickchartButtonEl.classList.add('colab-df-spinner');
          try {
            const charts = await google.colab.kernel.invokeFunction(
                'suggestCharts', [key], {});
          } catch (error) {
            console.error('Error during call to suggestCharts:', error);
          }
          quickchartButtonEl.classList.remove('colab-df-spinner');
          quickchartButtonEl.classList.add('colab-df-quickchart-complete');
        }
        (() => {
          let quickchartButtonEl =
            document.querySelector('#df-196b94d7-b776-402c-85a0-8001c541b87f button');
          quickchartButtonEl.style.display =
            google.colab.kernel.accessAllowed ? 'block' : 'none';
        })();
      </script>
    </div>

    </div>
  </div>
</div>
</div>
<p><strong>xresnet18</strong></p>
<div id="cell-24" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:454}}" data-outputid="d3906c9e-8828-4625-b89c-4e4fc9dc3488" data-execution_count="22">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">pd.crosstab(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], _df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>], values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>], aggfunc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>).sort_index(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).T.plot(logx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-11-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/index_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>We see a similar pattern for xresnet34:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">lr</th>
<th style="text-align: center;">Inflection point (bs)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1e-7</td>
<td style="text-align: center;">–</td>
</tr>
<tr class="even">
<td style="text-align: center;">1e-6</td>
<td style="text-align: center;">–</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1e-5</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">1e-4</td>
<td style="text-align: center;">16</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1e-3</td>
<td style="text-align: center;">16</td>
</tr>
<tr class="even">
<td style="text-align: center;">1e-2</td>
<td style="text-align: center;">16</td>
</tr>
</tbody>
</table>
<div id="cell-26" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:269}}" data-outputid="69811696-90e6-49a5-f85d-68a2c7fbfcf1" data-execution_count="23">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"model == 'xresnet34'"</span>)</span>
<span id="cb12-2">pd.crosstab(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], _df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>], values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>], aggfunc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>).sort_index(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<div id="df-20fd735d-8058-4aeb-ae6c-3a7dc3eee40d" class="colab-df-container">
    <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">bs</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">4</th>
<th data-quarto-table-cell-role="th">8</th>
<th data-quarto-table-cell-role="th">16</th>
<th data-quarto-table-cell-role="th">32</th>
<th data-quarto-table-cell-role="th">64</th>
<th data-quarto-table-cell-role="th">128</th>
<th data-quarto-table-cell-role="th">256</th>
<th data-quarto-table-cell-role="th">512</th>
<th data-quarto-table-cell-role="th">1024</th>
<th data-quarto-table-cell-role="th">2048</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">lr</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-7</td>
<td>NaN</td>
<td>0.109554</td>
<td>0.109045</td>
<td>0.087389</td>
<td>0.100892</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-6</td>
<td>NaN</td>
<td>0.243822</td>
<td>0.180127</td>
<td>0.201274</td>
<td>0.159236</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-5</td>
<td>NaN</td>
<td>0.385478</td>
<td>0.456051</td>
<td>0.446879</td>
<td>0.428535</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-4</td>
<td>0.388535</td>
<td>0.597962</td>
<td>0.704459</td>
<td>0.709809</td>
<td>0.715414</td>
<td>0.680764</td>
<td>0.641529</td>
<td>0.560510</td>
<td>0.489172</td>
<td>0.393376</td>
<td>0.269299</td>
<td>0.212229</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-3</td>
<td>0.403567</td>
<td>0.719745</td>
<td>0.784968</td>
<td>0.795414</td>
<td>0.800764</td>
<td>0.787261</td>
<td>0.765860</td>
<td>0.751592</td>
<td>0.728408</td>
<td>0.667771</td>
<td>0.580127</td>
<td>0.430828</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-2</td>
<td>0.246624</td>
<td>0.638217</td>
<td>0.721019</td>
<td>0.776306</td>
<td>0.795159</td>
<td>0.789554</td>
<td>0.790573</td>
<td>0.787006</td>
<td>0.764841</td>
<td>0.723312</td>
<td>0.662166</td>
<td>0.385732</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-20fd735d-8058-4aeb-ae6c-3a7dc3eee40d')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-20fd735d-8058-4aeb-ae6c-3a7dc3eee40d button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-20fd735d-8058-4aeb-ae6c-3a7dc3eee40d');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


    <div id="df-7ffdc5c0-ac7a-4ac5-8e6a-abd0a21467a8">
      <button class="colab-df-quickchart" onclick="quickchart('df-7ffdc5c0-ac7a-4ac5-8e6a-abd0a21467a8')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
      </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

      <script>
        async function quickchart(key) {
          const quickchartButtonEl =
            document.querySelector('#' + key + ' button');
          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
          quickchartButtonEl.classList.add('colab-df-spinner');
          try {
            const charts = await google.colab.kernel.invokeFunction(
                'suggestCharts', [key], {});
          } catch (error) {
            console.error('Error during call to suggestCharts:', error);
          }
          quickchartButtonEl.classList.remove('colab-df-spinner');
          quickchartButtonEl.classList.add('colab-df-quickchart-complete');
        }
        (() => {
          let quickchartButtonEl =
            document.querySelector('#df-7ffdc5c0-ac7a-4ac5-8e6a-abd0a21467a8 button');
          quickchartButtonEl.style.display =
            google.colab.kernel.accessAllowed ? 'block' : 'none';
        })();
      </script>
    </div>

    </div>
  </div>
</div>
</div>
<p><strong>xresnet34</strong></p>
<div id="cell-28" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:454}}" data-outputid="926c22f9-ed42-4eba-81ef-f61a16f7884e" data-execution_count="24">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">pd.crosstab(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], _df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>], values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>], aggfunc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>).sort_index(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).T.plot(logx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/index_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>And a similar pattern for xse_resnext50 as well:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">lr</th>
<th style="text-align: center;">Inflection point (bs)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1e-7</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td style="text-align: center;">1e-6</td>
<td style="text-align: center;">–</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1e-5</td>
<td style="text-align: center;">–</td>
</tr>
<tr class="even">
<td style="text-align: center;">1e-4</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1e-3</td>
<td style="text-align: center;">16</td>
</tr>
<tr class="even">
<td style="text-align: center;">1e-2</td>
<td style="text-align: center;">32</td>
</tr>
</tbody>
</table>
<div id="cell-30" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:269}}" data-outputid="bb96c37f-36d4-4ad4-ef06-4ad434436263" data-execution_count="25">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"model == 'xse_resnext50'"</span>)</span>
<span id="cb14-2">pd.crosstab(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], _df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>], values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>], aggfunc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>).sort_index(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div id="df-0c642622-075f-4525-b8d2-f0b575de9eb9" class="colab-df-container">
    <div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">bs</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">4</th>
<th data-quarto-table-cell-role="th">8</th>
<th data-quarto-table-cell-role="th">16</th>
<th data-quarto-table-cell-role="th">32</th>
<th data-quarto-table-cell-role="th">64</th>
<th data-quarto-table-cell-role="th">128</th>
<th data-quarto-table-cell-role="th">256</th>
<th data-quarto-table-cell-role="th">512</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">lr</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-7</td>
<td>0.106497</td>
<td>0.127389</td>
<td>0.150828</td>
<td>0.126624</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-6</td>
<td>0.117452</td>
<td>0.234395</td>
<td>0.291465</td>
<td>0.304713</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-5</td>
<td>0.239745</td>
<td>0.420127</td>
<td>0.502675</td>
<td>0.526624</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-4</td>
<td>0.203057</td>
<td>0.568917</td>
<td>0.661147</td>
<td>0.660382</td>
<td>0.652229</td>
<td>0.639236</td>
<td>0.623949</td>
<td>0.601019</td>
<td>0.575541</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1e-3</td>
<td>0.186752</td>
<td>0.642803</td>
<td>0.766624</td>
<td>0.774267</td>
<td>0.768917</td>
<td>0.743694</td>
<td>0.720510</td>
<td>0.699363</td>
<td>0.671847</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1e-2</td>
<td>0.140637</td>
<td>0.263185</td>
<td>0.564586</td>
<td>0.736306</td>
<td>0.788280</td>
<td>0.770701</td>
<td>0.776306</td>
<td>0.763567</td>
<td>0.725605</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-0c642622-075f-4525-b8d2-f0b575de9eb9')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-0c642622-075f-4525-b8d2-f0b575de9eb9 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-0c642622-075f-4525-b8d2-f0b575de9eb9');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


    <div id="df-713a171e-198c-4f12-b708-12d592f77d1d">
      <button class="colab-df-quickchart" onclick="quickchart('df-713a171e-198c-4f12-b708-12d592f77d1d')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
      </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

      <script>
        async function quickchart(key) {
          const quickchartButtonEl =
            document.querySelector('#' + key + ' button');
          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
          quickchartButtonEl.classList.add('colab-df-spinner');
          try {
            const charts = await google.colab.kernel.invokeFunction(
                'suggestCharts', [key], {});
          } catch (error) {
            console.error('Error during call to suggestCharts:', error);
          }
          quickchartButtonEl.classList.remove('colab-df-spinner');
          quickchartButtonEl.classList.add('colab-df-quickchart-complete');
        }
        (() => {
          let quickchartButtonEl =
            document.querySelector('#df-713a171e-198c-4f12-b708-12d592f77d1d button');
          quickchartButtonEl.style.display =
            google.colab.kernel.accessAllowed ? 'block' : 'none';
        })();
      </script>
    </div>

    </div>
  </div>
</div>
</div>
<p><strong>xse_xresnet50</strong></p>
<p>Looking at the accuracy curves for xse_resnext50 we see that it’s more robust to larger batch sizes (the accuracy tapers off slower than xresnet34 and xresnet18 as batch size increases past the inflection point).</p>
<div id="cell-33" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:454}}" data-outputid="dd37f3b1-85fe-45d0-b83e-cb3c9652442b" data-execution_count="26">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">pd.crosstab(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lr"</span>], _df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bs"</span>], values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"acc"</span>], aggfunc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>).sort_index(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).T.plot(logx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-15-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/index_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>I posted my initial musing (after I had analyzed results for batch sizes 32 to 2048) that lowering the batch size might continue to yield higher accuracy. Jeremy validated this approach:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
That’s a great question! Stable low bs training isn’t easy, but it’s a good plan :)
</p>
— Jeremy Howard (<span class="citation" data-cites="jeremyphoward">@jeremyphoward</span>) <a href="https://twitter.com/jeremyphoward/status/1930486029607416069?ref_src=twsrc%5Etfw">June 5, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I recently started using Gemini Pro 2.5 (preview), and hadn’t yet tried out its Deep Research tool. I thought this would be an excellent opportunity to do so. I prompted Gemini with the following:</p>
<blockquote class="blockquote">
<p>I recently have been training xresnet18, xresnet34 and xse_resnext50 on the Imagenette dataset by fast.ai (which is a 10k subset of ImageNet with 10 easily classified classes). I have trained on batch sizes from 1 to 2048. Generally speaking, the highest accuracy achieved increases from a batch size of 1 to a batch size of 8, 16 or 32 (depending on the model/learning rate) and then decreases as batch size increases to 2048. This makes me want to explore small batch size training for this project. Jeremy Howard tweeted that “Stable low bs training isn’t easy, but it’s a good plan”. I want you to help me answer two questions in this chat:</p>
<ol type="1">
<li><p>Why is stable low batch size training difficult?</p></li>
<li><p>What techniques are there (either in literature/arxiv, blog posts or forums) to make low batch size training stable? I’m most interested in improving the accuracy of low batch size trainings.</p></li>
</ol>
</blockquote>
<p>I also prompted Allen AI’s <a href="https://paperfinder.allen.ai/chat/606ffc67-5539-47a9-8c8a-ec5820666ccc?profile=paper-finder-only">Paper Finder</a> with:</p>
<blockquote class="blockquote">
<p>strategies to make low batch size training more stable and improve accuracy for image recognition</p>
</blockquote>
<p>I have shared the Gemini Deep Research and Ai2 Paper Finder findings in <a href="https://vishalbakshi.github.io/blog/posts/2025-06-18-small-batch-report/">another blog post</a>.</p>
<p>Imagenette will unlock an entire new lane of research and experimentation for me, helping me towards my <a href="https://vishalbakshi.github.io/blog/posts/2025-04-26-TinyScale-Lab-Kickoff/">TinyScaleLab project goal</a> of training and analyzing high performant tiny language models.</p>
<p>Lastly, I’m growing my YouTube channel so if you haven’t already, please check it out and <a href="https://www.youtube.com/@vishal_learner">subscribe</a>!</p>
</section>
<section id="appendix-training-code" class="level2">
<h2 class="anchored" data-anchor-id="appendix-training-code">Appendix: Training Code</h2>
<p>Since I’m going to run a significantly larger number of trainings than my initial notebook, I’ll wrap my code into functions/dictionaries for a tighter loop, similar to what Jeremy did in his <a href="https://www.kaggle.com/code/jhoward/scaling-up-road-to-the-top-part-3?scriptVersionId=99313071&amp;cellId=31">Scaling Up: Road to the Top, Part 3</a> notebook.</p>
<p>I ran <code>lr_find</code> for each model/batch size combination and checked that for all combinations, the loss curve was stable for the selected learning rates.</p>
<div id="cell-41" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:37}}" data-outputid="d8684f08-2d75-4679-bf77-4a8360d5a756">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> untar_data(URLs.IMAGENETTE_160)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="99008512" class="" max="99003388" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.01% [99008512/99003388 00:02&lt;00:00]
    </div>
    
</div>
</div>
<div id="cell-42" class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">lbl_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(</span>
<span id="cb17-2">    n01440764<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tench'</span>,</span>
<span id="cb17-3">    n02102040<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'English springer'</span>,</span>
<span id="cb17-4">    n02979186<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cassette player'</span>,</span>
<span id="cb17-5">    n03000684<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'chain saw'</span>,</span>
<span id="cb17-6">    n03028079<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'church'</span>,</span>
<span id="cb17-7">    n03394916<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'French horn'</span>,</span>
<span id="cb17-8">    n03417042<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'garbage truck'</span>,</span>
<span id="cb17-9">    n03425413<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gas pump'</span>,</span>
<span id="cb17-10">    n03445777<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'golf ball'</span>,</span>
<span id="cb17-11">    n03888257<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'parachute'</span></span>
<span id="cb17-12">)</span></code></pre></div>
</div>
<div id="cell-43" class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">imagenette <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataBlock(blocks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (ImageBlock, CategoryBlock),</span>
<span id="cb18-2">                       get_items <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_image_files,</span>
<span id="cb18-3">                       get_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline([parent_label, lbl_dict.<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__getitem__</span>]),</span>
<span id="cb18-4">                       splitter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GrandparentSplitter(valid_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'val'</span>),</span>
<span id="cb18-5">                       item_tfms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomResizedCrop(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.35</span>),</span>
<span id="cb18-6">                       batch_tfms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Normalize.from_stats(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>imagenet_stats))</span></code></pre></div>
</div>
<div id="cell-44" class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _dls(bs): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> imagenette.dataloaders(path, bs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>bs)</span></code></pre></div>
</div>
<div id="cell-45" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="618ee92e-6b22-4fe4-c71c-3c5fab4c5c3e">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">_dls(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>).bs, _dls(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>).bs, _dls(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>).bs</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(64, 128, 256)</code></pre>
</div>
</div>
<div id="cell-46" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:752}}" data-outputid="785bbfd1-dc53-45c7-ae6b-d7ed244e2e18">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">_dls(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>).show_batch()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-21-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/index_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="cell-47" class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">models <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xresnet34'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xse_resnext50'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xresnet18'</span>]</span></code></pre></div>
</div>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#batch_sizes = (32, 64, 128, 256, 512, 1024, 2048)</span></span>
<span id="cb24-2">batch_sizes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>)</span>
<span id="cb24-3">lrs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-2</span>)</span></code></pre></div>
</div>
<div id="cell-49" class="cell">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">wandb.login()</span></code></pre></div>
</div>
<div id="cell-50" class="cell">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> bs <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> batch_sizes:</span>
<span id="cb26-2">    dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _dls(bs)</span>
<span id="cb26-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> dls.bs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> bs</span>
<span id="cb26-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> model <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> models:</span>
<span id="cb26-5">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xse_resnext50'</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> bs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">continue</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># avoid Expected more than 1 value per channel when training, got input size torch.Size([1, 4096])</span></span>
<span id="cb26-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> lr <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> lrs:</span>
<span id="cb26-7">            wandb.init(</span>
<span id="cb26-8">                project<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tinyscale-lab"</span>,</span>
<span id="cb26-9">                name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"imagenette-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-bs-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>bs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-lr-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,</span>
<span id="cb26-10">                tags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"bs=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>bs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"lr=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb26-11">                )</span>
<span id="cb26-12">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xse_resnext50'</span>: learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, xse_resnext50, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy, pretrained<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, cbs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>WandbCallback())</span>
<span id="cb26-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xresnet18'</span>: learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Learner(dls, xresnet18(), metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy, cbs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>WandbCallback())</span>
<span id="cb26-14">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xresnet34'</span>: learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Learner(dls, xresnet34(n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy, cbs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>WandbCallback())</span>
<span id="cb26-15"></span>
<span id="cb26-16">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> learn.no_logging() <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> learn.no_mbar(): learn.fit_one_cycle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, lr)</span>
<span id="cb26-17">            wandb.finish()</span></code></pre></div>
</div>
<p>Using <code>lr_find</code>, I found that xresnet18 was stable for 1e-4, 1e-3 and 1e-2 LRs but the other two were not. Instead, they were stable for LRs of 1e-7, 1e-6 and 1e-5. So I ran training runs for those two models and LRs for batch sizes of 2, 4, 8 and 16.</p>
<div id="cell-52" class="cell">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> bs <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> batch_sizes:</span>
<span id="cb27-2">    dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _dls(bs)</span>
<span id="cb27-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> dls.bs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> bs</span>
<span id="cb27-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> model <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xresnet34'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xse_resnext50'</span>]:</span>
<span id="cb27-5">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xse_resnext50'</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> bs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">continue</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># avoid Expected more than 1 value per channel when training, got input size torch.Size([1, 4096])</span></span>
<span id="cb27-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> lr <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-5</span>]:</span>
<span id="cb27-7">            wandb.init(</span>
<span id="cb27-8">                project<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tinyscale-lab"</span>,</span>
<span id="cb27-9">                name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"imagenette-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-bs-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>bs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-lr-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,</span>
<span id="cb27-10">                tags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"bs=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>bs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"lr=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb27-11">                )</span>
<span id="cb27-12">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xse_resnext50'</span>: learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, xse_resnext50, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy, pretrained<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, cbs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>WandbCallback())</span>
<span id="cb27-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xresnet18'</span>: learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Learner(dls, xresnet18(), metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy, cbs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>WandbCallback())</span>
<span id="cb27-14">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xresnet34'</span>: learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Learner(dls, xresnet34(n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>accuracy, cbs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>WandbCallback())</span>
<span id="cb27-15"></span>
<span id="cb27-16">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> learn.no_logging() <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> learn.no_mbar(): learn.fit_one_cycle(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, lr)</span>
<span id="cb27-17">            wandb.finish()</span></code></pre></div>
</div>


</section>

 ]]></description>
  <category>python</category>
  <category>fastai</category>
  <category>deep learning</category>
  <category>TinyScaleLab</category>
  <category>imagenette</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/</guid>
  <pubDate>Wed, 18 Jun 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Cross Entropy Loss Explained</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-06-11-CELoss/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>As I’ve been preparing to build <a href="https://vishalbakshi.github.io/blog/index.html#category=TinyScaleLab">highly performant tiny LMs</a>, I realized I needed a refresher on how Cross-Entropy loss works. In this notebook I’m revisiting <a href="https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb">Chapter 5 of fastbook</a> to walk through the code and concepts introduced to build intuition around this loss function.</p>
<p>I was motivated to work on this refresher because I wanted to better visualize with a concrete example how small batch training can lead to noisy gradients.</p>
</section>
<section id="multi-class-classification" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification">Multi-Class Classification</h2>
<p>We are going to focus on a single-label, multiple class classification problem using the PETS dataset.</p>
<div id="cell-5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:37}}" data-outputid="916a6aea-9fc1-4095-a00d-b2d4462f90ed">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fastai.vision.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-2">path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> untar_data(URLs.PETS)</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="811712512" class="" max="811706944" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [811712512/811706944 00:14&lt;00:00]
    </div>
    
</div>
</div>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">pets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataBlock(blocks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (ImageBlock, CategoryBlock),</span>
<span id="cb2-2">                 get_items<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_image_files,</span>
<span id="cb2-3">                 splitter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>RandomSplitter(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>),</span>
<span id="cb2-4">                 get_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>using_attr(RegexLabeller(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">(</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">.</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">)</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">\d</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">.</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">jpg</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'name'</span>),</span>
<span id="cb2-5">                 item_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Resize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">460</span>),</span>
<span id="cb2-6">                 batch_tfms<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">224</span>, min_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>))</span>
<span id="cb2-7">dls <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pets.dataloaders(path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"images"</span>)</span></code></pre></div>
</div>
<div id="cell-7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:230}}" data-outputid="960ea3d6-91e1-47f4-9ddb-cfa755ef7c81">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">learn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vision_learner(dls, resnet34, metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>error_rate)</span>
<span id="cb3-2">learn.fine_tune(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet34-b627a593.pth" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth
100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 138MB/s]</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.499396</td>
<td>0.378620</td>
<td>0.125169</td>
<td>01:07</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.536576</td>
<td>0.329664</td>
<td>0.104871</td>
<td>01:08</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.326150</td>
<td>0.241802</td>
<td>0.078484</td>
<td>01:08</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Inspecting one batch, we see that the inputs are a batch of 64 3-channel 224x224 pixel images, and the targets are integers (between 0 and 36, the number of pet classes in the dataset)</p>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">x,y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dls.one_batch()</span></code></pre></div>
</div>
<div id="cell-10" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="33f8f547-092a-488e-a111-dcc96b36d461">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">x.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>torch.Size([64, 3, 224, 224])</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="3633a063-2f52-47f0-e469-9362a45be99a">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">y</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>TensorCategory([23, 13, 25,  3, 16, 31, 34,  8,  9, 20, 19, 10,  6, 19, 33, 35,
                34, 19, 18, 29, 26, 34, 24, 30, 31,  2,  3, 20, 28, 29,  0, 11,
                22, 31,  3, 36, 15, 16,  0, 18, 32, 17,  3, 18, 16,  6, 16,  2,
                32,  0,  9, 26, 26, 35, 18,  4, 36,  5, 17,  0, 11, 27, 21, 35],
               device='cuda:0')</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b4a33fcf-0b37-44c9-b9a3-cc80678a5273">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">y.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>torch.Size([64])</code></pre>
</div>
</div>
<p>Passing this batch to the trained model we get back 37 predictions for each item in the batch.</p>
<div id="cell-14" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:138}}" data-outputid="d002ca06-5bda-43f9-e418-fec34d0b2f52">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">preds,_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> learn.get_preds(dl<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(x,y)])</span>
<span id="cb12-2">preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor([2.0166e-08, 1.0262e-06, 6.8535e-06, 2.8387e-06, 7.7887e-08, 1.4034e-07,
        4.1744e-07, 9.1272e-06, 1.0166e-05, 1.3550e-07, 5.2067e-08, 3.7820e-06,
        4.2456e-06, 1.2272e-08, 1.2251e-07, 1.4040e-07, 7.0171e-06, 4.2623e-04,
        1.3611e-07, 7.3574e-08, 1.6146e-08, 5.0945e-06, 8.1139e-06, 9.9821e-01,
        4.9108e-05, 3.4051e-06, 7.8016e-09, 2.8509e-07, 1.0994e-03, 1.3271e-06,
        1.4462e-04, 4.8486e-07, 1.3296e-07, 7.4560e-07, 6.1535e-08, 7.6703e-07,
        9.5345e-07])</code></pre>
</div>
</div>
<div id="cell-15" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6b69e953-00de-43f1-bbd2-3e4379dd465a">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">preds.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>torch.Size([64, 37])</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4a8779a2-b7f7-40cf-c5bc-2cdbdecb7913">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">x.shape, y.shape, preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(torch.Size([64, 3, 224, 224]), torch.Size([64]), torch.Size([37]))</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="73e26c69-068c-4d62-e704-3a365b6fee15">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]),preds[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(37, tensor(1.))</code></pre>
</div>
</div>
<p>The predictions for each batch item sum to <code>1.0</code>.</p>
</section>
<section id="building-up-cross-entropy-loss" class="level2">
<h2 class="anchored" data-anchor-id="building-up-cross-entropy-loss">Building Up Cross-Entropy Loss</h2>
<p>We’ll work through a simpler example: binary classification with two classes (and thus two model outputs/activations).</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">torch.random.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<div id="cell-22" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2cf74b04-4eda-4dfe-8af7-475b9c628d44">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb21-2">acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>tensor([[ 0.6734,  0.2576],
        [ 0.4689,  0.4607],
        [-2.2457, -0.3727],
        [ 4.4164, -1.2760],
        [ 0.9233,  0.5347],
        [ 1.0698,  1.6187]])</code></pre>
</div>
</div>
<p>We want each row (batch item) to sum to 1.0. In other words, we want to turn these activations into probabilities. Passing each item through sigmoid does not help. While it does constrain the value to 0 and 1 it does so independently across the two classes so they don’t add up to 1.0.</p>
<div id="cell-24" class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_function(f, tx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, ty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)):</span>
<span id="cb23-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>)</span>
<span id="cb23-3">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>)</span>
<span id="cb23-4">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.linspace(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>,<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb23-5">    fig,ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>figsize)</span>
<span id="cb23-6">    ax.plot(x,f(x))</span>
<span id="cb23-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> tx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>: ax.set_xlabel(tx)</span>
<span id="cb23-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> ty <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>: ax.set_ylabel(ty)</span>
<span id="cb23-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> title <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>: ax.set_title(title)</span></code></pre></div>
</div>
<div id="cell-25" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:368}}" data-outputid="11a3618a-ebe2-49c2-937f-82be1c97eebb">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">plot_function(f<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.sigmoid, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-16-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-11-CELoss/index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="cell-26" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="3546e7a1-76ee-4404-98de-87468b39dcbb">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">acts.sigmoid()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor([[0.6623, 0.5641],
        [0.6151, 0.6132],
        [0.0957, 0.4079],
        [0.9881, 0.2182],
        [0.7157, 0.6306],
        [0.7446, 0.8346]])</code></pre>
</div>
</div>
<div id="cell-27" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2c5c3e3c-892d-4292-cd84-8997dba4b3a0">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">acts[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].sigmoid()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>tensor(0.6623)</code></pre>
</div>
</div>
<p>However, taking the difference <em>between the class activations</em> and then passing that through the sigmoid does give us our desired result: probabilities that sum up to 1.0 across classes for each batch item.</p>
<div id="cell-29" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e9b1877f-f9fb-4dd6-f59e-d79f755693ac">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">(acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]).sigmoid()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661])</code></pre>
</div>
</div>
<div id="cell-30" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a57c5464-916e-4088-809c-c728e94b0b4d">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">(acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]).sigmoid()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>tensor([0.3975, 0.4979, 0.8668, 0.0034, 0.4041, 0.6339])</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4a8f3e27-8877-42af-eba9-2fa12ffd447c">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">(acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]).sigmoid() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]).sigmoid()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])</code></pre>
</div>
</div>
<p>Why does this work? We can see why mathematically.</p>
<section id="deriving-softmax-from-sigmoid" class="level3">
<h3 class="anchored" data-anchor-id="deriving-softmax-from-sigmoid">Deriving Softmax from Sigmoid</h3>
<p>We start with taking the sigmoid of the difference between the first and second class activations.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsigmoid%7D(x_1-x_2)%5Ctext%7B%20=%20%7D%5Cfrac%7B1%7D%7B1+e%5E%7B-%7B(x_1-x_2)%7D%7D%7D"></p>
<p>Let’s rewrite the denominator, expanding the exponent term:</p>
<p><img src="https://latex.codecogs.com/png.latex?1+e%5E%7B-%7B(x_1-x_2)%7D%7D%20=%201%20+%20%5Cfrac%7B1%7D%7Be%5E%7B(x_1-x_2)%7D%7D%20=%20%5Cfrac%7B1%7D%7B1%7D+%5Cfrac%7B1%7D%7Be%5E%7B(x_1-x_2)%7D%7D%20=%20%5Cfrac%7Be%5E%7B(x_1-x_2)%7D+1%7D%7Be%5E%7B(x_1-x_2)%7D%7D"></p>
<p>Plugging that back into the original sigmoid fraction and simplifying:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B1+e%5E%7B-%7B(x_1-x_2)%7D%7D%7D%20=%20%5Cfrac%7B1%7D%7B%5Cfrac%7Be%5E%7B(x_1-x_2)%7D+1%7D%7Be%5E%7B(x_1-x_2)%7D%7D%7D%20=%20%5Cfrac%7Be%5E%7B(x_1-x_2)%7D%7D%7Be%5E%7B(x_1-x_2)%7D+1%7D"></p>
<p>Multiplying by a term equivalent to 1 (thanks Gemini):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Be%5E%7B(x_1-x_2)%7D%7D%7Be%5E%7B(x_1-x_2)%7D+1%7D%20%5Ctimes%20%5Cfrac%7Be%5E%7Bx_2%7D%7D%7Be%5E%7Bx_2%7D%7D%20=%20%5Cfrac%7Be%5E%7B(x_1-x_2)%7D%5Ctimes%20e%5E%7Bx_2%7D%7D%7B(e%5E%7B(x_1-x_2)%7D+1)%5Ctimes%20e%5E%7Bx_2%7D%7D%20=%20%5Cfrac%7Be%5E%7B(x_1-x_2+x_2)%7D%7D%7Be%5E%7B(x_1-x_2+x_2)%7D+e%5E%7Bx_2%7D%7D=%5Cfrac%7Be%5E%7Bx_1%7D%7D%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D%7D"></p>
<p>We are left with the softmax calculation for the first class probability. The chapter goes on to say:</p>
<blockquote class="blockquote">
<p>The second column (the probability of it being a 7) will then just be that value subtracted from 1.</p>
</blockquote>
<p>The sigmoid of the difference between the second and first class probabilities is 1 minus the sigmoid of the difference between the first and second class probabilities.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsigmoid%7D(x_2-x_1)%5Ctext%7B%20=%20%7D1%20-%20%5Cfrac%7Be%5E%7Bx_1%7D%7D%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D%7D%20=%20%5Cfrac%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D%7D%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D%7D%20-%20%5Cfrac%7Be%5E%7Bx_1%7D%7D%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D%7D%20=%20%5Cfrac%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D-e%5E%7Bx_1%7D%7D%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D%7D%20=%20%5Cfrac%7Be%5E%7Bx_2%7D%7D%7Be%5E%7Bx_1%7D+e%5E%7Bx_2%7D%7D"></p>
<div id="cell-46" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="197b5713-e23b-405e-d37c-561a6c5b6fd8">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> (acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>acts[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]).sigmoid()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([0.3975, 0.4979, 0.8668, 0.0034, 0.4041, 0.6339])</code></pre>
</div>
</div>
<p>We can also derive the same result with a different approach mathematically, I find this one to be simpler:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsigmoid%7D(x_1-x_2)%5Ctext%7B%20=%20%7D%5Cfrac%7B1%7D%7B1+e%5E%7B-%7B(x_1-x_2)%7D%7D%7D"></p>
<p>Expanding the exponent term in the denominator:</p>
<p><img src="https://latex.codecogs.com/png.latex?e%5E%7B-(x_1-x_2)%7D%20=%20%5Cfrac%7B1%7D%7Be%5E%7B(x_1-x_2)%7D%7D%20=%20%5Cfrac%7B1%7D%7B%5Cfrac%7Be%5E%7Bx_1%7D%7D%7Be%5E%7Bx_2%7D%7D%7D%20=%20%5Cfrac%7Be%5E%7Bx_2%7D%7D%7Be%5E%7Bx_1%7D%7D"></p>
<p>Plugging that back into the denominator and massaging it:</p>
<p><img src="https://latex.codecogs.com/png.latex?1+%5Cfrac%7Be%5E%7Bx_2%7D%7D%7Be%5E%7Bx_1%7D%7D%20=%20%5Cfrac%7Be%5E%7Bx_1%7D%7D%7Be%5E%7Bx_1%7D%7D%20+%20%5Cfrac%7Be%5E%7Bx_2%7D%7D%7Be%5E%7Bx_1%7D%7D%20=%20%5Cfrac%7Be%5E%7Bx_1%7D%20+%20e%5E%7Bx_2%7D%7D%7Be%5E%7Bx_1%7D%7D"></p>
<p>Plugging that back in to the original sigmoid equation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsigmoid%7D(x_1-x_2)%5Ctext%7B%20=%20%7D%5Cfrac%7B1%7D%7B1+e%5E%7B-%7B(x_1-x_2)%7D%7D%7D%20=%20%5Cfrac%7B1%7D%7B%5Cfrac%7Be%5E%7Bx_1%7D%20+%20e%5E%7Bx_2%7D%7D%7Be%5E%7Bx_1%7D%7D%7D%20=%20%5Cfrac%7Be%5E%7Bx_1%7D%7D%7Be%5E%7Bx_2%7D+e%5E%7Bx_1%7D%7D"></p>
<p>Mathematically, with some help from Gemini, that makes sense. But what is the conceptual intuition behind it? Why does taking the sigmoid of <em>the difference between activations</em> equate to softmax?</p>
<p>With loads of help with Gemini, I’ve come to the following conclusions/intuitions:</p>
<ul>
<li>sigmoid is designed for a simple binary outcome: A and not-A, so we can’t use it for more than 2 classes.</li>
<li>the difference in logits is a difference in “learned evidence” between the two classes. The value <code>x_1 - x_2</code> represents the log-odds of class 1 relative to class 2 (i.e., it’s equal to <code>log(P(class 1)/P(class 2))</code>). A difference of <code>0</code> (logits <code>x_1</code> and <code>x_2</code> are equal) passed through sigmoid yields a probability of <code>0.5</code>, meaning both classes are equally probable.</li>
<li>sigmoid is the inverse function of the log-odds (or logit) function. sigmoid(0) = 0.5, and log(0.5/(1-0.5)) = 0.</li>
<li>we call binary classification activations “logits” because we eventually turn them into probabilities using sigmoid.</li>
<li>the softmax function is the general template for all classification problems. For the special case of exactly two classes, this general formula mathematically simplifies to be identical to the sigmoid of the difference between the logits.</li>
<li>For 3+ classes, the model’s final activations are still called logits. However, they no longer have a simple interpretation as the log-odds of a single event. We think of them as some kind of learned “raw” or “uncalibrated” scores that softmax converts into a valid probability distribution.</li>
</ul>
<p>Let’s return back to the process of building up Cross Entropy loss.</p>
</section>
<section id="selecting-the-probabilities-used-and-calculating-their-logarithm" class="level3">
<h3 class="anchored" data-anchor-id="selecting-the-probabilities-used-and-calculating-their-logarithm">Selecting the Probabilities Used and Calculating their Logarithm</h3>
<p>So we have now converted logits into probabilities using softmax.</p>
<blockquote class="blockquote">
<p>Our activations, after softmax, are between 0 and 1, and sum to 1 for each row in the batch of predictions.</p>
</blockquote>
<div id="cell-61" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="659a6f60-3fd6-4033-b6ea-1cd5f18815fe">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">sm_acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.softmax(acts, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb37-2">sm_acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>tensor([[0.6025, 0.3975],
        [0.5021, 0.4979],
        [0.1332, 0.8668],
        [0.9966, 0.0034],
        [0.5959, 0.4041],
        [0.3661, 0.6339]])</code></pre>
</div>
</div>
<p>Okay great, now how do we use these values? We want to penalize the language model’s predictions when they wrong (the target label is not predicted with the highest probability) and especially when they are wrong <em>and</em> confident (the target label is predicted with a very low probability). On the other hand, we want to encourage the model to pick the right class confidently. We achieve this two-pronged behavior by taking the logarithm of the target probability.</p>
<p>Suppose the target classes (<code>0</code> or <code>1</code>) are as follows for the 6-item batch:</p>
<div id="cell-64" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="98ead48f-4666-473d-8ea2-23834f5efd43">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">targ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb39-2">targ</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor([0, 1, 0, 1, 1, 0])</code></pre>
</div>
</div>
<div id="cell-65" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="df800045-6655-4507-da0a-5af91422af66">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">sm_acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor([[0.6025, 0.3975],
        [0.5021, 0.4979],
        [0.1332, 0.8668],
        [0.9966, 0.0034],
        [0.5959, 0.4041],
        [0.3661, 0.6339]])</code></pre>
</div>
</div>
<p>We index into our probalities using <code>idx</code> (the batch item index) and <code>targ</code> (the probability column index).</p>
<div id="cell-67" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="aa0d1726-a44c-48eb-ce80-827a2242df46">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb43-2">sm_acts[idx, targ]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])</code></pre>
</div>
</div>
<div id="cell-68" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:388}}" data-outputid="3560d3c2-3e53-47d3-c2fe-7bc007c5928e">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">plot_function(torch.log, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, ty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log(x)'</span>, tx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-27-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-11-CELoss/index_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The log function is very negative for values close to 0 and less negative for values close to 1. We want our loss function to be very positively large for confidently wrong predictions (i.e.&nbsp;when the target class is predicted with a very low probability) so we want to flip the logarithm’s behavior by multiplying it by -1.</p>
<div id="cell-70" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:410}}" data-outputid="147b1069-226c-4741-e950-8d8808fe8e30">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">plot_function(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>torch.log(x), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, tx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>, ty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'- log(x)'</span>, title <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Log Loss when true label = 1'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-28-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://vishalbakshi.github.io/blog/posts/2025-06-11-CELoss/index_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<div id="cell-71" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="30e9ccac-b0ff-4e76-e05f-2722c489847f">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">sm_acts[idx, targ]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])</code></pre>
</div>
</div>
<div id="cell-72" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d72434ec-eca4-48a6-97db-b2cdf9062e9e">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>torch.log(sm_acts)[idx, targ]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])</code></pre>
</div>
</div>
<p>We now have large loss values (2.016, 5.6958) for batch items when the model predicted the target class with a very low probability (0.1332, 0.0034), and we have a relatively smaller loss value (0.5067) when the model predicted the target class with a higher probability than the incorrect class (0.6025).</p>
<p>The PyTorch <code>F.nll_loss</code> function does the indexing and multiplying by negative 1 operations for us, though it expects you to pass it activations after taking the logarithm of them.</p>
<div id="cell-75" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f01c7be6-2b16-48c4-c93e-545c184964ca">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">F.nll_loss(sm_acts, targ, reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])</code></pre>
</div>
</div>
<div id="cell-76" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="350aacb0-cae9-4606-9f55-849b99d6a93e">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">F.nll_loss(torch.log(sm_acts), targ, reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>As long as the activation columns sum to 1 (as they will, if we use softmax), then we’ll have a loss function that shows how well we’re predicting each digit. Therefore, making the activation for the correct label as high as possible must mean we’re also decreasing the activations of the remaining columns.</p>
</blockquote>
<p>We can also directly use <code>CrossEntropyLoss</code></p>
<div id="cell-79" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="103a97e6-a0a6-4ec7-add9-1b887a9226ee">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">nn.CrossEntropyLoss(reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none'</span>)(acts, targ)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])</code></pre>
</div>
</div>
<div id="cell-80" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fcf7d116-8fdd-49bd-8f43-9ffefa19e5d5">
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1">nn.CrossEntropyLoss(reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean'</span>)(acts, targ)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>tensor(1.8045)</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Taking the mean of the negative log of our probabilities (taking the mean of the loss column of our table) gives us the negative log likelihood loss, which is another name for cross-entropy loss.</p>
</blockquote>
</section>
</section>
<section id="the-gradient-of-cross-entropy-loss" class="level2">
<h2 class="anchored" data-anchor-id="the-gradient-of-cross-entropy-loss">The Gradient of Cross Entropy Loss</h2>
<p>Let’s recreate with code the following explanation of the cross-entropy loss gradient.</p>
<blockquote class="blockquote">
<p>An interesting feature about cross-entropy loss appears when we consider its gradient. The gradient of cross_entropy(a,b) is just softmax(a)-b. Since softmax(a) is just the final activation of the model, that means that the gradient is proportional to the difference between the prediction and the target. This is the same as mean squared error in regression (assuming there’s no final activation function such as that added by y_range), since the gradient of (a-b)**2 is 2*(a-b). Because the gradient is linear, that means we won’t see sudden jumps or exponential increases in gradients, which should lead to smoother training of models.</p>
</blockquote>
<p>First we’ll calculate the gradients using <code>los.backward()</code>.</p>
<div id="cell-86" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="96b8a5fa-789d-4286-e6b1-cc3a55137dc7">
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> acts.requires_grad_()</span>
<span id="cb59-2">acts</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor([[ 0.6734,  0.2576],
        [ 0.4689,  0.4607],
        [-2.2457, -0.3727],
        [ 4.4164, -1.2760],
        [ 0.9233,  0.5347],
        [ 1.0698,  1.6187]], requires_grad=True)</code></pre>
</div>
</div>
<div id="cell-87" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bbdbb429-a3c1-4688-bb9c-3760d869c144">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.CrossEntropyLoss(reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum'</span>)(acts, targ)</span>
<span id="cb61-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>tensor(10.8268, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-88" class="cell">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">loss.backward()</span></code></pre></div>
</div>
<div id="cell-89" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="411b5c8e-1118-4f02-c551-07a2ed885d7e">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">acts.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>tensor([[-0.3975,  0.3975],
        [ 0.5021, -0.5021],
        [-0.8668,  0.8668],
        [ 0.9966, -0.9966],
        [ 0.5959, -0.5959],
        [-0.6339,  0.6339]])</code></pre>
</div>
</div>
<p>Then, we’ll manually calculate them using <code>softmax(acts) - targ</code>:</p>
<div id="cell-91" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="66f4ee79-5af1-45e0-8e3f-b39d48487049">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.one_hot(targ, num_classes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>acts.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb66-2">b</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]])</code></pre>
</div>
</div>
<div id="cell-92" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e656eb87-1b8c-484b-c904-44378d247480">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">acts.softmax(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> b</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>tensor([[-0.3975,  0.3975],
        [ 0.5021, -0.5021],
        [-0.8668,  0.8668],
        [ 0.9966, -0.9966],
        [ 0.5959, -0.5959],
        [-0.6339,  0.6339]], grad_fn=&lt;SubBackward0&gt;)</code></pre>
</div>
</div>
<p>We see that they are equal! If we want to recreate the gradient when <code>reduction='mean'</code> we can divide <code>softmax(a) - b</code> by the batch size.</p>
<div id="cell-94" class="cell">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1">torch.random.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb70-2">acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb70-3">acts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> acts.requires_grad_()</span></code></pre></div>
</div>
<div id="cell-95" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="71064327-04a8-4b21-b19c-2ea8391b406e">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.CrossEntropyLoss(reduction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean'</span>)(acts, targ)</span>
<span id="cb71-2">loss</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor(1.8045, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-96" class="cell">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1">loss.backward()</span></code></pre></div>
</div>
<div id="cell-97" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f6b0b28e-6aeb-4217-fbd1-c13f338d78c2">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">acts.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>tensor([[-0.0663,  0.0663],
        [ 0.0837, -0.0837],
        [-0.1445,  0.1445],
        [ 0.1661, -0.1661],
        [ 0.0993, -0.0993],
        [-0.1056,  0.1056]])</code></pre>
</div>
</div>
<div id="cell-98" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1ec7f93f-2050-4bd2-fc8f-6acfe5b0ff6f">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">(acts.softmax(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> b) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> acts.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>tensor([[-0.0663,  0.0663],
        [ 0.0837, -0.0837],
        [-0.1445,  0.1445],
        [ 0.1661, -0.1661],
        [ 0.0993, -0.0993],
        [-0.1056,  0.1056]], grad_fn=&lt;DivBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>Next-token language generation is fundamentally a classification problem. The “classes” are the tokens in the vocabulary. The model predicts the most probable next token, learning to favor target tokens during training using cross-entropy. This refresher on cross-entropy loss helped me solidify my understanding of how this behavior is reinforced.</p>
<p>I’m growing my YouTube channel so if you like this type of content <a href="https://www.youtube.com/@vishal_learner">please subscribe</a>!</p>


</section>

 ]]></description>
  <category>python</category>
  <category>deep learning</category>
  <category>fastai</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-06-11-CELoss/</guid>
  <pubDate>Wed, 11 Jun 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Proof, Pricing, and Passion: Finding My Path in Machine Learning</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-06-09-fireside-chat/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Z328Nl6c-8k?si=Ey9ehhiHqoCQfmum" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
<p>A recent fireside chat with three successful AI consultants—Jason Liu, Andy Walters, and Vignesh Mohankumar—gave me a much clearer, more realistic perspective on what it takes to build a career in this space. While a previous <a href="https://www.youtube.com/watch?v=nyNQtkiW-ME">paradigm-shifting conversation between Jason Liu and Hamel Husain</a> had inspired me, this conversation answered a critical question I’ve been wrestling with:</p>
<blockquote class="blockquote">
<p>What do you think is the minimum amount of professional experience to have before going full-time consulting?</p>
</blockquote>
<p>I was all ears on this one. Below are the responses from Jason, Vignesh and Andy (emphasis mine)</p>
<blockquote class="blockquote">
<p>Jason: <mark><strong>the thing you want to have is proof.</strong></mark> There are people making a million dollars a year doing Zapier integrations and automations. You can probably just figure that out, just charge very little, do a couple of free jobs, get some proof, maybe if you help a business owner with their automations you can ask if they have anyone they can refer you to, then you get the next job and the next job. I think it depends on what kind of things you’re promising and what kind of solutions you’re promising.</p>
</blockquote>
<blockquote class="blockquote">
<p>Vignesh: I think the fact that I was basically a staff level engineer is really what’s helping me now, but it’s not to say that you don’t need that, I think like you said there’s a lot of things you can do, but <mark><strong>I think for the type of work I’m doing it would be very hard to do if you were less than 5 years of experience, or really, honestly, probably 8</strong></mark>….but it’s about what have you actually done and can you actually prove it.</p>
</blockquote>
<blockquote class="blockquote">
<p>Andy: I started with zero AI knowledge. I was previously a programmer. I never worked at a startup that was successful. I worked at little agencies my whole career. I am where I am today so it’s possible. <mark><strong>It all depends on your skill set. It’s about staffing your weaknesses.</strong></mark> If you want to go big and you’re not good at something if you can partner with somebody who’s good at that thing, now you can move on and go further. It depends on how fast you want to scale, how honest you can be with yourself about your weaknesses and how good you are at acquiring the talent to fill in those weaknesses.</p>
</blockquote>
<p>Jason and Vignesh agreed upon the fact that you can land jobs if you can prove that you can deliver results (at the appropriate scale). For someone like me, who has never worked professionally in ML, getting that proof (at scale, in a setting that emulates a real business) is the tricky part. I would imagine that any proof I can currently collect (research projects, Kaggle competitions, educational content, etc.) will help me get a foot in the door at a company or a lab, which will then unlock my ability to get more proof in a professional setting that could translate to consulting. I could be wrong (tweet at me if I am) but going from 0-to-consulting without any other professional ML experience doesn’t seem likely (or, at least I haven’t seen any examples of this, except for unicorns like Jeremy Howard).</p>
<p>But knowing when to start is only half the battle; understanding the pivotal moments that accelerate a consulting career is just as important, which the speakers addressed next.</p>
</section>
<section id="inflection-points-in-consulting" class="level2">
<h2 class="anchored" data-anchor-id="inflection-points-in-consulting">Inflection points in consulting</h2>
<p>Q: What felt like the inflection point for the way that you were able to bring in revenue or demand higher fees?</p>
<blockquote class="blockquote">
<p>Jason: For me a lot of it was coming down to my writing. At some point it became: “Hey Jason, I’ve seen a couple of your blog posts show up on my Slack, I don’t really know who you are, I finally clicked the website and I realized you were available so can we figure out if it makes sense to work together?” And then I would share another blog post after the call and they would be like “turns out my CTO already read this, what is your availability?”</p>
</blockquote>
<blockquote class="blockquote">
<p>Vignesh: There was one month where I got a project and I finished it in a week or two and it was more money than I had made if I just worked full-time, hourly, for a month and a half. It was a really pressing problem that came up. I realized I’d rather sit there for 29 days a month and find a project-based thing that I could do for one day a month, then work hourly. That’s what shifted it for me.</p>
</blockquote>
<blockquote class="blockquote">
<p>Jason: Going from time-based to project-based pricing, and then getting comfortable understanding the value that your project delivers and increase your fee to reflect the size of the problem.</p>
</blockquote>
<p>In the Hamel/Jason discussion on AI consulting, they talk about how publishing free content is a viable marketing plan. This really resonated with me as a fast.ai community member, encouraged by <a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">Rachel Thomas to blog</a> since “it’s like a resume, only better.” After I watched Jason and Hamel’s discussion, I immediately set a goal to publish 50 ML vidoes and 50 ML blog posts in 2025, and it has been driving me forward ever since. Reading that Jason’s leads often come from people reading his blog further solidified by belief that this is the way. Obviously, what you’re writing about matters, but I think that catches up with quantity over time.</p>
<p>Both of these inflection points are about detaching income from hours and tying it to the value you create. But what’s the upper limit? This led to an interesting exchange about whether AI is removing the ceiling on how much impact a single consultant can have.</p>
</section>
<section id="is-there-a-ceiling-for-one-consultant" class="level2">
<h2 class="anchored" data-anchor-id="is-there-a-ceiling-for-one-consultant">Is there a ceiling for one consultant?</h2>
<p>I’ve heard Jason say some iteration of the following in a couple of videos now:</p>
<blockquote class="blockquote">
<p>Jason: I would be surprised if there’s anybody I know that can be like an individual person even doing $10MM a year just feels very difficult whereas if you just have a couple of small operational staff you can probably get pretty far.</p>
</blockquote>
<p>This was the first video (that I’ve watched) where someone pushed back on that.</p>
<blockquote class="blockquote">
<p>Vignesh: If these tools are going to keep getting better, then I don’t know if there is a limit anymore at some point.</p>
</blockquote>
<p>I think this both an inspiring and realistic take. My own self-prescribed limits have continuously been broken with the help of AI. It’s hard for me to quantify how much my growth in ML has accelerated due to AI (as I obviously don’t have a control to compare for that) but anecdotally, every month and every year that I’ve increased my use of AI to augment my learning and implementation skills, I’ve accomplished tasks that I didn’t expect to. You could argue that every project, blog post, and video I’ve published could not have been possible without the support of AI. How this translates to getting paid (and how much I get paid), only time will tell. Going back to the previous topic on when to start consulting, I think the use of AI can help build that proof needed for a 0-to-consulting trajectory. It may require doing a couple free or low-paying jobs, but I think that’s reasonable to expect. This is probably a controversial take, and I’m not condoning or advocating for exploitation, but I do think that when you work for free as a beginner, you are “paying” (with your labor) to get that proof needed to unlock paid work. Working for free can also look like joining a study group/discord server and taking on tasks that need to be done.</p>
<p>While the potential to scale with AI is an exciting thought experiment, financial upside is rarely the sole motivator for taking the risk of going independent. The conversation also explored the fundamental ‘why’ behind this career path, and Vignesh’s reasons resonated deeply with my own.</p>
</section>
<section id="what-attracts-me-to-consulting" class="level2">
<h2 class="anchored" data-anchor-id="what-attracts-me-to-consulting">What attracts me to consulting</h2>
<blockquote class="blockquote">
<p>Q: Why didn’t you join a startup (or starting a company) versus doing your own thing?</p>
</blockquote>
<blockquote class="blockquote">
<p>Vignesh: I just loving working on new problems all of the time, ideally multiple problems at the same time. I think consulting is really interesting because you can price based on the actual value you’re delivering. That keeps me going in some ways—what’s the most valuable things I can be working on? How can I help people the most? The last piece is that I just love owning the brand. Anything I’m writing, anyone I’m meeting is all tied to me…I scale it by finding more valuable projects and more important problems.</p>
</blockquote>
<p>All of these points resonate with me. As a hobby ML researcher, I value the following freedoms:</p>
<ul>
<li>The freedom to start, pause, or stop a project at will.</li>
<li>The freedom of learning and building in public without the constraints of private, proprietary work.</li>
<li>The freedom to choose my focus.</li>
<li>The freedom to control my pace—going slow. Going fast. Going deep. Staying shallow. I choose at all times which of these modes I’m in.</li>
<li>The freedom from an obsession with results, allowing me to instead chase ideas, concepts, and their clear explanation.</li>
</ul>
<p>The cost of these freedoms, currently, is that I do this work for free. Finding a situation where I can continue to have these freedoms is my next challenge.</p>
<p>Vignesh’s drive for autonomy and interesting problems perfectly mirrors the freedoms I value as a hobby researcher. But what I found most encouraging was the discussion that followed, which highlighted that his path isn’t the only one; different personalities and preferences can also lead to fulfilling consulting careers.</p>
</section>
<section id="on-different-paths-to-consulting" class="level2">
<h2 class="anchored" data-anchor-id="on-different-paths-to-consulting">On different paths to consulting</h2>
<p>Vignesh and Andy had diametrically opposed preferences and paths to consulting (emphasis mine):</p>
<blockquote class="blockquote">
<p>Vignesh: I like coding a lot. I love writing code. I love learning things. So it works. It’s like my dream.</p>
</blockquote>
<blockquote class="blockquote">
<p>Andy: I think I’m different than Vignesh in that like I was an okay engineer, but I was never a great engineer, and I really enjoyed the relationship aspect of it, leading people, that kind of stuff, that’s actually more fun to me. I like meetings…I enjoy it. <mark><strong>It’s all about figuring out what you really enjoy and pushing on that a lot.</strong></mark></p>
</blockquote>
<p>I think it was awesome and important for viewers to see these diverse preferences both leading to fulfilling consulting careers. Ultimately, while landing paid work is goal 1B, doing what I enjoy and excites me is 1A and this confirmed that motivation.</p>
<p>The idea that you should build a career around what you genuinely enjoy is a powerful one. And what makes this path so compelling right now is the shared belief among all the speakers that the opportunity is vast enough to accommodate all of these different approaches.</p>
</section>
<section id="the-work-is-literally-falling-off-trucks" class="level2">
<h2 class="anchored" data-anchor-id="the-work-is-literally-falling-off-trucks">The work is literally falling off trucks</h2>
<p>I’ll end with Andy’s closing thoughts:</p>
<blockquote class="blockquote">
<p>Andy: I think the biggest lesson I’ve learned through all of this is it’s all about breaking your context into the next level. There is this cascading series of contexts you have to break yourself into and it’s always uncomfortable and it feels painful and risky…keep pushing, keep growing, and even when it doesn’t work the first time you just keep pulling the thread. It’s surprising how far you can get. We firmly believe that over the next 5 years there’s at least a trillion dollars of integration services on the table for companies to integrate AI into their offerings. The work is literally falling off the trucks. It’s out there to seize, go seize it.</p>
</blockquote>
<p>When you are constantly pushing yourself to grow and take on the next challenging context, you will find yourself working at the edge of your knowledge and capacity. This is an ideal place to be. However, the downside is never truly knowing or feeling how much you have progressed. It’s a constant state of “I am stuck on X which I need to resolve so I can get to Y”. I think documenting your journey (through writing blog posts/tweets and creating videos) helps balance this bleeding-edge approach by concretely and quantitatively showing you what you have learned and how much you have achieved.</p>
<p>Andy’s point that “the work is literally falling off the trucks” should be inspiring to everyone, regardless of whether or not you want to pursue consulting. I think we are still in the very early stages of AI adoption, which means that there is a lot of implementation work that needs to get done. It can seem (falsely) that the field of AI is accelerating at an unsustainable pace, but what is unsustainble in my opinion is that veneer. The fundamentals still matter. Communicating concepts clearly still matters. These are relatively low-hanging fruit.</p>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing thoughts</h2>
<p>I thoroughly enjoying this fireside chat. I came away from it with more questions and some answers.</p>
<p>To be honest, I don’t know where my ML journey is going to take me. When I’m feeling optimistic, I imagine myself working in an organization or independently with the same joy that I find myself working on my personal projects today. On my worst days, I wonder if I’ll ever leave the purgatory of hobby ML. I think the key to my success is to be consistent regardless of what I’m feeling.</p>
<p>Hearing Andy talk about staffing weaknesses and Vignesh about AI raising the ceiling made me reflect on my own approach. When it comes to using AI, and what capabilities it will unlock for me, I keep waiting for a “choir of angels” (<a href="https://xkcd.com/310/">xkcd</a>) but instead have found a steady plodding forward, task to task. I think Vignesh is right about AI removing the ceiling, and Andy is right about supplementing your weaknesses with others’ strengths. Combining these two ideas, I think my task is to become more aware about 1) what my current ceiling is/what my current weaknesses are and 2) understanding how to augment them with AI. The sycophantic nature of AI makes it difficult to use it for identifying my weaknesses, but I’ve found success by asking Claude to be a “hardass” (and then asking it to critique the hardass), or using Gemini 2.5-Pro for such questions, which I find is more straightforward and informative and less of a coach/cheerleader.</p>
<p>I’ll end by saying that I don’t usually write posts like these, and keep these thoughts to myself, but I’m finding that to strengthen my relationship with my audience I will have to share more of myself in my writing. This is the first of what I hope will be many posts connecting what I’m learning from experts in the field to my own journey.</p>
<p>You can reach me on <a href="https://x.com/vishal_learner">Twitter</a> and find more content on my <a href="https://www.youtube.com/@vishal_learner">YouTube channel</a>.</p>


</section>

 ]]></description>
  <category>Career</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-06-09-fireside-chat/</guid>
  <pubDate>Mon, 09 Jun 2025 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
