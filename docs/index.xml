<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Vishal Bakshi&#39;s Blog</title>
<link>https://vishalbakshi.github.io/blog/</link>
<atom:link href="https://vishalbakshi.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Machine Learning blog by Vishal Bakshi</description>
<generator>quarto-1.7.32</generator>
<lastBuildDate>Wed, 03 Sep 2025 07:00:00 GMT</lastBuildDate>
<item>
  <title>My Top-5 Blog Posts I’ve Written this Year</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-09-03-top-5-own-blog-posts/</link>
  <description><![CDATA[ 




<p>I recently surpassed my goal of publishing 50 machine learning blog posts in 2025. I shared that on Twitter and got this interesting question from Skylar Payne:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Do you have one that is your favorite or one you are most proud of?
</p>
— Skylar Payne (<span class="citation" data-cites="skylar_b_payne">@skylar_b_payne</span>) <a href="https://twitter.com/skylar_b_payne/status/1963262456811205081?ref_src=twsrc%5Etfw">September 3, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I browsed through the blog posts I’ve published this year, and I found it really hard to pick just one because each blog post either taught me something new (I learn through writing) or gave me an opportunity to try something new (writing style, blog post topic or technical project) that I was scared to do in public. They all served a purpose!</p>
<p>So instead I am picking my top 5 in no particular order.</p>
<ol type="1">
<li><a href="https://vishalbakshi.github.io/blog/posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/">RAGatouille/ColBERT Indexing Deep Dive</a>: This was a critical deep dive in my understanding of the ColBERT Library, and is the main reason why I am able to navigate the codebase today as a maintainer.</li>
<li><a href="https://vishalbakshi.github.io/blog/posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/">Memory Profiling raw ColBERT and RAGatouille</a>: This was the first time I had done non-trivial memory profiling, and it also served as a great mechanism to better understand the ColBERT and RAGatouille codebases.</li>
<li><a href="https://vishalbakshi.github.io/blog/posts/2025-06-09-fireside-chat/">Proof, Pricing, and Passion: Finding My Path in Machine Learning</a>: This was the first career-focused blog post I had published, which was a big step for me in sharing thoughts from a point of vulnerability publicly.</li>
<li><a href="https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/">An Analysis of Batch Size vs.&nbsp;Learning Rate on Imagenette</a>: This was an important exploration as it built my fundamental intuition around the relationship between batch size, learning rate, and downstream performance (accuracy).</li>
<li><a href="https://vishalbakshi.github.io/blog/posts/2025-06-26-portfolio-llm/">Introducing portfolio-llm: A Professional Portfolio You Can Chat With</a>: I don’t consider myself an innovative applied AI thinker so this project was a breakthrough for me because I feel like it’s the first interesting applied AI idea I’ve had.</li>
</ol>
<p>This was a fun exercise, and definitely something I’ll do again at the end of the year and routinely moving forward. It gave me a moment to reflect and appreciate the progress that I’ve made in my ML journey.</p>



 ]]></description>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-09-03-top-5-own-blog-posts/</guid>
  <pubDate>Wed, 03 Sep 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>PyTorch Version Impact on ColBERT Index Artifacts: 2.7.1 –&gt; 2.8.0</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-09-02-colbert-maintenance/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In a <a href="https://vishalbakshi.github.io/blog/posts/2025-08-18-colbert-maintenance/">previous blog post</a> I showed how I traced index artifact differences between <code>colbert-ai</code> installs using <code>torch==1.13.1</code> (the current pinned version) and <code>torch==2.1.0</code> (the first version which produces different index artifacts) to a difference in floating point differences in the forward pass of the underlying <code>BertModel</code>.</p>
<p>In a <a href="https://vishalbakshi.github.io/blog/posts/2025-08-26-colbert-maintenance/">subsequent blog post</a> I showed how the index artifact differences between <code>colbert-ai</code> installs using <code>torch==2.4.1</code> and <code>torch==2.5.0</code> (the next two versions with differences) was due to floating point divergence in BERT’s intermediate linear layer under mixed precision with small batch sizes.</p>
<p>In this blog post, I’ll show how the index artifacts differencces between <code>torch==2.7.1</code> and <code>torch==2.8.0</code> is due to floating point differences between half precision normalized centroid tensors.</p>
</section>
<section id="index-artifact-comparison" class="level2">
<h2 class="anchored" data-anchor-id="index-artifact-comparison">Index Artifact Comparison</h2>
<p>There are two index artifacts that are different between <code>colbert-ai</code> installs using <code>torch==2.7.1</code> and <code>torch==2.8.0</code>: centroids.pt and the related residuals.pt (the difference between document token embeddings and centroids). This divergence does <mark>NOT</mark> result in a divergence in the critical <code>ivf.pt</code> (document token IDs) and <code>values</code> (centroid IDs) tensors. In other words, the most important mapping from document token IDs to centroid IDs does not change even though centroids floating point values change enough to fail <code>torch.allclose</code>.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sampled_pids</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>num_passages</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>local_sample_embs</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>centroids</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>bucket_cutoffs</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>bucket_weights</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>avg_residual</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>residuals</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>sample</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>sample_heldout</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>embs</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>doclens</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>codes</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>ivf</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>values</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>tensorize_output</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>batches</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>D</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
</section>
<section id="inspecting-centroids.pt" class="level2">
<h2 class="anchored" data-anchor-id="inspecting-centroids.pt">Inspecting <code>centroids.pt</code></h2>
<p>I added the following <code>torch.save</code> calls inside <a href="https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/collection_indexer.py#L280"><code>CollectionIndexer._train_kmeans</code></a>:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> do_fork_for_faiss:</span>
<span id="cb1-2">    ...</span>
<span id="cb1-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-4">    args_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> args_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [[[sample]]]</span>
<span id="cb1-5">    centroids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_faiss_kmeans(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>args_)</span>
<span id="cb1-6">    torch.save(centroids, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>ROOT<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/prenorm_centroids.pt"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ADDED BY VISHAL</span></span>
<span id="cb1-7">    centroids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.functional.normalize(centroids, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-8">    torch.save(centroids, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>ROOT<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/postnorm_centroids.pt"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ADDED BY VISHAL</span></span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.use_gpu:</span>
<span id="cb1-11">    centroids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> centroids.half()</span>
<span id="cb1-12">    torch.save(centroids, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>ROOT<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/half_centroids.pt"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ADDED BY VISHAL</span></span>
<span id="cb1-13"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-14">    centroids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> centroids.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span></code></pre></div>
<p>I then compared <code>prenorm_centroids.pt</code>, <code>postnorm_centroids.pt</code> and <code>half_centroids.pt</code> between both <code>colbert-ai</code> installs using <code>torch.allclose</code>:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">prenorm_centroids.pt torch.allclose:    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-2">prenorm_centroids.pt MAD: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>   <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">postnorm_centroids.pt torch.allclose:   <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-5">postnorm_centroids.pt MAD: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.014875902378037e-10</span>        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb2-6"></span>
<span id="cb2-7">half_centroids.pt torch.allclose:       <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb2-8">half_centroids.pt MAD: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">9.313225746154785e-10</span>    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span></code></pre></div>
<p>The pre-normalization and post-normalization centroids are identical across torch versions, but the <mark>half precision normalized centroids</mark> diverge.</p>
</section>
<section id="inspecting-.half-behavior" class="level2">
<h2 class="anchored" data-anchor-id="inspecting-.half-behavior">Inspecting <code>.half</code> Behavior</h2>
<p>Are all half precision tensors different across torch versions? No.&nbsp;There are a number of index artifacts that are converted to half precision during indexing and are identical between torch versions: <code>avg_residual.pt</code>, <code>D.pt</code>, <code>bucket_weights.pt</code>, and <code>embs.pt</code>.</p>
<p>Furthermore, I created a random tensor, its half precision version, and its normalized version (full and half precision)…</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">torch.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>)</span>
<span id="cb3-2">t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.empty(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>).uniform_(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>)</span>
<span id="cb3-3">torch.save(t, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"t.pt"</span>)</span>
<span id="cb3-4">torch.save(t.half(), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"half_t.pt"</span>)</span>
<span id="cb3-5"></span>
<span id="cb3-6">t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.functional.normalize(t, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-7">torch.save(t, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"norm.pt"</span>)</span>
<span id="cb3-8">torch.save(t.half(), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"half_norm.pt"</span>)</span></code></pre></div>
<p>…and compared it between torch versions:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>t.pt</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>half_t.pt</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>norm.pt</code></td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>half_norm.pt</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
</tr>
</tbody>
</table>
<p>The half precision random tensors (before normalization) are identical between torch versions but the half precision normalized tensors are not. <mark>It was not apparent from a cursory review of the <a href="https://github.com/pytorch/pytorch/releases/tag/v2.8.0">PyTorch Release 2.8.0 Release Notes</a> what caused this behavior.</mark> Sonnet 4 is confident it’s due to PyTorch PR <a href="https://github.com/pytorch/pytorch/pull/153888">#153888</a> (upgrade cuDNN frontend submodule to 1.12) but that could just be a shot in the dark and I can’t verify it.</p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>I have now identified what causes index artifacts to diverge between the three pairs of PyTorch versions in question (1.13.1 –&gt; 2.1.0, 2.4.1 –&gt; 2.5.0, and 2.7.1 –&gt; 2.8.0). Next I will inspect search related artifacts and understand where there are differences and why. Once that’s complete, I’ll look into training artifacts. Finally, I’ll test index, search and training for different Python versions (3.9, 3.10, 3.11, 3.12, and 3.13). Unless something else emerges in my analysis, after Python version testing is complete, I’ll be able to push the next release of <code>colbert-ai</code> with the dependency change from <code>"torch==1.13.1"</code> to (most likely) <code>"torch&gt;=1.13.1,&lt;=2.8.0"</code>.</p>


</section>

 ]]></description>
  <category>ColBERT</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-09-02-colbert-maintenance/</guid>
  <pubDate>Tue, 02 Sep 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>A Summary of and My Thoughts on the DocWrangler Paper</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-09-01-DocWrangler/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I’ll summarize the main points from the <a href="https://arxiv.org/abs/2504.14764">“Steering Semantic Data Processing with DocWrangler” paper by Shreya Shankar et al</a> and share my commentary (<a href="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/">something I’ve been doing more of lately</a>).</p>
<p>This work is inspiring and fascinating. Shreya previewed DocWrangler during the AI Evals course, but reading the paper—especially the user study section—helped me grasp its magnitude. While I lack formal data visualization training, what I’ve read (like <a href="https://vishalbakshi.github.io/blog/posts/2023-05-20-visualization-analysis-and-design/">Visualization Analysis &amp; Design by Tamara Munzner</a>) taught me about the fundamental building blocks of data, UI, analysis goals, and their relationships. For example:</p>
<blockquote class="blockquote">
<p>Search can be classified according to whether the identity and location of targets are known or not</p>
<ul>
<li>both are known with <em>lookup</em></li>
<li>the target is known but its location is not for <em>locate</em></li>
<li>the location is known but the target is not for <em>browse</em></li>
<li>neither the target nor the location are known for <em>explore</em></li>
</ul>
</blockquote>
<p>or:</p>
<blockquote class="blockquote">
<p>The intent of the user is to generate new material.</p>
<p>There are three kinds of produce goals:</p>
<ul>
<li>annotate (adds a new attribute to the data)</li>
<li>record (saves visualization artifacts as persistent artifacts)</li>
<li>derive (produce new data elements based on existing data elements)</li>
</ul>
</blockquote>
<p>Reading the DocWrangler paper felt similar—like discovering building blocks of a fundamentally new paradigm. LLMs enabling large-scale data analysis is already paradigm-shifting, so our analysis methods should match that novelty. Exciting time to work with data!</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Paper Quotes Will be Collapsible
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>I’m trying out collapsible sections for paper quotes so that it shortens the blog post.</p>
</div>
</div>
</div>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The LLM Paradox
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>…building effective semantic data processing pipelines presents a departure from traditional data pipelines: <mark>users need to understand their data to write effective pipelines, yet they need to construct pipelines to extract the data necessary for that understanding</mark>…</p>
</div>
</div>
</div>
<p>With structured data, you have deterministic algorithms (groupby, aggregate, filter) available through stable APIs like Pandas. With unstructured data and LLMs, no stable API exists. Anthropic doesn’t have a reference page for semantic data processing—just <a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview">guidelines on structuring prompts</a>. These guidelines may or may not work depending on your model, task, and data. Success requires iteration: prompt → inspect results → improve prompt. Each pipeline step affects the next, accumulating ambiguity throughout.</p>
<p>Large collections of unstructured text (emails, documents, reports, transcripts) contain variations of similar data. An email thread about a project might refer to tasks using different words and phrases. Company reports express sentiment differently across documents. Compare this to a 5-point Likert scale from a survey. Before identifying characteristics in unstructured data, you must interpret how different variations of the same characteristic are expressed. This requires processing lots of information—where LLMs excel. But determining which concrete steps, in which order, with which prompts requires going back and forth between understanding your data and constructing/improving your pipeline.</p>
<p>Fellow fast.ai students will recognize this philosophy: you often need a quick and dirty model to view actual vs.&nbsp;predicted results before identifying what needs data cleaning.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
DocWrangler is an IDE
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>…[DocWrangler is] a mixed-initiative integrated development environment (IDE) for semantic data processing…</p>
</div>
</div>
</div>
<p>It’s important to highlight the difference between DocWrangler and DocETL. DocWrangler is the graphical user interface where Shreya and her team have designed intentionally a specific set of interaction components to construct semantic data processing pipelines. It is an opinionated frontend with specific goals. DocETL is the back-end which runs the pipeline as <a href="https://ucbepic.github.io/docetl/tutorial/#creating-the-pipeline:~:text=Create%20a%20file%20named%20pipeline.yaml%20with%20the%20following%20structure%3A">defined by a YAML file</a>. In theory, you can write whatever front-end you want on top of DocETL.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Semantic Data Processing
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>semantic data processing: a paradigm where users can instruct LLMs to manipulate data through familiar data processing operators like map, reduce, filter, and groupby.</p>
</div>
</div>
</div>
<p>Even though LLMs don’t have a deterministic, stable API to perform common algorithms, we can still use them for that end. Say we have a collection of 10,000 documents, we can use a semantic <code>map</code> “to extract mentions of medications and reported side effects, followed by a semantic reduce to summarize effects per medication.”</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Three Gulfs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><mark>gulf of comprehension</mark>: Documents contain too much information for humans to fully process [and for LLMs to accurately process].</p>
<p><mark>gulf of specification</mark>: users must first discover their true intent—often only possible after exploring sufficient data to understand what questions the data can reasonably answer.</p>
<p><mark>gulf of generalization</mark>: even with clear, unambiguous prompts, LLMs may fail to generalize correctly to the user’s actual data</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="The Three Gulfs Framework"><img src="https://vishalbakshi.github.io/blog/posts/2025-09-01-DocWrangler/1.png" class="img-fluid figure-img" alt="The Three Gulfs Framework"></a></p>
<figcaption>The Three Gulfs Framework</figcaption>
</figure>
</div>
<p>The Three Gulfs Framework drives DocWrangler’s design philosophy. The comprehension gulf motivates using LLMs, but LLMs aren’t a silver bullet. The specification gulf connects to the LLM paradox—deciding whether to apply semantic map or reduce requires knowing your data’s contents. In medical notes, some contain brand names, others generic names, others medication classes. You need to run map operations and examine samples to identify these patterns before crafting your final prompt and pipeline. The generalization gulf highlights LLMs’ fundamental limitation: they struggle with out-of-domain data.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
DocWrangler Features Address the Three Gulfs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The <mark><strong>in-situ user notes</strong></mark> feature tackles the comprehension gulf by enabling users to annotate observations directly on both documents and outputs. The <mark><strong>LLM-assisted prompt refinement</strong></mark> feature addresses the specification gulf through an interactive interface where an LLM analyzes the pipeline, documents, outputs, and user notes to suggest more effective prompts. The <mark><strong>LLM-assisted operation decomposition</strong></mark> feature targets the generalization gulf by identifying when the pipeline is inadequate for the documents, using an LLM-as-judge that runs in the background.</p>
</div>
</div>
</div>
<p>In-situ user notes attach as text attributes to specific data items, providing context when the LLM helps refine prompts or suggest task decomposition. I think the quality of in-situ user notes drives the entire effort to bridge the three gulfs.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How Users Intuitively Resolve the LLM Paradox
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Users also employed intentionally vague prompts in map operations to learn more about their data, reminiscent of epistemic actions, i.e., actions taken not to directly achieve a goal but to gather information that reveals new possibilities.</p>
</div>
</div>
</div>
<p>I want to highlight this because 1) this behavior stems from DocWrangler’s design, and 2) it demonstrates good problem-solving skills. I often run intermediate code while trying to understand relationships between high-level abstractions and low-level functionality in a codebase. Intentionally vague prompts expose the LLM’s and data’s “tells”—what does the LLM naturally parse from the data? What data characteristics work best with LLM processing? A common theme: the study users are smart, and DocWrangler facilitates smart decisions.</p>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related Work</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The LLM Stability-Capability Tradeoff
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In semantic data processing, LLMs aren’t just writing scripts in a traditional data processing language, they provide entirely new black-box capabilities for unstructured data transformations.</p>
</div>
</div>
</div>
<p>Returning to my point about stable APIs for structured data: LLMs offer a unique trade-off. No stable API, but functionality well-suited for unstructured data—summarization, theme extraction, sentiment analysis. My imagination around LLM capabilities is limited by my traditional data analysis background. Reading this paper, especially the user study, improved my understanding of how to interact with LLMs and leverage their semantic data processing capabilities.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How Users Intuitively Resolve the LLM Paradox
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>However, we lack general-purpose interfaces for semantic data processing across diverse document and operator types. Designing such interfaces is not straightforward, as users encounter the “gulf of envisioning”—<mark>the cognitive gap between having a goal and translating it into effective LLM instructions—while also understanding how to evaluate whether the output meets their original intentions</mark>.</p>
</div>
</div>
</div>
<p>I’m tired of the generic chatbot interface—the same blank screen with a narrow text box, buttons for tools/thinking/conciseness, and uniform static messages. Occasionally Claude generates an artifact. As someone without UI design expertise, I’ve wondered why we see this same interface everywhere, even for domain-specific tasks. This excerpt answers that question: it’s hard to design interactive LLM interfaces that alternate between user goals and LLM-generated outputs.</p>
</section>
<section id="docetl-background-and-example" class="level2">
<h2 class="anchored" data-anchor-id="docetl-background-and-example">DocETL Background and Example</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What’s in a DocETL LLM Operation?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Each LLM-powered operator is defined through two components: a natural language prompt that specifies what the operation should do, and an output schema that determines the structure of data the LLM should generate.</p>
</div>
</div>
</div>
<p>Every experience I’ve had with structured data responses has been positive. I discovered this concept through FastHTML’s <code>__ft__</code> method for dataclasses, then later through Anthropic’s XML response format documentation. Having an output schema makes you think about your needs—what data and data types fulfill them, and what format works for post-processing. In DocETL pipelines, the output schema determines the input schema for the next operation.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The <code>resolve</code> Operator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><code>resolve</code> (performs entity resolution and canonicalization across documents).</p>
</div>
</div>
</div>
<p>Highlighting this operator because I was not familiar with the terms ‘entity resolution’ (figuring out when different text references actually refer to the same real-world thing) and ‘canonicalization’ (converting these different variations into one standard, consistent form). One example could be resolving different medication names, generic or brand name.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Data Presentation Facilitates Different Analyses
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>showing aggregates first helps users identify patterns, while enabling drill-down into specific examples supports verification</p>
</div>
</div>
</div>
<p>Quick aside: I generally dislike dashboards because they facilitate bloated data presentation. I’ve repeatedly seen people equate visualization quantity with quality—like showing you have data is itself a feat. Sometimes it is, especially when organizations try to change their data culture. But after seven years as a data analyst, I think 90% of data presentation only requires (and is most effective with) simple tables or bar plots. Start with high-level aggregate tables or bar plots, then drill down to lower-level categories as you scroll. <em>chef’s kiss</em>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Five DocWrangler Design Goals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>D1. <strong>Scaffold Pipeline Initialization</strong>: Help users create and configure <mark>operations</mark> with minimal friction, with built-in guidance and quick experimentation.</li>
<li>D2. <strong>Facilitate Efficient Data Inspection and Notetaking</strong>: Enable users to <mark>validate</mark> inputs and outputs individually and in aggregate, while supporting note-taking to capture insights and patterns.</li>
<li>D3. <strong>Guide Pipeline Improvement</strong>: Offer assistance for translating user feedback into effective <mark>pipeline modifications</mark>, both at the individual operation level (e.g., prompt improvements) and pipeline level (e.g., operation decomposition).</li>
<li>D4. <strong>Maintain End-to-End Observability</strong>: Ensure <mark>transparency</mark> into transformation logic at each pipeline step (e.g., inputs, outputs, LLM prompts).</li>
<li>D5. <strong>Minimize Context Switching</strong>: Integrate all essential analytical capabilities within a <mark>unified interface</mark>, minimizing the need for external tools (e.g., spreadsheets, custom scripts, AI assistants like ChatGPT).</li>
</ul>
</div>
</div>
</div>
<p>Combining the highlighted terms into a single phrase: the goal of Doc Wrangler is to operationalize semantic data processing with built-in data validation and annotation, LLM-assisted pipeline modification, and full pipeline transparency in a unified interface.</p>
</section>
<section id="docwrangler-system" class="level2">
<h2 class="anchored" data-anchor-id="docwrangler-system">DocWrangler System</h2>
<p>I’m planning on doing a video demo of DocWrangler so I won’t go into details about the interface from the paper. Instead, I will highlight a couple of aspects of operation decomposition which I think is the most interesting part of the system. Mainly because task decomposition is something that I’ve been thinking a lot about recently. On that note, <a href="https://x.com/sh_reya/status/1957499705321210106">here’s a tweet from Shreya</a> that gives the best explanation and motivation for task decomposition that I’ve come across yet:</p>
<blockquote class="blockquote">
<p>What struck us was that these weren’t prompt-engineering problems. They were structural decomposition problems. Every serious task required breaking down into sub-task, where, crucially, each sub-task is something that an LLM can reliably do. But even expert engineers couldn’t predict the right decomposition without long cycles of trial and error. This is what makes LLM pipelines different from SQL.</p>
<p>In a database, users declare what they want, and the optimizer finds a good plan. We can assume the query itself is correct. For LLMs, we can’t even write a pipeline that works “as is” on state-of-the-art models. So the optimizer must generate and test different pipelines on the user’s behalf. Optimizers need to test many different logical rewrites of the initial pipeline to see what will work best.</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Operation Decomposition Feature
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>While they are inspecting symptom outputs, DocWrangler notifies the user (Fig. 6A) that the operation may be too complex (D3). Clicking on the notification triggers the the Operation Decomposition feature. A dialog appears, showing examples of incorrect LLM results when handling both discomfort assessment and symptom extraction simultaneously (Fig. 6B). The analyst clicks “Automatically Decompose” (Fig. 6C), and the system transparently streams its accuracy optimization process (Fig. 6D), evaluating different candidate plans with LLM-as-judge evaluators [105] (D4).</p>
</div>
</div>
</div>
<p>I like how data validation is built into the operation decomposition feature. The LLM presents evidence for why it thinks the task should be decomposed. The accuracy optimization process involves different candidate paths evaluated by an LLM judge, which is a really interesting way to use LLMs for pipeline optimization.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Managing Context Windows for AI-Assisted Features
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For each document, we remove text from the middle while preserving beginnings and endings, replacing the removed content with an ellipsis. Essentially, as the conversation history grows, documents progressively lose more middle content to accommodate new messages within the context window. We specifically preserve document beginnings and endings because introductions typically contain key metadata and conclusions often summarize content, both important for maintaining document context for the LLM.</p>
</div>
</div>
</div>
<p>This made me think about human document design. From elementary school, we learn to structure papers with introduction, thesis, body paragraphs (one per supporting point), and conclusion. We’ve collectively standardized information structure, even in unstructured formats. This enables clever removal of likely low-importance content to fit context windows.</p>
</section>
<section id="user-study-findings" class="level2">
<h2 class="anchored" data-anchor-id="user-study-findings">User Study Findings</h2>
<p>This is my favorite section of the paper, and I’m excited to share what I found particularly informative and interesting.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How many users do you need?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>even five participants can uncover valuable usability insights</p>
</div>
</div>
</div>
<p>I didn’t know research showed you only need 5 participants for valuable usability insights. Makes sense though—in my 3-5 person teams, getting their feedback on reports or data products significantly improves the output. This 5-person threshold makes UX research accessible for bootstrapped teams.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prompting goals beyond raw outputs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To better understand LLM behavior at a glance, participants often adjusted operation outputs for interpretability…these added attributes (e.g., rationales, summaries, indicators) were not used as final task outputs. Instead, they served to help participants verify whether the LLM had correctly interpreted their intent—bridging the specification gulf.</p>
</div>
</div>
</div>
<p>Returning to the LLM paradox: users cleverly augmented LLM outputs with interpretability clues to better understand how the LLM handles tasks, enabling better decisions about improving prompts or decomposing operations. This interpretability output also informs the LLM during prompt refinement or operation decomposition assistance. This exemplifies the stability-capability tradeoff I mentioned—sure, it takes fuzzy, squishy data processing to understand LLM behavior, but that fuzziness provides information richness you can’t get with structured data.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Binary Classification + Likert Scale Use!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>when analyzing doctor-patient trust, P1 initially used a free-form <code>trust_summary</code> attribute, but added a boolean trust attribute to validate results more easily via a histogram. As shown in Fig. 8, the LLM labeled all examples as “true,” so P1 switched to a 5-point Likert scale for more granularity</p>
</div>
</div>
</div>
<p>I’m always interested in effective Likert scale use (usually find them ineffective) and examples of binary classification (easier for LLMs). This excerpt highlights both. The <code>trust_summary</code> became a boolean <code>trust</code> attribute, which when all labeled “true” led to a 5-point Likert scale revealing proper trust distribution. Another example where structured data from a survey’s true/false trust question couldn’t tease out the nuance you get by transforming unstructured trust summaries into Likert scales with an LLM.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Let the Results Guide You
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Unlike typical data science workflows where users begin with exploratory data analysis [57], all participants skipped manual document review and jumped straight into writing map operations. As they inspected outputs, they frequently revised their pipelines in response to what they saw—what we call opportunistic realignment.</p>
</div>
</div>
</div>
<p>This reminds me of fast.ai’s approach. When confronting a problem, train models first and examine results—baseline heuristic, then traditional ML, then neural nets. Some upfront cleaning and quick viz helps, but much data intuition comes from training models and examining results. As Jeremy Howard shows in his <a href="https://vishalbakshi.github.io/blog/index.html#category=paddy%20doctor">Paddy Doctor Kaggle series</a>, get through your entire pipeline first, then change one thing at a time following intuition rather than running hyperparameter sweeps. You need fast, tight feedback loops—manual document review prohibits this. Makes sense users jump straight to operations. I’ll remember this when using DocWrangler.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Using Operation Decomposition to Write Better Prompts
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>even though the Operation Decomposition feature was designed to help address the generalization gulf, users sometimes adopted it as a way to improve specification too—using suggestions to rethink how they framed their tasks or restructure their prompts.</p>
</div>
</div>
</div>
<p>When tackling LLM-assisted problems, you may not know what tasks are involved or their order. Makes sense that complex task decomposition informs users’ overall approach. Another example of how “The LLM Paradox” creates new problem-solving opportunities.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LLMs Uncover Serendipitous Opportunities for Analysis
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Some users shifted direction after spotting surprising or useful patterns in the LLM’s outputs. These “serendipitous” findings weren’t requested explicitly, but appeared occasionally, revealing new opportunities for analysis.</p>
</div>
</div>
</div>
<p>This resembles exploring structured data through different groupbys and aggregations—unexpected insights surface in visualizations. But there’s a fundamental difference. With structured data, you can list all columns and explore methodically. With unstructured data at scale, you may not know what information is buried in documents. These “new opportunities” involve LLMs discovering previously unknown data attributes.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prompt Rubber Ducking
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>“prompt rubber ducking”: how interacting with LLMs helped them figure out what questions to ask about their data. In this way, semantic data processing pipelines don’t just answer predefined questions, they also help shape users’ understanding of what questions are worth asking—perhaps similar to the “berry picking” model of information seeking, where users iteratively refine their search as they gain new insights</p>
</div>
</div>
</div>
<p>The “berry picking” model definition:</p>
<blockquote class="blockquote">
<p>Bates, 1989 proposed the berry-picking model of information seeking, which has two main points. The first is that, in the process of reading and learning from the information encountered throughout the search process, the searchers’ information needs, and consequently their queries, continually shift (see Figure 3.3). Information encountered at one point in a search may lead in a new, unanticipated direction. The original goal may become partly fulfilled, thus lowering the priority of one goal in favor of another. The second point is that searchers’ information needs are not satisfied by a single, final retrieved set of documents, but rather by a series of selections and bits of information found along the way. This is in contrast to the assumption that the main goal of the search process is to hone down the set of retrieved documents into a perfect match of the original information need. (<a href="https://searchuserinterfaces.com/book/sui_ch3_models_of_information_seeking.html#section_3.3">source</a>)</p>
</blockquote>
<p>This model matches anyone’s Google or Wikipedia journey—you start with one page, encounter new terms, look those up, find related concepts, look those up, step back, move forward. Non-linear information seeking. This matches the earlier user behavior where users skip manual document review and jump straight into map operations to discover patterns. Semantic data processing is like LLMs going on their own Google/Wikipedia journey.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Thinking Fast and Slow
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>system responsiveness allowed rapid iteration…output schemas acted as “speed breaks”, slowing exploration just enough for meaningful reflection</p>
</div>
</div>
</div>
<p>This testifies to DocWrangler’s UI design. One reason vibe coding fails is that chatbot interfaces lack user interaction elements that switch between rapid iteration and meaningful reflection. Problem-solving (<a href="https://math.libretexts.org/Courses/Coalinga_College/Math_for_Educators_(MATH_010A_and_010B_CID120)/05%3A_Problem_Solving/5.02%3A_George_Polya's_Strategy">Polya’s method</a>) involves: understanding the problem, breaking it into steps, generating a plan, and working iteratively in small chunks. While users could follow this method themselves, having a UI that conduces this organically is impressive.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LLM Take the Wheel!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A key factor in whether participants accepted [LLM] suggestions was their confidence in implementing the changes on their own.</p>
</div>
</div>
</div>
<p>LLM users relate to situations where LLMs suggest commands or functions outside their understanding. I’ll often trust the LLM if the code works—especially for bash commands. But if suggested code is fundamental to my script and in Python, I’ll understand each line and likely implement from scratch.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LLMs Blur Traditional Data Analysis Phases
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>LLM pipelines blur the boundary between data cleaning and analysis, unlike traditional workflows where these phases are less intertwined</p>
</div>
</div>
</div>
<p>Users requested LLM error tracing features. In one example, a user wanted to identify where dosage was incorrectly listed as 200 grams instead of 200 milligrams. The LLM faithfully reproduced this error without understanding 200 grams is incorrect. This emphasizes the need for domain experts reviewing LLM inputs/outputs and the importance of citations now common in frontier model interfaces—though citations don’t guarantee correct LLM interpretation.</p>
</section>
<section id="real-world-deployment-and-usage" class="level2">
<h2 class="anchored" data-anchor-id="real-world-deployment-and-usage">Real-World Deployment and Usage</h2>
<p>In addition to their 10 Think-A-Loud interview sessions with participants in the user study, the authors also deployed DocWrangler online and collected telemetry data for about 1500 interactions (pipelines). This section details some of the interesting findings.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prompt Refinement Addresses Skill Issue
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>users often struggle with formulating effective prompts from the outset.</p>
</div>
</div>
</div>
<p>There was evidence that users needed the prompt confinement LLM assistance to improve their initial prompts, which were often ineffective. This highlights the importance of prompt engineering as a skill but also provides evidence that the DocWrangler interface was designed correctly.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-27-contents" aria-controls="callout-27" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pipeline Development Trends
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-27" class="callout-27-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>53% of pipelines grew more complex by adding operations or upgrading models; 18% actually became simpler through operation consolidation or reduced sample sizes; and 29% maintained the same operations while only changing prompts or output schemas</p>
</div>
</div>
</div>
<p>I’d like to understand which tasks led to complexity growth versus simplification. Did complexity grow because humans better understood workflows by breaking tasks into sub-tasks, or because they had to adjust to LLM limitations? I’m confused why “upgrading models” counts as complexity growth.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-28-contents" aria-controls="callout-28" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Creativity Support Tool Epistemic Artifacts
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-28" class="callout-28-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We observed users creating what creativity support tool (CST) research calls epistemic artifacts [89]—exploratory objects that help users discover possibilities…any system addressing ambiguous tasks may benefit from CST design principles; e.g., supporting exploration without predefined goals and allowing movement between different levels of abstraction</p>
</div>
</div>
</div>
<p>I love epistemic artifacts—one reason I love notebooks. Notebooks let you quickly probe variables and data structures, visualizing them, wrapping them in functions, adjusting code on-the-fly in a unified interface. It’s why chatbot interfaces resemble notebooks and why AnswerAI’s SolveIt platform works (combining LLM interaction with editable notebooks).</p>
<p>“Addressing ambiguous tasks” is core to problem-solving. Makes sense that DocWrangler’s interface—allowing data “exploration without predefined goals” and movement “between different levels of abstraction”—is such an effective problem-solving tool for study users.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Few papers light up different parts of my brain—connecting past experiences with unconsidered opportunities, intertwining with my interests while providing chances to build new skills. The ColBERT papers, TinyStories, and Small-scale Proxies are examples. DocWrangler fits this mold. I feel introduced to a fundamental interaction pattern beyond generic chatbot interfaces. I’m excited to try DocWrangler with familiar data and record findings in future blog posts or videos. Thanks for sticking around until the end!</p>


</section>

 ]]></description>
  <category>LLM</category>
  <category>DocWrangler</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-09-01-DocWrangler/</guid>
  <pubDate>Mon, 01 Sep 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Term “Non-Deterministic” and LLMs</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-30-non-deterministic/</link>
  <description><![CDATA[ 




<p>I have recently found myself using the term “non-deterministic” to describe LLM behavior. However, something feels off about using that term and I’m nearly convinced that not only is it (sometimes) incorrect, it is imprecise, as it leaves unexplained a critical charactericistic of LLM behavior that makes LLMs different from deterministic functions.</p>
<p>First, defining “deterministic algorithm” (Wikipedia):</p>
<blockquote class="blockquote">
<p>In computer science, a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states.</p>
</blockquote>
<p>LLMs can be deterministic (i.e.&nbsp;temperature = 0, <code>do_sample=False</code>). For example running the following code passes all 100 assertions:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb1-2"></span>
<span id="cb1-3">model_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HuggingFaceTB/SmolLM2-135M"</span></span>
<span id="cb1-4">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(model_id)</span>
<span id="cb1-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(model_id)</span>
<span id="cb1-6"></span>
<span id="cb1-7">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The best thing about artificial intelligence is "</span></span>
<span id="cb1-8">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(prompt, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb1-9">attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inputs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"attention_mask"</span>]</span>
<span id="cb1-10"></span>
<span id="cb1-11">texts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>):</span>
<span id="cb1-13">    outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.generate(</span>
<span id="cb1-14">        inputs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>],</span>
<span id="cb1-15">        attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask,</span>
<span id="cb1-16">        pad_token_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tokenizer.eos_token_id</span>
<span id="cb1-17">    )</span>
<span id="cb1-18"></span>
<span id="cb1-19">    text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer.decode(outputs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], skip_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-20">    texts.append(text)</span>
<span id="cb1-21"></span>
<span id="cb1-22"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> text <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> texts: <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The best thing about artificial intelligence is &nbsp;that it can be used to solve problems that would otherwise be impossible to solve.</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">For"</span></span></code></pre></div>
<p>What I think people mean by saying “LLMs are non-deterministic” is something like the following from the <a href="https://arxiv.org/abs/2504.14764">Steering Semantic Data Processing With DocWrangler</a> paper by Shreya Shankar, et al:</p>
<blockquote class="blockquote">
<p>users need to understand their data to write effective pipelines, yet they need to construct pipelines to extract the data necessary for that understanding</p>
</blockquote>
<p>Thinking on that a bit more, what I think people mean by saying “LLMs are non-deterministic” is: what inputs to give LLMs for a desired output is ambiguous. Prompt engineering being a thing is a great example of this. I don’t know enough mathematics to know if there’s a term for this. “Input ambiguous”? “Non-deterministic on both ends”? The best Sonnet 4 came up with was “non-invertible” (other option was “non-transparent”). GPT-5 Thinking came up with a more sophisticated response “Prompting LLMs is an ill-posed inverse problem.”</p>
<blockquote class="blockquote">
<ul>
<li>Inverse problem: you start from a desired output and try to find an input (prompt) that yields it.</li>
<li>Ill-posed (Hadamard): the inverse fails one or more of
<ul>
<li>existence (your target may be unreachable),</li>
<li>uniqueness (many prompts produce similar outputs → non-injective),</li>
<li>stability (tiny prompt tweaks swing the output a lot).</li>
</ul></li>
</ul>
<p>Separately, decoding can be stochastic (temperature/top-p), which is where “non-deterministic” actually applies. With temperature=0 and deterministic kernels, the model is deterministic—but the inverse remains ill-posed.</p>
</blockquote>
<p>A well-posed problem (Wiki):</p>
<blockquote class="blockquote">
<p>In mathematics, a well-posed problem is one for which the following properties hold:</p>
<ol type="1">
<li>The problem has a solution</li>
<li>The solution is unique</li>
<li>The solution’s behavior changes continuously with the initial conditions.</li>
</ol>
</blockquote>
<p>Problems we try to solve with LLMs often fail all three properties, but again, I don’t know enough about mathematics to know if this truly applies to LLMs.</p>
<p>Most of my interactions with LLMs are through Claude Projects for coding assistance, and I make sure I understand the code (and that it works) before using it, so input ambiguity is acceptable. As I learn to use LLMs to build pipelines the input ambiguity problem sharpens and quickly makes my pipeline brittle. Over the next couple weeks, I plan on learning more about DocWrangler and DSPy to better understand how to temper my pipeline.</p>



 ]]></description>
  <category>LLM</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-30-non-deterministic/</guid>
  <pubDate>Sat, 30 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>PyTorch Version Impact on ColBERT Index Artifacts: 2.4.1 –&gt; 2.5.0</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-26-colbert-maintenance/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In a previous blog post I outlined two things:</p>
<ol type="1">
<li>Which two subsequent PyTorch versions caused a divergence in stanford-futuredata/ColBERT index <code>.pt</code> artifacts (ConditionalQA document collection):</li>
</ol>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Version A</th>
<th style="text-align: center;">Version B</th>
<th style="text-align: center;">All <code>.pt</code> Shapes Match? (Matches)</th>
<th style="text-align: center;">All <code>.pt</code> Values Match? (Matches)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1.13.1</td>
<td style="text-align: center;">2.0.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.0.0</td>
<td style="text-align: center;">2.0.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark>2.0.1</mark></td>
<td style="text-align: center;"><mark>2.1.0</mark></td>
<td style="text-align: center;"><mark>No (9/10)</mark></td>
<td style="text-align: center;"><mark>No (0/10)</mark></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.1.0</td>
<td style="text-align: center;">2.1.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.1.1</td>
<td style="text-align: center;">2.1.2</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.1.2</td>
<td style="text-align: center;">2.2.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.2.0</td>
<td style="text-align: center;">2.2.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.2.1</td>
<td style="text-align: center;">2.2.2</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.2.2</td>
<td style="text-align: center;">2.3.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.3.0</td>
<td style="text-align: center;">2.3.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.3.1</td>
<td style="text-align: center;">2.4.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.4.0</td>
<td style="text-align: center;">2.4.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark>2.4.1</mark></td>
<td style="text-align: center;"><mark>2.5.0</mark></td>
<td style="text-align: center;"><mark>No (9/10)</mark></td>
<td style="text-align: center;"><mark>No (0/10)</mark></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.5.0</td>
<td style="text-align: center;">2.5.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.5.1</td>
<td style="text-align: center;">2.6.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.6.0</td>
<td style="text-align: center;">2.7.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.7.0</td>
<td style="text-align: center;">2.7.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark>2.7.1</mark></td>
<td style="text-align: center;"><mark>2.8.0</mark></td>
<td style="text-align: center;"><mark>Yes (10/10)</mark></td>
<td style="text-align: center;"><mark>No (6/10)</mark></td>
</tr>
</tbody>
</table>
<ol start="2" type="1">
<li>That the difference in ColBERT index artifacts between <code>torch==1.13.1</code> and <code>torch==2.1.0</code> was a result of floating point precision divergence during the forward pass of the underlying <code>BertModel</code>‘s 10 encoder layers, maximum absolute difference between each PyTorch version’s layers’ outputs:</li>
</ol>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5763e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-2"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.7684e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-3"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.9605e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-4"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.9605e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-5"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.1526e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-6"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.1526e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-7"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.1526e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-8"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">9.5367e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-9"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">9.5367e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb1-10"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.1921e-06</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span></code></pre></div>
<p>In this blog post I’m going to show that the difference in ColBERT indexes between <code>torch==2.4.1</code> and <code>torch==2.5.0</code> is due to <mark>mixed precision forward pass divergence in the <code>BertModel</code> for small batch sizes</mark>.</p>
</section>
<section id="torch2.4.1-vs-torch2.5.0-index-artifact-comparison" class="level2">
<h2 class="anchored" data-anchor-id="torch2.4.1-vs-torch2.5.0-index-artifact-comparison"><code>torch==2.4.1</code> vs <code>torch==2.5.0</code> Index Artifact Comparison</h2>
<p>Similar to the difference between <code>torch==1.13.1</code> and <code>torch==2.1.0</code>, most artifacts don’t match between 2.4.1 and 2.5.0:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sampled_pids</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>num_passages</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>local_sample_embs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>centroids</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>bucket_cutoffs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>bucket_weights</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>avg_residual</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>sample</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>sample_heldout</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>embs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>doclens</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>codes</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>ivf</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>values</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>tensorize_output</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>batches</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>D</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Also similar to 1.13.1 vs 2.1.0, swapping <code>local_sample_embs</code> resolves all intermediate artifact differences:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>centroids</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>bucket_cutoffs</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>bucket_weights</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>avg_residual</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>sample</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>sample_heldout</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>embs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>doclens</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>codes</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>ivf</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>values</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</section>
<section id="inspecting-batches" class="level2">
<h2 class="anchored" data-anchor-id="inspecting-batches">Inspecting <code>batches</code></h2>
<p>In 1.13.1 vs 2.1.0, all embeddings in generated when encoding documents were different between versions, this was explained by the divergence in <code>BertModel</code> per-layer outputs. For 2.4.1 vs 2.5.0, only the <em>last batch of embeddings</em> were different between versions. The first 31 batches of embeddings had shape <code>[32, 71, 96]</code> (batch size x max seq len x emb dim), the last batch had shape <code>[8, 71, 96]</code>. This was the first “smell” about where the problem was. These embeddings, <code>batches</code>, are generated with the following code in <code>colbert/modeling/checkpoint.py</code>:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb2-2">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.doc(input_ids, attention_mask, keep_dims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>keep_dims_, to_cpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>to_cpu)</span>
<span id="cb2-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> input_ids, attention_mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tqdm(</span>
<span id="cb2-4">        text_batches, disable<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> showprogress</span>
<span id="cb2-5">    )</span>
<span id="cb2-6">]</span></code></pre></div>
<p><code>checkpoint.doc</code> was the method of interest:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> doc(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>args, to_cpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kw_args):</span>
<span id="cb3-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb3-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.amp_manager.context():</span>
<span id="cb3-4">            D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().doc(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>args, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>kw_args)</span>
<span id="cb3-5"></span>
<span id="cb3-6">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> to_cpu:</span>
<span id="cb3-7">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (D[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].cpu(), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>D[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(D, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> D.cpu()</span>
<span id="cb3-8"></span>
<span id="cb3-9">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> D</span></code></pre></div>
<p>Here’s the super class’ <code>.doc</code> method, <code>ColBERT.doc</code>:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> doc(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, input_ids, attention_mask, keep_dims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>):</span>
<span id="cb4-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> keep_dims <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'return_mask'</span>]</span>
<span id="cb4-3"></span>
<span id="cb4-4">    input_ids, attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> input_ids.to(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device), attention_mask.to(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device)</span>
<span id="cb4-5">    D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.bert(input_ids, attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb4-6">    D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear(D)</span>
<span id="cb4-7">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask(input_ids, skiplist<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.skiplist), device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb4-8">    D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask</span>
<span id="cb4-9"></span>
<span id="cb4-10">    D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.functional.normalize(D, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb4-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.use_gpu:</span>
<span id="cb4-12">        D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.half()</span>
<span id="cb4-13"></span>
<span id="cb4-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> keep_dims <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>:</span>
<span id="cb4-15">        D, mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.cpu(), mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>().cpu().squeeze(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-16">        D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [d[mask[idx]] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx, d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(D)]</span>
<span id="cb4-17"></span>
<span id="cb4-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> keep_dims <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'return_mask'</span>:</span>
<span id="cb4-19">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> D, mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>()</span>
<span id="cb4-20"></span>
<span id="cb4-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> D</span></code></pre></div>
</section>
<section id="mixed-precision-bertmodel-forward-pass-divergence" class="level2">
<h2 class="anchored" data-anchor-id="mixed-precision-bertmodel-forward-pass-divergence">Mixed Precision <code>BertModel</code> Forward Pass Divergence</h2>
<p>I found that the similarity of intermediate artifacts generated in <code>checkpoint.doc</code> between PyTorch versions depended on floating point precision.</p>
<p>Here’s a table showing the different artifacts of different precision types I compared between <code>torch==2.4.1</code> and <code>torch.2.5.0</code>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Per-Layer <code>BertModel</code> Outputs</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>checkpoint.bert(input_ids, attention_mask=attention_mask)[0]</code></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>checkpoint.linear(D)</code></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.nn.functional.normalize(D, p=2, dim=2)</code></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Per-Layer <code>BertModel</code> Outputs</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>checkpoint.bert(input_ids, attention_mask=attention_mask)[0]</code></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>checkpoint.linear(D)</code></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.nn.functional.normalize(D, p=2, dim=2)</code></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Per-Layer <code>BertModel</code> Outputs</td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>checkpoint.bert(input_ids, attention_mask=attention_mask)[0]</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>checkpoint.linear(D)</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.nn.functional.normalize(D, p=2, dim=2)</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Per-Layer <code>BertModel</code> Outputs</td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>checkpoint.bert(input_ids, attention_mask=attention_mask)[0]</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>checkpoint.linear(D)</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>False</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.nn.functional.normalize(D, p=2, dim=2)</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>False</code></td>
</tr>
</tbody>
</table>
<p>Mixed precision (<code>with torch.cuda.amp.autocast():</code>) alone was not sufficient to cause divergence. When combining mixed precision with a batch size of 8, the floating point values diverge. Why? The intermediate linear layer (384 –&gt; 1536) appears to be the source of divergence for the batch-size of 8 + mixed precision divergence across PyTorch versions. Note that it didn’t matter which 8-items were selected (from the first or last batch, or in between), this divergence took place between PyTorch versions.</p>
<p>To isolate what in <code>checkpoint.bert</code> was causing this divergence, I replaced different <code>checkpoint.bert</code> modules with <code>Identity</code>, defined as:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Identity(torch.nn.Module):</span>
<span id="cb5-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb5-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span></code></pre></div>
<p>Ultimately I landed on the following code, replacing two of the dense layers with <code>Identity</code>:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> checkpoint.bert.encoder.layer:</span>
<span id="cb6-2">    layer.intermediate.dense <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Identity()</span>
<span id="cb6-3">    layer.output.dense <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Identity()</span></code></pre></div>
<p>After running the scripts with this model modification, mixed precision 8-item batches yielded identical results across PyTorch versions!</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Per-Layer <code>BertModel</code> Outputs</td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>checkpoint.bert(input_ids, attention_mask=attention_mask)[0]</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>checkpoint.linear(D)</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.nn.functional.normalize(D, p=2, dim=2)</code></td>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"><code>True</code></td>
</tr>
</tbody>
</table>
<p>Here are the two modules in question: (<code>layer.intermediate.dense</code> and <code>layer.output.dense</code>)</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">(intermediate): BertIntermediate(</span>
<span id="cb7-2">    (dense): Linear(in_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>, out_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1536</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-3">    (intermediate_act_fn): GELUActivation()</span>
<span id="cb7-4">)</span>
<span id="cb7-5">(output): BertOutput(</span>
<span id="cb7-6">    (dense): Linear(in_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1536</span>, out_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-7">    (LayerNorm): LayerNorm((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>,), eps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-12</span>, elementwise_affine<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-8">    (dropout): Dropout(p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb7-9">)</span></code></pre></div>
<p>Running the following small reproduction of the two linear layers:</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.bert.encoder.layer[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb8-2">x32 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">71</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>).cuda()</span>
<span id="cb8-3">x8 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x32[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]</span>
<span id="cb8-4"></span>
<span id="cb8-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.cuda.amp.autocast():</span>
<span id="cb8-6">    out32 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer.intermediate.dense(x32) </span>
<span id="cb8-7">    out8 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer.intermediate.dense(x8)</span>
<span id="cb8-8"></span>
<span id="cb8-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Intermediate Linear match: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>allclose(out32[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>], out8)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb8-10"></span>
<span id="cb8-11">x32_wide <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">71</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1536</span>).cuda()</span>
<span id="cb8-12">x8_wide <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x32_wide[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]</span>
<span id="cb8-13"></span>
<span id="cb8-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.cuda.amp.autocast():</span>
<span id="cb8-15">    out32 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer.output.dense(x32_wide)</span>
<span id="cb8-16">    out8 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer.output.dense(x8_wide)</span>
<span id="cb8-17"></span>
<span id="cb8-18"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Output Linear match: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>allclose(out32[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>], out8)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<p>Prints out the following:</p>
<pre><code>Intermediate Linear match: False
Output Linear match: True</code></pre>
<p>The intermediate layer (projecting from 384 to 1536 dimensions) causes the divergence in floating point values between the first 8 items of a batch of 32 and all items in the batch of 8 for the same PyTorch version (<code>2.4.1</code>). It’s interesting that the largest matrix multiplication is causing this divergence.</p>
<p>Additionally, this divergence between intermediate dense layer outputs of the first n-items of a batch size of 32 and a smaller batch size of n exists for n = 8, 9 and 10, as checked by the following code:</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.bert.encoder.layer[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb10-2">x32 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">71</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">384</span>).cuda()</span>
<span id="cb10-3"></span>
<span id="cb10-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>):</span>
<span id="cb10-5">    xs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x32[:i]</span>
<span id="cb10-6">    </span>
<span id="cb10-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.cuda.amp.autocast():</span>
<span id="cb10-8">        out32 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer.intermediate.dense(x32) </span>
<span id="cb10-9">        outs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer.intermediate.dense(xs)</span>
<span id="cb10-10">    </span>
<span id="cb10-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Intermediate Linear match: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>allclose(out32[:i], outs)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<pre><code>...
5 Intermediate Linear match: True
6 Intermediate Linear match: True
7 Intermediate Linear match: True
8 Intermediate Linear match: False
9 Intermediate Linear match: False
10 Intermediate Linear match: False
11 Intermediate Linear match: True
12 Intermediate Linear match: True
...</code></pre>
</section>
<section id="appendix-code" class="level2">
<h2 class="anchored" data-anchor-id="appendix-code">Appendix: Code</h2>
<p>Here’s the core functionality that I used to generate and save full precision <code>BertModel</code> (and related) artifacts:</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">text_batches, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>MOUNT<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>project<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>date<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>source<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>nranks<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/tensorize_output.pt'</span>)</span>
<span id="cb12-2">input_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_batches[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]</span>
<span id="cb12-3">attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_batches[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]</span>
<span id="cb12-4"></span>
<span id="cb12-5">outputs_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb12-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> capture_output(name):</span>
<span id="cb12-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> hook_fn(module, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, output):</span>
<span id="cb12-8">        outputs_dict[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].detach()</span>
<span id="cb12-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> hook_fn</span>
<span id="cb12-10"></span>
<span id="cb12-11">hooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb12-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>): hooks.append(checkpoint.bert.encoder.layer[i].register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb12-13"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(): D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.bert(input_ids, attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb12-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> h <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hooks: h.remove()</span>
<span id="cb12-15"></span>
<span id="cb12-16">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.bert(input_ids, attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb12-17">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.linear(D)</span>
<span id="cb12-18">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(checkpoint.mask(input_ids, skiplist<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>checkpoint.skiplist), device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>checkpoint.device).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb12-19">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask</span>
<span id="cb12-20">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.functional.normalize(D, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<p>For mixed precision I indented everything after a <code>with torch.cuda.amp.autocast():</code> line.</p>
<p>My code to compare two versions’ artifacts generally looked like this:</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb13-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> rich.console <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Console</span>
<span id="cb13-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> rich.panel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Panel</span>
<span id="cb13-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> rich.text <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Text</span>
<span id="cb13-5">console <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Console(force_terminal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb13-6"></span>
<span id="cb13-7">a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>root_a<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/outputs_dict.pt"</span>)</span>
<span id="cb13-8">b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>root_b<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/outputs_dict.pt"</span>)</span>
<span id="cb13-9"></span>
<span id="cb13-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb13-11">    a_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> a[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb13-12">    b_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> b[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb13-13">    console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, torch.allclose(a_, b_))</span>
<span id="cb13-14"></span>
<span id="cb13-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _print(string, flag, print_flag<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"[</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> flag <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>string<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>flag <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> print_flag <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'/green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> flag <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'/red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span></span>
<span id="cb13-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _compare(fn):</span>
<span id="cb13-17">    a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>root_a<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>fn<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-18">    b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>root_b<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>fn<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-19">    console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(_print(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>fn<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> torch.allclose:"</span>, torch.allclose(a, b), <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span>
<span id="cb13-20"></span>
<span id="cb13-21">_compare(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"D_bert.pt"</span>)</span>
<span id="cb13-22">_compare(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"D_linear.pt"</span>)</span>
<span id="cb13-23">_compare(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"D_mask.pt"</span>)</span>
<span id="cb13-24">_compare(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"D_norm.pt"</span>)</span></code></pre></div>


</section>

 ]]></description>
  <category>ColBERT</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-26-colbert-maintenance/</guid>
  <pubDate>Tue, 26 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>PyTorch Version Impact on ColBERT Index Artifacts</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-18-colbert-maintenance/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I recently released <code>colbert-ai==0.2.22</code> which removed the deprecated <code>transformers.AdamW</code> import <a href="https://github.com/stanford-futuredata/ColBERT/releases/tag/v0.2.22">among other changes</a>. I’m now turning my attention to upgrading the PyTorch dependency to 2.x, which will not only introduce compatibility with modern version installations of <code>torch</code> but will also allow the integration of the <a href="https://github.com/AnswerDotAI/fastkmeans">AnswerAI <code>fastkmeans</code> library</a> as a replacement for the <code>faiss-gpu</code> and <code>faiss-cpu</code> libraries (which are no longer officially maintained on PyPI).</p>
<p>I started this PyTorch 2.x upgrade effort by analyzing the impact of <code>torch==2.0.0</code> on <code>colbert-ai</code> as this was the first upgrade from the existing <code>torch==1.13.1</code> dependency. I approached this analysis by reviewing and documenting whether the 500+ PRs involved in <code>torch==2.0.0</code> would impact <code>colbert-ai</code>. The resulting <a href="https://docs.google.com/spreadsheets/d/1sUEN7xo5-hLVoxF9NL_ibGxPaKlPzxmnMU46zf3wd-U/edit?usp=sharing">spreadsheet</a> and <a href="https://vishalbakshi.github.io/blog/posts/2025-07-27-torch-colbert/">blog post</a> detail my findings. In short, I estimated that 28 PRs potentially impacted <code>colbert-ai</code>.</p>
<p>In this blog post I’m detailing a different approach, from the “other end” so to speak: what changes in <code>colbert-ai</code> index artifacts when changing PyTorch versions?</p>
</section>
<section id="indexing-conditionalqa-with-19-pytorch-versions" class="level2">
<h2 class="anchored" data-anchor-id="indexing-conditionalqa-with-19-pytorch-versions">Indexing ConditionalQA with 19 PyTorch Versions</h2>
<p>I started by indexing the <a href="https://huggingface.co/datasets/UKPLab/dapr">UKPLab/DAPR/ConditionalQA</a> document collection with 19 different <code>colbert-ai</code> installs (one for each version of PyTorch from <code>1.13.1</code> to <code>2.8.0</code>), using Modal. Each Dockerfile looks something like this:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode dockerfile code-with-copy"><code class="sourceCode dockerfile"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">FROM</span> mambaorg/micromamba:latest</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">USER</span> root</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> update <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span> git nano curl wget build-essential <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> clean <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rm</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-rf</span> /var/lib/apt/lists/<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> clone https://github.com/stanford-futuredata/ColBERT.git /ColBERT <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> /ColBERT <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-9">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert python=3.11 cuda <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> nvidia/label/11.7.1 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-10">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert faiss-gpu <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> pytorch <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-11">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert pip install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-e</span> . <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-12">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert pip install torch==2.2.0 transformers==4.38.2 pandas</span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">ENV</span> CONDA_DEFAULT_ENV=colbert</span>
<span id="cb1-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">ENV</span> PATH=/opt/conda/envs/colbert/bin:$PATH</span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">WORKDIR</span> /ColBERT</span>
<span id="cb1-18"></span>
<span id="cb1-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval </span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\"\$</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(micromamba shell hook --shell bash)</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\"</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;</span> ~/.bashrc <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-20">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"micromamba activate colbert"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;</span> ~/.bashrc</span>
<span id="cb1-21"></span>
<span id="cb1-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">CMD</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/bin/bash"</span>]</span></code></pre></div>
<p>I decided to <code>git clone</code> and <code>pip install -e .</code> the <code>main</code> branch of <a href="https://github.com/stanford-futuredata/colbert">stanford-futuredata/ColBERT</a> since I wanted to modify the files down the road to save/inject intermediate index artifacts (as we’ll see later on in this blog post).</p>
<p>My indexing function looks like:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@app.function</span>(gpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>GPU, image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image, timeout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3600</span>,</span>
<span id="cb2-2">              volumes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{MOUNT: VOLUME},</span>
<span id="cb2-3">              max_containers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _index(source, project, date, nranks, ndocs, root):</span>
<span id="cb2-5">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb2-6">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> subprocess</span>
<span id="cb2-7">    subprocess.run([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pwd'</span>], text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, shell<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-8">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Indexer</span>
<span id="cb2-9">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.infra <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RunConfig, ColBERTConfig</span>
<span id="cb2-10">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.infra.run <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Run</span>
<span id="cb2-11">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb2-12"></span>
<span id="cb2-13">    os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ROOT"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> root</span>
<span id="cb2-14"></span>
<span id="cb2-15">    dataset_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ConditionalQA"</span></span>
<span id="cb2-16">    passages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"UKPLab/dapr"</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>dataset_name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-corpus"</span>, split<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test"</span>)</span>
<span id="cb2-17">    queries <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"UKPLab/dapr"</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>dataset_name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-queries"</span>, split<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test"</span>)</span>
<span id="cb2-18">    qrels_rows <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"UKPLab/dapr"</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>dataset_name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-qrels"</span>, split<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test"</span>)</span>
<span id="cb2-19"></span>
<span id="cb2-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> Run().context(RunConfig(nranks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>nranks)):</span>
<span id="cb2-21">        config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ColBERTConfig(</span>
<span id="cb2-22">            doc_maxlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>,      </span>
<span id="cb2-23">            nbits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,             </span>
<span id="cb2-24">            dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>,             </span>
<span id="cb2-25">            kmeans_niters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>,</span>
<span id="cb2-26">            index_bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>,</span>
<span id="cb2-27">            bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>,</span>
<span id="cb2-28">            checkpoint<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"answerdotai/answerai-colbert-small-v1"</span></span>
<span id="cb2-29">        )</span>
<span id="cb2-30">        </span>
<span id="cb2-31">        indexer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Indexer(checkpoint<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"answerdotai/answerai-colbert-small-v1"</span>, config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>config)</span>
<span id="cb2-32">        _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> indexer.index(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>MOUNT<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>project<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>date<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>source<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>nranks<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/indexing/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>dataset_name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, collection<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>passages[:ndocs][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>], overwrite<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-33"></span>
<span id="cb2-34">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Index created!"</span>)</span></code></pre></div>
<p>I would run the indexing function (in my <code>main.py</code> file) using a terminal command like so:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">SOURCE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"0.2.22.main.torch.1.13.1"</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">DATE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"20250818"</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">PROJECT</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"torch2.x"</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">NRANKS</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>1 <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">GPU</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"L4"</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">modal</span> run main.py</span></code></pre></div>
</section>
<section id="comparing-index-artifacts-across-pytorch-versions" class="level2">
<h2 class="anchored" data-anchor-id="comparing-index-artifacts-across-pytorch-versions">Comparing Index Artifacts Across PyTorch Versions</h2>
<p>Once indexed, I ran my comparison script which starts by comparing index file names:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">[bold blue]INDEX FILE NAME COMPARISON[/bold blue]"</span>)</span>
<span id="cb4-2">a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> os.listdir(a_path)</span>
<span id="cb4-3">b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> os.listdir(b_path)</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb4-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, f <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(a): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> f <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> b[i]</span>
<span id="cb4-7">    console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"[green]✓ All </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(a)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> files match[/green]"</span>)</span>
<span id="cb4-8"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span>:</span>
<span id="cb4-9">    console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[red]✗ File names don't match[/red]"</span>)</span></code></pre></div>
<p>Then index tensor shapes:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, f <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(a_pts):</span>
<span id="cb5-2">    console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[bold]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>f<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/bold]"</span>)</span>
<span id="cb5-3">    a_pt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(a_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> f)</span>
<span id="cb5-4">    b_pt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(b_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> f)</span>
<span id="cb5-5">    </span>
<span id="cb5-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(a_pt, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>):</span>
<span id="cb5-7">        match1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape</span>
<span id="cb5-8">        match2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].shape</span>
<span id="cb5-9">        console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  Tensor[0]: [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match1 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> vs </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match1 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span>)</span>
<span id="cb5-10">        console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  Tensor[1]: [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match2 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> vs </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match2 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span>)</span>
<span id="cb5-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> (match1 <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> match2):</span>
<span id="cb5-12">            shape_mismatches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb5-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb5-14">        match <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> a_pt.shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> b_pt.shape</span>
<span id="cb5-15">        console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  Shape: [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>a_pt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> vs </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>b_pt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span>)</span>
<span id="cb5-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> match:</span>
<span id="cb5-17">            shape_mismatches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<p>and finally compare tensor values between indexes:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, f <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(a_pts):</span>
<span id="cb6-2">    console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[bold]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>f<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/bold]"</span>)</span>
<span id="cb6-3">    a_pt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(a_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> f)</span>
<span id="cb6-4">    b_pt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(b_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> f)</span>
<span id="cb6-5">    </span>
<span id="cb6-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(a_pt, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>):</span>
<span id="cb6-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape:</span>
<span id="cb6-8">            match1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.allclose(a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb6-9">            console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match1 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'✓'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match1 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'✗'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Tensor[0] values </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'match'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match1 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'differ'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match1 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span>)</span>
<span id="cb6-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb6-11">            console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"  [red]✗ Tensor[0] shape mismatch[/red]"</span>)</span>
<span id="cb6-12">            match1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-13">            </span>
<span id="cb6-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].shape:</span>
<span id="cb6-15">            match2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.allclose(a_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], b_pt[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb6-16">            console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match2 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'✓'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match2 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'✗'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Tensor[1] values </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'match'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match2 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'differ'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match2 <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span>)</span>
<span id="cb6-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb6-18">            console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"  [red]✗ Tensor[1] shape mismatch[/red]"</span>)</span>
<span id="cb6-19">            match2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-20">            </span>
<span id="cb6-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> (match1 <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> match2):</span>
<span id="cb6-22">            value_mismatches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb6-23">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb6-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> a_pt.shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> b_pt.shape:</span>
<span id="cb6-25">            match <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.allclose(a_pt, b_pt)</span>
<span id="cb6-26">            console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'✓'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'✗'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Values </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'match'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'differ'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">[/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> match <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span>)</span>
<span id="cb6-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb6-28">            console.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"  [red]✗ Shape mismatch[/red]"</span>)</span>
<span id="cb6-29">            match <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-30">            </span>
<span id="cb6-31">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> match:</span>
<span id="cb6-32">            value_mismatches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<p>I compared consecutive pairs of PyTorch version <code>colbert-ai</code> installs to understand between which versions the index artifacts change. Here are my results:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Version A</th>
<th style="text-align: center;">Version B</th>
<th style="text-align: center;">All <code>.pt</code> Shapes Match? (Matches)</th>
<th style="text-align: center;">All <code>.pt</code> Values Match? (Matches)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1.13.1</td>
<td style="text-align: center;">2.0.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.0.0</td>
<td style="text-align: center;">2.0.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark>2.0.1</mark></td>
<td style="text-align: center;"><mark>2.1.0</mark></td>
<td style="text-align: center;"><mark>No (9/10)</mark></td>
<td style="text-align: center;"><mark>No (0/10)</mark></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.1.0</td>
<td style="text-align: center;">2.1.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.1.1</td>
<td style="text-align: center;">2.1.2</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.1.2</td>
<td style="text-align: center;">2.2.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.2.0</td>
<td style="text-align: center;">2.2.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.2.1</td>
<td style="text-align: center;">2.2.2</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.2.2</td>
<td style="text-align: center;">2.3.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.3.0</td>
<td style="text-align: center;">2.3.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.3.1</td>
<td style="text-align: center;">2.4.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.4.0</td>
<td style="text-align: center;">2.4.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark>2.4.1</mark></td>
<td style="text-align: center;"><mark>2.5.0</mark></td>
<td style="text-align: center;"><mark>No (9/10)</mark></td>
<td style="text-align: center;"><mark>No (0/10)</mark></td>
</tr>
<tr class="even">
<td style="text-align: center;">2.5.0</td>
<td style="text-align: center;">2.5.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.5.1</td>
<td style="text-align: center;">2.6.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.6.0</td>
<td style="text-align: center;">2.7.0</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.7.0</td>
<td style="text-align: center;">2.7.1</td>
<td style="text-align: center;">Yes (10/10)</td>
<td style="text-align: center;">Yes (10/10)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark>2.7.1</mark></td>
<td style="text-align: center;"><mark>2.8.0</mark></td>
<td style="text-align: center;"><mark>Yes (10/10)</mark></td>
<td style="text-align: center;"><mark>No (6/10)</mark></td>
</tr>
</tbody>
</table>
<p>There are three PyTorch upgrades that cause a change in index artifacts: 2.0.1 –&gt; 2.1.0, 2.4.1 –&gt; 2.5.0, and 2.7.1 –&gt; 2.8.0.</p>
</section>
<section id="comparing-intermediate-index-artifacts" class="level2">
<h2 class="anchored" data-anchor-id="comparing-intermediate-index-artifacts">Comparing Intermediate Index Artifacts</h2>
<p>To better understand exactly where the index artifacts changed when upgrading PyTorch, I created my own copies of two stanford-futuredata/ColBERT files and added <code>torch.save</code> lines to save the intermediate artifacts listed below:</p>
<ul>
<li>colbert/indexing/collection_indexer.py
<ul>
<li><code>sampled_pids</code> (a set of integers corresponding to sampled passage IDs)</li>
<li><code>num_passages</code> (a single integers, the number of total passages in the collection)</li>
<li><code>local_sample_embs</code> (BERT encodings of the sample pids, created by <code>Checkpoint.docFromText</code>)</li>
<li><code>centroids</code> (from <code>_train_kmeans</code>)</li>
<li><code>bucket_cutoffs</code> (the bin “boundaries” used for quantization from <code>_compute_avg_residual</code>)</li>
<li><code>bucket_weights</code> (the quantized values, from <code>_compute_avg_residual</code>)</li>
<li><code>avg_residual</code> (a single float, from <code>_compute_avg_residual</code>)</li>
<li><code>sample</code> (95% of the values from <code>local_sample_embs.half()</code>)</li>
<li><code>sample_heldout</code> (5% of the values from <code>local_sample_embs.half()</code>)</li>
<li><code>embs</code> (encoded passages)</li>
<li><code>doclens</code> (number of tokens in each passage)</li>
<li><code>codes</code> (centroid IDs (values) and document token IDs (indices))</li>
<li><code>ivf</code> (document token IDs)</li>
<li><code>values</code> (centroid IDs)</li>
</ul></li>
<li>colbert/modeling/checkpoint.py
<ul>
<li><code>tensorize_output</code> (tuple (<code>text_batches</code>, <code>reverse_indices</code>) output from <code>DocTokenizer.tensorize</code>)</li>
<li><code>batches</code> (BERT encodings, output from <code>Checkpoint.doc</code>)</li>
<li><code>D</code> (sorted and reshaped <code>batches</code>)</li>
</ul></li>
</ul>
<p>I then replaced the corresponding files in the <code>/ColBERT</code> directory (which is why I used <code>git clone</code> and <code>pip install e .</code>) with the following lines for Modal:</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image.add_local_file(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"collection_indexer.py"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/ColBERT/colbert/indexing/collection_indexer.py"</span>)</span>
<span id="cb7-2">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image.add_local_file(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"checkpoint.py"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/ColBERT/colbert/modeling/checkpoint.py"</span>)</span></code></pre></div>
<p>Here are the results when comparing these artifacts between <code>colbert-ai</code> installs using <code>torch==1.13.1</code> and <code>torch==2.1.0</code>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>sampled_pids</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>num_passages</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>local_sample_embs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>centroids</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>bucket_cutoffs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>bucket_weights</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>avg_residual</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>sample</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>sample_heldout</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>embs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>doclens</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>codes</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>ivf</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>values</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>tensorize_output</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>batches</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>D</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>After reviewing these comparisons, my hypothesis was that the first difference (in <code>local_sample_embs</code>) affected all subsequent artifacts. The difference in <code>local_sample_embs</code> can be traced down to the difference in <code>batches</code> and <code>D</code>. To test this hypothesis, I “injected” the <code>local_sample_embs</code> from the <code>torch==1.13.1</code> install into the <code>collection_indexer.py</code> when indexing with <code>torch==2.1.0</code>:</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">local_sample_embs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.load(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/colbert-maintenance/torch2.x/20250818-0.2.22.main.torch.1.13.1-1k-1/local_sample_embs.pt"</span>)</span></code></pre></div>
<p>I then re-compared the artifacts, and my hypothesis was correct!</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Artifact</th>
<th style="text-align: center;"><code>torch.allclose</code></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>centroids</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>bucket_cutoffs</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>bucket_weights</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>avg_residual</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>sample</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>sample_heldout</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>embs</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>doclens</code></td>
<td style="text-align: center;"><code>True</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>codes</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><mark><code>ivf</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><mark><code>values</code></mark></td>
<td style="text-align: center;"><mark><code>False</code></mark></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</section>
<section id="comparing-the-bertmodels" class="level2">
<h2 class="anchored" data-anchor-id="comparing-the-bertmodels">Comparing the <code>BertModel</code>s</h2>
<p>Where do <code>local_sample_embs</code> come from? The highest-level method is <code>CollectionEncoder.encode_passages</code>. Inside <code>CollectionEncoder.encode_passages</code> the collection of texts, <code>passages</code> is fed to <code>Checkpoint.docFromText</code>. Inside there, the tokenized text is passed to <code>Checkpoint.doc</code>, which passes them to <code>ColBERT.doc</code>, which finally passes the <code>input_ids</code> and <code>attention_mask</code> to <code>ColBERT.bert</code>. Since there was a divergence in <code>local_sample_embs</code>, I figured there would be a divergence in either the weights and/or the logits of <code>ColBERT.bert</code> between both PyTorch version installs.</p>
<p>I installed each image of <code>colbert-ai</code> and separately saved the <code>BertModel</code> weights as well as a dictionary with outputs from each of the 10 <code>BertEncoder</code> layers. These outputs were accessed using a forward hook:</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">outputs_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb9-2"></span>
<span id="cb9-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> capture_output(name):</span>
<span id="cb9-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> hook_fn(module, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, output):</span>
<span id="cb9-5">        outputs_dict[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].detach()</span>
<span id="cb9-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> hook_fn</span>
<span id="cb9-7"></span>
<span id="cb9-8">hooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb9-10">    hooks.append(checkpoint.bert.encoder.layer[i].register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"1.13.1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb9-11"></span>
<span id="cb9-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb9-13">    D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.bert(input_ids, attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb9-14"></span>
<span id="cb9-15"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> h <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hooks: h.remove()</span></code></pre></div>
<p>Both <code>colbert-ai</code> installs (<code>torch==1.13.1</code> and <code>torch==2.1.0</code>) had equal <code>BertModel</code> weights. However, both of them have diverging <code>BertEncoder</code> outputs.</p>
<p>Here are the mean absolute differences between corresponding <code>BertEncoder</code> layer outputs between <code>torch==1.13.1</code> and <code>torch==2.1.0</code>:</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb10-2">    a_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> a[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"1.13.1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb10-3">    b_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> b[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"2.1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb10-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(i, torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(a_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> b_).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().mean())</span></code></pre></div>
<pre><code>0 tensor(2.8141e-08, device='cuda:0')
1 tensor(5.9652e-08, device='cuda:0')
2 tensor(8.0172e-08, device='cuda:0')
3 tensor(7.8228e-08, device='cuda:0')
4 tensor(7.9968e-08, device='cuda:0')
5 tensor(8.3589e-08, device='cuda:0')
6 tensor(8.7348e-08, device='cuda:0')
7 tensor(8.5140e-08, device='cuda:0')
8 tensor(8.5651e-08, device='cuda:0')
9 tensor(8.1636e-08, device='cuda:0')</code></pre>
<p>The difference increases about 2x as we go deeper through the model.</p>
<p>Here are the max absolute differences, which increases 2x by the final layer:</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5763e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-2"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.7684e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-3"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.9605e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-4"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.9605e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-5"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.1526e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-6"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.1526e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-7"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">7.1526e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-8"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">9.5367e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-9"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">9.5367e-07</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span>
<span id="cb12-10"><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span> tensor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.1921e-06</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda:0'</span>)</span></code></pre></div>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>From this analysis, I can conclude that the difference in index artifacts generated by <code>colbert-ai</code> using different <code>torch==1.13.1</code> vs.&nbsp;<code>torch==2.1.0</code> is due to floating point differences in the forward pass of the <code>BertModel</code> used to generate token-level embeddings from text passages. I have not yet analyzed the <code>torch==2.1.0</code> release notes to make an educated guess on why these differences occur. But given that it’s during the forward pass of the model, I would wager there was some update to the underlying C++ code for the <code>torch.nn</code> module.</p>
<p>I will move forward with comparing intermediate artifacts between each subsequent versions where the final index artifacts are different 2.4.1 –&gt; 2.5.0, and 2.7.1 –&gt; 2.8.0. Once that’s complete, I’ll dive into the PyTorch release notes and see if I can reasonably point to a few PRs behind this change. Once I have a reasonable handle on understanding <code>colbert-ai</code> indexing behavior with different versions of PyTorch 2.x, I’ll perform a similar analysis with <code>colbert-ai</code> training and document my findings.</p>
<p>Thanks for reading until the end! I’ll be posting more blog post and/or video updates around ColBERT maintenance as soon as I have something more to share.</p>


</section>

 ]]></description>
  <category>ColBERT</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-18-colbert-maintenance/</guid>
  <pubDate>Tue, 19 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>TIL: Launching Jupyter with a Custom Modal Image and Volume</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-17-modal-jupyter/</link>
  <description><![CDATA[ 




<p>Yesterday I learned of the Modal docs example showing how to start <a href="https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py">a jupyter server via a Modal tunnel</a>. I was elated to see this because it solved my problem of not being able to specify a custom image when using <code>modal launch jupyter</code>.</p>
<p>I have a Dockerfile which installs <code>colbert-ai</code> from the <code>main</code> branch of the stanford-futuredata/ColBERT repo with a specific PyTorch and Transformers version:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode dockerfile code-with-copy"><code class="sourceCode dockerfile"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">FROM</span> mambaorg/micromamba:latest</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">USER</span> root</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> update <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-y</span> git nano curl wget build-essential <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">apt-get</span> clean <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rm</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-rf</span> /var/lib/apt/lists/<span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> clone https://github.com/stanford-futuredata/ColBERT.git /ColBERT <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> /ColBERT <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-9">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert python=3.11 cuda <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> nvidia/label/11.7.1 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-10">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert faiss-gpu <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> pytorch <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-11">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert pip install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-e</span> . <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-12">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">micromamba</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> colbert pip install torch==1.13.1 transformers==4.38.2 pandas</span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">ENV</span> CONDA_DEFAULT_ENV=colbert</span>
<span id="cb1-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">ENV</span> PATH=/opt/conda/envs/colbert/bin:$PATH</span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">WORKDIR</span> /</span>
<span id="cb1-18"></span>
<span id="cb1-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">RUN</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval </span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\"\$</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(micromamba shell hook --shell bash)</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\"</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;</span> ~/.bashrc <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">&amp;&amp;</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-20">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">echo</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"micromamba activate colbert"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;&gt;</span> ~/.bashrc</span>
<span id="cb1-21"></span>
<span id="cb1-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">CMD</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/bin/bash"</span>]</span></code></pre></div>
<p>I then modified the Modal documentation example as follows (<code>jupyter_inside_modal.py</code>) to use my Dockerfile to create an image and use an existing Modal Volume:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> subprocess</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> modal</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> modal <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image, App, Secret, Volume</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datetime</span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb2-7"></span>
<span id="cb2-8">SOURCE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> os.environ.get(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SOURCE"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb2-9">VOLUME <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Volume.from_name(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"colbert-maintenance"</span>, create_if_missing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-10">MOUNT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/colbert-maintenance"</span></span>
<span id="cb2-11">image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.from_dockerfile(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Dockerfile.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>SOURCE<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, gpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"L4"</span>)</span>
<span id="cb2-12"></span>
<span id="cb2-13">app <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> App(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jupyter-tunnel"</span>, image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image.pip_install(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jupyter"</span>))</span>
<span id="cb2-14">JUPYTER_TOKEN <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># some list of characters you'll enter when accessing the Modal tunnel</span></span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@app.function</span>(max_containers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, volumes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{MOUNT: VOLUME}, timeout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span>, gpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"L4"</span>)</span>
<span id="cb2-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> run_jupyter(timeout: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>):</span>
<span id="cb2-18">    jupyter_port <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8888</span></span>
<span id="cb2-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> modal.forward(jupyter_port) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> tunnel:</span>
<span id="cb2-20">        jupyter_process <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subprocess.Popen(</span>
<span id="cb2-21">            [</span>
<span id="cb2-22">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jupyter"</span>,</span>
<span id="cb2-23">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"notebook"</span>,</span>
<span id="cb2-24">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--no-browser"</span>,</span>
<span id="cb2-25">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--allow-root"</span>,</span>
<span id="cb2-26">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--ip=0.0.0.0"</span>,</span>
<span id="cb2-27">                <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"--port=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>jupyter_port<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,</span>
<span id="cb2-28">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--NotebookApp.allow_origin='*'"</span>,</span>
<span id="cb2-29">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--NotebookApp.allow_remote_access=1"</span>,</span>
<span id="cb2-30">            ],</span>
<span id="cb2-31">            env<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>os.environ, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"JUPYTER_TOKEN"</span>: JUPYTER_TOKEN},</span>
<span id="cb2-32">        )</span>
<span id="cb2-33"></span>
<span id="cb2-34">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Jupyter available at =&gt; </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tunnel<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>url<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-35"></span>
<span id="cb2-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb2-37">            end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> timeout</span>
<span id="cb2-38">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> end_time:</span>
<span id="cb2-39">                time.sleep(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb2-40">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Reached end of </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>timeout<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> second timeout period. Exiting..."</span>)</span>
<span id="cb2-41">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">KeyboardInterrupt</span>:</span>
<span id="cb2-42">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Exiting..."</span>)</span>
<span id="cb2-43">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">finally</span>:</span>
<span id="cb2-44">            jupyter_process.kill()</span>
<span id="cb2-45"></span>
<span id="cb2-46"></span>
<span id="cb2-47"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@app.local_entrypoint</span>()</span>
<span id="cb2-48"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> main(timeout: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span>):</span>
<span id="cb2-49">    run_jupyter.remote(timeout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>timeout)</span></code></pre></div>
<p>I then run the following locally form my terminal:</p>
<pre><code>SOURCE="0.2.22.main.torch.1.13.1" modal run jupyter_inside_modal.py</code></pre>
<p>Where my Dockerfile is in the same folder as <code>jupyter_inside_modal.py</code> and titled <code>Dockerfile.0.2.22.main.torch.1.13.1</code>. I can then access the cloned repo as well as my mounted volume and use a Jupyter Notebook to explore data, iterate on function definitions, compare model weights, add hooks to ColBERT models, and so on. This unlocks a ton of productivity and iteration velocity that I was scratching my head on how to obtain without the use of notebooks.</p>



 ]]></description>
  <category>ColBERT</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-17-modal-jupyter/</guid>
  <pubDate>Sun, 17 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Reflections After Completing the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-17-ai-evals/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>This blog post is part journal, part reflection, and part planning around the topics of AI engineering, AI evals, applied AI, and my career path into machine learning. When I decided to take <a href="https://t.co/Zrmp6LRd9c">Hamel and Shreya’s AI evals course</a>, I had recently watched a very short talk on LLM reliability by Featherless AI CEO Eugene Cheah. I had also learned about the <a href="https://www.realevals.xyz/">realevals.xyz benchmark</a>, which shows just how bad LLMs are at reliably completing “boring” tasks that are necessary in commerce. I had also watched and <a href="https://youtu.be/9s88C8XBBiQ">deeply reflected</a> on a discussion between Jason Liu, Andy Walters, and Vignesh Mohankumar about the different routes into AI industry work. I had also reflected on my own fear of applied AI: the risk of automating tasks for which failures would have real, concrete consequences. The amalgamation of these experiences led me to adopt the “narrow fence/long leash” framework as discussed in the AnswerAI launch blog post, an idea that came from the old GE research and development lab. In this framework, the manager would provide the researchers a “long leash” to explore whatever problems they wanted to, at whatever depth they needed to, as long as they tied back to an applied AI project; as long as they stayed within the bounds of this “narrow fence” of an applied AI project.</p>
<p>A couple of weeks before the AI evals course started, I was chosen to be a maintainer of the stanford_futuredata/ColBERT (<code>colbert-ai</code> on PyPI) library. I had “suddenly” gone from “hobby ML researcher” to maintainer of a foundational library in the information retrieval ML space. I say “suddenly” because while everything I had done up to that point led to me landing that role, I hadn’t done everything with the goal of landing it. I had done what I did because it was fun, it was exciting, it was challenging, and I was learning a lot. Suddenly my priorities shifted. I put ColBERT maintenance (gratefully) at #1, and everything else I was working on (both long Leashes and narrow fences) became a distant #2. I had a number of goals of what I wanted to prepare before the AI evals course started, but I was not able to bring that energy and bring that preparation as I planned. Like anything, you get out what you put in. While I wasn’t able to put in my maximum in the AI Evals course, I was able to adapt to my circumstances and put in and get out something extremely valuable. I hope to distill those experiences in this blog post.</p>
</section>
<section id="applied-ai-is-different" class="level2">
<h2 class="anchored" data-anchor-id="applied-ai-is-different">Applied AI is Different</h2>
<p>Taking the AI Evals course was a humbling experience. I thought that because of how much time I had spent learning about deep learning through the fast.ai course, reading research papers, and diving into research codebases would give me some kind of running start for the course. I was completely wrong. Of course, there are transferable skills between any data-related fields, but successful applied AI folks are just built different: I can think of no better example than <a href="https://www.youtube.com/watch?v=N-qAOv_PNPc">the talk by Teresa Torres</a>.</p>
<p>Now, Teresa has a very impressive background. She studied <a href="https://symsys.stanford.edu/">symbolic systems at Stanford</a>, which was a cross-functional, interdisciplinary approach to what seems like human-software interaction and related systems. She also had experience as a technical product manager. However, her background does not make it any less impressive what she was able to accomplish in the given time frame: in a couple of weeks she created an Applied AI product that was set to be integrated with existing software in beta with robust evals. She applied data science skills because it just made sense to, because she needed to, because she had to, because she was terrified of creating a product that was not validated and tested. She was terrified of using AI-generated code that she didn’t understand, so she had to learn Python. She wanted to cleanly and quickly inspect data, so she learned data visualization. She wanted to be cost-effective when using AI, so she recreated the MapReduce methodology for LLMs from first principles. And she did all this in a few weeks. While her velocity and problem-solving abilities are likely unique to her, I don’t think her overall behavior is unique to folks in Applied AI. After my sense of awe and feeling inspired settled down, I couldn’t help but think that people like her just operate differently than I do. I’m a believer that almost anything can be learned, but there’s just something about applied AI that seems mutually exclusive from AI research, data science or data analysis. I can’t quite put my finger on it because I do believe I’m on the outside of that world looking in. But I felt it when I watched Teresa’s talk.</p>
<p>I also felt it when I was listening to other students ask questions and talk about their projects during office hours. I won’t go into specifics because office hours are meant to be private, but student after student had specific, applied, real-production-level questions and problems that they were trying to solve. I think what I was inspired by the most was that they didn’t wait to take the course to solve these problems. They were already figuring things out with whatever tools and skills they currently had, and were taking this course because they (correctly) bet it was going to provide them a system they needed to get the results they desired. You can also see this “production-ready” nature of the cohorts based on <a href="https://x.com/sh_reya/status/1957139727322411291">the hundreds of testimonials of the course</a>.</p>
<p>I also felt the unique nature of applied AI by a consistent theme taught in the course: the quality of error analysis and the quality of your AI evals depend on your product sense and knowledge. “Looking at data” is just as much of a data science skill as it is a product skill. Data science teaches you how, but product sense teaches you why (and where to look). I think it’s why someone like Teresa, who seems to have fantastic product sense, is able to pick up necessary data science skills to execute on her ability “follow the smell” of failure modes.</p>
<p>I think there certainly are unicorns where ML researchers or data scientists also have good product sense. The <a href="https://www.youtube.com/watch?v=DgPr3HVp0eg">folks at AnswerAI</a> certainly seem to fit this description as was evident in the SolveIt walkthrough during the course. Another example is Omar Khattab, who gave a guest talk on DSPy during the course, who seems to see problems and solutions (and systems) differently (and earlier) than the industry at-large. As a fast.ai student/community member and late interaction enjoyer, I am of course biased.</p>
</section>
<section id="the-value-of-mundane-tasks" class="level2">
<h2 class="anchored" data-anchor-id="the-value-of-mundane-tasks">The Value of Mundane Tasks</h2>
<p>A topic that I’ve been meaning to write about but haven’t yet found the time or right opportunity to do so is the skill of looking at data. I think many different paths in data science and ML (and elsewhere) provide opportunities to build this skill. Personally I built this skill by working in low-tech or even no-tech data environments. I think when you’re a lowly analyst cleaning data entry errors or doing manual data entry yourself, you learn viscerally about the pain points in data collection and how those pain points can find their way to inaccuracies in downstream analyses. When you’re going through binders of handwritten notes and printed PDF tables, cross-referencing aggregate numbers with poorly formatted Excel workbooks containing missing data, you build the resilience and patience necessary to thoroughly “look at data” going in and out of LLMs. I think something that has kept me from becoming a better programmer is I’m not lazy and I enjoy a moderate dose (and see the value) of mundane tasks (such as reading hundreds or thousands of LLM outputs). I think those are two characteristics that have helped me become a better data and LLM wrangler. I think it’s also what will help me become a good maintainer.</p>
<p>A more abstract “skill” is the urge to figure out: why are two things that are supposed to be equal, not equal? And then stubbornly resolving that discrepancy, encountering all sorts of roadblocks (and learning opportunities along the way). Prioritizing this urge has accounted for most of my professional development in ML.</p>
</section>
<section id="my-approach-to-the-course" class="level2">
<h2 class="anchored" data-anchor-id="my-approach-to-the-course">My Approach to the Course</h2>
<p>I somewhat organically found my rhythm in this course. I held myself to the following non-negotiable standard:</p>
<ol type="1">
<li>I would attend every lecture live.</li>
<li>I would attend every office hours live.</li>
<li>I would write a blog post with standout ideas <a href="https://vishalbakshi.github.io/blog/index.html#category=AI%20Evals">from each lesson and corresponding course reader chapter</a>.</li>
</ol>
<p>I didn’t do most of the homeworks, I didn’t apply what I learned to the (personal) projects I was working on (that were now back burner projects).</p>
</section>
<section id="applying-applied-ai-skills" class="level2">
<h2 class="anchored" data-anchor-id="applying-applied-ai-skills">Applying Applied AI Skills</h2>
<p>While I can’t commit to a daily or weekly allotment of hours that I’ll spend applying the learnings from this course, there are two concrete tasks that I can commit to finishing before the end of the year:</p>
<ol type="1">
<li>Perform error analysis on each pipeline step in my <a href="https://youtu.be/NwPKy1rqXT8">AgentFasbook project</a> for one chapter of fastbook.</li>
<li>Perform error analysis (and LLM Judge prompt error estimation “in production”) for my <a href="https://youtu.be/FXOXoaGjntc">TinyScaleLab project</a> for one training run.</li>
</ol>
<p>My AgentFastbook project involves expanding my manually curated fastbook-benchmark IR dataset using an LLM pipeline (decomposing a gold answer into atomic facts –&gt; retrieving chapter passages relevant to those facts –&gt; extracting only relevant text from those passages). I learned from Q+A in the course discord that I should perform error analysis on each step first and then on the full “trace” from end-to-end (because the quality of retrieval can for example effect the quality of relevant text extraction).</p>
<p>My TinyScaleLab project aims to train tiny models to 1) generate coherent english (as shown in <a href="https://arxiv.org/abs/2305.07759">the TinyStories paper</a>) and 2) perform small tasks (like gold answer decomposition or text extraction) reliably after multi-stage finetuning.</p>
</section>
<section id="closing-thoughtstestimonial" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughtstestimonial">Closing Thoughts/Testimonial</h2>
<p>I still have a lingering feeling that I didn’t maximize what I put into the course. I think that’s totally on me. The best I can do now is use that as motivation to apply what I’ve learned to my projects.</p>
<p>One unexpected benefit from this course is that my mind is more open to AI applications. Watching the examples in the course, listening to problems that students are solving in real life, really made me question my relationship with LLMs. I don’t think I fully see the potential and myriad of the problems LLMs can solve. I’m not even sure I see the “shapes” of those problems and solutions. I think the only way to bridge the gap between my understanding of applied AI and the understanding of so many people in the course that I witnessed first-hand is to actually engage in that work. Jeremy Howard and Jonathan Whitaker’s SolveIt talk, even though I was a student in their first cohort, made me question my relationship with problem-solving. Teresa’s talk made me question if I am moving with enough courage in this space. Omar’s talk made me question whether I’m investing enough time in systems thinking.</p>
<p>Engaging in this course with a lot of things going on in my life was a grounding and stabilizing experience. Shreya and Hamel, as knowledgeable and brilliant as they are, are equally welcoming and inclusive. I learned a lot about how to hold space for people to ask challenging or vulnerable questions and what it means to actively encourage community and belonging in a distributed, semi-asynchronous, remote setting. There was a very strong resonance amongst everyone in the cohort. You could tell that everyone was on the same frequency, thinking about the same problems, trying to figure out similar solutions. Everyone had a different angle, background, or story to share, whether it was the guest speakers or the students participating in the office hours. I strongly recommend that everyone using LLMs or building LLM applications take this course.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-17-ai-evals/</guid>
  <pubDate>Sun, 17 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Logit Divergence Between Models Differently Converted to torch.bfloat16</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-17-hf-torch-dtype/</link>
  <description><![CDATA[ 




<div id="cell-1" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span></code></pre></div>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post I’ll illustrate a recent head-scratcher I came across—how to convert a model to <code>torch.bfloat16</code> changes the intermediate and final outputs. I don’t know why this happens and not sure of a path to figure that out.</p>
<p>In <code>model1</code> I specify <code>torch_dtype</code> in <code>AutoModelForCausalLM.from_pretrained</code>. In <code>model2</code>, I don’t, and instead use <code>to(torch.bfloat16)</code> after the model is loaded.</p>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HuggingFaceTB/SmolLM2-135M"</span></span>
<span id="cb2-2">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span></span>
<span id="cb2-3">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(checkpoint)</span>
<span id="cb2-4">model1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16).to(device)</span>
<span id="cb2-5">model2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint).to(device).to(torch.bfloat16)</span></code></pre></div>
</div>
</section>
<section id="comparing-logits" class="level2">
<h2 class="anchored" data-anchor-id="comparing-logits">Comparing Logits</h2>
<p>Given a set of input tokens, the output logits of the two models are not identical.</p>
<div id="cell-8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="80502c56-d418-4fba-dde2-a134dcd209db">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Gravity is"</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>).to(device)</span>
<span id="cb3-2">inputs</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>tensor([[22007,  6463,   314]], device='cuda:0')</code></pre>
</div>
</div>
<div id="cell-9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="782d67bf-c79b-45cc-fb92-9981b1528788">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">model1.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb5-2">logits1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model1(inputs).logits</span>
<span id="cb5-3">logits1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>tensor([[[18.0000, 14.5625, 14.6875,  ..., 16.2500, 16.2500, 22.1250],
         [15.6875, -0.4180, -0.3477,  ...,  8.2500, 12.1250,  7.3438],
         [12.1875, -2.2812, -2.2031,  ...,  7.3750, 10.6875,  8.1875]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;UnsafeViewBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-10" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cf487415-91c9-4464-c064-a506c4a090cb">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">model2.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb7-2">logits2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model2(inputs).logits</span>
<span id="cb7-3">logits2</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>tensor([[[18.0000, 14.5625, 14.6875,  ..., 16.2500, 16.2500, 22.1250],
         [15.7500, -0.2715, -0.2002,  ...,  8.4375, 12.2500,  7.5000],
         [12.3125, -2.2188, -2.1406,  ...,  7.5000, 10.6875,  8.3125]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;UnsafeViewBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9c6a1f1e-6ce9-4a20-ee2d-ef92a3a9f82e">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">torch.allclose(logits1, logits2)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>False</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1377752c-96df-4195-9d6f-052bbbe3cf7d">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">(logits1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> logits2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor(0.3457, device='cuda:0')</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="41d1e426-3945-40e1-c97b-f2e20debb62e">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(logits1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> logits2).mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>tensor(0.0762, device='cuda:0', dtype=torch.bfloat16, grad_fn=&lt;MeanBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="comparing-weights" class="level2">
<h2 class="anchored" data-anchor-id="comparing-weights">Comparing Weights</h2>
<p>A helper function to inspect a particular submodule in a particular layer.</p>
<div id="cell-16" class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _print(model1, model2, module, submodule, layer_idx):</span>
<span id="cb15-2">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model1.model.layers[layer_idx], module), submodule).weight</span>
<span id="cb15-3">    w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model2.model.layers[layer_idx], module), submodule).weight</span>
<span id="cb15-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> torch.allclose: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>allclose(w1, w2)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<div id="cell-17" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2c8feb33-9f2e-4511-9518-0d313209b7d1">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">_print(model1, model2, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"self_attn"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"q_proj"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>self_attn.q_proj torch.allclose: True</code></pre>
</div>
</div>
<p>Looping through all weight matrices in state dicts, they are all identical—why are output logits not identical then? I would assume that something in the matrix ops is causing the divergence.</p>
<div id="cell-19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6f87344e-7033-4bee-e12c-ff02b8bb7a89">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb18-2">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb18-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> model1.state_dict().keys():</span>
<span id="cb18-4">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model1.state_dict()[k]</span>
<span id="cb18-5">    w2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model2.state_dict()[k]</span>
<span id="cb18-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> torch.allclose(w1, w2): n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb18-7">    d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb18-8">n, d, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>d</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(273, 273, 1.0)</code></pre>
</div>
</div>
</section>
<section id="forward-hooks" class="level2">
<h2 class="anchored" data-anchor-id="forward-hooks">Forward Hooks</h2>
<p>Hooking the two models to track intermediate layer outputs.</p>
<div id="cell-22" class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">model1.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb20-2">model2.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb20-3">outputs_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span></code></pre></div>
</div>
<div id="cell-23" class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> capture_output(name):</span>
<span id="cb21-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> hook_fn(module, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>, output):</span>
<span id="cb21-3">        outputs_dict[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].detach()</span>
<span id="cb21-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> hook_fn</span></code></pre></div>
</div>
<div id="cell-24" class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">hooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb22-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>):</span>
<span id="cb22-3">    hooks.append(model1.model.layers[i].register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb22-4">    hooks.append(model2.model.layers[i].register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span></code></pre></div>
</div>
<div id="cell-25" class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb23-2">    model1(inputs)</span>
<span id="cb23-3">    model2(inputs)</span></code></pre></div>
</div>
<div id="cell-26" class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> h <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hooks: h.remove()</span></code></pre></div>
</div>
<p>The difference in intermediate outputs diverges as you pass through the model. That smells of typical floating point precision error.</p>
<div id="cell-28" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6c7066d4-d24a-4f37-da3c-c396f78e69e0">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mean"</span></span>
<span id="cb25-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>):</span>
<span id="cb25-3">    o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb25-4">    o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb25-5"></span>
<span id="cb25-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> torch.allclose(o1, o2):</span>
<span id="cb25-7">        max_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>().item()</span>
<span id="cb25-8">        mean_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb25-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: max diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>max_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb25-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mean"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: mean diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mean_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0: mean diff = 0.0017547607421875
Layer 1: mean diff = 0.005035400390625
Layer 2: mean diff = 0.00830078125
Layer 3: mean diff = 0.010986328125
Layer 4: mean diff = 0.011962890625
Layer 5: mean diff = 0.01251220703125
Layer 6: mean diff = 0.01312255859375
Layer 7: mean diff = 0.0137939453125
Layer 8: mean diff = 0.015380859375
Layer 9: mean diff = 0.0172119140625
Layer 10: mean diff = 0.0189208984375
Layer 11: mean diff = 0.0185546875
Layer 12: mean diff = 0.01953125
Layer 13: mean diff = 0.020751953125
Layer 14: mean diff = 0.021728515625
Layer 15: mean diff = 0.0234375
Layer 16: mean diff = 0.026123046875
Layer 17: mean diff = 0.0263671875
Layer 18: mean diff = 0.0269775390625
Layer 19: mean diff = 0.0301513671875
Layer 20: mean diff = 0.03271484375
Layer 21: mean diff = 0.036376953125
Layer 22: mean diff = 0.044921875
Layer 23: mean diff = 0.05322265625
Layer 24: mean diff = 0.05810546875
Layer 25: mean diff = 0.06689453125
Layer 26: mean diff = 0.0771484375
Layer 27: mean diff = 0.091796875
Layer 28: mean diff = 0.1005859375
Layer 29: mean diff = 0.1484375</code></pre>
</div>
</div>
<p>The max difference in outputs reaches <code>6.0</code> by the 30th layer!</p>
<div id="cell-30" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d9b03a21-de97-48f9-919c-9c252f976f5c">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span></span>
<span id="cb27-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>):</span>
<span id="cb27-3">    o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb27-4">    o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb27-5"></span>
<span id="cb27-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> torch.allclose(o1, o2):</span>
<span id="cb27-7">        max_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>().item()</span>
<span id="cb27-8">        mean_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb27-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: max diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>max_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb27-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mean"</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: mean diff = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mean_diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer 0: max diff = 0.0625
Layer 1: max diff = 0.25
Layer 2: max diff = 0.25
Layer 3: max diff = 0.25
Layer 4: max diff = 0.25
Layer 5: max diff = 0.25
Layer 6: max diff = 0.25
Layer 7: max diff = 0.5
Layer 8: max diff = 0.5
Layer 9: max diff = 0.5
Layer 10: max diff = 1.0
Layer 11: max diff = 1.0
Layer 12: max diff = 1.0
Layer 13: max diff = 0.5
Layer 14: max diff = 0.5
Layer 15: max diff = 0.5
Layer 16: max diff = 0.5
Layer 17: max diff = 0.5
Layer 18: max diff = 0.5
Layer 19: max diff = 0.75
Layer 20: max diff = 0.5
Layer 21: max diff = 0.5
Layer 22: max diff = 0.5
Layer 23: max diff = 0.75
Layer 24: max diff = 1.0
Layer 25: max diff = 2.0
Layer 26: max diff = 2.0
Layer 27: max diff = 2.0
Layer 28: max diff = 1.0
Layer 29: max diff = 6.0</code></pre>
</div>
</div>
<p>Reloading the models and inspecting the outputs of intermediate modules like <code>self_attn</code> and <code>mlp</code>.</p>
<div id="cell-32" class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">model1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16).to(device)</span>
<span id="cb29-2">model2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(checkpoint).to(device).to(torch.bfloat16)</span></code></pre></div>
</div>
<div id="cell-33" class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">modules <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb30-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"self_attn"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"q_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"v_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"o_proj"</span>],</span>
<span id="cb30-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mlp"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gate_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"up_proj"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"down_proj"</span>],</span>
<span id="cb30-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layernorm"</span>: [],</span>
<span id="cb30-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"post_attention_layernorm"</span>: []</span>
<span id="cb30-6">    }</span></code></pre></div>
</div>
<div id="cell-34" class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">outputs_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span></code></pre></div>
</div>
<div id="cell-35" class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> module <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules.keys():</span>
<span id="cb32-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layernorm"</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"post_attention_layernorm"</span>:</span>
<span id="cb32-3">        hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model1.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb32-4">        hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model2.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb32-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb32-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> submodule <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules[module]:</span>
<span id="cb32-7">            hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model1.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module), submodule).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span>
<span id="cb32-8">            hooks.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">getattr</span>(model2.model.layers[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], module), submodule).register_forward_hook(capture_output(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)))</span></code></pre></div>
</div>
<div id="cell-36" class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb33-2">    model1(inputs)</span>
<span id="cb33-3">    model2(inputs)</span></code></pre></div>
</div>
<div id="cell-37" class="cell">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> h <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hooks: h.remove()</span></code></pre></div>
</div>
<p>Interestingly, the intermediate attention outputs are identical but there’s divergence in the outputs of the attention mechanism as it passes through <code>o_proj</code>.</p>
<div id="cell-39" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ce0830e3-31bc-4554-ce6e-ba3d91c133fb">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> module <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules.keys():</span>
<span id="cb35-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_layernorm"</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> module <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"post_attention_layernorm"</span>:</span>
<span id="cb35-3">        o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-4">        o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-5">        diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb35-6">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb35-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb35-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> submodule <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> modules[module]:</span>
<span id="cb35-9">            o1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model1_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-10">            o2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs_dict[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"model2_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>]</span>
<span id="cb35-11">            diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (o1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>o2).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>().mean().item()</span>
<span id="cb35-12">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>module<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>submodule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>diff<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>self_attn.q_proj: 0.0
self_attn.k_proj: 0.0
self_attn.v_proj: 0.0
self_attn.o_proj: 2.1457672119140625e-05
mlp.gate_proj: 0.000476837158203125
mlp.up_proj: 0.0003833770751953125
mlp.down_proj: 0.00177764892578125
input_layernorm: 0.0
post_attention_layernorm: 1.8715858459472656e-05</code></pre>
</div>
</div>
<p>Again I haven’t dug into why these differences exist, but wanted to document that they do.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>deep learning</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-17-hf-torch-dtype/</guid>
  <pubDate>Sun, 17 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 8 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-14-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-8-course-review-and-live-coding-an-annotation-app" class="level2">
<h2 class="anchored" data-anchor-id="lesson-8-course-review-and-live-coding-an-annotation-app">Lesson 8: Course Review and Live Coding an Annotation App</h2>
<p><strong>Idea 1: You should have AI-in-the-loop,</strong> as opposed to being the “human-in-the-loop”. You should drive the AI. You should understand every step of the AI pipeline (as is learned through the process of doing error analysis). As we’ve seen throughout the course, there are strategic opportunities to use LLMs to <em>supplement</em> your analysis, potentially evolving to full automation with routine human validation (e.g.&nbsp;production LLM Judge outputs sampled and reviewed every week).</p>
<p><strong>Idea 2: Provide a detailed prompt when vibe coding an annotation app.</strong> Shreya provided the direct path to the trace CSV, an explanation of the CSV and message column structures (content/roles), and the goals/key characteristics of the app and its UI/UX (such as open coding, a progress bar, navigation buttons and a dropdown of previously used annotations). Shreya asks the LLM to create a <code>plan.md</code> when coding something for the first time and reviews/provides feedback before finalizing it. You can always follow up with more requirements as you review the plan before executing on it.</p>
<p><strong>Idea 3: Implement heuristics for where the user’s attention should go.</strong> One example is semantic and/or keyword highlighting in the displayed trace based on previous annotations. This will help the user more easily identify common failure modes (which supports the core goal: reduce friction! Ease cognitive load!).</p>
<p><strong>Idea 4: Common preferred annotation app UI characteristics</strong> <mark>(note: these are just examples, you should think about your own app requirements when building it)</mark>: expanding/collapse the system prompt, “keywords from user query” at the top which are highlighted in the messages listed below it, annotation should always be visible (shouldn’t require scrolling to view it), highlight domain-specific details (like dates/times for scheduling requests), visually flagged duplicate assistant messages, cleanly render raw JSON messages/tool calls.</p>
<p><strong>Idea 5: Think step by step.</strong> That means you too, not just the LLM! To improve accuracy, start with low hanging fruit (disambiguate your prompt/instructions), then tackle more involved tasks (decomposing the task into smaller subtasks that are easier for the LLM to handle), and finally approach advanced strategies (e.g.&nbsp;fine-tuning, prompt optimization, human review loops to generate more labeled examples).</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-14-ai-evals/</guid>
  <pubDate>Thu, 14 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 7 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-12-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-7-interfaces-for-human-review" class="level2">
<h2 class="anchored" data-anchor-id="lesson-7-interfaces-for-human-review">Lesson 7: Interfaces for Human Review</h2>
<p><strong>Idea 1: Custom UIs = 10x review throughput</strong> compared to reviewing in a spreadsheet. This because custom UIs allow a domain-aware view (emails structured like your inbox instead of a string of text) and hotkeys for navigation or one-click tags, <em>and</em> takes only 1 hr to prototype nowadays. A middle-ground between spreadsheets and custom UIs: jupyter notebook (a pseudo-interface) especially with the <a href="https://ipython.readthedocs.io/en/stable/config/integrating.html#rich-display:~:text=_repr_html_%20should%20return%20HTML%20as%20a%20str"><code>_repr_html_</code> method</a>.</p>
<p><strong>Idea 2: HCI Principles for UIs (Nielsen, 1994).</strong> Visibility of status (let your user know where they are), recognition over recall (assign tags instead of free-form text in second round of error analysis and beyond), match the real world (native end user display form; results in catching errors only apparent in this form), user control (pass/fail 1-key press, undo, tag select w/number keys, “defer” for uncertainty, goal: <em>get the user into a flow state</em>), minimalist first (expand on demand). Add a progress bar whenever you’re making a user wait for something. Overall principle: <mark>reduce friction</mark>.</p>
<p><strong>Idea 3: Nerd-snipe your features.</strong> Shreya implemented a highlight feature where on the backend their app looks for semantic or keyword similarities with previous failed samples and highlights those words in the current example display to flag common issues for easier user identification. Super cool. Another similar example: batch-label similar traces after clustering to wipeout repeat bugs. I never considered integrating machine intelligence into error analysis before this!</p>
<p><strong>Idea 4: Criteria drift happens!</strong> Reviewers’ definitions change over time so keep rubrics and labels editable. What you think was acceptable/unacceptable changes as you review real traces. Additionally, humans’ understanding of LLM capabilities also evolves over time (i.e.&nbsp;humans align with LLMs as LLMs align with humans).</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-12-ai-evals/</guid>
  <pubDate>Tue, 12 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lessons 5 + 6 and Chapter 7 from the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-07-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-5-architecture-specific-evaluation-strategies" class="level2">
<h2 class="anchored" data-anchor-id="lesson-5-architecture-specific-evaluation-strategies">Lesson 5: Architecture-Specific Evaluation Strategies</h2>
<p><strong>Idea 1: Include edge cases in few shot examples</strong>. Show the LLM examples that you might struggle with, give it a lot of information in each example. I imagine that you will gain a better understanding of what truly contributes to Pass or Fail judgments as you curate difficult few shot examples. Don’t just randomly pick examples, cherry pick them based on how well they complement the rest of your prompt. Give examples that are the most instructive.</p>
<p><strong>Idea 2: Use automation strategically</strong>. We don’t want to not look at our data, but we also want to use the reasoning power of LLMs. Shreya fed an LLM their open codes, axial codes and traces and asked it to label true/false (if LLM responses in trace are substantiated with tool outputs) for each trace and provide a rationale. Shreya trusted the LLM’s true/false labels because they provided it with open codes.</p>
<p><strong>Idea 3: Don’t provide open/axial codes in LLM Judge Prompt few shot examples.</strong> We’re using this judge in production on unlabeled traces which will not have open/axial codes so we don’t want the LLM Judge to learn/expect these codes to be present.</p>
<p><strong>Idea 4: Aim for 80-85% LLM Judge TNR and TPR.</strong> 50% is random chance, 100% probably means something’s wrong in your judge prompt.</p>
<p><strong>Idea 5: The fastest way for you to fail in an AI project is for people to lose trust in what you’re doing.</strong> There’s a human bias that people have in trusting what a computer says. Don’t take the judge at face value. Run some tests to evaluate the confidence interval of the Judge, unbiasing its success rate (bias = Judge labels “Pass” more than “Fail” by default or vice vesa). The eval should align with the product experience.</p>
</section>
<section id="lesson-6-rag-cicd" class="level2">
<h2 class="anchored" data-anchor-id="lesson-6-rag-cicd">Lesson 6: RAG, CI/CD</h2>
<p><strong>Idea 1 Do error analysis on the whole system, but do evals on retrieval and LLM generation separately.</strong> Make sure your retriever’s Recall@k is 80%+, then perform error analysis, otherwise you’re evaluating generation errors based on flawed context. Don’t use popular metrics to evaluate generation—measure what’s relevant to your product, which you will uncover during your analysis.</p>
<p><strong>Idea 2 Bring domain knowledge to chunk size.</strong> Is there a natural breaking point in your document? What is a meaningful chunk in the context of your domain? The chunks ultimately represent the document during search. It’s okay to have variable chunk sizes.</p>
<p><strong>Idea 3 Likert scales have a use!</strong> Shreya asks an LLM to score synthetic queries (when creating an evaluation dataset) on a Likert scale and filters out queries with scores of 1 or 2 (out of 5). These scores are discarded after this filtering use.</p>
<p><strong>Idea 4 Ground synthetic queries in realism:</strong> User queries are often confusing to interpret, incomplete, and contain typos/grammatical errors. The queries in your evaluation dataset should reflect such nuances to provide meaningful use cases for retrieval.</p>
<p><strong>Idea 5 Optimize for Recall@k first.</strong> Shreya has rarely seen utility in optimizing for Precision@k first (how many of the top-k retrieved chunks are relevant?) because the consumer of these chunks in a RAG pipeline is an LLM, which cares more about how many of the total relevant chunks are present in the top-k retrieved chunks (Recall@k) to generate a relevant response. LLMs are getting better at reasoning over the retrieved chunks to determine relevance. Use MRR@k (how high up in the ranking is the first relevant chunk?) after optimizing for Recall@k as MRR@k measures how quickly the LLM finds <em>an</em> answer.</p>
<p><strong>Idea 6: Focus on process, not tools.</strong> Which goes against how most people think about building AI systems. If something’s not working, your first instinct should be to actually understand what is going wrong, not to plug-in a different tool in hopes for improvement, or sweep different hyperparameters. Additionally, don’t get lost in the vector DB sauce—start with basic BM25 keyword search first.</p>
</section>
<section id="lesson-7-evaluating-retrieval-augmented-generation-rag" class="level2">
<h2 class="anchored" data-anchor-id="lesson-7-evaluating-retrieval-augmented-generation-rag">Lesson 7: Evaluating Retrieval-Augmented Generation (RAG)</h2>
<p><strong>Idea 1: Multi-stage retrieval and the Recall/Precision trade-off</strong>. LLMs can handle many passages so we want to make sure it’s provided as many relevant passages as possible, this means increasing the number of passages provided. However, long contexts cost more and are limited by the LLM’s context window. We use a cheaper retriever to do a first pass on retrieving relevant passages (Recall@k) and a more powerful retriever to then re-rank them (Precision@k, MRR or NDCG@k). We take the top-k (where k is smaller than the first pass retrieved passages) and pass that to the LLM.</p>
<blockquote class="blockquote">
<p>Modern LLM attend more strongly to salient tokens, so they can often ignore irrelevant content if the key information is present. But if that information is missing altogether–low recall–then the generator has no way to produce a correct answer.</p>
</blockquote>
<p><strong>Idea 2: The ARES framework complements error analysis.</strong> Precisely evaluating “Answer Faithfulness” (failures include hallucinations, omissions and misinterpretations) and “Answer Relevance” (failure = factually correct based on context but irrelevant to the query) requires error analysis to identify where and how specific failure modes occur in our product.</p>
<p><strong>Idea 3: Be wary of synthentically generated queries</strong> as they often are not representative of the messy queries encountered in production. Regularly validate these queries, referncing real queries from logs or human-curated examples.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-07-ai-evals/</guid>
  <pubDate>Thu, 07 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 4 and Chapter 5 of the Course Reader from the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-08-03-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-4-automated-evaluators" class="level2">
<h2 class="anchored" data-anchor-id="lesson-4-automated-evaluators">Lesson 4: Automated Evaluators</h2>
<p><strong>Idea 1: You can’t measure what you don’t ask for.</strong> If your prompt didn’t include an instruction on providing links to X, and the LLM doesn’t provide those links, that’s a specification failure (fix the prompt!). If the prompt did include it but the LLM failed to apply the instruction, that’s a generalization failure and should be tracked by an automated evaluator.</p>
<p><strong>Idea 2: Use code-based evaluators if you can,</strong> as they are deterministic. They take as input the trace and a failure mode and return <code>True</code> or <code>False</code> or some score for objective rule-based checks (e.g.&nbsp;parsing structure, regex/string matching for keywords, structural constraints, tool execution errors).</p>
<p><strong>Idea 3: Just because you can ask an LLM Judge anything you want, doesn’t mean you should.</strong> Use LLM Judges to do specific, well-defined, binary failure mode classification (Pass/Fail) tasks. A Pass/Fail LLM Judge score is easier to assess and leads to easy-to-interpret Judge accuracy. Also, don’t pack multiple criteria into one prompt, create a prompt for each criterion.</p>
<p><strong>Idea 4: Don’t leak test instances into your process of building an LLM Judge</strong> Use 10-20% of labeled axial coding data to curate Judge prompt few shot examples (training set), ~40% to iteratively improve the prompt (dev set), and ~40% for final unbiased Judge evalation after prompt tuning is done (test set). The last thing you want in your prompt is a few shot example that’s in the test set. <mark>Low dev set performance (TPR and TNR) tell us that the few shot examples from the train set do not generalize.</mark></p>
<p><strong>Idea 5: Don’t show your LLM Judge what it’s already good at.</strong> Your few shot examples should show difficult/tricky situations for evaluation. To do this, manually iterate the examples. I would imagine that like open coding, you would build a more nuanced intuition about your data (and your product!) through this process. Other tips: write your examples like you are explaining it to a human; try to include the best example of a pass or fail; your examples can also contain reasoning to provide richer “grounding” to your LLM.</p>
</section>
<section id="chapter-5-implementing-automated-evaluators" class="level2">
<h2 class="anchored" data-anchor-id="chapter-5-implementing-automated-evaluators">Chapter 5: Implementing Automated Evaluators</h2>
<p><strong>Idea A: Reference-based and reference-free metrics serve different purposes.</strong> A “reference” here means “reference LLM output”. Reference-based metrics allow iterative development with holistic checks (whether the LLM output match the golden reference). Reference-free metrics better adapt at scale on new, unlabeled data (as they measure intrinsic properties or rules related to failure modes). Reference-based metric: LLM output matches a “golden” trace with a specific sequence of tool calls. Reference-free metric: LLM output contains valid tool call names.</p>
<p><strong>Idea B: Test your judge on unlabeled data.</strong> Even the test set is biased as it represents a portion of our labeled data, which may not be representative of broader out-of-domain situations your Judge will inevitably encounter. We use the Judge’s “raw success rate” (number of Pass labels/number of unlabeled traces) and a series of calculations on random test set samples to estimate within a confidence interval the Judge’s “true success rate.”</p>
<p><strong>Idea C: Judges don’t come pretrained on a product’s values—we have to teach them.</strong> This is why we validate the judge’s response and calculate metrics around alignment (like TNR and TPR). This is also why we provide few shot examples so the Judge can evaluate on the specific desired characteristics of our product (especially for vibe-y dimensions like tone).</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-08-03-ai-evals/</guid>
  <pubDate>Sun, 03 Aug 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>TIL: How LLM-Foundry Ignores Prompt Tokens in labels for Instruction Finetuning.</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-31-llm-foundry/</link>
  <description><![CDATA[ 




<p>I recently realized (after a conversation with a colleague) that I didn’t know how LLM-Foundry replaces prompt tokens with <code>-100</code> to exclude them from the loss calculation. I fed Claude project knowledge the LLM-Foundry GitHub repo and asked it to walk me through this process line-by-line. Since “Chat using integrations can’t be shared” I’m pasting Claude’s walkthrough here (with a couple of my notes). All the referenced functions are inside <a href="https://github.com/mosaicml/llm-foundry/blob/main/llmfoundry/data/finetuning/collator.py"><code>llmfoundry/data/finetuning/collator.py</code></a></p>
<section id="setup-input-example" class="level2">
<h2 class="anchored" data-anchor-id="setup-input-example">Setup: Input Example</h2>
<p>Let’s say we have this raw training example:</p>
<pre><code>{
    'prompt': 'What is photosynthesis?',
    'response': 'Photosynthesis is the process by which plants convert sunlight into energy.'
}</code></pre>
<p>After tokenization, this becomes:</p>
<pre><code>example = {
    'turns': [
        {
            'input_ids': [1127, 318, 2825, 43943, 30],      # "What is photosynthesis?"
            'labels': [21197, 43943, 318, 262, 1429, 416, 543, 6134, 10385, 4252, 1657, 656, 2568, 13]  # "Photosynthesis is the process..."
        }
    ]
}</code></pre>
</section>
<section id="line-by-line-processing" class="level2">
<h2 class="anchored" data-anchor-id="line-by-line-processing">Line-by-Line Processing</h2>
<section id="collator-entry-point-seq2seqfinetuningcollator.__call__" class="level3">
<h3 class="anchored" data-anchor-id="collator-entry-point-seq2seqfinetuningcollator.__call__">Collator Entry Point (<code>Seq2SeqFinetuningCollator.__call__</code>)</h3>
<pre><code>def __call__(self, examples: list[TokenizedExample]) -&gt; dict[str, torch.Tensor]:
    # examples is a list of TokenizedExample objects (batch)
    
    if self.decoder_only_format:
        batch = self._process_and_batch_decoder_only(examples)  # Most common path</code></pre>
</section>
<section id="batch-processing-_process_and_batch_decoder_only" class="level3">
<h3 class="anchored" data-anchor-id="batch-processing-_process_and_batch_decoder_only">Batch Processing (<code>_process_and_batch_decoder_only</code>)</h3>
<pre><code>def _process_and_batch_decoder_only(self, examples: list[TokenizedExample]):
    processed_examples = []
    
    # Step 1: Process each example in the batch through stitch_turns_decoder_only
    input_ids_and_labels = [
        stitch_turns_decoder_only(
            example_turns=example['turns'],           # The prompt/response turns
            target_prompts=self.target_prompts,      # 'none' 
            target_responses=self.target_responses,   # 'last'
            eos_token_id=self.tokenizer.eos_token_id,
        ) for example in examples
    ]</code></pre>
</section>
<section id="core-processing-stitch_turns_decoder_only" class="level3">
<h3 class="anchored" data-anchor-id="core-processing-stitch_turns_decoder_only">Core Processing (<code>stitch_turns_decoder_only</code>)</h3>
<p>This is where the magic happens:</p>
<pre><code>def stitch_turns_decoder_only(example_turns, target_prompts, target_responses, eos_token_id):
    target_prompts = target_prompts.lower()  # 'none'
    target_responses = target_responses.lower()  # 'last'
    
    # Line 1: Look up the target policy functions
    prompt_to_target = _TARGET_POLICY_LOOKUP[target_prompts]  # Gets _sequence_to_labels_none
    response_to_target = _TARGET_POLICY_LOOKUP[target_responses]  # Gets _sequence_to_labels_last
    
    # Line 2: Initialize output sequences
    input_ids = []
    labels = []
    
    # Line 3: Process each turn in the conversation
    for idx, turn in enumerate(example_turns):
        is_last_turn = idx + 1 == len(example_turns)  # True for our single-turn example
        
        # Line 4: Extract the prompt and response token sequences
        context = ensure_list(turn['input_ids'])   # [1127, 318, 2825, 43943, 30] - "What is photosynthesis?"  
        target = ensure_list(turn['labels'])       # [21197, 43943, 318, ...] - "Photosynthesis is..."
        
        # Line 5: Add EOS token to final response if needed
        if is_last_turn and eos_token_id is not None:
            if target[-1] != eos_token_id:
                target = target + [eos_token_id]   # Append &lt;|endoftext|&gt; token
        
        # Line 6: Build the concatenated input_ids sequence
        input_ids += context  # Add prompt tokens: [1127, 318, 2825, 43943, 30]
        input_ids += target   # Add response tokens: [1127, 318, 2825, 43943, 30, 21197, 43943, 318, ...]
        
        # Line 7: Build the labels sequence using target policies
        labels += prompt_to_target(context, is_last_turn, prompt_cutoff)  # Calls _sequence_to_labels_none
        labels += response_to_target(target, is_last_turn)                # Calls _sequence_to_labels_last</code></pre>
<blockquote class="blockquote">
<p>Vishal note: in Line 7, <code>labels</code> starts out as an empty list <code>[]</code> so the prompt is added first (via <code>+= prompt_to_target(...)</code>), after which the response is added (via <code>+= response_to_target</code>).</p>
</blockquote>
</section>
<section id="target-policy-functions" class="level3">
<h3 class="anchored" data-anchor-id="target-policy-functions">Target Policy Functions</h3>
<pre><code>def _sequence_to_labels_none(sequence, is_last_turn, cutoff):
    # Line 1: Convert ALL prompt tokens to -100 (ignore during loss)
    return [CROSS_ENTROPY_IGNORE_INDEX] * len(sequence)  # [-100, -100, -100, -100, -100]

def _sequence_to_labels_last(sequence, is_last_turn, cutoff):  
    # Line 1: Check if this is the final turn
    if is_last_turn:  # True in our case
        return sequence  # Keep actual token IDs: [21197, 43943, 318, ...]
    else:
        return [CROSS_ENTROPY_IGNORE_INDEX] * len(sequence)  # Would be [-100, -100, ...] for non-final turns</code></pre>
</section>
<section id="result-after-stitch_turns_decoder_only" class="level3">
<h3 class="anchored" data-anchor-id="result-after-stitch_turns_decoder_only">Result After <code>stitch_turns_decoder_only</code></h3>
<pre><code># Returned values:
input_ids = [1127, 318, 2825, 43943, 30, 21197, 43943, 318, 262, 1429, 416, 543, 6134, 10385, 4252, 1657, 656, 2568, 13, 50256]
#            |----------prompt----------|  |-------------------response---------------------------|  |EOS|

labels =    [-100, -100, -100, -100, -100, 21197, 43943, 318, 262, 1429, 416, 543, 6134, 10385, 4252, 1657, 656, 2568, 13, 50256]
#            |------ignore prompt-------|  |-------------------keep response--------------------|  |EOS|</code></pre>
<blockquote class="blockquote">
<p>Vishal note: we added <code>prompt_to_target</code> first to the empty <code>labels</code> list so the <code>-100</code>s come first.</p>
</blockquote>
</section>
<section id="back-to-batch-processing-continued" class="level3">
<h3 class="anchored" data-anchor-id="back-to-batch-processing-continued">Back to Batch Processing (continued)</h3>
<pre><code># Step 2: Handle sequence length limits
for input_ids, labels in input_ids_and_labels:
    orig_size = len(input_ids)
    
    # Truncate if too long
    if orig_size &gt; max_seq_len:
        input_ids = input_ids[:max_seq_len]
        labels = labels[:max_seq_len]
    
    # Create attention mask
    attention_mask = [1] * len(input_ids)  # All 1s for actual tokens
    
    # Step 3: Pad sequences to max_seq_len
    n_total = len(input_ids)
    i_pad = [CROSS_ENTROPY_IGNORE_INDEX] * (max_seq_len - n_total)  # Padding with -100
    
    if self.tokenizer.padding_side == 'left':
        labels = i_pad + labels  # Left pad labels with -100
    else:
        labels = labels + i_pad  # Right pad labels with -100 (more common)
    
    processed_example = {
        'input_ids': input_ids,      # Will be padded by tokenizer.pad()
        'labels': labels,            # Already padded with -100
        'attention_mask': attention_mask,  # Will be padded by tokenizer.pad()
    }
    processed_examples.append(processed_example)

# Step 4: Final padding using HuggingFace tokenizer
batch = self.tokenizer.pad(
    processed_examples,
    padding='max_length',
    max_length=max_seq_len,
    return_tensors='pt',
)</code></pre>
</section>
<section id="final-batch-output" class="level3">
<h3 class="anchored" data-anchor-id="final-batch-output">Final Batch Output</h3>
<pre><code>batch = {
    'input_ids': torch.tensor([
        [1127, 318, 2825, 43943, 30, 21197, 43943, 318, ..., 0, 0, 0]  # Padded with pad_token_id (0)
    ]),
    'labels': torch.tensor([
        [-100, -100, -100, -100, -100, 21197, 43943, 318, ..., -100, -100, -100]  # Padded with -100
    ]),
    'attention_mask': torch.tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0]  # 1 for real tokens, 0 for padding
    ])
}</code></pre>
</section>
<section id="key-points" class="level3">
<h3 class="anchored" data-anchor-id="key-points">Key Points</h3>
<ol type="1">
<li>Prompt tokens → Labels = -100: The _sequence_to_labels_none function converts all prompt tokens to -100</li>
<li>Response tokens → Labels = actual token IDs: The _sequence_to_labels_last function keeps response tokens as-is</li>
<li>Padding tokens → Labels = -100: All padding positions also get -100</li>
<li>Loss calculation: PyTorch’s CrossEntropyLoss(ignore_index=-100) will only compute loss on response tokens</li>
</ol>
<p>This ensures the model learns to generate responses but never tries to “predict” the input prompt!</p>


</section>
</section>

 ]]></description>
  <category>LLM-Foundry</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-31-llm-foundry/</guid>
  <pubDate>Thu, 31 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 3 and Chapter 4 of the Course Reader from the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-31-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-3-error-analysis-contd." class="level2">
<h2 class="anchored" data-anchor-id="lesson-3-error-analysis-contd.">Lesson 3: Error Analysis (cont’d.)</h2>
<p><strong>Idea 1: Open code at the trace-level</strong>. Many errors only emerge in the context of the whole trace and mistakes often cascade across turns (i.e.&nbsp;label only the <em>first</em> failure).</p>
<p><strong>Idea 2: Manually classify open codes using AI-generated axial codes.</strong> This verifies whether the AI-generated codes are applicable (accurate and relevant). During the lesson ChatGPT missed “Did not invoke tool” in its initial axial code generation based on open codes. This type of mistake is common and is why we manually apply LLM-generated axial codes.</p>
<p><strong>Idea 3: If possible, reproduce a multi-turn error with a simpler single-turn test case</strong>. For example if a multi-turn conversation fails when the LLM tries to retrieve some information, (e.g.&nbsp;return the correct price for product X), create a new single-turn conversation targeting just that task. This “minimal reproducible error” is analogous to software engineering’s “minimal reproducible bug”. In both cases, you’re cutting through the noise and targeting the single task that fails; this help find the root cause.</p>
</section>
<section id="chapter-4-collaborative-evaluation-practices" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4-collaborative-evaluation-practices">Chapter 4: Collaborative Evaluation Practices</h2>
<p><strong>Idea A: When possible, have only one person make the final judgment call on AI evaluation</strong>. You want a single person making decisions about the success or failure of the AI outputs to reduce noisy cooks in the kitchen. You need that person to have deep domain knowledge or be someone who represents the target users. Hamel and Shreya call this person the Principal Domain Expert.</p>
<p><strong>Idea B: Annotator disagreements inform rubric improvements, not retroactive label updates</strong>. You’re not trying to win an argument. This idea reminds me of being on an interview panel: when you discuss rubric scores with the highest disagreement, the goal is not for panelists to change their scores, the goal is to come to a common understanding of what rubric item was measuring, and if needed, update your understanding to reach concensus for future candidate assessments. If your process is flawed, try to correct it as soon as you can.</p>
<p><strong>Idea C: Battle-test your artifacts manually</strong>. The iteratively improved human annotator rubric and the concensus labeled dataset becomes the gold standard used for automated evaluators. The rubric becomes the specification passed to the LLM-Judge. A recurring theme in this course: don’t build something in the abstract, build it while grounded in real data. Just as we don’t predefine axial codes for failure modes before we look at the data and document open codes, we don’t predefine a rubric for the LLM Judge before we look at the data. There’s a feedback loop between error analysis artifacts (open codes, axial codes, annotator rubrics, annotation scores) and looking at your data. Looking at data builds your intuition which then informs the error analysis artifacts.</p>
<p><strong>Idea D: Even with multiple annotators, there’s an escalation path.</strong> A benevolent dictator may need to intervene and make the final call if the annotators can’t come to concensus. This underscores the importance of identifying a single Principal Domain Expert <em>even during collaborative evaluation</em>.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-31-ai-evals/</guid>
  <pubDate>Thu, 31 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Standout Ideas from Lesson 2 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-28-ai-evals/</link>
  <description><![CDATA[ 




<section id="lesson-2-error-analysis" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-error-analysis">Lesson 2: Error Analysis</h2>
<p><strong>Idea 1: Error analysis is how we close the gulf of comprehension.</strong> The gulf of comprehension is the gap between the information the data contains and your understanding of it. While it’s either impossible or unreasonable to look at every single piece of data collected from your users, Hamel and Shreya recommend looking at at least 100 traces.</p>
<p><strong>Idea 2: You need ~100 diverse traces to get a good representation of failure modes.</strong> A trace is a full record of the pipeline’s interaction (initil user query, all LLM inputs/outs, intermediate reasoning or tool calls, and final user-facing result). The number 100 is not a hard and fast rule, but signifies that 10-15 traces is not going to be enough. You want to look at enough traces such that you reach a theoretical saturation of failure modes. What that means is you’ve seen enough traces that are diverse enough that looking at any more traces will not introduce any new failure modes that you haven’t seen yet.</p>
<p><strong>Idea 3: Open coding is Hamel’s favorite subject in evals.</strong> Open codes are brief, descriptive notes about any observed problems, surprising actions or where behavior feels wrong or unexpected in the trace. As you write open codes, categories of errors will emerge.</p>
<p><strong>Idea 4: During open coding, don’t find the root cause, just observe and note.</strong> As you come across failed traces, it’s tempting to make note of why you think this failure occurred. For example, if you see that the LLM tool call return value did not contain the correct information, it’s tempting to step back and think about why that took place. Maybe the input to the tool was incorrect because your voice agent converted speech to text incorrectly. Maybe there’s something wrong in the database which resulted in leading to missed matches. As you can see, this type of pontification can potentially be endless and is not fruitful for the focused process of identifying failure modes. Just note the failure and move on to the next trace. We’ll think about root causes later.</p>
<p><strong>Idea 5: Pull the main failure mode from each trace. don’t get embroiled in the details.</strong> Relatedly, it’s tempting to zoom in as deep as you can, putting each word of the trace under a microscope. For folks who like analyzing data, this is satisfying. But it introduces noise in identifying high-priority failure modes. These open codes are later going to be clustered by theme (Axial Coding) and higher-level themes will be extracted; too granular of an analysis is a waste of time.</p>
<p><strong>Idea 6: The quality of open coding is going to hinge on your product sense.</strong> Successful open coding depends on your ability to skillfully look at data. This involves two broad skills: 1) you have to know what to look for, 2) you have to be detail-oriented enough to find the failure by looking at a sequence of messages. Knowing what to look for requires a deep understanding of your product from the user’s perspective.</p>
<p><strong>Idea 7: Start with a simple approach to get value fast.</strong> Fatigue from cognitive load is a real thing, especially when you’re looking at a hundred traces, many of which can involve multiple messages between the user and the assistant and multiple tool calls. The success of error analysis is going to depend on how fresh your mind is during the process. To achieve this, keep your open coding and axial coding heuristics simple. You can always do a second, third, and fourth pass through future traces after you fix the most pressing failure modes.</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-28-ai-evals/</guid>
  <pubDate>Mon, 28 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Analyzing PyTorch 2.0 Release Notes for ColBERT Dependency Impact</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-27-torch-colbert/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post, I walk through the <a href="https://github.com/pytorch/pytorch/releases/tag/v2.0.0">PyTorch 2.0 Release Note</a> PRs where I’m estimating there will be some kind of impact to ColBERT as I update the torch dependency to 2.0 (ColBERT is currently dependent on torch==1.13.1). The level of detail in my analysis of a PyTorch PR is not necessarily signifying its importance. In some cases, I am using this analysis as an opportunity to get more familiar with details about the ColBERT codebase (such as the number of instances where <code>torch.cat</code> is used).</p>
</section>
<section id="full-release-notes-analysis" class="level2">
<h2 class="anchored" data-anchor-id="full-release-notes-analysis">Full Release Notes Analysis</h2>
<p>You can find my item-by-item PyTorch 2.0 release notes analysis for ColBERT in <a href="https://docs.google.com/spreadsheets/d/1sUEN7xo5-hLVoxF9NL_ibGxPaKlPzxmnMU46zf3wd-U/edit?usp=sharing">this Google Sheet</a>.</p>
<p>Overall, across 508 PyTorch PRs, I have estimated that 455 of them are not applicable to ColBERT and 42 (8.7%) have a potential impact. I was unclear if or how 11 of the PyTorch 2.0 PRs would affect ColBERT (2.6%).</p>
<p>There are 5 sections in the PyTorch 2.0 Release Notes, here’s a break down of PRs by section that will have a potential (or unclear) impact on ColBERT:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Section</th>
<th style="text-align: center;"># of PRs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Improvements</td>
<td style="text-align: center;">22</td>
</tr>
<tr class="even">
<td style="text-align: center;">Bug Fixes</td>
<td style="text-align: center;">21</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;">Backwards Incompatible Changes</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Deprecations</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>In my estimation, the improvements and bug fixes PRs in PyTorch 2.0 will only improve the performance of ColBERT. That being said, there still may be noticeable differences in indexing, search, and training artifacts which may break tests I write for before/after comparisons.</p>
<p>Fortunately, only two backward-compatible changes may affect ColBERT.</p>
<p>There are 11 subsections in the PyTorch 2.0 Release Notes, here’s a break down of PRs that will have a potential (or unclear) impact on ColBERT:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Subsection</th>
<th style="text-align: center;"># of PRs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">MPS</td>
<td style="text-align: center;">12</td>
</tr>
<tr class="even">
<td style="text-align: center;">Python API</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cuda</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td style="text-align: center;">Releng</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Distributed</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">Build</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ONNX</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td style="text-align: center;">Cpu</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Cpp API</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">torch.nn API</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>I am including MPS-related PRs in this analysis just in case we consider making ColBERT compatible with MPS in the future.</p>
<p>I’ll start by analyzing breaking changes, which are likely going to be the most impactful.</p>
</section>
<section id="backwards-incompatible-changes" class="level2">
<h2 class="anchored" data-anchor-id="backwards-incompatible-changes">Backwards Incompatible Changes</h2>
<section id="pr-92731" class="level3">
<h3 class="anchored" data-anchor-id="pr-92731">PR <a href="https://github.com/pytorch/pytorch/pull/92731">#92731</a></h3>
<blockquote class="blockquote">
<p>Gradients are now set to None instead of zeros by default in <code>torch.optim.*.zero_grad()</code> and <code>torch.nn.Module.zero_grad()</code> (#92731)</p>
</blockquote>
<p>There are two lines in ColBERT where<code>zero_grad</code> is called: in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L37">colbert/utils/amp.py</a> and in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L61">colbert/training/training.py</a>. I’m not sure how this change would affect ColBERT behavior, but flagging it as something to keep in mind.</p>
</section>
<section id="pr-92306" class="level3">
<h3 class="anchored" data-anchor-id="pr-92306">PR <a href="https://github.com/pytorch/pytorch/pull/92306">#92306</a></h3>
<blockquote class="blockquote">
<p>Algorithms <code>{Adadelta, Adagrad, Adam, Adamax, AdamW, ASGD, NAdam, RAdam, RMSProp, RProp, SGD}</code> default to faster <code>foreach</code> implementation when on CUDA + differentiable=<code>False</code></p>
</blockquote>
<p>This PR adds the following lines to <code>AdamW</code>, which is used in ColBERT’s <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L60">`training.py</a>:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> foreach <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb1-2">    foreach <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _default_to_foreach(</span>
<span id="cb1-3">        [params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps],</span>
<span id="cb1-4">        differentiable<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>differentiable)</span></code></pre></div>
<p><code>foreach</code> is <code>None</code> in ColBERT, as it’s not specified and that’s what it defaults to (<code>foreach: Optional[bool] = None</code>):</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AdamW(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> p: p.requires_grad, colbert.parameters()), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>config.lr, eps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-8</span>)</span></code></pre></div>
<p>Since this is described as a “faster implementation”, I would expect the training time to decrease. <mark>I’ll be on the lookout for this when comparing training time benchmarks before/after upgrading to PyTorch 2.0</mark>.</p>
</section>
<section id="pr-88913" class="level3">
<h3 class="anchored" data-anchor-id="pr-88913">PR <a href="https://github.com/pytorch/pytorch/pull/88913">#88913</a></h3>
<blockquote class="blockquote">
<p>Update <code>torch.tensor</code> and <code>nn.Parameter</code> to serialize all their attributes (#88913)</p>
</blockquote>
<p><mark>It’s unclear what this PR is doing but since it’s touching the <code>nn.Parameter</code> definition, I’m flagging it.</mark></p>
</section>
</section>
<section id="bug-fixes" class="level2">
<h2 class="anchored" data-anchor-id="bug-fixes">Bug Fixes</h2>
<p>I would expect PyTorch PRs that introduce bug fixes to only positively affect ColBERT. That being said, a positive effect is still a change and can potentially impact concrete artifacts during indexing, search and training. I am planning on curating a baseline set of these artifacts before I test the upgrade to PyTorch 2.0.</p>
<section id="pr-92810" class="level3">
<h3 class="anchored" data-anchor-id="pr-92810">PR <a href="https://github.com/pytorch/pytorch/pull/92810">#92810</a></h3>
<blockquote class="blockquote">
<p>Fix SIGSEGV on a big-endian machine when reading pickle data (#92810)</p>
</blockquote>
<p>The PR states:</p>
<blockquote class="blockquote">
<p>This PR fixes SIGSEGV on a big-endian machine when reading pickle data.</p>
</blockquote>
<p>I’m not familiar with the term “big-endian” so had to look it up:</p>
<blockquote class="blockquote">
<p>A big-endian system stores the most significant byte of a word at the smallest memory address and the least significant byte at the largest. A little-endian system, in contrast, stores the least-significant byte at the smallest address. (<a href="https://en.wikipedia.org/wiki/Endianness">source</a>)</p>
</blockquote>
<p>Claude’s understanding of the cpp method affected by this PR is that it affects the <code>torch.load</code> method. There are a number of ColBERT files that use <code>torch.load</code>:</p>
<ul>
<li>colbert/utils/coalesce.py uses it to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L47">load <code>codes.pt</code></a> (centroid id for each embedding in chunk) and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L66">load <code>residuals.pt</code></a> (16-bits residual for each embedding in chunk).</li>
<li>colbert/search/index_loader.py uses it to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_loader.py#L33">load <code>ivf.pid.pt</code></a> or <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_loader.py#L36"><code>ivf.pt</code></a>.</li>
<li>colbert/utils/utils.py uses it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L44">in <code>torch_load_dnn</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L91"><code>load_checkpoint_raw</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/utils.py#L205"><code>load_ranking</code></a>.</li>
<li>colbert/indexing/index_manager.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/index_manager.py#L20"><code>load_index_part</code></a>.</li>
<li>colbert/indexing/codecs/residual_embeddings.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual_embeddings.py#L86"><code>load_codes</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual_embeddings.py#L93"><code>load_residuals</code></a>.</li>
<li>colbert/indexing/codecs/residual.py uses it in <code>load</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L141">centroids</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L142"><code>avg_residual</code></a>, and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L143"><code>bucket_cutoffs, bucket_weights</code></a>.</li>
<li>colbert/indexing/collection_indexer.py uses it in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L256"><code>_concatenate_and_split_sample</code></a>.</li>
<li>colbert/index_updater.py uses it in <code>_load_disk_ivf</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L281"><code>ivf.pid.pt</code></a> or <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L286"><code>ivf.pt</code></a>, in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L312"><code>load_chunk_codes</code></a>, and in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L316"><code>_load_chunk_residuals</code></a>.</li>
<li>colbert/tests/index_coalesce_test.py uses it to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L57">multi-file <code>codes.pt</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L66">single-file <code>codes.pt</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L83">multi-file <code>residuals.pt</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/tests/index_coalesce_test.py#L92">single-file <code>residuals.pt</code></a>.</li>
</ul>
</section>
<section id="pr-92315" class="level3">
<h3 class="anchored" data-anchor-id="pr-92315">PR <a href="https://github.com/pytorch/pytorch/pull/92315">#92315</a></h3>
<blockquote class="blockquote">
<p>Fix NVML visible device parsing (#92315)</p>
</blockquote>
<blockquote class="blockquote">
<p>CUDA_VISIBLE_DEVICES can contain either ordinals or UUIDs Extend the logic to be able to parse it by UUID</p>
</blockquote>
<p><mark>I don’t think this would affect any artifacts created during indexing/searching/training but would make it easier for PyTorch to identify GPUs.</mark></p>
</section>
<section id="pr-93095" class="level3">
<h3 class="anchored" data-anchor-id="pr-93095">PR <a href="https://github.com/pytorch/pytorch/pull/93095">#93095</a></h3>
<p>This PR fixes an error in <a href="https://github.com/pytorch/pytorch/issues/93006">#93006</a> when using <code>topk</code>, which is used in the following places in ColBERT:</p>
<ul>
<li><code>get_cells</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L17">colbert/search/candidate_generation.py</a></li>
<li><code>score_pids</code> in colbert/search/index_storage.py to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L127">filter centroids by the threshold</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L152">filter <code>pids</code> using pruned centroid scores</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L161">filter <code>pids</code> using full centroid scores</a></li>
<li>filter scores in <code>colbert_score_reduce</code> for the <code>"flipr"</code> interaction method: <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L146">link1</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L150">link 2</a></li>
</ul>
<p>Claude recommended also consider uses of <code>max</code> and <code>argmax</code> to be potentially impacted:</p>
<ul>
<li><code>get_cells</code> if <code>ncells==1</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L15">colbert/search/candidate_generation.py</a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L121"><code>ColBERT.score</code></a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L135"><code>colbert_score_reduce</code></a></li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCodec.compress_into_codes</code> on <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">GPU</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">CPU</a></li>
<li>colbert/search/strided_tensor.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L74">`StridedTensor.lookup</a></li>
<li>colbert/search/strided_tensor_core.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L27"><code>StridedTensorCore.__init__</code></a></li>
<li>colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L215">`Checkpoint.score</a></li>
<li>colbert/indexing/utils.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L48"><code>optimize_ivf</code></a></li>
</ul>
<p><mark>I would assume that the only impact this PR would have on PyTorch is avoiding any errors during the use of <code>topk</code> (no such errors have been reported on in the open issues).</mark></p>
</section>
<section id="pr-85596" class="level3">
<h3 class="anchored" data-anchor-id="pr-85596">PR <a href="https://github.com/pytorch/pytorch/pull/85596">#85596</a></h3>
<blockquote class="blockquote">
<p>Fix: half reduction with multiple sub-iterators (#85596)</p>
</blockquote>
<p>Fixes <a href="https://github.com/pytorch/pytorch/issues/74438">cuda low-precision reductions on large tensors produce wrong results #74438</a>:</p>
<blockquote class="blockquote">
<p>Reductions with low precision inputs (half, bfloat16) that need sub-iterators accumulate directly in output and thus truncate intermediate results</p>
</blockquote>
<p>This would fix any issues related to the use of <code>half</code> in the following ColBERT files:</p>
<ul>
<li>in <code>ResidualCodec</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L27"><code>centroids</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L35"><code>avg_residual</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L40"><code>bucket_weights</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L159">saving <code>centroids</code></a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L172">compressing token embeddings</a>, calculating cosine similarity between token embeddings and centroids <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">on GPU</a>.</li>
<li>colbert/search/candidate_generation.py in <code>CandidateGeneration.generate_candidates</code> for queries when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L52">using the GPU</a>.</li>
<li>colbert/indexing/collection_indexer.py in <code>CollectionIndexer._sample_embeddings</code> when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L181">saving <code>local_sample_embs</code></a>, in <code>CollectionIndexer.train_kmeans</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L308">centroids on the GPU</a>, and in <code>CollectionIndexer.index</code> when <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_indexer.py#L370">saving token embeddings</a>.</li>
<li>colbert/modeling/colbert.py in <code>ColBERT.doc</code> for <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L106">document token embeddings on the GPU</a></li>
</ul>
<p><mark>If this PR fix is relevant to the use of <code>half</code> in the above files I would expect there to be numeric differences in indexing/search artifacts</mark>.</p>
</section>
<section id="pr-86492" class="level3">
<h3 class="anchored" data-anchor-id="pr-86492">PR <a href="https://github.com/pytorch/pytorch/pull/86492">#86492</a></h3>
<blockquote class="blockquote">
<p>Fixes a memory leak by making autocast cache global instead of thread-local (#86492)</p>
</blockquote>
<p>This PR adds a PyTorch test which:</p>
<blockquote class="blockquote">
<p>Verifies that the autocast cache is global. This is done by mocking out cache clearing at the end of the forward pass, running forward+backward with an explicit call to autocast in the backward, and verifying that the weight only get cast to float16 once.</p>
</blockquote>
<p>Claude’s analysis:</p>
<blockquote class="blockquote">
<p>This PyTorch enhancement directly benefits ColBERT. By making the autocast cache global, this PR provides a performance improvement when training ColBERT with mixed precision. It reduces redundant computations during the backward pass, leading to faster and more efficient training without changing the model’s functionality.</p>
</blockquote>
<p>ColBERT uses <code>torch.cuda.amp.autocast</code> in the following files:</p>
<ul>
<li>colbert/utils/amp.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L15"><code>MixedPrecisionManager.context</code></a> which is used in colbert/training/training.py during <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L96"><code>train</code></a>, and in colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L87"><code>Checkpoint.query</code></a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L93"><code>Checkpoint.doc</code></a> to calculate query and document token embeddings, respectively.</li>
<li>colbert/distillation/scorer in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/distillation/scorer.py#L48"><code>Scorer._score_pairs</code></a>.</li>
</ul>
</section>
<section id="pr-88898" class="level3">
<h3 class="anchored" data-anchor-id="pr-88898">PR <a href="https://github.com/pytorch/pytorch/pull/88898">#88898</a></h3>
<p>Fixes PyTorch <a href="https://github.com/pytorch/pytorch/issues/88873">#88873</a>:</p>
<blockquote class="blockquote">
<p>torch_extension.py should be fixed or ninja compile will fail.</p>
</blockquote>
<p>Gemini’s analysis: Because ColBERT uses the very feature this PR is fixing (torch.utils.cpp_extension.py), the change is directly relevant. This bug fix is important for any developer or user who needs to compile and run ColBERT on a Windows machine. It ensures that ColBERT’s performance-critical custom CUDA code can be built correctly, preventing potential compilation errors.</p>
<p>ColBERT uses <code>torch.utils.cpp_extension</code> in the following files:</p>
<ul>
<li>colbert/modeling/colbert.py in <code>ColBERT.try_load_torch_extensions</code> to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L12">load <code>segmented_lookup.cpp</code></a> on CPU.</li>
<li>colbert/search/index_storage.py in <code>IndexScorer.try_load_torch_extensions</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L38">filter_pids.cpp</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L51">decompress_residuals.cpp</a>.</li>
<li>colbert/search/strided_tensor.py in <code>StridedTensor.try_load_torch_extensions</code> to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L26">load <code>segmented_lookup.cpp</code></a> on CPU.</li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCode.try_load_torch_extensions</code> to load <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L103">decompress_residuals.cpp</a></li>
</ul>
<p><mark>This might be related to ColBERT <a href="https://github.com/stanford-futuredata/ColBERT/issues/371">#317</a></mark></p>
</section>
<section id="pr-90149" class="level3">
<h3 class="anchored" data-anchor-id="pr-90149">PR <a href="https://github.com/pytorch/pytorch/pull/90149">#90149</a></h3>
<blockquote class="blockquote">
<p>Fix a static initialization order fiasco in c10d (#90149)</p>
</blockquote>
<p>Gemini’s analysis: Because ColBERT’s multi-GPU functionality is built directly on the PyTorch library that this PR is fixing, this change is highly relevant. This is a crucial stability improvement that makes ColBERT’s distributed training and inference more reliable by preventing potential crashes at startup.</p>
<p><mark>If Gemini’s analysis is correct, this will make ColBERT’s multi-GPU functionality more reliable and might address related open issues.</mark></p>
</section>
<section id="prs-86956-86958" class="level3">
<h3 class="anchored" data-anchor-id="prs-86956-86958">PRs <a href="https://github.com/pytorch/pytorch/pull/86956">#86956</a>, <a href="https://github.com/pytorch/pytorch/pull/86958">#86958</a></h3>
<blockquote class="blockquote">
<p>Fix issues with non-contiguous Tensor handling (#86956, #86958)</p>
</blockquote>
<p><mark>These are both MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS</mark></p>
</section>
<section id="prs-94119-86240-91520-94442-94386" class="level3">
<h3 class="anchored" data-anchor-id="prs-94119-86240-91520-94442-94386">PRs <a href="https://github.com/pytorch/pytorch/pull/94119">#94119</a>, <a href="https://github.com/pytorch/pytorch/pull/86240">#86240</a>, <a href="https://github.com/pytorch/pytorch/pull/91520">#91520</a>, <a href="https://github.com/pytorch/pytorch/pull/94442">#94442</a>, <a href="https://github.com/pytorch/pytorch/pull/94386">#94386</a></h3>
<blockquote class="blockquote">
<p>Fix issues with ops implementation torch.median (#90326, #88807), torch.{std,var} correction argument (#91203), torch.index_select (#94117, #91064), torch.cumsum (#94119), torch.where (#86240), torch.nn.Embedding (#82809), torch.nn.Softplus (#88555), torch.nn.functional.pad (#89864), torch.max (#91520), padding functions (#91522), torch.nn.functional.upsample (#91669), pooling functions (#91519, #94348), torch.nn.{NLLLoss,SmoothL1Loss} (#94226), torch.nn.SoftPlus (#94256), torch.masked_fill (#94263), torch.fill_ (#94479), torch.median (#94489), torch.nonzero (#94442), torch.nn.BatchNorm (#94351), torch.{min,max} (#94386), torch.nn.GELU (#94529), torch.nn.LSTM (#94889), #95137),torch.nn.Conv2d(#95078),torch.nn.functional.bilinear(#94892),torch.copy_ (#95272),torch.max_pool2d(#94963),torch.div (#95769)</p>
</blockquote>
<p>ColBERT uses topk in the following files:</p>
<ul>
<li><code>get_cells</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L17">colbert/search/candidate_generation.py</a></li>
<li><code>score_pids</code> in colbert/search/index_storage.py to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L127">filter centroids by the threshold</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L152">filter <code>pids</code> using pruned centroid scores</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L161">filter <code>pids</code> using full centroid scores</a></li>
<li>filter scores in <code>colbert_score_reduce</code> for the <code>"flipr"</code> interaction method: <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L146">link1</a>, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L150">link 2</a></li>
</ul>
<p>ColBERT uses max/argmax in the following files:</p>
<ul>
<li><code>get_cells</code> if <code>ncells==1</code> in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L15">colbert/search/candidate_generation.py</a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L121"><code>ColBERT.score</code></a></li>
<li>colbert/modeling/colbert.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L135"><code>colbert_score_reduce</code></a></li>
<li>colbert/indexing/codecs/residual.py in <code>ResidualCodec.compress_into_codes</code> on <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">GPU</a> and <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">CPU</a></li>
<li>colbert/search/strided_tensor.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L74">`StridedTensor.lookup</a></li>
<li>colbert/search/strided_tensor_core.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L27"><code>StridedTensorCore.__init__</code></a></li>
<li>colbert/modeling/checkpoint.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L215">`Checkpoint.score</a></li>
<li>colbert/indexing/utils.py in <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L48"><code>optimize_ivf</code></a></li>
</ul>
<p>ColBERT uses <code>torch.cumsum</code> in the following files to calculate <code>offsets</code>.:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L50">colbert/indexing/utils.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L31">colbert/search/strided_tensor_core.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L48">colbert/search/strided_tensor.py</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L68">colbert/search/index_storage.py</a></li>
</ul>
<p>ColBERT uses <code>torch.where</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L49">colbert/modeling/checkpoint.py</a> to pool embeddings within each cluster.</li>
</ul>
<p>ColBERT uses <code>torch.nonzero</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L351">colbert/index_updater.py</a> to construct mask of where pids to be removed appear in ivf.</li>
</ul>
<p><mark>These are all MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91120-94464" class="level3">
<h3 class="anchored" data-anchor-id="prs-91120-94464">PRs <a href="https://github.com/pytorch/pytorch/pull/91120">#91120</a>, <a href="https://github.com/pytorch/pytorch/pull/94464">#94464</a></h3>
<blockquote class="blockquote">
<p>Fix issues with torch.bool for Unary ops (#91120), scatter ops (#94464)</p>
</blockquote>
<p>Claude’s analysis: Claude: The PR fixes compatibility issues where boolean tensors needed to be cast to int8 on older macOS versions, then cast back. This would be important for ColBERT’s masking operations which rely heavily on boolean tensors for attention and padding masks.</p>
<p><mark>These are MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-94484" class="level3">
<h3 class="anchored" data-anchor-id="pr-94484">PR <a href="https://github.com/pytorch/pytorch/pull/94484">#94484</a></h3>
<blockquote class="blockquote">
<p>Properly cast torch.int64 to torch.int32 for reduction ops and raise warning. (#94484)</p>
</blockquote>
<p>Claude’s analysiss: The PR changes TORCH_CHECK (which throws an error) to TORCH_WARN_ONCE (which just warns) and automatically casts int64 to int32 for min/max operations. This would allow ColBERT to run on MPS with int64 tensors instead of failing, though with potential precision loss.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91120-94464-1" class="level3">
<h3 class="anchored" data-anchor-id="prs-91120-94464-1">PRs <a href="https://github.com/pytorch/pytorch/pull/91197">#91120</a>, <a href="https://github.com/pytorch/pytorch/pull/91514">#94464</a></h3>
<blockquote class="blockquote">
<p>Fix handling of ops taking multiple dtypes as input (#91197, #91514)</p>
</blockquote>
<p>Claude’s analysis: The PR fixes MPS scatter to handle type mismatches between source and destination tensors automatically.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-91786-94662" class="level3">
<h3 class="anchored" data-anchor-id="prs-91786-94662">PRs <a href="https://github.com/pytorch/pytorch/pull/91786">#91786</a>, <a href="https://github.com/pytorch/pytorch/pull/94662">#94662</a></h3>
<blockquote class="blockquote">
<p>Fix handling of channels last for torch.cat (#91786, #94662), torch.Conv2d (#91822, #94384), torch.nn.{ELU,ReLU,Hardswish} (#94664), torch.nn.BatchNorm (#94760), torch.nn.MaxPool2d (#94877)</p>
</blockquote>
<p>ColBERT uses <code>.cat</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/utils.py#L45">colbert/indexing/utils.py</a> to concatenate a list of tensors (<code>unique_pids_per_centroid</code>) into a single tensor (<code>ivf</code>).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/index_manager.py#L23">colbert/indexing/index_manager.py</a> to concatenate multiple path names.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L31"></a> to calculate <code>offsets</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor_core.py#L39"></a> to add padding to a tensor.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/distillation/scorer.py#L60"></a> to concatenate <code>scores</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_encoder.py#L38"></a> to concatenate document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L181"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L182"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L220"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L237"></a> to concatenate <code>centroids</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L276"></a> to concatenate document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L51"></a> to concatenate <code>packed_tensor</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L152"></a> to concatenate <code>all_orders</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/strided_tensor.py#L155"></a> to concatenate <code>all_lengths</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L101"></a> to concatenate <code>compressed_embs.codes</code> (centroid IDs corresponding to document token embeddings).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L108"></a> to concatenate <code>compressed_embs.residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L117"></a> to concatenate <code>doclens</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L431"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L434"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L506"></a> to concatenate <code>codes</code> (centroid IDs).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/index_updater.py#L507"></a> to concatenate <code>residuals</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L115"></a> to concatenate <code>batches</code> (of queries).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L168"></a> to concatenate document token embeddings (in order).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L169"></a> to concatenate <code>mask</code> for document token embeddings (in order).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/coalesce.py#L48"></a> to concatenate <code>code</code> chunks.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L69"></a> to concatenate <code>offsets</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L149"></a> to concatenate <code>approx_scores</code>.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/query_tokenization.py#L88"></a> to concatenate <code>ids</code> (for query tokens).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/query_tokenization.py#L89"></a> to concatenate <code>masks</code> (for query tokens).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/utils.py#L72"></a> to concatenate prefix token.</li>
</ul>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="prs-94259-94278-95145-95762-95905" class="level3">
<h3 class="anchored" data-anchor-id="prs-94259-94278-95145-95762-95905">PRs <a href="https://github.com/pytorch/pytorch/pull/94259">#94259</a>, <a href="https://github.com/pytorch/pytorch/pull/94278">#94278</a>, <a href="https://github.com/pytorch/pytorch/pull/95145">#95145</a>, <a href="https://github.com/pytorch/pytorch/pull/95762">#95762</a>, <a href="https://github.com/pytorch/pytorch/pull/95905">#95905</a></h3>
<blockquote class="blockquote">
<p>Fix view operations handling (#94259, #94278,#95145, #95762, #95905)</p>
</blockquote>
<p>Claude’s analysis: This PR fixes crashes in view operations when slicing with incorrect lengths, which ColBERT uses for tensor reshaping and indexing operations.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-87853" class="level3">
<h3 class="anchored" data-anchor-id="pr-87853">PR <a href="87853">#87853</a></h3>
<blockquote class="blockquote">
<p>Move incorrectly placed closing curly brace of extern “C” block (#87853)</p>
</blockquote>
<p>Gemini’s analysis: This pull request is a foundational C++ correctness fix for the PyTorch framework. Because ColBERT compiles its own C++ extensions that depend on these core headers, this change is directly beneficial. It ensures the stability and reliability of ColBERT’s own build process, preventing potential compilation failures.</p>
<p>ColBERT’s C++ extensions:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/modeling/segmented_maxsim.cpp">segmented_maxsim.cpp</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/search/filter_pids.cpp">filter_pids.cpp</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/search/decompress_residuals.cpp">decompress_residuals.cpp</a></li>
</ul>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-93322" class="level3">
<h3 class="anchored" data-anchor-id="pr-93322">PR <a href="https://github.com/pytorch/pytorch/pull/93322">#93322</a></h3>
<blockquote class="blockquote">
<p>Fix MSVC compiler error in basic_ops.h (#93322)</p>
</blockquote>
<p>Gemini’s take: This pull request is a crucial build-system and correctness fix. It directly impacts ColBERT by ensuring that its custom C++ code can be compiled successfully on Windows machines that use the affected MSVC compiler. Without this fix, users in that environment would be unable to run ColBERT. This change makes ColBERT’s build process more robust and widens its platform compatibility.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
<section id="pr-89310" class="level3">
<h3 class="anchored" data-anchor-id="pr-89310">PR <a href="https://github.com/pytorch/pytorch/pull/89310">#89310</a></h3>
<blockquote class="blockquote">
<p>Fix a bug that redefines __STDC_FORMAT_MACROS (#89310)</p>
</blockquote>
<p>Gemini’s take: This pull request provides a stability and correctness fix to the underlying PyTorch framework. Because ColBERT compiles its own C++ code that depends on these core PyTorch headers, this change is directly beneficial. It makes ColBERT’s own compilation process more reliable and prevents a potential class of build failures.</p>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-90411" class="level3">
<h3 class="anchored" data-anchor-id="pr-90411">PR <a href="https://github.com/pytorch/pytorch/pull/90411">#90411</a></h3>
<blockquote class="blockquote">
<p>Add manual cuda deps search logic (#90411)</p>
</blockquote>
<p>Gemini’s take: This PyTorch pull request adds a new mechanism to help PyTorch find its essential CUDA libraries (cuBLAS and cuDNN) on Linux systems.</p>
<p><mark>Unsure how to measure the impact but if it’s about reliability perhaps it will address some open issues. TBD.</mark></p>
</section>
<section id="pr-89759" class="level3">
<h3 class="anchored" data-anchor-id="pr-89759">PR <a href="https://github.com/pytorch/pytorch/pull/89759">#89759</a></h3>
<blockquote class="blockquote">
<p>Workaround for NumPy builds that ship with a broken Dlpack deleter (#89759)</p>
</blockquote>
<p><mark>TBD if this improves reliability as ColBERT uses NumPy</mark>/.</p>
</section>
<section id="pr-86288" class="level3">
<h3 class="anchored" data-anchor-id="pr-86288">PR <a href="https://github.com/pytorch/pytorch/pull/86288">#86288</a></h3>
<blockquote class="blockquote">
<p>Workaround MSVC ICE due to constexpr char* template argument (#86288)</p>
</blockquote>
<p>Gemini’s take: It directly impacts ColBERT by ensuring that its custom C++ code can be compiled successfully on Windows machines that use an affected MSVC compiler.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
<section id="pr-85408" class="level3">
<h3 class="anchored" data-anchor-id="pr-85408">PR <a href="https://github.com/pytorch/pytorch/pull/85408">#85408</a></h3>
<blockquote class="blockquote">
<p>Add define to fix issue with compatibility with latest Windows SDK (#85408)</p>
</blockquote>
<p>Gemini’s take: It directly impacts ColBERT by ensuring that the underlying PyTorch framework can be successfully built on modern Windows environments.</p>
<p><mark>TBD if this addressed open issues related to Windows machines.</mark></p>
</section>
</section>
<section id="improvements" class="level2">
<h2 class="anchored" data-anchor-id="improvements">Improvements</h2>
<p>These PyTorch PRs are related to improvements, which could affect ColBERT by speeding things up (and therefore seeing a speed up in indexing/search/training time) or changing baseline indexing/search/training artifacts if improvements impact numeric precision.</p>
<section id="pr-56398" class="level3">
<h3 class="anchored" data-anchor-id="pr-56398">PR <a href="https://github.com/pytorch/pytorch/pull/56398">#56398</a></h3>
<blockquote class="blockquote">
<p>Set std/var correction overloads default value to None (#56398)</p>
</blockquote>
<p><mark>Unclear if and how this affects ColBERT but highlighting it since it changes code in PyTorch’s aten/src/ATen/native.</mark></p>
</section>
<section id="pr-86309" class="level3">
<h3 class="anchored" data-anchor-id="pr-86309">PR <a href="https://github.com/pytorch/pytorch/pull/86309">#86309</a></h3>
<blockquote class="blockquote">
<p>Add support for int32 indices in index/index_put ops (#86309)</p>
</blockquote>
<p><mark>I think this PR is related to <a href="https://github.com/stanford-futuredata/ColBERT/pull/180">this ColBERT PR</a> which I think is related to <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L163">this line of code in <code>IndexScorer</code></a></mark>.</p>
</section>
<section id="pr-87022" class="level3">
<h3 class="anchored" data-anchor-id="pr-87022">PR <a href="https://github.com/pytorch/pytorch/pull/87022">#87022</a></h3>
<blockquote class="blockquote">
<p>Enable where to have cpu scalar args (#87022)</p>
</blockquote>
<p>ColBERT uses <code>torch.where</code> in the following files:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L49">colbert/modeling/checkpoint.py</a> to pool embeddings within each cluster.</li>
</ul>
<p><mark>Unclear if this will affect ColBERT but there are currently no open issues related to <code>torch.where</code></mark>.</p>
</section>
<section id="pr-90914" class="level3">
<h3 class="anchored" data-anchor-id="pr-90914">PR <a href="https://github.com/pytorch/pytorch/pull/90914">#90914</a></h3>
<blockquote class="blockquote">
<p>Add support for NumPy scalars to torch.tensor.asarray (#90914)</p>
</blockquote>
<p><mark>Found <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L198">1 use of <code>asarray</code></a> but it doesn’t deal with a scalar so probably won’t be affected</mark>.</p>
</section>
<section id="pr-85926" class="level3">
<h3 class="anchored" data-anchor-id="pr-85926">PR <a href="https://github.com/pytorch/pytorch/pull/85926">#85926</a></h3>
<blockquote class="blockquote">
<p>Enable out variant of torch.max(#85926)</p>
</blockquote>
<p><mark>Unclear what this PR does but highlighting it since ColBERT uses <code>torch.max</code></mark>.</p>
</section>
<section id="pr-91846" class="level3">
<h3 class="anchored" data-anchor-id="pr-91846">PR <a href="https://github.com/pytorch/pytorch/pull/91846">#91846</a></h3>
<blockquote class="blockquote">
<p>Implement faster gradient clipping using foreach function (#91846)</p>
</blockquote>
<p>ColBERT uses <code>torch.nn.utils.clip_grad_norm_</code> in two lines:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">colbert/utils/amp.py#L26</a></li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L31">colbert/utils/amp.py#L31</a></li>
</ul>
<p><mark>IIUC this won’t affect ColBERT since it doesn’t set <code>foreach</code> in <code>torch.nn.utils.clip_grad_norm_</code>.</mark></p>
</section>
<section id="pr-92334" class="level3">
<h3 class="anchored" data-anchor-id="pr-92334">PR <a href="https://github.com/pytorch/pytorch/pull/92334">#92334</a></h3>
<blockquote class="blockquote">
<p>Enable DDP to handle custom dataclass forward outputs (#92334)</p>
</blockquote>
<p><mark>ColBERT does use DistributedDataParallel (<a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L56">in <code>train</code></a>) butit’s not being passed a custom dataclass, it’s being passed a <code>colbert</code> model so I don’t think this PR applies.</mark></p>
</section>
<section id="pr-89137" class="level3">
<h3 class="anchored" data-anchor-id="pr-89137">PR <a href="https://github.com/pytorch/pytorch/pull/89137">#89137</a></h3>
<blockquote class="blockquote">
<p>Skip collective communications for NO_SHARD in clip_grad_norm_ (#89137)</p>
</blockquote>
<p><mark>ColBERT doesn’t use FullyShardedDataParallel but it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">does use <code>torch.nn.utils.clip_grad_norm</code></a> so not sure if this PyTorch PR affects it</mark>.</p>
</section>
<section id="pr-90028" class="level3">
<h3 class="anchored" data-anchor-id="pr-90028">PR <a href="https://github.com/pytorch/pytorch/pull/90028">#90028</a></h3>
<blockquote class="blockquote">
<p>Apply the “largest” dtype across all parameters/gradients as defined by PyTorch’s type promotion semantics for the total norm returned in clip_grad_norm_ for low prec grads (#90028)</p>
</blockquote>
<p><mark>ColBERT doesn’t use FullyShardedDataParallel but it <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/utils/amp.py#L26">does use <code>torch.nn.utils.clip_grad_norm</code></a> so not sure if this PyTorch PR affects it</mark>.</p>
</section>
<section id="pr-85692" class="level3">
<h3 class="anchored" data-anchor-id="pr-85692">PR <a href="https://github.com/pytorch/pytorch/pull/85692">#85692</a></h3>
<blockquote class="blockquote">
<p>Set CUDA_MODULE_LOADING to LAZY when not set by the user (#85692)</p>
</blockquote>
<p><mark>Unclear exactly what this does but it relates to the CUDA_MODULE_LOADING env var which is not set in ColBERT</mark></p>
</section>
<section id="pr-89172" class="level3">
<h3 class="anchored" data-anchor-id="pr-89172">PR <a href="https://github.com/pytorch/pytorch/pull/89172">#89172</a></h3>
<blockquote class="blockquote">
<p>Add an option to disable reduced precision reductions for BF16 GEMM (#89172)</p>
</blockquote>
<p><mark>Unclear exactly what this does, but in the PR they mentioned it improves H100 usage, so I’ll keep that in mind.</mark></p>
</section>
<section id="pr-91436" class="level3">
<h3 class="anchored" data-anchor-id="pr-91436">PR <a href="https://github.com/pytorch/pytorch/pull/91436">#91436</a></h3>
<blockquote class="blockquote">
<p>Add an env variable to disable addmm_cuda_lt kernel (#91436)</p>
</blockquote>
<p><mark>Unclear what this does, but it’s adding a variable, so it’s a new feature.</mark></p>
</section>
<section id="pr-86041-93022" class="level3">
<h3 class="anchored" data-anchor-id="pr-86041-93022">PR <a href="https://github.com/pytorch/pytorch/pull/86041">#86041</a>, <a href="https://github.com/pytorch/pytorch/pull/93022">#93022</a></h3>
<blockquote class="blockquote">
<p>Clean up flatbuffer lib dependency and fixed its test to match pkl models (#86041, #93022)</p>
</blockquote>
<p><mark>I am not sure what these PRs are doing. The title refers “pkl models” which ColBERT doesn’t use to my knowledge.</mark></p>
</section>
<section id="pr-93898" class="level3">
<h3 class="anchored" data-anchor-id="pr-93898">PR <a href="https://github.com/pytorch/pytorch/pull/93898">#93898</a></h3>
<blockquote class="blockquote">
<p>Type corrections to avoid unnecessary static_casts (#93898)</p>
</blockquote>
<p><mark>Unclear what this PR does but it touches a lot of what seem to be core files so I’m flagging it</mark>.</p>
</section>
<section id="pr-87245" class="level3">
<h3 class="anchored" data-anchor-id="pr-87245">PR <a href="https://github.com/pytorch/pytorch/pull/87245">#87245</a></h3>
<blockquote class="blockquote">
<p>Integrate all ONNX operators with a new JitScalarType API (#87245)</p>
</blockquote>
<p><mark>It’s onnx related, which ColBERT doesn’t use, but it also says: “this PR addresses not only the issue above, but the entire family of issues related to torch._C.Value.type() parsing when scalarType() or dtype() is not available.”</mark></p>
</section>
<section id="pr-87343" class="level3">
<h3 class="anchored" data-anchor-id="pr-87343">PR <a href="https://github.com/pytorch/pytorch/pull/87343">#87343</a></h3>
<blockquote class="blockquote">
<p>Add share_from_this to torch::jit::Graph (#87343)</p>
</blockquote>
<p><mark>Is ONNX related, but unclear if it affects anything else?</mark></p>
</section>
<section id="pr-84789" class="level3">
<h3 class="anchored" data-anchor-id="pr-84789">PR <a href="https://github.com/pytorch/pytorch/pull/84789">#84789</a></h3>
<blockquote class="blockquote">
<p>Use optional op to keep None in results for ONNX internal tests (#84789)</p>
</blockquote>
<p><mark>Is ONNX related, but unclear if it affects anything else?</mark></p>
</section>
<section id="pr-86218" class="level3">
<h3 class="anchored" data-anchor-id="pr-86218">PR <a href="https://github.com/pytorch/pytorch/pull/86218">#86218</a></h3>
<blockquote class="blockquote">
<p>Add fp16 support for torch.nn.Linear (#89774), torch.nn.GELU (#86218)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91884" class="level3">
<h3 class="anchored" data-anchor-id="pr-91884">PR <a href="https://github.com/pytorch/pytorch/pull/91884">#91884</a></h3>
<blockquote class="blockquote">
<p>Add support for empty Tensors in torch.bitwise_not (#87286), torch.nn.LayerNorm (#94212), many backward functions (#94343), torch.nn.functional.hardswish (#94342), torch.topk (#91884), torch.arange (#94485), torch.linal.inv (#94551),</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91734" class="level3">
<h3 class="anchored" data-anchor-id="pr-91734">PR <a href="https://github.com/pytorch/pytorch/pull/91734">#91734</a></h3>
<blockquote class="blockquote">
<p>Add support for reduction ops on multiple axis at a time (#91734)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-94639" class="level3">
<h3 class="anchored" data-anchor-id="pr-94639">PR <a href="https://github.com/pytorch/pytorch/pull/94639">#94639</a></h3>
<blockquote class="blockquote">
<p>Add support for k greater than 16 for torch.topk (#94639)</p>
</blockquote>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
<section id="pr-91576" class="level3">
<h3 class="anchored" data-anchor-id="pr-91576">PR <a href="https://github.com/pytorch/pytorch/pull/91576">#91576</a></h3>
<blockquote class="blockquote">
<p>Simplify OpenMP detection in CMake (#91576)</p>
</blockquote>
<p>Claude’s take: While ColBERT should continue working, there could be subtle performance or compilation differences depending on how PyTorch’s simplified OpenMP detection affects the runtime compilation of ColBERT’s C++ extensions, particularly in multi-threaded scenarios.</p>
<p><mark>Unclear what this PR is doing but flagging it as it might improve performance as Claude states.</mark></p>
</section>
</section>
<section id="deprecations" class="level2">
<h2 class="anchored" data-anchor-id="deprecations">Deprecations</h2>
<p>These are PRs I would think would have a significant impact if applicable.</p>
<section id="pr-92143" class="level3">
<h3 class="anchored" data-anchor-id="pr-92143">PR <a href="https://github.com/pytorch/pytorch/pull/92143">#92143</a></h3>
<blockquote class="blockquote">
<p>Deprecate tensor.mT,tensor.T,tensor.mH,tensor.H on 0D-tensors (#92143)</p>
</blockquote>
<p><mark>There are five instances where .T is used, but pretty sure none of these are 0-D tensors, will confirm</mark>:</p>
<ul>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L13">colbert/search/candidate_generation.py#L13</a>: cosine similarity between centroids and query token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/candidate_generation.py#L43">colbert/search/candidate_generation.py#L43</a>: used in <code>generate_candidate_scores</code> which uses <code>lookup_eids</code> which I can’t find anywhere else in the codebase.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/colbert.py#L195">colbert/modeling/colbert.py#L195</a>: Cosine similarity between query and document token embeddings.</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L215">colbert/indexing/codecs/residual.py#L215</a>: Cosine similarity between centroids and document token embeddings (GPU).</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/codecs/residual.py#L217">colbert/indexing/codecs/residual.py#L217</a>: Cosine similarity between centroids and document token embeddings (CPU).</li>
</ul>
</section>
</section>
<section id="performance" class="level2">
<h2 class="anchored" data-anchor-id="performance">Performance</h2>
<p>Similar to improvements, I would only expect this set of PRs to improve ColBERT performance, keeping an eye on how different artifacts changed because of that.</p>
<section id="pr-93234" class="level3">
<h3 class="anchored" data-anchor-id="pr-93234">PR <a href="https://github.com/pytorch/pytorch/pull/93234">#93234</a></h3>
<blockquote class="blockquote">
<p>Improve performance for functional.multi_head_attention_forward() (#93234, #89847)</p>
</blockquote>
<p><mark>ColBERT uses BERT, which has its own attention implementation, so this likely wouldn’t impact it unless the BERT model specifically uses <code>torch.nn.functional.multi_head_attention_forward</code> or <code>torch.nn.MultiheadAttention</code>.</mark></p>
</section>
<section id="pr-84981" class="level3">
<h3 class="anchored" data-anchor-id="pr-84981">PR <a href="https://github.com/pytorch/pytorch/pull/84981">#84981</a></h3>
<blockquote class="blockquote">
<p>Use atomicAdd for bfloat16 in Ampere and above (#84981)</p>
</blockquote>
<p>Gemini’s take: This pull request directly accelerates a fundamental operation used during the training of ColBERT. By replacing a slow, emulated function with a fast, hardware-native instruction, this change leads to a noticeable increase in training speed for anyone training ColBERT with bfloat16 mixed precision on an Ampere or newer GPU.</p>
<p><mark>If Gemini is correct, then I will see a speedup in training.</mark></p>
</section>
<section id="pr-94034" class="level3">
<h3 class="anchored" data-anchor-id="pr-94034">PR <a href="https://github.com/pytorch/pytorch/pull/94034">#94034</a></h3>
<blockquote class="blockquote">
<p>Add various performance fixes to c++ STL usage (#94034)</p>
</blockquote>
<p>Gemini’s take: The changes in this PR touch several core PyTorch components that are critical to ColBERT’s operation:</p>
<ul>
<li>Autograd Engine (function.h): Every gradient calculation during training will benefit from these optimizations.</li>
<li>CUDA Communication (comm.cpp): The code that handles broadcasting and gathering tensors across GPUs for multi-GPU training and inference is made more efficient.</li>
<li>Mixed Precision (autocast_mode.h): The logic for automatic mixed precision, which is key for training ColBERT efficiently, is also slightly optimized.</li>
</ul>
<p><mark>If Gemini is correct, then I will see a speed up in all aspects of ColBERT. </mark></p>
</section>
<section id="pr-86568" class="level3">
<h3 class="anchored" data-anchor-id="pr-86568">PR <a href="https://github.com/pytorch/pytorch/pull/86568">#86568</a></h3>
<blockquote class="blockquote">
<p>Add fmsub to vectorization primitives (#86568)</p>
</blockquote>
<p>Gemini’s take: This pull request is a CPU-specific performance optimization. It adds support for the fmsub (fused multiply-subtract) instruction to PyTorch’s CPU vectorization library. This allows PyTorch to perform the operation (a * b) - c in a single, faster instruction on modern CPUs that support it (e.g., via AVX or NEON).</p>
<p><mark>I’m pretty sure ColBERT doesn’t use multiply-subtract, but keeping it in here just in case it comes up. </mark></p>
</section>
<section id="pr-92300" class="level3">
<h3 class="anchored" data-anchor-id="pr-92300">PR <a href="https://github.com/pytorch/pytorch/pull/92300">#92300</a></h3>
<blockquote class="blockquote">
<p>Fix biasadd OMP perf issue for the packed MKL SGEMM (#92300)</p>
</blockquote>
<p>Gemini’s take: This pull request is a CPU-specific performance optimization. It fixes a parallelization issue within the Intel MKL (Math Kernel Library) backend for linear layers. This change improves the efficiency of adding a bias term to the output of a matrix multiplication when running on a CPU.</p>
<p><mark>If Gemini is correct, I would expect a speedup on CPU. </mark></p>
</section>
<section id="pr-91114" class="level3">
<h3 class="anchored" data-anchor-id="pr-91114">PR <a href="https://github.com/pytorch/pytorch/pull/91114">#91114</a></h3>
<blockquote class="blockquote">
<p>Increase performance of torch.add{cmul,cdiv,mm}(#94214, #94534)torch.multinomial (#86342), faster op launch time (#86437), torch.linear (#91114), view handling (#91743, #94218), convolutions(#94661), scatter/gather (#94663)</p>
</blockquote>
<p>Gemini’s take: While the Adam optimizer used by ColBERT does use the addcdiv operation, this is executed on the GPU via CUDA, not MPS. This pull request is a performance optimization for the torch.nn.Linear layer, but it is exclusively for the MPS (Metal Performance Shaders) backend.</p>
<p><mark>MPS-related and will only affect ColBERT if we in the future choose to make it compatible with MPS.</mark></p>
</section>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>Based on my analysis, I’m optimistic about upgrading ColBERT from torch==1.13.1 to 2.0. The upgrade should deliver concrete benefits with reasonable testing overhead. Performance-wise, I’ll be watching for training time improvements from the faster <code>foreach</code> optimizer implementations and expect speedups across all aspects of ColBERT from C++ optimizations and CUDA improvements. For validation, I’ll need to check for numeric differences in indexing/search artifacts from half-precision bug fixes and benchmark retrieval quality metrics after reindexing to avoid regressions. The reliability improvements should make ColBERT’s multi-GPU functionality more reliable and might address related open issues. Plus there are fixes for operations like <code>topk</code> and <code>torch.load</code> that ColBERT uses extensively. Most MPS-related changes will only affect ColBERT if we choose future compatibility, so they’re not immediate concerns but good to have.</p>
<p>My next step will be to establish training time benchmarks and indexing/retrieval/training baseline artifacts so that I can concretely monitor even subtle performance/behavior changes when using <code>torch==2.0</code> in my development branch.</p>


</section>

 ]]></description>
  <category>ColBERT</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-27-torch-colbert/</guid>
  <pubDate>Sun, 27 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>TIL: Custom Composer Callback to Push Checkpoints to HuggingFace Hub During Training.</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-26-push-to-hub/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this short TIL blog post, I’m going to share the code I wrote with Claude’s help for a custom Composer callback which pushes the model to Hugging Face Hub every specified number of steps. The purpose of doing so is so that you can run evaluation after training so it doesn’t slow down training.</p>
</section>
<section id="custom-composer-callback" class="level2">
<h2 class="anchored" data-anchor-id="custom-composer-callback">Custom Composer Callback</h2>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> HFPushCallback(Callback):</span>
<span id="cb1-2">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, repo_id: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, push_every_n_steps: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb1-3">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.repo_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> repo_id</span>
<span id="cb1-4">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.push_every_n_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> push_every_n_steps</span>
<span id="cb1-5">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> os.getenv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HF_TOKEN"</span>)</span>
<span id="cb1-6">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hf_api <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> HfApi(token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.token)</span>
<span id="cb1-7">  </span>
<span id="cb1-8">      create_repo(</span>
<span id="cb1-9">          repo_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.repo_id,</span>
<span id="cb1-10">          token<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.token,</span>
<span id="cb1-11">          private<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-12">          exist_ok<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb1-13">      )</span>
<span id="cb1-14">  </span>
<span id="cb1-15">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> batch_end(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, state: State, logger: Logger) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:            </span>
<span id="cb1-16">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> state.timestamp.batch.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.push_every_n_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb1-17">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._push_model(state)</span>
<span id="cb1-18">  </span>
<span id="cb1-19">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _push_model(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, state: State):</span>
<span id="cb1-20">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> tempfile.TemporaryDirectory() <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> temp_dir:</span>
<span id="cb1-21">          state.model.model.save_pretrained(temp_dir)</span>
<span id="cb1-22">          </span>
<span id="cb1-23">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hf_api.upload_folder(</span>
<span id="cb1-24">              folder_path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>temp_dir,</span>
<span id="cb1-25">              repo_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.repo_id,</span>
<span id="cb1-26">              commit_message<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>state<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>timestamp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>batch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>value<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb1-27">          )</span></code></pre></div>
<p>Important to note, <code>state.model</code> is the <code>ComposerHFCausalLM</code> wrapper around the HuggingFace model, so you have to access <code>state.model.model</code> to use the attribute <code>save_pretrained</code>.</p>
</section>
<section id="running-inference" class="level2">
<h2 class="anchored" data-anchor-id="running-inference">Running Inference</h2>
<p>You can use the following code to run inference on the model, just as you would any set of PEFT adapters from Hugging Face Hub.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> peft <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PeftModel</span>
<span id="cb2-3"></span>
<span id="cb2-4"></span>
<span id="cb2-5">model_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HuggingFaceTB/SmolLM2-135M"</span></span>
<span id="cb2-6">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(model_id)</span>
<span id="cb2-7">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(model_id)</span>
<span id="cb2-8"></span>
<span id="cb2-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PeftModel.from_pretrained(</span>
<span id="cb2-10">    model,</span>
<span id="cb2-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;repo_id&gt;"</span>,</span>
<span id="cb2-12">    revision <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"&lt;revision&gt;"</span></span>
<span id="cb2-13">)</span>
<span id="cb2-14"></span>
<span id="cb2-15">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The best thing about artificial intelligence is "</span></span>
<span id="cb2-16">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(prompt, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb2-17">attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inputs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"attention_mask"</span>]</span>
<span id="cb2-18"></span>
<span id="cb2-19">outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.generate(</span>
<span id="cb2-20">  inputs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>],</span>
<span id="cb2-21">  attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask,</span>
<span id="cb2-22">  pad_token_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tokenizer.eos_token_id</span>
<span id="cb2-23">)</span>
<span id="cb2-24"></span>
<span id="cb2-25"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tokenizer.decode(outputs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], skip_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))      </span></code></pre></div>
<p>The <code>revision</code> parameter is the commit ID in your Hugging Face repo. In this way, if you, say, push your model every 100 steps, then you can use the <code>revision</code> argument for each of those checkpoints and run your evaluations. Then you can log those evaluations to your W&amp;B project so that your evaluation log is comparable with other training logs.</p>


</section>

 ]]></description>
  <category>LLM</category>
  <category>Custom Composer Callback</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-26-push-to-hub/</guid>
  <pubDate>Sat, 26 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Takeaways from Lesson 1 of the AI Evals Course</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-22-evals-course/</link>
  <description><![CDATA[ 




<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In this blog post, I’m going highlight ideas that stood out to me from the first lesson and first three chapters of the course reader from the AI evals course by Hamel Husain and Shreya Shankar.</p>
</section>
<section id="ideas-that-stood-out-from-the-first-three-chapters-of-the-course-reader" class="level2">
<h2 class="anchored" data-anchor-id="ideas-that-stood-out-from-the-first-three-chapters-of-the-course-reader">Ideas that Stood Out from the First Three Chapters of the Course Reader</h2>
<p><strong>Idea 1: LLMs used for intermediate tasks still need evals.</strong> I think this is an important reminder because I often think of LLM tasks as complex or mission-critical or user-facing. However, there are smaller subtasks that happen “behind the scenes”, assisting the rest of the pipeline. These tasks can be overlooked. For example you could have an LLM-assisted task of curating few-shot examples that are used for a larger prompt downstream. This task needs evals.</p>
<p><strong>Idea 2: looking at data, whether it’s curating few-shot examples manually or reviewing LLM traces to classify failures, deepens our understanding of what the user wants and how the LLM fails to deliver.</strong> What makes a good example? We have to think about objectives, content, format, tone, instruction, and output. What type of failure are we witnessing in a trace? We have to think about those dimensions, and identify specifically what the LLM failed to do and categorize it precisely.</p>
<p><strong>Idea 3: We allow ourselves to adjust annotations or revise failure mode definitions as needed as we organize our thoughts and observations on LLM traces.</strong> It’s normal for annotation schemas to evolve after reviewing more data. I fully agree and relate to this idea. <a href="https://youtu.be/FXOXoaGjntc">After evaluating 1350 LLM judge scores (for LM-generated tiny stories)</a> I improved my understanding of what makes a high quality story (in terms of creativity, grammar, context-tracking, plot, factual knowledge, and reasoning) and modified the criteria I used to evaluate those dimensions.</p>
<p><strong>Idea 4: Forcing binary decisions about whether a failure mode occurs or not produces more reproducible annotations than the Likert scale.</strong> As someone who has advised, designed and administered a dozen or more surveys, I plead the case for binary decisions and groan at the sight of a Likert scale. What’s the difference between “Good” and “Very Good”? What about “Mostly Satisfied” and “Somewhat Satisfied”? If you’re asking someone to evaluate even 10s of LLM traces using a Likert scale, the cognitive load quickly fatigues the evaluator. It’s much simpler to answer “Did the LLM fail to use an appropriate tone?” (Yes/No) than “How well did the LLM use an appropriate tone?” (Exceptionally Well, Very Well, Somewhat Well, Not Well at All).</p>
<p>Note: Allen Downey has an excellent blog post, <a href="https://www.allendowney.com/blog/2024/05/03/the-mean-of-a-likert-scale/"><em>The Mean of a Likert Scale</em></a> which tackles the challenges of summarizing Likert scale data.</p>
</section>
<section id="ideas-from-lesson-1" class="level2">
<h2 class="anchored" data-anchor-id="ideas-from-lesson-1">Ideas from Lesson 1</h2>
<p><strong>Idea A: LLM Evaluation is the systematic measurement of LLM pipeline quality.</strong> This is the first (and best) concise definition of evals I’ve come across. We want to create a system (that’s reproducible and reliable) to measure the quality of an LLM pipeline. What to measure in the pipeline and how to define quality is what this course will teach us.</p>
<p><strong>Idea B: Pay attention to your prompt.</strong> Many folks don’t read the prompt they’re sending to the LLM! Think about the prompt first before writing an AI-assisted prompt. Specifying your problem involves writing your prompts with care. Use an LLM where it makes sense to be used (e.g.&nbsp;to improve the clarity of your already specific prompt).</p>
<p><strong>Idea C: You have to wear all these hats.</strong> To cross the gulf of comprehension (i.e.&nbsp;what is my data and what stories does it tell?) you must wear your “data scientist” hat. For specification, your “product” hat, and for generalization your “engineer” hat. Shreya and Hamel encourage us to move slowly as we navigate these gulfs. As Kawhi Leonard says: <a href="https://x.com/patbev21/status/1884687382412132558">slow is pro</a>.</p>
<p><strong>Idea D: Collecting representative samples and analyzing failure modes is the most important step in AI evals.</strong> This is where you learn the most, is what most people skip, what has the least guidance out there in the industry. Shreya spends 75-80% of her time on error analysis. Asking the question “what would make a user unhappy?” uncovers your failure modes.</p>
<p><strong>Idea E: The process of writing a good prompt sets the foundation for your evaluation success.</strong> If your prompt fails to specify, in no uncertain terms, what the LLM should and shouldn’t do, how can you expect to evaluate the LLM’s output? “The LLM didn’t structure its response as JSON”—did you ask it to? “The LLM didn’t provide measurements for the recipe”—did you ask it to?</p>
<p><strong>Idea F: The process of removing errors from the LLM pipeline is a process of improving your communication to the LLM,</strong> and this requires empathy! You have to put yourself in the user’s shoes and ask: what do I want from this interaction? What will make me happy? What will make me frustrated? And then you have to relay those user needs to the LLM via the prompt. And finally you have to evaluate whether the desired experience was delivered by the LLM. This dance between playing data scientist/product manager/user/engineer/communicator makes applied AI such a fascinating field.</p>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>I’m swimming in tasks right now (including a new puppy!) so it’s important for me to move slowly and deliberately. Taking this course at this juncture in my life is itself a process of evaluation: what do I focus my time on? How deep do I go into which assignments? What questions should I ask during office hours? The excellent structure and resources that Hamel and Shreya provided us will help ease my cognitive load, allowing me to focus on high value tasks. I don’t know if I’ll be able to write a blog post or make a video for each lesson, but I’ll try to publish at least once a week (on average). Our HW is to write a system prompt for a recipe chatbot, so expect some of my musings on that soon!</p>


</section>

 ]]></description>
  <category>AI Evals</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-22-evals-course/</guid>
  <pubDate>Tue, 22 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Revisiting ColBERTv1 : A Return to First Principles</title>
  <dc:creator>Vishal Bakshi</dc:creator>
  <link>https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/</link>
  <description><![CDATA[ 




<div id="cell-1" class="cell">
<details class="code-fold">
<summary>pip installs</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install transformers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.49.0</span></span>
<span id="cb1-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install ragatouille</span></code></pre></div>
</details>
</div>
<div id="cell-2" class="cell">
<details class="code-fold">
<summary>imports</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.indexing.collection_encoder <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CollectionEncoder</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.infra <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ColBERTConfig</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.modeling.checkpoint <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Checkpoint</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colbert.modeling.tokenization.utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> _split_into_batches, _sort_by_length, _insert_prefix_token</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb2-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> inspect <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> getsource</span>
<span id="cb2-8"></span>
<span id="cb2-9">checkpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Checkpoint(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"answerdotai/answerai-colbert-small-v1"</span>, ColBERTConfig())</span>
<span id="cb2-10">ce <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CollectionEncoder(config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ColBERTConfig(), checkpoint<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>checkpoint)</span></code></pre></div>
</details>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<ul>
<li>Main takeaways (Omar’s thread)</li>
<li>Abstract</li>
<li>Background: Neural Rankers</li>
<li>How does ColBERT compare to previous architectures?</li>
<li>The ColBERT architecture</li>
<li>Encoding Queries and Documents</li>
<li>MaxSim</li>
<li>Offline Indexing</li>
<li>Experimental Evaluation</li>
<li>Results</li>
<li>Ablation Studies</li>
<li>Indexing Throughput &amp; Footprint</li>
<li>Conclusion</li>
</ul>
</section>
<section id="main-takeaways-omars-thread" class="level2">
<h2 class="anchored" data-anchor-id="main-takeaways-omars-thread">Main Takeaways (Omar’s Thread)</h2>
<p>Instead of coming up with my own takeaways, I’m going to do something different this time, where I’m going to walk through Omar’s thread from 2023, where he himself summarizes the main takeaways from the ColBERT v1 paper.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Progress on dense retrievers is saturating.<br><br>The best retrievers in 2024 will apply new forms of late interaction, i.e.&nbsp;scalable attention-like scoring for multi-vector embeddings.<br><br>A🧵on late interaction, how it works efficiently, and why/where it's been shown to improve quality <a href="https://t.co/2XG33TtM9R">pic.twitter.com/2XG33TtM9R</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804963760976092?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I think it’s important to highlight that he talks about new forms of late interaction as “scalable attention-like scoring for multi-vector embeddings”. From my understanding of ColBERT, the attention-like scoring mechanism refers to the BERT contextualization of meaning of tokens. As the query or the document goes through BERT, it passes through the attention mechanism, and tokens attend to each other. So no single query token or document token is isolated; it exists in the context of the entire query or the entire document that it’s in, respectively. The “multi-vector embeddings” part of his tweet is referring to this idea: we don’t compress an entire document or an entire query into a single vector, but instead have more than one vector representing different dimensions of meaning of the text.</p>
<p><mark><strong>Update</strong>: Omar clarified on Twitter that by “attention-like scoring mechanism” he was actually referring specifically to MaxSim as fast attention-like scoring, rather than the BERT contextualization during encoding that I initially described above. Which in hindsight makes sense—emphasis on “attention-like <strong>scoring mechanism</strong>”</mark>.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Say you have 1M documents. With infinite GPUs, what would your retriever look like?<br><br>Maybe a cross-encoder? Finetune a large LM to take &lt;query,doc&gt; pairs. Run it 1M times to get a score for all docs.<br><br>Expressive! Given a query, the LM can pay attention to every detail in the doc! <a href="https://t.co/P4t7bYe9dT">pic.twitter.com/P4t7bYe9dT</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804965942013978?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Something I want to highlight is that you still have to fine-tune a language model to become a retriever because the language model itself has this general knowledge about language and the relationship between words but it doesn’t have explicitly the capability of accurately producing a score that measures the relevancy of one body of text to another. So fine-tuning brings out that implicit skill that is in the latent space of the model into an actionable task. If we had infinite GPUs, we would want to get the relationship of every query token to every document token encoded. We would want to do this during training and we would want to do this during inference. That’s why he says “expressive” - this is the ultimate expressiveness or maximum expressiveness that you can achieve between query and document tokens. Your query tokens are no longer just contextualized within the query; they are contextualized within the query and every single document. That’s a really powerful expressive way of capturing meaning between two bodies of text to determine if they are related to each other to some level.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
But cross-encoders are expensive: must re-run each doc thru the LM <em>for every new query</em>.<br><br>Most retrievers are <em>single-vector</em> encoders: Cram each doc into a vector in advance; match queries/docs with a dot-product.<br><br>Scalable! We can apply dot-product search at the billion scale! <a href="https://t.co/lvLp6vSd8K">pic.twitter.com/lvLp6vSd8K</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804967942697408?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>On the other side of the spectrum, we have the least expressive encoding but it is also more scalable: single vector encoders. You’re cramming each document into a vector and then you’re matching the queries and documents with a dot product.</p>
<p>You’re getting limited contextualization because of the limited expressiveness of what the tokens mean because everything is expressed by a single vector. But this is the most scalable.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
A huge burden on these bi-encoders: They must create <em>one</em> vector that captures <em>every</em> question you may ask about <em>any</em> content in the doc<br><br>Repeatedly been shown to be really fragile (especially OOD) and data-inefficient.<br><br>Can we build far more effective, yet scalable, encoders?
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804970144710981?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I think the key here is that the motivation is not just to build more effective encoders but effective <em>and</em> scalable encoders.</p>
<p>A brief aside on the limitations of single vector representations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/1.png" class="img-fluid figure-img" alt="The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video"></a></p>
<figcaption>The 9:30 mark of the Late Interaction Beats Single Vectors for RAG Introduction to PyLate video</figcaption>
</figure>
</div>
<p>There is a great talk by Antoine Chaffin published recently called <a href="https://youtu.be/1x3k0V2IITo?feature=shared">“Late Interaction Beats Single Vectors for RAG Introduction to PyLate”</a> which is part of Hamel Husain’s AI Evals course. He talks about how pooling is the intrinsic flaw of dense models: the pooling operation compresses <code>n</code> tokens into a single one, because of this selective behavior is learned during training through data, it gets more extreme with longer context because you have to compress more, and the compressed representation learns one notion of similarity. This is what Omar means by the “huge burden” on the bi-encoders. They have to compress a lot of information into a single representation.</p>
<p>If you think about it, during training, which Antoine is talking about here (and he also had a good thread on Twitter, which I can’t find because Twitter’s search is horrible) is that as you’re training a single vector representation, small changes in the query will result in wholesale changes of the single vector that represents the entire document.</p>
<p>So for example if you have a query about actors the document embedding will be trained on expressing that one notion of movies. If you have a separate query about the plot now the document encoding has to represent that different notion of similarity with one vector.</p>
<p>Antoine says in his thread that because of this you get a very noisy training experience for the document encodings because they’re constantly being tossed around left and right to match different notions of similarity with each query in the training step. They can’t match all of the notions because they are a single vector representation or compression of multiple tokens.</p>
<p>Contrast this with the late interaction setup where you have one representation for each token. Now, when you are training, the token embedding in the document that corresponds to the query about the actors gets modified and adjusted to result in a better relevance score. Later on in training, a query about the plot is going to activate the token in the document about the plot, and the query about the visual effects will activate the visual effect tokens in the document, and so on. So, you get this fine-grained, nuanced representation aligning with fine-grained, nuanced meaning in queries during training. The benefit of this is that n each step of training you can have a new, nuanced gradient update to the weights that produce these granular representations.</p>
<p>Back to Omar’s thread.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Late interaction (ColBERT) is simple.<br><br>Let's <em>not</em> cram docs into a vector. Instead, let's make the attention (interaction) scalable.<br><br>How? Build a <em>late</em> interaction fn.<br><br>(1) Applied after, not during, encoding.<br>(2) Pruning-capable, i.e.&nbsp;scales better than linear. This is key! <a href="https://t.co/m7fS15SdIE">pic.twitter.com/m7fS15SdIE</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804972455768167?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>If we want efficiency, we can’t get the full effect of attention. We can’t get all query tokens and all document tokens attending to each other. And we can get an interaction between query token embeddings and document token embeddings that is done efficiently if it is applied after, not during, encoding. The “late” in “late interaction” is what allows the pruning capability, which we’ll talk about later in the presentation.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
Is there a function like this that preserves the retrieval precision of BERT attention?<br><br>Oddly, yes and it's incredibly simple: just aggregate (sum) the MaxSim from query tokens to doc tokens.<br><br>Basically, softly locate each query vector in the doc (w dot product), and aggregate.
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804974485811685?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The key aspect of MaxSim is it takes the maximum similarity. Unlike something like average similarity, it doesn’t care about all tokens; it cares about the document token that has the maximum similarity with the given query token. This eliminates all but one document token from final consideration, which is what allows pruning capability to unlock. Because the interaction between query and document token embeddings happens after they’re encoded, you can encode queries and documents separately.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
It was a last-ditch run on a Sunday night (3 Nov'19) after complex scoring failed.<br><br>I spent weeks looking for a "bug". ColBERT w cheap scoring rivaled BERT-large cross-encoders with 10,000x more FLOPs?!<br><br>Called it ColBERT as a pun: late show / late interaction<br><br>log scale latency: <a href="https://t.co/4MGmzAYHYG">pic.twitter.com/4MGmzAYHYG</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804976406802703?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Incredible that events like these happen, and I think happen quite frequently.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
There's a little-known trick that was essential for ColBERT's results: Query Augmentation.<br><br>ColBERT appends [MASK] tokens to the query encoder to allow BERT to create more query vectors that aren't there!<br><br>Is it the earliest form of a scratchpad / chain of thought? From Nov 2019! <a href="https://t.co/2rIoMn1jxP">pic.twitter.com/2rIoMn1jxP</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804978667434048?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This is a super interesting concept that we’ll look at in detail later on.</p>
<p>As another aside—one of the reasons I was motivated to re-read the ColBERT V1 paper was this thread below by Antoine.</p>
<blockquote class="twitter-tweet blockquote" data-theme="dark">
<p lang="en" dir="ltr">
I am starting to be more and more convinced that MaxSim generalize very well to long documents but struggles on longer query, most probably due to the asymmetry<br>Larger documents are bound by the number of query tokens, but larger queries might get noisy<br>Either it is a query…
</p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1942909502723883381?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>If you have a document with 1k tokens and a query with 32 tokens, at most 32 document tokens will pass the maximum similarity threshold, hence “larger documents are bound by the number of query tokens.” As your query gets larger the meaning of tokens becomes diverse, this may create noise in the meaning expressed in the query. Query tokens with vastly different meanings will have maximum similarity with document tokens that are vastly different in meaning as well.</p>
<p>The conversation continues with:</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Yes, that is also why it was not a big issue with query expansion because all the queries had the same number of tokens<br>but with longer queries and no query exp, meh
</p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1943002350274179544?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>slm tokens makes a really interesting point. Let’s assume that for each of the 32 query tokens, you find a document token that has a cosine similarity of 1. You add them up, and the maximum similarity is 32, the length of the query. That’s what they’re saying by the maximum MaxSim is the length of the query. Assuming that as your query gets longer, the meaning of the tokens starts to vary, and potentially the maximum similarity between the query and document tokens starts to vary.</p>
<p>You can imagine that you could have a very long query where one or more tokens are kind of obscure and may be on the fringes of the intent and meaning of the whole query, and potentially dissimilar to any document in the collection. These will potentially have a very small maximum similarity with some document token. In this situation, you can imagine that you have a very high variance because some query tokens will have high maximum similarities, some will have low, and you get this kind of noisy distribution of similarities across the query.</p>
<p>They go on to say that normalization can’t be something like the mean over query tokens, but has to be related to the distribution. Why does Antoine say that there is no query expansion? We’ll see that in the next tweet.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
btw let's rule out query expansion because I am training models using flash attention and so there isn't any query expansion (I keep forgetting ffs 😭)
</p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1942969244183834786?ref_src=twsrc%5Etfw">July 9, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This will make more sense after we look at query expansion/augmentation later in this blog post. Basically, query expansion relies on masked tokens containing semantic meaning, but, IIUC, Flash Attention negates masked tokens, so it nullifies the interaction between masked tokens and other tokens.</p>
<p>Here’s a tweet from Antoine 9 months ago that explains this:</p>
<blockquote class="twitter-tweet blockquote" data-theme="dark">
<p lang="en" dir="ltr">
</p><p>Funny insight:<br><br>ColBERT query expansion works by adding tokens that are not attended but attend to the others <br><br>This works in the OG attention implementation with attention mask (as the attention values are computed for these tokens, their contributions are just masked out for the other tokens, but their representations are computed w.r.t the unmasked tokens)</p>
<p>However, with Flash Attention, masked tokens embeddings are just zeros, meaning the contribution to MaxSim is always zeros and these tokens are not used, as if there was no query expansion at all</p>
This might explain Jina-ColBERT results of attending vs not attending to those (which seems contrary to our results): if they activated FA during the tests, the comparaison is actually no query expansion vs query expansion with attending, not not attending vs attending
<p></p>
— Antoine Chaffin (<span class="citation" data-cites="antoine_chaffin">@antoine_chaffin</span>) <a href="https://twitter.com/antoine_chaffin/status/1862059400271389138?ref_src=twsrc%5Etfw">November 28, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Here is a slide from a previous video that I made titled <a href="https://youtu.be/u_v6HHyv4No?feature=shared">“Understanding Eager Bidirectional Attention via the Attention Mask”</a>. In this case, we have 16 tokens, including four masked tokens. The large 16x16 tensor at the bottom is the attention scores tensor. The masked tokens are correctly not being attended to, but they do attend to other tokens. The last four columns are set to negative infinity because they are masked tokens, so their attention scores will be zero. But the last four rows do contain some 1s. The attention scores will not be zero for those rows and columns where we have 1s. Masked tokens are not being attended to, but they do attend to other tokens, and therefore they do have an attention score, and therefore they will have hidden states in the embedding space.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="16x16 attention mask"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/2.png" class="img-fluid figure-img" alt="16x16 attention mask"></a></p>
<figcaption>16x16 attention mask</figcaption>
</figure>
</div>
<p>Back to Omar’s thread.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
OK, but how can ColBERT search 100M docs in ~100 milliseconds?<br><br>Late interaction is pruning-capable: it only needs to "touch" &lt; 0.1% of the documents to find the top-K.<br><br>This is by design: it's composed of monotonic functions (Max/Sum), which enable some neat algorithmic tricks.
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804980840182079?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-theme="dark">
<p lang="en" dir="ltr">
We can decompose late interaction into dozens of tiny nearest-neighbor searches, at the token level.<br><br>We'll only fetch &amp; score docs in which at least one token in close to (at least one token in) the query.<br><br>Otherwise, we can prove the score will be too small, and we can skip it! <a href="https://t.co/6vBZp1U0Ku">pic.twitter.com/6vBZp1U0Ku</a>
</p>
— Omar Khattab (<span class="citation" data-cites="lateinteraction">@lateinteraction</span>) <a href="https://twitter.com/lateinteraction/status/1736804982949917032?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>As shown in the diagram, we have clusters of document token embeddings, clustered by some vector-similarity indexing process. For each query token embedding, we find the closest few clusters to it. We perform our MaxSim operation between the query tokens and all those clustered documents’ tokens. With this initial clustering step, we’re filtering out low-relevance documents using nearest neighbor search from the start.</p>
<p>With these main takeaways under our belt, in the following sections we’ll walk through each part of the ColBERTv1 paper in detail. I’ll provide excerpts from the paper in block quotes (highlighted emphasis mine) and then my thoughts after that.</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a <mark>late interaction architecture that independently encodes the query and the document using BERT</mark> and then employs a <mark>cheap yet powerful interaction step that models their fine-grained similarity.</mark> By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to <mark>pre-compute document representations offine</mark>, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT’s <mark>pruning-friendly interaction mechanism</mark> enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT’s effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing <mark>two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query</mark>.</p>
</blockquote>
<p>The key part about late interaction is that the architecture independently encodes the query and document. This allows you to index document representations offline which allows you to delay the query-document interaction until the end of the architecture.</p>
<p>What took me some unpacking is the line: &gt; ColBERT’s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection.</p>
<p>IIUC, the pruning-friendliness of ColBERT is unlocked by the fact that the interaction mechanism uses <strong>maximum</strong> similarity, and because of the nature of maximum (i.e.&nbsp;only 1 token can satisfy maximum) you can ignore low-similarity documents. Vector similarity indexes group together documents by similarity, so if a cluster of documents is not close to the query token embedding in question, it can be ignored completely.</p>
</section>
<section id="background-neural-rankers" class="level2">
<h2 class="anchored" data-anchor-id="background-neural-rankers">Background: Neural Rankers</h2>
<p>On the terms “ranker” vs.&nbsp;“retriever” and what that brings up or me:</p>
<p>The terms Ranker and Retriever give me different mental images.</p>
<p>When I think of Ranker, I think of you already having some passages that are deemed relevant and you’re ranking them, bringing the best ones to the top.</p>
<p>When I think of Retriever, the mental image I have is that you have this collection of data documents, a corpus of text where you have irrelevant and relevant passages all mixed together. The retriever then goes in, sifts through this text, and finds the relevant passages.</p>
<p>It’s been a bit of an adjustment for me using these two as synonyms. So that’s something I just want to keep in mind as I’m reading literature is that Ranker and Retriever should give me the same mental image, but they don’t.</p>
<blockquote class="blockquote">
<p><mark>By computing deeply-contextualized semantic representations of query-document pairs, these LMs help bridge the pervasive vocabulary mismatch [21, 42] between documents and queries [30].</mark></p>
</blockquote>
<p>I wanted to highlight this sentence from the background section because I thought it was getting to the core of something about BERT that I didn’t really know. We’ll see in a bit. But first–</p>
<p>The following excerpt is from the paper Modeling and Solving Term Mismatch for Full-Text Retrieval Which is reference [42], written in 2012:</p>
<blockquote class="blockquote">
<p>Even though modern retrieval systems typically use a multitude of features to rank documents, the backbone for search ranking is usually the standard tf.idf retrieval models.</p>
<p>This thesis addresses a limitation of the fundamental retrieval models, the term mismatch problem, which happens when query terms fail to appear in the documents that are relevant to the query. The term mismatch problem is a long standing problem in information retrieval.</p>
</blockquote>
<p>I haven’t read the full thesis, but it does make sense that for keyword-based search, the query term failing to appear in the document that is relevant to the query would be a major problem as it’s frequency in the document would be 0.</p>
<p>Another paper referenced on this vocabulary mismatch problem is “Understanding the Behaviors of BERT and Ranking” where they say:</p>
<blockquote class="blockquote">
<p>The observations suggest that, BERT’s pre-training on surrounding contexts favors text sequence pairs that are closer in their semantic meaning.</p>
</blockquote>
<p>So, it seems like even in the embedding space, the term mismatch problem is present. Another excerpt from the same paper:</p>
<blockquote class="blockquote">
<p>[BERT] prefers semantic matches between paraphrase tokens</p>
</blockquote>
<p>Here’s Figure 2 from the same paper where each point on the chart corresponds to one query-passage pair with a random regular term removed from the passage:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure 2 from “Understanding the Behaviors of BERT and Ranking”"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/3.png" class="img-fluid figure-img" alt="Figure 2 from “Understanding the Behaviors of BERT and Ranking”"></a></p>
<figcaption>Figure 2 from “Understanding the Behaviors of BERT and Ranking”</figcaption>
</figure>
</div>
<p>The x-axis is the original ranking score, and the y-axis is the score after the term is removed. One takeaway they had in the paper is that BERT in general has extreme scores. It either scores 1 or 0. But that’s not the main take away here when it comes to the concept of query-document-term mismatch. In the bottom right corner of the BERT chart, we can see that there are query-passage pairs with a high orginal ranking score and a low score after a term is removed. The original ranking of 1.0 drops to a ranking of 0.0. This is evidence that the query document term mismatch problem occurs in semantic space as well. If you remove a term that’s semantically similar in the query to the document, then BERT will not recognize the similarity between the two and will give the pair a low ranking score.</p>
<p>Let’s continue a little bit more into the background of neural rankers, but now in the context of how does ColBERT compare to these previous neural architectures?</p>
</section>
<section id="how-does-colbert-compare-to-previous-architectures" class="level2">
<h2 class="anchored" data-anchor-id="how-does-colbert-compare-to-previous-architectures">How does ColBERT compare to previous architectures?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure 2 from ColBERTv1 paper"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/4.png" class="img-fluid figure-img" alt="Figure 2 from ColBERTv1 paper"></a></p>
<figcaption>Figure 2 from ColBERTv1 paper</figcaption>
</figure>
</div>
<p>The small rectangles in this graphic represent words, subwords or tokens. The wider rectangle represent large dimension vectors or representations.</p>
<p>Representation-based Similarity (figure 2a) calculates a single cosine similarity score between a single query embedding and a single document embedding. Query-Document interaction (2b) feeds an interaction matrix with similarity scores between every pair of query-document tokens to a neural net which produces a single final similarity score. BERT (2c), all-to-all interaction, attends each token in the query to all other tokens in the query, and each token in the document to all other tokens in the document, contextualizing each token with all other query/document tokens. From the <a href="https://arxiv.org/pdf/1901.04085">Passage Re-Ranking with BERT paper</a>:</p>
<blockquote class="blockquote">
<p>We use a BERT_LARGE model as a binary classification model, that is, we use the [CLS] vector as input to a single layer neural network to obtain the probability of the passage being relevant</p>
</blockquote>
<p>Late interaction (2d), ColBERT, combines the best of both worlds: the offline computation of representation-based similarity and the richness/granularity of interaction-based similarity. Query tokens attend to each other during encoding, document tokens attend to each other during (offline) encoding; during interaction, each query token interacts with all document tokens and the document token with the maximum similarity is selected; these maximum similarities are summed across all query tokens, giving you one score per document. Not all documents in the collection need to be considered; vector similarity indexes naturally group relevant documents together. Searching for document token embeddings in clusters close to the query token embeddings reduces the number of candidates considered.</p>
<p>These architectural differences of ColBERT give it a ton of advantages:</p>
<blockquote class="blockquote">
<p>As Figure 1 illustrates, ColBERT can serve queries in tens or few hundreds of milliseconds. For instance, when used for reranking as in “ColBERT (re-rank)”, it delivers over 170× speedup (and requires 14,000× fewer FLOPs) relative to existing BERT-based models, while being more effective than every non-BERT baseline (§4.2 &amp; 4.3). ColBERT’s indexing—the only time it needs to feed documents through BERT—is also practical: it can index the MS MARCO collection of 9M passages in about 3 hours using a single server with four GPUs (§4.5), retaining its effectiveness with a space footprint of as little as few tens of GiBs.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="5.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure 1 from the ColBERTv1 paper"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/5.png" class="img-fluid figure-img" alt="Figure 1 from the ColBERTv1 paper"></a></p>
<figcaption>Figure 1 from the ColBERTv1 paper</figcaption>
</figure>
</div>
<p>Figure 1 shows that ColBERT has comparable performance to BERT Large and BERT Base but a 100x faster query latency. This is not an insignificant difference, as they say that “raising query response times by as little as 100ms is known to impact user experience and even measurably diminish revenue”. So, it follows that a decrease from 10,000 milliseconds to 100s of milliseconds is going to measurably increase revenue.</p>
<p>One related paper I briefly want to focus on is this 2018 paper by Zamani et al, which introduces SNRM (emphasis mine):</p>
<blockquote class="blockquote">
<p>In 2018, Zamani et al.&nbsp;[41] introduced SNRM, a representationfocused IR model that encodes each query and each document as a single, sparse high-dimensional vector of “latent terms”. By producing a <mark>sparse-vector representation for each document</mark>, SNRM is able to use a traditional IR inverted index for representing documents, allowing fast end-to-end retrieval. Despite highly promising results and insights, SNRM’s effectiveness is substantially outperformed by the state of the art on the datasets with which it was evaluated (e.g., see [18, 38]). <mark>While SNRM employs sparsity to allow using inverted indexes</mark>, we relax this assumption and compare a (dense) BERT-based representation-focused model against our late-interaction ColBERT in our ablation experiments</p>
</blockquote>
<p>I fed the SNRM paper to Claude Sonnet 4 and asked it to give me an example of sparsity allows the use of an inverted file index:</p>
<blockquote class="blockquote">
<p><strong>Simple SNRM example:</strong></p>
<p><strong>Training produces latent terms like:</strong> - Latent term 47: “sports concepts” - Latent term 203: “technology concepts”</p>
<p><strong>Document representations:</strong> - Doc A (about basketball): [0, 0, …, 0.8, 0, …, 0, 0, …] (high weight at position 47) - Doc B (about AI): [0, 0, …, 0, 0, …, 0.9, 0, …] (high weight at position 203)</p>
<p><strong>Inverted index:</strong> - Latent term 47 → [Doc A: 0.8, Doc C: 0.3] - Latent term 203 → [Doc B: 0.9, Doc D: 0.5]</p>
<p><strong>Query time:</strong> Sports query activates latent term 47 → quickly finds Doc A and C without checking every document.</p>
</blockquote>
<p>Let’s say we have two document representations: document A which is about basketball and document B which is about AI. SNRM generates sparse representations, so a number of values are zero. Let’s say that we have a latent term 47 that’s about sports concepts and a latent term 203 that’s about technology concepts. The inverted index for latent term 47 is going to store the value of that term in doc A (which is 0.8) and in doc D (let’s say that’s 0.3). For the latent term 203 (which is technology concepts), the inverted index will store 0.9 for doc B (which is the highest position value) and 0.5 for doc D (which would be some relatively low position). At query time, a sports query activates the latent term for 47, and because that’s efficiently stored in the inverted index, it’s a quick lookup and you don’t have to check every document.</p>
<p>So now that we have a sense of where the ColBERT architecture falls in the context of previous work, we can now dive into the ColBERT architecture itself.</p>
</section>
<section id="the-colbert-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-colbert-architecture">The ColBERT Architecture</h2>
<blockquote class="blockquote">
<p>delaying the query–document interaction can facilitate cheap neural re-ranking (i.e., through pre-computation) and even support practical end-to-end neural retrieval (i.e., through pruning via vector-similarity search)</p>
</blockquote>
<p>ColBERT balances neural retrieval quality and cost, benefiting both re-ranking and end-to-end retrieval. The delayed query-document interaction enables offline document indexing. At query time, you only encode the query and run MaxSim operations. For end-to-end retrieval, this same offline indexing allows vector similarity clustering—instead of searching all documents, you query the closest clusters, dramatically reducing candidates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="6.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/6.png" class="img-fluid figure-img" alt="Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d"></a></p>
<figcaption>Figure 3 from the ColBERTv1 paper: the general architecture of ColBERT given a query q and a document d</figcaption>
</figure>
</div>
<p>The general architecture of ColBERT, comprises of: a query encoder fQ (shown in green), a document encoder fD (shown in blue), and the late interaction mechanism S(shown in gray). Given a query q and document d, fQ encodes q into a bag of embeddings Eq while fD encodes d into another bag Ed. Each embeddings in Eq and Ed is contextualized based on the other terms in q or d, respectively.</p>
<p>Before we look at the Late Interaction Mechanism, let’s look closer at what is involved during the encoding of queries and documents.</p>
</section>
<section id="encoding-queries-and-documents" class="level2">
<h2 class="anchored" data-anchor-id="encoding-queries-and-documents">Encoding Queries and Documents</h2>
<blockquote class="blockquote">
<p>We share a single BERT model among our query and document encoders but distinguish input sequences that correspond to queries and documents by prepending a special token [Q] to queries and another token [D] to documents.</p>
</blockquote>
<blockquote class="blockquote">
<p>Given BERT’s representation of each token, our encoder passes the contextualized output representations through a linear layer with no activations. This layer serves to control the dimension of ColBERT’s embeddings, producing m-dimensional embeddings for the layer’s output size m. As we discuss later in more detail, we typically x m to be much smaller than BERT’s xed hidden dimension.</p>
</blockquote>
<blockquote class="blockquote">
<p>While ColBERT’s embedding dimension has limited impact on the efficiency of query encoding, this step is crucial for controlling the space footprint of documents</p>
</blockquote>
<p>A quick note about embedding dimension: there are models such as <a href="https://huggingface.co/answerdotai/answerai-colbert-small-v1">answerai-colbert-small-v1</a> where the embedding dimension is as small as 96.</p>
<p>Here’s the desription of the query encoder:</p>
<blockquote class="blockquote">
<p><strong>Query Encoder.</strong> Given a textual query q, we tokenize it into its BERT-based WordPiece [35] tokens q1, q2…ql . We prepend the token [Q] to the query. We place this token right after BERT’s sequence-start token [CLS]. If the query has fewer than a pre-defined number of tokens Nq , <mark>we pad it with BERT’s special [mask] tokens up to length Nq</mark> (otherwise, we truncate it to the first Nq tokens). This padded sequence of input tokens is then passed into BERT’s deep transformer architecture, which computes a contextualized representation of each token.</p>
</blockquote>
<p>Here is a key contribution of this paper, that I am going to do a dive into next:</p>
<blockquote class="blockquote">
<p>We denote the padding with masked tokens as <strong>query augmentation</strong>, a step that allows BERT to produce query-based embeddings at the positions corresponding to these masks. Query augmentation is intended to serve as a <mark>soft, differentiable mechanism for learning to expand queries with new terms or to re-weigh existing terms based on their importance for matching the query</mark>. As we show in §4.4, this operation is essential for ColBERT’s effectiveness.</p>
</blockquote>
<section id="query-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="query-augmentation">Query Augmentation</h3>
<p>Query augmentation is the idea that mask tokens carry semantic meaning, so padding short queries up to some fixed length expands the queries with these new semantically relevant terms, adding more nuance to help match similar terms in documents. In this side quest, I want to understand just how semantically similar these mask token embeddings are to the non-mask query tokens. I’ll start by digging into the code in the repo which takes text and converts it to embeddings.</p>
<p>Here’s how the <code>Searcher</code> encodes the query, where it uses <code>queryFromText</code></p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encode(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, text: TextQueries, full_length_search<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb3-2">    queries <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(text) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> [text]</span>
<span id="cb3-3">    bsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(queries) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-4"></span>
<span id="cb3-5">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.checkpoint.query_tokenizer.query_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.query_maxlen</span>
<span id="cb3-6">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.checkpoint.queryFromText(queries, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>bsize, to_cpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, full_length_search<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>full_length_search)</span>
<span id="cb3-7"></span>
<span id="cb3-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> Q</span></code></pre></div>
<div id="cell-88" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="37d9aab6-94cb-4de6-fcf1-f5a8e2827de4" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">checkpoint.query_tokenizer.query_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb4-2">Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.queryFromText([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"this is a short query"</span>], bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, to_cpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, full_length_search<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast() if self.activated else NullContextManager()</code></pre>
</div>
</div>
<div id="cell-89" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f66d2856-acc2-454d-911b-3f3b98324759" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">Q.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>torch.Size([1, 32, 96])</code></pre>
</div>
</div>
<p>Note that even though there are less than 32 tokens in <code>"this is a short query"</code>, the norm of all <code>Q</code> embeddings is <code>1.0</code>. This is because ColBERT adds <code>[MASK]</code> tokens to pad the query to a 32-token length, which we’ll see next.</p>
<div id="cell-91" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b21a9f43-389e-4837-bb58-ef047821bddb" data-execution_count="199">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">Q.norm(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="199">
<pre><code>tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])</code></pre>
</div>
</div>
<p><code>checkpoint.queryFromText</code> uses <code>QueryTokenizer.tokenizer</code> which does the following:</p>
<p>It first tokenizes the text</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># tokenize with max_length - 1 to add the marker id afterwards</span></span>
<span id="cb10-2">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tok(batch_text, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_length'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb10-3">                return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(max_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).to(DEVICE)</span>
<span id="cb10-4"></span>
<span id="cb10-5">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Q_marker_token_id)</span>
<span id="cb10-6">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p>And then replaces the padding token with the mask token.</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># postprocess for the [MASK] augmentation</span></span>
<span id="cb11-2">ids[ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.pad_token_id] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask_token_id</span></code></pre></div>
<p>Looking at that concretely:</p>
<div id="cell-94" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.query_tokenizer.tok([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"this is a short query"</span>], padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_length'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb12-2">                return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb12-3"></span>
<span id="cb12-4">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], checkpoint.query_tokenizer.Q_marker_token_id)</span>
<span id="cb12-5">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-95" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6d429500-8377-4845-c844-08099e97adcc" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]])</code></pre>
</div>
</div>
<div id="cell-96" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="14e03a64-ea80-4a01-db59-1d2dc156355a" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">ids[ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> checkpoint.query_tokenizer.pad_token_id] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> checkpoint.query_tokenizer.mask_token_id</span>
<span id="cb15-2">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103]])</code></pre>
</div>
</div>
<div id="cell-97" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="7d24afd2-b0ea-4f04-a1bf-ad826c24ce00" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">checkpoint.query_tokenizer.tok.decode([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">103</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>'[MASK]'</code></pre>
</div>
</div>
<div id="cell-98" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:71}}" data-outputid="900df54d-3ce7-428a-d888-c7cd3023214d" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">checkpoint.query_tokenizer.tok.decode(ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>'[CLS] [unused0] this is a short query [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'</code></pre>
</div>
</div>
<p>This replacement of pad tokens with mask tokens is critical because <code>queryFromText</code> calls <code>query</code> which is defined as:</p>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> query(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, input_ids, attention_mask):</span>
<span id="cb21-2">    input_ids, attention_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> input_ids.to(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device), attention_mask.to(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device)</span>
<span id="cb21-3">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.bert(input_ids, attention_mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_mask)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb21-4">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear(Q)</span>
<span id="cb21-5"></span>
<span id="cb21-6">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask(input_ids, skiplist<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]), device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.device).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb21-7">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask</span>
<span id="cb21-8"></span>
<span id="cb21-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.nn.functional.normalize(Q, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<p>It actually creates its own <code>mask</code> to multiply <code>Q</code> by—it doesn’t use <code>attention_mask</code>.</p>
<p>Looking at <code>mask</code>:</p>
<div id="cell-102" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f57d4111-5908-4afa-8094-80164a8f80af" data-execution_count="20">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(getsource(checkpoint.mask))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    def mask(self, input_ids, skiplist):
        mask = [[(x not in skiplist) and (x != self.pad_token) for x in d] for d in input_ids.cpu().tolist()]
        return mask
</code></pre>
</div>
</div>
<p>If the token is <code>not in skiplist</code> and <code>!= self.pad_token</code> it gets a <code>1</code> in the <code>mask</code>. Since we swapped pad tokens with MASK tokens, they get a <code>1</code>.</p>
<div id="cell-104" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c8d1cd3a-ee95-43f9-f43f-0ceffc6c2aaa" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">ids</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor([[  101,     1,  2023,  2003,  1037,  2460, 23032,   102,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,
           103,   103]])</code></pre>
</div>
</div>
<div id="cell-105" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="99bb7805-01f9-43be-f484-4bfd3e7a088c" data-execution_count="24">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">torch.tensor(checkpoint.mask(ids, skiplist<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[])).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])</code></pre>
</div>
</div>
<p>As we can see, the <code>mask</code> is all <code>1</code>s, so <code>Q</code> remains unchanged.</p>
<p>As an aside, I’ve been thinking about how RAGatouille sets the maximum query length based on the full query length (instead of fixing to 32):</p>
<div class="sourceCode" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(query, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>):</span>
<span id="cb28-2">    query_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(query.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.35</span>)</span>
<span id="cb28-3">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._upgrade_searcher_maxlen(query_length, base_model_max_tokens)</span>
<span id="cb28-4">    results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._search(query, k, pids)]</span>
<span id="cb28-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb28-6">    longest_query_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>([<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.35</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> query])</span>
<span id="cb28-7">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._upgrade_searcher_maxlen(longest_query_length, base_model_max_tokens)</span>
<span id="cb28-8">    results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._batch_search(query, k)</span></code></pre></div>
<p>I think the following note about <code>full_length_search</code> in the ColBERT repo is related but I’m not currently sure:</p>
<div class="sourceCode" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Full length search is only available for single inference (for now)</span></span>
<span id="cb29-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Batched full length search requires far deeper changes to the code base</span></span>
<span id="cb29-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span>(full_length_search <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(batch_text) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(batch_text) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb29-4"></span>
<span id="cb29-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> full_length_search:</span>
<span id="cb29-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Tokenize each string in the batch</span></span>
<span id="cb29-7">    un_truncated_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tok(batch_text, add_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).to(DEVICE)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>]</span>
<span id="cb29-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the longest length in the batch</span></span>
<span id="cb29-9">    max_length_in_batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> un_truncated_ids)</span>
<span id="cb29-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set the max length</span></span>
<span id="cb29-11">    max_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_len(max_length_in_batch)</span>
<span id="cb29-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb29-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Max length is the default max length from the config</span></span>
<span id="cb29-14">    max_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.query_maxlen</span></code></pre></div>
</section>
<section id="mask-token-embeddings-are-not-meaningless" class="level3">
<h3 class="anchored" data-anchor-id="mask-token-embeddings-are-not-meaningless"><code>MASK</code> token embeddings are not meaningless</h3>
<p>So what meaning is embedded for the MASK token in the semantic space? To (lightly) explore this, I’ll calculate the cosine similarity between the non-MASK and MASK tokens.</p>
<div id="cell-110" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1d97c78d-80f4-426d-9feb-7aaa3f8dcd15" data-execution_count="25">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>tensor([  101,     1,  2023,  2003,  1037,  2460, 23032])</code></pre>
</div>
</div>
<div id="cell-111" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="55585a26-af0a-4b64-8453-102cc78da962" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">checkpoint.query_tokenizer.tok.decode(ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>'[CLS] [unused0] this is a short query'</code></pre>
</div>
</div>
<div id="cell-112" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="9e6e1ed8-eca4-4a74-b2da-f98270723158" data-execution_count="32">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">checkpoint.query_tokenizer.tok.decode([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2460</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">23032</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>'short query'</code></pre>
</div>
</div>
<div id="cell-113" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="620147a3-0f6b-4a41-cd88-120864057e16" data-execution_count="27">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>:]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor([103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103,
        103, 103, 103, 103, 103, 103, 103, 103, 103, 103])</code></pre>
</div>
</div>
<div id="cell-114" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:53}}" data-outputid="91f261b2-0a15-4150-cf9d-58c6933c3069" data-execution_count="28">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">checkpoint.query_tokenizer.tok.decode(ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>:])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>'[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'</code></pre>
</div>
</div>
<p>Gathering the 96-dimensional embeddings for my non-MASK tokens.</p>
<div id="cell-116" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="249a17b0-4f81-4171-84a9-dc10719f7f0a" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">Qnm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]</span>
<span id="cb40-2">Qnm.shape, Qnm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(torch.Size([7, 96]), torch.Size([1, 7, 96]))</code></pre>
</div>
</div>
<p>Gathering the 96-dimensional embeddings for my MASK tokens.</p>
<div id="cell-118" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f68d9c1b-3b29-4da5-e522-dcd4e2a19cee" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">Qm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>:]</span>
<span id="cb42-2">Qm.shape, Qm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(torch.Size([25, 96]), torch.Size([25, 1, 96]))</code></pre>
</div>
</div>
<p>Taking the cosine similarity between the non-MASK and MASK tokens, we see that the MASK tokens (rows) are considerably similar in meaning (and in one case exactly the same) to the non-MASK tokens (columns)!</p>
<div id="cell-120" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fbf9c790-8b74-49db-e9a9-248b313bf574" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">torch.nn.functional.cosine_similarity(Qnm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), Qm.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>tensor([[1.0000, 0.9709, 0.9150, 0.9461, 0.9672, 0.7417, 0.6532],
        [0.9793, 0.9566, 0.9038, 0.9301, 0.9495, 0.7177, 0.6400],
        [0.9804, 0.9585, 0.9053, 0.9320, 0.9517, 0.7195, 0.6380],
        [0.9814, 0.9604, 0.9059, 0.9340, 0.9534, 0.7198, 0.6394],
        [0.9812, 0.9598, 0.9065, 0.9342, 0.9533, 0.7220, 0.6390],
        [0.9814, 0.9601, 0.9073, 0.9355, 0.9537, 0.7223, 0.6341],
        [0.9816, 0.9612, 0.9068, 0.9367, 0.9549, 0.7222, 0.6354],
        [0.9809, 0.9604, 0.9050, 0.9357, 0.9535, 0.7212, 0.6333],
        [0.9823, 0.9623, 0.9064, 0.9385, 0.9562, 0.7228, 0.6365],
        [0.9826, 0.9629, 0.9070, 0.9389, 0.9567, 0.7249, 0.6373],
        [0.9818, 0.9614, 0.9062, 0.9387, 0.9554, 0.7248, 0.6390],
        [0.9819, 0.9608, 0.9044, 0.9387, 0.9550, 0.7241, 0.6388],
        [0.9821, 0.9620, 0.9032, 0.9394, 0.9561, 0.7252, 0.6372],
        [0.9830, 0.9655, 0.9048, 0.9428, 0.9601, 0.7278, 0.6390],
        [0.9836, 0.9690, 0.9081, 0.9459, 0.9632, 0.7273, 0.6392],
        [0.9860, 0.9761, 0.9165, 0.9517, 0.9717, 0.7343, 0.6475],
        [0.9850, 0.9848, 0.9299, 0.9542, 0.9805, 0.7550, 0.6576],
        [0.9805, 0.9872, 0.9360, 0.9545, 0.9827, 0.7667, 0.6658],
        [0.9780, 0.9878, 0.9385, 0.9544, 0.9833, 0.7705, 0.6693],
        [0.9774, 0.9879, 0.9389, 0.9543, 0.9832, 0.7701, 0.6699],
        [0.9764, 0.9887, 0.9397, 0.9549, 0.9839, 0.7709, 0.6731],
        [0.9762, 0.9887, 0.9394, 0.9548, 0.9838, 0.7714, 0.6736],
        [0.9763, 0.9885, 0.9396, 0.9548, 0.9837, 0.7715, 0.6738],
        [0.9776, 0.9881, 0.9392, 0.9551, 0.9837, 0.7709, 0.6713],
        [0.9772, 0.9887, 0.9400, 0.9555, 0.9840, 0.7714, 0.6736]])</code></pre>
</div>
</div>
<p>It’s interesting to note that the first MASK token (first row) has a cosine similarity of 1 with the first non-MASK token (the <code>[CLS]</code> tokens, first column). Other interesting observations:</p>
<ul>
<li>the second non-MASK token (<code>[unused0]</code>, second column) is more similar to the last MASK token than most of the other MASK tokens.</li>
<li>In general, the MASK tokens are much less similar to the last two non-MASK token (<code>short query</code>, 6th and 7th columns) than they are to the first five non-MASK tokens.</li>
</ul>
<p>I think this is enough evidence to show that the MASK tokens carry semantic meaning important to the query.</p>
<p>Returning to the paper, let’s see what they have to say about the document encoder:</p>
<blockquote class="blockquote">
<p><strong>Document Encoder</strong>. Our document encoder has a very similar architecture. We first segment a document d into its constituent tokens d1, d2…dm, to which we prepend BERT’s start token [CLS] followed by our special token [D] that indicates a document sequence. <mark>Unlike queries, we do not append [mask] tokens to documents</mark>. After passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list. This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness. In summary, given q = q0, q1…ql and d = d0, d1…dn , we compute the bags of embeddings Eq and Ed in the following manner, where # refers to the [mask] tokens:</p>
<p>Eq := Normalize( CNN( BERT(“[Q]q0q1…ql ##…#”) ) ) (1)</p>
<p>Ed := Filter( Normalize( CNN( BERT(“[D]d0d1…dn”) ) ) ) (2)</p>
</blockquote>
<p>I want to highlight something they say about how they encode their documents:</p>
<blockquote class="blockquote">
<p>When batching, <mark>we pad all documents to the maximum length of a document within the batch</mark>. To make capping the sequence length on a per-batch basis more effective, our indexer proceeds through documents in groups of B (e.g., B = 100,000) documents. <mark>It sorts these documents by length</mark> and then feeds batches of b (e.g., b = 128) <mark>documents of comparable length</mark> through our encoder.</p>
</blockquote>
<p>So let’s look at some of the code. In the <code>CollectionEncoder</code> class, which is what’s used to encode documents and queries, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/indexing/collection_encoder.py#L26">they call <code>docFromText</code></a> and they pass to it the passages which are currently strings:</p>
<div class="sourceCode" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> passages_batch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> batch(passages, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config.index_bsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>):</span>
<span id="cb46-2">    embs_, doclens_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.checkpoint.docFromText(</span>
<span id="cb46-3">        passages_batch,</span>
<span id="cb46-4">        ...)</span></code></pre></div>
<p>Inside <code>docFromText</code>, the document tokenizer’s <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/checkpoint.py#L138"><code>tensorize</code> method is called</a>, and you pass to it the documents which are still strings:</p>
<div class="sourceCode" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> bsize:</span>
<span id="cb47-2">    text_batches, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.doc_tokenizer.tensorize(</span>
<span id="cb47-3">        docs, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>bsize</span>
<span id="cb47-4">    )</span></code></pre></div>
<p>And then <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/doc_tokenization.py#L48">inside <code>DocTokenizer.tensorize</code></a>, you first convert the text into tokens. And then you pass those tokens into the <code>_sort_by_length</code> helper method:</p>
<div class="sourceCode" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> tensorize(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, batch_text, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb48-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(batch_text) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>], (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(batch_text))</span>
<span id="cb48-3"></span>
<span id="cb48-4">    obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tok(batch_text, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb48-5">                    return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).to(DEVICE)</span>
<span id="cb48-6"></span>
<span id="cb48-7">    ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.D_marker_token_id)</span>
<span id="cb48-8">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb48-9"></span>
<span id="cb48-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> bsize:</span>
<span id="cb48-11">        ids, mask, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _sort_by_length(ids, mask, bsize)</span>
<span id="cb48-12">        batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _split_into_batches(ids, mask, bsize)</span>
<span id="cb48-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> batches, reverse_indices</span>
<span id="cb48-14"></span>
<span id="cb48-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ids, mask</span></code></pre></div>
<p>And finally, <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/modeling/tokenization/utils.py#L40">inside the <code>_sort_by_length</code> method</a>, it sums the mask in the last dimension, which is the sequence length dimension. Then it sorts it, grabs those indices, and returns the tokens of the passages in order. Using those indices:</p>
<div class="sourceCode" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _sort_by_length(ids, mask, bsize):</span>
<span id="cb49-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> ids.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> bsize:</span>
<span id="cb49-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ids, mask, torch.arange(ids.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb49-4"></span>
<span id="cb49-5">    indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).sort().indices</span>
<span id="cb49-6">    reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> indices.sort().indices</span>
<span id="cb49-7"></span>
<span id="cb49-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ids[indices], mask[indices], reverse_indices</span></code></pre></div>
<p>They’re summing the mask across the sequence length dimension (<code>mask.sum(-1)</code>). The mask contains 1s where you have non-padding tokens and 0s where you have padding tokens. So the sum of the mask across a sequence is the number of non-padding tokens in it. Sorting by this sum sorts the sequences by non-padding token length in ascending order.</p>
<p>Let’s look at this concretely through code.</p>
</section>
<section id="sorting-documents-by-length-for-batching" class="level3">
<h3 class="anchored" data-anchor-id="sorting-documents-by-length-for-batching">Sorting Documents by Length for Batching</h3>
<p>To better understand how ColBERT sorts documents by length for batching, I’m going to walk through a toy example using the internal methods provided in the repo.</p>
<p>I’ll start by intentionally creating a list of passages of four different lengths: 40, 60, 80, and 100</p>
<div id="cell-133" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">passages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb50-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>):</span>
<span id="cb50-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb50-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>)</span>
<span id="cb50-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>)</span>
<span id="cb50-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>)</span></code></pre></div>
</div>
<p>I now pass the passages with a bat size of 32 into the Checkpoints.DocFromText method, and as a result, I get encoded documents where the bat size is 128, the maximum document length is 103, and the embedding dimension is 96 because I’m using answerai-colbert-small-v1.</p>
<div id="cell-135" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="68ad41b5-0de8-47c5-a676-37ecdf8bc6db" data-execution_count="5">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.docFromText(docs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>passages, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast() if self.activated else NullContextManager()</code></pre>
</div>
</div>
<div id="cell-136" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1414b4c2-4f36-407c-b617-2b1e08a96169" data-execution_count="6">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">res[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>torch.Size([128, 103, 96])</code></pre>
</div>
</div>
<p>If we look one level deeper, inside docfromtext it calls the docTokenizer’s tensorize method which converts the string of text into tokens</p>
<div id="cell-138" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">text_batches, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tensorize(passages, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span></code></pre></div>
</div>
<div id="cell-139" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1880f14a-a499-48af-a232-1866041700f1" data-execution_count="8">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">text_batches[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>torch.Size([32, 103])</code></pre>
</div>
</div>
<div id="cell-140" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="eb4ab00f-7d27-4e08-f589-29937da66e0c" data-execution_count="10">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">text_batches[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([ 101,    2, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037,
        1037, 1037, 1037, 1037, 1037, 1037,  102,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0], device='cuda:0')</code></pre>
</div>
</div>
<p>Taking a look at the number of tokens in each of the batch items.</p>
<div id="cell-142" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1c30db69-0973-428c-8334-db7e12190cce" data-execution_count="11">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>]:</span>
<span id="cb60-2">    obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tok(passages[i], padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb60-3">                       return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb60-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>].shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1, 102])
torch.Size([1, 82])
torch.Size([1, 62])
torch.Size([1, 42])</code></pre>
</div>
</div>
<p><code>tensorize</code> adds the <code>[Q]</code> or <code>[D]</code> token, so that’s why the first batch item only has 102 tokens, whereas after Tensorize, it has 103 tokens.</p>
<p>Before I run the rest of the sorting code, I’m going to shuffle the passages so that we can see if sorting actually takes place.</p>
<div id="cell-145" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1">random.shuffle(passages)</span></code></pre></div>
</div>
<div id="cell-146" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1d2981e8-e3eb-4cf4-9e2d-477bc4d8b4f4" data-execution_count="14">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> o <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> passages[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(o))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>80
120
80
80
160
80
160
160
120
200</code></pre>
</div>
</div>
<p>When passing all of the passages to the <code>.tok</code> method, the tokenized batch has a number of tokens equal to the largest, longest passage. All 128 passages are tokenized up to a length of 102.</p>
<div id="cell-148" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="91d5a4ed-20e5-4e49-c275-63186d596371" data-execution_count="15">
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tok(passages, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb65-2">                       return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb65-3">obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>torch.Size([128, 102])</code></pre>
</div>
</div>
<p>Here’s the step where we add the prefixed tokens for the documents, which is <code>[D]</code>.</p>
<div id="cell-150" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], ce.checkpoint.doc_tokenizer.D_marker_token_id)</span>
<span id="cb67-2">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-151" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e9702bae-2d9f-4243-d67e-6493a571f295" data-execution_count="17">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">ids.shape, mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Looking at the number of non-zero tokens in the batch (i.e., the non-padding tokens), we can see that our batch is still currently unsorted</p>
<div id="cell-153" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fbe6f4d0-b7ee-4ccb-a64a-91cba295e447" data-execution_count="18">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb70-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(63)
tensor(43)
tensor(43)
tensor(83)
tensor(43)
tensor(83)
tensor(83)
tensor(63)
tensor(103)</code></pre>
</div>
</div>
<p>Looking at the sum of the masks, we can see that the sum of masks is equal to the number of non-padding tokens in the batch item.</p>
<div id="cell-155" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="51eeac0c-0838-41fa-8d40-413f43fc1f82" data-execution_count="19">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>): <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(mask[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(63)
tensor(43)
tensor(43)
tensor(83)
tensor(43)
tensor(83)
tensor(83)
tensor(63)
tensor(103)</code></pre>
</div>
</div>
<p>Alright, here’s the main part where it sorts by length the batches.</p>
<div id="cell-157" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="54ca1995-bd89-4a39-955d-bc69bdc07331" data-execution_count="20">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">ids, mask, reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _sort_by_length(ids, mask, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span>
<span id="cb74-2">ids.shape, mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Looking at the lengths of non-zero values in ids and mask. The items are now sorted by token length in increasing order.</p>
<div id="cell-159" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="942cc8ed-af36-4b59-838a-ccf99c63b13f" data-execution_count="21">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb76-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[i]), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(mask[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)</code></pre>
</div>
</div>
<p>It then splits it into batches of 32. Note that all batches are padded up to the maximum document length.</p>
<div id="cell-161" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1">batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _split_into_batches(ids, mask, bsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>)</span></code></pre></div>
</div>
<div id="cell-162" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5388ec21-f3f5-4c15-c667-582abdd8b2f1" data-execution_count="23">
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>): <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(batches[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 103])
torch.Size([32, 103])
torch.Size([32, 103])
torch.Size([32, 103])</code></pre>
</div>
</div>
<p>Looking at the lengths of non-zero values in each batch we can see that the batches are now sorted by length of passage.</p>
<div id="cell-164" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1424fa48-3507-4b53-8f82-52ccbd0ad231" data-execution_count="24">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>):</span>
<span id="cb81-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"------ Batch </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb81-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb81-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(batches[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][j]), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(batches[i][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][j]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>------ Batch 0
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
tensor(43) tensor(43)
------ Batch 1
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
tensor(63) tensor(63)
------ Batch 2
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
tensor(83) tensor(83)
------ Batch 3
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)
tensor(103) tensor(103)</code></pre>
</div>
</div>
<p>Now let’s go a layer deeper and look at <code>_sort_by_length</code> to see how the sorting actually happens. I’ll reinstantiate the passages and shuffle them to make sure the sorting actually happens.</p>
<div id="cell-166" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1">passages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb83-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>):</span>
<span id="cb83-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb83-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>)</span>
<span id="cb83-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>)</span>
<span id="cb83-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">96</span>: passages.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>)</span></code></pre></div>
</div>
<div id="cell-167" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">random.shuffle(passages)</span></code></pre></div>
</div>
<p>I’ll also tokenize the passages and insert the prefix tokens.</p>
<div id="cell-169" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b456ac02-a0ec-4535-e5c3-35adcb8a43e7" data-execution_count="27">
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1">obj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ce.checkpoint.doc_tokenizer.tok(passages, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest'</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longest_first'</span>,</span>
<span id="cb85-2">                       return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pt'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(ce.checkpoint.doc_tokenizer.doc_maxlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb85-3">obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>].shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>torch.Size([128, 102])</code></pre>
</div>
</div>
<div id="cell-170" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1">ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_ids'</span>], ce.checkpoint.doc_tokenizer.D_marker_token_id)</span>
<span id="cb87-2">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _insert_prefix_token(obj[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'attention_mask'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-171" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d8ef3186-088e-46b3-c0fe-2fb711f56713" data-execution_count="29">
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">ids.shape, mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(torch.Size([128, 103]), torch.Size([128, 103]))</code></pre>
</div>
</div>
<p>Checking to make sure that my batch is shuffled</p>
<div id="cell-173" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9d2b5d99-f4b1-40a3-f4f4-6610581b7912" data-execution_count="30">
<div class="sourceCode cell-code" id="cb90" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb90-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[i]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(103)
tensor(103)
tensor(43)
tensor(83)
tensor(83)
tensor(103)
tensor(63)
tensor(83)
tensor(43)
tensor(43)</code></pre>
</div>
</div>
<p>To sort the batch items by length of non-padding tokens, they sum the mask across the last dimension, which is the number of tokens.</p>
<div id="cell-175" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="351136c3-ea11-4339-ddd8-297058bc2d4d" data-execution_count="31">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">mask.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>torch.Size([128, 103])</code></pre>
</div>
</div>
<p>So, what you get here is basically the number of non-padding tokens in each of the 128 items.</p>
<div id="cell-177" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9b928051-0462-461c-907c-d1f39e828bd9" data-execution_count="32">
<div class="sourceCode cell-code" id="cb94" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1">mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>tensor([103, 103,  43,  83,  83, 103,  63,  83,  43,  43, 103,  83,  43,  83,
         63,  83,  43, 103,  43,  63, 103, 103,  43,  43, 103,  63,  83, 103,
        103, 103, 103,  63,  43,  83,  63,  83,  63, 103,  43,  43,  63,  63,
        103,  43, 103,  43,  63, 103,  43,  43,  63,  63,  83,  63, 103, 103,
         83,  63, 103,  83,  43, 103, 103,  63,  83,  43, 103,  83,  83, 103,
        103,  63,  63,  83,  43,  83,  83,  43,  83,  43, 103,  83, 103,  43,
         83,  83,  43,  63,  63,  63, 103,  63,  43, 103,  83,  63,  63,  43,
         63, 103,  83,  83,  83, 103,  63,  63,  63,  83,  83,  43,  43,  63,
        103,  83,  63,  83,  43,  83,  63,  63,  43,  43,  43,  63, 103,  43,
         83,  43])</code></pre>
</div>
</div>
<p>Then they sort it and get the indices.</p>
<div id="cell-179" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c32564d4-1263-41dd-fa9e-a2e6326b6039" data-execution_count="33">
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1">indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).sort().indices</span>
<span id="cb96-2">indices</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>tensor([ 49,  22,  23, 110, 109,  32,  38,  43,  45,  48,  39,  97,  60,  92,
         65,  86,  83,  74,  79,  77, 121, 127,   2, 125, 122,   8,   9, 116,
        120,  12,  18,  16,  63,  51,  53,  98,  96,  57,  95,  25, 114,  91,
         31,  89,   6,  88,  87, 123,  71,  72,  19, 119,  34, 106,  36, 105,
         14,  50,  40,  41, 104, 118,  46, 111,  64, 107, 126,  81, 108, 115,
         84,  85,  94, 102, 101, 113, 117, 100,  15,  33,  35,  26,  52,  56,
         59,  13,  11,   7,  67,  76,  78,  75,   3,  73,   4,  68,  17,   1,
         20,  21,  24, 124, 112,  27,  28,  29,  10,   5,  93,  80,  82,  70,
         69,  66,   0,  90,  62,  61,  30,  58,  55,  54,  99,  47,  44, 103,
         42,  37])</code></pre>
</div>
</div>
<div id="cell-180" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="110c7559-8e92-4cda-b255-2b6360b377d5" data-execution_count="37">
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">mask[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">49</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(), mask[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">37</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>(tensor(43), tensor(103))</code></pre>
</div>
</div>
<p>If we look at the first index, the corresponding mask sum is 43, which is the smallest non-padding token length. And if we look at the last index, the sum of the mask is 103, which is the largest.</p>
<p>And then they sort these indices and then get the indices of that sort</p>
<div id="cell-183" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4d611ab4-29ea-4cbd-e22a-4833bcd1c8b5" data-execution_count="38">
<div class="sourceCode cell-code" id="cb100" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1">reverse_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> indices.sort().indices</span>
<span id="cb100-2">reverse_indices</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor([114,  97,  22,  92,  94, 107,  44,  87,  25,  26, 106,  86,  29,  85,
         56,  78,  31,  96,  30,  50,  98,  99,   1,   2, 100,  39,  81, 103,
        104, 105, 118,  42,   5,  79,  52,  80,  54, 127,   6,  10,  58,  59,
        126,   7, 124,   8,  62, 123,   9,   0,  57,  33,  82,  34, 121, 120,
         83,  37, 119,  84,  12, 117, 116,  32,  64,  14, 113,  88,  95, 112,
        111,  48,  49,  93,  17,  91,  89,  19,  90,  18, 109,  67, 110,  16,
         70,  71,  15,  46,  45,  43, 115,  41,  13, 108,  72,  38,  36,  11,
         35, 122,  77,  74,  73, 125,  60,  55,  53,  65,  68,   4,   3,  63,
        102,  75,  40,  69,  27,  76,  61,  51,  28,  20,  24,  47, 101,  23,
         66,  21])</code></pre>
</div>
</div>
<div id="cell-184" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="8b07ff83-1783-4921-e11e-131109581342" data-execution_count="39">
<div class="sourceCode cell-code" id="cb102" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1">indices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">114</span>], indices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">97</span>], indices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">22</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(tensor(0), tensor(1), tensor(2))</code></pre>
</div>
</div>
<p>Looking at the first three values of <code>reverse_indices</code>: the reverse indices’ first value corresponds to the original index of 0, the reverse indices’ second value corresponds to the original index of 1, and the reverse indices’ third value corresponds to the original index of 2.</p>
<p>Finally, using <code>indices</code> to index into <code>ids</code></p>
<div id="cell-187" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2208b96c-fa28-4691-82b7-da4f0c41477a" data-execution_count="40">
<div class="sourceCode cell-code" id="cb104" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1">ids[indices]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([[ 101,    2, 1037,  ...,    0,    0,    0],
        [ 101,    2, 1037,  ...,    0,    0,    0],
        [ 101,    2, 1037,  ...,    0,    0,    0],
        ...,
        [ 101,    2, 1037,  ..., 1037, 1037,  102],
        [ 101,    2, 1037,  ..., 1037, 1037,  102],
        [ 101,    2, 1037,  ..., 1037, 1037,  102]])</code></pre>
</div>
</div>
<p>We can see that <code>ids[indices]</code> is sorted.</p>
<div id="cell-189" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5f05b8f3-8f3a-4a69-fa8a-0b19dfbf5175" data-execution_count="43">
<div class="sourceCode cell-code" id="cb106" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>):</span>
<span id="cb106-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb106-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(torch.count_nonzero(ids[indices][i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>j]))</span>
<span id="cb106-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(43)
tensor(43)
tensor(43)
tensor(43)
tensor(43)
------------------------------
tensor(63)
tensor(63)
tensor(63)
tensor(63)
tensor(63)
------------------------------
tensor(83)
tensor(83)
tensor(83)
tensor(83)
tensor(83)
------------------------------
tensor(103)
tensor(103)
tensor(103)
tensor(103)
tensor(103)
------------------------------</code></pre>
</div>
</div>
<p>And that is what they mean by “sorting documents by length and feeding the batches with documents of comparable length to the encoder”.</p>
<p>So that’s how queries and documents are encoded. We can now look at the interaction between the two at query time, which is the MaxSim operator.</p>
</section>
</section>
<section id="maxsim" class="level2">
<h2 class="anchored" data-anchor-id="maxsim">MaxSim</h2>
<blockquote class="blockquote">
<p>Using Eq and Ed , ColBERT computes the relevance score between q and d via late interaction, which we define as a summation of maximum similarity (MaxSim) operators. In particular, we find the maximum cosine similarity of each v ∈ Eq with vectors in Ed , and combine the outputs via summation.</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?S_%7Bq,d%7D%20:=%20%5Csum_%7Bi%20%5Cin%20%5B%5C%7CE_q%5C%7C%5D%7D%20%5Cmax_%7Bj%20%5Cin%20%5B%5C%7CE_d%5C%7C%5D%7D%20E_%7Bqi%7D%20%5Ccdot%20E_%7Bdj%7D%5ET"></p>
<p>Looking at the equation—we iterate through the queries, for each query we iterate through the document tokens and calculate the cosine similarity. we keep the maximum and sum it to S. Note that cosine similarity can be implemented as dot product because the embeddings are normalized. Another way to put it, taken from <a href="https://www.mixedbread.com/blog/maxsim-cpu#:~:text=For%20each%20candidate%20document%2C%20MaxSim%20iterates%20through%20every%20token%20within%20the%20query%2C%20and%20compares%20its%20similarity%20to%20every%20token%20within%20the%20document%2C%20before%20keeping%20the%20maximum%20value%20for%20each%20query%20token%20(hence%20the%20Max)%20and%20summing%20them%20up%20to%20produce%20a%20document%2Dlevel%20score.">Ben Clavie’s recent maxsim-cpu release blog post</a>:</p>
<blockquote class="blockquote">
<p>For each candidate document, MaxSim iterates through every token within the query, and compares its similarity to every token within the document, before keeping the maximum value for each query token (hence the Max) and summing them up to produce a document-level score.</p>
</blockquote>
<p>Beautiful.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="7.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Example of a MaxSim calculation between a query and a document"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/7.png" class="img-fluid figure-img" alt="Example of a MaxSim calculation between a query and a document"></a></p>
<figcaption>Example of a MaxSim calculation between a query and a document</figcaption>
</figure>
</div>
<p>Here is an example of the summation of MaxSim between query and document token embeddings. In this example, we have four query tokens and eight document tokens. For the first query token q1, the highest cosine similarity is with the fifth document token d5. d1 has the maximum cosine similarity for that q2, d2 for q3, and d8 for q4. Adding up these maximum cosine similarities, we get a final relevance score of 3.1. Since there are four tokens, the maximum possible MaxSim value is 4.0.</p>
<blockquote class="blockquote">
<p>this interaction mechanism softly searches for each query term tq —in a manner that reflects its context in the query—against the document’s embeddings, quantifying the strength of the “match” via the largest similarity score between tq and a document term td . Given these term scores, it then estimates the document relevance by summing the matching evidence across all query terms.</p>
</blockquote>
<p>The query tokens have passed through a transformer model and as such have passed through an attention mechanism so that all tokens attend to all other tokens. So the query itself now has interdependent relationships across tokens. When we’re searching for one token and looking to find the closest document, we’re not just looking to find the closest document to that token in isolation, we’re trying to find the closest document to that token within the context of the entire query. Some contextualized query token embeddings will find strong matches in certain documents, but what we’re looking for is the document for which the total maximum similarity for all query tokens is the largest. You can imagine that as a query gets very long, and the words in the query drift farther apart in meaning, the MaxSim values (before summation) for a document will have high variance.</p>
<blockquote class="blockquote">
<p>more sophisticated matching is possible with other choices such as deep convolution and attention layers (i.e., as in typical interaction-focused models),</p>
</blockquote>
<p>This reminds me of the <a href="https://arxiv.org/abs/2502.05364">Hypencoder paper</a> where they use a neural net for each query that takes as input a document embeddings and outputs a scalar relevance score. This is motivated by the fact that inner product (which is what cosine similarity is) is a linear operation and can thus only linearly separate two groups of vectors (such as embeddings). When your embedding dimension is much smaller than the number of vectors that you have, you can’t separate two groups linearly. In our case, our embedding dimension may be 96 and the number of vectors could be in the millions. Mathematically, you cannot linearly separate such a high number of vectors when they’re in a relatively low-dimensional space.</p>
<p>So you need a complex function because a line doesn’t work, and anytime you need a complex function where it’s more squiggly than a line, a neural net is a good choice!</p>
<p>However, the simplicity of MaxSim has two benefits:</p>
<blockquote class="blockquote">
<p>First, it stands out as a particularly cheap interaction mechanism, as we examine its FLOPs in §4.2. Second, and more importantly, it is amenable to highly-efficient pruning for top-k retrieval, as we evaluate in §4.3. This enables using vector-similarity algorithms for skipping documents without materializing the full interaction matrix or even considering each document in isolation. Other cheap choices (e.g., a summation of average similarity scores, instead of maximum) are possible; however, many are less amenable to pruning.</p>
</blockquote>
<p>ColBERT’s MaxSim mechanism enables efficient pruning: document tokens are clustered by similarity in vector indexes. At query time, each query token searches only the nearest clusters, skipping irrelevant documents. The “maximum” aggregation makes this possible—you only need the best matches, not exhaustive comparison across all documents.</p>
<p>We’re going to take a look at this paragraph, and then we’re going to look at the code that corresponds to it. Note that this is for a single query, which is what I’m going to focus on.</p>
<blockquote class="blockquote">
<p>Given a query q, we compute its bag of contextualized embeddings Eq (Equation 1) and, concurrently, gather the document representations into a 3-dimensional tensor D consisting of k document matrices. We pad the k documents to their maximum length to facilitate batched operations, and move the tensor D to the GPU’s memory. On the GPU, <mark>we compute a batch dot-product of Eq and D</mark>, possibly over multiple mini-batches. e output materializes a 3-dimensional tensor that is a collection of cross-match matrices between q and each document. To compute the score of each document, <mark>we reduce its matrix across document terms via a max-pool (i.e., representing an exhaustive implementation of our MaxSim computation) and reduce across query terms via a summation.</mark> Finally, we sort the k documents by their total scores.</p>
</blockquote>
<p>So let’s first look at the higher level class which is the <code>IndexScorer</code>. <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/search/index_storage.py#L188C9-L189C75">In <code>score_pids</code></a>, if the query size is 1 (which it is in our case), it’s going to pass the query and the documents to <code>colbert_score_packed</code>.</p>
<div class="sourceCode" id="cb108" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> Q.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb108-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> colbert_score_packed(Q, D_packed, D_mask, config), pids</span></code></pre></div>
<p>Inside <code>colbert_score_packed</code>, it removes the unit batch axis of the queries, and makes sure that q and d both have two dimensions. Then it performs the dot product between the two, and we can do this instead of explicitly calling cosine similarity because q and d are both normalized embeddings. The dot product results in a <code>scores</code> tensor that has size <code>number of document tokens x number of query tokens</code>. It then passes these scores into the <code>StridedTensor</code> and it gets back <code>scores_padded</code> and <code>scores_mask</code> which are then passed to <code>colbert_score_reduce</code>.</p>
<div class="sourceCode" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> colbert_score_packed(Q, D_packed, D_lengths, config<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ColBERTConfig()):</span>
<span id="cb109-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb109-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Works with a single query only.</span></span>
<span id="cb109-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb109-5"></span>
<span id="cb109-6">    use_gpu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> config.total_visible_gpus <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb109-7"></span>
<span id="cb109-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> use_gpu:</span>
<span id="cb109-9">        Q, D_packed, D_lengths <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q.cuda(), D_packed.cuda(), D_lengths.cuda()</span>
<span id="cb109-10"></span>
<span id="cb109-11">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q.squeeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># removes the unit batch axis</span></span>
<span id="cb109-12"></span>
<span id="cb109-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> Q.dim() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, Q.size()                     <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># num query tokens x emb dim</span></span>
<span id="cb109-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> D_packed.dim() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, D_packed.size()       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># num doc tokens   x emb dim</span></span>
<span id="cb109-15"></span>
<span id="cb109-16">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D_packed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Q.to(dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>D_packed.dtype).T  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># num doc tokens x num query tokens</span></span>
<span id="cb109-17"></span>
<span id="cb109-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> use_gpu <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> config.interaction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"flipr"</span>:</span>
<span id="cb109-19">        scores_padded, scores_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StridedTensor(scores, D_lengths, use_gpu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>use_gpu).as_padded_tensor()</span>
<span id="cb109-20"></span>
<span id="cb109-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> colbert_score_reduce(scores_padded, scores_mask, config)</span>
<span id="cb109-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb109-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ColBERT.segmented_maxsim(scores, D_lengths)</span></code></pre></div>
<p><code>scores_padded</code> has shape <code>number of documents x maximum number of tokens in the documents x number of query tokens</code>. So if we have three documents, a maximum of 13 document tokens, and 32 query tokens, <code>scores_padded</code> has shape 3 x 13 x 32.</p>
<p>Finally, <code>colbert_score_reduce</code> is called which takes the maximum of <code>scores_padded</code> across the second dimension (number of document tokens) to leave us with one score for each query token per document.</p>
<div class="sourceCode" id="cb110" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> colbert_score_reduce(scores_padded, D_mask, config: ColBERTConfig):</span>
<span id="cb110-2">    D_padding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>D_mask.view(scores_padded.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), scores_padded.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>()</span>
<span id="cb110-3">    scores_padded[D_padding] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9999</span></span>
<span id="cb110-4">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores_padded.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).values</span>
<span id="cb110-5"></span>
<span id="cb110-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># flipr code removed for brevity</span></span>
<span id="cb110-7"></span>
<span id="cb110-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> scores.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p>Taking <code>scores.sum(-1)</code>, the summation across the query token dimension, leaves us with one score per document, our desired result.</p>
<blockquote class="blockquote">
<p>Relative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in fact, its cost is dominated by the cost of gathering and transferring the pre-computed embeddings. To illustrate, ranking k documents via typical BERT rankers requires feeding BERT k different inputs each of length l = |q| + |di | for query q and documents di , where attention has quadratic cost in the length of the sequence. In contrast, ColBERT feeds BERT only a single, much shorter sequence of length l = |q|. Consequently, ColBERT is not only cheaper, it also scales much better with k as we examine in §4.2.</p>
</blockquote>
<p>So, what is involved in that cost of gathering and transferring the pre-computed embeddings? We will look at what they say about offline indexing next.</p>
</section>
<section id="offline-indexing" class="level2">
<h2 class="anchored" data-anchor-id="offline-indexing">Offline Indexing</h2>
<blockquote class="blockquote">
<p>Instead of applying MaxSim between one of the query embeddings and all of one document’s embeddings, we can use fast vector-similarity data structures to efficiently conduct this search between the query embedding and all document embeddings across the full collection. For this, we employ an off-the-shelf library for large-scale vector-similarity search, namely faiss [15] from Facebook. In particular, at the end of offline indexing (§3.4), we maintain a mapping from each embedding to its document of origin and then index all document embeddings into faiss.</p>
</blockquote>
<p>The current implementation in the repo uses a more efficient indexing system, the PLAID index, as opposed to what is written here (indexing “all document embeddings into faiss” and “mantain a mapping from each embeddings to its document of origin”). Instead, the PLAID index uses residual compression with centroids, maintains an Inverted File (IVF) structure that maps centroids to passage IDs, and stores embeddings as compressed residuals relative to centroids.</p>
<p>There’s a lot to unpack in the following section:</p>
<blockquote class="blockquote">
<p>Subsequently, when serving queries, we use a two-stage procedure to retrieve the top-k documents from the entire collection. Both stages rely on ColBERT’s scoring: the first is an approximate stage aimed at filtering while the second is a refinement stage. For the first stage, we concurrently issue Nq vector-similarity queries (corresponding to each of the embeddings in Eq ) onto our faiss index. This retrieves the top-k’ (e.g., k’ = k/2) matches for that vector over all document embeddings. We map each of those to its document of origin, producing Nq × k’ document IDs, only K ≤ Nq × k’ of which are unique. These K documents likely contain one or more embeddings that are highly similar to the query embeddings. For the second stage, we refine this set by exhaustively re-ranking only those K documents in the usual manner described in §3.5. In our faiss-based implementation, we use an IVFPQ index (“inverted file with product quantization”). This index partitions the embedding space into P (e.g., P = 1000) cells based on k-means clustering and then assigns each document embedding to its nearest cell based on the selected vector-similarity metric. For serving queries, when searching for the top-k’ matches for a single query embedding, only the nearest p (e.g., p = 10) partitions are searched. To improve memory efficiency, every embedding is divided into s (e.g., s = 16) sub-vectors, each represented using one byte. Moreover, the index conducts the similarity computations in this compressed domain, leading to cheaper computations and thus faster search.</p>
</blockquote>
<ul>
<li>There are NQ query token embeddings, and for each one, we find the top K’ document IDs.</li>
<li>NQ x k’ (say 32 x 500) documents will include some duplicates, Meaning that some documents will contain document token embeddings that are close to more than one query token. Removing those duplicates will give us a K number of documents, which is less than the number of query embeddings x k’.</li>
<li>This first stage has greatly reduced the number of documents in consideration. You go from all documents in consideration, which could be tens of millions, down to just NQ x k’ or fewer documents in consideration. This K documents are then re-ranked according to the MaxSim computation across query token embeddings that we saw earlier.</li>
<li>The late interaction architecture allows for pruning, as exhibited by the IVFPQ index. That index starts by partitioning the embedding space into some number of clusters, where that number of clusters is much much less than the number of token embeddings. It assigns each document token embedding to its nearest cluster based on whatever similarity metric is being used. This reduces the number of potential candidates that are close to a query token. When we search for a document similar to a given query, you only search the nearest p partitions, and p is small, say 10. And so think about this: you started out with maybe tens of millions of documents, you’ve narrowed that down to a thousand clusters of documents, and then you’re now narrowing that even further down to only 10 clusters that are considered for a single query token. That we can index these documents offline before the interaction takes place and organize them into clusters is what allows this pruning to take place.</li>
<li>The last thing I’ll say about this section is that the compression that they’re explaining here, where they divide every embedding into sub-vectors each represented using one byte, is now replaced with the PLAID compression where they quantize the residual embeddings into n bits.</li>
</ul>
<p>So with that, we have pretty much covered all of the conceptual foundations of ColBERT. To understand the impact of those foundations, we’ll now look at the experimental evaluation section from the paper.</p>
</section>
<section id="experimental-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="experimental-evaluation">Experimental Evaluation</h2>
<p>And we’ll explore the four research questions that the ColBERT authors have put forth in this section.</p>
<ul>
<li>RQ1: In a typical re-ranking setup, how well can ColBERT bridge the existing gap between highly-efficient and highly-effective neural models?</li>
<li>RQ2: Beyond re-ranking, can ColBERT effectively support end-to-end retrieval directly from a large collection?</li>
<li>RQ3: What does each component of ColBERT (e.g., late interaction, query augmentation) contribute to its quality?</li>
<li>RQ4: What are ColBERT’s indexing-related costs in terms of offline computation and memory overhead?</li>
</ul>
<p>Some training details to prepare the ColBERT retriever: they fine-tune ColBERT models on the MS MARCO and TREC CAR datasets with a learning rate of 3e-6 and a batch size of 32. They fix the number of embeddings per query at 32, meaning that they have 32 tokens per query, and the embedding dimension is 128. The model is trained on a triple of query, positive document and negative document. ColBERT is used to produce a score for each document individually, and is optimized via pairwise softmax cross-entropy loss over the computed scores of the positive and negative document.</p>
<p>Here are two lines from the <a href="https://github.com/stanford-futuredata/ColBERT/blob/8627585ad290c21720eaa54e325e7c8c301d15f6/colbert/training/training.py#L74-L119C21"><code>train</code> function</a> where you can see the loss method, which is cross-entropy loss, and that the labels are just zeros because the document that is positive is first in the batch item (the zero-eth index):</p>
<div class="sourceCode" id="cb111" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1">...</span>
<span id="cb111-2"></span>
<span id="cb111-3">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(config.bsize, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">long</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>DEVICE)</span>
<span id="cb111-4"></span>
<span id="cb111-5">...</span>
<span id="cb111-6"></span>
<span id="cb111-7">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.CrossEntropyLoss()(scores, labels[:scores.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)])</span></code></pre></div>
<p>We’ll now dig into the results of their evaluation of the ColBERT architecture vs.&nbsp;existing methods to address the four research questions listed.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="8.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Table 1"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/8.png" class="img-fluid figure-img" alt="Table 1"></a></p>
<figcaption>Table 1</figcaption>
</figure>
</div>
<p>Here are the results for the first type of evaluation where Colbert and other architectures are used to re-rank The top 1000 results produced by BM25, which is full text search. There are three notable takeaways from this table:</p>
<ol type="1">
<li>ColBERT beats non-BERT-based models in terms of retrieval metric MRR@10 and is comparable to BERT models.</li>
<li>ColBERT is three orders of magnitude faster than the more performant BERT models and is comparable in latency to the non-BERT neural rankers.</li>
<li>Except for KNRM, ColBERT requires 11x to 48600x fewer FLOPs per query.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="9.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure 4"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/9.png" class="img-fluid figure-img" alt="Figure 4"></a></p>
<figcaption>Figure 4</figcaption>
</figure>
</div>
<p>They also compared ColBERT to BERT-base trained on retrieval. In this comparison, they increased the number of documents considered for re-ranking, calculated the FLOPs required to perform the re-ranking and then calculated the retrieval performance. The purple line at the top shows BERT-base, the green line at the bottom shows ColBERT. ColBERT for each value of k (number of documents reranked) requires fewer FLOPs and is comparable in retrieval performance. Most importantly, ColBERT scales much better than BERT-base as the number of document candidates considered increases from 10 to 2000. ColBERT stays within the same order of magnitude for FLOPs whereas BERT-base FLOPs increase by two orders of magnitude.</p>
<p>Next, we’ll look at their full retrieval results where they retrieve the top 1000 documents from the 8.8 million document MS Marco Corpus.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="10.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Table 2"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/10.png" class="img-fluid figure-img" alt="Table 2"></a></p>
<figcaption>Table 2</figcaption>
</figure>
</div>
<p>What immediately jumps off the page here is the MRR improvement that ColBERT provides, twice that of the Anserini BM25 method, which is an excellent baseline. However, ColBERT has about 5-8 times the latency of these other methods–that could be justifiable given the increased improvement. ColBERT (end-to-end) has the best Recall across all methods.</p>
<p>Next, we’ll look at some of the ablation studies that they performed, which to me are the most exciting results.</p>
</section>
<section id="ablation-studies" class="level2">
<h2 class="anchored" data-anchor-id="ablation-studies">Ablation Studies</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="11.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure 5"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/11.png" class="img-fluid figure-img" alt="Figure 5"></a></p>
<figcaption>Figure 5</figcaption>
</figure>
</div>
<p>This figure is really interesting, and there’s a lot to unpack here. I think it serves as a really comprehensive summary of the main architectural decisions that they’ve made in this work.</p>
<p>Models A through E are used in a re-ranking setting. The first comparison we’ll look at is between model A and model D. Model A is a BERT model, and it takes the CLS token embedding representation for query and document and performs an inner product between them to calculate similarity for re-ranking, which achieves an MRR@10 of about 0.26 which is 6 points fewer than a 5-layer ColBERT model. So, fine grained, token-wise embedding interaction with ColBERT is yielding better results than a single vector interaction with BERT. This is a confirmation of the fundamental concept behind late interaction.</p>
<p>The second comparison is between model B and model D. Model B is using average similarity, and model D is using the MaxSim operator. Model D again has about a 6 point increase in MRR. This validates the second fundamental concept behind late interaction: the MaxSim operator.</p>
<p>The third comparison is between Model C and Model D. In Model C, the query is <strong>not</strong> padded to 32 tokens with <code>[MASK]</code> tokens. Model D uses query augmentation (it pads to 32 with <code>[MASK]</code> tokens) and has a 2 point increase in MRR showing that these <code>[MASK]</code> tokens, which carry semantic meaning in embedding space, improve the model’s ability to find relevant documents given a query.</p>
<p>The final comparison is between Model E and Model F. Model E is ColBERT used as a re-ranker for the top 1000 documents retrieved by full-text search. Model F is ColBERT used for end-to-end retrieval using a vector similarity index to cluster documents before retrieval. Using ColBERT end-to-end gives another boost to performance.</p>
<p>Something to keep in mind is that BERT requires you to pass in the query and document embedding one pair at a time. For one query, you have to do a thousand forward passes if you have a thousand documents that you want to compare it to. Whereas ColBERT, because of late interaction, can utilize vector similarity indexes because documents are indexed offline, and the interaction calculation is much quicker because you are considering fewer candidate documents—the documents that are close to the query token embeddings via the clusters created by the indexing process.</p>
</section>
<section id="indexing-throughpout-footprint" class="level2">
<h2 class="anchored" data-anchor-id="indexing-throughpout-footprint">Indexing Throughpout &amp; Footprint</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="12.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure 6"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/12.png" class="img-fluid figure-img" alt="Figure 6"></a></p>
<figcaption>Figure 6</figcaption>
</figure>
</div>
<p>I’ll start by showing figure 6, where it shows that on top of basic ColBERT indexing, adding these optimizations increases the throughput, which means it increases the number of documents that are processed each minute. The two that I’ll highlight here is that length-based bucketing, which we saw in detail, and per-batch maximum sequence length, where they pad all items in the batch to the maximum document length, both improve the throughput.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="13.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Table 4"><img src="https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/13.png" class="img-fluid figure-img" alt="Table 4"></a></p>
<figcaption>Table 4</figcaption>
</figure>
</div>
<p>This table shows the space footprint and MRR@10 for different settings, dimensions, and bytes per dimension. The most space-effcient setting, re-ranking with cosine similarity with 24-dimensional vectors stored as 2-byte floats, which takes up 27 GB, is only 1% worse in MRR@10 than the most space-consuming one which takes up 286 GB.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The body of the ColBERTv1 paper is only about 9 pages long, but it is incredibly information dense. What I thought would be a 1-day foray turned into a 6-day deep dive. I found it helpful to interleave twitter conversations with core concepts from the paper, as casual conversations are often more accessible, and equally impressive as formal work.</p>
<p>As a new canonical ColBERT maintainer I wanted to ground myself in the first principles of late interaction. There are three key elements involved:</p>
<ol type="1">
<li>Independent encoding of queries and documents.</li>
<li>Offline document indexing.</li>
<li>the MaxSim operation.</li>
</ol>
<p>Encoding queries and documents separately allows for offline document indexing, and delays the interaction to the end of the architecture. Offline indexing and MaxSim both unlock pruning in their own ways. Vector-similarity indexes, through clustering, eliminate low-relevance documents from consideration before the interaction takes place. MaxSim eliminates low-relevance tokens during the interaction.</p>
<p>MaxSim is further enhanced by query augmentation, as meaningful <code>[MASK]</code> tokens are introduced in the query to improve the chance of matching relevant terms in the document.</p>
<p>That the most space-efficient setting is only 1% less performant than the most space-consuming setting foreshadows the compression opportunities realized in the PLAID paper.</p>
<p>With these foundations reinforced, I’ll revisit the ColBERTv2 and PLAID papers next, and will continue to concretely witness the concepts at play in the repo’s codebase.</p>


</section>

 ]]></description>
  <category>ColBERT</category>
  <category>information retrieval</category>
  <guid>https://vishalbakshi.github.io/blog/posts/2025-07-16-ColBERTv1/</guid>
  <pubDate>Wed, 16 Jul 2025 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
