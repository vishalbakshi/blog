

[Feb 16, 19:02:25] #> Note: Output directory /home/ColBERT/experiments/notebook/indexes/Genomics.2bits already exists


[Feb 16, 19:02:25] #> Will delete 166 files already at /home/ColBERT/experiments/notebook/indexes/Genomics.2bits in 20 seconds...
#> Starting...
nranks = 1 	 num_gpus = 1 	 device=0
{
    "query_token_id": "[unused0]",
    "doc_token_id": "[unused1]",
    "query_token": "[Q]",
    "doc_token": "[D]",
    "ncells": null,
    "centroid_score_threshold": null,
    "ndocs": null,
    "load_index_with_mmap": false,
    "index_path": null,
    "index_bsize": 64,
    "nbits": 2,
    "kmeans_niters": 4,
    "resume": false,
    "pool_factor": 1,
    "clustering_mode": "hierarchical",
    "protected_tokens": 0,
    "similarity": "cosine",
    "bsize": 64,
    "accumsteps": 1,
    "lr": 1e-5,
    "maxsteps": 15626,
    "save_every": null,
    "warmup": 781,
    "warmup_bert": null,
    "relu": false,
    "nway": 32,
    "use_ib_negatives": false,
    "reranker": false,
    "distillation_alpha": 1.0,
    "ignore_scores": false,
    "model_name": "answerdotai\/AnswerAI-ColBERTv2.5-small",
    "query_maxlen": 32,
    "attend_to_mask_tokens": false,
    "interaction": "colbert",
    "dim": 96,
    "doc_maxlen": 256,
    "mask_punctuation": true,
    "checkpoint": "answerdotai\/answerai-colbert-small-v1",
    "triples": "\/home\/bclavie\/colbertv2.5_en\/data\/msmarco\/triplets.jsonl",
    "collection": [
        "list with 2000000 elements starting with...",
        [
            "Deconstructing a Disease: RAR, Its Fusion Partners, and Their\r\nRoles  in the Pathogenesis of Acute Promyelocytic Leukemia\r\n",
            "By\r\nAri Melnick and\r\nJonathan D. Licht\r\n",
            "From the Derald H.\u00a0Ruttenberg Cancer Center and Department of\r\nMedicine, Mount Sinai School of Medicine, New York, NY.\r\n"
        ]
    ],
    "queries": "\/home\/bclavie\/colbertv2.5_en\/data\/msmarco\/queries.tsv",
    "index_name": "Genomics.2bits",
    "overwrite": false,
    "root": "\/home\/ColBERT\/experiments",
    "experiment": "notebook",
    "index_root": null,
    "name": "2025-02\/16\/19.02.07",
    "rank": 0,
    "nranks": 1,
    "amp": true,
    "gpus": 1,
    "avoid_fork_if_possible": false
}
[Feb 16, 19:06:10] [0] 		 # of sampled PIDs = 247871 	 sampled_pids[:3] = [873715, 1536552, 21326]
Filename: /home/ColBERT/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102   1966.3 MiB   1966.3 MiB           1           @profile
   103                                                 def _sample_pids_profiled():
   104   1997.3 MiB     31.0 MiB           1               return self._sample_pids()


[Feb 16, 19:08:31] [0] 		 #> Encoding 247871 passages..
Filename: /home/ColBERT/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   146   1997.3 MiB   1997.3 MiB           1           @profile
   147                                                 def _encode_passages_profiled(*args, **kwargs):
   148   6689.6 MiB   4692.3 MiB           1               return self.encoder.encode_passages(*args, **kwargs)


[Feb 16, 19:14:43] [0] 		 avg_doclen_est = 88.1692123413086 	 len(local_sample) = 247,871
Filename: /home/ColBERT/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   144   1997.3 MiB   1997.3 MiB           1       @profile
   145                                             def _sample_embeddings(self, sampled_pids):
   146   1997.3 MiB      0.0 MiB           2           @profile
   147   1997.3 MiB      0.0 MiB           1           def _encode_passages_profiled(*args, **kwargs):
   148   6689.6 MiB   4692.3 MiB           1               return self.encoder.encode_passages(*args, **kwargs)
   149                                                     
   150   1997.3 MiB      0.0 MiB           1           local_pids = self.collection.enumerate(rank=self.rank)
   151   1997.3 MiB      0.0 MiB     2000003           local_sample = [passage for pid, passage in local_pids if pid in sampled_pids]
   152                                         
   153   6689.6 MiB      0.0 MiB           1           local_sample_embs, doclens = _encode_passages_profiled(local_sample)
   154                                         
   155   6689.6 MiB      0.0 MiB           1           if torch.cuda.is_available():
   156   6689.6 MiB      0.0 MiB           1               if torch.distributed.is_available() and torch.distributed.is_initialized():
   157   6689.6 MiB      0.0 MiB           1                   self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   158   6781.4 MiB     91.8 MiB           1                   torch.distributed.all_reduce(self.num_sample_embs)
   159                                         
   160   6781.4 MiB      0.0 MiB           1                   avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   161   6781.4 MiB      0.0 MiB           1                   avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   162   6781.4 MiB      0.0 MiB           1                   torch.distributed.all_reduce(avg_doclen_est)
   163                                         
   164   6781.4 MiB      0.0 MiB           1                   nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   165   6781.4 MiB      0.0 MiB           1                   torch.distributed.all_reduce(nonzero_ranks)
   166                                                     else:
   167                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   168                                         
   169                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   170                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   171                                         
   172                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   173                                                 else:
   174                                                     if torch.distributed.is_available() and torch.distributed.is_initialized():
   175                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   176                                                         torch.distributed.all_reduce(self.num_sample_embs)
   177                                         
   178                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   179                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   180                                                         torch.distributed.all_reduce(avg_doclen_est)
   181                                         
   182                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   183                                                         torch.distributed.all_reduce(nonzero_ranks)
   184                                                     else:
   185                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   186                                         
   187                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   188                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   189                                         
   190                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   191                                         
   192   6781.4 MiB      0.0 MiB           1           avg_doclen_est = avg_doclen_est.item() / nonzero_ranks.item()
   193   6781.4 MiB      0.0 MiB           1           self.avg_doclen_est = avg_doclen_est
   194                                         
   195   6781.4 MiB      0.0 MiB           1           Run().print(f'avg_doclen_est = {avg_doclen_est} \t len(local_sample) = {len(local_sample):,}')
   196                                         
   197   6781.4 MiB      0.0 MiB           1           torch.save(local_sample_embs.half(), os.path.join(self.config.index_path_, f'sample.{self.rank}.pt'))
   198                                         
   199   6781.4 MiB      0.0 MiB           1           return avg_doclen_est


Filename: /home/ColBERT/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   106   1997.3 MiB   1997.3 MiB           1           @profile
   107                                                 def _sample_embeddings_profiled(pids):
   108   2779.7 MiB    782.4 MiB           1               return self._sample_embeddings(pids)


[Feb 16, 19:14:47] [0] 		 Creating 131,072 partitions.
[Feb 16, 19:14:47] [0] 		 *Estimated* 176,338,424 embeddings.
[Feb 16, 19:14:47] [0] 		 #> Saving the indexing plan to /home/ColBERT/experiments/notebook/indexes/Genomics.2bits/plan.json ..
Clustering 21804591 points in 96D to 131072 clusters, redo 1 times, 4 iterations
  Preprocessing in 0.79 s
  Iteration 0 (42.50 s, search 41.17 s): objective=877690 imbalance=4.520 nsplit=1114         Iteration 1 (88.32 s, search 86.70 s): objective=569087 imbalance=3.977 nsplit=35         Iteration 2 (135.81 s, search 133.91 s): objective=540361 imbalance=3.887 nsplit=11         Iteration 3 (184.17 s, search 182.00 s): objective=530239 imbalance=5.560 nsplit=1       Filename: /home/ColBERT/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    82   1966.3 MiB   1966.3 MiB           1       @profile
    83                                             def setup(self):
    84                                                 '''
    85                                                 Calculates and saves plan.json for the whole collection.
    86                                                 
    87                                                 plan.json { config, num_chunks, num_partitions, num_embeddings_est, avg_doclen_est}
    88                                                 num_partitions is the number of centroids to be generated.
    89                                                 '''
    90   1966.3 MiB      0.0 MiB           1           if self.config.resume:
    91                                                     if self._try_load_plan():
    92                                                         if self.verbose > 1:
    93                                                             Run().print_main(f"#> Loaded plan from {self.plan_path}:")
    94                                                             Run().print_main(f"#> num_chunks = {self.num_chunks}")
    95                                                             Run().print_main(f"#> num_partitions = {self.num_chunks}")
    96                                                             Run().print_main(f"#> num_embeddings_est = {self.num_embeddings_est}")
    97                                                             Run().print_main(f"#> avg_doclen_est = {self.avg_doclen_est}")
    98                                                         return
    99                                         
   100   1966.3 MiB      0.0 MiB           1           self.num_chunks = int(np.ceil(len(self.collection) / self.collection.get_chunksize()))
   101                                                 
   102   1966.3 MiB      0.0 MiB           2           @profile
   103   1966.3 MiB      0.0 MiB           1           def _sample_pids_profiled():
   104   1997.3 MiB     31.0 MiB           1               return self._sample_pids()
   105                                             
   106   1997.3 MiB      0.0 MiB           2           @profile
   107   1966.3 MiB      0.0 MiB           1           def _sample_embeddings_profiled(pids):
   108   2779.7 MiB    782.4 MiB           1               return self._sample_embeddings(pids)
   109                                                     
   110                                                 # Saves sampled passages and embeddings for training k-means centroids later 
   111   1997.3 MiB      0.0 MiB           1           sampled_pids = _sample_pids_profiled()
   112   2779.7 MiB      0.0 MiB           1           avg_doclen_est = _sample_embeddings_profiled(sampled_pids)
   113                                         
   114                                                 # Select the number of partitions
   115   2779.7 MiB      0.0 MiB           1           num_passages = len(self.collection)
   116   2779.7 MiB      0.0 MiB           1           self.num_embeddings_est = num_passages * avg_doclen_est
   117   2779.7 MiB      0.0 MiB           1           self.num_partitions = int(2 ** np.floor(np.log2(16 * np.sqrt(self.num_embeddings_est))))
   118                                         
   119   2779.7 MiB      0.0 MiB           1           if self.verbose > 0:
   120   2779.7 MiB      0.0 MiB           1               Run().print_main(f'Creating {self.num_partitions:,} partitions.')
   121   2779.7 MiB      0.0 MiB           1               Run().print_main(f'*Estimated* {int(self.num_embeddings_est):,} embeddings.')
   122                                         
   123   2794.7 MiB     15.0 MiB           1           self._save_plan()


[Feb 16, 19:19:14] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
[Feb 16, 19:19:15] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
[0.012, 0.012, 0.011, 0.012, 0.011, 0.012, 0.012, 0.012, 0.013, 0.011, 0.012, 0.013, 0.013, 0.012, 0.012, 0.014, 0.01, 0.013, 0.012, 0.011, 0.011, 0.012, 0.013, 0.012, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.012, 0.013, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.013, 0.012, 0.011, 0.013, 0.013, 0.012, 0.011, 0.013, 0.013, 0.012, 0.013, 0.012, 0.011, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.013, 0.013, 0.012, 0.013, 0.013, 0.013, 0.012, 0.012, 0.012, 0.013, 0.011, 0.011, 0.013, 0.012, 0.012, 0.011, 0.011, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.013, 0.012, 0.012]
[Feb 16, 19:19:16] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')
[Feb 16, 19:19:16] #> Got bucket_cutoffs = tensor([-9.4299e-03,  1.5259e-05,  9.6436e-03], device='cuda:0') and bucket_weights = tensor([-0.0170, -0.0042,  0.0044,  0.0172], device='cuda:0')
[Feb 16, 19:19:16] avg_residual = 0.0120697021484375
[Feb 16, 19:19:17] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:19:36] [0] 		 #> Saving chunk 0: 	 25,000 passages and 1,897,577 embeddings. From #0 onward.
[Feb 16, 19:19:38] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:19:58] [0] 		 #> Saving chunk 1: 	 25,000 passages and 1,877,590 embeddings. From #25,000 onward.
[Feb 16, 19:19:59] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:20:18] [0] 		 #> Saving chunk 2: 	 25,000 passages and 1,953,069 embeddings. From #50,000 onward.
[Feb 16, 19:20:20] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:20:40] [0] 		 #> Saving chunk 3: 	 25,000 passages and 2,028,875 embeddings. From #75,000 onward.
[Feb 16, 19:20:41] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:21:00] [0] 		 #> Saving chunk 4: 	 25,000 passages and 1,962,099 embeddings. From #100,000 onward.
[Feb 16, 19:21:02] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:21:21] [0] 		 #> Saving chunk 5: 	 25,000 passages and 2,017,239 embeddings. From #125,000 onward.
[Feb 16, 19:21:23] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:21:42] [0] 		 #> Saving chunk 6: 	 25,000 passages and 2,015,742 embeddings. From #150,000 onward.
[Feb 16, 19:21:44] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:22:03] [0] 		 #> Saving chunk 7: 	 25,000 passages and 1,989,287 embeddings. From #175,000 onward.
[Feb 16, 19:22:05] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:22:24] [0] 		 #> Saving chunk 8: 	 25,000 passages and 2,092,270 embeddings. From #200,000 onward.
[Feb 16, 19:22:25] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:22:45] [0] 		 #> Saving chunk 9: 	 25,000 passages and 2,127,784 embeddings. From #225,000 onward.
[Feb 16, 19:22:47] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:23:06] [0] 		 #> Saving chunk 10: 	 25,000 passages and 2,065,709 embeddings. From #250,000 onward.
[Feb 16, 19:23:08] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:23:27] [0] 		 #> Saving chunk 11: 	 25,000 passages and 2,133,460 embeddings. From #275,000 onward.
[Feb 16, 19:23:29] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:23:48] [0] 		 #> Saving chunk 12: 	 25,000 passages and 2,091,715 embeddings. From #300,000 onward.
[Feb 16, 19:23:50] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:24:09] [0] 		 #> Saving chunk 13: 	 25,000 passages and 2,178,334 embeddings. From #325,000 onward.
[Feb 16, 19:24:11] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:24:31] [0] 		 #> Saving chunk 14: 	 25,000 passages and 2,095,639 embeddings. From #350,000 onward.
[Feb 16, 19:24:32] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:24:52] [0] 		 #> Saving chunk 15: 	 25,000 passages and 2,110,115 embeddings. From #375,000 onward.
[Feb 16, 19:24:53] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:25:13] [0] 		 #> Saving chunk 16: 	 25,000 passages and 2,163,097 embeddings. From #400,000 onward.
[Feb 16, 19:25:14] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:25:34] [0] 		 #> Saving chunk 17: 	 25,000 passages and 2,163,011 embeddings. From #425,000 onward.
[Feb 16, 19:25:35] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:25:55] [0] 		 #> Saving chunk 18: 	 25,000 passages and 2,193,365 embeddings. From #450,000 onward.
[Feb 16, 19:25:56] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:26:16] [0] 		 #> Saving chunk 19: 	 25,000 passages and 2,146,291 embeddings. From #475,000 onward.
[Feb 16, 19:26:17] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:26:37] [0] 		 #> Saving chunk 20: 	 25,000 passages and 2,148,289 embeddings. From #500,000 onward.
[Feb 16, 19:26:38] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:26:58] [0] 		 #> Saving chunk 21: 	 25,000 passages and 2,169,782 embeddings. From #525,000 onward.
[Feb 16, 19:27:00] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:27:20] [0] 		 #> Saving chunk 22: 	 25,000 passages and 2,196,228 embeddings. From #550,000 onward.
[Feb 16, 19:27:21] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:27:41] [0] 		 #> Saving chunk 23: 	 25,000 passages and 2,180,247 embeddings. From #575,000 onward.
[Feb 16, 19:27:43] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:28:03] [0] 		 #> Saving chunk 24: 	 25,000 passages and 2,263,261 embeddings. From #600,000 onward.
[Feb 16, 19:28:05] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:28:25] [0] 		 #> Saving chunk 25: 	 25,000 passages and 2,041,369 embeddings. From #625,000 onward.
[Feb 16, 19:28:26] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:28:47] [0] 		 #> Saving chunk 26: 	 25,000 passages and 2,173,092 embeddings. From #650,000 onward.
[Feb 16, 19:28:48] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:29:08] [0] 		 #> Saving chunk 27: 	 25,000 passages and 2,191,100 embeddings. From #675,000 onward.
[Feb 16, 19:29:10] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:29:30] [0] 		 #> Saving chunk 28: 	 25,000 passages and 2,252,093 embeddings. From #700,000 onward.
[Feb 16, 19:29:32] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:29:52] [0] 		 #> Saving chunk 29: 	 25,000 passages and 2,143,917 embeddings. From #725,000 onward.
[Feb 16, 19:29:54] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:30:14] [0] 		 #> Saving chunk 30: 	 25,000 passages and 2,248,400 embeddings. From #750,000 onward.
[Feb 16, 19:30:15] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:30:36] [0] 		 #> Saving chunk 31: 	 25,000 passages and 2,180,884 embeddings. From #775,000 onward.
[Feb 16, 19:30:37] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:30:57] [0] 		 #> Saving chunk 32: 	 25,000 passages and 2,225,688 embeddings. From #800,000 onward.
[Feb 16, 19:30:59] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:31:19] [0] 		 #> Saving chunk 33: 	 25,000 passages and 2,255,695 embeddings. From #825,000 onward.
[Feb 16, 19:31:21] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:31:42] [0] 		 #> Saving chunk 34: 	 25,000 passages and 2,314,447 embeddings. From #850,000 onward.
[Feb 16, 19:31:44] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:32:06] [0] 		 #> Saving chunk 35: 	 25,000 passages and 2,188,810 embeddings. From #875,000 onward.
[Feb 16, 19:32:07] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:32:28] [0] 		 #> Saving chunk 36: 	 25,000 passages and 2,246,470 embeddings. From #900,000 onward.
[Feb 16, 19:32:30] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:32:50] [0] 		 #> Saving chunk 37: 	 25,000 passages and 2,226,571 embeddings. From #925,000 onward.
[Feb 16, 19:32:52] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:33:12] [0] 		 #> Saving chunk 38: 	 25,000 passages and 2,278,898 embeddings. From #950,000 onward.
[Feb 16, 19:33:14] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:33:33] [0] 		 #> Saving chunk 39: 	 25,000 passages and 2,328,273 embeddings. From #975,000 onward.
[Feb 16, 19:33:35] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:33:55] [0] 		 #> Saving chunk 40: 	 25,000 passages and 2,190,828 embeddings. From #1,000,000 onward.
[Feb 16, 19:33:56] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:34:16] [0] 		 #> Saving chunk 41: 	 25,000 passages and 2,220,059 embeddings. From #1,025,000 onward.
[Feb 16, 19:34:17] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:34:37] [0] 		 #> Saving chunk 42: 	 25,000 passages and 2,242,434 embeddings. From #1,050,000 onward.
[Feb 16, 19:34:39] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:34:58] [0] 		 #> Saving chunk 43: 	 25,000 passages and 2,209,088 embeddings. From #1,075,000 onward.
[Feb 16, 19:35:00] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:35:20] [0] 		 #> Saving chunk 44: 	 25,000 passages and 2,264,340 embeddings. From #1,100,000 onward.
[Feb 16, 19:35:21] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:35:41] [0] 		 #> Saving chunk 45: 	 25,000 passages and 2,192,322 embeddings. From #1,125,000 onward.
[Feb 16, 19:35:43] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:36:02] [0] 		 #> Saving chunk 46: 	 25,000 passages and 2,272,434 embeddings. From #1,150,000 onward.
[Feb 16, 19:36:04] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:36:24] [0] 		 #> Saving chunk 47: 	 25,000 passages and 2,270,559 embeddings. From #1,175,000 onward.
[Feb 16, 19:36:25] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:36:45] [0] 		 #> Saving chunk 48: 	 25,000 passages and 2,309,151 embeddings. From #1,200,000 onward.
[Feb 16, 19:36:47] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:37:06] [0] 		 #> Saving chunk 49: 	 25,000 passages and 2,269,163 embeddings. From #1,225,000 onward.
[Feb 16, 19:37:08] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:37:27] [0] 		 #> Saving chunk 50: 	 25,000 passages and 2,348,837 embeddings. From #1,250,000 onward.
[Feb 16, 19:37:29] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:37:49] [0] 		 #> Saving chunk 51: 	 25,000 passages and 2,260,695 embeddings. From #1,275,000 onward.
[Feb 16, 19:37:50] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:38:10] [0] 		 #> Saving chunk 52: 	 25,000 passages and 2,228,384 embeddings. From #1,300,000 onward.
[Feb 16, 19:38:12] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:38:31] [0] 		 #> Saving chunk 53: 	 25,000 passages and 2,243,473 embeddings. From #1,325,000 onward.
[Feb 16, 19:38:33] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:38:53] [0] 		 #> Saving chunk 54: 	 25,000 passages and 2,296,392 embeddings. From #1,350,000 onward.
[Feb 16, 19:38:54] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:39:14] [0] 		 #> Saving chunk 55: 	 25,000 passages and 2,207,825 embeddings. From #1,375,000 onward.
[Feb 16, 19:39:16] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:39:35] [0] 		 #> Saving chunk 56: 	 25,000 passages and 2,232,994 embeddings. From #1,400,000 onward.
[Feb 16, 19:39:37] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:39:57] [0] 		 #> Saving chunk 57: 	 25,000 passages and 2,332,999 embeddings. From #1,425,000 onward.
[Feb 16, 19:39:58] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:40:18] [0] 		 #> Saving chunk 58: 	 25,000 passages and 2,291,078 embeddings. From #1,450,000 onward.
[Feb 16, 19:40:20] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:40:39] [0] 		 #> Saving chunk 59: 	 25,000 passages and 2,276,751 embeddings. From #1,475,000 onward.
[Feb 16, 19:40:41] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:41:00] [0] 		 #> Saving chunk 60: 	 25,000 passages and 2,337,733 embeddings. From #1,500,000 onward.
[Feb 16, 19:41:02] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:41:22] [0] 		 #> Saving chunk 61: 	 25,000 passages and 2,323,894 embeddings. From #1,525,000 onward.
[Feb 16, 19:41:24] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:41:43] [0] 		 #> Saving chunk 62: 	 25,000 passages and 2,349,523 embeddings. From #1,550,000 onward.
[Feb 16, 19:41:45] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:42:05] [0] 		 #> Saving chunk 63: 	 25,000 passages and 2,356,277 embeddings. From #1,575,000 onward.
[Feb 16, 19:42:06] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:42:26] [0] 		 #> Saving chunk 64: 	 25,000 passages and 2,334,527 embeddings. From #1,600,000 onward.
[Feb 16, 19:42:28] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:42:49] [0] 		 #> Saving chunk 65: 	 25,000 passages and 2,310,049 embeddings. From #1,625,000 onward.
[Feb 16, 19:42:50] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:43:15] [0] 		 #> Saving chunk 66: 	 25,000 passages and 2,277,458 embeddings. From #1,650,000 onward.
[Feb 16, 19:43:17] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:43:41] [0] 		 #> Saving chunk 67: 	 25,000 passages and 2,327,870 embeddings. From #1,675,000 onward.
[Feb 16, 19:43:43] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:44:07] [0] 		 #> Saving chunk 68: 	 25,000 passages and 2,337,675 embeddings. From #1,700,000 onward.
[Feb 16, 19:44:09] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:44:33] [0] 		 #> Saving chunk 69: 	 25,000 passages and 2,287,788 embeddings. From #1,725,000 onward.
[Feb 16, 19:44:34] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:44:59] [0] 		 #> Saving chunk 70: 	 25,000 passages and 2,294,792 embeddings. From #1,750,000 onward.
[Feb 16, 19:45:00] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:45:24] [0] 		 #> Saving chunk 71: 	 25,000 passages and 2,282,529 embeddings. From #1,775,000 onward.
[Feb 16, 19:45:26] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:45:50] [0] 		 #> Saving chunk 72: 	 25,000 passages and 2,256,940 embeddings. From #1,800,000 onward.
[Feb 16, 19:45:51] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:46:16] [0] 		 #> Saving chunk 73: 	 25,000 passages and 2,237,733 embeddings. From #1,825,000 onward.
[Feb 16, 19:46:17] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:46:42] [0] 		 #> Saving chunk 74: 	 25,000 passages and 2,266,237 embeddings. From #1,850,000 onward.
[Feb 16, 19:46:43] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:47:07] [0] 		 #> Saving chunk 75: 	 25,000 passages and 2,261,814 embeddings. From #1,875,000 onward.
[Feb 16, 19:47:09] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:47:33] [0] 		 #> Saving chunk 76: 	 25,000 passages and 2,301,404 embeddings. From #1,900,000 onward.
[Feb 16, 19:47:34] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:47:59] [0] 		 #> Saving chunk 77: 	 25,000 passages and 2,330,090 embeddings. From #1,925,000 onward.
[Feb 16, 19:48:00] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:48:25] [0] 		 #> Saving chunk 78: 	 25,000 passages and 2,296,688 embeddings. From #1,950,000 onward.
[Feb 16, 19:48:27] [0] 		 #> Encoding 25000 passages..
[Feb 16, 19:48:51] [0] 		 #> Saving chunk 79: 	 25,000 passages and 2,218,361 embeddings. From #1,975,000 onward.
[Feb 16, 19:48:52] [0] 		 #> Checking all files were saved...
[Feb 16, 19:48:52] [0] 		 Found all files!
[Feb 16, 19:48:52] [0] 		 #> Building IVF...
[Feb 16, 19:48:53] [0] 		 #> Loading codes...
[Feb 16, 19:48:53] [0] 		 Sorting codes...
[Feb 16, 19:48:56] [0] 		 Getting unique codes...
[Feb 16, 19:48:56] #> Optimizing IVF to store map from centroids to list of pids..
[Feb 16, 19:48:56] #> Building the emb2pid mapping..
[Feb 16, 19:49:16] len(emb2pid) = 176704970
[Feb 16, 19:49:30] #> Saved optimized IVF to /home/ColBERT/experiments/notebook/indexes/Genomics.2bits/ivf.pid.pt
[Feb 16, 19:49:30] [0] 		 #> Saving the indexing metadata to /home/ColBERT/experiments/notebook/indexes/Genomics.2bits/metadata.json ..
Filename: /home/ColBERT/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    62   1966.3 MiB   1966.3 MiB           1       @profile
    63                                             def run(self, shared_lists):
    64   1966.3 MiB      0.0 MiB           1           with torch.inference_mode():
    65   2788.5 MiB    822.2 MiB           1               self.setup() # Computes and saves plan for whole collection
    66   2788.5 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    67   2788.5 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    68                                         
    69   2788.5 MiB      0.0 MiB           1               if not self.config.resume or not self.saver.try_load_codec():
    70   2921.9 MiB    133.5 MiB           1                   self.train(shared_lists) # Trains centroids from selected passages
    71   2921.9 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    72   2921.9 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    73                                         
    74   3880.0 MiB    958.0 MiB           1               self.index() # Encodes and saves all tokens into residuals
    75   3880.0 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    76   3880.0 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    77                                         
    78   4370.2 MiB    490.2 MiB           1               self.finalize() # Builds metadata and centroid to passage mapping
    79   4370.2 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    80   4370.2 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')


Filename: /home/ColBERT/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    31   1829.9 MiB   1829.9 MiB           1   @profile
    32                                         def encode(config, collection, shared_lists, shared_queues, verbose: int = 3):
    33   1966.3 MiB    136.4 MiB           1       encoder = CollectionIndexer(config=config, collection=collection, verbose=verbose)
    34   4370.2 MiB   2403.9 MiB           1       encoder.run(shared_lists)



#> Joined...
Filename: /home/ColBERT/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    28   4000.6 MiB   4000.6 MiB           1       @profile
    29                                             def launch(self, custom_config, *args):
    30   4000.6 MiB      0.0 MiB           1           assert isinstance(custom_config, BaseConfig)
    31   4000.6 MiB      0.0 MiB           1           assert isinstance(custom_config, RunSettings)
    32                                                 
    33   4000.6 MiB      0.0 MiB           1           return_value_queue = mp.Queue()
    34   4000.6 MiB      0.0 MiB           1           rng = random.Random(time.time())
    35   4000.6 MiB      0.0 MiB           1           port = str(12355 + rng.randint(0, 1000))  # randomize the port to avoid collision on launching several jobs.
    36   4000.6 MiB      0.0 MiB           1           all_procs = []
    37   4011.6 MiB      0.0 MiB           2           for new_rank in range(0, self.nranks):
    38   4011.6 MiB     11.0 MiB           1               new_config = type(custom_config).from_existing(custom_config, self.run_config, RunConfig(rank=new_rank))
    39                                         
    40   4011.6 MiB      0.0 MiB           1               args_ = (self.callee, port, return_value_queue, new_config, *args)
    41   4011.6 MiB      0.0 MiB           1               all_procs.append(mp.Process(target=setup_new_process, args=args_))
    42                                         
    43                                                 # Clear GPU space (e.g., after a `Searcher` on GPU-0 is deleted)
    44                                                 # TODO: Generalize this from GPU-0 only!
    45                                                 # TODO: Move this to a function. And call that function from __del__ in a class that's inherited by Searcher, Indexer, etc.
    46                                         
    47                                                 # t = torch.cuda.get_device_properties(0).total_memory
    48                                                 # r = torch.cuda.memory_reserved(0)
    49                                                 # a = torch.cuda.memory_allocated(0)
    50                                                 # f = r-a
    51                                         
    52                                                 # print_message(f"[Pre-Emptying] GPU memory check: r={r}, a={a}, f={f}")
    53                                         
    54   4011.6 MiB      0.0 MiB           1           torch.cuda.empty_cache()
    55                                         
    56                                                 # t = torch.cuda.get_device_properties(0).total_memory
    57                                                 # r = torch.cuda.memory_reserved(0)
    58                                                 # a = torch.cuda.memory_allocated(0)
    59                                                 # f = r-a
    60                                         
    61                                                 # print_message(f"[Post-Emptying] GPU memory check: r={r}, a={a}, f={f}")
    62                                         
    63   4011.6 MiB      0.0 MiB           1           print_memory_stats('MAIN')
    64                                         
    65   4877.1 MiB      0.0 MiB           2           for proc in all_procs:
    66   4011.6 MiB      0.0 MiB           1               print("#> Starting...")
    67   4877.1 MiB    865.4 MiB           1               proc.start()
    68                                         
    69   4877.1 MiB      0.0 MiB           1           print_memory_stats('MAIN')
    70                                         
    71                                                 # TODO: If the processes crash upon join, raise an exception and don't block on .get() below!
    72                                         
    73   4877.1 MiB      0.0 MiB           4           return_values = sorted([return_value_queue.get() for _ in all_procs])
    74   4877.1 MiB      0.0 MiB           4           return_values = [val for rank, val in return_values]
    75                                         
    76   4877.1 MiB      0.0 MiB           1           if not self.return_all:
    77   4877.1 MiB      0.0 MiB           1               return_values = return_values[0]
    78                                                 
    79   4877.1 MiB      0.0 MiB           2           for proc in all_procs:
    80   4877.1 MiB      0.0 MiB           1               proc.join()
    81   4877.1 MiB      0.0 MiB           1               print("#> Joined...")
    82                                         
    83   4877.1 MiB      0.0 MiB           1           print_memory_stats('MAIN')
    84                                                 
    85   4877.1 MiB      0.0 MiB           1           return return_values


Filename: /home/ColBERT/colbert/indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    85   4000.6 MiB   4000.6 MiB           1       @profile
    86                                             def __launch(self, collection):
    87   4000.6 MiB      0.0 MiB           1           launcher = Launcher(encode)
    88   4000.6 MiB      0.0 MiB           1           if self.config.nranks == 1 and self.config.avoid_fork_if_possible:
    89                                                     shared_queues = []
    90                                                     shared_lists = []
    91                                                     launcher.launch_without_fork(self.config, collection, shared_lists, shared_queues, self.verbose)
    92                                         
    93                                                     return
    94                                         
    95   4000.6 MiB      0.0 MiB           1           manager = mp.Manager()
    96   4000.6 MiB      0.0 MiB           4           shared_lists = [manager.list() for _ in range(self.config.nranks)]
    97   4000.6 MiB      0.0 MiB           4           shared_queues = [manager.Queue(maxsize=1) for _ in range(self.config.nranks)]
    98                                         
    99                                                 # Encodes collection into index using the CollectionIndexer class
   100   4877.1 MiB    876.4 MiB           1           launcher.launch(self.config, collection, shared_lists, shared_queues, self.verbose)


Filename: /home/ColBERT/colbert/indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    60   4000.6 MiB   4000.6 MiB           1       @profile
    61                                             def index(self, name, collection, overwrite=False):
    62   4000.6 MiB      0.0 MiB           1           assert overwrite in [True, False, 'reuse', 'resume', "force_silent_overwrite"]
    63                                         
    64   4000.6 MiB      0.0 MiB           1           self.configure(collection=collection, index_name=name, resume=overwrite=='resume')
    65                                                 # Note: The bsize value set here is ignored internally. Users are encouraged
    66                                                 # to supply their own batch size for indexing by using the index_bsize parameter in the ColBERTConfig.
    67   4000.6 MiB      0.0 MiB           1           self.configure(bsize=64, partitions=None)
    68                                         
    69   4000.6 MiB      0.0 MiB           1           self.index_path = self.config.index_path_
    70   4000.6 MiB      0.0 MiB           1           index_does_not_exist = (not os.path.exists(self.config.index_path_))
    71                                         
    72   4000.6 MiB      0.0 MiB           1           assert (overwrite in [True, 'reuse', 'resume', "force_silent_overwrite"]) or index_does_not_exist, self.config.index_path_
    73   4000.6 MiB      0.0 MiB           1           create_directory(self.config.index_path_)
    74                                         
    75   4000.6 MiB      0.0 MiB           1           if overwrite == 'force_silent_overwrite':
    76                                                     self.erase(force_silent=True)
    77   4000.6 MiB      0.0 MiB           1           elif overwrite is True:
    78   4000.6 MiB      0.0 MiB           1               self.erase()
    79                                         
    80   4000.6 MiB      0.0 MiB           1           if index_does_not_exist or overwrite != 'reuse':
    81   4877.1 MiB    876.4 MiB           1               self.__launch(collection)
    82                                         
    83   4877.1 MiB      0.0 MiB           1           return self.index_path


Filename: ../colbert_index_2M.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     8   4000.6 MiB   4000.6 MiB           1   @profile
     9                                         def _index(indexer, name, collection):
    10   4877.1 MiB    876.4 MiB           1       return indexer.index(name=name, collection=collection, overwrite=True)


