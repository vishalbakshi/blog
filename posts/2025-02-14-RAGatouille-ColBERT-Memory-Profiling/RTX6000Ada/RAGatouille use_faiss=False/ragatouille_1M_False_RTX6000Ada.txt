1000000
Filename: /home/RAGatouille/ragatouille/RAGPretrainedModel.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   201   3599.8 MiB   3599.8 MiB           1           @profile
   202                                                 def _process_corpus_profiled(*args, **kwargs):
   203   5040.1 MiB   1440.2 MiB           1               return self._process_corpus(*args, **kwargs)


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   131   5040.1 MiB   5040.1 MiB           1       @profile
   132                                             def __init__(self, config: ColBERTConfig) -> None:
   133   5040.1 MiB      0.0 MiB           1           super().__init__(config)
   134   5040.1 MiB      0.0 MiB           1           self.searcher: Optional[Searcher] = None




[Feb 17, 03:43:21] #> Note: Output directory .ragatouille/colbert/indexes/Genomics_index already exists


[Feb 17, 03:43:21] #> Will delete 114 files already at .ragatouille/colbert/indexes/Genomics_index in 20 seconds...
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102   5040.7 MiB   5040.7 MiB           1           @profile
   103                                                 def _sample_pids_profiled():
   104   5040.7 MiB      0.0 MiB           1               return self._sample_pids()


[Feb 17, 03:52:03] [0] 		 #> Encoding 203532 passages..
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   146   5040.7 MiB   5040.7 MiB           1           @profile
   147                                                 def _encode_passages_profiled(*args, **kwargs):
   148   8956.0 MiB   3915.3 MiB           1               return self.encoder.encode_passages(*args, **kwargs)


[Feb 17, 04:09:44] [0] 		 avg_doclen_est = 96.709716796875 	 len(local_sample) = 203,532
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   144   5040.7 MiB   5040.7 MiB           1       @profile
   145                                             def _sample_embeddings(self, sampled_pids):
   146   5040.7 MiB      0.0 MiB           2           @profile
   147   5040.7 MiB      0.0 MiB           1           def _encode_passages_profiled(*args, **kwargs):
   148   8956.0 MiB   3915.3 MiB           1               return self.encoder.encode_passages(*args, **kwargs)
   149                                                     
   150   5040.7 MiB      0.0 MiB           1           local_pids = self.collection.enumerate(rank=self.rank)
   151   5040.7 MiB      0.0 MiB     1348475           local_sample = [passage for pid, passage in local_pids if pid in sampled_pids]
   152                                         
   153   8956.0 MiB      0.0 MiB           1           local_sample_embs, doclens = _encode_passages_profiled(local_sample)
   154                                         
   155   8956.0 MiB      0.0 MiB           1           if torch.cuda.is_available():
   156   8956.0 MiB      0.0 MiB           1               if torch.distributed.is_available() and torch.distributed.is_initialized():
   157                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   158                                                         torch.distributed.all_reduce(self.num_sample_embs)
   159                                         
   160                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   161                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   162                                                         torch.distributed.all_reduce(avg_doclen_est)
   163                                         
   164                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   165                                                         torch.distributed.all_reduce(nonzero_ranks)
   166                                                     else:
   167   8956.0 MiB      0.0 MiB           1                   self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   168                                         
   169   8956.0 MiB      0.0 MiB           1                   avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   170   8956.0 MiB      0.0 MiB           1                   avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   171                                         
   172   8956.0 MiB      0.0 MiB           1                   nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   173                                                 else:
   174                                                     if torch.distributed.is_available() and torch.distributed.is_initialized():
   175                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   176                                                         torch.distributed.all_reduce(self.num_sample_embs)
   177                                         
   178                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   179                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   180                                                         torch.distributed.all_reduce(avg_doclen_est)
   181                                         
   182                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   183                                                         torch.distributed.all_reduce(nonzero_ranks)
   184                                                     else:
   185                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   186                                         
   187                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   188                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   189                                         
   190                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   191                                         
   192   8956.0 MiB      0.0 MiB           1           avg_doclen_est = avg_doclen_est.item() / nonzero_ranks.item()
   193   8956.0 MiB      0.0 MiB           1           self.avg_doclen_est = avg_doclen_est
   194                                         
   195   8956.0 MiB      0.0 MiB           1           Run().print(f'avg_doclen_est = {avg_doclen_est} \t len(local_sample) = {len(local_sample):,}')
   196                                         
   197   8956.0 MiB      0.0 MiB           1           torch.save(local_sample_embs.half(), os.path.join(self.config.index_path_, f'sample.{self.rank}.pt'))
   198                                         
   199   8956.0 MiB      0.0 MiB           1           return avg_doclen_est


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   106   5040.7 MiB   5040.7 MiB           1           @profile
   107                                                 def _sample_embeddings_profiled(pids):
   108   8956.0 MiB   3915.3 MiB           1               return self._sample_embeddings(pids)


[Feb 17, 04:09:47] [0] 		 Creating 131,072 partitions.
[Feb 17, 04:09:47] [0] 		 *Estimated* 130,410,345 embeddings.
[Feb 17, 04:09:47] [0] 		 #> Saving the indexing plan to .ragatouille/colbert/indexes/Genomics_index/plan.json ..
Clustering 19633522 points in 96D to 131072 clusters, redo 1 times, 4 iterations
  Preprocessing in 0.70 s
  Iteration 0 (35.63 s, search 34.13 s): objective=790120 imbalance=4.001 nsplit=790         Iteration 1 (72.87 s, search 71.06 s): objective=529103 imbalance=3.454 nsplit=27         Iteration 2 (111.61 s, search 109.54 s): objective=491788 imbalance=3.369 nsplit=9         Iteration 3 (151.43 s, search 149.10 s): objective=475910 imbalance=3.335 nsplit=0       Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    82   5040.7 MiB   5040.7 MiB           1       @profile
    83                                             def setup(self):
    84                                                 '''
    85                                                 Calculates and saves plan.json for the whole collection.
    86                                                 
    87                                                 plan.json { config, num_chunks, num_partitions, num_embeddings_est, avg_doclen_est}
    88                                                 num_partitions is the number of centroids to be generated.
    89                                                 '''
    90   5040.7 MiB      0.0 MiB           1           if self.config.resume:
    91                                                     if self._try_load_plan():
    92                                                         if self.verbose > 1:
    93                                                             Run().print_main(f"#> Loaded plan from {self.plan_path}:")
    94                                                             Run().print_main(f"#> num_chunks = {self.num_chunks}")
    95                                                             Run().print_main(f"#> num_partitions = {self.num_chunks}")
    96                                                             Run().print_main(f"#> num_embeddings_est = {self.num_embeddings_est}")
    97                                                             Run().print_main(f"#> avg_doclen_est = {self.avg_doclen_est}")
    98                                                         return
    99                                         
   100   5040.7 MiB      0.0 MiB           1           self.num_chunks = int(np.ceil(len(self.collection) / self.collection.get_chunksize()))
   101                                                 
   102   5040.7 MiB      0.0 MiB           2           @profile
   103   5040.7 MiB      0.0 MiB           1           def _sample_pids_profiled():
   104   5040.7 MiB      0.0 MiB           1               return self._sample_pids()
   105                                             
   106   5040.7 MiB      0.0 MiB           2           @profile
   107   5040.7 MiB      0.0 MiB           1           def _sample_embeddings_profiled(pids):
   108   8956.0 MiB   3915.3 MiB           1               return self._sample_embeddings(pids)
   109                                                     
   110                                                 # Saves sampled passages and embeddings for training k-means centroids later 
   111   5040.7 MiB      0.0 MiB           1           sampled_pids = _sample_pids_profiled()
   112   8956.0 MiB      0.0 MiB           1           avg_doclen_est = _sample_embeddings_profiled(sampled_pids)
   113                                         
   114                                                 # Select the number of partitions
   115   8956.0 MiB      0.0 MiB           1           num_passages = len(self.collection)
   116   8956.0 MiB      0.0 MiB           1           self.num_embeddings_est = num_passages * avg_doclen_est
   117   8956.0 MiB      0.0 MiB           1           self.num_partitions = int(2 ** np.floor(np.log2(16 * np.sqrt(self.num_embeddings_est))))
   118                                         
   119   8956.0 MiB      0.0 MiB           1           if self.verbose > 0:
   120   8956.0 MiB      0.0 MiB           1               Run().print_main(f'Creating {self.num_partitions:,} partitions.')
   121   8956.0 MiB      0.0 MiB           1               Run().print_main(f'*Estimated* {int(self.num_embeddings_est):,} embeddings.')
   122                                         
   123   8956.0 MiB      0.0 MiB           1           self._save_plan()


[Feb 17, 04:15:24] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
[Feb 17, 04:15:25] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
[0.012, 0.012, 0.011, 0.012, 0.011, 0.012, 0.012, 0.012, 0.013, 0.011, 0.012, 0.013, 0.014, 0.011, 0.012, 0.014, 0.01, 0.013, 0.012, 0.012, 0.011, 0.012, 0.013, 0.012, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.013, 0.011, 0.011, 0.013, 0.013, 0.012, 0.011, 0.013, 0.013, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.013, 0.012, 0.013, 0.013, 0.013, 0.012, 0.012, 0.012, 0.012, 0.011, 0.011, 0.012, 0.012, 0.012, 0.011, 0.011, 0.012, 0.013, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.011, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.013, 0.012, 0.012]
[Feb 17, 04:15:25] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:17:01] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:18:36] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:20:10] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:21:46] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:23:21] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:24:56] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:26:45] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:28:35] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:30:24] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:32:16] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:34:07] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:35:53] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:37:34] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:39:14] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:40:55] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:42:30] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:44:06] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:45:41] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:47:16] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:48:51] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:50:26] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:52:05] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:53:48] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:55:30] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:57:08] [0] 		 #> Encoding 25000 passages..
[Feb 17, 04:58:58] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:00:34] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:02:15] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:03:57] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:05:43] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:07:26] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:09:03] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:10:54] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:12:48] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:14:40] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:16:26] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:18:07] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:19:42] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:21:18] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:22:53] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:24:40] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:26:19] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:27:55] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:29:38] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:31:28] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:33:21] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:35:14] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:37:07] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:39:00] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:40:54] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:42:49] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:44:40] [0] 		 #> Encoding 25000 passages..
[Feb 17, 05:46:27] [0] 		 #> Encoding 23472 passages..
[Feb 17, 05:48:07] #> Optimizing IVF to store map from centroids to list of pids..
[Feb 17, 05:48:07] #> Building the emb2pid mapping..
[Feb 17, 05:48:40] len(emb2pid) = 130340848
[Feb 17, 05:49:10] #> Saved optimized IVF to .ragatouille/colbert/indexes/Genomics_index/ivf.pid.pt
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    62   5040.7 MiB   5040.7 MiB           1       @profile
    63                                             def run(self, shared_lists):
    64   6633.3 MiB      0.0 MiB           2           with torch.inference_mode():
    65   8956.0 MiB   3915.3 MiB           1               self.setup() # Computes and saves plan for whole collection
    66   8956.0 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    67   8956.0 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    68                                         
    69   8956.0 MiB      0.0 MiB           1               if not self.config.resume or not self.saver.try_load_codec():
    70   9147.5 MiB    191.5 MiB           1                   self.train(shared_lists) # Trains centroids from selected passages
    71   9147.5 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    72   9147.5 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    73                                         
    74   6230.1 MiB  -2917.3 MiB           1               self.index() # Encodes and saves all tokens into residuals
    75   6230.1 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    76   6230.1 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    77                                         
    78   6633.3 MiB    403.2 MiB           1               self.finalize() # Builds metadata and centroid to passage mapping
    79   6633.3 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    80   6633.3 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    31   5040.1 MiB   5040.1 MiB           1   @profile
    32                                         def encode(config, collection, shared_lists, shared_queues, verbose: int = 3):
    33   5040.7 MiB      0.6 MiB           1       encoder = CollectionIndexer(config=config, collection=collection, verbose=verbose)
    34   6633.3 MiB   1592.7 MiB           1       encoder.run(shared_lists)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   112   5040.1 MiB   5040.1 MiB           1       @profile
   113                                             def _callee_profiled(*args, **kwargs):
   114   6633.3 MiB   1593.3 MiB           1           return callee(*args, **kwargs)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   110   5040.1 MiB   5040.1 MiB           1   @profile
   111                                         def run_process_without_mp(callee, config, *args):
   112   5040.1 MiB      0.0 MiB           2       @profile
   113   5040.1 MiB      0.0 MiB           1       def _callee_profiled(*args, **kwargs):
   114   6633.3 MiB   1593.3 MiB           1           return callee(*args, **kwargs)
   115                                         
   116   5040.1 MiB      0.0 MiB           1       set_seed(12345)
   117   5040.1 MiB      0.0 MiB           1       os.environ["CUDA_VISIBLE_DEVICES"] = ','.join(map(str, config.gpus_[:config.nranks]))
   118                                         
   119   6633.3 MiB      0.0 MiB           2       with Run().context(config, inherit_config=False):
   120   6633.3 MiB      0.0 MiB           1           return_val = _callee_profiled(config, *args)
   121   6633.3 MiB      0.0 MiB           1           torch.cuda.empty_cache()
   122   6633.3 MiB      0.0 MiB           1           return return_val


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    94   5040.1 MiB   5040.1 MiB           1           @profile
    95                                                 def _run_process_profiled(*args, **kwargs):
    96   6633.3 MiB   1593.3 MiB           1               return run_process_without_mp(*args, **kwargs)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    87   5040.1 MiB   5040.1 MiB           1       @profile
    88                                             def launch_without_fork(self, custom_config, *args):
    89   5040.1 MiB      0.0 MiB           1           assert isinstance(custom_config, BaseConfig)
    90   5040.1 MiB      0.0 MiB           1           assert isinstance(custom_config, RunSettings)
    91   5040.1 MiB      0.0 MiB           1           assert self.nranks == 1
    92   5040.1 MiB      0.0 MiB           1           assert (custom_config.avoid_fork_if_possible or self.run_config.avoid_fork_if_possible)
    93                                                 
    94   5040.1 MiB      0.0 MiB           2           @profile
    95   5040.1 MiB      0.0 MiB           1           def _run_process_profiled(*args, **kwargs):
    96   6633.3 MiB   1593.3 MiB           1               return run_process_without_mp(*args, **kwargs)
    97                                                     
    98   5040.1 MiB      0.0 MiB           1           new_config = type(custom_config).from_existing(custom_config, self.run_config, RunConfig(rank=0))
    99   6633.3 MiB      0.0 MiB           1           return_val = _run_process_profiled(self.callee, new_config, *args)
   100                                         
   101   6633.3 MiB      0.0 MiB           1           return return_val


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    85   5040.1 MiB   5040.1 MiB           1       @profile
    86                                             def __launch(self, collection):
    87   5040.1 MiB      0.0 MiB           1           launcher = Launcher(encode)
    88   5040.1 MiB      0.0 MiB           1           if self.config.nranks == 1 and self.config.avoid_fork_if_possible:
    89   5040.1 MiB      0.0 MiB           1               shared_queues = []
    90   5040.1 MiB      0.0 MiB           1               shared_lists = []
    91   6633.3 MiB   1593.3 MiB           1               launcher.launch_without_fork(self.config, collection, shared_lists, shared_queues, self.verbose)
    92                                         
    93   6633.3 MiB      0.0 MiB           1               return
    94                                         
    95                                                 manager = mp.Manager()
    96                                                 shared_lists = [manager.list() for _ in range(self.config.nranks)]
    97                                                 shared_queues = [manager.Queue(maxsize=1) for _ in range(self.config.nranks)]
    98                                         
    99                                                 # Encodes collection into index using the CollectionIndexer class
   100                                                 launcher.launch(self.config, collection, shared_lists, shared_queues, self.verbose)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    60   5040.1 MiB   5040.1 MiB           1       @profile
    61                                             def index(self, name, collection, overwrite=False):
    62   5040.1 MiB      0.0 MiB           1           assert overwrite in [True, False, 'reuse', 'resume', "force_silent_overwrite"]
    63                                         
    64   5040.1 MiB      0.0 MiB           1           self.configure(collection=collection, index_name=name, resume=overwrite=='resume')
    65                                                 # Note: The bsize value set here is ignored internally. Users are encouraged
    66                                                 # to supply their own batch size for indexing by using the index_bsize parameter in the ColBERTConfig.
    67   5040.1 MiB      0.0 MiB           1           self.configure(bsize=64, partitions=None)
    68                                         
    69   5040.1 MiB      0.0 MiB           1           self.index_path = self.config.index_path_
    70   5040.1 MiB      0.0 MiB           1           index_does_not_exist = (not os.path.exists(self.config.index_path_))
    71                                         
    72   5040.1 MiB      0.0 MiB           1           assert (overwrite in [True, 'reuse', 'resume', "force_silent_overwrite"]) or index_does_not_exist, self.config.index_path_
    73   5040.1 MiB      0.0 MiB           1           create_directory(self.config.index_path_)
    74                                         
    75   5040.1 MiB      0.0 MiB           1           if overwrite == 'force_silent_overwrite':
    76                                                     self.erase(force_silent=True)
    77   5040.1 MiB      0.0 MiB           1           elif overwrite is True:
    78   5040.1 MiB      0.0 MiB           1               self.erase()
    79                                         
    80   5040.1 MiB      0.0 MiB           1           if index_does_not_exist or overwrite != 'reuse':
    81   6633.3 MiB   1593.3 MiB           1               self.__launch(collection)
    82                                         
    83   6633.3 MiB      0.0 MiB           1           return self.index_path


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   198   5040.1 MiB   5040.1 MiB           1           @profile
   199                                                 def _index_with_profiling(indexer, name, collection, overwrite):
   200   6633.3 MiB   1593.3 MiB           1               return indexer.index(name=name, collection=collection, overwrite=overwrite)


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   162   5040.1 MiB   5040.1 MiB           1       @profile
   163                                             def build(
   164                                                 self,
   165                                                 checkpoint: Union[str, Path],
   166                                                 collection: List[str],
   167                                                 index_name: Optional["str"] = None,
   168                                                 overwrite: Union[bool, str] = "reuse",
   169                                                 verbose: bool = True,
   170                                                 **kwargs,
   171                                             ) -> "PLAIDModelIndex":
   172                                                 
   173   5040.1 MiB      0.0 MiB           1           bsize = kwargs.get("bsize", PLAIDModelIndex._DEFAULT_INDEX_BSIZE)
   174   5040.1 MiB      0.0 MiB           1           assert isinstance(bsize, int)
   175                                         
   176   5040.1 MiB      0.0 MiB           1           nbits = 2
   177   5040.1 MiB      0.0 MiB           1           if len(collection) < 10000:
   178                                                     nbits = 4
   179   5040.1 MiB      0.0 MiB           2           self.config = ColBERTConfig.from_existing(
   180   5040.1 MiB      0.0 MiB           1               self.config, ColBERTConfig(nbits=nbits, index_bsize=bsize)
   181                                                 )
   182                                         
   183                                                 # Instruct colbert-ai to disable forking if nranks == 1
   184   5040.1 MiB      0.0 MiB           1           self.config.avoid_fork_if_possible = True
   185                                         
   186   5040.1 MiB      0.0 MiB           1           if len(collection) > 100000:
   187   5040.1 MiB      0.0 MiB           1               self.config.kmeans_niters = 4
   188                                                 elif len(collection) > 50000:
   189                                                     self.config.kmeans_niters = 10
   190                                                 else:
   191                                                     self.config.kmeans_niters = 20
   192                                         
   193                                                 # Monkey-patch colbert-ai to avoid using FAISS
   194   5040.1 MiB      0.0 MiB           1           monkey_patching = (
   195   5040.1 MiB      0.0 MiB           1               len(collection) < 75000 and kwargs.get("use_faiss", False) is False
   196                                                 )
   197                                         
   198   5040.1 MiB      0.0 MiB           2           @profile
   199   5040.1 MiB      0.0 MiB           1           def _index_with_profiling(indexer, name, collection, overwrite):
   200   6633.3 MiB   1593.3 MiB           1               return indexer.index(name=name, collection=collection, overwrite=overwrite)
   201                                                     
   202   5040.1 MiB      0.0 MiB           1           if monkey_patching:
   203                                                     print(
   204                                                         "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----"
   205                                                     )
   206                                                     print("This is a behaviour change from RAGatouille 0.8.0 onwards.")
   207                                                     print(
   208                                                         "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations."
   209                                                     )
   210                                                     print(
   211                                                         "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour."
   212                                                     )
   213                                                     print("--------------------")
   214                                                     CollectionIndexer._train_kmeans = self.pytorch_kmeans
   215                                         
   216                                                     # Try to keep runtime stable -- these are values that empirically didn't degrade performance at all on 3 benchmarks.
   217                                                     # More tests required before warning can be removed.
   218                                                     try:
   219                                                         indexer = Indexer(
   220                                                             checkpoint=checkpoint,
   221                                                             config=self.config,
   222                                                             verbose=verbose,
   223                                                         )
   224                                                         indexer.configure(avoid_fork_if_possible=True)
   225                                                         _index_with_profiling(indexer, index_name, collection, overwrite)
   226                                                     except Exception as err:
   227                                                         print(
   228                                                             f"PyTorch-based indexing did not succeed with error: {err}",
   229                                                             "! Reverting to using FAISS and attempting again...",
   230                                                         )
   231                                                         monkey_patching = False
   232   5040.1 MiB      0.0 MiB           1           if monkey_patching is False:
   233   5040.1 MiB      0.0 MiB           1               CollectionIndexer._train_kmeans = self.faiss_kmeans
   234   5040.1 MiB      0.0 MiB           1               if torch.cuda.is_available():
   235   5040.1 MiB      0.0 MiB           1                   import faiss
   236                                         
   237   5040.1 MiB      0.0 MiB           1                   if not hasattr(faiss, "StandardGpuResources"):
   238                                                             print(
   239                                                                 "________________________________________________________________________________\n"
   240                                                                 "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
   241                                                                 "This means that indexing will be slow. To make use of your GPU.\n"
   242                                                                 "Please install `faiss-gpu` by running:\n"
   243                                                                 "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
   244                                                                 "________________________________________________________________________________",
   245                                                             )
   246                                                             print("Will continue with CPU indexing in 5 seconds...")
   247                                                             time.sleep(5)
   248   5040.1 MiB      0.0 MiB           2               indexer = Indexer(
   249   5040.1 MiB      0.0 MiB           1                   checkpoint=checkpoint,
   250   5040.1 MiB      0.0 MiB           1                   config=self.config,
   251   5040.1 MiB      0.0 MiB           1                   verbose=verbose,
   252                                                     )
   253   5040.1 MiB      0.0 MiB           1               indexer.configure(avoid_fork_if_possible=True)
   254   6633.3 MiB      0.0 MiB           1               _index_with_profiling(indexer, index_name, collection, overwrite)
   255                                         
   256   6633.3 MiB      0.0 MiB           1           return self


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   136   5040.1 MiB   5040.1 MiB           1       @staticmethod
   137                                             @profile
   138                                             def construct(
   139                                                 config: ColBERTConfig,
   140                                                 checkpoint: Union[str, Path],
   141                                                 collection: List[str],
   142                                                 index_name: Optional["str"] = None,
   143                                                 overwrite: Union[bool, str] = "reuse",
   144                                                 verbose: bool = True,
   145                                                 **kwargs,
   146                                             ) -> "PLAIDModelIndex":
   147   6633.3 MiB   1593.3 MiB           3           return PLAIDModelIndex(config).build(
   148   5040.1 MiB      0.0 MiB           2               checkpoint, collection, index_name, overwrite, verbose, **kwargs
   149                                                 )


Filename: /home/RAGatouille/ragatouille/models/colbert.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   342   5040.1 MiB   5040.1 MiB           1           @profile
   343                                                 def _construct_model_index(*args, **kwargs):
   344   6633.3 MiB   1593.3 MiB           1               return ModelIndexFactory.construct(*args, **kwargs)


Done indexing!
Filename: /home/RAGatouille/ragatouille/RAGPretrainedModel.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   205   5040.1 MiB   5040.1 MiB           1           @profile
   206                                                 def _model_index_profiled(*args, **kwargs):
   207   6234.2 MiB   1194.1 MiB           1               return self.model.index(*args, **kwargs)


Filename: ../ragatouille_index_1M_False.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    11   1766.8 MiB   1766.8 MiB           1   @profile
    12                                         def _index(): 
    13   6112.9 MiB   2513.1 MiB           2       return RAG.index(
    14   1766.8 MiB      0.0 MiB           1           index_name=f"{dataset_name}_index", 
    15   3023.1 MiB   1256.3 MiB           1           collection=passages[:ndocs]["text"], 
    16   3599.8 MiB    576.8 MiB           1           document_ids=passages[:ndocs]["_id"],
    17   3599.8 MiB      0.0 MiB           1           use_faiss=False
    18                                             )



