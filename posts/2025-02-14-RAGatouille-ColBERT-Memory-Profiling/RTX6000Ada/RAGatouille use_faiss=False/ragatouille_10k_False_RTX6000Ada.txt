10000
Filename: /home/RAGatouille/ragatouille/RAGPretrainedModel.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   201   1785.5 MiB   1785.5 MiB           1           @profile
   202                                                 def _process_corpus_profiled(*args, **kwargs):
   203   1835.1 MiB     49.6 MiB           1               return self._process_corpus(*args, **kwargs)


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   131   1835.1 MiB   1835.1 MiB           1       @profile
   132                                             def __init__(self, config: ColBERTConfig) -> None:
   133   1835.1 MiB      0.0 MiB           1           super().__init__(config)
   134   1835.1 MiB      0.0 MiB           1           self.searcher: Optional[Searcher] = None


---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----
This is a behaviour change from RAGatouille 0.8.0 onwards.
This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.
If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.
--------------------


[Feb 17, 13:54:04] #> Note: Output directory .ragatouille/colbert/indexes/Genomics_index already exists


[Feb 17, 13:54:04] #> Will delete 442 files already at .ragatouille/colbert/indexes/Genomics_index in 20 seconds...
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102   1849.2 MiB   1849.2 MiB           1           @profile
   103                                                 def _sample_pids_profiled():
   104   1849.2 MiB      0.0 MiB           1               return self._sample_pids()


[Feb 17, 13:54:42] [0] 		 #> Encoding 11924 passages..
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   146   1849.2 MiB   1849.2 MiB           1           @profile
   147                                                 def _encode_passages_profiled(*args, **kwargs):
   148   2675.7 MiB    826.5 MiB           1               return self.encoder.encode_passages(*args, **kwargs)


[Feb 17, 13:55:58] [0] 		 avg_doclen_est = 79.69187927246094 	 len(local_sample) = 11,924
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   144   1849.2 MiB   1849.2 MiB           1       @profile
   145                                             def _sample_embeddings(self, sampled_pids):
   146   1849.2 MiB      0.0 MiB           2           @profile
   147   1849.2 MiB      0.0 MiB           1           def _encode_passages_profiled(*args, **kwargs):
   148   2675.7 MiB    826.5 MiB           1               return self.encoder.encode_passages(*args, **kwargs)
   149                                                     
   150   1849.2 MiB      0.0 MiB           1           local_pids = self.collection.enumerate(rank=self.rank)
   151   1849.2 MiB      0.0 MiB       11927           local_sample = [passage for pid, passage in local_pids if pid in sampled_pids]
   152                                         
   153   2675.7 MiB      0.0 MiB           1           local_sample_embs, doclens = _encode_passages_profiled(local_sample)
   154                                         
   155   2675.7 MiB      0.0 MiB           1           if torch.cuda.is_available():
   156   2675.7 MiB      0.0 MiB           1               if torch.distributed.is_available() and torch.distributed.is_initialized():
   157                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   158                                                         torch.distributed.all_reduce(self.num_sample_embs)
   159                                         
   160                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   161                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   162                                                         torch.distributed.all_reduce(avg_doclen_est)
   163                                         
   164                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   165                                                         torch.distributed.all_reduce(nonzero_ranks)
   166                                                     else:
   167   2675.7 MiB      0.0 MiB           1                   self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   168                                         
   169   2675.7 MiB      0.0 MiB           1                   avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   170   2675.7 MiB      0.0 MiB           1                   avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   171                                         
   172   2675.7 MiB      0.0 MiB           1                   nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   173                                                 else:
   174                                                     if torch.distributed.is_available() and torch.distributed.is_initialized():
   175                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   176                                                         torch.distributed.all_reduce(self.num_sample_embs)
   177                                         
   178                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   179                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   180                                                         torch.distributed.all_reduce(avg_doclen_est)
   181                                         
   182                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   183                                                         torch.distributed.all_reduce(nonzero_ranks)
   184                                                     else:
   185                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   186                                         
   187                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   188                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   189                                         
   190                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   191                                         
   192   2675.7 MiB      0.0 MiB           1           avg_doclen_est = avg_doclen_est.item() / nonzero_ranks.item()
   193   2675.7 MiB      0.0 MiB           1           self.avg_doclen_est = avg_doclen_est
   194                                         
   195   2675.7 MiB      0.0 MiB           1           Run().print(f'avg_doclen_est = {avg_doclen_est} \t len(local_sample) = {len(local_sample):,}')
   196                                         
   197   2675.7 MiB      0.0 MiB           1           torch.save(local_sample_embs.half(), os.path.join(self.config.index_path_, f'sample.{self.rank}.pt'))
   198                                         
   199   2675.7 MiB      0.0 MiB           1           return avg_doclen_est


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   106   1849.2 MiB   1849.2 MiB           1           @profile
   107                                                 def _sample_embeddings_profiled(pids):
   108   2675.7 MiB    826.5 MiB           1               return self._sample_embeddings(pids)


[Feb 17, 13:55:59] [0] 		 Creating 8,192 partitions.
[Feb 17, 13:55:59] [0] 		 *Estimated* 950,245 embeddings.
[Feb 17, 13:55:59] [0] 		 #> Saving the indexing plan to .ragatouille/colbert/indexes/Genomics_index/plan.json ..
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    82   1849.2 MiB   1849.2 MiB           1       @profile
    83                                             def setup(self):
    84                                                 '''
    85                                                 Calculates and saves plan.json for the whole collection.
    86                                                 
    87                                                 plan.json { config, num_chunks, num_partitions, num_embeddings_est, avg_doclen_est}
    88                                                 num_partitions is the number of centroids to be generated.
    89                                                 '''
    90   1849.2 MiB      0.0 MiB           1           if self.config.resume:
    91                                                     if self._try_load_plan():
    92                                                         if self.verbose > 1:
    93                                                             Run().print_main(f"#> Loaded plan from {self.plan_path}:")
    94                                                             Run().print_main(f"#> num_chunks = {self.num_chunks}")
    95                                                             Run().print_main(f"#> num_partitions = {self.num_chunks}")
    96                                                             Run().print_main(f"#> num_embeddings_est = {self.num_embeddings_est}")
    97                                                             Run().print_main(f"#> avg_doclen_est = {self.avg_doclen_est}")
    98                                                         return
    99                                         
   100   1849.2 MiB      0.0 MiB           1           self.num_chunks = int(np.ceil(len(self.collection) / self.collection.get_chunksize()))
   101                                                 
   102   1849.2 MiB      0.0 MiB           2           @profile
   103   1849.2 MiB      0.0 MiB           1           def _sample_pids_profiled():
   104   1849.2 MiB      0.0 MiB           1               return self._sample_pids()
   105                                             
   106   1849.2 MiB      0.0 MiB           2           @profile
   107   1849.2 MiB      0.0 MiB           1           def _sample_embeddings_profiled(pids):
   108   2675.7 MiB    826.5 MiB           1               return self._sample_embeddings(pids)
   109                                                     
   110                                                 # Saves sampled passages and embeddings for training k-means centroids later 
   111   1849.2 MiB      0.0 MiB           1           sampled_pids = _sample_pids_profiled()
   112   2675.7 MiB      0.0 MiB           1           avg_doclen_est = _sample_embeddings_profiled(sampled_pids)
   113                                         
   114                                                 # Select the number of partitions
   115   2675.7 MiB      0.0 MiB           1           num_passages = len(self.collection)
   116   2675.7 MiB      0.0 MiB           1           self.num_embeddings_est = num_passages * avg_doclen_est
   117   2675.7 MiB      0.0 MiB           1           self.num_partitions = int(2 ** np.floor(np.log2(16 * np.sqrt(self.num_embeddings_est))))
   118                                         
   119   2675.7 MiB      0.0 MiB           1           if self.verbose > 0:
   120   2675.7 MiB      0.0 MiB           1               Run().print_main(f'Creating {self.num_partitions:,} partitions.')
   121   2675.7 MiB      0.0 MiB           1               Run().print_main(f'*Estimated* {int(self.num_embeddings_est):,} embeddings.')
   122                                         
   123   2675.7 MiB      0.0 MiB           1           self._save_plan()


PyTorch-based indexing did not succeed with error: CUDA out of memory. Tried to allocate 27.55 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.88 GiB is free. Process 236161 has 42.61 GiB memory in use. Of the allocated memory 35.03 GiB is allocated by PyTorch, and 7.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ! Reverting to using FAISS and attempting again...


[Feb 17, 13:56:03] #> Note: Output directory .ragatouille/colbert/indexes/Genomics_index already exists


[Feb 17, 13:56:03] #> Will delete 1 files already at .ragatouille/colbert/indexes/Genomics_index in 20 seconds...
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   102   2652.6 MiB   2652.6 MiB           1           @profile
   103                                                 def _sample_pids_profiled():
   104   2652.6 MiB      0.0 MiB           1               return self._sample_pids()


[Feb 17, 13:56:39] [0] 		 #> Encoding 11924 passages..
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   146   2652.6 MiB   2652.6 MiB           1           @profile
   147                                                 def _encode_passages_profiled(*args, **kwargs):
   148   3025.6 MiB    373.0 MiB           1               return self.encoder.encode_passages(*args, **kwargs)


[Feb 17, 13:58:07] [0] 		 avg_doclen_est = 79.69187927246094 	 len(local_sample) = 11,924
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   144   2652.6 MiB   2652.6 MiB           1       @profile
   145                                             def _sample_embeddings(self, sampled_pids):
   146   2652.6 MiB      0.0 MiB           2           @profile
   147   2652.6 MiB      0.0 MiB           1           def _encode_passages_profiled(*args, **kwargs):
   148   3025.6 MiB    373.0 MiB           1               return self.encoder.encode_passages(*args, **kwargs)
   149                                                     
   150   2652.6 MiB      0.0 MiB           1           local_pids = self.collection.enumerate(rank=self.rank)
   151   2652.6 MiB      0.0 MiB       11927           local_sample = [passage for pid, passage in local_pids if pid in sampled_pids]
   152                                         
   153   3025.6 MiB      0.0 MiB           1           local_sample_embs, doclens = _encode_passages_profiled(local_sample)
   154                                         
   155   3025.6 MiB      0.0 MiB           1           if torch.cuda.is_available():
   156   3025.6 MiB      0.0 MiB           1               if torch.distributed.is_available() and torch.distributed.is_initialized():
   157                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   158                                                         torch.distributed.all_reduce(self.num_sample_embs)
   159                                         
   160                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   161                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   162                                                         torch.distributed.all_reduce(avg_doclen_est)
   163                                         
   164                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   165                                                         torch.distributed.all_reduce(nonzero_ranks)
   166                                                     else:
   167   3025.6 MiB      0.0 MiB           1                   self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
   168                                         
   169   3025.6 MiB      0.0 MiB           1                   avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   170   3025.6 MiB      0.0 MiB           1                   avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
   171                                         
   172   3025.6 MiB      0.0 MiB           1                   nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
   173                                                 else:
   174                                                     if torch.distributed.is_available() and torch.distributed.is_initialized():
   175                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   176                                                         torch.distributed.all_reduce(self.num_sample_embs)
   177                                         
   178                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   179                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   180                                                         torch.distributed.all_reduce(avg_doclen_est)
   181                                         
   182                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   183                                                         torch.distributed.all_reduce(nonzero_ranks)
   184                                                     else:
   185                                                         self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
   186                                         
   187                                                         avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
   188                                                         avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
   189                                         
   190                                                         nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
   191                                         
   192   3025.6 MiB      0.0 MiB           1           avg_doclen_est = avg_doclen_est.item() / nonzero_ranks.item()
   193   3025.6 MiB      0.0 MiB           1           self.avg_doclen_est = avg_doclen_est
   194                                         
   195   3025.6 MiB      0.0 MiB           1           Run().print(f'avg_doclen_est = {avg_doclen_est} \t len(local_sample) = {len(local_sample):,}')
   196                                         
   197   3025.6 MiB      0.0 MiB           1           torch.save(local_sample_embs.half(), os.path.join(self.config.index_path_, f'sample.{self.rank}.pt'))
   198                                         
   199   3025.6 MiB      0.0 MiB           1           return avg_doclen_est


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   106   2652.6 MiB   2652.6 MiB           1           @profile
   107                                                 def _sample_embeddings_profiled(pids):
   108   3025.6 MiB    373.0 MiB           1               return self._sample_embeddings(pids)


[Feb 17, 13:58:08] [0] 		 Creating 8,192 partitions.
[Feb 17, 13:58:08] [0] 		 *Estimated* 950,245 embeddings.
[Feb 17, 13:58:08] [0] 		 #> Saving the indexing plan to .ragatouille/colbert/indexes/Genomics_index/plan.json ..
Clustering 902734 points in 96D to 8192 clusters, redo 1 times, 20 iterations
  Preprocessing in 0.06 s
  Iteration 0 (0.14 s, search 0.10 s): objective=70638.9 imbalance=1.859 nsplit=30         Iteration 1 (0.26 s, search 0.21 s): objective=47496.4 imbalance=1.628 nsplit=4         Iteration 2 (0.38 s, search 0.31 s): objective=42857.3 imbalance=1.577 nsplit=0         Iteration 3 (0.50 s, search 0.41 s): objective=40832 imbalance=1.555 nsplit=0         Iteration 4 (0.62 s, search 0.52 s): objective=39850.8 imbalance=1.546 nsplit=0         Iteration 5 (0.76 s, search 0.60 s): objective=39270.9 imbalance=1.543 nsplit=0         Iteration 6 (0.88 s, search 0.69 s): objective=38920.3 imbalance=1.543 nsplit=0         Iteration 7 (0.99 s, search 0.78 s): objective=38670.6 imbalance=1.543 nsplit=0         Iteration 8 (1.09 s, search 0.87 s): objective=38478.8 imbalance=1.544 nsplit=0         Iteration 9 (1.19 s, search 0.96 s): objective=38344.7 imbalance=1.545 nsplit=0         Iteration 10 (1.30 s, search 1.05 s): objective=38252 imbalance=1.545 nsplit=0         Iteration 11 (1.40 s, search 1.13 s): objective=38180.8 imbalance=1.547 nsplit=0         Iteration 12 (1.50 s, search 1.22 s): objective=38120.4 imbalance=1.548 nsplit=0         Iteration 13 (1.60 s, search 1.31 s): objective=38068.3 imbalance=1.549 nsplit=0         Iteration 14 (1.71 s, search 1.40 s): objective=38027 imbalance=1.550 nsplit=0         Iteration 15 (1.81 s, search 1.49 s): objective=37987.9 imbalance=1.551 nsplit=0         Iteration 16 (2.03 s, search 1.58 s): objective=37957.6 imbalance=1.552 nsplit=0         Iteration 17 (2.14 s, search 1.67 s): objective=37924.7 imbalance=1.553 nsplit=0         Iteration 18 (2.24 s, search 1.76 s): objective=37897 imbalance=1.554 nsplit=0         Iteration 19 (2.34 s, search 1.85 s): objective=37877.2 imbalance=1.555 nsplit=0       Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    82   2652.6 MiB   2652.6 MiB           1       @profile
    83                                             def setup(self):
    84                                                 '''
    85                                                 Calculates and saves plan.json for the whole collection.
    86                                                 
    87                                                 plan.json { config, num_chunks, num_partitions, num_embeddings_est, avg_doclen_est}
    88                                                 num_partitions is the number of centroids to be generated.
    89                                                 '''
    90   2652.6 MiB      0.0 MiB           1           if self.config.resume:
    91                                                     if self._try_load_plan():
    92                                                         if self.verbose > 1:
    93                                                             Run().print_main(f"#> Loaded plan from {self.plan_path}:")
    94                                                             Run().print_main(f"#> num_chunks = {self.num_chunks}")
    95                                                             Run().print_main(f"#> num_partitions = {self.num_chunks}")
    96                                                             Run().print_main(f"#> num_embeddings_est = {self.num_embeddings_est}")
    97                                                             Run().print_main(f"#> avg_doclen_est = {self.avg_doclen_est}")
    98                                                         return
    99                                         
   100   2652.6 MiB      0.0 MiB           1           self.num_chunks = int(np.ceil(len(self.collection) / self.collection.get_chunksize()))
   101                                                 
   102   2652.6 MiB      0.0 MiB           2           @profile
   103   2652.6 MiB      0.0 MiB           1           def _sample_pids_profiled():
   104   2652.6 MiB      0.0 MiB           1               return self._sample_pids()
   105                                             
   106   2652.6 MiB      0.0 MiB           2           @profile
   107   2652.6 MiB      0.0 MiB           1           def _sample_embeddings_profiled(pids):
   108   3025.6 MiB    373.0 MiB           1               return self._sample_embeddings(pids)
   109                                                     
   110                                                 # Saves sampled passages and embeddings for training k-means centroids later 
   111   2652.6 MiB      0.0 MiB           1           sampled_pids = _sample_pids_profiled()
   112   3025.6 MiB      0.0 MiB           1           avg_doclen_est = _sample_embeddings_profiled(sampled_pids)
   113                                         
   114                                                 # Select the number of partitions
   115   3025.6 MiB      0.0 MiB           1           num_passages = len(self.collection)
   116   3025.6 MiB      0.0 MiB           1           self.num_embeddings_est = num_passages * avg_doclen_est
   117   3025.6 MiB      0.0 MiB           1           self.num_partitions = int(2 ** np.floor(np.log2(16 * np.sqrt(self.num_embeddings_est))))
   118                                         
   119   3025.6 MiB      0.0 MiB           1           if self.verbose > 0:
   120   3025.6 MiB      0.0 MiB           1               Run().print_main(f'Creating {self.num_partitions:,} partitions.')
   121   3025.6 MiB      0.0 MiB           1               Run().print_main(f'*Estimated* {int(self.num_embeddings_est):,} embeddings.')
   122                                         
   123   2956.1 MiB    -69.5 MiB           1           self._save_plan()


[Feb 17, 13:58:14] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
[Feb 17, 13:58:14] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
[0.015, 0.015, 0.014, 0.015, 0.014, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.017, 0.018, 0.015, 0.015, 0.017, 0.013, 0.017, 0.015, 0.015, 0.014, 0.015, 0.017, 0.015, 0.015, 0.016, 0.016, 0.016, 0.015, 0.015, 0.015, 0.014, 0.015, 0.016, 0.015, 0.017, 0.015, 0.016, 0.016, 0.016, 0.015, 0.016, 0.015, 0.015, 0.014, 0.016, 0.016, 0.016, 0.015, 0.016, 0.017, 0.015, 0.015, 0.015, 0.014, 0.015, 0.016, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.016, 0.017, 0.017, 0.015, 0.015, 0.015, 0.016, 0.015, 0.015, 0.016, 0.015, 0.016, 0.014, 0.014, 0.015, 0.016, 0.016, 0.015, 0.016, 0.015, 0.015, 0.016, 0.014, 0.015, 0.015, 0.015, 0.015, 0.016, 0.015, 0.016, 0.015, 0.015]
[Feb 17, 13:58:14] [0] 		 #> Encoding 11924 passages..
[Feb 17, 13:59:26] #> Optimizing IVF to store map from centroids to list of pids..
[Feb 17, 13:59:26] #> Building the emb2pid mapping..
[Feb 17, 13:59:27] len(emb2pid) = 950246
[Feb 17, 13:59:29] #> Saved optimized IVF to .ragatouille/colbert/indexes/Genomics_index/ivf.pid.pt
Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    62   2652.6 MiB   2652.6 MiB           1       @profile
    63                                             def run(self, shared_lists):
    64   2965.0 MiB      0.0 MiB           2           with torch.inference_mode():
    65   2956.1 MiB    303.5 MiB           1               self.setup() # Computes and saves plan for whole collection
    66   2956.1 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    67   2956.1 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    68                                         
    69   2956.1 MiB      0.0 MiB           1               if not self.config.resume or not self.saver.try_load_codec():
    70   2967.4 MiB     11.2 MiB           1                   self.train(shared_lists) # Trains centroids from selected passages
    71   2967.4 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    72   2967.4 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    73                                         
    74   2964.0 MiB     -3.4 MiB           1               self.index() # Encodes and saves all tokens into residuals
    75   2964.0 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    76   2964.0 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')
    77                                         
    78   2965.0 MiB      1.0 MiB           1               self.finalize() # Builds metadata and centroid to passage mapping
    79   2965.0 MiB      0.0 MiB           1               distributed.barrier(self.rank)
    80   2965.0 MiB      0.0 MiB           1               print_memory_stats(f'RANK:{self.rank}')


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    31   2819.5 MiB   2819.5 MiB           1   @profile
    32                                         def encode(config, collection, shared_lists, shared_queues, verbose: int = 3):
    33   2652.6 MiB   -166.9 MiB           1       encoder = CollectionIndexer(config=config, collection=collection, verbose=verbose)
    34   2965.0 MiB    312.4 MiB           1       encoder.run(shared_lists)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   112   2819.5 MiB   2819.5 MiB           1       @profile
   113                                             def _callee_profiled(*args, **kwargs):
   114   2965.0 MiB    145.5 MiB           1           return callee(*args, **kwargs)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   110   2819.5 MiB   2819.5 MiB           1   @profile
   111                                         def run_process_without_mp(callee, config, *args):
   112   2819.5 MiB      0.0 MiB           2       @profile
   113   2819.5 MiB      0.0 MiB           1       def _callee_profiled(*args, **kwargs):
   114   2965.0 MiB    145.5 MiB           1           return callee(*args, **kwargs)
   115                                         
   116   2819.5 MiB      0.0 MiB           1       set_seed(12345)
   117   2819.5 MiB      0.0 MiB           1       os.environ["CUDA_VISIBLE_DEVICES"] = ','.join(map(str, config.gpus_[:config.nranks]))
   118                                         
   119   2965.0 MiB      0.0 MiB           2       with Run().context(config, inherit_config=False):
   120   2965.0 MiB      0.0 MiB           1           return_val = _callee_profiled(config, *args)
   121   2965.0 MiB      0.0 MiB           1           torch.cuda.empty_cache()
   122   2965.0 MiB      0.0 MiB           1           return return_val


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    94   2819.5 MiB   2819.5 MiB           1           @profile
    95                                                 def _run_process_profiled(*args, **kwargs):
    96   2965.0 MiB    145.5 MiB           1               return run_process_without_mp(*args, **kwargs)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/infra/launcher.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    87   2819.5 MiB   2819.5 MiB           1       @profile
    88                                             def launch_without_fork(self, custom_config, *args):
    89   2819.5 MiB      0.0 MiB           1           assert isinstance(custom_config, BaseConfig)
    90   2819.5 MiB      0.0 MiB           1           assert isinstance(custom_config, RunSettings)
    91   2819.5 MiB      0.0 MiB           1           assert self.nranks == 1
    92   2819.5 MiB      0.0 MiB           1           assert (custom_config.avoid_fork_if_possible or self.run_config.avoid_fork_if_possible)
    93                                                 
    94   2819.5 MiB      0.0 MiB           2           @profile
    95   2819.5 MiB      0.0 MiB           1           def _run_process_profiled(*args, **kwargs):
    96   2965.0 MiB    145.5 MiB           1               return run_process_without_mp(*args, **kwargs)
    97                                                     
    98   2819.5 MiB      0.0 MiB           1           new_config = type(custom_config).from_existing(custom_config, self.run_config, RunConfig(rank=0))
    99   2965.0 MiB      0.0 MiB           1           return_val = _run_process_profiled(self.callee, new_config, *args)
   100                                         
   101   2965.0 MiB      0.0 MiB           1           return return_val


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    85   2819.5 MiB   2819.5 MiB           1       @profile
    86                                             def __launch(self, collection):
    87   2819.5 MiB      0.0 MiB           1           launcher = Launcher(encode)
    88   2819.5 MiB      0.0 MiB           1           if self.config.nranks == 1 and self.config.avoid_fork_if_possible:
    89   2819.5 MiB      0.0 MiB           1               shared_queues = []
    90   2819.5 MiB      0.0 MiB           1               shared_lists = []
    91   2965.0 MiB    145.5 MiB           1               launcher.launch_without_fork(self.config, collection, shared_lists, shared_queues, self.verbose)
    92                                         
    93   2965.0 MiB      0.0 MiB           1               return
    94                                         
    95                                                 manager = mp.Manager()
    96                                                 shared_lists = [manager.list() for _ in range(self.config.nranks)]
    97                                                 shared_queues = [manager.Queue(maxsize=1) for _ in range(self.config.nranks)]
    98                                         
    99                                                 # Encodes collection into index using the CollectionIndexer class
   100                                                 launcher.launch(self.config, collection, shared_lists, shared_queues, self.verbose)


Filename: /home/ragatouille-env/lib/python3.10/site-packages/colbert/indexer.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    60   2819.5 MiB   2819.5 MiB           1       @profile
    61                                             def index(self, name, collection, overwrite=False):
    62   2819.5 MiB      0.0 MiB           1           assert overwrite in [True, False, 'reuse', 'resume', "force_silent_overwrite"]
    63                                         
    64   2819.5 MiB      0.0 MiB           1           self.configure(collection=collection, index_name=name, resume=overwrite=='resume')
    65                                                 # Note: The bsize value set here is ignored internally. Users are encouraged
    66                                                 # to supply their own batch size for indexing by using the index_bsize parameter in the ColBERTConfig.
    67   2819.5 MiB      0.0 MiB           1           self.configure(bsize=64, partitions=None)
    68                                         
    69   2819.5 MiB      0.0 MiB           1           self.index_path = self.config.index_path_
    70   2819.5 MiB      0.0 MiB           1           index_does_not_exist = (not os.path.exists(self.config.index_path_))
    71                                         
    72   2819.5 MiB      0.0 MiB           1           assert (overwrite in [True, 'reuse', 'resume', "force_silent_overwrite"]) or index_does_not_exist, self.config.index_path_
    73   2819.5 MiB      0.0 MiB           1           create_directory(self.config.index_path_)
    74                                         
    75   2819.5 MiB      0.0 MiB           1           if overwrite == 'force_silent_overwrite':
    76                                                     self.erase(force_silent=True)
    77   2819.5 MiB      0.0 MiB           1           elif overwrite is True:
    78   2819.5 MiB      0.0 MiB           1               self.erase()
    79                                         
    80   2819.5 MiB      0.0 MiB           1           if index_does_not_exist or overwrite != 'reuse':
    81   2965.0 MiB    145.5 MiB           1               self.__launch(collection)
    82                                         
    83   2965.0 MiB      0.0 MiB           1           return self.index_path


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   198   2819.5 MiB   2819.5 MiB           1           @profile
   199                                                 def _index_with_profiling(indexer, name, collection, overwrite):
   200   2965.0 MiB    145.5 MiB           1               return indexer.index(name=name, collection=collection, overwrite=overwrite)


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   162   1835.1 MiB   1835.1 MiB           1       @profile
   163                                             def build(
   164                                                 self,
   165                                                 checkpoint: Union[str, Path],
   166                                                 collection: List[str],
   167                                                 index_name: Optional["str"] = None,
   168                                                 overwrite: Union[bool, str] = "reuse",
   169                                                 verbose: bool = True,
   170                                                 **kwargs,
   171                                             ) -> "PLAIDModelIndex":
   172                                                 
   173   1835.1 MiB      0.0 MiB           1           bsize = kwargs.get("bsize", PLAIDModelIndex._DEFAULT_INDEX_BSIZE)
   174   1835.1 MiB      0.0 MiB           1           assert isinstance(bsize, int)
   175                                         
   176   1835.1 MiB      0.0 MiB           1           nbits = 2
   177   1835.1 MiB      0.0 MiB           1           if len(collection) < 10000:
   178                                                     nbits = 4
   179   1835.1 MiB      0.0 MiB           2           self.config = ColBERTConfig.from_existing(
   180   1835.1 MiB      0.0 MiB           1               self.config, ColBERTConfig(nbits=nbits, index_bsize=bsize)
   181                                                 )
   182                                         
   183                                                 # Instruct colbert-ai to disable forking if nranks == 1
   184   1835.1 MiB      0.0 MiB           1           self.config.avoid_fork_if_possible = True
   185                                         
   186   1835.1 MiB      0.0 MiB           1           if len(collection) > 100000:
   187                                                     self.config.kmeans_niters = 4
   188   1835.1 MiB      0.0 MiB           1           elif len(collection) > 50000:
   189                                                     self.config.kmeans_niters = 10
   190                                                 else:
   191   1835.1 MiB      0.0 MiB           1               self.config.kmeans_niters = 20
   192                                         
   193                                                 # Monkey-patch colbert-ai to avoid using FAISS
   194   1835.1 MiB      0.0 MiB           1           monkey_patching = (
   195   1835.1 MiB      0.0 MiB           1               len(collection) < 75000 and kwargs.get("use_faiss", False) is False
   196                                                 )
   197                                         
   198   2819.5 MiB      0.0 MiB           3           @profile
   199   1835.1 MiB      0.0 MiB           1           def _index_with_profiling(indexer, name, collection, overwrite):
   200   2992.6 MiB   1303.0 MiB           2               return indexer.index(name=name, collection=collection, overwrite=overwrite)
   201                                                     
   202   1835.1 MiB      0.0 MiB           1           if monkey_patching:
   203   1835.1 MiB      0.0 MiB           2               print(
   204   1835.1 MiB      0.0 MiB           1                   "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----"
   205                                                     )
   206   1835.1 MiB      0.0 MiB           1               print("This is a behaviour change from RAGatouille 0.8.0 onwards.")
   207   1835.1 MiB      0.0 MiB           2               print(
   208   1835.1 MiB      0.0 MiB           1                   "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations."
   209                                                     )
   210   1835.1 MiB      0.0 MiB           2               print(
   211   1835.1 MiB      0.0 MiB           1                   "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour."
   212                                                     )
   213   1835.1 MiB      0.0 MiB           1               print("--------------------")
   214   1835.1 MiB      0.0 MiB           1               CollectionIndexer._train_kmeans = self.pytorch_kmeans
   215                                         
   216                                                     # Try to keep runtime stable -- these are values that empirically didn't degrade performance at all on 3 benchmarks.
   217                                                     # More tests required before warning can be removed.
   218   1835.1 MiB      0.0 MiB           1               try:
   219   1835.1 MiB      0.0 MiB           2                   indexer = Indexer(
   220   1835.1 MiB      0.0 MiB           1                       checkpoint=checkpoint,
   221   1835.1 MiB      0.0 MiB           1                       config=self.config,
   222   1835.1 MiB      0.0 MiB           1                       verbose=verbose,
   223                                                         )
   224   1835.1 MiB      0.0 MiB           1                   indexer.configure(avoid_fork_if_possible=True)
   225   2992.6 MiB      0.0 MiB           1                   _index_with_profiling(indexer, index_name, collection, overwrite)
   226   2992.6 MiB      0.0 MiB           1               except Exception as err:
   227   2992.6 MiB      0.0 MiB           2                   print(
   228   2992.6 MiB      0.0 MiB           1                       f"PyTorch-based indexing did not succeed with error: {err}",
   229   2992.6 MiB      0.0 MiB           1                       "! Reverting to using FAISS and attempting again...",
   230                                                         )
   231   2819.5 MiB   -173.1 MiB           1                   monkey_patching = False
   232   2819.5 MiB      0.0 MiB           1           if monkey_patching is False:
   233   2819.5 MiB      0.0 MiB           1               CollectionIndexer._train_kmeans = self.faiss_kmeans
   234   2819.5 MiB      0.0 MiB           1               if torch.cuda.is_available():
   235   2819.5 MiB      0.0 MiB           1                   import faiss
   236                                         
   237   2819.5 MiB      0.0 MiB           1                   if not hasattr(faiss, "StandardGpuResources"):
   238                                                             print(
   239                                                                 "________________________________________________________________________________\n"
   240                                                                 "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
   241                                                                 "This means that indexing will be slow. To make use of your GPU.\n"
   242                                                                 "Please install `faiss-gpu` by running:\n"
   243                                                                 "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
   244                                                                 "________________________________________________________________________________",
   245                                                             )
   246                                                             print("Will continue with CPU indexing in 5 seconds...")
   247                                                             time.sleep(5)
   248   2819.5 MiB      0.0 MiB           2               indexer = Indexer(
   249   2819.5 MiB      0.0 MiB           1                   checkpoint=checkpoint,
   250   2819.5 MiB      0.0 MiB           1                   config=self.config,
   251   2819.5 MiB      0.0 MiB           1                   verbose=verbose,
   252                                                     )
   253   2819.5 MiB      0.0 MiB           1               indexer.configure(avoid_fork_if_possible=True)
   254   2965.0 MiB    -27.7 MiB           1               _index_with_profiling(indexer, index_name, collection, overwrite)
   255                                         
   256   2965.0 MiB      0.0 MiB           1           return self


Filename: /home/RAGatouille/ragatouille/models/index.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   136   1835.1 MiB   1835.1 MiB           1       @staticmethod
   137                                             @profile
   138                                             def construct(
   139                                                 config: ColBERTConfig,
   140                                                 checkpoint: Union[str, Path],
   141                                                 collection: List[str],
   142                                                 index_name: Optional["str"] = None,
   143                                                 overwrite: Union[bool, str] = "reuse",
   144                                                 verbose: bool = True,
   145                                                 **kwargs,
   146                                             ) -> "PLAIDModelIndex":
   147   2965.0 MiB   1129.9 MiB           3           return PLAIDModelIndex(config).build(
   148   1835.1 MiB      0.0 MiB           2               checkpoint, collection, index_name, overwrite, verbose, **kwargs
   149                                                 )


Filename: /home/RAGatouille/ragatouille/models/colbert.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   342   1835.1 MiB   1835.1 MiB           1           @profile
   343                                                 def _construct_model_index(*args, **kwargs):
   344   2965.0 MiB   1129.9 MiB           1               return ModelIndexFactory.construct(*args, **kwargs)


Done indexing!
Filename: /home/RAGatouille/ragatouille/RAGPretrainedModel.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   205   1835.1 MiB   1835.1 MiB           1           @profile
   206                                                 def _model_index_profiled(*args, **kwargs):
   207   2965.0 MiB   1129.9 MiB           1               return self.model.index(*args, **kwargs)


Filename: ../ragatouille_index_10k_False.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    11   1770.5 MiB   1770.5 MiB           1   @profile
    12                                         def _index(): 
    13   2965.0 MiB   1179.5 MiB           2       return RAG.index(
    14   1770.5 MiB      0.0 MiB           1           index_name=f"{dataset_name}_index", 
    15   1780.5 MiB     10.0 MiB           1           collection=passages[:ndocs]["text"], 
    16   1785.5 MiB      5.0 MiB           1           document_ids=passages[:ndocs]["_id"],
    17   1785.5 MiB      0.0 MiB           1           use_faiss=False
    18                                             )



