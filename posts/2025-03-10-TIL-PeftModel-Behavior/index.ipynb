{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: TIL&#58; PeftModel Base Model Behavior\n",
        "date: \"2025-03-10\"\n",
        "author: Vishal Bakshi\n",
        "description: TIL that the base model gets altered (merged) after you call PeftModel.pretrained to load LoRA adapter weights. I compare weight matrices and analyze memory usage.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "    - machine learning\n",
        "    - LLM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQWWJaKm1x_6"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUHIM18A1zF6"
      },
      "source": [
        "In this TIL blog post I share some unexpected behavior when using `PeftModel`. In short, when merging LoRA adapter weights with the base model, the base model gets overwritten. While unexpected, in hindsight this makes sense if you want to minimize memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u8XpCNQa18Nq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import os\n",
        "import torch\n",
        "import psutil\n",
        "import copy\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v58dTeMx2KIl"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "os.environ['HUGGING_FACE_HUB_TOKEN'] = userdata.get('HUGGING_FACE_HUB_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "steqE5gl8NVm"
      },
      "outputs": [],
      "source": [
        "def _mem(): print(f\"RAM Usage: {psutil.virtual_memory().percent}% (Used: {psutil.virtual_memory().used / (1024**3):.2f} GB / Total: {psutil.virtual_memory().total / (1024**3):.2f} GB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juJXnZIP8U7R"
      },
      "source": [
        "## Merging LoRA Adapter Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3cMzcsY8aMx"
      },
      "source": [
        "Before loading any model, here is the memory usage. I'm using an A100 GPU with Colab Pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrYR4_jN8cxg",
        "outputId": "b461255e-c263-4bba-c06b-4148a74092f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM Usage: 3.5% (Used: 2.10 GB / Total: 83.48 GB)\n"
          ]
        }
      ],
      "source": [
        "_mem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deen0jJk2MGf"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\").to(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Cns5QC8j0h"
      },
      "source": [
        "After loading the base model (Llama2-7B) the memory usage increases to 27GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goDjhaxW85ZE",
        "outputId": "8bf9d682-b07b-4bc0-c632-3081f1f8dce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM Usage: 33.8% (Used: 27.35 GB / Total: 83.48 GB)\n"
          ]
        }
      ],
      "source": [
        "_mem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELnjx4o89Gi3"
      },
      "source": [
        "Loading the LoRA adapter weights increases the memory usage to 28 GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdKZUBkd5msG"
      },
      "outputs": [],
      "source": [
        "model_to_merge = PeftModel.from_pretrained(\n",
        "    model=base_model,\n",
        "    model_id=\"LoRA-TMLR-2024/magicoder-lora-rank-64-alpha-128\"\n",
        ").to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfDuSpSb9LFk",
        "outputId": "c5110313-9aa0-4808-dd9f-36ab1d38cb0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM Usage: 34.8% (Used: 28.22 GB / Total: 83.48 GB)\n"
          ]
        }
      ],
      "source": [
        "_mem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JZAjWo-A52xX"
      },
      "outputs": [],
      "source": [
        "merged_model = model_to_merge.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq3n2ki09WY_"
      },
      "source": [
        "Merging the model essentially keeps the memory usage constant at 28GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUaOcQD99QO_",
        "outputId": "53481002-dcec-4742-84c1-ed3e7770493f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM Usage: 34.9% (Used: 28.28 GB / Total: 83.48 GB)\n"
          ]
        }
      ],
      "source": [
        "_mem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOzvG4B89ujz"
      },
      "source": [
        "## Comparing `base_model` and `merged_model` Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEmkxtxV9xjE"
      },
      "source": [
        "However, saving memory comes at a cost! You no longer have access to the base model. I'll first do a visual inspection of one of the weight matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBlZHvJV5_c_",
        "outputId": "71d52a31-eed0-4dec-f48c-bf132f221ed5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0020, -0.0156,  0.0023,  ...,  0.0098, -0.0017, -0.0031],\n",
              "        [ 0.0283, -0.0176,  0.0062,  ..., -0.0076,  0.0004,  0.0087],\n",
              "        [-0.0230,  0.0225,  0.0001,  ...,  0.0028,  0.0190, -0.0063],\n",
              "        ...,\n",
              "        [ 0.0003,  0.0016, -0.0013,  ...,  0.0081, -0.0308,  0.0110],\n",
              "        [ 0.0259,  0.0203,  0.0045,  ..., -0.0310, -0.0147, -0.0111],\n",
              "        [-0.0077, -0.0174,  0.0012,  ...,  0.0182,  0.0181, -0.0070]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.model.layers[0].self_attn.q_proj.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSm3kPHk6Rib",
        "outputId": "9b2f1df8-7c78-4f23-ef3b-77c2b981ff99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0020, -0.0156,  0.0023,  ...,  0.0098, -0.0017, -0.0031],\n",
              "        [ 0.0283, -0.0176,  0.0062,  ..., -0.0076,  0.0004,  0.0087],\n",
              "        [-0.0230,  0.0225,  0.0001,  ...,  0.0028,  0.0190, -0.0063],\n",
              "        ...,\n",
              "        [ 0.0003,  0.0016, -0.0013,  ...,  0.0081, -0.0308,  0.0110],\n",
              "        [ 0.0259,  0.0203,  0.0045,  ..., -0.0310, -0.0147, -0.0111],\n",
              "        [-0.0077, -0.0174,  0.0012,  ...,  0.0182,  0.0181, -0.0070]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_model.model.layers[0].self_attn.q_proj.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO5ldEOt-AXs"
      },
      "source": [
        "Both matrices are equal. Analyzing weight matrix differences more systematically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W6rDImNl-Ft3"
      },
      "outputs": [],
      "source": [
        "def _diffs(model1, model2):\n",
        "    n_diff = 0\n",
        "    for layer_idx in range(32):\n",
        "        for component in [\"q_proj\", \"k_proj\", \"o_proj\", \"v_proj\"]:\n",
        "            W1 = getattr(model1.model.layers[layer_idx].self_attn, component).weight\n",
        "            W2= getattr(model2.model.layers[layer_idx].self_attn, component).weight\n",
        "            if not torch.allclose(W1, W2, rtol=1e-5, atol=1e-8): n_diff += 1\n",
        "    print(f\"Different Self-Attention Matrices: {n_diff}\")\n",
        "    n_diff = 0\n",
        "    for layer_idx in range(32):\n",
        "        for component in [\"up_proj\", \"down_proj\", \"gate_proj\"]:\n",
        "            W1 = getattr(model1.model.layers[layer_idx].mlp, component).weight\n",
        "            W2 = getattr(model2.model.layers[layer_idx].mlp, component).weight\n",
        "            if not torch.allclose(W1, W2, rtol=1e-5, atol=1e-8): n_diff += 1\n",
        "    print(f\"Different MLP Weight Matrices: {n_diff}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJoyLWmX-KWv",
        "outputId": "3725270e-f6e2-494a-ea46-28c2fa835116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Different Self-Attention Matrices: 0\n",
            "Different MLP Weight Matrices: 0\n"
          ]
        }
      ],
      "source": [
        "_diffs(base_model, merged_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Sr9tzt-QTe"
      },
      "source": [
        "For both self-attention and MLP modules, all weight matrices between the `base_model` and the `merged_model` are the same. Using the `is` operator we can see that they reference the same object in memory (which is where the memory savings come from):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgtN3PUA-hyA",
        "outputId": "f6d7747c-a8d9-4c32-8172-0b6ba59831ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model is merged_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6KME_U7-r6n"
      },
      "source": [
        "## Copying the Base Model for Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ejR762c-t2t"
      },
      "source": [
        "I'll now load the base model again to compare with the merged model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY7SL6l9_frS",
        "outputId": "7cefff4e-025f-4b4c-bff5-09676d8af019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM Usage: 35.4% (Used: 28.68 GB / Total: 83.48 GB)\n"
          ]
        }
      ],
      "source": [
        "_mem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zbqYEop8_g1a"
      },
      "outputs": [],
      "source": [
        "del base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmELEzfo_mLT",
        "outputId": "213ac2ca-846f-4460-cce6-4b4bed5ad05f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "483"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry23fz5o_kz6",
        "outputId": "8a10f443-157a-456a-cb0a-34e11771d03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM Usage: 35.5% (Used: 28.78 GB / Total: 83.48 GB)\n"
          ]
        }
      ],
      "source": [
        "_mem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur7rfw4U_vgb"
      },
      "source": [
        "Note that deleting the base model did not change the memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ee5aafe00d32446f95ab002be2ec8fcc",
            "f9dc6aaa7eb44ef699a9c26254ccc8aa",
            "8c9a09c8f29e4fdea46c3ec12e7634c0",
            "a799cc447e60429aa2addf21a79da9b6",
            "213bb6bdae6f4ab2a0811e0f334106be",
            "a16b5de3f2c54549b2578d69750d7dba",
            "ae93858484fe4286bf3b8f570e62251d",
            "7096f5afe8854430830178d1bdff1d81",
            "71cf14119af546dea789a63280d18599",
            "b815c09f54cc419ebf7b4a6934570a8e",
            "8816784cd0cb4a27a8931d8d4c3ff2f9"
          ]
        },
        "id": "NC-0W2UG_y-D",
        "outputId": "74341ad5-ee28-474b-d505-0a5f04e140e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee5aafe00d32446f95ab002be2ec8fcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\").to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx4wj9Af_6L4",
        "outputId": "faa5c925-2fdc-45b9-b943-07f8baf2e19c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM Usage: 65.7% (Used: 53.94 GB / Total: 83.48 GB)\n"
          ]
        }
      ],
      "source": [
        "_mem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv3d4MHC_7Tl"
      },
      "source": [
        "With a new base model loaded, the memory usage jumps up to 54 GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yqfAHuL8opn",
        "outputId": "83d16db7-457a-4ca2-c6a9-f24e099d27c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Different Self-Attention Matrices: 128\n",
            "Different MLP Weight Matrices: 96\n"
          ]
        }
      ],
      "source": [
        "_diffs(base_model, merged_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_aP2xwUAJwy"
      },
      "source": [
        "There are 32 layers in this Llama model, and each model's self-attention module has 4 weight matrices we are comparing, resulting in 128 matrices in total. The MLP module has 3 weight matrices we are comparing, resulting in 96 total across the model. The base model and merged model are fully different models (in terms of weight matrix values)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jg6rVjGAx7u"
      },
      "source": [
        "## Using `.get_base_model`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Y-Sgw4A1Ym"
      },
      "source": [
        "Looking at the `PeftModel` documentation, I noted the method `get_base_model` which seems relevant to this exercise. However, using that method results in the same weights as the merged model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "iml0zQCnBENO",
        "outputId": "6ba3da16-ba96-4c24-9b8b-d0abcff91230"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>peft.peft_model.PeftModel.get_base_model</b><br/>def get_base_model() -&gt; torch.nn.Module</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/peft/peft_model.py</a>Returns the base model.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 912);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "<bound method PeftModel.get_base_model of PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_to_merge.get_base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV1V-6NOBAWf",
        "outputId": "28cc252a-570d-41b6-cecd-2a7dbbab953d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Different Self-Attention Matrices: 0\n",
            "Different MLP Weight Matrices: 0\n"
          ]
        }
      ],
      "source": [
        "_diffs(merged_model, model_to_merge.get_base_model())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBOWI4LAAi_f"
      },
      "source": [
        "---\n",
        "\n",
        "I am planning to do more of these short TIL blog posts this year! It helps me solidify concepts as I come across them. I hope you enjoyed this blog post!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "213bb6bdae6f4ab2a0811e0f334106be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7096f5afe8854430830178d1bdff1d81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71cf14119af546dea789a63280d18599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8816784cd0cb4a27a8931d8d4c3ff2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c9a09c8f29e4fdea46c3ec12e7634c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7096f5afe8854430830178d1bdff1d81",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71cf14119af546dea789a63280d18599",
            "value": 2
          }
        },
        "a16b5de3f2c54549b2578d69750d7dba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a799cc447e60429aa2addf21a79da9b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b815c09f54cc419ebf7b4a6934570a8e",
            "placeholder": "​",
            "style": "IPY_MODEL_8816784cd0cb4a27a8931d8d4c3ff2f9",
            "value": " 2/2 [00:05&lt;00:00,  2.50s/it]"
          }
        },
        "ae93858484fe4286bf3b8f570e62251d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b815c09f54cc419ebf7b4a6934570a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee5aafe00d32446f95ab002be2ec8fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9dc6aaa7eb44ef699a9c26254ccc8aa",
              "IPY_MODEL_8c9a09c8f29e4fdea46c3ec12e7634c0",
              "IPY_MODEL_a799cc447e60429aa2addf21a79da9b6"
            ],
            "layout": "IPY_MODEL_213bb6bdae6f4ab2a0811e0f334106be"
          }
        },
        "f9dc6aaa7eb44ef699a9c26254ccc8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a16b5de3f2c54549b2578d69750d7dba",
            "placeholder": "​",
            "style": "IPY_MODEL_ae93858484fe4286bf3b8f570e62251d",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
