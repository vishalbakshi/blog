{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: TinyScaleLab Update&#58; Training Cost Analysis and Evaluation Infrastructure Plans\n",
        "date: \"2025-04-27\"\n",
        "author: Vishal Bakshi\n",
        "description: I share my progress on the TinyScaleLab project where I'm studying small language models trained on the Tiny Stories dataset. I discuss the architecture of my four model sizes (5M, 25M, 60M, and 125M parameters), compare training costs between L4 and A100 GPUs, and outline my plans for developing an evaluation framework using LLM judges. This project aims to investigate both the training dynamics of tiny models and their language capabilities across grammar, context tracking, factual knowledge, reasoning, creativity, and storytelling.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - LLM\n",
        "    - deep learning\n",
        "    - TinyScaleLab\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9J3qHIS7XhZ"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqi_GTcC7Yl0"
      },
      "source": [
        "In this notebook I'll share results from some quick and dirty training runs executed so I get a rough but reasonable estimate of training time and costs using L4 and A100 GPUs on Google Colab Pro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW-JVcyy7ktx"
      },
      "source": [
        "## Model Sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rZh2h6a7ner"
      },
      "source": [
        "In these experiments, I'm training four model sizes: 5M, 25M, 60M and 125M. I've chosen to roughly follow the TinyStories models, using the hidden dimension and intermediate dimension for the TinyStories-1M, -8M, -28M and -33M models, in each case there is a 4x increase from hidden to intermediate dimension in the MLP layers. These are just initial architectural choices which might change over the course of the project as I learn more about what results in a better performing model.\n",
        "\n",
        "For now, I'm using 8 attention heads (for all models) and the Llama-2 tokenizer with a 32000 vocab size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1ZrEDqG8YVY"
      },
      "source": [
        "|Model Name|Hidden Dim|Intermediate Dim|Number of Layers|Number of Params|\n",
        "|:-:|:-:|:-:|:-:|:-:|\n",
        "|5M|64|256|13|4_949_696\n",
        "|25M|256|1024|8|24_776_960\n",
        "|60M|512|2048|6|57_940_480\n",
        "|125M|768|3072|8|124_662_528"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McJSxmZU_KV7"
      },
      "source": [
        "## Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVTvxJYE_MBp"
      },
      "source": [
        "I've tokenized the [TinyStories_all_data.tar.gz](https://huggingface.co/datasets/roneneldan/TinyStories/blob/main/TinyStories_all_data.tar.gz) dataset which contains 4.9M stories generated by GPT3.5 and GPT4, using the `meta-llama/Llama-2-7b-hf` tokenizer. I haven't performed any data cleaning (yet). The total number of tokens in this dataset is little over 1B: **1_028_013_532**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uok_BJoO_o9O"
      },
      "source": [
        "## Training Duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn5nmvq9_qD1"
      },
      "source": [
        "I'm training all initial runs for 1 epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdchxlfs_tr-"
      },
      "source": [
        "## Training GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gleFkU6_us2"
      },
      "source": [
        "I trained the 5M and 25M models on both L4 (22.5 GB VRAM) and A100 (40GB VRAM) GPUs. I trained the 60M and 125M models on the A100 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89IFa2Jn_6tX"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O17RhjOlAloN"
      },
      "source": [
        "All models are trained for 1 epoch (1.03 tokens):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJnunH47_8H3"
      },
      "source": [
        "| Hardware | Model Size | Time (hr) | Batch Size | Max Memory | Cost |\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "| L4 | 5M | 0.87 | 384 | 20% | \\$0.18 |\n",
        "| L4 | 25M | 1.45 | 288 | 65% | \\$0.30 |\n",
        "| A100-40GB | 5M | 0.32 | 2048 | 78% | \\$0.25 |\n",
        "| A100-40GB | 25M | 0.35 | 1536 | 98% | \\$0.27 |\n",
        "| A100-40GB | 60M | 0.54 | 1152 | 86% | \\$0.41 |\n",
        "| A100-40GB | 125M | 1.10| 512 | 99% | \\$0.84 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KqyvwdFBHVg"
      },
      "source": [
        "## Takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1D0NLqUAq7A"
      },
      "source": [
        "From this analysis, only the 5M model makes sense to train on the L4. It's 3 cents cheaper per hour to train the 25M model on the A100, though I'm flirting with OOM so I should reduce the batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpIEqFaLBRr0"
      },
      "source": [
        "I'll need to perform longer trainings to get a sense of how many full epochs I need to produce coherent language-generating models, but from my TinyHackathon experience, it took 20 epochs for the 60M model to perform decently (3/5 LLM Judge overall score). I would expect the 125M model to require less epochs, and the smaller models more epochs, to achieve comparable performance. But we'll see!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2ptWyWhBtl1"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw5XRuBlBuzN"
      },
      "source": [
        "Here are the `LlamaConfig` objects for each model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX-OqPxCBx1t"
      },
      "source": [
        "### 5M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcQBcY-YBy6F"
      },
      "source": [
        "```python\n",
        "config = LlamaConfig(\n",
        "    vocab_size=32000,\n",
        "    hidden_size=64,\n",
        "    intermediate_size=256,\n",
        "    num_hidden_layers=13,\n",
        "    num_attention_heads=8,\n",
        "    max_position_embeddings=512,\n",
        "    rope_theta=10000.0,\n",
        "    attention_bias=False,\n",
        "    mlp_bias=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=\"bfloat16\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtL2VrwdB2do"
      },
      "source": [
        "### 25M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYaAZgbBB3Rl"
      },
      "source": [
        "```python\n",
        "config = LlamaConfig(\n",
        "    vocab_size=32000,\n",
        "    hidden_size=256,\n",
        "    intermediate_size=1024,\n",
        "    num_hidden_layers=8,\n",
        "    num_attention_heads=8,\n",
        "    max_position_embeddings=512,\n",
        "    rope_theta=10000.0,\n",
        "    attention_bias=False,\n",
        "    mlp_bias=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=\"bfloat16\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxUk7CjqB6lY"
      },
      "source": [
        "### 60M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY3_36yqB7Sf"
      },
      "source": [
        "```python\n",
        "config = LlamaConfig(\n",
        "    vocab_size=32000,\n",
        "    hidden_size=512,\n",
        "    intermediate_size=2048,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    max_position_embeddings=512,\n",
        "    rope_theta=10000.0,\n",
        "    attention_bias=False,\n",
        "    mlp_bias=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=\"bfloat16\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGeoc19nB-tI"
      },
      "source": [
        "### 125M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g9xqqpDB_og"
      },
      "source": [
        "```python\n",
        "config = LlamaConfig(\n",
        "    vocab_size=32000,\n",
        "    hidden_size=768,\n",
        "    intermediate_size=3072,\n",
        "    num_hidden_layers=8,\n",
        "    num_attention_heads=8,\n",
        "    max_position_embeddings=512,\n",
        "    rope_theta=10000.0,\n",
        "    attention_bias=False,\n",
        "    mlp_bias=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=\"bfloat16\"\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
