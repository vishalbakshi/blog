{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Debugging Flash Attention in LLM-Foundry (and a 20% Slow Down!)\n",
        "date: \"2025-06-30\"\n",
        "author: Vishal Bakshi\n",
        "description: flash_attn_varlen_func in LLM Foundry resulted in a surprising 20% training slowdown. This post details the debugging process that uncovered the cause &#58; significant overhead from the HuggingFace implementation repeatedly un-padding and re-padding the batch at every layer.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "    - LLM-Foundry\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDSRUzUNjXHm"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbybNIv4sfhf"
      },
      "source": [
        "I'm learning a lot about LLM-Foundry while working on a group research project. In this blog post I'll walk through how we figured out two things:\n",
        "\n",
        "1. LLM-Foundry, by default when using a HuggingFace LlamaModel, does not use `flash_attn_varlen_func` and instead uses `flash_attn_func`. In other words, it doesn't unpad the batch by default.\n",
        "2. When forcing LLM-Foundry to use `flash_attn_varlen_func`, it slows down training time.\n",
        "\n",
        "I'll start by walking through the forward pass of the HuggingFace `LlamaModel` down to the attention mechanism which uses the `_flash_attention_forward` utility function which decides which Flash Attention interface is being used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BWO5guW7Kl4?si=l30jswKSmduhzd2g\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYoi2J3a1TOE"
      },
      "source": [
        "## What is the value of the `attention_mask`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a528MMN1W1w"
      },
      "source": [
        "The model we're using is SmolLM2-135M which uses the now-deprecated [`LlamaFlashAttention2` module](https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src/transformers/models/llama/modeling_llama.py#L324)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyRlB4Ps1UMl"
      },
      "source": [
        "Inspecting the [`LlamaModel` forward pass](https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src/transformers/models/llama/modeling_llama.py#L828), the first instance of where the `attention_mask` is used:\n",
        "\n",
        "```python\n",
        "causal_mask = self._update_causal_mask(\n",
        "    attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65hhTBUoJtkf"
      },
      "source": [
        "Looking at `_update_causal_mask`:\n",
        "\n",
        "```python\n",
        "if self.config._attn_implementation == \"flash_attention_2\":\n",
        "    if attention_mask is not None and 0.0 in attention_mask:\n",
        "        return attention_mask\n",
        "    return None\n",
        "```\n",
        "\n",
        "If `0.0 in attention_mask` then the `attention_mask` will be returned, other `None` is returned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BlZlN5vKLhw"
      },
      "source": [
        "We'll come back to this point later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b74XjbP_MUu6"
      },
      "source": [
        "## When is `flash_attn_varlen_func` called?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlKFuPnoKOJv"
      },
      "source": [
        "Let's assume `0.0 in attention_mask` is `True`, so the `attention_mask` is kept as is and is passed onto the `LlamaDecoderLayer` and eventually the attention mechanism [which calls `_flash_attention_forward`](https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src/transformers/models/llama/modeling_llama.py#L414). `flash_attention_forward` is [defined in transformers/modeling_flash_utils.py](https://github.com/huggingface/transformers/blob/ea0ea392e57f8816f9ab8e5f740577a0343a1594/src/transformers/modeling_flash_attention_utils.py#L409), and triggers the use of `flash_attn_varlen_func` if one of two conditions are true:\n",
        "\n",
        "```python\n",
        "if attention_mask is not None\n",
        "```\n",
        "\n",
        "or\n",
        "\n",
        "```python\n",
        "elif (\n",
        "    position_ids is not None\n",
        "    and query_states.shape[0] == 1\n",
        "    and (max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all()))\n",
        ")\n",
        "```\n",
        "\n",
        "The `elif` condition is `True` if `position_ids is not None` and `query_states.shape[0] == 1` and either `max_length_q is not None` or `(query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())`. The `torch.diff` expression is `False` if the difference in consecutive values in `position_ids` are not greater than `0`. For example, the following `position_ids` would yield `False`:\n",
        "\n",
        "```python\n",
        "[0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 0, 1, 2]\n",
        "```\n",
        "\n",
        "`torch.diff` for the 4th and 5th position (`3` to `0`) is `-3`. We would expect such a `position_ids` sequence when you have packed sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jADjsb76Xdvs"
      },
      "source": [
        "## How do we check the value of `attention_mask` during training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2DJFrowXjSQ"
      },
      "source": [
        "To do so, I wrote the following Composer callback:\n",
        "\n",
        "```python\n",
        "class FlashAttentionDebug(Callback):\n",
        "  def before_forward(self, state: State, logger: Logger) -> None:\n",
        "      model = state.model\n",
        "      print(model.config._attn_implementation)\n",
        "      self.hooks = []\n",
        "\n",
        "      def create_hook_fn(name):\n",
        "          def hook_fn(module, args, kwargs, output):\n",
        "              if 'attention_mask' in kwargs:\n",
        "                  print(f\"{name} FlashAttentionDebug: attention_mask is None:\", kwargs['attention_mask'] is None)\n",
        "                  if kwargs['attention_mask'] is not None:\n",
        "                      print(f\"{name} FlashAttentionDebug: attention_mask:\", kwargs['attention_mask'])\n",
        "                      print(f\"{name} FlashAttentionDebug: 0.0 in attention_mask:\", 0.0 in kwargs['attention_mask'])\n",
        "                      print(f\"{name} FlashAttentionDebug: attention_mask.shape:\", kwargs['attention_mask'].shape)\n",
        "                      print(f\"{name} FlashAttentionDebug: attention_mask.sum():\", kwargs[\"attention_mask\"].sum())\n",
        "          return hook_fn\n",
        "\n",
        "      attn_layer = model.model.base_model.model.model.layers[0].self_attn\n",
        "      hook_handle = attn_layer.register_forward_hook(create_hook_fn(\"attn_layer\"), with_kwargs=True)\n",
        "      self.hooks.append(hook_handle)\n",
        "\n",
        "      decoder_layer = model.model.base_model.model.model.layers[0]\n",
        "      print(type(decoder_layer))\n",
        "      hook_handle = decoder_layer.register_forward_hook(create_hook_fn(\"decoder_layer\"), with_kwargs=True)\n",
        "      self.hooks.append(hook_handle)\n",
        "\n",
        "      _model = model.model.base_model.model.model\n",
        "      print(type(_model))\n",
        "      hook_handle = _model.register_forward_hook(create_hook_fn(\"model\"), with_kwargs=True)\n",
        "      self.hooks.append(hook_handle)\n",
        "\n",
        "  def after_forward(self, state: State, logger: Logger) -> None:\n",
        "      for hook in self.hooks:\n",
        "          hook.remove()\n",
        "      self.hooks = []\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlRhq322X65t"
      },
      "source": [
        "`create_hook_fn` is a closure which returns `hook_fn`. I used this pattern so I could log the name of the module the hook is attached to. Note that when using `register_forward_hook` you must specify `with_kwargs=True` to pass kwargs to the hook function.\n",
        "\n",
        "Here are the outputs when using the default LLM-Foundry pretraining setup:\n",
        "\n",
        "```\n",
        "flash_attention_2\n",
        "<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
        "<class 'transformers.models.llama.modeling_llama.LlamaModel'>\n",
        "attn_layer FlashAttentionDebug: attention_mask is None: True\n",
        "decoder_layer FlashAttentionDebug: attention_mask is None: True\n",
        "model FlashAttentionDebug: attention_mask is None: False\n",
        "model FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')\n",
        "model FlashAttentionDebug: 0.0 in attention_mask: False\n",
        "model FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])\n",
        "model FlashAttentionDebug: attention_mask.sum(): tensor(2048, device='cuda:0')\n",
        "```\n",
        "\n",
        "Note that in the attention layer, `attention_mask` is `None` because as we can see in the `model` forward output, `0.0` is not in `attention_mask` (it's full of `1`s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLTtA63oYr0M"
      },
      "source": [
        "## How do we create an `attention_mask` with `0.0`s?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSjlQuSaYyJ3"
      },
      "source": [
        "With the help of Cursor (my first time using it!) I was able to add one simple line to the `__call__` method of the default pretraining collator `ConcatenatedSequenceCollatorWrapper`:\n",
        "\n",
        "```python\n",
        "batch['attention_mask'] = (batch['input_ids'] != 0).long()\n",
        "```\n",
        "\n",
        "Where `input_ids` are not `0` (the EOS token id used for padding) `attention_mask` will be `1`; it will be `0` where there are padding tokens.\n",
        "\n",
        "Since I'm using Modal for training, and since the image brings down our LLM-Foundry fork, and since I need to modify the `ConcatenatedSequenceCollatorWrapper.__call__` method (which lives in `llmfoundry/data/text_data.py`) I add the following line after my Modal is built:\n",
        "\n",
        "```python\n",
        "image = image.add_local_file(\"text_data.py\", \"/llm-foundry/llmfoundry/data/text_data.py\")\n",
        "```\n",
        "\n",
        "Running training with this modified collator the `FlashAttentionDebug` callback logs the following:\n",
        "\n",
        "```\n",
        "flash_attention_2\n",
        "<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
        "<class 'transformers.models.llama.modeling_llama.LlamaModel'>\n",
        "attn_layer FlashAttentionDebug: attention_mask is None: False\n",
        "attn_layer FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
        "attn_layer FlashAttentionDebug: 0.0 in attention_mask: True\n",
        "attn_layer FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])\n",
        "attn_layer FlashAttentionDebug: attention_mask.sum(): tensor(201, device='cuda:0')\n",
        "decoder_layer FlashAttentionDebug: attention_mask is None: False\n",
        "decoder_layer FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
        "decoder_layer FlashAttentionDebug: 0.0 in attention_mask: True\n",
        "decoder_layer FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])\n",
        "decoder_layer FlashAttentionDebug: attention_mask.sum(): tensor(201, device='cuda:0')\n",
        "model FlashAttentionDebug: attention_mask is None: False\n",
        "model FlashAttentionDebug: attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
        "model FlashAttentionDebug: 0.0 in attention_mask: True\n",
        "model FlashAttentionDebug: attention_mask.shape: torch.Size([1, 2048])\n",
        "model FlashAttentionDebug: attention_mask.sum(): tensor(201, device='cuda:0')\n",
        "```\n",
        "\n",
        "Now we can see that in the attention layer, the `attention_mask` is not `None`. It contains `0.0` values (note how the `sum`, 201, is less than the sequence length of 2048) which is why the `_update_causal_mask` method returned attention_mask as is. We can also visually inspect the `attention_mask` tensor in the model, decoder layer and attention mechanism forward pass and see both `1`s and `0`s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSb4qY8XcH7e"
      },
      "source": [
        "## How do we know if `flash_attn_varlen_func` is being used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSFlE5QpcMDF"
      },
      "source": [
        "Now that we know that introducing `0`s in the `attention_mask` allows it to be passed through the model, including the attention mechanism, we should confirm that `flash_attn_varlen_func` is called. If you recall, one of the conditions for it being called was that `attention_mask is not None`. To check this, we can monkey-patch `_upad_input` which is the method called to unpad the batch if `attention_mask is not None`:\n",
        "\n",
        "```python\n",
        "import transformers.modeling_flash_attention_utils as flash_utils\n",
        "original_upad_input = flash_utils._upad_input\n",
        "original_prepare_fa2_from_position_ids = flash_utils.prepare_fa2_from_position_ids\n",
        "\n",
        "def debug_upad_input(query_states, key_states, value_states, attention_mask, query_length):\n",
        "    print(\"DEBUG: Using _upad_input\")\n",
        "    print(f\"  query_states: {query_states.shape}\")\n",
        "    print(f\"  key_states: {key_states.shape}\")\n",
        "    print(f\"  value_states: {value_states.shape}\")\n",
        "    print(f\"  attention_mask: {attention_mask.shape}\")\n",
        "    print(f\"  query_length: {query_length}\")\n",
        "    query_layer, key_layer, value_layer, indices_q, (cu_seqlens_q, cu_seqlens_k),(max_seqlen_in_batch_q, max_seqlen_in_batch_k) = original_upad_input(query_states, key_states, value_states, attention_mask, query_length)\n",
        "    print(f\"query_layer.shape: \", query_layer.shape)\n",
        "    print(f\"key_layer.shape: \", key_layer.shape)\n",
        "    print(f\"value_layer.shape: \", value_layer.shape)\n",
        "    print(f\"indices_q.shape: \", indices_q.shape)\n",
        "    print(f\"cu_seqlens_q.shape: \", cu_seqlens_q.shape)\n",
        "    print(f\"cu_seqlens_q: \", cu_seqlens_q.tolist())\n",
        "    print(f\"cu_seqlens_k.shape: \", cu_seqlens_k.shape)\n",
        "    print(f\"cu_seqlens_k: \", cu_seqlens_k.tolist())\n",
        "    print(f\"max_seqlen_in_batch_q: \", max_seqlen_in_batch_q)\n",
        "    print(f\"max_seqlen_in_batch_k: \", max_seqlen_in_batch_k)\n",
        "    print(f\"indices_q: \", indices_q.tolist())\n",
        "    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n",
        "    print(seqlens_in_batch.tolist())\n",
        "    print(attention_mask[0].tolist())\n",
        "    return original_upad_input(query_states, key_states, value_states, attention_mask, query_length)\n",
        "\n",
        "flash_utils._upad_input = debug_upad_input\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlXcIL_Uc4mN"
      },
      "source": [
        "I added a `original_upad_input` call and stored the output so I could see what gets passed on to `flash_attn_varlen_func`.\n",
        "\n",
        "During the training run, with our modified collator, we see the following output (this was for a run with a batch size of 6):\n",
        "\n",
        "```\n",
        "DEBUG: Using _upad_input\n",
        "  query_states: torch.Size([6, 2048, 9, 64])\n",
        "  key_states: torch.Size([6, 2048, 3, 64])\n",
        "  value_states: torch.Size([6, 2048, 3, 64])\n",
        "  attention_mask: torch.Size([6, 2048])\n",
        "  query_length: 2048\n",
        "query_layer.shape:  torch.Size([1395, 9, 64])\n",
        "key_layer.shape:  torch.Size([1395, 3, 64])\n",
        "value_layer.shape:  torch.Size([1395, 3, 64])\n",
        "indices_q.shape:  torch.Size([1395])\n",
        "cu_seqlens_q.shape:  torch.Size([7])\n",
        "cu_seqlens_q:  [0, 220, 437, 732, 915, 1045, 1395]\n",
        "cu_seqlens_k.shape:  torch.Size([7])\n",
        "cu_seqlens_k:  [0, 220, 437, 732, 915, 1045, 1395]\n",
        "max_seqlen_in_batch_q:  350\n",
        "max_seqlen_in_batch_k:  350\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIuzLuXsdgf9"
      },
      "source": [
        "Some key observations:\n",
        "\n",
        "`query_states` has size 6 (batch size) x 2048 (sequence length) x 9 (num heads) x 64 (head dim).\n",
        "\n",
        "`query_layer` (one of the `_upad_input` outputs and `flash_attn_varlen_func` inputs) has size 1395 (total sequence length) x 9 (num heads) x 64 (head dim).\n",
        "\n",
        "The `cu_seqlens_q` that is a critical input to `flash_attn_varlen_func` show us that there are 6 sequences packed together and the \"boundaries\" of the sequences are `[0, 220, 437, 732, 915, 1045, 1395]`. Using my [`DataInspector` callback](https://vishalbakshi.github.io/blog/posts/2025-05-13-DataInspector-BinPackCollator/index.html#datainspector) I confirmed the number of non-padding tokens in the batch: 220, 217, 295, 183, 130, 350. The sum of these counts is 1395, the total sequence length passed to `flash_attn_varlen_func`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7oJWWIyeuAl"
      },
      "source": [
        "## Wait, `flash_attn_varlen_func` slows down training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkENRHDbe5mA"
      },
      "source": [
        "When using my modified collator, and therefore utilizing `flash_attn_varlen_func` the training time slows down by _over 20%_. This was certainly a surprise for me! After discussing this with our research advisor, we learned that this is likely because the HuggingFace implementation of the model unpads and re-pads the batch for each layer.\n",
        "\n",
        "We can see this [in the `_flash_attention_forward` method, after `flash_attn_varlen_func` is called](https://github.com/huggingface/transformers/blob/03db2700abf84971351c7374a548a9d4fc156916/src/transformers/modeling_flash_attention_utils.py#L532):\n",
        "\n",
        "\n",
        "```python\n",
        "attn_output = _pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n",
        "```\n",
        "\n",
        "The solution to mitigating this slow-down is to implement our own custom model where it unpads the batch only once. This will be our next task!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![L4 GPU training runs showig that the non variable length `flash_attn_func` usage results in a faster training. Click image to expand](1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![A100-40GB GPU training runs showig that the non variable length `flash_attn_func` usage results in a faster training. Click image to expand](2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_nUSCzf3Jp"
      },
      "source": [
        "## Closing Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLxNKZHf4fi"
      },
      "source": [
        "The biggest takeaway from this experience, as has been the case for all practical training experiments I've run (whether for LMs or vision models) is that there's a difference between what is theoretically efficient and whether that is practically efficient. In theory, `flash_attn_varlen_func` should be faster because you are not wasting the quadratic attention compute on padding tokens. In practice, unpadding and re-padding the batch for each layer for each forward pass adds an overhead which not only cancels out that attention computation speedup, but slows down the training compared to a fully-padded forward pass. This is a critical lesson I experience again and again, and it helps me understand the value of choosing the right implementation to actualize theoretical efficiencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leXDT-RvrWYf"
      },
      "source": [
        "I'm growing my YouTube channel this year, so if you like this type of content [please subscribe](https://www.youtube.com/@vishal_learner)!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b74XjbP_MUu6"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
