{"cells":[{"attachments":{},"cell_type":"raw","metadata":{},"source":["---\n","title: Fine-tuning a Language Model Using LoRA\n","date: \"2023-09-01\"\n","author: Vishal Bakshi\n","description: In this notebook I want to compare fine-tuning a pretrained model with and without using LoRA.\n","categories:\n","    - deep learning\n","    - python\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Background"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this notebook I want to compare the differences between fine-tuning a pretrained model with and without using LoRA. This exercise is a fastai community study group homework assignment.\n","\n","Here is a comparison of the full-fine-tuning (Full FT) vs. LoRA fine-tuning (LoRA FT) process on the `EleutherAI/pythia-70m` model using the `roneneldan/TinyStoriesInstruct` dataset (which comes from the [TinyStories paper](https://arxiv.org/abs/2305.07759)):\n","\n","|Type|Parameters|Training Set|Validation Set|Perplexity|Batch Size|Epochs|Train Steps|Train Time (Minutes)|\n","|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n","|Full FT|70.4M|240k|60k|8.51|16|3|22500|100|\n","|LoRA FT|98k|256k|64k|12.68|16|4|32000|120|"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Resources\n","\n","- I'll use a small subset of the `roneneldan/TinyStoriesInstruct` dataset from HuggingFace for both trainings since when I use the full dataset I'm getting CUDA out-of-memory errors.\n","- I'm referencing the following to patch together the code in this notebook:\n","    - Jeremy Howard's [Getting started with NLP for absolute beginners\n","](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners) for fundamental setup of data, model, and tokenizer.\n","    - HuggingFace's [Causal language modeling tutorial](https://huggingface.co/docs/transformers/tasks/language_modeling) for updating the tokenizer with a pad token, `data_collator` and training arguments.\n","    - [This forum response](https://discuss.huggingface.co/t/how-to-sample-dataset-according-to-the-index/12940/2) that shows how to select a subset of a dataset with a given set of indexes.\n","    - The TinyStories author's hyperparameters as listed in [their 33M parameter model page](https://huggingface.co/roneneldan/TinyStories-33M)\n","    - HuggingFace's [LoRA Conceptual Guide](https://huggingface.co/docs/peft/conceptual_guides/lora) for steps on how to implement LoRA using `peft`.\n","    - [This blog post](https://www.philschmid.de/fine-tune-flan-t5-peft#3-fine-tune-t5-with-lora-and-bnb-int-8) which walks through an example LoRA training.\n","    - [This forum response by Sylvain Gugger](https://discuss.huggingface.co/t/disable-checkpointing-in-trainer/2335/5) which says to set `save_strategy` to `\"no\"` to avoid the `Trainer` creating checkpoints as I was running into errors around this.\n","   "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Plan of Attack\n","\n","In my first iteration of this exercise (see below) I manually ran multiple different trainings with different models, dataset sizes and training arguments. The code was flexible and easy to update but I through that process I re-ran a lot of cells with different values and lost track a bit exactly the order of things I was running. In this second iteration, I'll create a helper function `get_trainer` which takes various arguments (`model`, `bs`,`tokz`, `train_ds`, etc.) and returns a HuggingFace `Trainer`. This will help clear up some of the redundancy in my code and make it a bit cleaner to read."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:51:27.111684Z","iopub.status.busy":"2023-08-31T19:51:27.110991Z","iopub.status.idle":"2023-08-31T19:51:58.810690Z","shell.execute_reply":"2023-08-31T19:51:58.809683Z","shell.execute_reply.started":"2023-08-31T19:51:27.111646Z"},"trusted":true},"outputs":[],"source":["# all the imports\n","!pip install peft accelerate evaluate -Uqq\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoModelForCausalLM, pipeline\n","from peft import LoraConfig, get_peft_model, TaskType\n","from evaluate import load\n","import math"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## `get_trainer` Helper Function"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This function prepares and returns `Trainer` object for a given model, tokenizer (and tokenize function), training/validation dataset, learning rate, batch size and number of epochs:"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:52:09.000547Z","iopub.status.busy":"2023-08-31T19:52:09.000165Z","iopub.status.idle":"2023-08-31T19:52:09.009015Z","shell.execute_reply":"2023-08-31T19:52:09.007853Z","shell.execute_reply.started":"2023-08-31T19:52:09.000493Z"},"trusted":true},"outputs":[],"source":["def get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr, bs, epochs):\n","    # get tokenized datasets\n","    train_tok_ds = train_ds.map(tok_func, batched=True)\n","    eval_tok_ds = eval_ds.map(tok_func, batched=True)\n","    \n","    # sometimes for whatever reason the datasets are not the right size so checking it here\n","    print(train_tok_ds)\n","    \n","    # not sure what this does but I get an error that the model didn't return a loss value without it\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokz, mlm=False)\n","    \n","    # define training arguments\n","    training_args = TrainingArguments(\n","        output_dir=\"outputs\",\n","        evaluation_strategy=\"epoch\",\n","        learning_rate=lr,\n","        lr_scheduler_type = \"cosine\",\n","        weight_decay=0.1,\n","        per_device_train_batch_size=bs, \n","        per_device_eval_batch_size=bs,\n","        num_train_epochs=epochs,\n","        report_to='none',\n","        fp16=True,\n","        logging_steps=10,\n","        save_strategy=\"no\"\n","    )\n","    \n","    # define Trainer\n","    trainer = Trainer(model, training_args, train_dataset=train_tok_ds, eval_dataset=eval_tok_ds,\n","                  tokenizer=tokz, data_collator=data_collator)\n","    \n","    return trainer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load the Dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As recommended in the study group, I'll use the `TinyStoriesInstruct` dataset which comes from the paper [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:52:09.176847Z","iopub.status.busy":"2023-08-31T19:52:09.176468Z","iopub.status.idle":"2023-08-31T19:53:48.573914Z","shell.execute_reply":"2023-08-31T19:53:48.567349Z","shell.execute_reply.started":"2023-08-31T19:52:09.176817Z"},"trusted":true},"outputs":[],"source":["ds = load_dataset(\"roneneldan/TinyStoriesInstruct\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:54:02.525022Z","iopub.status.busy":"2023-08-31T19:54:02.524379Z","iopub.status.idle":"2023-08-31T19:54:02.532843Z","shell.execute_reply":"2023-08-31T19:54:02.531905Z","shell.execute_reply.started":"2023-08-31T19:54:02.524986Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 21755681\n","    })\n","    validation: Dataset({\n","        features: ['text'],\n","        num_rows: 218380\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["ds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Full Fine-Tuning with `EleutherAi/pythia-70m`"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["First, I'll fully fine-tune an existing pretrained model on a subset of the `TinyStoriesInstruct` dataset using the `EleutherAI/pythia-70m` model. I chose this model because larger models were giving me CUDA-out-of-memory errors even for small dataset and batch sizes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:54:06.919076Z","iopub.status.busy":"2023-08-31T19:54:06.918374Z","iopub.status.idle":"2023-08-31T19:54:07.678518Z","shell.execute_reply":"2023-08-31T19:54:07.677460Z","shell.execute_reply.started":"2023-08-31T19:54:06.919038Z"},"trusted":true},"outputs":[],"source":["model_nm = 'EleutherAI/pythia-70m'\n","tokz = AutoTokenizer.from_pretrained(model_nm)\n","tokz.add_special_tokens({'pad_token': '[PAD]'})\n","def tok_func(x): return tokz(x[\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T15:51:41.193077Z","iopub.status.busy":"2023-08-31T15:51:41.192703Z","iopub.status.idle":"2023-08-31T15:51:44.217286Z","shell.execute_reply":"2023-08-31T15:51:44.216316Z","shell.execute_reply.started":"2023-08-31T15:51:41.193045Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(model_nm)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T15:51:46.633971Z","iopub.status.busy":"2023-08-31T15:51:46.632910Z","iopub.status.idle":"2023-08-31T15:51:46.644993Z","shell.execute_reply":"2023-08-31T15:51:46.643945Z","shell.execute_reply.started":"2023-08-31T15:51:46.633930Z"},"trusted":true},"outputs":[{"data":{"text/plain":["GPTNeoXForCausalLM(\n","  (gpt_neox): GPTNeoXModel(\n","    (embed_in): Embedding(50304, 512)\n","    (layers): ModuleList(\n","      (0-5): 6 x GPTNeoXLayer(\n","        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attention): GPTNeoXAttention(\n","          (rotary_emb): RotaryEmbedding()\n","          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n","          (dense): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (mlp): GPTNeoXMLP(\n","          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n","          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n","          (act): GELUActivation()\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T17:49:23.240785Z","iopub.status.busy":"2023-08-30T17:49:23.240314Z","iopub.status.idle":"2023-08-30T17:49:23.250283Z","shell.execute_reply":"2023-08-30T17:49:23.249060Z","shell.execute_reply.started":"2023-08-30T17:49:23.240749Z"},"trusted":true},"outputs":[{"data":{"text/plain":["70426624"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model.num_parameters()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["I first trained the model on a very small subset (1000 rows) for both full-finetuning and LoRA to make sure it worked, then slowly increased the training and validation size until I got the CUDA out-of-memory error. \n","\n","For small datasets, I noticed that the validation loss started increasing after 3 epochs so I've kept the number of epochs at 3. With larger datasets I could try to increase the number of epochs and see if it still overfits.\n","\n","I couldn't figure out how to implement perplexity during training. I was getting a `Sizes of tensors must match except in dimension 0.` error when passing any function to `compute_metrics` so I calculate perplexity at the end of training instead.\n","\n","When I tried to train the model with 240k, 220k or 200k training samples, I got the following error after 1.60, 1.75 and 1.92 epochs respectively:\n","\n","```python\n","RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/9: file write failed\n","```\n","\n","I set the `save_strategy` argument in the training arguments dictionary to `\"no\"` and this resolved this error. However, in the future, if I wanted checkpoints during my training I would have to figure out how to resolve this error differently.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T19:31:12.085700Z","iopub.status.busy":"2023-08-30T19:31:12.085305Z","iopub.status.idle":"2023-08-30T19:31:12.139259Z","shell.execute_reply":"2023-08-30T19:31:12.138237Z","shell.execute_reply.started":"2023-08-30T19:31:12.085669Z"},"trusted":true},"outputs":[],"source":["train_ds = ds['train'].select(range(240000))\n","eval_ds = ds['validation'].select(range(60000))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T19:31:13.644970Z","iopub.status.busy":"2023-08-30T19:31:13.644617Z","iopub.status.idle":"2023-08-30T19:31:45.055085Z","shell.execute_reply":"2023-08-30T19:31:45.054133Z","shell.execute_reply.started":"2023-08-30T19:31:13.644942Z"},"trusted":true},"outputs":[],"source":["trainer = get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr=5e-4, bs=16, epochs=3)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T19:31:48.449086Z","iopub.status.busy":"2023-08-30T19:31:48.448728Z","iopub.status.idle":"2023-08-30T21:12:05.403825Z","shell.execute_reply":"2023-08-30T21:12:05.402409Z","shell.execute_reply.started":"2023-08-30T19:31:48.449057Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='22500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [22500/22500 1:40:16, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.385700</td>\n","      <td>2.407521</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.098300</td>\n","      <td>2.192903</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.841100</td>\n","      <td>2.141196</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=22500, training_loss=2.1849648211161297, metrics={'train_runtime': 6016.472, 'train_samples_per_second': 119.671, 'train_steps_per_second': 3.74, 'total_flos': 1.64194783592448e+16, 'train_loss': 2.1849648211161297, 'epoch': 3.0})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T21:12:26.775479Z","iopub.status.busy":"2023-08-30T21:12:26.775029Z","iopub.status.idle":"2023-08-30T21:15:52.799689Z","shell.execute_reply":"2023-08-30T21:15:52.798434Z","shell.execute_reply.started":"2023-08-30T21:12:26.775443Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1875/1875 03:25]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 8.51\n"]}],"source":["eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["I'll generate some text from the pretrained model and fully fine-tuned model to see how they compare:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T21:15:58.470606Z","iopub.status.busy":"2023-08-30T21:15:58.469987Z","iopub.status.idle":"2023-08-30T21:16:04.436121Z","shell.execute_reply":"2023-08-30T21:16:04.434259Z","shell.execute_reply.started":"2023-08-30T21:15:58.470566Z"},"trusted":true},"outputs":[],"source":["prompt = \"Once upon a time,\"\n","generator = pipeline('text-generation', model=model_nm, tokenizer=tokz)\n","generator(prompt, max_length = 100, repetition_penalty=1.2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Generated text:\n","\n",">'Once upon a time, the\\nthe first two are not in agreement. The second is to be expected; and it would have been an easy task for them if they had known that he was going on their way from home as soon after leaving his house at night or when there were no other guests than himself who wanted him back with all of her belongings before returning into town again by midnight (and then later). But this one has never seen such things since I\\'ve lived here.\"\\n\\n  *'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T21:16:07.850056Z","iopub.status.busy":"2023-08-30T21:16:07.849553Z","iopub.status.idle":"2023-08-30T21:16:11.365280Z","shell.execute_reply":"2023-08-30T21:16:11.364080Z","shell.execute_reply.started":"2023-08-30T21:16:07.850011Z"},"trusted":true},"outputs":[],"source":["generator = pipeline('text-generation', model=trainer.model.to('cpu'), tokenizer=tokz)\n","generator(prompt, max_length = 100, repetition_penalty=1.2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Generated text:\n","\n",">'Once upon a time, there was an old man. He had a big mustache and he loved to wear it every day. One morning when the sun came out, his eyes lit up with joy! \\xa0He wanted to go outside but couldn\\'t find anything else. So he decided to take off his hat and coat so that no one could see him. The old man smiled at Jimmy\\'s face and said \"I\\'m glad you like it\". Jimmy was happy again and thanked the old man'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The pre-trained model as is does not generate text that resembles a story whatsoever. The fully fine-tuned model's generated text is somewhat coherent and it resembles a story although elements of it still don't make sense."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Fine-Tuning `EleutherAI/pythia-70m` with LoRA"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since a LoRA model has less trainable parameters, I can increase the dataset size for the training. I'll also see if I can train for more epochs without overfitting since I'm using more data.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:55:04.251289Z","iopub.status.busy":"2023-08-31T19:55:04.250930Z","iopub.status.idle":"2023-08-31T19:55:04.306144Z","shell.execute_reply":"2023-08-31T19:55:04.305201Z","shell.execute_reply.started":"2023-08-31T19:55:04.251255Z"},"trusted":true},"outputs":[],"source":["train_ds = ds['train'].select(range(256000))\n","eval_ds = ds['validation'].select(range(64000))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:54:14.300684Z","iopub.status.busy":"2023-08-31T19:54:14.300312Z","iopub.status.idle":"2023-08-31T19:54:17.348465Z","shell.execute_reply":"2023-08-31T19:54:17.347401Z","shell.execute_reply.started":"2023-08-31T19:54:14.300652Z"},"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(task_type=TaskType.CAUSAL_LM)\n","model = AutoModelForCausalLM.from_pretrained(model_nm)\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["```\n","trainable params: 98,304 || all params: 70,524,928 || trainable%: 0.13938901149959346\n","```"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:54:17.980036Z","iopub.status.busy":"2023-08-31T19:54:17.979664Z","iopub.status.idle":"2023-08-31T19:54:17.988806Z","shell.execute_reply":"2023-08-31T19:54:17.987396Z","shell.execute_reply.started":"2023-08-31T19:54:17.980006Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): GPTNeoXForCausalLM(\n","      (gpt_neox): GPTNeoXModel(\n","        (embed_in): Embedding(50304, 512)\n","        (layers): ModuleList(\n","          (0-5): 6 x GPTNeoXLayer(\n","            (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attention): GPTNeoXAttention(\n","              (rotary_emb): RotaryEmbedding()\n","              (query_key_value): Linear(\n","                in_features=512, out_features=1536, bias=True\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=512, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=1536, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","            )\n","            (mlp): GPTNeoXMLP(\n","              (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n","              (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n","              (act): GELUActivation()\n","            )\n","          )\n","        )\n","        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n","    )\n","  )\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:55:08.782308Z","iopub.status.busy":"2023-08-31T19:55:08.781937Z","iopub.status.idle":"2023-08-31T19:55:40.833488Z","shell.execute_reply":"2023-08-31T19:55:40.832333Z","shell.execute_reply.started":"2023-08-31T19:55:08.782276Z"},"trusted":true},"outputs":[],"source":["trainer = get_trainer(model, tokz, tok_func, train_ds, eval_ds, lr=5e-4, bs=16, epochs=4)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T19:55:43.809125Z","iopub.status.busy":"2023-08-31T19:55:43.808237Z","iopub.status.idle":"2023-08-31T21:56:36.480522Z","shell.execute_reply":"2023-08-31T21:56:36.479395Z","shell.execute_reply.started":"2023-08-31T19:55:43.809087Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='32000' max='32000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32000/32000 2:00:46, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.616000</td>\n","      <td>2.614058</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.575500</td>\n","      <td>2.570585</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.605000</td>\n","      <td>2.547680</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.493900</td>\n","      <td>2.540338</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=32000, training_loss=2.621225409567356, metrics={'train_runtime': 7252.3347, 'train_samples_per_second': 141.196, 'train_steps_per_second': 4.412, 'total_flos': 2.33350953959424e+16, 'train_loss': 2.621225409567356, 'epoch': 4.0})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T21:57:08.937551Z","iopub.status.busy":"2023-08-31T21:57:08.937152Z","iopub.status.idle":"2023-08-31T22:00:41.420405Z","shell.execute_reply":"2023-08-31T22:00:41.419361Z","shell.execute_reply.started":"2023-08-31T21:57:08.937492Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2000/2000 03:32]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 12.68\n"]}],"source":["eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T22:02:36.565557Z","iopub.status.busy":"2023-08-31T22:02:36.565123Z","iopub.status.idle":"2023-08-31T22:02:39.190400Z","shell.execute_reply":"2023-08-31T22:02:39.189416Z","shell.execute_reply.started":"2023-08-31T22:02:36.565499Z"},"trusted":true},"outputs":[],"source":["prompt = \"Once upon a time,\"\n","generator = pipeline('text-generation', model=trainer.model.to('cpu'), tokenizer=tokz)\n","generator(prompt, max_length = 100, repetition_penalty=1.2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Generated text:\n","\n",">\"Once upon a time, there was an old man who lived in the park. He had many friends and loved to play with him every day at his house all night long! One morning he decided that it would be best for everyone else because they were so happy together as each other on their own one by another's side of town hall or doorstep...so when something unexpected happened she started playing outside - her mommy said no but could help herself out here until someone came up close enough.. She\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The generated text resembles a story and is a bit coherent for the first couple of sentences before it stops making sense in the second half."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here is a comparison of the full-fine-tuning (Full FT) vs. LoRA fine-tuning (LoRA FT) process on the `EleutherAI/pythia-70m`:\n","\n","|Type|Parameters|Training Set|Validation Set|Perplexity|Batch Size|Epochs|Train Steps|Train Time (Minutes)|\n","|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n","|Full FT|70.4M|240k|60k|8.51|16|3|22500|100|\n","|LoRA FT|98k|256k|64k|12.68|16|4|32000|120|"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Generating Text from the Pre-Trained TinyStories Model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The authors of the paper that this dataset comes released their fine-tuned model on HuggingFace, so I'll use it to generate text to see how a state-of-the-art TinyStories model performs:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T22:06:22.150040Z","iopub.status.busy":"2023-08-31T22:06:22.149630Z","iopub.status.idle":"2023-08-31T22:06:23.651043Z","shell.execute_reply":"2023-08-31T22:06:23.650038Z","shell.execute_reply.started":"2023-08-31T22:06:22.150009Z"},"trusted":true},"outputs":[],"source":["model_nm = \"EleutherAI/gpt-neo-125M\"\n","tokz = AutoTokenizer.from_pretrained(model_nm)\n","tokz.add_special_tokens({'pad_token': '[PAD]'})\n","def tok_func(x): return tokz(x[\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T22:06:25.802966Z","iopub.status.busy":"2023-08-31T22:06:25.802589Z","iopub.status.idle":"2023-08-31T22:06:31.895432Z","shell.execute_reply":"2023-08-31T22:06:31.894425Z","shell.execute_reply.started":"2023-08-31T22:06:25.802935Z"},"trusted":true},"outputs":[],"source":["generator = pipeline('text-generation', model='roneneldan/TinyStories-33M', tokenizer=tokz)\n","generator(prompt, max_length = 100, repetition_penalty=1.2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Generated text:\n","\n",">'Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine and pick flowers. One day, she found an ancient book on her porch. It had lots of pictures inside that looked very old.\\n\\nLily opened the book and saw many words written around it. But then, she heard a loud noise coming from the house next door. She went to investigate and found out that someone had broken into their home. \\n\\nShe ran back to'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The model is so good! It can hold a consistent, coherent theme in story format for multiple sentences."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Final Thoughts"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["I'm happy to have got this all to work, as that alone was a big step in my learning process. This is the first time I have trained a causal language model using HuggingFace. One thought to close out this exercise: Would restructuring the data help? Currently the dataset has text values like \"Summary:\" and \"Features:\", which are the prompts used by the TinyStories paper authors to generate stories using GPT-3.5 and 4. Perhaps removing these prompts from the dataset and keeping only the story text would help improve the model. I'll explore this in a future exercise.\n","\n","I hope you enjoyed this blog post!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
