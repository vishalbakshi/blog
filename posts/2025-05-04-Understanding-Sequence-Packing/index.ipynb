{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Understanding Sequence Packing - Initial Musings\n",
        "date: \"2025-05-04\"\n",
        "author: Vishal Bakshi\n",
        "description: A hands-on investigation into how sequence packing interacts with Flash Attention in HuggingFace Transformers. Through print statements and code exploration, I discovered that position_ids are crucial for sequence packing to work correctlyâ€”without them, the wrong Flash Attention function gets called, leading to incorrect outputs and loss values. This post walks through the debugging process, comparing packed sequences with padded batches, and reveals the critical requirement for properly constructed position_ids in sequence-packed training.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "    - LLM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XVp0n6WeiqV"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFngbHHdSjzW"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show pip installs and imports\"\n",
        "!pip install -qq -U flash-attn --no-build-isolation\n",
        "!pip uninstall transformers -y\n",
        "!pip install git+https://github.com/vishalbakshi/transformers.git -qq\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import inspect\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPNm90buenn8"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gbVkY1ZeoeE"
      },
      "source": [
        "In this blog post, I'm walking through `transformers` code to start exploring functionality between sequence packing and Flash Attention. I'm new to both concepts, so this is purely an exploratory exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL20t7Kfe3nG"
      },
      "source": [
        "To assist my exploration, I've forked the Transformers library and added print statements at key junctures related to sequence packing and FA2. Referencing the original repo here's where I've inserted print statements:\n",
        "\n",
        "- Right after the function signature for `flash_attention_forward` in [src/transformers/integrations/flash_attention.py](https://github.com/huggingface/transformers/blob/2932f318a20d9e54cc7aea052e040164d85de7d6/src/transformers/integrations/flash_attention.py#L22) (which is called from inside `model.model.layers[0].self_attn.forward`).\n",
        "\n",
        "```python\n",
        "print(\"\\n=== FLASH_ATTENTION_FORWARD ENTRY ===\")\n",
        "print(f\"kwargs received: {list(kwargs.keys())}\")\n",
        "```\n",
        "\n",
        "- Right after the function signature/docstring for `_flash_attention_forward` in [src/transformers/modeling_flash_attention_utils.py](https://github.com/huggingface/transformers/blob/2932f318a20d9e54cc7aea052e040164d85de7d6/src/transformers/modeling_flash_attention_utils.py#L324):\n",
        "\n",
        "```python\n",
        "print(\"\\n=== _FLASH_ATTENTION_FORWARD ENTRY ===\")\n",
        "print(f\"kwargs received: {list(kwargs.keys())}\")\n",
        "\n",
        "print(\"\\n attention_mask\")\n",
        "print(attention_mask)\n",
        "\n",
        "print(\"\\n position_ids\")\n",
        "print(position_ids)\n",
        "```\n",
        "\n",
        "In the same file, later on:\n",
        "\n",
        "```python\n",
        "# Contains at least one padding token in the sequence\n",
        "if attention_mask is not None:\n",
        "    print(\"attention_mask is not None\")\n",
        "    ...\n",
        "```\n",
        "\n",
        "and later on further in the `_flash_attention_forward` function definition:\n",
        "\n",
        "```python\n",
        "elif position_ids is not None and (\n",
        "    max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\n",
        "):\n",
        "    print(\"position_ids is not None and max_length_q check\")\n",
        "    batch_size = query_states.size(0)\n",
        "\n",
        "    if cu_seq_lens_q is None or cu_seq_lens_k is None:\n",
        "        print(f\"cu_seq_lens_q is None: {cu_seq_lens_q is None}\")\n",
        "        print(f\"cu_seq_lens_k is None: {cu_seq_lens_q is None}\")\n",
        "        query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = (\n",
        "            prepare_fa2_from_position_ids(query_states, key_states, value_states, position_ids)\n",
        "        )\n",
        "\n",
        "        print(\"\\n cu_seq_lens\")\n",
        "        print(cu_seq_lens)\n",
        "        cu_seq_lens_q, cu_seq_lens_k = cu_seq_lens\n",
        "        max_length_q, max_length_k = max_seq_lens\n",
        "\n",
        "    else:\n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOyTERiBgsDm"
      },
      "source": [
        "I originally identified these functions by using the `inspect` library, e.g.:\n",
        "\n",
        "```python\n",
        "print(inspect.getsource(model.model.layers[0].self_attn.forward))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJLI9sq5gxSV"
      },
      "source": [
        "The goal of these print functions initially was to understand how `cu_seqlens` is utilized (if at all) and then after realizing it wasn't being used, my goal became to understand which function form `flash_attn` is being used: `flash_attn_func` or `flash_attn_varlen_func`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tjR02LqhFjn"
      },
      "source": [
        "## Initial Example: Passing in `input_ids`, `cu_seqlens` and `max_seqlen` to the SmolLM2-135M Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unmAfizjcyAf"
      },
      "source": [
        "At first, based on a Claude-generated example, I passed in the following fake input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNryIlReTPf5",
        "outputId": "6bc4c7d5-d953-4f4b-911c-b1b2936fbb5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[7, 6, 8, 5, 1, 3, 8, 6, 5, 3]], device='cuda:0'),\n",
              " 'cu_seqlens': [tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32)],\n",
              " 'max_seqlen': [10]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "test_params = {\n",
        "    'input_ids': torch.randint(1, 10, size=(1,10)).to(\"cuda\"),\n",
        "    'cu_seqlens': [torch.tensor([0, 3, 10], dtype=torch.int32).to(\"cuda\")],\n",
        "    'max_seqlen': [10]\n",
        "}\n",
        "test_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgpl-7tcTSzQ"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad(): output = model(**test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L7YunT5hWpZ"
      },
      "source": [
        "The following was printed out for each attention mechanism call in each of the model's 30 layers:\n",
        "\n",
        "```\n",
        "=== FLASH_ATTENTION_FORWARD ENTRY ===\n",
        "kwargs received: ['position_ids', 'output_attentions', 'use_cache', 'cu_seqlens', 'max_seqlen']\n",
        "\n",
        "=== _FLASH_ATTENTION_FORWARD ENTRY ===\n",
        "kwargs received: ['output_attentions', 'use_cache', 'cu_seqlens', 'max_seqlen']\n",
        "\n",
        " attention_mask\n",
        "None\n",
        "\n",
        " position_ids\n",
        "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='cuda:0')\n",
        "flash_attn_func is called\n",
        "flash_kwargs received: ['deterministic']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JmmbzoGhfut"
      },
      "source": [
        "I was surprised to see that `flash_attn_func` was called, because IIUC that doesn't handle sequence packed inputs. Looking at [its function signature](https://github.com/Dao-AILab/flash-attention/blob/fd2fc9d85c8e54e5c20436465bca709bc1a6c5a1/hopper/flash_attn_interface.py#L501), there's no `cu_seqlens` or similar parameter:\n",
        "\n",
        "```python\n",
        "def flash_attn_func(\n",
        "    q,\n",
        "    k,\n",
        "    v,\n",
        "    softmax_scale=None,\n",
        "    causal=False,\n",
        "    qv=None,\n",
        "    q_descale=None, k_descale=None, v_descale=None,\n",
        "    window_size=(-1, -1),\n",
        "    attention_chunk=0,\n",
        "    softcap=0.0,\n",
        "    num_splits=1,\n",
        "    pack_gqa=None,\n",
        "    deterministic=False,\n",
        "    sm_margin=0,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiQkoo3xjt2M"
      },
      "source": [
        "Additionally, `position_ids` is defined even though I didn't pass it in. IIUC, that's done in the model's forward pass with the line:\n",
        "\n",
        "```python\n",
        "if position_ids is None:\n",
        "    position_ids = cache_position.unsqueeze(0)\n",
        "```\n",
        "\n",
        "Where `cache_position` is defined earlier in that forward pass. This can be observed by running:\n",
        "\n",
        "```python\n",
        "forward_method = inspect.getsource(model.model.forward)\n",
        "print(forward_method)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmAM16k9iI5_"
      },
      "source": [
        "## Second Attempt: Passing in `position_ids` to the Forward Pass as Well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2uvTnjliPLd"
      },
      "source": [
        "Claude helped me understand that what triggers the function call of `flash_attn_varlen_func` is the following conditional in [`_flash_attention_forward`](https://github.com/huggingface/transformers/blob/2932f318a20d9e54cc7aea052e040164d85de7d6/src/transformers/modeling_flash_attention_utils.py#L378):\n",
        "\n",
        "```python\n",
        "elif position_ids is not None and (\n",
        "        max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\n",
        "    )\n",
        "```\n",
        "\n",
        "In particular, this line was of interest: `torch.diff(position_ids, dim=-1) >= 0`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqfOALD4ixsZ"
      },
      "source": [
        "In the following contrived example, `position_ids` is not a list of consecutive numbers (which seems to be the default value constructed is no `position_ids` value is passed to the model's forward pass)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ixOEdfrNZlFy"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([[0, 1, 2, 10, 11, 12, 13, 14, 15, 16]]).to(\"cuda\")\n",
        "position_ids = torch.tensor([[0, 1, 2, 0, 1, 2, 3, 4, 5, 6]]).to(\"cuda\")\n",
        "cu_seqlens = [torch.tensor([0, 3, 10], dtype=torch.int32).to(\"cuda\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POcLDIvUkO_V",
        "outputId": "46c76718-2cd2-411b-eb4e-5822cb0d73d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(False, device='cuda:0')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(torch.diff(position_ids, dim=-1) >= 0).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5IudaexkUvG",
        "outputId": "5bed5591-9d67-4d15-cb42-8b1ab15f3089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ True,  True, False,  True,  True,  True,  True,  True,  True]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.diff(position_ids, dim=-1) >= 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78l3Bv27kgsP",
        "outputId": "d15b4b7a-5cee-4651-e5b3-861f7c4d03b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1,  1, -2,  1,  1,  1,  1,  1,  1]], device='cuda:0')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.diff(position_ids, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc-q38hIkZJg"
      },
      "source": [
        "Some diffs between consecutive elements in `position_ids` are negative (because we are defining two sequences' position ids)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjyP3DQ4ksNY"
      },
      "source": [
        "I would now expect `flash_attn_varlen_func` to be called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rhyLuz2Z1Ss",
        "outputId": "bbd565ce-415c-4756-98f0-364a438ed5ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[7, 6, 8, 5, 1, 3, 8, 6, 5, 3]], device='cuda:0'),\n",
              " 'position_ids': tensor([[0, 1, 2, 0, 1, 2, 3, 4, 5, 6]], device='cuda:0'),\n",
              " 'cu_seqlens': [tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32)],\n",
              " 'max_seqlen': [7]}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "test_params = {\n",
        "    'input_ids': torch.randint(1, 10, size=(1,10)).to(\"cuda\"),\n",
        "    'position_ids': torch.tensor([[0, 1, 2, 0, 1, 2, 3, 4, 5, 6]]).to(\"cuda\"),\n",
        "    'cu_seqlens': [torch.tensor([0, 3, 10], dtype=torch.int32).to(\"cuda\")],\n",
        "    'max_seqlen': [7]\n",
        "}\n",
        "test_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HT9045_Z9XV"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad(): output = model(**test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ4XZmWwkzmd"
      },
      "source": [
        "Passing `test_params` through the model's forward pass yields:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawcYO-Uk3m7"
      },
      "source": [
        "```\n",
        "=== FLASH_ATTENTION_FORWARD ENTRY ===\n",
        "kwargs received: ['position_ids', 'output_attentions', 'use_cache', 'cu_seqlens', 'max_seqlen']\n",
        "\n",
        "=== _FLASH_ATTENTION_FORWARD ENTRY ===\n",
        "kwargs received: ['output_attentions', 'use_cache', 'cu_seqlens', 'max_seqlen']\n",
        "\n",
        " attention_mask\n",
        "None\n",
        "\n",
        " position_ids\n",
        "tensor([[0, 1, 2, 0, 1, 2, 3, 4, 5, 6]], device='cuda:0')\n",
        "position_ids is not None and max_length_q check\n",
        "cu_seq_lens_q is None: True\n",
        "cu_seq_lens_k is None: True\n",
        "\n",
        " cu_seq_lens\n",
        "(tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32), tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL_1lRmxk756"
      },
      "source": [
        "The `position_ids` are as passed in. However, it does not use `cu_seqlens` directly from `kwargs`. Instead it builds it [in the following line](https://github.com/huggingface/transformers/blob/2932f318a20d9e54cc7aea052e040164d85de7d6/src/transformers/modeling_flash_attention_utils.py#L383):\n",
        "\n",
        "```python\n",
        "query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = (\n",
        "    prepare_fa2_from_position_ids(query_states, key_states, value_states, position_ids)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ6KfP-vlSta"
      },
      "source": [
        "The value of `cu_seqlens` is the tuple:\n",
        "\n",
        "```python\n",
        "(tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32), tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32))\n",
        "```\n",
        "\n",
        "Which is deconstructed into `cu_seq_lens_q` and `cu_seql_lens_k` which are then passed as arguments to `flash_attn_varlen_func`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AZvQSz-lf5d"
      },
      "source": [
        "The main takeaway from this: Flash Attention will not handle sequence packing correctly unless you pass in `position_ids`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRMqGhaDbfAM"
      },
      "source": [
        "## Packed Sequence Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eqZEEM-loHl"
      },
      "source": [
        "In the remaining sections of this blog post, I'll explore how to correctly handle calculating loss for a packed sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YanT7ZgCa4II",
        "outputId": "05592fff-ecf2-4adf-e347-a4526778898f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 49152])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo4Exg2nl_el"
      },
      "source": [
        "Following how labels are constructed in HuggingFace's [`DataCollatorWithFlattening`](https://github.com/RhuiDih/transformers/blob/90305596c1f14376bb2049f408a4c53e024b2450/src/transformers/data/data_collator.py#L1643), the first token in each sequence is replaced with `-100`. This is because the HuggingFace CausalLM loss function handles the shifting of labels to allow next-token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB17Rw0ra5en",
        "outputId": "0df22f75-897e-4207-8285-5b4edffc30c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-100,    6,    8, -100,    1,    3,    8,    6,    5,    3],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = torch.tensor([-100, 6, 8, -100, 1, 3, 8, 6, 5, 3]).to(\"cuda\")\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxbWoo8fmQrR"
      },
      "source": [
        "The following two lines are taken from the model's loss function which can be inspected with `print(inspect.getsource(model.loss_function))`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgOHZCcXa_EJ",
        "outputId": "19309094-2315-487f-badc-574c5b8fab60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   6,    8, -100,    1,    3,    8,    6,    5,    3, -100],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_labels = torch.nn.functional.pad(labels, (0, 1), value=-100)\n",
        "shift_labels = _labels[..., 1:].contiguous()\n",
        "shift_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxqF5uahmdbo"
      },
      "source": [
        "We can see that the labels have been shifted to the left by 1 element, and a `-100` ignore index has been added to the right, which is needed because the last token in the input doesn't predict anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxH7lyP_mp0P"
      },
      "source": [
        "Calculating the loss using `F.cross_entropy` directly and the model's `loss_function` (providing it unshifted `labels`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xgmd5o_bAjy",
        "outputId": "d77b6fcb-8635-4fc2-dc2b-e767fd428d7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(20.2832, device='cuda:0')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = F.cross_entropy(\n",
        "    output.logits.reshape(-1, output.logits.size(-1)).float(),\n",
        "    shift_labels.reshape(-1)\n",
        ")\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eX_Jr-ubEsp",
        "outputId": "91c2e8da-2901-44a0-e234-5dbe77787361"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(20.2832, device='cuda:0')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.loss_function(output.logits, labels, 49152)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNpWtc-ibdHM"
      },
      "source": [
        "## Padded Batch Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27o0R7JVmx7C"
      },
      "source": [
        "Sequence packing shouldn't change the loss value of a given input batch. To test this, I'll construct a padded batch from our fake data and calculate its outputs, labels and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFvhtIhZbpqj",
        "outputId": "e7066f5a-18f6-43b6-a247-bf162df07b79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7, 6, 8, 5, 1, 3, 8, 6, 5, 3], device='cuda:0')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids = test_params['input_ids'][0]\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR4NEe2TbWFS",
        "outputId": "8c2cc60a-1319-480c-e975-b60ea1e94a7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cu_seqlens = test_params['cu_seqlens'][0]\n",
        "cu_seqlens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04xRw-StbiW3",
        "outputId": "bbb44c85-d6d9-4d4a-d91d-7ced9a893f42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(tensor(0, device='cuda:0', dtype=torch.int32),\n",
              "  tensor(3, device='cuda:0', dtype=torch.int32)),\n",
              " (tensor(3, device='cuda:0', dtype=torch.int32),\n",
              "  tensor(10, device='cuda:0', dtype=torch.int32))]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_boundaries = list(zip(cu_seqlens[:-1], cu_seqlens[1:]))\n",
        "seq_boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcpNDDq8bmp3",
        "outputId": "eca4d729-ec35-4d60-b6d3-0504fd4cda7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([7, 6, 8], device='cuda:0'),\n",
              " tensor([5, 1, 3, 8, 6, 5, 3], device='cuda:0'))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq1 = input_ids[seq_boundaries[0][0]: seq_boundaries[0][1]]\n",
        "seq2 = input_ids[seq_boundaries[1][0]: seq_boundaries[1][1]]\n",
        "seq1, seq2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhkfB5Yim9FS"
      },
      "source": [
        "The first item in the batch has 3 elements, and the second item in the batch has 7 elements. We need to pad the first item so it's 7 elements long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOrxu4mcbrbf",
        "outputId": "2260cb77-daca-41dc-9d40-5fce0bf2d638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7, 6, 8, 0, 0, 0, 0], device='cuda:0')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq1 = torch.cat([seq1, torch.tensor([0, 0, 0, 0]).to(\"cuda\")])\n",
        "seq1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYFb9h-hbvbP",
        "outputId": "99b50860-f6ea-472e-b244-0424fa99d756"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[7, 6, 8, 0, 0, 0, 0],\n",
              "         [5, 1, 3, 8, 6, 5, 3]], device='cuda:0'),\n",
              " torch.Size([2, 7]))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_batch = torch.stack([seq1, seq2], dim=0)\n",
        "padded_batch, padded_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ-aaFfXnDN7"
      },
      "source": [
        "Similarly, we need to construct `labels` such that the last four elements in the first batch item are ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Woa5iDq8b0wn",
        "outputId": "79c4035c-71f7-4d78-f722-05623c1f5c7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([7, 6, 8], device='cuda:0'),\n",
              " tensor([5, 1, 3, 8, 6, 5, 3], device='cuda:0'))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq1 = input_ids[seq_boundaries[0][0]: seq_boundaries[0][1]]\n",
        "seq2 = input_ids[seq_boundaries[1][0]: seq_boundaries[1][1]]\n",
        "seq1, seq2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ5lkmdtb2Q3",
        "outputId": "ea05103b-0a1e-4a5d-89ca-a7e243c5f66a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   7,    6,    8, -100, -100, -100, -100], device='cuda:0')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq1 = torch.cat([seq1, torch.tensor([-100, -100, -100, -100]).to(\"cuda\")])\n",
        "seq1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4vV6NcAb4IJ",
        "outputId": "bf8a766d-0c21-4ce8-c288-7d654906f228"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   7,    6,    8, -100, -100, -100, -100],\n",
              "        [   5,    1,    3,    8,    6,    5,    3]], device='cuda:0')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_labels = torch.stack([seq1, seq2], dim=0)\n",
        "padded_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h8ZAHjlnHlz"
      },
      "source": [
        "Calculating the logits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg6Dcq_cb6rJ"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad(): padded_output = model(input_ids=padded_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0UXtqbhnLyx"
      },
      "source": [
        "Noting that I haven't pass any `position_ids` and the printed output shows us that `flash_attn_func` is indeed the \"vanilla\" implementation of Flash Attention for padded batches:\n",
        "\n",
        "```\n",
        "=== FLASH_ATTENTION_FORWARD ENTRY ===\n",
        "kwargs received: ['position_ids', 'output_attentions', 'use_cache']\n",
        "\n",
        "=== _FLASH_ATTENTION_FORWARD ENTRY ===\n",
        "kwargs received: ['output_attentions', 'use_cache']\n",
        "\n",
        " attention_mask\n",
        "None\n",
        "\n",
        " position_ids\n",
        "tensor([[0, 1, 2, 3, 4, 5, 6]], device='cuda:0')\n",
        "flash_attn_func is called\n",
        "flash_kwargs received: ['deterministic']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-W7F-w7nWm6"
      },
      "source": [
        "Comparing the packed output logits with the padded output logits. The shapes are different but the values are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZIZwwzPb-xp",
        "outputId": "3aadf767-b684-4d5a-b77d-c61d133baa74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 10, 49152]), torch.Size([2, 7, 49152]))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.logits.shape, padded_output.logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UIEC-JGb_Xp",
        "outputId": "070e6ffd-013f-4ecd-befc-9ac8dae7cc91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1., device='cuda:0')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(output.logits[0, 0:3, :] == padded_output.logits[0, 0:3, :]).float().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLt-FFiVb_aM",
        "outputId": "8598f24d-6e50-4bc8-ce46-e04918d5afc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1., device='cuda:0')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(output.logits[0, 3:, :] == padded_output.logits[1, :, :]).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meZ090i_ndeb"
      },
      "source": [
        "Finally, calculating the padded batch's loss gives us the same value as the sequence packed loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6-joHS8b_ci",
        "outputId": "d4daac3a-2429-438a-deb4-c09d5bca924c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(20.2832, device='cuda:0')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_loss = model.loss_function(padded_output.logits, padded_labels, vocab_size=49152)\n",
        "padded_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egjXtBCPoCuz"
      },
      "source": [
        "## Not Passing in `position_ids` With Packed Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Fc9joLo-MQ"
      },
      "source": [
        "To confirm that not passing in position_ids does in indeed make HuggingFace use the wrong Flash Attention implementation for a packed sequence, I'll compare the logits and loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFxf7F41oP21",
        "outputId": "990a6acb-870f-46fd-e6e1-8e9c49adde95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[7, 6, 8, 5, 1, 3, 8, 6, 5, 3]], device='cuda:0'),\n",
              " 'cu_seqlens': [tensor([ 0,  3, 10], device='cuda:0', dtype=torch.int32)],\n",
              " 'max_seqlen': [10]}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "test_params = {\n",
        "    'input_ids': torch.randint(1, 10, size=(1,10)).to(\"cuda\"),\n",
        "    'cu_seqlens': [torch.tensor([0, 3, 10], dtype=torch.int32).to(\"cuda\")],\n",
        "    'max_seqlen': [10]\n",
        "}\n",
        "test_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUgavm3ooQ5I"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad(): output2 = model(**test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MMNp30-pQlh"
      },
      "source": [
        "The logits are not the same as when `flash_attn_varlen_func` is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiP9VAv7omkm",
        "outputId": "91d780a7-01bb-4650-e622-6901fc72cc7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3012, device='cuda:0')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(output.logits == output2.logits).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feCB0JlVpWAw"
      },
      "source": [
        "It follows that the loss value is not the same either."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nD0r-tGoWbw",
        "outputId": "6ec37ab6-7dac-4d44-f171-901f927ad8b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4632, device='cuda:0')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = torch.tensor([-100, 6, 8, -100, 1, 3, 8, 6, 5, 3]).to(\"cuda\")\n",
        "model.loss_function(output.logits, labels, 49152)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRuMGm4tnkdk"
      },
      "source": [
        "## Closing Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xMZn0ScnlXs"
      },
      "source": [
        "I'll reiterate that I'm not familiar with how sequence packing is implemented (in HuggingFace or ModernBERT) and even less familiar with how Flash Attention is implemented. That being said, this cursory investigation allowed me to understand high-level concepts of how these two interact. My key takeaway is that the correct `position_ids` need to be passed to the model otherwise HuggingFace will not use the correct `flash_attn_varlen_func` for sequence packed inputs and that will result in incorrect logits and loss values."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
