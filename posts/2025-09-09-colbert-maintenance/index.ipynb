{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0d196fdc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: PyTorch `.sort` Behavior Changes from Version `2.0.1` to `2.1.0`\n",
    "date: \"2025-09-09\"\n",
    "author: Vishal Bakshi\n",
    "description: While comparing `colbert-ai` index artifacts (for two installs using different PyTorch versions) I come across an unexpected finding--`.sort` indices are ordered differently in `torch==2.1.0` than in `torch==2.0.1` and thus change an intermediate artifact even with all else equal. Thankfully, this doesn't break ColBERT's indexing functionality and final index artifacts.\n",
    "filters:\n",
    "   - lightbox\n",
    "lightbox: auto\n",
    "categories:\n",
    "    - ColBERT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb9aa8-66fa-4b86-adb4-691e9b186a34",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5df70-26c3-4ec7-bb1a-90b923fbf5dd",
   "metadata": {},
   "source": [
    "In this notebook I'm going to explore how (and hopefully why) you can start with different `codes.indices` but end up with the same `ivf` and `ivf_lengths` when indexing a document collection using `colbert-ai`.\n",
    "\n",
    "I came across this behavior by accident. I was comparing final and intermediate `colbert-ai` index artifacts between installs using `torch==2.0.1` and `torch==2.1.0` and found that even after swapping `local_sample_embs.pt` (the document token embeddings clustered to find centroids) and `embs_{chunk_idx}.pt` (the full set of document token embeddings) from 2.0.1 to 2.1.0, the intermediate `codes.indices` (centroid ID for each document token embedding) did not pass `torch.equal` <mark>but the final `ivf.pid.pt` tensors did</mark>. How could that be possible? How can you start with different intermediate centroid-to-document token mappings and end up with the same final centroid-to-document token mappings? Furthermore, how can you end up with different `codes.indices` when your processing the same `embs`?\n",
    "\n",
    "First a bit of review of where `codes` comes from. The highest-level abstraction we start with is the `Indexer`:\n",
    "\n",
    "```python\n",
    "with Run().context(RunConfig(nranks=nranks)):\n",
    "    config = ColBERTConfig(...)\n",
    "    indexer = Indexer(checkpoint=\"answerdotai/answerai-colbert-small-v1\", config=config)\n",
    "    _ = indexer.index(name=\"...\", collection=collection)\n",
    "```\n",
    "\n",
    "Inside `Indexer`, [`encode` is called](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexer.py#L80) within which `CollectionIndexer` is instantiated:\n",
    "\n",
    "```python\n",
    "def encode(config, collection, shared_lists, shared_queues, verbose: int = 3):\n",
    "    encoder = CollectionIndexer(config=config, collection=collection, verbose=verbose)\n",
    "    encoder.run(shared_lists)\n",
    "```\n",
    "\n",
    "Inside [`CollectionIndexer.index`](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/collection_indexer.py#L346) the following line saves (i.e. compresses and stores the residuals of) document token embeddings (the input `embs` are manually forced to be identical b/w PyTorch version `colbert-ai` installs):\n",
    "\n",
    "```python\n",
    "self.saver.save_chunk(chunk_idx, offset, embs, doclens) # offset = first passage index in chunk\n",
    "```\n",
    "\n",
    "Once saved, the embeddings are deleted, which is why `colbert-ai` is so memory efficient! It's also why indexing and embedding are tied together with the same model.\n",
    "\n",
    "[`IndexSaver.save_chunk`](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/index_saver.py#L70) is defined as:\n",
    "\n",
    "```python\n",
    "def save_chunk(self, chunk_idx, offset, embs, doclens):\n",
    "    compressed_embs = self.codec.compress(embs)\n",
    "    \n",
    "    self.saver_queue.put((chunk_idx, offset, compressed_embs, doclens))\n",
    "```\n",
    "\n",
    "The `codec` is a `ResidualCodec` object and [its `compress` method](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/codecs/residual.py#L167) contains the following line:\n",
    "\n",
    "```python\n",
    "codes_ = self.compress_into_codes(batch, out_device=batch.device)\n",
    "```\n",
    "\n",
    "We're almost there! [`compress_into_codes`](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/codecs/residual.py#L204) is defined as:\n",
    "\n",
    "```python\n",
    "def compress_into_codes(self, embs, out_device):\n",
    "    codes = []\n",
    "\n",
    "    bsize = (1 << 29) // self.centroids.size(0)\n",
    "    for batch in embs.split(bsize):\n",
    "        if self.use_gpu:\n",
    "            indices = (self.centroids @ batch.T.cuda().half()).max(dim=0).indices.to(device=out_device)\n",
    "        else:\n",
    "            indices = (self.centroids @ batch.T.cpu().float()).max(dim=0).indices.to(device=out_device)\n",
    "        codes.append(indices)\n",
    "\n",
    "    return torch.cat(codes)\n",
    "```\n",
    "\n",
    "So, `codes` are the indices (i.e \"IDs\") of the centroids with the maximum cosine similarity with the document token embeddings. \n",
    "\n",
    "Let's say our `centroids` have shape `(1024, 96)` and the `batch` contains thirty-two 96-dimensional embeddings (shape `(32, 96)`), each corresponding to a different document token embedding. The transpose of `batch` has shape `(96, 32)` and the matrix multiplication `centroids @ batch.T` has shape `(1024, 32)` where the rows represent centroid indices and the columns represent token indices. Taking `.max(dim=0).indices` returns the row indices corresponding to the maximum value in each of the 32 columns. In other words, the 32 centroid IDs that are closest to the document token embeddings. Note that since `centroids` [are normalized](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/collection_indexer.py#L306) as are [document token embeddings](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/modeling/colbert.py#L104), the matrix multiplication _is_ the cosine similarity between the two sets of vectors. \n",
    "\n",
    "Which goes back to my question: how can different `codes` yield the same final `ivf` and `ivf_lengths`? And why are `codes` different to begin with?\n",
    "\n",
    "To set the stage, let's look at how `ivf` and `ivf_lengths` are created, starting with [`CollectionIndexer._build_ivf`](https://github.com/stanford-futuredata/ColBERT/blob/501c29d9e0b7f7b393e36c4177ec2b141a253114/colbert/indexing/collection_indexer.py#L438), the trimmed down version which is:\n",
    "\n",
    "```python\n",
    "def _build_ivf(self):\n",
    "    codes = codes.sort()\n",
    "    ivf, values = codes.indices, codes.values\n",
    "    ivf_lengths = torch.bincount(values, minlength=self.num_partitions)\n",
    "\n",
    "    _, _ = optimize_ivf(ivf, ivf_lengths, self.config.index_path_)\n",
    "```\n",
    "\n",
    "The `codes` are first sorted. The sorted indices (the document token IDs) are assigned as `ivf` and the values (the centroid IDs) after being `bincount`-ed (i.e. the frequency of each centroid ID---the number of tokens associated with each centroid ID) are assigned as `ivf_lengths`. These are the first iteration of `ivf` and `ivf_lengths` and will change later on in `optimize_ivf`, the trimmed down version of which is:\n",
    "\n",
    "```python\n",
    "def optimize_ivf(orig_ivf, orig_ivf_lengths, index_path, verbose:int=3):\n",
    "    all_doclens = load_doclens(index_path, flatten=False)\n",
    "    all_doclens = flatten(all_doclens)\n",
    "    total_num_embeddings = sum(all_doclens)\n",
    "\n",
    "    emb2pid = torch.zeros(total_num_embeddings, dtype=torch.int)\n",
    "\n",
    "    offset_doclens = 0\n",
    "    for pid, dlength in enumerate(all_doclens):\n",
    "        emb2pid[offset_doclens: offset_doclens + dlength] = pid\n",
    "        offset_doclens += dlength\n",
    "\n",
    "    ivf = emb2pid[orig_ivf]\n",
    "    unique_pids_per_centroid = []\n",
    "    ivf_lengths = []\n",
    "\n",
    "    offset = 0\n",
    "    for length in tqdm.tqdm(orig_ivf_lengths.tolist()):\n",
    "        pids = torch.unique(ivf[offset:offset+length])\n",
    "        unique_pids_per_centroid.append(pids)\n",
    "        ivf_lengths.append(pids.shape[0])\n",
    "        offset += length\n",
    "    ivf = torch.cat(unique_pids_per_centroid)\n",
    "    ivf_lengths = torch.tensor(ivf_lengths)\n",
    "    \n",
    "    original_ivf_path = os.path.join(index_path, 'ivf.pt')\n",
    "    optimized_ivf_path = os.path.join(index_path, 'ivf.pid.pt')\n",
    "    torch.save((ivf, ivf_lengths), optimized_ivf_path)\n",
    "\n",
    "    return ivf, ivf_lengths\n",
    "```\n",
    "\n",
    "We'll actually start from the bottom:\n",
    "\n",
    "```python\n",
    "ivf = torch.cat(unique_pids_per_centroid)\n",
    "ivf_lengths = torch.tensor(ivf_lengths)\n",
    "```\n",
    "\n",
    "`ivf` is a flattened tensor of pids (unique passage IDs per centroid). Looking at the loop right above this:\n",
    "\n",
    "```python\n",
    "offset = 0\n",
    "for length in tqdm.tqdm(orig_ivf_lengths.tolist()):\n",
    "    pids = torch.unique(ivf[offset:offset+length])\n",
    "    unique_pids_per_centroid.append(pids)\n",
    "    ivf_lengths.append(pids.shape[0])\n",
    "    offset += length\n",
    "```\n",
    "\n",
    "`ivf_lengths` is the flattened tensor of the _number_ of pids per centroid.\n",
    "\n",
    "So again: how can we start with different `codes` (a list of centroid IDs, where the indices are the document token embedding IDs) and end up with the same `ivf` (unique pids corresponding to centroids) and `ivf_lengths` (number of pids per centroid)?\n",
    "\n",
    "I fed this background section to Sonnet 4 (with the stanford-futuredata/ColBERT repo attached as Project Knowledge) to fact check me and it said:\n",
    "\n",
    "> The key insight is that the final IVF only cares about which passages are associated with each centroid, not which specific token embeddings within those passages. If the different codes still result in the same set of passages being associated with each centroid (even if individual token assignments differ), the final ivf and ivf_lengths would be identical\n",
    "\n",
    "\n",
    "TBD if that's correct, certainly seems plausible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e7fb3-8dd8-4504-a6d3-f13776c8f999",
   "metadata": {},
   "source": [
    "## Inspecting `codes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7716c53b-4299-4663-aece-43f778810757",
   "metadata": {},
   "source": [
    "First, I'll show that `codes.indices` (document token IDs) are not equal between my `torch==2.0.1` install and the `torch==2.1.0` install where I _swapped_ its `local_sample_embs.pt` and `embs_{chunk_idx}.pt` with 2.0.1's tensors. In other words, I \"forced\" the `2.1.0` install to cluster the same sample of document token embeddings when calculating centroids and then forced it to use the same document token embeddings to compress as residuals and centroid IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c32453-b312-4bcd-aa85-d856041f6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from colbert.indexing.loaders import load_doclens\n",
    "from colbert.utils.utils import print_message, flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16b8160-9d0d-4797-907b-e20d3816cd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_indices_a = torch.load(\"20250909-0.2.22.main.torch.2.0.1-1/ivf.pt\")\n",
    "codes_indices_b = torch.load(\"20250909-0.2.22.main.torch.2.1.0-swap-1/ivf.pt\")\n",
    "codes_values_a = torch.load(\"20250909-0.2.22.main.torch.2.0.1-1/values.pt\")\n",
    "codes_values_b = torch.load(\"20250909-0.2.22.main.torch.2.1.0-swap-1/values.pt\")\n",
    "torch.equal(codes_indices_a, codes_indices_b), torch.equal(codes_values_a, codes_values_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457ebf9c-d62f-4794-a952-a99fa693a42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     0,  ..., 16383, 16383, 16383])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_values_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca62459-2e06-481c-a5c4-3e06630fb1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     0,  ..., 16383, 16383, 16383])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_values_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b324ccb-6cbd-4d85-bf6c-c3be519e9d56",
   "metadata": {},
   "source": [
    "Note that `codes.values` (the centroid IDs) are identical. So _which_ centroid IDs are closest to the document token embeddings stays consistent across versions, but which document token IDs they correspond to does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bebf034-54ad-470a-8ad9-7b24d3a85866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivf_a, ivf_lengths_a = torch.load(\"20250909-0.2.22.main.torch.2.0.1-1/indexing/ConditionalQA/ivf.pid.pt\")\n",
    "ivf_b, ivf_lengths_b = torch.load(\"20250909-0.2.22.main.torch.2.1.0-swap-1/indexing/ConditionalQA/ivf.pid.pt\")\n",
    "torch.equal(ivf_a, ivf_b), torch.equal(ivf_lengths_a, ivf_lengths_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c7d57-2bc2-4aa0-8ffc-6596245282bc",
   "metadata": {},
   "source": [
    "Furthermore, the final unique passage IDs for each centroid (`ivf`) and the number of passage IDs per centroid ID (`ivf_lengths`) are equal across versions. What this tells me (re: Sonnet's hypothesis) is that the document token IDs, while dissimilar across versions, come from the same passages!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c1ba2-5b23-4d53-b3f0-078fc9432ade",
   "metadata": {},
   "source": [
    "## Recreating `optimize_ivf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe051c-3a5a-4aca-ae72-b686da9d00cc",
   "metadata": {},
   "source": [
    "To explore the relationship between document token IDs and passage IDs, I'll use the code in `optimize_ivf`, where initially, `ivf` means `codes.indices` and `ivf_lengths` mean `torch.bincount(codes.values)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5129ecad-90a2-47dc-b90d-8bf7d6ed590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_values_a = torch.bincount(codes_values_a, minlength=16384)\n",
    "codes_values_b = torch.bincount(codes_values_b, minlength=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc5fe7b6-ceb4-4b47-9310-e1efebdb0ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1110,   36,  173,  ...,  104,   95,   25]),\n",
       " tensor([1110,   36,  173,  ...,  104,   95,   25]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_values_a, codes_values_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0315d-070a-4b00-9d99-45809f17de09",
   "metadata": {},
   "source": [
    "I'll start by loading the mapping between passages and tokens: `doclens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b24e79-31bf-417e-a4ec-c51a073244c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doclens_a = load_doclens(\"20250909-0.2.22.main.torch.2.0.1-1/indexing/ConditionalQA/\", flatten=False)\n",
    "all_doclens_a = flatten(all_doclens_a)\n",
    "total_num_embeddings_a = sum(all_doclens_a)\n",
    "\n",
    "all_doclens_b = load_doclens(\"20250909-0.2.22.main.torch.2.1.0-swap-1/indexing/ConditionalQA\", flatten=False)\n",
    "all_doclens_b = flatten(all_doclens_b)\n",
    "total_num_embeddings_b = sum(all_doclens_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb42376a-e264-4c03-abfd-822843cd070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doclens_a == all_doclens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca17167-53cb-462c-a5d9-405eb319e16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_embeddings_a == total_num_embeddings_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca9301c-bc51-4c99-a087-45c8c990078a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146937"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_embeddings_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7eafc-89d2-44eb-b5c0-8f65efdd19cf",
   "metadata": {},
   "source": [
    "Next we create `emb2pid` which is a tensor that has 1146937 indices (one for each token across the collection) and values (passage IDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a4ccd5-c96d-4c3e-a72c-6490aec768cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _emb2pid(total_num_embeddings, all_doclens):\n",
    "    emb2pid = torch.zeros(total_num_embeddings, dtype=torch.int)\n",
    "    offset_doclens = 0\n",
    "    for pid, dlength in enumerate(all_doclens):\n",
    "        emb2pid[offset_doclens: offset_doclens + dlength] = pid\n",
    "        offset_doclens += dlength\n",
    "    return emb2pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5757b1c8-3a06-4981-af67-92cfe3dd049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2pid_a = _emb2pid(total_num_embeddings_a, all_doclens_a)\n",
    "emb2pid_b = _emb2pid(total_num_embeddings_b, all_doclens_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4111b48e-e67f-4246-80ed-a8b57eb80337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     0,  ..., 69198, 69198, 69198], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb2pid_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4412d37-1159-4438-80b7-ce79ffd7e1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     0,  ..., 69198, 69198, 69198], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb2pid_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e5a257-3567-4259-b530-cdcf266b6e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(emb2pid_a, emb2pid_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487b152-6b14-4161-8fce-c58d6174c103",
   "metadata": {},
   "source": [
    "The first three tokens we see correspond to passage ID `0`, and the last three tokens to passage ID `69198`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a785a3e-8fc9-4c26-84af-2e0d8f0d9988",
   "metadata": {},
   "source": [
    "Let's now see if the tokens in the two `codes_indices` come from the same passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6e78240-66c7-4cbf-a025-b118e70e4ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([377624, 285309, 285322,  ..., 117986, 118780, 128088])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_indices_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14eacdad-a983-45f9-9c2a-e2e9445277eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2776,   2808,   5974,  ..., 309906, 579450, 884128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_indices_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be00bed9-6268-424d-a64c-424191dfd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_a = emb2pid_a[codes_indices_a]\n",
    "pids_b = emb2pid_b[codes_indices_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ba56de0-58d2-4d93-961a-f9e589d5f0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23120, 17145, 17145,  ...,  7128,  7172,  7691], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27f3a838-3ba3-4cf1-9838-f59d92e21d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  170,   170,   377,  ..., 18739, 35561, 53527], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd0a83-53f9-4ef2-a923-feb4bea38822",
   "metadata": {},
   "source": [
    "Looking at the resulting passage IDs: the first two tokens of `pids_a` (`torch==2.0.1`) come from passages `23120` and `17145`, respectively. The first two tokens of `pids_b` (`torch==2.0.1` _swapped_) come from passage `170`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988ecbc-702a-4b21-9676-140b8558bf84",
   "metadata": {},
   "source": [
    "If we count the number of times each passage ID occurs in each tensor (`pids_a` or `pids_b`) they are identical! This is the first hint of Sonnet's hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af8ab632-932c-4a0f-8a6e-1d967b8b3f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(torch.bincount(pids_a), torch.bincount(pids_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8dae3-c0c0-496b-80b7-f46eb7e0eea5",
   "metadata": {},
   "source": [
    "Let's keep moving along in recreating `optimize_ivf`:\n",
    "\n",
    "```python\n",
    "ivf = emb2pid[orig_ivf]\n",
    "unique_pids_per_centroid = []\n",
    "ivf_lengths = []\n",
    "\n",
    "offset = 0\n",
    "for length in tqdm.tqdm(orig_ivf_lengths.tolist()):\n",
    "    pids = torch.unique(ivf[offset:offset+length])\n",
    "    unique_pids_per_centroid.append(pids)\n",
    "    ivf_lengths.append(pids.shape[0])\n",
    "    offset += length\n",
    "ivf = torch.cat(unique_pids_per_centroid)\n",
    "ivf_lengths = torch.tensor(ivf_lengths)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b75d9-9e30-44fe-8530-17ddb123d264",
   "metadata": {},
   "source": [
    "Instead of:\n",
    "\n",
    "```python\n",
    "ivf = emb2pid[orig_ivf]\n",
    "```\n",
    "\n",
    "I did:\n",
    "\n",
    "```python\n",
    "pids_a = emb2pid_a[codes_indices_a]\n",
    "```\n",
    "\n",
    "I'll move onto the for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79136fe2-c17f-4d3f-9d82-5726af13387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop(orig_ivf_lengths, ivf):\n",
    "    unique_pids_per_centroid = []\n",
    "    ivf_lengths = []\n",
    "    offset = 0\n",
    "    for length in orig_ivf_lengths.tolist():\n",
    "        pids = torch.unique(ivf[offset:offset+length])\n",
    "        unique_pids_per_centroid.append(pids)\n",
    "        ivf_lengths.append(pids.shape[0])\n",
    "        offset += length\n",
    "    return unique_pids_per_centroid, ivf_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd34f1d4-35f5-4d57-8c84-4691a9499fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pids_per_centroid_a, _ivf_lengths_a = _loop(orig_ivf_lengths=codes_values_a, ivf=codes_indices_a)\n",
    "unique_pids_per_centroid_b, _ivf_lengths_b = _loop(orig_ivf_lengths=codes_values_b, ivf=codes_indices_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0205892-4e54-45f3-b292-10a0a1ac89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(unique_pids_per_centroid_a): assert torch.equal(item, unique_pids_per_centroid_b[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084121f8-b511-4098-bb85-dd690fd1f4d2",
   "metadata": {},
   "source": [
    "And there we see it! While the order of the passage IDs is different, both `codes.indices` tensors contain the same unique passage IDs per centroid. The key reason for this is that in the for-loop, we use `torch.unique` which sorts the values in ascending order. So as long as the set of `ivf[offset:offset_length]` passages IDs are identical across PyTorch versions, even if sorted differently will have the same order after being sorted by `torch.unique`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dfb109-3639-408a-9661-5edef050f1e9",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff41da4-240c-4cd0-8a99-caac633be022",
   "metadata": {},
   "source": [
    "Let's revisit Sonnet 4's hypothesis:\n",
    "\n",
    "> The key insight is that the final IVF only cares about which passages are associated with each centroid, not which specific token embeddings within those passages\n",
    "\n",
    "While true, the reality was a bit different---the specific token IDs are identical across torch versions, it's just that they are sorted differently! However, this begs the question: why are the token IDs sorted differently across torch versions? I'll explore that in the Appendix section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3851b9-2530-4dc0-8373-c8adb2880ec5",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db7ce4-acd3-4acf-ac75-2e65a770aeae",
   "metadata": {},
   "source": [
    "To understand how the max cosine similarity calculation deviates between `torch==2.0.1` and `torch==2.1.0` (using `2.0.1`'s `local_sample_embs.pt`) I'll start by comparing the `embs` that I `torch.save`-d right before they were compressed. This is more of a sanity check as these were explicitly swapped from `torch==2.0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58706b53-428f-4263-b423-2ff53868f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\"embs_0.pt\", \"embs_1.pt\", \"embs_2.pt\"]:\n",
    "    a = torch.load(f\"20250909-0.2.22.main.torch.2.0.1-1/{f}\")\n",
    "    b = torch.load(f\"20250909-0.2.22.main.torch.2.1.0-swap-1/{f}\")\n",
    "    assert torch.allclose(a, b, atol=1e-4, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ddff05-dd4f-4c56-bd62-f53e02052eb7",
   "metadata": {},
   "source": [
    "They are all close enough! Next, I'll compare the single batch and centroids that I saved in the `ResidualCodec.compress_into_codes` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60a6aa0-787a-41c1-9332-e814223512ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_a = torch.load(f\"20250909-0.2.22.main.torch.2.0.1-1/compress_batch.pt\")\n",
    "batch_b = torch.load(f\"20250909-0.2.22.main.torch.2.1.0-swap-1/compress_batch.pt\")\n",
    "torch.allclose(batch_a, batch_b, atol=1e-4, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f55e3a-a2bb-4599-a24f-a8234495ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids_a = torch.load(f\"20250909-0.2.22.main.torch.2.0.1-1/compress_centroids.pt\")\n",
    "centroids_b = torch.load(f\"20250909-0.2.22.main.torch.2.1.0-swap-1/compress_centroids.pt\")\n",
    "torch.allclose(centroids_a, centroids_b, atol=1e-4, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c5805-984d-4521-a26f-22647fa3822e",
   "metadata": {},
   "source": [
    "Both the token embeddings and the centroids are close enough (both are float16). Next I'll compare a batch of `codes` (`indices`) saved inside `compress_into_codes`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c34fc6d6-9f4e-4f7d-ac38-ee6245bbc2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_a = torch.load(f\"20250909-0.2.22.main.torch.2.0.1-1/compress_indices.pt\")\n",
    "indices_b = torch.load(f\"20250909-0.2.22.main.torch.2.1.0-swap-1/compress_indices.pt\")\n",
    "torch.equal(indices_a, indices_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739bfb6-d1d8-42e8-a086-5a41084f1b3c",
   "metadata": {},
   "source": [
    "Interestingly, they are equal across the PyTorch versions. Next I'll compare the `codes` for each batch of `embs` in `compress`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37189c07-f016-48b8-b3c4-87d8559af709",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\"compress_codes_0.pt\", \"compress_codes_1.pt\", \"compress_codes_2.pt\"]:\n",
    "    a = torch.load(f\"20250909-0.2.22.main.torch.2.0.1-1/{f}\")\n",
    "    b = torch.load(f\"20250909-0.2.22.main.torch.2.1.0-swap-1/{f}\")\n",
    "    assert torch.equal(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09d2f6-6df1-4a38-8692-4457b92abb07",
   "metadata": {},
   "source": [
    "They are all equal as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3fb8b-340c-453f-baed-25dee9c39eb4",
   "metadata": {},
   "source": [
    "At this point it was clear to me that the cosine similarity calculation was not the root cause of the `codes.indices` diverging between PyTorch versions. The next place to look: the sorting of codes! I added a line in `CollectionIndexer._build_ivf` which saved the pre-sorted `codes`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d1728-f048-4157-9a71-22b5b66edbdb",
   "metadata": {},
   "source": [
    "Surprisingly: the `codes` _before being sorted_ are identical between PyTorch versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1dff886-85ae-4539-9cd2-754802ab3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(f\"20250909-0.2.22.main.torch.2.0.1-1/presort_codes.pt\")\n",
    "b = torch.load(f\"20250909-0.2.22.main.torch.2.1.0-swap-1/presort_codes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834bba2b-15fe-4dde-a4de-eb333ec29320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1269,   582, 10939,  ...,  5013,  4582,   431])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d84268d3-d374-4e6e-954a-0a70ee7d1433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1269,   582, 10939,  ...,  5013,  4582,   431])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11423d8d-09d1-423f-b6c2-81aac1668ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2f7e9-d780-4491-8e66-59ab8af77a1a",
   "metadata": {},
   "source": [
    "However, _after being sorted_ the `codes.indices` diverge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9b7b4d-adeb-4581-8c4f-6a7c6455e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(f\"20250909-0.2.22.main.torch.2.0.1-1/codes.pt\")\n",
    "b = torch.load(f\"20250909-0.2.22.main.torch.2.1.0-swap-1/codes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f862aead-56af-44e9-b6ae-57e861337323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([    0,     0,     0,  ..., 16383, 16383, 16383]),\n",
       "indices=tensor([377624, 285309, 285322,  ..., 117986, 118780, 128088]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00005e6b-fa99-4c7e-ba3a-41c9fbddcb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([    0,     0,     0,  ..., 16383, 16383, 16383]),\n",
       "indices=tensor([  2776,   2808,   5974,  ..., 309906, 579450, 884128]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59723db-6e47-4223-8938-17a231a1a71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(a.indices, b.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f0f43-15c5-4fa3-abb8-3852b1e64eed",
   "metadata": {},
   "source": [
    "There was the source of discrepancy! The order of indices _after_ being sorted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25459f-5fac-4e16-bfa0-d58962c9cc4e",
   "metadata": {},
   "source": [
    "Is this the case for all `sort` calls between these PyTorch versions? To test this, I ran the following code with each PyTorch install (`torch==2.0.1` and `torch==2.1.0`) and saved `t` before and after `.sort` was called:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "t = torch.randint(low=0, high=16383, size=(1146937,))\n",
    "t = t.sort()\n",
    "```\n",
    "\n",
    "For both PyTorch versions, `t.indices` was not equal (i.e. `torch.equal` was `False`). This is evidence that `sort`'s behavior changes from 2.0.1 to 2.1.0. After keyword searching the release notes, I couldn't find a PR that could be the culprit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c051e-7c6c-4a38-a843-3d2f434f93e2",
   "metadata": {},
   "source": [
    "Thankfully, `colbert-ai` is robust to such changes! Since we only care about the unique passage IDs (and number of passage IDs) for `ivf` and `ivf_lengths`, respectively, and _not_ the order of token IDs, this PyTorch change does not break the indexing pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
