{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: LossInspector&#58; A Deep Dive Into LLM-Foundry's Next-Token Prediction with a Custom Composer Callback\n",
        "date: \"2025-04-22\"\n",
        "author: Vishal Bakshi\n",
        "description: I'm working on a research project where we're fine-tuning small models with various techniques and datasets using LLM-Foundry. As part of our infrastructure setup, I wanted to thoroughly understand how a batch of data is prepared, and how the outputs of a model, along with the labels, are passed to the loss function. Enter the custom Composer callback LossInspector!\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "    - LLM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhfBalGyoJwF"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSYiLY_voLAX"
      },
      "source": [
        "I'm working on a research project where we'll be fine-tuning small models with various techniques and datasets using LLM-Foundry. As part of our infrastructure setup, we wanted to make sure that we thoroughly understood how a batch of data is prepared by LLM-Foundry, and how the outputs of a model, along with the labels, are passed to the loss function to calculate the loss. To do so, with the help of Claude, I wrote up a custom Composer Callback. This is the third custom callback I've written for Composer/LLM-Foundry, you can read more about [my first](https://vishalbakshi.github.io/blog/posts/2025-03-30-Composer-Callback/) and [second](https://vishalbakshi.github.io/blog/posts/2025-04-02-Composer-Callback-Logging-dtypes/) callbacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nL0IKl7poKD"
      },
      "source": [
        "I was initially going to have two or three callbacks: one to inspect inputs/outputs to the embedding, one to inspect the input/outputs to the model's forward pass, and one to inspect the loss function. 27 commits later, I had a relatively lean single callback that gave me all the information I needed.\n",
        "\n",
        "I focused on three events during Composer's [training loop](https://docs.mosaicml.com/projects/composer/en/stable/trainer/events.html):\n",
        "\n",
        "- `before_loss`: to store the \"untouched\" batch from Composer's `state`.\n",
        "- `before_forward`: to store the untouched `input_ids` and `labels` from the state's batch.\n",
        "- `after_loss`: to both capture the calculated loss and \"manually\" calculate the loss using the model's loss function.\n",
        "\n",
        "Before we go further into detail, here's the callback code (and necessary imports):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's my video walkthrough of the code in this notebook:\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9ffnmeiDF_M?si=DVAZhHFDfxkuzG6n\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USMNPHDUqwR0"
      },
      "source": [
        "## `LossInspector` Callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzbXd5JEqze9"
      },
      "source": [
        "```python\n",
        "from composer.core.callback import Callback\n",
        "from composer.core import State\n",
        "from composer.loggers import Logger\n",
        "import torch\n",
        "\n",
        "\n",
        "class LossInspector(Callback):       \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inspected = False\n",
        "        self.input_ids = None\n",
        "        self.labels = None\n",
        "    \n",
        "    def before_loss(self, state: State, logger: Logger) -> None:\n",
        "        if self.inspected:\n",
        "            return\n",
        "        self.state_outputs = state.outputs\n",
        "        self.state_batch = state.batch\n",
        "        \n",
        "\n",
        "    def before_forward(self, state: State, logger: Logger) -> None:\n",
        "        # check that input_ids and labels are the same as after loss\n",
        "        self.input_ids = state.batch['input_ids'][0].detach().cpu()\n",
        "        self.labels = state.batch['labels'][0].detach().cpu()\n",
        "    \n",
        "    def after_loss(self, state: State, logger: Logger) -> None:\n",
        "        if self.inspected:\n",
        "            return\n",
        "            \n",
        "        print(\"\\n=== LOSS CALCULATION INSPECTION ===\")\n",
        "        \n",
        "        # Get the framework loss from state\n",
        "        framework_loss = state.loss.item()\n",
        "        print(f\"Framework loss: {framework_loss:.6f}\")\n",
        "        \n",
        "        # Access model's loss_function directly\n",
        "        logits = self.state_outputs['logits']\n",
        "        labels = self.state_batch['labels']\n",
        "        vocab_size = state.model.model.config.vocab_size\n",
        "        \n",
        "        direct_loss = state.model.model.loss_function(\n",
        "            logits=logits,\n",
        "            labels=labels,\n",
        "            vocab_size=vocab_size\n",
        "        )\n",
        "        \n",
        "        print(f\"Direct call to model.loss_function: {direct_loss.item():.6f}\")\n",
        "        \n",
        "        print(\"\\n-------- input_ids --------\")\n",
        "        input_ids = self.state_batch['input_ids'][0].detach().cpu()\n",
        "        print(input_ids.tolist())\n",
        "        decoded_input = state.model.tokenizer.decode(input_ids)\n",
        "        print(decoded_input[:1000])\n",
        "        \n",
        "        print(\"\\n-------- labels --------\")\n",
        "        labels = self.state_batch['labels'][0].detach().cpu()\n",
        "        print(labels.tolist())\n",
        "        valid_labels = labels[labels != -100]\n",
        "        decoded_labels = state.model.tokenizer.decode(valid_labels)\n",
        "        print(decoded_labels)\n",
        "\n",
        "        print(\"\\n-------- matches before_forward values? --------\")\n",
        "        print(f\"input_ids: {torch.allclose(input_ids, self.input_ids)}\")\n",
        "        print(f\"labels: {torch.allclose(labels, self.labels)}\")\n",
        "        \n",
        "        self.inspected = True\n",
        "```\n",
        "\n",
        "The callback is then appended to the `callbacks` list before passed to the Composer trainer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvDG4zuYrx0V"
      },
      "source": [
        "## SmolLM2-135M Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXRSFNJdrz28"
      },
      "source": [
        "It was surprisingly difficult to inspect the loss function. Or rather my lack of Composer/HuggingFace internals knowledge immediately surfaced with this task! Looking through the Composer GitHub repo and documentation, I found the following references to the model's loss function---all quite helpful but too general:\n",
        "\n",
        "```python\n",
        "loss = model.loss(outputs, targets)\n",
        "```\n",
        "\n",
        "```python\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for inputs, targets in dataloader:\n",
        "        outputs = model.forward(inputs)\n",
        "        loss = model.loss(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "```python\n",
        "def loss(self, outputs, batch):\n",
        "    # pass batches and `forward` outputs to the loss\n",
        "    _, targets = batch\n",
        "    return F.cross_entropy(outputs, targets)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZkB9X0dtIVf"
      },
      "source": [
        "I looked at their MixUp algorithm's source code in hopes for more detail but found none---though it did help me confirm how batches are handled:\n",
        "\n",
        "```python\n",
        "class MixUp(Algorithm):\n",
        "    def match(self, event: Event, state: State) -> bool:\n",
        "        \"\"\"Determines whether the algorithm should run on a given event.\"\"\"\n",
        "        return event in [Event.AFTER_DATALOADER, Event.AFTER_LOSS]\n",
        "\n",
        "    def apply(self, event: Event, state: State, logger: Logger) -> None:\n",
        "        \"\"\"Run the algorithm by modifying the State.\"\"\"\n",
        "        input, target = state.batch\n",
        "\n",
        "        if event == Event.AFTER_DATALOADER:\n",
        "            new_input, self.permuted_target, self.mixing = mixup_batch(input, target, alpha=0.2)\n",
        "            state.batch = (new_input, target)\n",
        "\n",
        "        if event == Event.AFTER_LOSS:\n",
        "            modified_batch = (input, self.permuted_target)\n",
        "            new_loss = state.model.loss(state.outputs, modified_batch)\n",
        "            state.loss *= (1 - self.mixing)\n",
        "            state.loss += self.mixing * new_loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "931dFXc7tGU6"
      },
      "source": [
        "Looking at Composer's `HuggingFaceModel` did not give me the necessary detail, but provided the key for the next step: the loss was stored in `outputs`.\n",
        "\n",
        "```python\n",
        "def loss(self, outputs, batch):\n",
        "    if self.config.use_return_dict:\n",
        "        return outputs['loss']\n",
        "    else:\n",
        "        # loss is at index 0 in the output tuple\n",
        "        return outputs[0]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmbvpSpMtKZ6"
      },
      "source": [
        "Did this mean that the loss function was tucked away in the forward pass? Let's take a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5AKgeEV2tkb0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd4w4b7puR9d"
      },
      "outputs": [],
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ucCD9_UtuYpc"
      },
      "outputs": [],
      "source": [
        "import inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8e-FJ6-umB-"
      },
      "outputs": [],
      "source": [
        "forward_method = inspect.getsource(model.forward)\n",
        "print(forward_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGysLHm-topN"
      },
      "source": [
        "I won't print out the whole forward method, but will highlight that tucked away in there was the loss function call!\n",
        "\n",
        "\n",
        "```python\n",
        "loss = None\n",
        "if labels is not None:\n",
        "    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
        "```\n",
        "\n",
        "Aha! The function in question is `loss_function`. Inspecting that in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHEl-8jBuspo",
        "outputId": "d9ba0d35-319f-444b-e060-7842ce9c6ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(hasattr(model, 'loss_function'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4nkGJcVt1_1"
      },
      "source": [
        "This was a great opportunity for a refresher on the next-token objective and auto-regressive nature of this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFXBb3Q6vF7k",
        "outputId": "be12edc7-9551-4d19-99f6-16d6af06abe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def ForCausalLMLoss(\n",
            "    logits,\n",
            "    labels,\n",
            "    vocab_size: int,\n",
            "    num_items_in_batch: Optional[int] = None,\n",
            "    ignore_index: int = -100,\n",
            "    shift_labels: Optional[torch.Tensor] = None,\n",
            "    **kwargs,\n",
            ") -> torch.Tensor:\n",
            "    # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
            "    logits = logits.float()\n",
            "\n",
            "    if shift_labels is None:\n",
            "        # Shift so that tokens < n predict n\n",
            "        labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n",
            "        shift_labels = labels[..., 1:].contiguous()\n",
            "\n",
            "    # Flatten the tokens\n",
            "    logits = logits.view(-1, vocab_size)\n",
            "    shift_labels = shift_labels.view(-1)\n",
            "    # Enable model parallelism\n",
            "    shift_labels = shift_labels.to(logits.device)\n",
            "    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n",
            "    return loss\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(model.loss_function))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EfvGbf_uC31"
      },
      "source": [
        "The key for understanding next-token prediction are the following lines:\n",
        "\n",
        "```python\n",
        "if shift_labels is None:\n",
        "    # Shift so that tokens < n predict n\n",
        "    labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "```\n",
        "\n",
        "`nn.functional.pad` adds padding tokens to `labels`, specifically `0` to the left-most end of the last dimension and `1` padding token to the right-most end. The token it uses as padding is `ignore_index`, which is `-100`.\n",
        "\n",
        "Next, it _shifts_ the labels by 1 element to the left with `labels[..., 1:]`. I took a moment to realize what this meant: the `input_ids` and `labels`, in terms of position, are the same! To align the `labels` with the `logits` (which are already \"shifted\" in the sense that the first position in `logits` corresponds to the first predicted token: the second token in the context) we have to shift the `labels` by 1. To ensure that the final token in `input_ids` doesn't predict anything, we pad `labels` with `-100`, the value ignored in the loss calculation.\n",
        "\n",
        "As a reminder, if the context we're training our model on is \"the cat sat on the table\", each next token is predicted based on all previous tokens:\n",
        "\n",
        "```\n",
        "the --> cat\n",
        "the cat --> sat\n",
        "the cat sat --> on\n",
        "the cat sat on --> the\n",
        "the cat sat on the --> table\n",
        "```\n",
        "\n",
        "This is a good time to return to our callback and analyze its output, but before I do, here's a quick demo of the label shifting operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q9bTziuRzhkN"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import pad\n",
        "from torch import tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXh2wN04zlpV",
        "outputId": "71616038-3f2d-43e5-85f2-adcc9b90a766"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3, 6, 4, 2])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = tensor([3, 6, 4, 2])\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8c5ZdFuzr31",
        "outputId": "72be094b-fdd3-4317-fea6-53ce914d3b3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   3,    6,    4,    2, -100])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad(labels, (0,1), value=-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43338AlezxCA",
        "outputId": "1184fe03-3d4d-41db-e07f-3d0cf1de0823"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-100,    3,    6,    4,    2])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad(labels, (1,0), value=-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHvAD96Wzylg",
        "outputId": "5893adca-e559-46bd-c16c-c02c7a876779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-100,    3,    6,    4,    2, -100])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad(labels, (1,1), value=-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAY5Ot7Pz_GC",
        "outputId": "4d957eab-06ec-4c64-9c1e-54b4a13e8a59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   6,    4,    2, -100])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad(labels, (0,1), value=-100)[...,1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqYcl151vQKP"
      },
      "source": [
        "## Callback Logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHPOcHymvYnd"
      },
      "source": [
        "There were four key print statements of interest in my callback. I'll display each and show their printed value:\n",
        "\n",
        "1. `print(f\"Framework loss: {framework_loss:.6f}\")`\n",
        "\n",
        "```\n",
        "Framework loss: 1.067513\n",
        "```\n",
        "\n",
        "2. `print(f\"Direct call to model.loss_function: {direct_loss.item():.6f}\")`\n",
        "\n",
        "```\n",
        "Direct call to model.loss_function: 1.067513\n",
        "```\n",
        "\n",
        "3. `print(input_ids.tolist())`\n",
        "4. `print(labels.tolist())`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZC54dv8wM1c"
      },
      "source": [
        "![`input_ids` (top) and `labels` (bottom) with the response highlighted in yellow](1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNgkLytMwVeU"
      },
      "source": [
        "The first two print statements confirmed that I was calling `state.model.loss_function` correctly. It also confirmed that the loss function doesn't take in the `input_ids`.\n",
        "\n",
        "The last two print statements confirmed my understanding: positionally speaking, the `input_ids` and `labels` are the same. In `labels` the positions of `input_ids` tokens that contain the prompt (and EOS tokens) are replaced with `-100` and the tokens that represent the response are kept as is. For reference, here's what `input_ids` looks like (both the prompt and the response) coming from an item of the MetaMathQA dataset (I have ommitted the hundreds of padding EOS tokens and formatted the text for clearer presentation):\n",
        "\n",
        "```\n",
        "A box with a volume of 16 $\\text{cm}^3$ can hold X paperclips.\n",
        "How many paperclips could a box with a volume of 48 $\\text{cm}^3$ hold?\n",
        "If we know the answer to the above question is 150, what is the value of unknown variable X?\n",
        "\n",
        "We are given that a box with a volume of 16 $\\text{cm}^3$ can hold $X$ paperclips.\n",
        "To find out how many paperclips a box with a volume of 48 $\\text{cm}^3$ can hold, we can set up a proportion using the given information.\n",
        "We can write the proportion as:\n",
        "16 $\\text{cm}^3$ / $X$ paperclips = 48 $\\text{cm}^3$ / 150 paperclips\n",
        "We can cross-multiply and solve for $X$:\n",
        "16 * 150 = 48 * $X$\n",
        "2400 = 48 * $X$\n",
        "Dividing both sides by 48, we get:\n",
        "$X$ = 50\n",
        "The value of $X$ is 50.\n",
        "The answer is: 50<|endoftext|>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiC1Nnrxi9b"
      },
      "source": [
        "`labels` has the prompt replaced with `-100`s, and the loss function then left-shifts the `labels` tokens by 1 spot to align with the logits for next-token prediction comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGn-U3JYx2Am"
      },
      "source": [
        "Unsurprisingly, the `input_ids` and `labels` before the forward pass and after the loss calculation are the same:\n",
        "\n",
        "```\n",
        "print(\"\\n-------- matches before_forward values? --------\")\n",
        "print(f\"input_ids: {torch.allclose(input_ids, self.input_ids)}\")\n",
        "print(f\"labels: {torch.allclose(labels, self.labels)}\")\n",
        "```\n",
        "\n",
        "```\n",
        "-------- matches before_forward values? --------\n",
        "input_ids: True\n",
        "labels: True\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEoA4FeQyLz6"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd15oX6AyM40"
      },
      "source": [
        "With this baseline established, I can use this callback everytime we have processed a new dataset for training, inspecting the tokens, decoded text and loss values to ensure that the training loop will run properly for next-token prediction, whether it's a continued pretraining or instruction fine-tuning dataset! Working with LLM-Foundry is a steep learning curve but I am learning a TON."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
