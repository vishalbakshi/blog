{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Comparing PyTorch `Embeddings` with Custom Embeddings\n",
        "lightbox: true\n",
        "date: \"2024-06-25\"\n",
        "author: Vishal Bakshi\n",
        "description: In this notebook I compare the code required to build a collaborative filtering model using PyTorch `Embedding`s and custom embeddings.\n",
        "categories:\n",
        "    - machine learning\n",
        "    - fastai\n",
        "    - python\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AVhRuQizHxR"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqthWeyYzJDQ"
      },
      "source": [
        "In this notebook I'll work through the following \"Further Research\" prompt at the end of Chapter 8 of the fastai textbook:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKP1m11hTx-f"
      },
      "source": [
        "> Take a look at all the differences between the `Embedding` version of `DotProductBias` and the `create_params` version, and try to understand why each of those changes is required. If you're not sure, try reverting each change to see what happens (even the type of brackets used in `forward` has changed!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47gve5qZ7Ge9"
      },
      "source": [
        "## Visual Inspection of Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlUBPO47zR99"
      },
      "source": [
        "I'll start by visually inspecting and annotating the differences between the two functions (I made this visual in Google Slides using the beautiful Ubuntu Mono font):\n",
        "\n",
        "![Visual inspection of two `DotProductBias` Modules](1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba5oDcu87Jt6"
      },
      "source": [
        "## Stepping Through the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVsG0JQj7LDs"
      },
      "source": [
        "Next, I'll step through each `DotProductBias` implementation's code using real data. I was getting an SSL error when using `untar_data(URLs.ML_100k)` so I manually downloaded the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "StvswmaUTeK8"
      },
      "outputs": [],
      "source": [
        "from fastai.collab import *\n",
        "from fastai.tabular.all import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beOdgAm48_Te"
      },
      "outputs": [],
      "source": [
        "!unzip /content/ml-100k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iuy65EJd_MlI"
      },
      "outputs": [],
      "source": [
        "path = Path('/content/ml-100k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "nrRebKSK7-LL",
        "outputId": "cc4e8288-4ef2-4a1c-cf82-2f481619ec35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>title</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>713</td>\n",
              "      <td>Wings of the Dove, The (1997)</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>788</td>\n",
              "      <td>In &amp; Out (1997)</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>270</td>\n",
              "      <td>Benny &amp; Joon (1993)</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>682</td>\n",
              "      <td>Searching for Bobby Fischer (1993)</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>543</td>\n",
              "      <td>Fantasia (1940)</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>535</td>\n",
              "      <td>Contact (1997)</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>463</td>\n",
              "      <td>Waiting for Guffman (1996)</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>326</td>\n",
              "      <td>Man Who Would Be King, The (1975)</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>712</td>\n",
              "      <td>Around the World in 80 Days (1956)</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>804</td>\n",
              "      <td>North by Northwest (1959)</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, names=['user', 'movie', 'rating', 'timestamp'])\n",
        "movies = pd.read_csv(path/'u.item', delimiter='|', encoding='latin-1', usecols=(0,1), names=('movie', 'title'), header=None)\n",
        "ratings = ratings.merge(movies)\n",
        "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
        "dls.show_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XzzFI9JB-08",
        "outputId": "afecb17f-1e0b-4473-ec64-ca9a3ec9b044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(944, 1665, 5)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_users = len(dls.classes['user'])\n",
        "n_movies = len(dls.classes['title'])\n",
        "n_factors = 5\n",
        "\n",
        "n_users, n_movies, n_factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE_Lqs78_OrJ",
        "outputId": "b2122fc0-9e5d-4f6e-8d78-208d90849d9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 2]), torch.Size([64, 1]))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xb, yb = dls.one_batch()\n",
        "xb.shape, yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9_--g1HdBPAX"
      },
      "outputs": [],
      "source": [
        "def create_params(size):\n",
        "  return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWkt4ykhGvFX"
      },
      "source": [
        "### Creating User Latent Factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQXnpi23BHy8"
      },
      "source": [
        "The first line in each implementation creates the (untrained) latent factors for users. For all variable names, I'll append `_emb` for the `Embedding` model implementation and `_cp` for the `create_params` implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImbnZ9M3CLdo"
      },
      "source": [
        "The first difference between the two implementations (other than the obvious that one uses `Embedding` and the other uses `create_params`) is that `Embedding` is given two arguments to determine its size (`ni` and `nf`), whereas `create_params` is given one list with both sizes. I'll illustrate this by showing the error thrown by each one when called without passing arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBTwBTUD_hPX",
        "outputId": "02d1e9d0-2590-4d1d-dda2-71bc657705dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Embedding(944, 5), torch.Size([944, 5]))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_factors_emb = Embedding(n_users, n_factors)\n",
        "user_factors_cp = create_params([n_users, n_factors])\n",
        "\n",
        "user_factors_emb, user_factors_cp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "SbkchjNlCKgJ",
        "outputId": "72998486-a7ab-43d7-af88-ccdc76e0d946"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Embedding.__init__() missing 2 required positional arguments: 'ni' and 'nf'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e7169ece36a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Embedding.__init__() missing 2 required positional arguments: 'ni' and 'nf'"
          ]
        }
      ],
      "source": [
        "Embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "QEQXQz_KCZ0D",
        "outputId": "c2255c44-e056-419a-ece4-13072dfcdd3b"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "create_params() missing 1 required positional argument: 'size'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-bbdd11ed5a1f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: create_params() missing 1 required positional argument: 'size'"
          ]
        }
      ],
      "source": [
        "create_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZZO0ft4G0G4"
      },
      "source": [
        "### Creating User Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmD3Fk3zCv3m"
      },
      "source": [
        "The next line in each model creates the bias object for users:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmjmEaJC9sj"
      },
      "source": [
        "The main difference here is that the output dimension of `1` is specified for the `Embedding` but only a single `size` is given to `create_params`, the reason being that `create_params` is just a tensor which will return a tensor when indexed, whereas `Embedding` needs an explicit output feature size. The consequence of this will be seen later on in the `forward` pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwIGUO59CeUk",
        "outputId": "008b3855-5ce7-4ba3-df02-fa0b71650f55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Embedding(944, 1), torch.Size([944]))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_bias_emb = Embedding(n_users, 1)\n",
        "user_bias_cp = create_params([n_users])\n",
        "\n",
        "user_bias_emb, user_bias_cp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa4rEJUYC80m",
        "outputId": "cea65782-4ac4-4e0e-9c70-efe1a4069c24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 1]), torch.Size([64]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_bias_emb(xb[:,0]).shape, user_bias_cp[xb[:,0]].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23a5rBSzG7EG"
      },
      "source": [
        "### Creating Movie Latent Factors and Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOPx3uM1HAZD"
      },
      "source": [
        "The next two lines in each model do the same thing but for the movies (items):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R64fatQGndL",
        "outputId": "072ea119-b510-4a2b-8d12-00c0337bdcfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Embedding(1665, 5),\n",
              " torch.Size([1665, 5]),\n",
              " Embedding(1665, 1),\n",
              " torch.Size([1665]))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_factors_emb = Embedding(n_movies, n_factors)\n",
        "movie_factors_cp = create_params([n_movies, n_factors])\n",
        "\n",
        "movie_bias_emb = Embedding(n_movies, 1)\n",
        "movie_bias_cp = create_params([n_movies])\n",
        "\n",
        "movie_factors_emb, movie_factors_cp.shape, movie_bias_emb, movie_bias_cp.shape,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDd28Y5ZHYmL"
      },
      "source": [
        "### Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTvZH0BXHbo_"
      },
      "source": [
        "There are two differences in how the `forward` method is defined when using `Embedding` versus `create_params`:\n",
        "\n",
        "- You have to **call** an `Embedding` but **index** the tensor created by `create_params`.\n",
        "- You have to specify `keepdim=True` for the dot product in the `Embedding` model when using `sum` but don't need to do so in the `create_params` model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r06nhgeQI1pA"
      },
      "source": [
        "The first bullet point can be illustrated easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNsidDOjHUZF",
        "outputId": "743f0124-5149-4be3-dcc8-9caeadfb2e9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 5]), torch.Size([64, 5]))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(\n",
        "    movie_factors_emb(xb[:,1]).shape, # call Embeddings to get output (a tensor)\n",
        "    movie_factors_cp[xb[:,1]].shape # index `create_params` output (a tensor)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfmBfQOYI7J_"
      },
      "source": [
        "The second bullet point is illustrated by **not** passing `keepdim=True` to the `sum` call for the `Embedding` output product and seeing what happens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5zdFtQRJdl7"
      },
      "source": [
        "The output product here has a single dimension with 64 items in that dimension. In the next line of the `forward` pass, we will try to add the bias vectors to this product:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc8WdxI5IbqT",
        "outputId": "be212d3c-0a10-4767-fe12-1a4e1fd71b4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res_emb = (user_factors_emb(xb[:,0]) * movie_factors_emb(xb[:,1])).sum(dim=1)\n",
        "res_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "dg5-m3AMJv9g",
        "outputId": "161eecaa-6cc6-4588-d966-853adaf9ec46"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "output with shape [64] doesn't match the broadcast shape [64, 64]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e454f67ea331>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_emb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0muser_bias_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmovie_bias_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [64] doesn't match the broadcast shape [64, 64]"
          ]
        }
      ],
      "source": [
        "res_emb += user_bias_emb(xb[:,0]) + movie_bias_emb(xb[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TEDOxBFJ3Jm"
      },
      "source": [
        "But that doesn't work because the bias `Embedding` outputs a tensor with dimensions 64 x 1. We need that unit axis at the end in order to allow for tensor addition to take place. `keepdim=True` preserves that unit axis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc-yMQuuKFu8",
        "outputId": "9dd81482-ba0d-459e-9afc-480e220e0282"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 1])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res_emb = (user_factors_emb(xb[:,0]) * movie_factors_emb(xb[:,1])).sum(dim=1, keepdim=True) # unit axis preserved\n",
        "res_emb += user_bias_emb(xb[:,0]) + movie_bias_emb(xb[:,1])\n",
        "res_emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIiTft7KJ1A"
      },
      "source": [
        "We don't get this issue when using `create_params` because we didn't specify a unit axis to begin with.\n",
        "\n",
        "Recall that when creating the user and movie bias in the model's `__init__` method, only a single `size` is given to `create_params`:\n",
        "\n",
        "```python\n",
        "self.user_bias = create_params([n_users])\n",
        "...\n",
        "self.movie_bias = create_params([n_movies])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4jptkIGKkuQ"
      },
      "source": [
        "When we perform the dot product between users and movies in the `create_params` model, we don't need to preserve that second dimension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqoA3xRPKtF4",
        "outputId": "1a9635b7-df81-41f5-fe8f-0d86f4068eb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res_cp = (user_factors_cp[xb[:,0]] * movie_factors_cp[xb[:,1]]).sum(dim=1) # keepdim=False\n",
        "res_cp += user_bias_cp[xb[:,0]] + movie_bias_cp[xb[:,1]]\n",
        "res_cp.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIBErdgXLcgE"
      },
      "source": [
        "The output of the `create_params` model has a single dimension, while the output of the `Embedding` model has an additional unit axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2DrZK0LJaZm"
      },
      "source": [
        "I'm displaying the visual inspection again to recap the differences we saw in the implementation of the two models:\n",
        "\n",
        "![Visual inspection of two `DotProductBias` Modules](1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBiwXwAVN2EH"
      },
      "source": [
        "## Training Each Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcbsOzXCN4NM"
      },
      "source": [
        "To cap off this experiment, I'll show that both models train similarly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "IxIRx7hQN-V8"
      },
      "outputs": [],
      "source": [
        "class DotProductBiasEmb(Module):\n",
        "  def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
        "    self.user_factors = Embedding(n_users, n_factors)\n",
        "    self.user_bias = Embedding(n_users, 1)\n",
        "    self.movie_factors = Embedding(n_movies, n_factors)\n",
        "    self.movie_bias = Embedding(n_movies, 1)\n",
        "    self.y_range = y_range\n",
        "\n",
        "  def forward(self, x):\n",
        "    users = self.user_factors(x[:,0])\n",
        "    movies = self.movie_factors(x[:,1])\n",
        "    res = (users * movies).sum(dim=1, keepdim=True)\n",
        "    res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
        "    return sigmoid_range(res, *self.y_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tEdltRwON8H_",
        "outputId": "3ca76a25-874f-4bca-9f85-66ff04da14a4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.889749</td>\n",
              "      <td>0.947634</td>\n",
              "      <td>00:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.663791</td>\n",
              "      <td>0.881707</td>\n",
              "      <td>00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.526889</td>\n",
              "      <td>0.861133</td>\n",
              "      <td>00:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.465532</td>\n",
              "      <td>0.845524</td>\n",
              "      <td>00:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.452247</td>\n",
              "      <td>0.841644</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_emb = DotProductBiasEmb(n_users, n_movies, n_factors=50)\n",
        "learn_emb = Learner(dls, model_emb, loss_func=MSELossFlat())\n",
        "learn_emb.fit_one_cycle(5, 5e-3, wd=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8fdXlLyVOXc3"
      },
      "outputs": [],
      "source": [
        "class DotProductBiasCP(Module):\n",
        "  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n",
        "    self.user_factors = create_params([n_users, n_factors])\n",
        "    self.user_bias = create_params([n_users])\n",
        "    self.movie_factors = create_params([n_movies, n_factors])\n",
        "    self.movie_bias = create_params([n_movies])\n",
        "    self.y_range = y_range\n",
        "\n",
        "  def forward(self, x):\n",
        "    users = self.user_factors[x[:,0]]\n",
        "    movies = self.movie_factors[x[:,1]]\n",
        "    res = (users * movies).sum(dim=1)\n",
        "    res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n",
        "    return sigmoid_range(res, *self.y_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LwWPtpWxOdeS",
        "outputId": "913bdcdb-44b2-46c4-c708-f32a64f3db79"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.892142</td>\n",
              "      <td>0.941568</td>\n",
              "      <td>00:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.681598</td>\n",
              "      <td>0.885387</td>\n",
              "      <td>00:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.526930</td>\n",
              "      <td>0.868586</td>\n",
              "      <td>00:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.448445</td>\n",
              "      <td>0.854881</td>\n",
              "      <td>00:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.445660</td>\n",
              "      <td>0.849560</td>\n",
              "      <td>00:14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_cp = DotProductBiasCP(n_users, n_movies, n_factors=50)\n",
        "learn_cp = Learner(dls, model_cp, loss_func=MSELossFlat())\n",
        "learn_cp.fit_one_cycle(5, 5e-3, wd=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NGJ9C5sQmax"
      },
      "source": [
        "The top 5 movies (based on learned bias) for each model are the same. Note that since `DotProductBiasEmb` uses `Embedding`s for the bias, I have to access the `weight` of the `Embedding` so I can sort the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9waSmYjPRcB",
        "outputId": "f220b8a6-20a0-4a00-d121-fbb43a59a870"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Schindler's List (1993)\",\n",
              " 'Titanic (1997)',\n",
              " 'Shawshank Redemption, The (1994)',\n",
              " 'Good Will Hunting (1997)',\n",
              " 'Star Wars (1977)']"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Embedding model\n",
        "movie_bias_emb = learn_emb.model.movie_bias.weight.squeeze()\n",
        "idxs = movie_bias_emb.argsort(descending=True)[:5]\n",
        "[dls.classes['title'][i] for i in idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ9WEc-ZQdyx",
        "outputId": "1d55b121-5544-46ea-92a4-1256900b0960"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Schindler's List (1993)\",\n",
              " 'Titanic (1997)',\n",
              " 'Shawshank Redemption, The (1994)',\n",
              " 'Good Will Hunting (1997)',\n",
              " 'Star Wars (1977)']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create_params model\n",
        "movie_bias_cp = learn_cp.model.movie_bias.squeeze()\n",
        "idxs = movie_bias_emb.argsort(descending=True)[:5]\n",
        "[dls.classes['title'][i] for i in idxs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRYVswnlLDpK"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFRQF297LjvV"
      },
      "source": [
        "As always, I am reminded of the value that Jupyter Notebooks can bring to the coding (and learning) experience. Being able to run each line of code in the two variants of the `DotProductBias` model while writing out formatted explanatory text solidifies my understanding of how a batch of data passes through each model. Telling a story while writing code is a satisfying experience.\n",
        "\n",
        "This exercise also illustrates how much the behavior of a model changes with a seemingly small difference----both `Embedding` and `create_params` output tensors (and allow for the same type of training) but they are of different shapes; the construction of `Embedding`s and the inputs to `create_params` are different as well. These differences trickle through the entire model as well as the post-training analysis.\n",
        "\n",
        "I hope you enjoyed this blog post! Follow me on Twitter [@vishal_learner](https://twitter.com/vishal_learner)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
