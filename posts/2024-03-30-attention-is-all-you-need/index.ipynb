{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: 'Paper Summary&#58; Attention is All You Need'\n",
        "date: \"2024-03-30\"\n",
        "author: Vishal Bakshi\n",
        "description: A summary of research introducing the Transformer architecture and a code walkthrough for the Encoder and Decoder.\n",
        "categories:\n",
        "    - paper summary\n",
        "    - deep learning\n",
        "    - LLM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J7Wx_SuOuNn"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwhVFmpJO4hU"
      },
      "source": [
        "In this notebook I'll provide a summary of the [Attention is All You Need]() paper. I'll also heavily reference the fantastic code walkthroughs by CodeEmporium on YouTube for the [Encoder](https://youtu.be/g3sEsBGkLU0?feature=shared) and [Decoder](https://youtu.be/MqDehUoMk-E?feature=shared).\n",
        "\n",
        "Other resources that were critical to my understanding of this paper:\n",
        "\n",
        "- Benjamin Warner's two-part blog post on creating a transformer from scratch ([Attention mechanism](https://benjaminwarner.dev/2023/07/01/attention-mechanism) and the [rest of the transformer](https://benjaminwarner.dev/2023/07/28/rest-of-the-transformer)).\n",
        "- [Introduction to RNNs](https://maximilian-weichart.de/posts/rnn-1/) by Max Weichart.\n",
        "- [The Illustrated Transformer]() and [The Illustrated GPT-2](https://maximilian-weichart.de/posts/rnn-1/) by Jay Alammar.\n",
        "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.\n",
        "- [Understanding Encoder and Decoder LLMs](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder) by Sebastian Raschka.\n",
        "- [What are Word and Sentence Embeddings?](https://txt.cohere.com/sentence-word-embeddings/) by Cohere.\n",
        "- [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8) by the AI Hacker.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lPS0QePQUrK"
      },
      "source": [
        "## Sequence Modeling Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzR4FV8Cf13z"
      },
      "source": [
        "Before getting into the details of the Transformer architecture introduced in this paper, I'll do a short overview of the main type of architecture (RNN) that the Transformer is improving upon. Most importantly, the Transformer is improves upon the dependencies between tokens in long sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqplH6Vbb0xG"
      },
      "source": [
        "### Recurrent Neural Nets (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQY0I2mpcNg5"
      },
      "source": [
        "In Max's post he provides the following illustration of RNNs, where the inputs are recursively passed through the hidden laye and at each iteration, the hidden layer state from the previous step is incorporated in the current state's calculation. In this way, RNNs store information about the previous step in the next step.\n",
        "\n",
        "The `forward` pass in Max's post is given as:\n",
        "\n",
        "```python\n",
        "def forward(self, X, i=0, h=0):\n",
        "  l1 = lin(X[i], self.w1)\n",
        "  h = relu(l1 + h*self.w2)\n",
        "  if (i+1 != len(X)):\n",
        "    return self.foward(X, i+1, h)\n",
        "```\n",
        "\n",
        "The hidden state from the previous iteration `h` is multiplied by a trainable weight `w2`, added to the output of the linear function `lin(X[i], self.w1)` and passed through a non-linearity (in this case a ReLU) to get the current state `h`. Until the end of the input sequence is reached, the forward pass continues to recursively incorporate previous state information into the current input's calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g_H1CFQhD6B"
      },
      "source": [
        "<img src=\"2.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuYAfZdSdUYC"
      },
      "source": [
        "> The study of RNNs highlights how, in the basic RNN architecture, as the time instants considered increase, the product chain determined by backpropagation through time **tends to zero** or **tends to extremely large values**. In the first case, we have a **vanishing gradient**, in the second case an **exploding gradient**. ([source](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvFJhGc-b2wk"
      },
      "source": [
        "### Long Short-Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui_sK4xZdlWB"
      },
      "source": [
        "To combat this training instability for long sequences, the LSTM network is used. This is an RNN architecutre capable of learning long-term dependencies with long-term memory (cell state C in the diagram) and short-term memory (hidden state H in the diagram)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MYI-JEvhHRE"
      },
      "source": [
        "<img src=\"3.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MmzCdyOd1CN"
      },
      "source": [
        "([source](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kkIJTnnd68Z"
      },
      "source": [
        "The LSTM uses past information (H) and new information (X) to **update long-term memory (C)**. It then uses C to **update H**, and the cycle continues for the next input sequence.\n",
        "\n",
        "In the diagram below, at the bottom left, the Hidden state from the previous step, $\\textbf{H}_{t-1}$ is combined with the new Input of the current step $\\textbf{X}_t$ and goes into the different gates for different purposes (Forget gate, Input gate, Candidate memory, and Output gate).\n",
        "\n",
        "The $+$ operator is the combining of long-term memory from the previous step $\\textbf{C}_{t-1}$ with the output of the Candidate memory $\\tilde{C_t}$.\n",
        "\n",
        "Finally, the Output gate $\\textbf{O}_t$ combines the long-term memory $\\textbf{C}_t$ with the sigmoid output of $\\textbf{H}_{t-1}$ and $\\textbf{X}_t$ to create the hidden state for the current step $\\textbf{H}_t$.\n",
        "\n",
        "The hidden state and long-term memory are then used in the next input step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ABbK2bAhsRR"
      },
      "source": [
        "<img src=\"4.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grs0CJ_NgRyz"
      },
      "source": [
        "## Transformer Architecture Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_79A1rLgUxX"
      },
      "source": [
        "- Does not use recurrence.\n",
        "- Relies entirely on the **attention mechanism** for global dependencies.\n",
        "- Allows for parallelization (as opposed to sequential processing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0jikuUPggpE"
      },
      "source": [
        "> The Transformer achieves better [BLUE scores]() than previous state-of-the-art (SOTA) models on the English-to-German and English-to-French machine translation tests at a fraction of the training cost.\n",
        "\n",
        "<img src=\"5.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFANG0sjh49r"
      },
      "source": [
        "Here are the current English-to-German SOTA results:\n",
        "\n",
        "<img src=\"6.png\" style=\"width:100%;\">\n",
        "\n",
        "[source](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkS9BT0MiBcr"
      },
      "source": [
        "And the current English-to-French SOTA results:\n",
        "\n",
        "<img src=\"7.png\" style=\"width:100%;\">\n",
        "\n",
        "[source](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-french)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHYZzYTtiLa6"
      },
      "source": [
        "Here is the paper's beautiful diagram (with my annotations) of an Encoder-Decoder Transformer architecture:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUw-59P4jFXx"
      },
      "source": [
        "<img src=\"8.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzVwIvfojGul"
      },
      "source": [
        "The inputs (numericalized tokens) pass through the Input Embedding which projects these numbers into a much larger number of dimensions, dimensions in which different information about the tokens will be learned through training. In this paper they use a dimension of 512 (referred to as the \"hidden dimension\"). This value is a hyperparameter and different architectures use different numbers of hidden dimensions.\n",
        "\n",
        "The output of this Embedding is passed through a Positional Encoding step which quantitatively stores information about the position of each token. Since Transformers don't explicitly express position as sequence modeling does, we have to implicitly express position in this way.\n",
        "\n",
        "The inputs, after going through the Embedding and Positional Encoding, now enter the **Encoder** which is a type of **Transformer Block** containing Mult-Head Attention, Add & Norm layers and a Feed Forward Network.\n",
        "\n",
        "The outputs follow a similar path, first through an Output Embedding, then Positional Encoding, and then a **Decoder** which is another type of **Transform Block**. A Transformer can be Encoder-only, Decoder-only or Encoder-Decoder. In this paper they focus on Encoder-Decoder Transformers, where information learned in the Encoder is used in the Decoder in a process call cross-attention that we'll look into shortly. In this paper, they have 6 Encoder blocks and 6 Decoder blocks. The number of blocks can be varied (i.e. it's a hyperparameter).\n",
        "\n",
        "The outputs of the Decoder pass through a final linear layer and then a softmax layer (transformed into 0 to 1.0 probabilities)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJNVIS_Qzfqm"
      },
      "source": [
        "> The encoder receives the input text that is to be translated, and the decoder generates the translated text. ([source](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder))\n",
        "\n",
        "\n",
        "> Fundamentally, both encoder- and decoder-style architectures use the same self-attention layers to enocde word tokens. However, the main difference is that **encoders are designed to learn embeddings** that can be used for various predictive modeling tasks such as classification. In contrast, **decoders are designed to generate new texts**, for example, answering user queries. ([source](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder))\n",
        "\n",
        "> The encoder part in the original transformer...**is responsible for understanding and extracting the relevant information from the input text**. It then outputs a continuous representation (embedding) of the input text that is passed to the decoder. Finally, **the decoder generates the translated text** (target language) based on the continuous representation received from the encoder. ([source](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2_nBm3v0OQ6"
      },
      "source": [
        "From Benjamin Warner's post:\n",
        "\n",
        "> If we use the first sentence in this post and assume each word is a token:\n",
        ">  \n",
        "> Transformers are everywhere.  \n",
        ">   \n",
        "> then the \"Transformers\" token would predict \"are\", and \"are\" would predict \"everywhere.\"\n",
        ">  \n",
        "> To create our inputs (line 1) we'll drop the last token and to create the labels (line 2) we'll remove the first token:\n",
        ">  \n",
        "> 1. Transformers are\n",
        "> 2. are everywhere  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AmedT4m00vy"
      },
      "source": [
        "## Code Overview: Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1LYpYBv03-Y"
      },
      "source": [
        "In this section I'll walk through some of the code presented in the YouTube video by CodeEmporium.\n",
        "\n",
        "I'll start by defining some constants that I'll use throughout. `d_model` is the hidden dimension hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "TyE1L5AL_ti4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "fcSel6M5OoUv"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "max_sequence_length = 200\n",
        "d_model = 512\n",
        "vocab_size = 10_000\n",
        "context_size = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVh_Os65_vwY"
      },
      "source": [
        "In a real scenarios, the inputs (`tokens`) would be numericalized tokens corresponding to a real natural language dataset. In this example, I'll use random integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3liQP7C_rqJ",
        "outputId": "0dbd523c-439d-4a03-98e0-148d9dfb8eb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 200]), tensor(0), tensor(9999))"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = torch.randint(0, vocab_size, (batch_size, max_sequence_length))\n",
        "tokens.shape, tokens.min(), tokens.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zwW7iESBv08"
      },
      "source": [
        "I have 64 batches of 200 tokens each, where each token is an integer from 0 to 10_000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaDP_nqSCE2s"
      },
      "source": [
        "### Input Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY-rvFsoCHTR"
      },
      "source": [
        "The input Embedding is a PyTorch object which takes an integer and returns a tensor of a given dimension (in this case 512)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTDMaI_8Brut",
        "outputId": "4947c29e-1a86-46b0-b8a2-ccb14279e91f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(10000, 512)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_embed = nn.Embedding(vocab_size, d_model)\n",
        "vocab_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4aebwCYCgUA"
      },
      "source": [
        "When I pass a tensor integer, I get in return a 512 dimension tensor filled with float values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paqqkr2qCYiv",
        "outputId": "33cc5ecd-f487-47dd-93bf-ac77576741d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_embed(torch.tensor([4])).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UjE9ti8CcQV",
        "outputId": "77a63ecb-d4ec-47cd-e918-a6ccadda8829"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0996,  1.2077, -0.8627, -0.4755,  0.5210], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_embed(torch.tensor([4]))[0][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsvPdPnuCstb"
      },
      "source": [
        "When I pass my batched `tokens` to the Embedding, I get back a batched set of 512 float values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAJy9l1ZCmrj",
        "outputId": "95b2b9c4-8398-43d6-d44f-a97913517acf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 512])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_embs = vocab_embed(tokens)\n",
        "token_embs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVx5p8JfC95z"
      },
      "source": [
        "In other words, my tokens, which are integers that represent natural language, are now projected into 512 dimensions, dimensions in which the Embedding will learn something about the tokens and therefore about language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQyLzXvDN28"
      },
      "source": [
        "### Positional Encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHrMt1rhIhu4"
      },
      "source": [
        "The formula used in the paper for positional encodings are as follows (sine for even i values and cosine for odd):\n",
        "\n",
        "$$PE_{(pos, 2i)} = \\sin(\\text{pos} / 10000^{2i/d_{model}})$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos(\\text{pos} / 10000^{2i/d_{model}})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51wbvG3XJ5Yl"
      },
      "source": [
        "I'll reuse the code provided in [Benjamin's blog post](https://benjaminwarner.dev/2023/07/28/rest-of-the-transformer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "MQitrizDC3wo"
      },
      "outputs": [],
      "source": [
        "# create the positional encoding tensor of shape\n",
        "# maximum sequence length (MS) by embedding dimension (C)\n",
        "pe = torch.zeros(context_size, d_model, dtype=torch.float)\n",
        "\n",
        "# pre-populate the position and the div_terms\n",
        "position = torch.arange(context_size).unsqueeze(1)\n",
        "div_term = torch.exp(\n",
        "    torch.arange(0, d_model, 2) * (-math.log(10000) / d_model)\n",
        ")\n",
        "\n",
        "# even positional encodings use sine, odd cosine\n",
        "pe[:, 0::2] = torch.sin(position * div_term)\n",
        "pe[:, 1::2] = torch.cos(position * div_term)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCcVYP0BNepj"
      },
      "source": [
        "I want to make sure I understand the `div_term` since I didn't understand it at first glance:\n",
        "\n",
        "```python\n",
        "torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000) / d_model))\n",
        "```\n",
        "\n",
        "Translating that to math gives us:\n",
        "\n",
        "$$\\exp\\big(-2i * \\ln(10000) / d_{model}\\big)$$\n",
        "\n",
        "Using the negative exponent rule: $\\exp(-a) = \\frac{1}{\\exp(a)}$:\n",
        "\n",
        "$$\\exp\\big({\\frac{-2i * ln(10000)}{d_{model}}}\\big) = \\frac{1}{\\exp \\big( \\frac{2i * ln(10000)}{d_{model}}\\big)}$$\n",
        "\n",
        "Using the power of a power rule: $\\exp(ab) = \\exp(a)^b$:\n",
        "\n",
        "$$\\frac{1}{\\exp \\big( \\frac{2i * ln(10000)}{d_{model}}\\big)} = \\frac{1}{\\exp\\big(\\ln(10000)\\big)^{2i/d_{model}}}$$\n",
        "\n",
        "The term $\\exp(\\ln(10000))$ equals just $10000$:\n",
        "\n",
        "$$\\frac{1}{\\exp\\big(\\ln(10000)\\big)^{2i/d_{model}}} = \\frac{1}{10000^{2i/d_{model}}}$$\n",
        "\n",
        "Which is the same as the divison term in the paper's math formula.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb1XGJEsmQHl",
        "outputId": "b562b045-e99c-4adb-bba1-1a149789594c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0., 1., 0., 1., 0.]),\n",
              " tensor([0.8415, 0.5403, 0.8219, 0.5697, 0.8020]))"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pe[0][:5], pe[1][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0fU6CQsnHnE"
      },
      "source": [
        "I'll add the positional encoding to the embedded tokens---note that here PyTorch uses broadcasting to \"copy\" `pe` over each of the 64 batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqZYp7WFnzKJ",
        "outputId": "92ff5f5e-fb66-4617-c466-3ee1102a263a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([200, 512]), torch.Size([64, 200, 512]))"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pe.shape, token_embs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHExlz91moZ8",
        "outputId": "628f707e-e9ba-4c65-ba49-6a78e8fbf6b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 512])"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_embs = token_embs + pe\n",
        "token_embs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE8SgmgNn5oj"
      },
      "source": [
        "In CodeEmporium's implementation, at this point `token_embs` is passed through a `Dropout` layer, so I'll do the same:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuIBYC3nn2zw",
        "outputId": "4bf64501-6e43-44c5-e535-f2e7b911c870"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 512])"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_drop = nn.Dropout(0.1)\n",
        "x = embed_drop(token_embs)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKjf_4RsoMqs"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC_LrZ_BoROU"
      },
      "source": [
        "At this point, the inputs are now ready to enter the attention mechanism. Before we do that, I'll save the current state of the inputs in a variable so that later on I can add it to the output of the attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "vUr_2UeCoJ1r"
      },
      "outputs": [],
      "source": [
        "residual_x = x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qe9FvXMomXQ"
      },
      "source": [
        "The particular flavor of attention used at this point is Scaled Dot-Product Attention across multiple heads. Here's the steps taken in Scaled Dot-Product Attention:\n",
        "\n",
        "<img src=\"9.png\" style=\"width:100%;\">\n",
        "\n",
        "Where $Q$ (query), $K$ (key) and $V$ (value) are matrices (initially of random numbers) that consist of learned weights during training.\n",
        "\n",
        "The first step is the matrix multiplication of $Q$ and $K^T$, followed by scaling that result by the square root of the dimension $d_k$. The encoder doesn't have a mask (the decoder does). Finally, the softmax is taken of that scaled dot product and its output matrix multiplied with $V$.\n",
        "\n",
        "Here's a conceptual understanding of attention from the [Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/):\n",
        "\n",
        "<img src=\"10.png\" style=\"width:100%;\">\n",
        "\n",
        "And here's a visualization of attention values between tokens:\n",
        "\n",
        "<img src=\"11.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLfQKKNf3kRU"
      },
      "source": [
        "Before we get into the code for attention, here is a visualization of Mult-Head Attention, where the Scaled Dot-Product Attention occurs simultaneously across multiple heads, displaying the parallelization capability of Transformers:\n",
        "\n",
        "<img src=\"12.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6-JBRpM4wRp"
      },
      "source": [
        "We'll go bottom-up in the diagram:\n",
        "\n",
        "- Create Q, K, V matrices. Split them across $h$ heads.\n",
        "- Perform Scaled Dot-Product Attention.\n",
        "- Concatenate them from $h$ heads.\n",
        "- Pass them through a final Linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRSU-i725WpJ"
      },
      "source": [
        "Both Benjamin and CodeEmporium created a single Linear layer and then split them into $Q$, $K$ and $V$, so I'll do the same. A reminder (to myself and the reader) that these are weight matrices that will be used eventually to multiply by the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wau6d8zkodwm",
        "outputId": "fcaea4bf-91bb-4805-8d51-56206a952bbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=1536, bias=True)"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
        "qkv_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGBTOY4k3i0B"
      },
      "source": [
        "Passing the inputs through this linear layer gives us the matrices:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvS_8EuR3lLG",
        "outputId": "e69fbc95-a72c-4d21-d006-3b3ce1016c17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 1536])"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qkv = qkv_layer(x)\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blcr1U3o3tdd"
      },
      "source": [
        "Next, we project the $Q$, $K$ and $V$ combined matrix across 8 heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZuPvdfX3soG",
        "outputId": "53aea929-2095-4fbb-e364-838d30780957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 8, 192])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_heads = 8\n",
        "head_dim = d_model // num_heads\n",
        "\n",
        "qkv = qkv.reshape(batch_size, max_sequence_length, num_heads, 3 * head_dim)\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDjNnHmnhHR3"
      },
      "source": [
        "This splits the 1536 values into 8 sets of 192."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4xGhnDu4N0S"
      },
      "source": [
        "In CodeEmporium's code, they swap the middle two dimensions so it's broadcastable with tensors later on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NXvRUNx4akf",
        "outputId": "8b699017-98c7-4d58-f9d0-cd6559eefae6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 8, 200, 192])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qkv = qkv.permute(0, 2, 1, 3)\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRKiElhR4exk"
      },
      "source": [
        "We then split `qkv` into three separate matrices, each with 200 x 64 values on each of the 8 heads:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJLeGENC4diG",
        "outputId": "8473b3a4-112e-457b-ebeb-19a68117225d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 8, 200, 64]),\n",
              " torch.Size([64, 8, 200, 64]),\n",
              " torch.Size([64, 8, 200, 64]))"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q, k, v = qkv.chunk(3, dim=-1)\n",
        "q.shape, k.shape, v.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGStQwF742NC"
      },
      "source": [
        "Finally, we can create the attention matrix. First we perform the scaled dot-product between $Q$ and $K$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB1qBwyJ5GFQ",
        "outputId": "284129bf-3ccd-4946-ded0-036d91bc4cb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 8, 200, 200])"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_k = torch.tensor(q.shape[-1]) # 64\n",
        "\n",
        "scaled_dot_product = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(d_k)\n",
        "scaled_dot_product.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-9uk1TahcNi"
      },
      "source": [
        "Note that when $K$ is transposed, the last two dimensions are swapped to allow for correct matrix multiplication dimension order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bj0rLjhhTE1",
        "outputId": "a711b3a6-3d6d-4882-be3e-d3d2e41d985c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64, 8, 200, 64]),\n",
              " torch.Size([64, 8, 200, 64]),\n",
              " torch.Size([64, 8, 64, 200]))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.shape, k.shape, k.transpose(-1, -2).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz6As2uwho30"
      },
      "source": [
        "The dimension of 64 matches between $Q$ and $K^T$ after `.transpose(-1 ,-2)` swaps the last two dimensions of $K$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4fUsxPRifok"
      },
      "source": [
        "One thing I noticed in both Benjamin and CodeEmporium's code is that they define attention as the output of passing the scaled dot-product through softmax. This is the \"attention matrix\" I've seen referred to in places. The paper defines attention as the product of the matrix multiplication between that softmax output and the $V$ (values) matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgEUgqQAjElX"
      },
      "source": [
        "<img src=\"13.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1ICdkYYWhkpC"
      },
      "outputs": [],
      "source": [
        "attention = F.softmax(scaled_dot_product, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0xKFS5_mu1U",
        "outputId": "e58df8a7-d850-4a54-91cc-e8282e8bd944"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 8, 200, 200])"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK7QtGCqjo_H"
      },
      "source": [
        "`dim` is set to `-1` so that the values in the last dimension are between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eRba42vlthp",
        "outputId": "cb0fb0ba-05a6-4de0-aee3-1bc2f3bfe8e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([200]), tensor(1., grad_fn=<SumBackward0>))"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention[0,0,0].shape, attention[0,0,0].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX7WfU82mwf3"
      },
      "source": [
        "`attention`'s final dimensions are of size 200 x 200, representing weights corresponding to the relationship between each of the 200 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2psyipu4mMdw",
        "outputId": "93492e91-1fbc-4434-95bd-ff4b3f119004"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 8, 200, 64])"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = torch.matmul(attention, v)\n",
        "v.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3j64WMcmCDi"
      },
      "source": [
        "From Benjamin's post:\n",
        "\n",
        "> Next we matrix multiply the Attention weights with our value matrix $V$ which applies the Attention weights to our propagating token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLxLCfV6mUEP",
        "outputId": "ae602d87-64c7-407b-8a92-bfc7f48c0130"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 512])"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RQbh7A4k_QG",
        "outputId": "e2efde1c-ace5-47bd-913c-7609a4d2a85d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 8, 200, 64])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = attention @ v\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVfuoNF-mguu"
      },
      "source": [
        "Next, we have to concatenate across the 8 heads:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CInhUeymXEx",
        "outputId": "e595a3fc-bf0d-4ed0-b206-dee32d5486cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 512])"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = x.reshape(batch_size, max_sequence_length, num_heads * head_dim)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmJ7K-WCocA8"
      },
      "source": [
        "Now the 64 dimensions across 8 heads are concatenated to get back to the embedding size of 512. We still maintain the 64 batches and 200 sequence length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFCxk1Olpn9X"
      },
      "source": [
        "The last step before the attention mechanism is fully complete is to pass these values through a linear layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmLoWUeKobKi",
        "outputId": "c2ba3590-9b0a-44ea-febb-4f67c36d3537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 200, 512])"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_layer = nn.Linear(d_model, d_model)\n",
        "x = linear_layer(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z21AfGLapzYI"
      },
      "source": [
        "The linear layer maintains the dimension (512 in, 512 out)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEFvJOkqPYQ"
      },
      "source": [
        "`x` then passes through a Dropout layer and a Layer Normalization layer. Note that `residual_x` is added to `x` before the sum is passed through the Layer Normalization.\n",
        "\n",
        "I won't walk through the details of Layer Normalization, but CodeEmporium provides the following code that I'll highlight the following few lines from:\n",
        "\n",
        "```python\n",
        "mean = x.mean(dim=[-1], keepdim=True)\n",
        "var = ((x - mean) ** 2).mean(dim=[-1], keepdim=True)\n",
        "std = (var + 1e-5).sqrt()\n",
        "\n",
        "y = (x - mean) / std\n",
        "x = gamma * y + beta\n",
        "```\n",
        "\n",
        "Where `gamma` and `beta` are learnable `nn.Parameter` weights. Note that the values are normalized (resulting in `y`) and then normalization occurs across all samples (`gamma * y + beta`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mCX1161r4As"
      },
      "source": [
        "`x` is stored as `residual_x` to add on later, and then `x` goes through a Feed Forward Network (a non-linearity, in this case a GELU, and a Dropout layer sandwiched between two linear layers), and then through another Dropout layer and Layer Normalization (where `residual_x` is added to `x`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siGnvMFssXUr"
      },
      "source": [
        "## Code Overview: Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjC5dZfwsv5a"
      },
      "source": [
        "There are some similarities and some differences between the Encoder and the Decoder. Note that in CodeEmporium's implementation, the Decoder contains Self Attention and Encoder-Decoder Attention (also called [Cross Attention in Benjamin's post](https://benjaminwarner.dev/2023/07/01/attention-mechanism#cross-attention)).\n",
        "\n",
        "<img src=\"14.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0n3ngkMFexM"
      },
      "source": [
        "The first main difference is that what goes into the Decoder are the outputs (the inputs shifted by one token)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHMhpgXhuRxl"
      },
      "source": [
        "In the Decoder, attention is masked. Only the current token and previous output tokens are \"visible\" to the model. Future tokens are masked. How does this masking take place? Here's CodeEmporium's code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB6CsDThCU4S"
      },
      "source": [
        "Start by creating a 200 x 200 tensor full of negative infinity (negative infinity is used so that when you take the softmax of it, it goes to 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFlYq0rXpyWk",
        "outputId": "696ce332-24a3-4906-8b07-f4638762ab64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        ...,\n",
              "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = torch.full([max_sequence_length, max_sequence_length], float('-inf'))\n",
        "mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2BPFM4VCdRd"
      },
      "source": [
        "Keep the upper triangle as `-inf` and make everything else 0 with `torch.triu`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M6kclVECcpE",
        "outputId": "65c248a7-9792-4a0c-a427-9234184d5f7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = torch.triu(mask, diagonal=1)\n",
        "mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUJ643K8Co2p"
      },
      "source": [
        "Now, when the mask is added to the scaled dot-product, the upper triangle will go to `-inf` (since anything plus `-inf` is `-inf`). Taking the softmax of that to get the attention matrix will result in a matrix with an upper triangle of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zCCBQFuCnRd",
        "outputId": "c05aa863-952e-4670-b570-ca43e3f27cf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-9.5668e-01,        -inf,        -inf,  ...,        -inf,\n",
              "                -inf,        -inf],\n",
              "        [-3.3439e-01, -1.0772e+00,        -inf,  ...,        -inf,\n",
              "                -inf,        -inf],\n",
              "        [-2.8391e-01,  2.7374e-02,  7.6844e-01,  ...,        -inf,\n",
              "                -inf,        -inf],\n",
              "        ...,\n",
              "        [ 3.8771e-04, -2.7279e-01,  3.2622e-01,  ..., -1.5672e-01,\n",
              "                -inf,        -inf],\n",
              "        [ 9.1237e-01,  9.8978e-01,  8.4105e-02,  ...,  7.8569e-01,\n",
              "         -1.8654e-03,        -inf],\n",
              "        [-8.3207e-01, -1.6773e-01, -8.6295e-01,  ..., -3.1891e-01,\n",
              "         -7.7460e-01, -8.4962e-01]], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled_dot_product = scaled_dot_product + mask\n",
        "scaled_dot_product[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5BPidM3C98y",
        "outputId": "c35ccf8d-b114-4270-e8b4-f3942beadfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.6776, 0.3224, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.1912, 0.2610, 0.5477,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        ...,\n",
              "        [0.0049, 0.0037, 0.0068,  ..., 0.0042, 0.0000, 0.0000],\n",
              "        [0.0073, 0.0079, 0.0032,  ..., 0.0065, 0.0029, 0.0000],\n",
              "        [0.0031, 0.0060, 0.0030,  ..., 0.0052, 0.0033, 0.0030]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.softmax(scaled_dot_product, dim=-1)[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niYKqTHRDRq6"
      },
      "source": [
        "Cross Attention works differently---the \"Cross\" in Cross Attention is talking about the relationship between the Encoder and Decoder. Specifically, the $K$ and $V$ weights are applied to the Encoder outputs and the $Q$ weights are applied to the Decoder outputs. The rest of the process (scaled dot product, softmax, concatenation, linear layer) are the same as before (with the addition of adding the `mask` to the scaled dot product)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYjwHxJ9FWfA"
      },
      "source": [
        "After passing the through Cross Attention, the outputs go through Dropout and Layer Normalization, then a Feed Forward Network, and then through another Dropout and Layer Normalization step. The inputs to the Layer Normalization call are the `residual_x` plus `x`, which is said to stabilize the training process.\n",
        "\n",
        "Finally, the outputs go through a final linear layer which projects the outputs to the vocabulary size and then a final softmax call which converts those logits to probabilities per vocabulary token (in other words, answering the question: what are the probabilities that the next token will be each token in the vocabulary?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tVMc7xGgoB"
      },
      "source": [
        "There are a lot of details that I have left out of this post for brevity so to get the full Transformers code experience, see Benjamin's [commented-transformers](https://github.com/warner-benjamin/commented-transformers) repository, and CodeEmporium's [Encoder](https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Transformer_Encoder_EXPLAINED!.ipynb)/[Decoder](https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Transformer_Decoder_EXPLAINED!.ipynb) notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fazPyUeUHIAM"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f4eXu_yHJ5n"
      },
      "source": [
        "I was pleasantly surprised at how understandable the code is for the Transformer architecture. The paper does a great job of helping the reader visualizing the concepts in play, especially the process involved in calculating scaled dot-product attention across multiple heads. The number excellent resources available that I've referenced throughout this blog post are also essential to understanding the code and concepts involved.\n",
        "\n",
        "On a personal note, I recall going to a presentation on this paper a few years ago and leaving feeling so incredibly lost, and that maybe I wouldn't understand how this critical architecture actually works---like I had hit a wall of complexity that I wouldn't be able to overcome. Reading this paper, understanding it, presenting on it and writing this blog post felt like redemption for me. I obviously couldn't have done it without the excellent resources I've linked above.\n",
        "\n",
        "As always, I hope you enjoyed this paper summary!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
