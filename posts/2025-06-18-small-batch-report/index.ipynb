{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Takeaways from Gemini Deep Research Report on Small Batch Training Challenges\n",
        "date: 2025-06-18\n",
        "author: Vishal Bakshi\n",
        "description: Motivated by a twitter interaction, I had Gemini generate a report on the challenges and solutions to small batch training. In this blog post I highlight key takeaways from that report, supplemented by my own deep dives on CrossEntropyLoss and Group Normalization, to arrive at next steps for my Imagenette experiments.\n",
        "filters:\n",
        "    - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - fastai\n",
        "    - imagenette\n",
        "    - TinyScaleLab\n",
        "    - deep learning\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbpUCa3fVPTX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pp7Faz6efpR"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQk_Jfs6ey7C"
      },
      "source": [
        "I've recently been training models on the fastai [Imagenette dataset](https://github.com/fastai/imagenette?tab=readme-ov-file) to gain some intuition on what improves downstream performance (accuracy). [My latest experiment](https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/) was to train three models on different batch sizes and learning rates (for a fixed 5 epochs) to understand the relationship between the two.\n",
        "\n",
        "I posted my initial musing (after I had analyzed results for batch sizes 32 to 2048) that lowering the batch size might continue to yield higher accuracy. Jeremy validated this approach:\n",
        "\n",
        "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">That’s a great question! Stable low bs training isn’t easy, but it’s a good plan :)</p>&mdash; Jeremy Howard (@jeremyphoward) <a href=\"https://twitter.com/jeremyphoward/status/1930486029607416069?ref_src=twsrc%5Etfw\">June 5, 2025</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMwAPnIMgr7K"
      },
      "source": [
        "I also prompted Allen AI's [Paper Finder](https://paperfinder.allen.ai/chat/606ffc67-5539-47a9-8c8a-ec5820666ccc?profile=paper-finder-only) with:\n",
        "\n",
        "> strategies to make low batch size training more stable and improve accuracy for image recognition\n",
        "\n",
        "Imagenette is unlocking an entire new lane of research and experimentation for me, helping me towards my [TinyScaleLab project goal](https://vishalbakshi.github.io/blog/posts/2025-04-26-TinyScale-Lab-Kickoff/) of training and analyzing high performant tiny language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb52nZFVgvBt"
      },
      "source": [
        "In this blog post, I'll walk through the Gemini Deep Research and Ai2 Paper Finder findings. The full Deep Research report is in [this Google Doc](https://docs.google.com/document/d/1AWoW4sOQ_iR_3pxsTRzYOT0cql60ux0Sjrt4rbLilZM/edit?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5-v_XXHh24_"
      },
      "source": [
        "## Gemini's Deep Research Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsG4wm3FoWNR"
      },
      "source": [
        "Here was my initial prompt:\n",
        "\n",
        "> I recently have been training xresnet18, xresnet34 and xse_resnext50 on the Imagenette dataset by fast.ai (which is a 10k subset of ImageNet with 10 easily classified classes). I have trained on batch sizes from 1 to 2048. Generally speaking, the highest accuracy achieved increases from a batch size of 1 to a batch size of 8, 16 or 32 (depending on the model/learning rate) and then decreases as batch size increases to 2048. This makes me want to explore small batch size training for this project. Jeremy Howard tweeted that \"Stable low bs training isn’t easy, but it’s a good plan\". I want you to help me answer two questions in this chat:\n",
        ">\n",
        ">\n",
        "> 1. Why is stable low batch size training difficult?\n",
        ">\n",
        "> 2. What techniques are there (either in literature/arxiv, blog posts or forums) to make low batch size training stable? I'm most interested in improving the accuracy of low batch size trainings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rirm3NYujWZO"
      },
      "source": [
        "I'll paste the entire executive summary it generated, as its a good one (emphasis mine):\n",
        "\n",
        "> Training deep learning models with small batch sizes presents a unique set of challenges, primarily stemming from high gradient variance during optimization and the inherent limitations of standard Batch Normalization with few samples. However, this training regime also offers the potential for <mark>improved model generalization</mark>. This report investigates the difficulties associated with stable low batch size training and explores techniques to mitigate these issues, with a particular focus on enhancing model accuracy. For architectures like xresnet, overcoming the instability of Batch Normalization with small batches is a critical first step, often addressed by substituting it with alternatives like <mark>Group Normalization</mark>. Stability and accuracy can be further improved through <mark>careful management of learning rates</mark>, including the use of adaptive schedulers and awareness of optimizer-specific phenomena such as the \"surge\" in optimal learning rates for Adam-family optimizers. <mark>The choice of optimizer itself</mark>, typically between SGD with momentum and adaptive methods like AdamW, also plays a significant role, alongside appropriate <mark>regularization strategies tailored to the small-batch context</mark>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WHUCobDkEf8"
      },
      "source": [
        "The report starts by outlining why small batch sizes lead to unstable training. Gradient updates using a small batch size \"may not accurately reflect the gradient of the true loss function that would be computed over the entire dataset\" and \"is an estimate derived from a very limited subset of the training data\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auk9PqQufGqU"
      },
      "source": [
        "## Small Batch Sizes: The Double Edged Sword"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze0jLBvxfUpu"
      },
      "source": [
        "On one hand, small batch size for a fixed number epochs provide more gradient updates and thus lower the loss. In the plot below (xresnet18) the ideal batch size (16-32) performs better than larger batch sizes. On the other hand, the larger number of updates will take longer and the GPU will be less utilized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**xresnet18**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![xresnet18 batch size vs accuracy for different learning rates](lr_acc_18.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7i4Hiwyf7L-"
      },
      "source": [
        "<blockquote class=\"twitter-tweet\" data-dnt=\"true\" align=\"center\" data-conversation=\"none\"><p lang=\"en\" dir=\"ltr\">We covered this in some of our earlier courses - lower batch sizes provide more updates, which should give better results for a fixed # epochs.</p>&mdash; Jeremy Howard (@jeremyphoward) <a href=\"https://twitter.com/jeremyphoward/status/1927192030335132090?ref_src=twsrc%5Etfw\">May 27, 2025</a></blockquote>\n",
        "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJtQBE8alYlK"
      },
      "source": [
        "Additionally, as the Gemini report goes on to say:\n",
        "\n",
        "> Small batches can lead to gradients that are \"bigger and chaotic\" because an incorrect prediction on a single data point within a tiny batch can result in a disproportionately large loss and, subsequently, a large gradient update compared to its effect in a larger batch...high gradient noise, stemming from the limited data used for each estimation, leads to more volatile and less precise parameter updates\n",
        "\n",
        "This was a good opportunity to do a quick refresher on cross entropy loss so I worked through [Chapter 5 of fastbook](https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb) again and wrote up [a blog post](https://vishalbakshi.github.io/blog/posts/2025-06-11-CELoss/) and [video walkthrough](https://www.youtube.com/watch?v=swHhoP53jq4) on that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AheKUC7sVKXS"
      },
      "source": [
        "Suppose we have a batch size of two for a binary classification task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gNdXEdFg-cq",
        "outputId": "90fce2ca-d659-41a1-9e29-457566223b40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.4000, 9.5000],\n",
              "        [4.0000, 5.0000]], requires_grad=True)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acts = tensor([[0.4, 9.5], [4, 5]], requires_grad=True)\n",
        "acts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qb33PEAVmk5"
      },
      "source": [
        "With the following targets---the first batch item target is the first class (index `0`), which has a very wrong small activation.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fqR2k2DVevaC"
      },
      "outputs": [],
      "source": [
        "targ = tensor([0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTqOn2-V2TL",
        "outputId": "14e2581f-1023-4b2d-e27b-4abdfd25ab17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.7067, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = CrossEntropyLoss()(acts, targ)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmHAgkLMhm9n",
        "outputId": "a9c7c33f-a43e-49d1-a45b-adb6bf7a6fca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.4999,  0.4999],\n",
              "        [ 0.1345, -0.1345]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.backward()\n",
        "acts.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxOuQNMpXscd"
      },
      "source": [
        "Now suppose we had the same two items but in a batch size of 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w42nLQuvV5xy"
      },
      "outputs": [],
      "source": [
        "acts = tensor(\n",
        "    [[ 0.4000,  9.5000], # same item as before\n",
        "     [ 4.0000,  5.0000], # same item as before\n",
        "     [ 0.3367,  0.1288],\n",
        "     [ 0.2345,  0.2303],\n",
        "     [-1.1229, -0.1863],\n",
        "     [ 2.2082, -0.6380],\n",
        "     [ 0.4617,  0.2674],\n",
        "     [ 0.5349,  0.8094]], requires_grad=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy7FKaa9X8zY"
      },
      "source": [
        "Adding 6 items to the targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6WuPydKSX5PL"
      },
      "outputs": [],
      "source": [
        "targ = tensor([0, 1, 0, 1, 1, 0, 1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjzO9j7yd7dv",
        "outputId": "c1f16f85-df02-4d87-f5c2-2ca579425447"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.5563, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = CrossEntropyLoss()(acts, targ)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPTQOJLqIziC"
      },
      "source": [
        "Note that the loss is smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKMmTsO5hyse",
        "outputId": "01eb068c-1918-4dad-c0f4-594b01c14ee5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1250,  0.1250],\n",
              "        [ 0.0336, -0.0336],\n",
              "        [-0.0560,  0.0560],\n",
              "        [ 0.0626, -0.0626],\n",
              "        [ 0.0352, -0.0352],\n",
              "        [-0.0069,  0.0069],\n",
              "        [ 0.0686, -0.0686],\n",
              "        [ 0.0540, -0.0540]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss.backward()\n",
        "acts.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFNhj-VNeIz7"
      },
      "source": [
        "The loss and gradients are much smaller. The impact of the one confidently wrong prediction has decreased with larger batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VERqvXBAfOnU"
      },
      "source": [
        "There are also two other factors at play, as the Gemini report states:\n",
        "\n",
        "> On one hand, the noisy gradients can cause the optimization process to oscillate significantly around an optimal solution, making it challenging for the model to settle into a good minimum and potentially slowing down overall convergence\n",
        "\n",
        "> On the other hand, this very noise and the resultant exploratory behavior can be beneficial. The stochasticity introduced by small batches can act as a form of implicit regularization, helping the model to escape sharp, narrow local minima in the loss landscape and instead find flatter, broader minima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdsXc5i-g4Lg"
      },
      "source": [
        "The push and pull between the pros (faster updates, implicit regularization) and cons (longer training time, GPU underutilization, oscillating around minima, chaotic gradients/high gradient noise) makes small batch training a fascinating topic. The very characteristics that provide benefit to the training process also damage it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUQ0VhPSlqQo"
      },
      "source": [
        "## Small Batch Size and Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3h3ZBMkmvha"
      },
      "source": [
        "In addition to making the gradient noisy, small batch sizes make Batch Normalization statistics noisy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vVVSBa7mrY8"
      },
      "source": [
        "> BN standardizes the activations within a network by calculating mean and variance statistics per batch. When batch sizes are very small (e.g., 1, 2, or 4 samples), these batch-wise statistics become extremely noisy and unreliable estimators of the true population statistics across the entire dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-0VCvY0m2B_"
      },
      "source": [
        "From the [Group Normalization paper](https://arxiv.org/abs/1803.08494) (for which I have done a [video walkthrough](https://www.youtube.com/watch?v=ZCTcxNEGens)):\n",
        "\n",
        "> normalizing along the batch dimension\n",
        "introduces problems — BN’s error increases rapidly when\n",
        "the batch size becomes smaller, caused by inaccurate batch\n",
        "statistics estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMZQ288hvBPz"
      },
      "source": [
        "The three models I'm using all have a considerable number of Batch Normalization layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZBMZ9MaYeG1G"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDqtEZYCn9Oa",
        "outputId": "bbe244fe-90e6-455f-fdb1-f14334b8c350"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bn_count = 0\n",
        "for module in xresnet18().modules():\n",
        "  if isinstance(module, nn.BatchNorm2d): bn_count += 1\n",
        "bn_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy-Sqen4uK_c",
        "outputId": "9b07f053-6422-4605-ea78-61677ea481f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bn_count = 0\n",
        "for module in xresnet34().modules():\n",
        "  if isinstance(module, nn.BatchNorm2d): bn_count += 1\n",
        "bn_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eoWM03tu7xD",
        "outputId": "176953d5-7ca7-4866-bd9c-695e5105e9a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bn_count = 0\n",
        "for module in xse_resnext50().modules():\n",
        "  if isinstance(module, nn.BatchNorm2d): bn_count += 1\n",
        "bn_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0cPEzdCvSi7"
      },
      "source": [
        "## Techniques for Stable and Accurate Low Batch Size Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHZzEPGLvcCx"
      },
      "source": [
        "The fascinating complexity of stable small batch training is summarized in the report with the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjjo77TIvZnY"
      },
      "source": [
        "> Addressing the challenges of small batch training requires a multi-pronged approach, focusing on adapting learning rate strategies, rethinking normalization layers, selecting appropriate optimizers, and employing advanced gradient management and regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tP59P_PvwTA"
      },
      "source": [
        "### Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1G-SkwVvxLI"
      },
      "source": [
        "> for small batches, the high gradient noise often necessitates smaller learning rates to prevent divergence and ensure stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2u5GQn9KSDT"
      },
      "source": [
        "We can see that there's evidence for that from my [Imagenette experiments](https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/). For the xresnet18, xresnet34 and xse_resnext50 models (top to bottom charts, respectively), 1e-3 yields higher accuracy for lower batch sizes than 1e-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**xresnet18**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyq3M9Z-KZQi"
      },
      "source": [
        "![xresnet18 batch size vs accuracy for different learning rates](lr_acc_18.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**xresnet34**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UazUqC2hKs4J"
      },
      "source": [
        "![xresnet34 batch size vs accuracy for different learning rates](lr_acc_34.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**xse_resnext50**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvlmBYq8K01X"
      },
      "source": [
        "![xse_resnext50 batch size vs accuracy for different learning rates](lr_acc_50.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFWP5C5kv-uW"
      },
      "source": [
        "> small batch sizes (e.g., 2 to 32) can be more robust to learning rate choices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xze4bI8QwIJS"
      },
      "source": [
        "I witnessed the second point during my training runs as well. For xresnet34 and a batch size of 8 or 16, the difference in accuracy between learning rates of 1e-4, 1e-3 and 1e-2 was 10%. For a batch size of 1024, the difference in accuracy was 40%. xresnet18 had a similar trend while xse_resnext50 was more robust to changes in learning rates (1e-4, 1e-3, 1e-2) as the accuracy varied about 10-20% across batch sizes of 2 to 1024. In the charts above, this is visualized by the diverging LR curves as batch size increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM9zsQFxyiCv"
      },
      "source": [
        "> A learning rate warm-up strategy, where training begins with a very small learning rate that is gradually increased to its target value over a few initial epochs, can significantly stabilize the early phases of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVPlNvShynoI"
      },
      "source": [
        "My training runs already did this by default (using fastai's `Learner` and `vision_learner`), though I haven't experimented with any available parameters related to this (e.g. number of warmup steps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiGlQVfAz1VC"
      },
      "source": [
        "> The optimal learning rate strategy is non-linear, optimizer-dependent, and may require empirical tuning guided by these more nuanced theoretical understandings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaAR1BS7z4O7"
      },
      "source": [
        "### Alternative Forms of Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1hNq4fGLldo"
      },
      "source": [
        "Gemini found in its research that alternative forms of normalization (other than Batch Normalization) can improve small batch size performance. Most notably, in the Group Normalization paper they found that Group Normalization resulted in a 10% lower error rate than Batch Normalization for small batch sizes. Group Normalization calculates mean and variance for a group of channels for a single image, thereby being _batch independent_, so a small batch size doesn't make the statistics any noisier (as is the case for Batch Normalization where the mean and variance are calculated across all images in the batch). However, there's two sides to this trade-off. As the paper states:\n",
        "\n",
        "> BN's mean and variance computation introduces uncertainty caused by the stochastic batch sampling, which helps regularization. This uncertainty is missing in GN (and LN/IN). But it is possible that GN combined with a suitable regularizer will improve results. This can be a future research topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-QFTVMfM9fN"
      },
      "source": [
        "Gemini also suggests Layer Normalization (calculating mean and variance across all channels for a single image) and Instance Normalization (across a single channel for a single image) as these are both also batch independent. However, the Group Normalization paper finds that these two perform worse than Group Normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJIJN_fGOEY1"
      },
      "source": [
        "### Optimizers and Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxzVViQbOKIi"
      },
      "source": [
        "Finally, Gemini suggested to try out different optimizers (and tuning their hyperparameters) and regularization techniques (weight decay, data augmentation), both things I wanted to experiment with going into this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj_Uf29mNc-v"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmFLzOt-Nfkq"
      },
      "source": [
        "Based on Gemini's research and my reading of the Group Normalization paper I plan on experimenting on the following:\n",
        "\n",
        "- Replace Batch Normalization layers with Group Normalization.\n",
        "- Add a regularization method like weight decay.\n",
        "- Try out different optimizers using Benjamin Warner's [optimi](https://optimi.benjaminwarner.dev/) library.\n",
        "- Try out different data augmentation techniques from Benjamin Warner's [fastxtend](https://fastxtend.benjaminwarner.dev/vision.augment.batch.html) library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-ALRybIOr0D"
      },
      "source": [
        "At each step, I use `lr_find` and sample three stable LRs, and focus on small batch sizes of {2, 4, 8, 16, 32, 64}. I'm particularly interested in seeing how these experiments affect the three models I'm using (xresnet18, xresnet34, xse_resnext50) as xse_resnext50 was more robust to larger LRs than the other two.\n",
        "\n",
        "I'll be documenting my findings in blog posts as well as on my [YouTube channel](https://www.youtube.com/@vishal_learner) so please subscribe to follow along!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
