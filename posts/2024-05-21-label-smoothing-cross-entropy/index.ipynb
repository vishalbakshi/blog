{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Understanding the Code in fastai's `LabelSmoothingCrossEntropy`\n",
        "date: \"2024-05-21\"\n",
        "author: Vishal Bakshi\n",
        "description: Inspired by Aman Arora's blog post, I walk through code of the fastai function LabelSmoothingCrossEntropy.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - deep learning\n",
        "    - fastai\n",
        "    - python\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QAtJFVdCmgx"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjC1Z3N7CokA"
      },
      "source": [
        "In this blog post I'll walk through fastai's `LabelSmoothingCrossEntropy` function line-by-line and compare it to the helpful Excel example and explanation presented by Aman Arora in his [Label Smoothing Explained using Microsoft Excel](https://amaarora.github.io/posts/2020-07-18-label-smoothing.html#fastaipytorch-implementation-of-label-smoothing-cross-entropy-loss) blog post. This process helped me better visualize how something in Excel (which is visually intuitive for beginners) translates to PyTorch (not always intuitive for beginners)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyh-rKX5D2zS"
      },
      "source": [
        "## Excel Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CtIDfTyFRJF"
      },
      "source": [
        "I'll start be recreating Aman's Excel example with the following columns:\n",
        "\n",
        "- `image_name`: example name of training data\n",
        "- `is_cat`: ground truth noisy label\n",
        "- `is_dog`: ground truth noisy label\n",
        "- `logit (cat)`: model output (activation) for cat class\n",
        "- `logit (dog)`: model output (activation) for dog class\n",
        "- `exp (cat)`: exponential of the cat logit\n",
        "- `exp (dog)`: exponential of the dog logit\n",
        "- `sum (exp)`: sum of cat and dog exponential for each image\n",
        "- `prob (cat)`: exponential of cat divided by sum of exponential o fdog and exponential of cat\n",
        "- `prob (dog)`: exponential of dog divided by sum of exponential o fdog and exponential of cat\n",
        "- `LS X-entropy`: the negative sum of the ground truth noisy label times the natural log of the class probability (for both dog and cat). The screenshot below shows how this value is calculated in Excel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py2oWA-qGnla"
      },
      "source": [
        "<img src=\"1.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1yXuNugG7yY"
      },
      "source": [
        "## fastai's `LabelSmoothingCrossEntropy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-zf6v0yGIADQ"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbcB731HHUAl"
      },
      "source": [
        "Here is the `forward` method of [fastai's `LabelSmoothingCrossEntropy` class](https://github.com/fastai/fastai/blob/6db9f9cd77d6bb1cd8a939852b0a0c48ce20e01b/fastai/losses.py#L188).\n",
        "\n",
        "```python\n",
        "def forward(self, output:Tensor, target:Tensor) -> Tensor:\n",
        "        \"Apply `F.log_softmax` on output then blend the loss/num_classes(`c`) with the `F.nll_loss`\"\n",
        "        c = output.size()[1]\n",
        "        log_preds = F.log_softmax(output, dim=1)\n",
        "        if self.reduction=='sum': loss = -log_preds.sum()\n",
        "        else:\n",
        "            loss = -log_preds.sum(dim=1) #We divide by that size at the return line so sum and not mean\n",
        "            if self.reduction=='mean':  loss = loss.mean()\n",
        "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UexVLrmMHyzw"
      },
      "source": [
        "I'll start by defining the `output` and `target` tensors. I'll also define the noisy target defined in the Excel spreadsheet (`is_cat` and `is_dog`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JY-fglcWCloj"
      },
      "outputs": [],
      "source": [
        "# logits\n",
        "output = torch.tensor([\n",
        "    [4.2, -2.4],\n",
        "    [1.6, -0.6],\n",
        "    [3.6, 1.2],\n",
        "    [-0.5, 0.5],\n",
        "    [-0.25, 1.7]\n",
        "])\n",
        "\n",
        "# labels\n",
        "target = torch.tensor([0,1,1,0,0])\n",
        "\n",
        "# noisy labels\n",
        "noisy_target = torch.tensor([\n",
        "    [0.95, 0.05],\n",
        "    [0.05, 0.95],\n",
        "    [0.05, 0.95],\n",
        "    [0.95, 0.05],\n",
        "    [0.95, 0.05]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFiWPIBRI6uJ"
      },
      "source": [
        "First let's calculate the loss with fastai to show that it matches the Excel calculations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii2DxcapI_Nz",
        "outputId": "fb4795e6-a60e-4770-88c6-601373780875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LabelSmoothingCrossEntropy(eps=0.1, reduction='none')(output,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1vNHnkuJD6R"
      },
      "source": [
        "Note the `eps` parameter which is $\\epsilon$ in Aman's blog post. I understand this to be the total \"noisiness\" divided across the classes. In our case, this value is `0.1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMcSh0j6JkMF"
      },
      "source": [
        "Next, I'll run through the lines of code in `LabelSmoothingCrossEntropy`'s `forward` method if `reduction='none'` (which is the case for our Excel example), and show that it outputs the same values as Excel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHcnyni5JwLZ",
        "outputId": "fa6c8b39-4241-44b9-d45c-54f9aa467931"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eps=0.1\n",
        "c = output.size()[1]\n",
        "log_preds = F.log_softmax(output, dim=1)\n",
        "loss = -log_preds.sum(dim=1)\n",
        "loss*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), reduction='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAq4jsdmKgNV"
      },
      "source": [
        "Here, `c` is the number of classes (`2`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFdNjMLyKoAi"
      },
      "source": [
        "## Recreating Excel Calculation in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icCKIst_KrBR"
      },
      "source": [
        "I found it a bit more intuitive to recreate the Excel calculation in PyTorch in a slightly different order of operations.\n",
        "\n",
        "In Excel, we take the softmax of the logits to get the probability of cat and dog (highlighted in the screenshot below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBqLFBIDLDly"
      },
      "source": [
        "<img src=\"2.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaFJpXJWLDy_"
      },
      "source": [
        "In PyTorch, we can recreate those values with `F.softmax`. `dim=-1` tells it to take the softmax across the last dimension (of `2` classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btgBA96tKHUI",
        "outputId": "a79fbe9c-1da6-4b41-a1aa-99a8b20cfbf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9986, 0.0014],\n",
              "        [0.9002, 0.0998],\n",
              "        [0.9168, 0.0832],\n",
              "        [0.2689, 0.7311],\n",
              "        [0.1246, 0.8754]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.softmax(output, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjdM43SYLRUH"
      },
      "source": [
        "Next, to calculate cross entropy, we multiply the noisy label with the log probability, sum across classes and multiply by negative 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyg48MttLkUy"
      },
      "source": [
        "<img src=\"3.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irJ6tOVDLkiq"
      },
      "source": [
        "In PyTorch, we do that by multiplying `noisy_target`s by the `torch.log` probabilities (`F.softmax`), summing across each row (`dim=-1`) and multiplying by negative 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-7bszhtLJVS",
        "outputId": "9a6f2f00-d699-4c8f-dbb8-57105300dc0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.3314, 2.1951, 2.3668, 1.2633, 1.9855])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "-1 * (noisy_target * torch.log(F.softmax(output, dim=-1))).sum(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qjvm2ugMNMS"
      },
      "source": [
        "This gives us the desired result. Although this looks different from the fastai implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG6QrehOMRkN"
      },
      "source": [
        "## Bringing it All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEbDwsUDMVb9"
      },
      "source": [
        "The Excel calculation that I recreated in PyTorch and the fastai implementation look different but achieve the same result. I'll try to connect and reason through the two approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYeavbJzMg7l"
      },
      "source": [
        "The first two lines of interest in `LabelSmoothingCrossEntropy` are straightforward---they define constants used later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh0_8dCJMkU9",
        "outputId": "13f0370b-8cb9-4511-8be6-68a8e1b4a428"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.1, 2)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eps=0.1\n",
        "c = output.size()[1]\n",
        "eps, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQdQH-6Mm9x"
      },
      "source": [
        "In the next line, `log_preds` is defined as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdXvg8wxMsOu",
        "outputId": "b75c0278-01dc-4f1f-9c97-16e8bf9e0d2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.3595e-03, -6.6014e+00],\n",
              "        [-1.0508e-01, -2.3051e+00],\n",
              "        [-8.6836e-02, -2.4868e+00],\n",
              "        [-1.3133e+00, -3.1326e-01],\n",
              "        [-2.0830e+00, -1.3302e-01]])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_preds = F.log_softmax(output, dim=1)\n",
        "log_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuYXbx86M8Ho"
      },
      "source": [
        "In Excel, we fold this step into the following formula (multiplying the noisy labels with the log probabilities and summing both classes):\n",
        "\n",
        "<img src=\"4.png\" style=\"width:100%;\">\n",
        "\n",
        "`log_preds` is just the `LN(I2)` and `LN(J2)` parts in the Excel formula for each image (or row)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWUes_VXNj9O"
      },
      "source": [
        "The next line in `LabelSmoothingCrossEntropy` sums the log probabilities across each row (or image) and multiplies the sum by negative 1.\n",
        "\n",
        "In Excel, this is would be the same as the part of the formula with the noisy labels removed: `=-LN(I2)-LN(J2)`.\n",
        "\n",
        "<img src=\"5.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiu8bqoBNhl3",
        "outputId": "3230f8fa-c3e8-4223-e48d-ea54fd94b37a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([6.6027, 2.4102, 2.5737, 1.6265, 2.2160])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = -log_preds.sum(dim=1)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8GPCEQSPFoY"
      },
      "source": [
        "The last part is where the noisy label magic happens in PyTorch.\n",
        "\n",
        "```python\n",
        "loss*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), reduction='none')\n",
        "```\n",
        "\n",
        "In the first term, `loss*eps/c`, the log probabilities summed across both classes for each image is multiplied by `0.1/2` or `0.05`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbudfIMpPVy8",
        "outputId": "a583dee1-c7c2-4030-b66c-374868469397"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.3301, 0.1205, 0.1287, 0.0813, 0.1108])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss*eps/c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE0cgRIRPaq1"
      },
      "source": [
        "The second term, `(1-eps) * F.nll_loss(log_preds, target.long(), reduction='none')` does a couple of things:\n",
        "\n",
        "First, it calculates the negative log likelihood loss given the log probabilities (`log_preds`) and the targets. Note that all `nll_loss` does is pick out the `log_preds` items at the `target` indices for each row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSnK_xDCQW_h",
        "outputId": "aaba87a1-2379-434f-b209-45d5f11f4f7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.3595e-03, 2.3051e+00, 2.4868e+00, 1.3133e+00, 2.0830e+00])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.nll_loss(log_preds, target.long(), reduction='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oRk2k0Qbyn"
      },
      "source": [
        "Since `reduction` is `'none'`, this is the same as just indexing each row with our `target` tensor and multiplying by -1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQUnzwudQneo",
        "outputId": "fa5a290d-5036-4a04-c6fd-6300461bc8f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.3595e-03, 2.3051e+00, 2.4868e+00, 1.3133e+00, 2.0830e+00])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "-1 * log_preds[[0, 1, 2, 3, 4], target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH7niqx8Q2cl",
        "outputId": "16e85a07-e631-4b9d-e229-5f9620bee7a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.3595e-03, -6.6014e+00],\n",
              "        [-1.0508e-01, -2.3051e+00],\n",
              "        [-8.6836e-02, -2.4868e+00],\n",
              "        [-1.3133e+00, -3.1326e-01],\n",
              "        [-2.0830e+00, -1.3302e-01]])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_preds # reminder of what log_preds looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6wwzkeCQ5WE",
        "outputId": "873e64f2-97b9-49fa-d820-61fbacaf3f2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 1, 0, 0])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target # reminder of what target looks like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qTwIRX-Q8EM"
      },
      "source": [
        "So, basically, `nll_loss` with `reduction='none'` takes the 0-th element of the first row (`-1.3595e-03`), the 1-th element in the second row (`-2.3051e+00`) and so on. **`nll_loss` picks only the chosen label's probabilities, whereas `loss` is the sum of both class' probabilities.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWTFye4eR4rm"
      },
      "source": [
        "The chosen probabilities are then multiplied by `1-eps` or `0.90`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOXXBImwQnA-"
      },
      "source": [
        "Let's visualize what that last line in `LabelSmoothCrossEntropy` is doing, row by row, given the `log_preds` values. I've rewritten `loss` as `-log_preds.sum(dim=1)`.\n",
        "\n",
        "```python\n",
        "(-log_preds.sum(dim=1))*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), reduction='none')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH2CTYHPSkDd"
      },
      "source": [
        "|row|-log_preds.sum(dim=1)\\*eps/c|(1-eps) \\* F.nll_loss(log_preds, target.long(), reduction='none')|\n",
        "|:-:|:-:|:-:|\n",
        "1|-(-1.3595e-03 + -6.6014e+00) * 0.05|0.90 * 1.3595e-03|\n",
        "2|-(-1.0508e-01 + -2.3051e+00) * 0.05|0.90 * 2.3051e+00|\n",
        "3|-(-8.6836e-02 + -2.4868e+00) * 0.05|0.90 * 2.4868e+00|\n",
        "4|-(-1.3133e+00, -3.1326e-01) * 0.05|0.90 * 1.3133e+00|\n",
        "5|-(-2.0830e+00, -1.3302e-01) * 0.05|0.90 * 2.0830e+00|\n",
        "\n",
        ": {tbl-colwidths=\"[10,40,50]\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmiKyOZuT6Mj"
      },
      "source": [
        "In each row you'll notice that the target log probability is multiplied first by `0.05` (which is `eps/c`) and then multiplied by `0.90` (which is `1-eps`) and then added together. We can rewrite this as follows (adding together `0.05` and `0.90` to get `0.95` for the target class)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Mf0sOIUORB"
      },
      "source": [
        "|row|-log_preds.sum(dim=1)\\*eps/c + (1-eps) \\* F.nll_loss(log_preds, target.long(), reduction='none')|\n",
        "|:-:|:-:|\n",
        "1|0.05 * 6.6014e+00 + 0.95 * 1.3595e-03|\n",
        "2|0.05 * 1.0508e-01 + 0.95 * 2.3051e+00|\n",
        "3|0.05 * 8.6836e-02 + 0.95 * 2.4868e+00|\n",
        "4|0.05 * 3.1326e-01 + 0.95 * 1.3133e+00|\n",
        "5|0.05 * 1.3302e-01 + 0.95 * 2.0830e+00|\n",
        "\n",
        ": {tbl-colwidths=\"[5,95]\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EngeeOUU3ME"
      },
      "source": [
        "I'll expand the Excel version a bit more to match this form so we can see the parallels:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"6.png\" style=\"width:100%;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1OgLlE8VnRp"
      },
      "source": [
        "In this way, the fastai implementation, Aman Arora's Excel implementation and my PyTorch implementation are visualized and aligned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zynfPnPKVwX5"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iehnh00UVx_Y"
      },
      "source": [
        "I often underestimate how much time and thinking it takes to unpack the amount of calculation done in a few lines of code. That's the beauty and elegance of fastai and PyTorch! But it also emphasizes the time and care needed to walk through each step manually to visualize what is going on.\n",
        "\n",
        "I hope you enjoyed this blog post!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
