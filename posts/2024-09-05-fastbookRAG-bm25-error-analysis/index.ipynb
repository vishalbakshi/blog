{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Conducting a Question-by-Question Error Analysis on Full Text Search Results\n",
        "date: \"2024-09-05\"\n",
        "author: Vishal Bakshi\n",
        "description: In this blog post, I conduct a detailed error analysis of 39 questions from a set of 202, where none of the 6 full text search methods retrieved sufficient context to answer them. I examine each question, categorize the errors, and discuss potential improvements and implications for future work.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - RAG\n",
        "    - information retrieval\n",
        "    - fastbookRAG\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2nicZ2WLrik"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnhj-8z0LstL"
      },
      "source": [
        "In this notebook I'll do a deep dive error analysis of my [full text search results](https://vishalbakshi.github.io/blog/posts/2024-09-03-fastbookRAG-bm25-baselines/), where I implemented 6 different keyword-based full text searches to retrieve context sufficient to answer questions from the end-of-chapter Questionnaires in [fastbook](https://github.com/fastai/fastbook/tree/master). Here is the summary of results from those experiments:\n",
        "\n",
        "|Chapter|BM25_A (Top-1 1p)|BM25_B (Top-3 1p)|BM25_C (Top-5 1p)|BM25_D (Top-1 3p)|BM25_E (Top-3 3p)|BM25_F (Top-5 3p)|\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "1|40% (12/30)|56.7% (17/30)|60% (18/30)|63.3% (19/30)|83.3% (25/30)|90% (27/30)|\n",
        "2|38.5% (10/26)|65.4% (17/26)|69.2% (18/26)|46.2% (12.26)|80.8% (21/26)|80.8% (21/26)|\n",
        "4|25% (8/32)|68.8% (22/32)|71.9% (23/32)|31.3% (10/32)|71.9% (23/32)|75% (24/32)|\n",
        "8|13.8% (4/29)|34.5% (10/29)|44.8% (13/29)|31% (9/29)|55.2% (16/29)|65.5% (19/29)|\n",
        "9|13.8% (4/29)|48.3% (14/29)|58.6% (17/29)|34.5% (10/29)|72.4% (21/29)|79.3% (23/29)|\n",
        "10|47.6% (12/21)|42.9% (9/21)|61.9% (13/21)|38% (8/21)|57.1% (12/21)|61.9% (13/21)|\n",
        "13|37.1% (13/35)|54.3% (19/35)|60% (21/35)|42.9% (15/35)|68.6% (24/35)|80% (28/35)|\n",
        "**All**|**30.2% (61/202)**|**53.5% (108/202)**|**60.9% (123/202)**|**41.1% (83/202)**|**70.3% (142/202)**|**76.7% (155/202)**|\n",
        "\n",
        "The granular results are available in [this public gist](https://gist.github.com/vishalbakshi/4379c92665695b8bd9ab83a1f3ab6b55).\n",
        "\n",
        "As a reminder, the two metrics I use for evaluation are **Score** and **Answer Rate**\n",
        "\n",
        "The evaluation metric for each question, that I'm simply calling **score**, is binary: can the retrieved context answer the question (`1`) or not (`0`)? The evaluation metric across a set of questions, which I'm calling the **Answer Rate**, is the mean score for those questions.\n",
        "\n",
        "While this is a straightforward pair of metrics, they do involve some judgment. After reading the retrieved context, I decide if it's enough to answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is a part of [series of blog posts](https://vishalbakshi.github.io/blog/#category=fastbookRAG) for a project I'm calling fastbookRAG where I'm trying to answer questions from the [fastbook](https://github.com/fastai/fastbook/tree/master) end-of-chapter Questionnaires using the following pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![fastbookRAG diagram](1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the number of unanswered questions per chapter:\n",
        "\n",
        "|Chapter|# of Questions|\n",
        "|:-:|:-:|\n",
        "|1|2\n",
        "|2|4\n",
        "|4|7\n",
        "|8|10\n",
        "|9|5\n",
        "|10|5\n",
        "|13|6\n",
        "|**Total**|**39**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZZH1GIfM13y"
      },
      "source": [
        "## Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0oFau8M24q"
      },
      "source": [
        "For 39 questions, none of the 6 full-text search methods retrieved enough context to provide an answer. I'll be looking at each of those 39 questions, their \"gold standard\" answer (obtained from the fastai Forums Questionnaire wikis), and the relevant context from the fastbook chapter.\n",
        "\n",
        "I have three objectives for this error analysis:\n",
        "\n",
        "- Understand what kinds of questions are difficult to answer using full text search.\n",
        "- Identify ambigious questions that need to be removed from the evaluation set (e.g. \"How does it solve it?\").\n",
        "- Identify unanswerable questions (i.e. \"to be done by the reader\" type questions) that need to be removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFD3AI3jN6uH"
      },
      "source": [
        "The underlying goal of this analysis: look at your data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yej7M1KAXCTs"
      },
      "source": [
        "For each of the 39 questions, I will write four sections:\n",
        "\n",
        "- Relevant Context: the paragraph(s) from the fastbook text that are sufficient to answer the question.\n",
        "- Analysis: my interpretation/explanation for why a keyword-based search did not retrieve the context.\n",
        "- Conclusion: what I think is needed to retrieve the sufficient context (and if I think this question should be removed)\n",
        "- Tags: keywords (no pun intented) that describe the type of error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuf1ryF3Ln4X",
        "outputId": "e2564fcf-8ed6-4ded-8a7b-d6a204092baa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(39, 19)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "url = 'https://gist.githubusercontent.com/vishalbakshi/4379c92665695b8bd9ab83a1f3ab6b55/raw/97a22f0736b5e179efb8c9d70e5ec80a5b8f4817/fastbookRAG_bm25_all.csv'\n",
        "df = pd.read_csv(url)\n",
        "score_columns = df.filter(regex='_score$').columns\n",
        "df['total_score'] = df[score_columns].sum(axis=1)\n",
        "no_answer = df.query(\"total_score == 0\")\n",
        "no_answer.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eb-PZIeMSPpq"
      },
      "outputs": [],
      "source": [
        "def print_data(idx):\n",
        "  row = no_answer.iloc[idx]\n",
        "  print('Chapter, Question Number:',row['chapter'], row['question_number'])\n",
        "  print('Question Text:', row['question_text'])\n",
        "  print('Answer:', row['answer'])\n",
        "  print('Keywords:', row['keywords'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx5XFzGTT-QE"
      },
      "source": [
        "### Chapter 1 (2 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJFIL8Gyqjg2"
      },
      "source": [
        "#### Question 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERh3NRGiSRMj",
        "outputId": "5ac5435b-7fe4-4510-e176-d055470ece54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 1 5\n",
            "Question Text: \"\"What were the two theoretical misunderstandings that held back the field of neural networks?\"\"\n",
            "Answer: \"\"In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, \"Perceptrons\", that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\n",
            "In the 1980's, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential.\"\"\n",
            "Keywords: \"neural, networks, theoretical, misunderstandings, field\"\n"
          ]
        }
      ],
      "source": [
        "print_data(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G2bAnfjSm93"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> An MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called Perceptrons (MIT Press), about Rosenblatt's invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. **Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.**\n",
        "\n",
        "> In the 1980's most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their “pattern of connectivity among units,” to use the framework above). And indeed, neural networks were widely used during the ’80s and ’90s for real, practical projects. **However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na1j6GdsTP76"
      },
      "source": [
        "**Analysis:**\n",
        "\n",
        "I think the reason none of the methods retrieved both contexts (they generally only retrieved the second paragraph) is due to the choice of keywords. The second paragraph contains four of the five keywords (neural, networks, theoretical, field). The first paragraph does not contain any of the keywords.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I expect semantic search to retrieve both paragraphs.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HguNCfBeURtY"
      },
      "source": [
        "#### Question 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGAZDUsHSfYl",
        "outputId": "6c7116b0-fc7b-4163-ad24-6eacea75ea77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 1 16\n",
            "Question Text: \"\"What do you need in order to train a model?\"\"\n",
            "Answer: \"\"You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer).\"\"\n",
            "Keywords: \"train, model, training, models, dataset, data\"\n"
          ]
        }
      ],
      "source": [
        "print_data(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50kvaAn-UV0g"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61dEZfWEVdyp"
      },
      "source": [
        "> From this picture we can now see some fundamental things about training a deep learning model:\n",
        ">\n",
        ">- A model cannot be created without data.\n",
        "- A model can only learn to operate on the patterns seen in the input data used to train it.\n",
        "- This learning approach only creates predictions, not recommended actions.\n",
        "- It's not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren't enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n",
        "\n",
        "\n",
        ">Machine learning is a discipline where we define a program not by writing it entirely ourselves, but by learning from data. Deep learning is a specialty within machine learning that uses neural networks with multiple layers. Image classification is a representative example (also known as image recognition). We start with labeled data; that is, a set of images where we have assigned a label to each image indicating what it represents. Our goal is to produce a program, called a model, which, given a new image, will make an accurate prediction regarding what that new image represents.\n",
        ">\n",
        "> Every model starts with a choice of architecture, a general template for how that kind of model works internally. The process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data. In order to define how well a model does on a single prediction, we need to define a loss function, which determines how we score a prediction as good or bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfZEvBelWUcn"
      },
      "source": [
        "**Analysis:**\n",
        "\n",
        "The keywords for this question are not sufficient (they are too general) to find the 3 relevant paragraphs. You can imagine that the words \"train\", \"model\", \"training\", \"models\", \"dataset\", and \"data\" are plentiful in an introductory chapter about machine and deep learning.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. This is another question for which the sufficient context that needs to be retrieved is better suited for semantic search.\n",
        "\n",
        "**Tags:** insufficient keywords, distracting keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhkksmmGXo0x"
      },
      "source": [
        "### Chapter 2 (4 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQogHjHZqsu5"
      },
      "source": [
        "#### Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IyJtLy_URLm",
        "outputId": "569b059f-b35a-4d55-e7fd-75aaa663253d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 2 1\n",
            "Question Text: \"\"Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\"\"\n",
            "Answer: \"\"Working with video data instead of images\n",
            "Handling nighttime images, which may not appear in this dataset\n",
            "Dealing with low-resolution camera images\n",
            "Ensuring results are returned fast enough to be useful in practice\n",
            "Recognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\"\"\n",
            "Keywords: \"bear, classification, model, production, training, data, structural, style, differences\"\n"
          ]
        }
      ],
      "source": [
        "print_data(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQhPzq9BX5Kr"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        ">This can result in disaster! For instance, let's say we really were rolling out a bear detection system that will be attached to video cameras around campsites in national parks, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded there would be all kinds of problems in practice, such as:\n",
        ">\n",
        ">- Working with video data instead of images\n",
        "- Handling nighttime images, which may not appear in this dataset\n",
        "- Dealing with low-resolution camera images\n",
        "- Ensuring results are returned fast enough to be useful in practice\n",
        "- Recognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozwsxE9wYOeV"
      },
      "source": [
        "**Analysis:**\n",
        "\n",
        "Only two of the keywords are in the relevant context (\"model\" and \"data\").\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. This is another question that might perform better using semantic search.\n",
        "\n",
        "**Tags:** insufficient keywords, distracting keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyzkCTHMYds6"
      },
      "source": [
        "#### Question 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AifsXGXYbfT",
        "outputId": "5c5c88d1-9acd-4bf4-9bf8-884dfd973961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 2 12\n",
            "Question Text: \"\"What does the `splitter` parameter to `DataBlock` do?\"\"\n",
            "Answer: \"\"In fastai DataBlock, you provide the splitter argument a way for fastai to split up the dataset into subsets (usually train and validation set). For example, to randomly split the data, you can use fastai's predefined RandomSplitter class, providing it with the proportion of the data used for validation.\"\"\n",
            "Keywords: \"splitter, DataBlock, parameter, function, data\"\n"
          ]
        }
      ],
      "source": [
        "print_data(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg9mZRPvYqfx"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Often, datasets that you download will already have a validation set defined. Sometimes this is done by placing the images for the training and validation sets into different folders. Sometimes it is done by providing a CSV file in which each filename is listed along with which dataset it should be in. There are many ways that this can be done, and fastai provides a very general approach that allows you to use one of its predefined classes for this, or to write your own. In this case, however, we simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed (computers don't really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time—called the seed—then you will get the exact same list each time):\n",
        ">\n",
        "> `splitter=RandomSplitter(valid_pct=0.2, seed=42)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex8q8mhXZMg2"
      },
      "source": [
        "**Analysis:**\n",
        "\n",
        "There are 4 occurences of the word \"splitter\" in Chapter 2 of the fastbook. Two of them, `splitter` and `Splitter`, occur in the relevant paragraph. I think what misled the full text search were the other keywords (DataBlock, parameter, function, data). I'm not sure if semantic search will perform better, since the explanatory text doesn't explicitly say something like \"the `splitter` parameter's purpose is...\" as it focuses more on the randomness of the split (which is more related to `RandomSplitter` than the `splitter` parameter itself).\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "This may just be a question that is unanswerable. I'll keep it in the evaluation set for now, but might remove it if the semantic search baselines aren't able to retrieve the relevant context.\n",
        "\n",
        "**Tags:** difficult question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLa_qMGRtjR0"
      },
      "source": [
        "#### Question 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqqNjupNY5iK",
        "outputId": "13141f8a-81db-4253-c726-2ef11c47a7d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 2 24\n",
            "Question Text: \"\"What are three examples of problems that could occur when rolling out a bear warning system in practice?\"\"\n",
            "Answer: \"\"The model we trained will likely perform poorly when:\n",
            "Handling night-time images\n",
            "Dealing with low-resolution images (ex: some smartphone images)\n",
            "The model returns prediction too slowly to be useful\"\"\n",
            "Keywords: \"bear, warning, system, problems, rollout, examples\"\n"
          ]
        }
      ],
      "source": [
        "print_data(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b9A8Gnwu_8T"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        ">This can result in disaster! For instance, let's say we really were rolling out a bear detection system that will be attached to video cameras around campsites in national parks, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded there would be all kinds of problems in practice, such as:\n",
        ">\n",
        ">- Working with video data instead of images\n",
        "- Handling nighttime images, which may not appear in this dataset\n",
        "- Dealing with low-resolution camera images\n",
        "- Ensuring results are returned fast enough to be useful in practice\n",
        "- Recognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI3a-9y-vBKD"
      },
      "source": [
        "**Analysis:** This question has the same relevant context as Chapter 2, Question 1. Only 1 of the keywords is in the context (\"problems\") and that keyword appears 10 times in the chapter text.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I would expect semantic search to perform better for this question.\n",
        "\n",
        "**Tags:** insufficient keywords, distracting keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAvP1ilivaNi"
      },
      "source": [
        "#### Question 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWH4N2KmvaYb",
        "outputId": "372a8e0f-3dbb-4811-94eb-c95602554d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 2 27\n",
            "Question Text: \"\"What are the three steps in the deployment process?\"\"\n",
            "Answer: \"\"Manual process - the model is run in parallel and not directly driving any actions, with humans still checking the model outputs.\n",
            "Limited scope deployment - The model's scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised.\n",
            "Gradual expansion - The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better).\"\"\n",
            "Keywords: \"deployment, process, steps\"\n"
          ]
        }
      ],
      "source": [
        "print_data(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBaHjebAwhHv"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        ">Where possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense. For instance, with our bear classifier a park ranger could have a screen displaying video feeds from all the cameras, with any possible bear sightings simply highlighted in red. The park ranger would still be expected to be just as alert as before the model was deployed; the model is simply helping to check for problems at this point.\n",
        ">\n",
        ">The second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time-constrained trial of the model-driven approach. Rather than rolling our bear classifier out in every national park throughout the country, we could pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out.\n",
        ">\n",
        ">Then, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process. For instance, if the number of bear alerts doubles or halves after rollout of the new system in some location, we should be very concerned. Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and ensure that your regular reporting includes that information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxNf9m4xwutI"
      },
      "source": [
        "**Analysis:** Only one of the keywords, \"process\", appears in the relevant context for this question, a word that appears 25 times in Chapter 2.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Not enough of the keywords in the relevant context are in the keywords used for full text search.\n",
        "\n",
        "**Tags:** insufficient keywords, distracting keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O81Lo7nxxGcX"
      },
      "source": [
        "### Chapter 4 (7 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBlQBLkUq5Zl"
      },
      "source": [
        "#### Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3my7CvRvbCo",
        "outputId": "e00c4850-0b88-42fd-f0cf-d00a288e6fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 4 1\n",
            "Question Text: \"\"How is a grayscale image represented on a computer? How about a color image?\"\"\n",
            "Answer: \"\"Images are represented by arrays with pixel values representing the content of the image. For grayscale images, a 2-dimensional array is used with the pixels representing the grayscale values, with a range of 256 integers. A value of 0 represents black, and a value of 255 represents white, with different shades of gray in between. For color images, three color channels (red, green, blue) are typically used, with a separate 256-range 2D array used for each channel. A pixel value of 0 represents black, with 255 representing solid red, green, or blue. The three 2D arrays form a final 3D array (rank 3 tensor) representing the color image.\"\"\n",
            "Keywords: \"grayscale, image, images, color, computer, representation\"\n"
          ]
        }
      ],
      "source": [
        "print_data(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qJS6m-A-v0I"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> You can see that the background white pixels are stored as the number 0, black is the number 255, and shades of gray are between the two. The entire image contains 28 pixels across and 28 pixels down, for a total of 784 pixels. (This is much smaller than an image that you would get from a phone camera, which has millions of pixels, but is a convenient size for our initial learning and experiments. We will build up to bigger, full-color images soon.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHCKKKdh_fT7"
      },
      "source": [
        "**Analysis:** I scanned the Chapter 2 notebook and did not find an explanation about how color images are represented on a computer. Some of the methods did retrieve the relevant context shown above.\n",
        "\n",
        "**Conclusion:** Remove this question from the evaluation set as the chapter doesn't include context to answer it.\n",
        "\n",
        "**Tags:** unanswerable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meyboctbDFX7"
      },
      "source": [
        "#### Question 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN2z8ZBMxIu9",
        "outputId": "30d792bf-6d9a-43db-eb1b-7cc8431cc785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 4 12\n",
            "Question Text: \"\"What is SGD?\"\"\n",
            "Answer: \"\"SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\"\"\n",
            "Keywords: \"SGD, stochastic, gradient, descent, optimization, algorithm\"\n"
          ]
        }
      ],
      "source": [
        "print_data(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_0tZq7BFSNW"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> To be exact, we'll discuss the roles of arrays and tensors and of broadcasting, a powerful technique for using them expressively. We'll explain stochastic gradient descent (SGD), the mechanism for learning by updating weights automatically. We'll discuss the choice of a loss function for our basic classification task, and the role of mini-batches. We'll also describe the math that a basic neural network is actually doing. Finally, we'll put all these pieces together.\n",
        "\n",
        ">To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n",
        ">\n",
        ">1. Initialize the weights.\n",
        ">2. For each image, use these weights to predict whether it appears to be a 3 or a 7.\n",
        ">3. Based on these predictions, calculate how good the model is (its loss).\n",
        ">4. Calculate the gradient, which measures for each weight, how changing that weight would change the loss\n",
        ">5. Step (that is, change) all the weights based on that calculation.\n",
        ">6. Go back to the step 2, and repeat the process.\n",
        ">7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndiiby33G9uz"
      },
      "source": [
        "**Analysis:** The paragraphs that answer this question are relatively far apart in the document so even a 3-paragraph chunk would not capture both. Furthermore, the paragraph that lists the seven steps of SGD only contains one of the keywords, \"gradient.\"\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I'm not so sure this question will be answerable by semantic search but I don't see any reason to remove it (other than it's hard to answer).\n",
        "\n",
        "**Tags:** insufficient keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFcxapRHjlG"
      },
      "source": [
        "#### Question 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ScnuTSWDEb-",
        "outputId": "b5be2c37-4e7d-45f2-bda7-20421323535d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 4 13\n",
            "Question Text: \"\"Why does SGD use mini-batches?\"\"\n",
            "Answer: \"\"We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using mini-batches are also more computationally efficient than single items on a GPU.\"\"\n",
            "Keywords: \"SGD, mini-batches, minibatches, stochastic, gradient, descent\"\n"
          ]
        }
      ],
      "source": [
        "print_data(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiIl1ejDHyMg"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        ">In order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you'd be going to the trouble of updating the weights, but taking into account only how that would improve the model's performance on that single item.\n",
        "\n",
        ">So instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a mini-batch. The number of data items in the mini-batch is called the batch size. A larger batch size means that you will get a more accurate and stable estimate of your dataset's gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this book.\n",
        "\n",
        ">Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it's helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!\n",
        "\n",
        "**Analysis:** I'm surprised that none of the methods retrieved this context. The term \"mini-batch\" appears 14 times in the Chapter 4 text and 5 times in the three paragraphs of the relevant context.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. My guess is that the other keywords (\"SGD\", \"stochastic\", \"gradient\", \"descent\") distract the full text search. I would expect semantic search to succeed for this question.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXXnQ5iBIbAZ"
      },
      "source": [
        "#### Question 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUKOflvHHklb",
        "outputId": "cb59eda9-9517-45f1-fe25-5cd66e72385f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 4 14\n",
            "Question Text: \"\"What are the seven steps in SGD for machine learning?\"\"\n",
            "Answer: \"\"Initialize the parameters - Random values often work best.\n",
            "Calculate the predictions - This is done on the training set, one mini-batch at a time.\n",
            "Calculate the loss - The average loss over the minibatch is calculated\n",
            "Calculate the gradients - this is an approximation of how the parameters need to change in order to minimize the loss function\n",
            "Step the weights - update the parameters based on the calculated weights\n",
            "Repeat the process\n",
            "Stop - In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\"\"\n",
            "Keywords: \"SGD, steps, machine, learning, gradient, descent\"\n"
          ]
        }
      ],
      "source": [
        "print_data(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX5F5eO5ImRo"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        ">To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n",
        ">\n",
        ">1. Initialize the weights.\n",
        ">2. For each image, use these weights to predict whether it appears to be a 3 or a 7.\n",
        ">3. Based on these predictions, calculate how good the model is (its loss).\n",
        ">4. Calculate the gradient, which measures for each weight, how changing that weight would change the loss\n",
        ">5. Step (that is, change) all the weights based on that calculation.\n",
        ">6. Go back to the step 2, and repeat the process.\n",
        ">7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUhvG72BIo_u"
      },
      "source": [
        "**Analysis:** Similar to Chapter 4, Question 12, only two of the keywords, \"machine\" and \"learning\", are found in the relevant context. Not only that, but those two keywords are rather distracting as there are 8 occurences of \"machine\" and 53 occurences of \"learning\" in the chapter text, distracting the full text search from finding the relevant context.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I think this a tough question to answer for full text search, but semantic search might perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, distracting keywords, difficult question, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu0AYSDRKKau"
      },
      "source": [
        "#### Question 23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVQoLNy1KKib",
        "outputId": "4a592ab8-a997-4d43-bd46-98ef29932e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 4 23\n",
            "Question Text: \"\"What is the function to calculate new weights using a learning rate?\"\"\n",
            "Answer: \"\"The optimizer step function\"\"\n",
            "Keywords: \"function, calculate, weight, weights, learning, rate\"\n"
          ]
        }
      ],
      "source": [
        "print_data(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nho8SZv4Lxhk"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Deciding how to change our parameters based on the values of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything. Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we'll show you a better approach later in this book, called the learning rate finder). Once you've picked a learning rate, you can adjust your parameters using this simple function:\n",
        ">\n",
        "> `w -= gradient(w) * lr`\n",
        ">\n",
        "> This is known as stepping your parameters, using an optimizer step. Notice how we subtract the gradient * lr from the parameter to update it. This allows us to adjust the parameter in the direction of the slope by increasing the parameter when the slope is negative and decreasing the parameter when the slope is positive. We want to adjust our parameters in the direction of the slope because our goal in deep learning is to minimize the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6toLC2UZMPry"
      },
      "source": [
        "**Analysis:** Some of the methods (like BM25_C) did retrieve the first paragraph (\"Deciding how to change...\") but did not capture the third paragraph that has the answer (\"This is known...optimizer step\"). The keywords are also distracting, here are the occurences of each keyword in the chapter text:\n",
        "\n",
        "|keyword|Occurences|\n",
        "|:-:|:-:|\n",
        "|function|157|\n",
        "|calculate|69|\n",
        "|weight|67|\n",
        "|learning|53|\n",
        "|weights|43|\n",
        "|rate|33|\n",
        "\n",
        "That being said, \"learning rate\" does appear five times in the first paragraph. The problem is that none of the keywords appear in the second paragraph, which has the answer.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. This is an example of where chunking strategy would make a difference. I'm not so sure semantic search will be successful unless the three paragraphs in the relevant context are in one chunk.\n",
        "\n",
        "**Tags:** insufficient keywords, chunking strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLj7HE5RN9IX"
      },
      "source": [
        "#### Question 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVR8hql0N9Q0",
        "outputId": "9bf3fd94-c35d-4fdf-a723-ff8c129a393e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 4 28\n",
            "Question Text: \"\"What are the \"\"bias\"\" parameters in a neural network? Why do we need them?\"\"\n",
            "Answer: \"\"Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.\"\"\n",
            "Keywords: \"bias, biases, neural, network, networks, parameters\"\n"
          ]
        }
      ],
      "source": [
        "print_data(11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjJ5jS20OCft"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The function `weights*pixels` won't be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its intercept is 0). You might remember from high school math that the formula for a line is y=w*x+b; we still need the b. We'll initialize it to a random number too:\n",
        ">\n",
        "> `bias = init_params(1)`\n",
        ">\n",
        "> In neural networks, the `w` in the equation `y=w*x+b` is called the weights, and the `b` is called the bias. Together, the weights and bias make up the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTg6__SaO8MV"
      },
      "source": [
        "**Analysis:** This is another example of how chunking strategy effects the retrieval performance. Multiple approaches retrieved the third paragraph (\"In neural networks...\") as it contains 4 of the 6 keywords (\"bias\", \"neural\", \"networks\", and \"parameters\"). However, if the chunking strategy (1- or 3-paragraph) didn't capture the two paragraphs before it, the answer was not retrieved (\"...it is always equal to 0...\").\n",
        "\n",
        "**Conclusion:** This question is answerable, but is sensitive to chunking strategy for full text search. Perhaps semantic search will perform better.\n",
        "\n",
        "**Tags:** chunking strategy, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAxktuqKPmTz"
      },
      "source": [
        "#### Question 31"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Za_W5BcPmhh",
        "outputId": "091e0c56-80ff-4a3b-dadb-406498fa49a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 4 31\n",
            "Question Text: \"\"Why do we have to zero the gradients?\"\"\n",
            "Answer: \"\"PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value.\"\"\n",
            "Keywords: \"zero, zeroing, gradient, gradients, optimization\"\n"
          ]
        }
      ],
      "source": [
        "print_data(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oicRcnIAP0Bu"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The gradients have changed! The reason for this is that `loss.backward` actually adds the gradients of `loss` to any gradients that are currently stored. So, we have to set the current gradients to 0 first:\n",
        ">\n",
        "> `weights.grad.zero_()`\n",
        ">\n",
        "> `bias.grad.zero_();`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo-ba1XnQGeo"
      },
      "source": [
        "**Analysis:** The methods did not retrieve any of the relevant context. My guess is that the key keyword is `0` but the Claude-generated keywords don't include that number (they only have \"zero\" and \"zeroing\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGRioA6AQgna"
      },
      "source": [
        "**Conclusion:** Keep the question in the evaluation set. I would imagine semantic search performing better for this question.\n",
        "\n",
        "**Tags** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-FJYlCGQpuK"
      },
      "source": [
        "### Chapter 8 (10 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwX2LjdhrIHt"
      },
      "source": [
        "#### Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGo0q-s2Qp2E",
        "outputId": "9f765c5c-cfe6-4999-d0a7-8ffa0a29b3c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 2\n",
            "Question Text: \"\"How does it solve it?\"\"\n",
            "Answer: \"\"The key idea of collaborative filtering is latent factors. The idea is that the model can tell what kind of items you may like (ex: you like sci-fi movies/books) and these kinds of factors are learned (via basic gradient descent) based on what items other users like.\"\"\n",
            "Keywords: \"solve, solves, solution, solutions, how\"\n"
          ]
        }
      ],
      "source": [
        "print_data(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJR-n1VxQu0q"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people's movie watching decisions.\n",
        "\n",
        "> Since we don't know what the latent factors actually are, and we don't know how to score them for each user and movie, we should learn them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOwt7MYoQ7T-"
      },
      "source": [
        "**Analysis:** This is an amiguous question as it doesn't provide any context for Claude to generate relevant keywords. Both paragraphs of the relevant context are needed to fully answer this question and they are far apart in the document, so having the right keywords is critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvRV7qXPRiQ2"
      },
      "source": [
        "**Conclusion:** Remove this question from the evaluation set because it is too ambiguous for keyword-based or semantic search.\n",
        "\n",
        "**Tags:** unanswerable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZDCsFz5Rrhi"
      },
      "source": [
        "#### Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNU2oK-uRrrU",
        "outputId": "98d5714f-b5bb-468c-d51e-03eb1bef9be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 3\n",
            "Question Text: \"\"Why might a collaborative filtering predictive model fail to be a very useful recommendation system?\"\"\n",
            "Answer: \"\"If there are not many recommendations to learn from, or enough data about the user to provide useful recommendations, then such collaborative filtering systems may not be useful.\"\"\n",
            "Keywords: \"collaborative, filtering, predictive, model, recommendation, system, fail\"\n"
          ]
        }
      ],
      "source": [
        "print_data(14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qHoO6chS4N9"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\n",
        "\n",
        "**Analysis:** The keywords are distracting the full text search. \"collaborative\" (12 times), \"filtering\" (12), and \"model\" (63) appear many other times in the chapter text. A better keyword would have been \"problem\" which appears twice in the relevant context.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I think this question will perform better with semantic search.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzXWrXieUTha"
      },
      "source": [
        "#### Question 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hObFGHEeUT6k",
        "outputId": "43d36c61-4d6a-4b4a-93c6-9de772825106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 4\n",
            "Question Text: \"\"What does a crosstab representation of collaborative filtering data look like?\"\"\n",
            "Answer: \"\"In the crosstab representation, the users and items are the rows and columns (or vice versa) of a large matrix with the values filled out based on the user's rating of the item.\"\"\n",
            "Keywords: \"crosstab, collaborative, filtering, data, representation\"\n"
          ]
        }
      ],
      "source": [
        "print_data(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_MLEK86V_Uw"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> We have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\n",
        "\n",
        "> Because each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle.\n",
        "\n",
        "**Analysis:** The two main pieces of context that answer this question are images which I have not shown above. The text that I have shown doesn't fully explain the answer to this question.\n",
        "\n",
        "**Conclusion:** I will remove this question from the evaluation set because it requires images to answer this question and currently I am not storing images in the database.\n",
        "\n",
        "**Tags:** unanswerable, requires image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu_ZJz44nWrv"
      },
      "source": [
        "#### Question 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCqRXsZPnXfP",
        "outputId": "155081bc-8c84-4284-ed49-494dcdcaaab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 6\n",
            "Question Text: \"\"What is a latent factor? Why is it \"\"latent\"\"?\"\"\n",
            "Answer: \"\"As described above, a latent factor are factors that are important for the prediction of the recommendations, but are not explicitly given to the model and instead learned (hence \"latent\").\"\"\n",
            "Keywords: \"latent, factor, factors, hidden, unobservable, underlying\"\n"
          ]
        }
      ],
      "source": [
        "print_data(16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPaYXE40t9Vc"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people's movie watching decisions.\n",
        "\n",
        "> Since we don't know what the latent factors actually are, and we don't know how to score them for each user and movie, we should learn them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgKNtramt_2p"
      },
      "source": [
        "**Analysis:** This is similar to Question #2 in this chapter. The word \"latent\" shows up 17 times in the chapter, and \"factors\" 61 times.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. It's likely that full text search, even with better keywords, just might not be able to retrieve this context. I would think semantic search would.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqGvNWsEu2zI"
      },
      "source": [
        "#### Question 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMG3K7LYu3Az",
        "outputId": "bbc96853-950e-45c7-f481-2c652fde9705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 8\n",
            "Question Text: \"\"What does pandas.DataFrame.merge do?\"\"\n",
            "Answer: \"\"It allows you to merge DataFrames into one DataFrame.\"\"\n",
            "Keywords: \"pandas, dataframe, merge, join, combine\"\n"
          ]
        }
      ],
      "source": [
        "print_data(17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E1BsrPyvBur"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> We can merge this with our ratings table to get the user ratings by title:\n",
        ">\n",
        "> `ratings = ratings.merge(movies)`\n",
        ">\n",
        "> `ratings.head()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgVwn3AnvNrU"
      },
      "source": [
        "**Analysis:** Only those three lines are relevant for this question and they don't explicitly answer the question (i.e. there isn't an explicit definition of `.merge`).\n",
        "\n",
        "**Conclusion:** Remove this question from the evaluation set.\n",
        "\n",
        "**Tags:** unanswerable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM57PFZaxKNp"
      },
      "source": [
        "#### Question 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAHwVELHxKdu",
        "outputId": "143feed7-1247-431c-a9ef-998c37025fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 12\n",
            "Question Text: \"\"What does an embedding contain before we start training (assuming we're not using a pretained model)?\"\"\n",
            "Answer: \"\"The embedding is randomly initialized.\"\"\n",
            "Keywords: \"embedding, embeddings, training, pretrained, model, models\"\n"
          ]
        }
      ],
      "source": [
        "print_data(18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1PGV9No0cSo"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> This is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\n",
        "\n",
        "> So far, we've used Embedding without thinking about how it really works. Let's re-create DotProductBias without using this class. We'll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall from <<chapter_mnist_basics>> that optimizers require that they can get all the parameters of a module from the module's parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOwLosaZ4hFG"
      },
      "source": [
        "**Analysis:** There are over 50 occurences of the words \"embedding\" and \"embeddings\". Perhaps if \"initialize\" was a keyword the full text search would have retrieved the relevant context.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Chalk this one up to a lack of appropriate keywords. Perhaps semantic search would do better in this situation.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8KUHYxO5VmZ"
      },
      "source": [
        "#### Question 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1od4DyZ5azC",
        "outputId": "2e781398-5e65-403e-9145-83b789e6106e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 17\n",
            "Question Text: \"\"What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\"\"\n",
            "Answer: \"\"We would need to ensure the model outputs 5 predictions. For example, with a neural network model, we need to change the last linear layer to output 5, not 1, predictions. Then this is passed into the Cross Entropy loss.\"\"\n",
            "Keywords: \"cross-entropy, loss, MovieLens, model, change\"\n"
          ]
        }
      ],
      "source": [
        "print_data(19)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw_yUf2P586z"
      },
      "source": [
        "**Analysis:** There is no relevant context from the chapter text that answers this question.\n",
        "\n",
        "**Conclusion:** Remove this question from the evaluation set.\n",
        "\n",
        "**Tags:** unanswerable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL1YoLq26kx7"
      },
      "source": [
        "#### Question 23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qIWEM1g6k88",
        "outputId": "198ce9f4-2eeb-49ad-8584-cf41c903bd43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 23\n",
            "Question Text: \"\"What does argsort do in PyTorch?\"\"\n",
            "Answer: \"\"This just gets the indices in the order that the original PyTorch Tensor is sorted.\"\"\n",
            "Keywords: \"argsort, pytorch, sorting, indices, tensor, arrays\"\n"
          ]
        }
      ],
      "source": [
        "print_data(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oURec16K8uxT"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> `idxs = movie_bias.argsort()[:5]`\n",
        "\n",
        "> `idxs = movie_bias.argsort(descending=True)[:5]`\n",
        "\n",
        "> `idxs = movie_bias.argsort(descending=True)[:5]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlgHkeLs_OEE"
      },
      "source": [
        "**Analysis:** This is another example of the chapter text not having the answer explicit. A\n",
        "\n",
        "**Conclusion:** I'll remove this question from the evaluation set. Although an LLM would likely know what `argsort` does, so the code examples might be enough.\n",
        "\n",
        "**Tags:** unanswerable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-8LI1l2AqX7"
      },
      "source": [
        "#### Question 29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d9CsifeAqiE",
        "outputId": "50c971d5-c367-4efb-84e7-be577cc2f20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 29\n",
            "Question Text: \"\"When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\"\"\n",
            "Answer: \"\"In this case, we are not taking the dot product but instead concatenating the embedding matrices, so the number of factors can be different.\"\"\n",
            "Keywords: \"neural, networks, collaborative, filtering, factors, movies, users\"\n"
          ]
        }
      ],
      "source": [
        "print_data(21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5CripdjA1lR"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Since we'll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0EmAgpiA4DZ"
      },
      "source": [
        "**Analysis:** If \"different\" was one of the keywords, the relevant context would likely have been retrieved by one of the methods as it appears only twice in the document, both times in this paragraph.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. . I expect semantic search to retrieve this context.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P16WLndBQzi"
      },
      "source": [
        "#### Question 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaayzqd3BRaK",
        "outputId": "4628329f-b28f-49d8-d767-d69b3c1590a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 8 30\n",
            "Question Text: \"\"Why is there an nn.Sequential in the CollabNN model?\"\"\n",
            "Answer: \"\"This allows us to couple multiple nn.Module layers together to be used. In this case, the two linear layers are coupled together and the embeddings can be directly passed into the linear layers.\"\"\n",
            "Keywords: \"sequential, collabnn, model, neural, networks\"\n"
          ]
        }
      ],
      "source": [
        "print_data(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3breTTmbCVZf"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "```python\n",
        "class CollabNN(Module):\n",
        "    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n",
        "        self.user_factors = Embedding(*user_sz)\n",
        "        self.item_factors = Embedding(*item_sz)\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(user_sz[1]+item_sz[1], n_act),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_act, 1))\n",
        "        self.y_range = y_range\n",
        "        \n",
        "    def forward(self, x):\n",
        "        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n",
        "        x = self.layers(torch.cat(embs, dim=1))\n",
        "        return sigmoid_range(x, *self.y_range)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqhQ5z-JClLU"
      },
      "source": [
        "**Analysis:** The explanatory text does not reference `nn.Sequential`. It's likely that an LLM would know what that is and does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPH3BvaoCu6T"
      },
      "source": [
        "**Conclusion:** Remove question from evaluation set.\n",
        "\n",
        "\n",
        "**Tags:** unanswerable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRNU0RfCzbq"
      },
      "source": [
        "### Chapter 9 (5 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Wvp6QRC3OW"
      },
      "source": [
        "#### Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxOMyqcFC0kj",
        "outputId": "42bc3f19-5d43-4f11-985c-ca0ba2753edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 9 2\n",
            "Question Text: \"\"What is a categorical variable?\"\"\n",
            "Answer: \"\"This refers to variables that can take on discrete levels that correspond to different categories.\"\"\n",
            "Keywords: \"categorical, variable, variables, statistics, data\"\n"
          ]
        }
      ],
      "source": [
        "print_data(23)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvUXJhArCivs"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> In tabular data some columns may contain numerical data, like \"age,\" while others contain string values, like \"sex.\" The numerical data can be directly fed to the model (with some optional preprocessing), but the other columns need to be converted to numbers. Since the values in those correspond to different categories, we often call this type of variables categorical variables. The first type are called continuous variables.\n",
        "\n",
        "> jargon: Continuous and Categorical Variables: Continuous variables are numerical data, such as \"age,\" that can be directly fed to the model, since you can add and multiply them directly. Categorical variables contain a number of discrete levels, such as \"movie ID,\" for which addition and multiplication don't have meaning (even if they're stored as numbers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u79ZO1k3DHwQ"
      },
      "source": [
        "**Analysis:** There are 50+ occurences of the word \"categorical\" and 30+ occurences of the word \"variable\" in this chapter. I think it's just too common a keyword to give the desired result.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Semantic search might perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BijCapUEFxE"
      },
      "source": [
        "#### Question 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQbipi71EGAu",
        "outputId": "5a76cbd0-370e-404e-d492-cd132ee80d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 9 13\n",
            "Question Text: \"\"How are mse, samples, and values calculated in the decision tree drawn in this chapter?\"\"\n",
            "Answer: \"\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"\"\n",
            "Keywords: \"mse, samples, values, decision, tree, calculated, calculation\"\n"
          ]
        }
      ],
      "source": [
        "print_data(24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-5pcjZlIOR-"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remember that unless you see m_rmse, or a root mean squared error, then the value you are looking at is before taking the square root, so it is just the average of the square of the differences.) We can also see that there are 404,710 auction records in this group—that is the total size of our training set. The final piece of information shown here is the decision criterion for the best split that was found, which is to split based on the coupler_system column.\n",
        ">\n",
        "> Moving down and to the left, this node shows us that there were 360,847 auction records for equipment where coupler_system was less than 0.5. The average value of our dependent variable in this group is 10.21. Moving down and to the right from the initial model takes us to the records where coupler_system was greater than 0.5.\n",
        ">\n",
        "> The bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node containing records where coupler_system was greater than 0.5. The average value here is 9.21, so we can see the decision tree algorithm did find a single binary decision that separated high-value from low-value auction results. Asking only about coupler_system predicts an average value of 9.21 versus 10.1.\n",
        ">\n",
        "> Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether YearMade is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, based on coupler_system and YearMade) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR6adofTIVqn"
      },
      "source": [
        "**Analysis:** This is one of the harder questions to answer with full text search because it's not explicitly written out in the text. You have to infer the answer from at a minimum the first two paragraphs.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. While it's a difficult question to answer given the context, there's no reason to remove it from the evaluation text.\n",
        "\n",
        "**Tags:** insufficient keywords, difficult question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCny1x6nIzv8"
      },
      "source": [
        "#### Question 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRFPy5mCI0Ye",
        "outputId": "d1ad6e7e-343a-4c20-d373-60b52f103d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 9 14\n",
            "Question Text: \"\"How do we deal with outliers, before building a decision tree?\"\"\n",
            "Answer: \"\"Finding out of domain data (Outliers)\n",
            "Sometimes it is hard to even know whether your test set is distributed in the same way as your training data or, if it is different, then what columns reflect that difference. There's actually a nice easy way to figure this out, which is to use a random forest!\n",
            "But in this case we don't use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set.\"\"\n",
            "Keywords: \"outliers, outlier, decision, tree, trees\"\n"
          ]
        }
      ],
      "source": [
        "print_data(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3QjGToGJChU"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Sometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There's actually an easy way to figure this out, which is to use a random forest!\n",
        ">\n",
        "> But in this case we don't use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set. To see this in action, let's combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVCtHaxQJF_E"
      },
      "source": [
        "**Analysis:** This is a tough one for full text search because the keyword in the question text, \"outlier\", is not used in the relevant context.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I would expect semantic search to retrieve the relevant context.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLnCzf98K0qE"
      },
      "source": [
        "#### Question 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5fGAH1QK068",
        "outputId": "98dbcf69-1e71-4bc2-d2ef-eb7b392ee01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 9 22\n",
            "Question Text: \"\"Explain why random forests are well suited to answering each of the following question:\n",
            "How confident are we in our predictions using a particular row of data?\n",
            "For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\n",
            "Which columns are the strongest predictors?\n",
            "How do predictions vary as we vary these columns?\"\"\n",
            "Answer: \"\"Look at standard deviation between the estimators\n",
            "Using the treeinterpreter package to check how the prediction changes as it goes through the tree, adding up the contributions from each split/feature. Use waterfall plot to visualize.\n",
            "Look at feature importance\n",
            "Look at partial dependence plots\"\"\n",
            "Keywords: \"random forests, predictions, confidence, factors, influence, columns, predictors, variation\"\n"
          ]
        }
      ],
      "source": [
        "print_data(26)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP4bVC2dLRso"
      },
      "source": [
        "**Analysis:** The five questions asked are answered across 10+ paragraphs so I have not listed them here. I'm not sure if I should remove this question because it's five questions in one, or break them apart into five questions. I'm leaning toward removing it since I don't want to alter the question text.\n",
        "\n",
        "**Conclusion:** Remove the question from the evaluation set.\n",
        "\n",
        "**Tags:** unanswerable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuxnDpw8LurW"
      },
      "source": [
        "#### Question 29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysleW4_-Lvcm",
        "outputId": "003f8037-6777-4e42-d4fe-8acb39dce441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 9 29\n",
            "Question Text: \"\"How could we use embeddings with a random forest? Would we expect this to help?\"\"\n",
            "Answer: \"\"Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model.\"\"\n",
            "Keywords: \"embeddings, random forest, forests, machine learning, algorithms\"\n"
          ]
        }
      ],
      "source": [
        "print_data(27)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3hEp4uhMGTg"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The abstract of the entity embedding paper we mentioned at the start of this chapter states: \"the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead\". It includes the very interesting table in <<embedding_mixed>>.\n",
        "\n",
        "> This is showing the mean average percent error (MAPE) compared among four different modeling techniques, three of which we have already seen, along with k-nearest neighbors (KNN), which is a very simple baseline method. The first numeric column contains the results of using the methods on the data provided in the competition; the second column shows what happens if you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. As you see, in every case, the models are dramatically improved by using the embeddings instead of the raw categories.\n",
        "\n",
        "> This is a really important result, because it shows that you can get much of the performance improvement of a neural network without actually having to use a neural network at inference time. You could just use an embedding, which is literally just an array lookup, along with a small decision tree ensemble.\n",
        "\n",
        "**Analysis:** Perhaps the full text search methods would have retrieved the relevant context (or at least parts of it) if the context said \"random forest\" instead of \"decision tree ensemble\".\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I expect semantic search to retrieve the relevant context.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi5bT9NMMfBb"
      },
      "source": [
        "### Chapter 10 (5 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqxjI08SMoAG"
      },
      "source": [
        "#### Question 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO5HHSvjMgVu",
        "outputId": "3d549a24-9dd9-460c-b4c6-1c7c7fc8510c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 10 8\n",
            "Question Text: \"\"What are the three steps to prepare your data for a language model?\"\"\n",
            "Answer: \"\"Tokenization\n",
            "Numericalization\n",
            "Language model DataLoader\"\"\n",
            "Keywords: \"steps, prepare, data, language, model, models\"\n"
          ]
        }
      ],
      "source": [
        "print_data(28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh17J52WMfqh"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Each of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are:\n",
        ">\n",
        "> - Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\n",
        "> - Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab\n",
        "> - Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required\n",
        "> - Language model creation:: We need a special kind of model that does something we haven't seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the <<chapter_nlp_dive>>, but for now, you can think of it as just another deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYKe5RImNDVu"
      },
      "source": [
        "**Analysis:** The issue here is that while the question says \"steps to prepare your data\" the relevant context has the phrasing \"steps necessary to create a language model\". There are 11 other occurences of the word \"steps\" in the chapter, and 0 occurences of the word \"prepare.\"\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Semantic search might perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws8vWlUQNmvR"
      },
      "source": [
        "#### Question 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIqKq7g8Nnlh",
        "outputId": "15e34ac4-055d-4b31-e9bf-69176c8ba903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 10 12\n",
            "Question Text: \"\"List four rules that fastai applies to text during tokenization.\"\"\n",
            "Answer: \"\"Here are all the rules:\n",
            "fix_html :: replace special HTML characters by a readable version (IMDb reviews have quite a few of them for instance) ;\n",
            "replace_rep :: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it's repeated, then the character ;\n",
            "replace_wrep :: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it's repeated, then the word ;\n",
            "spec_add_spaces :: add spaces around / and # ;\n",
            "rm_useless_spaces :: remove all repetitions of the space character ;\n",
            "replace_all_caps :: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\n",
            "replace_maj :: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\n",
            "lowercase :: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\"\"\n",
            "Keywords: \"rules, fastai, text, tokenization, tokens\"\n"
          ]
        }
      ],
      "source": [
        "print_data(29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBHaywS5N1SU"
      },
      "source": [
        "**Retrieved Context:**\n",
        "\n",
        "> These special tokens don't come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. In a sense, we are translating the original English language sequence into a simplified tokenized language—a language that is designed to be easy for a model to learn.\n",
        "\n",
        "> For instance, the rules will replace a sequence of four exclamation points with a special repeated character token, followed by the number four, and then a single exclamation point. In this way, the model's embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization.\n",
        "\n",
        "> Here is a brief summary of what each does:\n",
        ">\n",
        "> - fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n",
        "> - replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it's repeated, then the character\n",
        "> - replace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it's repeated, then the word\n",
        "> - spec_add_spaces:: Adds spaces around / and #\n",
        "> - rm_useless_spaces:: Removes all repetitions of the space character\n",
        "> - replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\n",
        "> - replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\n",
        "> - lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIfyW-KKOQs7"
      },
      "source": [
        "**Analysis:** The two most important keywords, \"four\" and \"rules\", are not present in the key bulleted list of relevant context that lists all of the fastai rules.\n",
        "\n",
        "**Conclusion:** Perhaps a different chunking strategy would have yielded better retrieval using the keyword-based search.\n",
        "\n",
        "**Tags:** chunking strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gRqtqvuPE2J"
      },
      "source": [
        "#### Question 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FW6cDNcPGNs",
        "outputId": "ef2741c8-1522-415e-a7cc-f8b20a44e789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 10 17\n",
            "Question Text: \"\"Why do we need padding for text classification? Why don't we need it for language modeling?\"\"\n",
            "Answer: \"\"Since the documents have variable sizes, padding is needed to collate the batch. Other approaches. like cropping or squishing, either to negatively affect training or do not make sense in this context. Therefore, padding is used. It is not required for language modeling since the documents are all concatenated.\"\"\n",
            "Keywords: \"padding, text, classification, language, modeling, need\"\n"
          ]
        }
      ],
      "source": [
        "print_data(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWGgg0WsPUW9"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> We will expand the shortest texts to make them all the same size. To do this, we use a special padding token that will be ignored by our model. Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend to be of similar lengths. We won't pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, but at the time of writing no library provides good support for this yet, and there aren't any papers covering it. It's something we're planning to add to fastai soon, however, so keep an eye on the book's website; we'll add information about this as soon as we have it working well.)\n",
        "\n",
        "> The sorting and padding are automatically done by the data block API for us when using a TextBlock, with is_lm=False. (We don't have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM5iM3jMPpve"
      },
      "source": [
        "**Analysis:** There are four occurences of the word \"padding\" in the chapter, it occurs once in each of the relevant paragraphs. There are 80+ occurences of the keywords \"text\" and 70+ occurences of \"language\" in the chapter.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Semantic search might perform better.\n",
        "\n",
        "**Tags:** distracting keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufmonDzqQtUC"
      },
      "source": [
        "#### Question 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LAqrWdeQuLS",
        "outputId": "e8013f54-a603-44b9-ec2d-9ab7af388f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 10 20\n",
            "Question Text: \"\"Why do we have to pass the vocabulary of the language model to the classifier data block?\"\"\n",
            "Answer: \"\"This is to ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LM fine-tuning.\"\"\n",
            "Keywords: \"vocabulary, language, model, classifier, data, block\"\n"
          ]
        }
      ],
      "source": [
        "print_data(31)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IngqSk4cRcRn"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> The reason that we pass the vocab of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won't make any sense to this model, and the fine-tuning step won't be of any use.\n",
        "\n",
        "**Analysis:** Only two of the keywords (\"language\" and \"model\") are in the relevant context. Those two words also have very high occurences in the chapter (70 and 120, respectively). The main keyword, \"vocabulary\" is not in the relevant context (a different form of it is, \"vocab\").\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Semantic search might work better (finding similarity between \"vocab\" and \"vocabulary\").\n",
        "\n",
        "**Tags:** insufficient keywords, distracting keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13qOfpxzStc4"
      },
      "source": [
        "#### Question 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1tNVWR0SuOO",
        "outputId": "bffafdcb-d5d3-4fa8-b55c-baf061cb21e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 10 22\n",
            "Question Text: \"\"Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\"\"\n",
            "Answer: \"\"The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead.\"\"\n",
            "Keywords: \"text, generation, automatic, identification, machine-generated, texts\"\n"
          ]
        }
      ],
      "source": [
        "print_data(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDmg6DFzS4NG"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Many people assume or hope that algorithms will come to our defense here—that we will develop classification algorithms that can automatically recognise autogenerated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms.\n",
        "\n",
        "**Analysis:** None of the keywords are present in the relevant context. (\"automatically\" is, but not \"automatic\").\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I expect semantic search to perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLWMzs-TTR66"
      },
      "source": [
        "### Chapter 13 (6 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkX0rrmRTW7t"
      },
      "source": [
        "#### Question 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKnDeHckTS3D",
        "outputId": "7f0cf2dc-a124-4d53-a828-c7e9e4e6f9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 13 4\n",
            "Question Text: \"\"What is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\"\"\n",
            "Answer: \"\"A zero matrix.\"\"\n",
            "Keywords: \"convolutional, kernel, matrix, zeros, value\"\n"
          ]
        }
      ],
      "source": [
        "print_data(33)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3ktg3z7UTou"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Now we're going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we'll add them up, like so:\n",
        "\n",
        "> `im3_t = tensor(im3)`\n",
        ">\n",
        "> `im3_t[0:3,0:3] * top_edge`\n",
        ">\n",
        "> `Output: tensor([[-0., -0., -0.],\n",
        "        [0., 0., 0.],\n",
        "        [0., 0., 0.]])`\n",
        ">\n",
        "> `(im3_t[0:3,0:3] * top_edge).sum()`\n",
        ">\n",
        "> `Output: tensor(0.)`\n",
        "\n",
        "> Not very interesting so far—all the pixels in the top-left corner are white."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMTQF3u3U7fb"
      },
      "source": [
        "**Analysis:** This is a tough context to retrieve for full text search because it relies on interpreting multiple lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7BimVuzVIMK"
      },
      "source": [
        "**Conclusion:** Keep the question in the evaluation set. Semantic search might perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPwdwtfaVPDe"
      },
      "source": [
        "#### Question 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6Y6FfXpVQDi",
        "outputId": "ae722425-d527-4cea-d825-715fa2784865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 13 5\n",
            "Question Text: \"\"What is \"\"padding\"\"?\"\"\n",
            "Answer: \"\"Padding is the additional pixels that are added around the outside of the image, allows the kernel to be applied to the edge of the image for a convolution.\"\"\n",
            "Keywords: \"padding, paddings, cushion, cushions, fill, filler\"\n"
          ]
        }
      ],
      "source": [
        "print_data(34)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr6bac6VVWBZ"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> It would be nice to not lose those two pixels on each axis. The way we do that is to add padding, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added.\n",
        "\n",
        "**Analysis:** I was surprised that none of the full text search methods retrieved this relevant context, since it's usually good with terms/definitions. There are 15 other occurences of \"padding\" in the chapter.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Semantic search might perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_CEqIvV-x2"
      },
      "source": [
        "#### Question 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D94TpnYQWAG_",
        "outputId": "6115122c-4876-4cef-8631-63e68e5d545a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 13 15\n",
            "Question Text: \"\"Why does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications?\"\"\n",
            "Answer: \"\"There are 1168 parameters for that layer, and ignoring the 16 parameters (=number of filters) of the bias, the (1168-16) parameters is applied to the 7x7 grid.\"\"\n",
            "Keywords: \"MNIST, CNN, layer, layers, multiplication, multiplications\"\n"
          ]
        }
      ],
      "source": [
        "print_data(35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4GAiW6YWKu3"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> There is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let's ignore the batch axis to keep things simple. So for each of 14*14=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that's 196*288=56_448 multiplications at this layer. The next layer will have 7*7*(1168-16)=56_448 multiplications.\n",
        "\n",
        "**Analysis:** I'm not sure an LLM would be able to deduce why the multiplication 7*7*(1168-16) is needed. This might be too difficult a question to answer.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. Even though it's difficult to answer.\n",
        "\n",
        "**Tags:** difficult question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUySRWbPXx0g"
      },
      "source": [
        "#### Question 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqG3uT8pXyss",
        "outputId": "8421b232-1d4b-41f9-e710-150f86d0c88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 13 22\n",
            "Question Text: \"\"What method can we use to see that data in DataLoaders?\"\"\n",
            "Answer: \"\"show_batch\"\"\n",
            "Keywords: \"method, data, dataloaders, dataloader, view, visualize\"\n"
          ]
        }
      ],
      "source": [
        "print_data(36)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-2Cjh6X7Xg"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> Remember, it's always a good idea to look at your data before you use it:\n",
        "\n",
        "> `dls.show_batch(max_n=9, figsize=(4,4))`\n",
        "\n",
        "**Analysis:** The keyword \"view\" does not appear in the relevant context, though it does appear in 6 other places in the chapter.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I expect semantic search to perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SujF1GJHYeY_"
      },
      "source": [
        "#### Question 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYDwDg6GYe-J",
        "outputId": "3e0a3042-9303-433d-a96a-c9eb260b7e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 13 24\n",
            "Question Text: \"\"Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\"\"\n",
            "Answer: \"\"With the first layer, if the kernel size is 3x3, with four output filters, then nine pixels are being used to produce 8 output numbers so there is not much learning since input and output size are almost the same. Neural networks will only create useful features if they're forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer.\"\"\n",
            "Keywords: \"kernel, conv, MNIST, simple_cnn, larger\"\n"
          ]
        }
      ],
      "source": [
        "print_data(37)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH1mq0ekY7um"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "> But there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3x3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn't really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they're forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs.\n",
        "\n",
        "> To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5x5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features:\n",
        "\n",
        "**Analysis:** The most important keyword, \"first\" was not generated by Claude for this question. Two of the other keywords, \"conv\" and \"kernel\", have almost 200 occurences in the chapter.\n",
        "\n",
        "**Conclusion:** Keep the question in the evaluation set. I expect semantic search to perform better.\n",
        "\n",
        "**Tags:** insufficient keywords, distracting keywords, semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTpZW4eSaEMx"
      },
      "source": [
        "#### Question 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIhBnvSnaE_B",
        "outputId": "6b7a8d4a-914e-4257-ee72-ed5d3c052a52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter, Question Number: 13 27\n",
            "Question Text: \"\"What are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\"\"\n",
            "Answer: \"\"The mean and standard deviation of the activations, as well as the percentage of activation near zero. The x-axis represents the progress of training (batch number).\"\"\n",
            "Keywords: \"plot_layer_stats, statistics, x-axis, plotted, layers\"\n"
          ]
        }
      ],
      "source": [
        "print_data(38)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TGaLxX0aSFb"
      },
      "source": [
        "**Relevant Context:**\n",
        "\n",
        "\n",
        "> `ActivationStats` includes some handy utilities for plotting the activations during training. `plot_layer_stats(idx)` plots the mean and standard deviation of the activations of layer number `idx`, along with the percentage of activations near zero. Here's the first layer's plot:\n",
        "\n",
        "**Analysis:** Many of the full text search methods retrieved this paragraph. However, that only answers the first part of the question. An answer to the second part of the question, \"What does the x-axis represent?\" is not explicitly provided in the chapter text. You would have to infer it from the image (which is not included in the database).\n",
        "\n",
        "**Conclusion:** Remove the question from the evaluation set.\n",
        "\n",
        "**Tags:** unanswerable, requires image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEyQltAOh90H"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow-wsABeiHOd"
      },
      "source": [
        "Here is a summary of the tags I assigned to each question:\n",
        "\n",
        "|Tag|Count|Percentage of 39|Percentage of 202\n",
        "|:-:|:-:|:-:|:-:|\n",
        "|insufficient keywords|25|64%|12%\n",
        "|semantic search|23|59%|11%\n",
        "|unanswerable|9|21%|4%\n",
        "|distracting keywords|8|10%|4%\n",
        "|difficult questions|5|13%|2%\n",
        "|chunking strategy|3|8%|1%\n",
        "|requires image|2|5%|1%\n",
        "\n",
        "<br>\n",
        "\n",
        "If my intuition is correct, and 23 of the questions that were not answered by any full text search method will be answered by semantic search, I might see a 11% increase in overall performance (from 76.7% to ~88%).\n",
        "\n",
        "There are 9 unanswerable questions, which is 4% of the overall evaluation set. This would improve the overall **Answer Rate** from 76.7% to ~81% (which is not bad for a baseline full text search!)\n",
        "\n",
        "Pursuing a different chunking strategy will only improve performance by 1%. However, if the rate of questions that might be improved by a better chunking strategy is 8% (for these 39 questions) and that rate is applicable to the rest of the 163 questions, I could potentially improve the overall **Answer Rate** by 8%.\n",
        "\n",
        "Of course, this is all speculation at this point and will be determined after I evaluate the semantic search baselines.\n",
        "\n",
        "<br>\n",
        "\n",
        "Here are the number of unanswered questions per chapter:\n",
        "\n",
        "|Chapter|# of Questions|\n",
        "|:-:|:-:|\n",
        "|1|2\n",
        "|2|4\n",
        "|4|7\n",
        "|8|10\n",
        "|9|5\n",
        "|10|5\n",
        "|13|6\n",
        "|**Total**|**39**\n",
        "\n",
        "<br>\n",
        "\n",
        "Here are the tags broken down by chapter:\n",
        "\n",
        "<br>\n",
        "\n",
        "|Chapter|Tag|Count|\n",
        "|:-:|:-:|:-:|\n",
        "|1|semantic search|2|\n",
        "|1|insufficient keywords|2|\n",
        "|1|distracting keywords|1|\n",
        "|2|insufficient keywords|3|\n",
        "|2|distracting keywords|3|\n",
        "|2|semantic search|2|\n",
        "|2|difficult question|1|\n",
        "|4|insufficient keywords|5|\n",
        "|4|semantic search|4|\n",
        "|4|chunking strategy|2|\n",
        "|4|unanswerable|1|\n",
        "|4|difficult question|1|\n",
        "|4|distracting keywords|1|\n",
        "|8|unanswerable|6|\n",
        "|8|semantic search|4|\n",
        "|8|insufficient keywords|4|\n",
        "|8|requires image|1|\n",
        "|9|insufficient keywords|4|\n",
        "|9|semantic search|3|\n",
        "|9|unanswerable|1|\n",
        "|9|difficult question|1|\n",
        "|10|semantic search|4|\n",
        "|10|insufficient keywords|3|\n",
        "|10|distracting keywords|2|\n",
        "|10|chunking strategy|1|\n",
        "|13|semantic search|4|\n",
        "|13|insufficient keywords|4|\n",
        "|13|unanswerable|1|\n",
        "|13|distracting keywords|1|\n",
        "|13|difficult question|1|\n",
        "|13|requires image|1|\n",
        "\n",
        "<br>\n",
        "\n",
        "I want to highlight that 6 of the 29 Chapter 8 questions are \"unanswerable\", so removing those from the evaluation set will greatly increase the **Answer Rate** for that chapter.\n",
        "\n",
        "Here are the updated full text search baseline results with the \"unanswerable\" questions removed from consideration.\n",
        "\n",
        "|Chapter|BM25_A (Top-1 1p)|BM25_B (Top-3 1p)|BM25_C (Top-5 1p)|BM25_D (Top-1 3p)|BM25_E (Top-3 3p)|BM25_F (Top-5 3p)|\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "1|40% (12/30)|56.7% (17/30)|60% (18/30)|63.3% (19/30)|83.3% (25/30)|90% (27/30)|\n",
        "2|38.5% (10/26)|65.4% (17/26)|69.2% (18/26)|46.2% (12.26)|80.8% (21/26)|80.8% (21/26)|\n",
        "4|25.8% (8/31)|71% (22/31)|74.2% (23/31)|32.3% (10/31)|74.2% (23/31)|77.4% (24/31)|\n",
        "8|17.4% (4/23)|43.5% (10/23)|56.5% (13/23)|39.1% (9/23)|69.6% (16/23)|82.6% (19/23)|\n",
        "9|13.8% (4/28)|48.3% (14/28)|58.6% (17/28)|34.5% (10/28)|72.4% (21/28)|79.3% (23/28)|\n",
        "10|47.6% (12/21)|42.9% (9/21)|61.9% (13/21)|38% (8/21)|57.1% (12/21)|61.9% (13/21)|\n",
        "13|38.2% (13/34)|55.9% (19/34)|61.8% (21/34)|44.1% (15/34)|70.6% (24/34)|82.4% (28/34)|\n",
        "**All**|**31.6% (61/193)**|**56% (108/193)**|**63.7% (123/193)**|**43% (83/193)**|**73.6% (142/193)**|**80.3% (155/193)**|\n",
        "\n",
        "With the \"unanswerable\" questions removed, the overall performance of each method increases, with the best-performing BM25_D (Top-1 3-paragraph chunks) reaching an 80.3% **Answer Rate**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BRJEM0gxSe6"
      },
      "source": [
        "I have a much better understanding of the errors made by all of the full text search methods, and this will undoubtedly improve my intuition when applying semantic search to this task.\n",
        "\n",
        "I hope you enjoyed this blog post! Follow me on Twitter [@vishal_learner](https://twitter.com/vishal_learner)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
