---
title: "Regression and Other Stories - Notes and Excerpts"
author: "Vishal Bakshi"
editor: visual
categories: 
  - R
  - statistics
---

In this blog post, I will work through the textbook [*Regression and Other Stories*](https://users.aalto.fi/~ave/ROS.pdf) by Andrew Gelman, Jennifer Hill, and Aki Vehtari. I'll be posting excerpts (sometimes verbatim), notes (paraphrased excerpts) and my thoughts about the content (and any other related reading that I come across while understanding the textbook topics).

Before I get into the reading, I'll note that I have not posted my solutions to exercises to honor the request of one of the authors, Aki Vehtari:

[![](aki_vehtari_tweet_thread.png){fig-alt="A tweet thread of Aki Vehtari explaining why they do not want solutions to the exercises published." width="438"}](https://twitter.com/avehtari/status/1486683277297598476?s=20)

However, to learn by writing, I will write about the process of doing the exercises, the results I got, and what I learned from it all.

# Installation

The data for this textbook is available in the `rosdata` package.

A couple of errors I had to resolve that I'm documenting here.

If you get the following error when installing a package:

```         
Error in file(filename, "r", encoding = encoding) : 
  cannot open the connection
Calls: source -> file
In addition: Warning message:
In file(filename, "r", encoding = encoding) :
  cannot open file 'renv/activate.R': No such file or directory
Execution halted
```

remove the line `source("renv/activate.R")` from both the project directory and home directory `.Rprofile` files.

When running `quarto render` if you get the following error:

`there is no package called …`

the issue might be that Quarto is not pointing towards the right `.libPaths()`. In your home directory's `.Rprofile`, add `.libPaths("path/to/lib", "path/to/lib", …)` and it will show up in `quarto check` and may resolve this issue.

```{r}
#| echo: false
#| include: false
library(tidyverse)
library(rosdata)
library(ggplot2)
library(knitr)
library(bayesplot)
library(rstanarm)
library(rstan)
library(loo)
library(survey)
library(arm)
library(rprojroot)
```

# Preface

-   After reading this book and working through the exercises, you should be able to simulate regression models on the computer and build, critically evaluate, and use them for applied problems.
-   The other special feature of this book is its broad coverage: basics of statistics and measurement, linear regression, multiple regression, Bayesian inference, logistic regression and generalized linear models, extrapolation from sample to population, and causal inference.
-   After completing Part 1 you should have access to the tools of mathematics, statistics, and computing that will allow you to work with regression models.
-   Part 1 Goals
    -   displaying and exploring data
    -   computing and graphing linear relations
    -   understanding basic probability distributions and statistical inferences
    -   simulation of random processes to represent inferential and forecast uncertainty
-   After completing Part 2 you should be able to build, fit, understand, use, and assess the fit of linear regression models.
-   After completing Part 3, you should be able to similarly work with logistic regression and other generalized linear models.
-   Part 4 covers data collection and extrapolation from sample to population.
-   Part 5 covers casual inference.
-   Part 6 introduces more advanced regression models.

## What you should be able to do after reading and working through this book

-   Part 1: Review key tools and concepts in mathematics, statistics and computing

    -   *Chapter 1*: Have a sense of the goals and challenges of regression

    -   *Chapter 2*: Explore data and be aware of issues of measurement and adjustment

    -   *Chapter 3*: Graph a straight line and know some basic mathematical tools and probability distributions

    -   *Chapter 4*: Understand statistical estimation and uncertainty assessment, along with the problems of hypothesis testing in applied statistics

    -   *Chapter 5*: Simulate probability models and uncertainty about inferences and predictions

-   Part 2: Build linear regression models, use them in real problems, and evaluate their assumptions and fit to data

    -   *Chapter 6*: Distinguish between descriptive and causal interpretations of regression, understanding these in historical context

    -   *Chapter 7*: Understand and work with simple linear regression with one predictor

    -   *Chapter 8*: Gain a conceptual understanding of least squares fitting and be able to perform these fits on the computer

    -   *Chapter 9*: Perform and understand probabilistic and simple Bayesian information aggregation, and be introduced to prior distributions and Bayesian inference

    -   *Chapter 10*: Build, fit, and understand linear models with multiple predictors

    -   *Chapter 11*: Understand the relative importance of different assumptions of regression models and be able to check models and evaluate their fit to data

    -   *Chapter 12*: Apply linear regression more effectively by transforming and combining predictors

-   Part 3: Build and work with logistic regression and generalized linear models

    -   *Chapter 13*: Fit, understand, and display logistic regression models for binary data

    -   *Chapter 14*: Build, understand and evaluate logistic regressions with interactions and other complexities

    -   *Chapter 15*: Fit, understand, and display generalized linear models, including the Poisson and negative binomial regression, ordered logistic regression, and other models

-   Part 4: Design studies and use data more effectively in applied settings

    -   *Chapter 16*: Use probability theory and simulation to guide data-collection decisions, without falling into the trap of demanding unrealistic levels of certainty

    -   *Chapter 17*: Use poststratification to generalize from sample to population, and use regression models to impute missing data

-   Part 5: Implement and understand basic statistical designs and analyses for causal inference

    -   *Chapter 18*: Understand assumptions underlying causal inference with a focus on randomized experiments

    -   *Chapter 19*: Perform causal inference in simple setting using regressions to estimate treatments and interactions

    -   *Chapter 20*: Understand the challenges of causal inference from observational data and statistical tools for adjusting for differences between treatment and control groups

    -   *Chapter 21*: Understand the assumptions underlying more advanced methods that use auxiliary variables or particular data structures to identify causal effects, and be able to fit these models to data

-   Part 6: Become aware of more advanced regression models

    -   *Chapter 22*: Get a sense of the directions in which linear and generalized linear models can be extended to attack various classes of applied problems

-   Appendixes

    -   *Appendix A*: Get started in the statistical software R, with a focus on data manipulation, statistical graphics, and fitting using regressions

    -   *Appendix B*: Become aware of some important ideas in regression workflow

After working through the book, you should be able to fit, graph, understand, and evaluate linear and generalized linear models and use these model fits to make predictions and inferences about quantities of interest, including causal effects of treatments and exposures.

# Part 1: Fundamentals

## Chapter 1: Overview

Alternate title: *Prediction as a unifying theme in statistics and causal inference*

### 1.1 The three challenges of statistics

1.  *Generalizing from sample to population*
2.  *Generalizing from treatment to support group*
3.  *Generalizing from observed measurements to the underlying constructs of interest*

**Prediction**: for new people or new items that are not in the sample, future outcomes under differently potentially assigned treatments, and underlying constructs of interest, if they could be measured exactly.

Key skills you should learn from this book:

-   *Understanding regression models:* mathematical models for predicting an outcome variable from a set of predictors, starting with straight-line fits and moving to various nonlinear generalizations

-   *Constructing regression models:* with many options involving the choice of what variables to include and how to transform and constrain them

-   *Fitting regression models to data*

-   *Displaying and interpreting the results*

**Inference**: using mathematical models to make general claims from particular data.

### 1.2 Why learn regression?

**Regression**: a method that allows researchers to summarize how predictions or average values of an *outcome* vary across individuals defined by a set of *predictors*.

```{r}
# load the data
data(hibbs)

# make a scatterplot
plot(
  hibbs$growth, 
  hibbs$vote, 
  xlab = "Average recent growth in personal income",
  ylab = "Incumbent party's vote share")
```

```{r}
#| output: false
# estimate the regression: y = a + bx + error
M1 <- stan_glm(vote ~ growth, data=hibbs)

```

```{r}
# make a scatter plot
plot(
  hibbs$growth, 
  hibbs$vote, 
  xlab = "Average recent growth in personal income",
  ylab = "Incumbent party's vote share")

# add fitted line to the graph
abline(coef(M1), col = 'gray')
```

```{r}
# display the fitted model
print(M1)
```

*y* = 46.3 + 3.0*x*

**MAD**: Median Absolute Deviations.

**sigma**: the scale of the variation in the data explained by the regression model (the scatter of points above and below the regression line). The linear model predicts vote share roughly to an accuracy of 3.9 percentage points.

Some of the most important uses of regression:

-   *Prediction*: Modeling existing observations or forecasting new data.

-   *Exploring associations*: Summarizing how well one variable, or set of variables, predicts the outcome. One can use a model to explore associations, stratifications, or structural relationships between variables.

-   *Extrapolation*: Adjusting for known differences between the *sample* (observed data) and a population of interest.

-   *Causal inference*: Estimating treatment effects. A key challenge of causal inference is ensuring that treatment and control groups are similar, on average, before exposure to treatment, or else adjusting for differences between these groups.

### 1.3 Some examples of regression

#### Estimating public opinion from an opt-in internet survey

-   A characteristic problem of big data: a very large sample, relatively inexpensive to collect, but not immediately representative of the larger population

#### A randomized experiment on the effect of an educational television program

#### Estimating the effects of United Nations peacekeeping, using pre-treatment variables to adjust for differences between treatment and control groups

-   Selection bias: perhaps peacekeepers chose the easy cases, which would explain the difference in outcomes. The treatment--peacekeeping--was not randomly assigned. They had an *observational study* rather than an *experiment*, where we must do our best to adjust for pre-treatment differences between the treatment and control groups.

-   Censoring: certain ranges of data cannot be observed (e.g. countries where civil war had not yet returned by the time data collection ended)

-   When adjusting for badness, peacekeeping was performed in tougher conditions, on average. As a result, the adjustment increases the estimated beneficial effects of peacekeeping, at least during the study.

#### Estimating the effects of gun laws, and the difficulty of inference using regression with a large number of predictors

-   In this sort of regression with 50 data points and 30 predictors and no prior information to guide the inference, the coefficient estimates will be hopelessly noisy and compromised by dependence among predictors.

-   The treatments are observational and not externally applied. There is no reason to believe that the big differences in gun-related deaths between states are mostly attributable to particular policies.

#### Comparing the peacekeeping and gun-control studies

-   In both cases, policy conclusions have been drawn from observational data, using regression modeling to adjust for differences between treatment and control groups.

-   It is more practical to perform adjustments when there is a single goal (peacekeeping study)

    -   particular concern that the UN might be more likely to step in when the situation on the ground was not so bad

    -   the data analysis found the opposite

    -   the measure of badness is constructed based on particular measured variables and so it is possible that there are important unmeasured characteristics that would cause adjustments to go the other way

-   The gun-control model adjusts for many potential causal variables at once

    -   The effect of each law is estimated conditional on all others being held constant (not realistic---no particular reason for their effects to add up in a simple manner)

    -   The comparison is between states, which vary in systematic ways (it is not at all clear that a simple model can hope to adjust for the relevant differences)

    -   Regression analysis was taken naively to be able to control for variation and give valid causal inference from observational data

### 1.4 Challenges in building, understanding, and interpreting regressions

Two different ways in which regression is used for causal inference: estimating a relationship and adjusting for background variables.

#### Regression to estimate a relationship of interest

**Randomization:** a design in which people---or, more generally, experimental units---are randomly assigned to treatment or control groups.

There are various ways to attain approximate comparability of treatment and control groups, and to adjust for known or modeled differences between the groups.

We assume the comparability of the groups assigned to different treatments so that a regression analysis predicting the outcome given the treatment gives us a direct estimate of the causal effect.

It is always possible to estimate a linear model, even if it does not fit the data.

**Interactions**: treatment effects that vary as a function of other predictors in a model.

-   Example: the relation between cancer rate and radon is different for smokers and nonsmokers. The effect of radon is assumed to be linear but with an interaction with smoking.

#### Regression to adjust for differences between treatment and control groups

In most real-world causal inference problems, there are systematic differences between experimental units that receive treatment and control. In such settings it is important to *adjust* for pre-treatment differences between the groups, and we can use regression to do this.

Adjusting for background variables is particularly important when there is *imbalance* so that the treated and control groups differ on key pre-treatment predictors.

The estimated treatment effect is necessarily model based.

#### Interpreting coefficients in a predictive model

A model fit to survey data: earnings = 11000 + 1500 \* (height - 60) + error, with errors in the range of mean of 0 and standard deviation of 22000.

-   *Prediction*: useless for forecasting because errors are too large

-   *Exploring an association*: best fit for this example, as the estimated slope is positive, can lead to further research to study reasons taller people earn more than shorter people

-   *Sampling inference*: the regression coefficients can be interpreted directly to the extent that people in the survey are a representative sample of the population of interest

-   *Causal inference*: height is not a randomly assigned treatment. Tall and short people may differ in many other ways.

#### Building, interpreting and checking regression models

Statistical analysis cycles through four steps:

1.  Model building
2.  Model fitting
3.  Understanding model fits
4.  Criticism

Accept that we can learn from statistical analysis---we can generalize from sample to population, from treatment to control, and from observed measurements to underlying constructs of interest---even while these inferences can be flawed.

*Overinterpretation of noisy data*: the gun control study took existing variations among states and too eagerly attributed it to available factors.

No study is perfect.

We should *recognize* challenges in extrapolation and then work to *adjust* for them.

### 1.5 Classical and Bayesian Inference

Three concerns related to fitting models to data and using models to make predictions:

1.  what *information* is being used in the estimation process
2.  what *assumptions* are being made
3.  how estimates and predictions are *interpreted*, in a classical or Bayesian framework

#### Information

-   The starting point for any regression problem is data on an outcome variable *y* and one or more predictors *x*.

-   Information should be available on what data were observed at all

-   We typically have prior knowledge coming from sources other than the data at hand. Where local data are weak it would be foolish to draw conclusions without using prior knowledge

#### Assumptions

-   Three sorts of assumptions that are essential to any regression model of an outcome *y* given predictors *x*.

    -   The functional form of the relation between *x* and *y*

    -   Where the data comes from (strongest assumptions tend to be simple and easy to understand, weaker assumptions, being more general, can also be more complicated)

    -   Real-world relevance of the measured data

        -   The interpretation of a regression of *y* on *x* depends also on the relation between the measured *x* and the underlying predictors of interest, and on the relation between the measured *y* and the underlying outcomes of interest

#### Classical inference

The traditional approach to statistical analysis is based on summarizing the information in the data, not using prior information, but getting estimates and predictions that have well-understood statistical properties, low bias and low variance.

**Unbiasedness:** estimates should be true on average

**Coverage:** confidence intervals should cover the true parameter value 95% of the time

**Conservatism:** sometimes data are weak and we can't make strong statements, but we'd like to be able to say, at least approximately, that our estimates are unbiased and our intervals have the advertised coverage.

In classical statistics there should be a clear and unambiguous ("objective") path from data to inferences, which in turn should be checkable, at least in theory, based on their frequency properties.

#### Bayesian inference

Incorporates prior information into inferences, going beyond the goal of merely summarizing existing data. The analysis gives more reasonable results and can be used to make direct predictions about future outcomes and about the results of future experiments.

The prior distribution represents the arena over which any predictions will be evaluated.

We have a choice: classical inference, leading to pure summaries of data which can have limited value as predictions; or Bayesian inference, which in theory can yield valid predictions even with weak data, but relies on additional assumptions.

All Bayesian inferences are probabilistic and thus can be represented by random simulations. For this reason, whenever we want to summarize uncertainty in estimation beyond simple confidence intervals, and whenever we want to use regression models for predictions, we go Bayesian.

### 1.6 Computing least squares and Bayesian regression

In general, we recommend using Bayesian inference for regression

-   If prior information is available, you can use it

-   if not, Bayesian regression with weakly informative default priors still has the advantage of yielding stable estimates and producing simulations that enable you to express inferential and predictive uncertainty (estimates with uncertainties and probabilistic predictions or forecasts)

Bayesian regression in R:

```         
fit <- stan_glm(y ~ x, data=mydata)

# stan_glm can run slow for large datasets, make it faster by running in optimizing mode
fit <- stan_glm(y ~ x, data=mydata, algorithm="optimizing")
```

least squares regression (classical statistics)

```         
fit <- lm(y ~ x, data=mydata)
```

Bayesian and simulation approaches become more important when fitting regularized regression and multilevel models.

### 1.8 Exercises

It took me 8-9 days to complete the 10 exercises at the end of this chapter. A few observations:

-   Experiment design, even with simple paper helicopters, still requires time, effort and attention to detail. Even then, a well thought out and executed plan can still end up with uninspiring results. It is therefore the process of experimentation and not the result where I learned the most. It was important to continue experimentation (as it resulted in finding a high-performing copter) even after my initial approach didn't seem to work as I had thought it would.

-   Finding good and bad examples of research is hard on your own! I relied on others existing work where they explicitly called out (and explained) good and bad research, sometimes with great detail, in order to answer exercises 1.7 and 1.8.

-   Think about the problem in real life! I don't know if my answer to 1.9 is correct, but it helped to think about the physical phenomenon that was taking place (a helicopter falling through air, pulled by gravity) in order to come up with a better model fit than the example provided. I found that thinking of physical constraints of the real world problem helped narrow the theoretical model form.

-   Pick something you're interested in and is worth your while! For exercise 1.10, the dataset I have chosen is the [NFL Play-by-Play dataset](https://github.com/nflverse/nflverse-data/releases/tag/pbp), as I'm already working on [a separate project where I'll be practicing writing SQL queries to analyze the data](http://vishalbakshi.com/blog/posts/2023-02-11-nflverse/2023-02-11-nflverse.html). Play-by-play data going back to 1999 will provide me with ample opportunities to pursue different modeling choices. My interest in football will keep me dedicated through the tough times.

## Chapter 2: Data and Measurement

In this book we will be:

-   fitting lines and curves to data.

-   making comparisons and predictions and assessing our uncertainties in the resulting inferences.

-   discuss the assumptions underlying regression models, methods for checking these assumptions, and directions for improving fitted models.

-   discussing the challenges of extrapolating from available data to make causal inferences and predictions for new data.

-   using computer simulations to summarize the uncertainties in our estimates and predictions.

### 2.1 Examining where data comes from

I'll recreate the charts shown in the textbook by following the code provided in the website supplement to the book, using tidyverse where it makes sense:

```{r}
# load the Human Development Index data
# start using DT to produce better table outputs
library(DT)
hdi <- read.table("../../../ROS_data/HDI/data/hdi.dat", header=TRUE)
DT::datatable(head(hdi))
```

```{r}
# load income data via the votes data
library(foreign)
votes <- read.dta("../../../ROS_data/HDI/data/state vote and income, 68-00.dta")
DT::datatable(head(votes), options=list(scrollX=TRUE))
```

```{r}
# preprocess
income2000 <- votes[votes[,"st_year"]==2000, "st_income"]

# it seems like the income dataset doesn't have DC
# whereas HDI does
# so we're placing an NA for DC in the income list
state.income <- c(income2000[1:8], NA, income2000[9:50])
state.abb.long <- c(state.abb[1:8], "DC", state.abb[9:50])
state.name.long <- c(state.name[1:8], "Washington, D.C.", state.name[9:50])
hdi.ordered <- rep(NA, 51)
can <- rep(NA, 51)

for (i in 1:51) {
  ok <- hdi[,"state"]==state.name.long[i]
  hdi.ordered[i] <- hdi[ok, "hdi"]
  can[i] <- hdi[ok, "canada.dist"]
}
no.dc <- state.abb.long != "DC"
```

```{r}
# plot average state income and Human Development Index
par(mar=c(3, 3, 2.5, 1), mgp=c(1.5, .2, 0), tck=-0.01, pty='s')
plot(
  state.income, 
  hdi.ordered, 
  xlab="Average state income in 2020",
  ylab="Human Development Index",
  type="n")

text(state.income, hdi.ordered, state.abb.long)
```

The pattern between HDI numbers and average state income is strong and nonlinear. If anyone is interested in following up on this, we suggest looking into South Carolina and Kentucky, which are so close in average income and so far apart on HDI.

```{r}
# plot rank of average state income and Human Development Index
par(mar=c(3, 3, 2.5, 1), mgp=c(1.5, 0.2, 0), tck=-0.01, pty='s')
plot(
  rank(state.income[no.dc]), 
  rank(hdi.ordered[no.dc]),
  xlab="Rank of average state income in 2000",
  ylab="Rank of Human Development Index", 
  type="n")

text(rank(state.income[no.dc]), rank(hdi.ordered[no.dc]), state.abb)
```

There is a high correlation between the ranking of HDI and state income.

I will redo the above data processing and plotting with tidyverse and ggplot for practice:

```{r}
# plot average state income and Human Development Index
# using tidyverse and ggplot
merged_data <- hdi %>% dplyr::left_join(
  votes %>% filter(st_year==2000),
  by = c("state" = "st_state")
) 

p <- ggplot(
  merged_data,
  aes(x = st_income, y = hdi, label =  st_stateabb)
) + theme(
    plot.margin = margin(3, 3, 2.5, 1, "lines"),
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    panel.background = element_rect(fill = 'white'),
    aspect.ratio = 1
  ) +
  labs(
    x = "Average state income in 2020",
    y = "Human Development Index"
  )

p + geom_text()
```

None of the tidyverse methods I found (`row_number`, `min_rank`, `dense_rank`, `percent_rank`, `cume_dist`, or `ntile`) work quite like rank (where "ties" are averaged), so I continued using `rank` for this plot:

```{r}
# plot rank of average state income and Human Development Index
p <- ggplot(
  merged_data %>% filter(state != 'Washington, D.C.'), 
  aes(x = rank(st_income), y = rank(hdi), label = st_stateabb),
  ) + theme(
    plot.margin = margin(3, 3, 2.5, 1, "lines"),
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    panel.background = element_rect(fill = 'white'),
    aspect.ratio = 1
  ) +
  labs(
    x = "Rank of average state income in 2000",
    y = "Rank of Human Development Index"
  )

p + geom_text()
```

The relevance of this example is that we were better able to understand the data by plotting them in different ways.

#### Details of measurement can be important

In the code chunks below, I will recreate the plots in Figure 2.3 in the text (distribution of political ideology by income and distribution of party identification by income).

```{r}
pew_pre <- read.dta("../../../ROS_data/Pew/data/pew_research_center_june_elect_wknd_data.dta")
n <- nrow(pew_pre)
head(pew_pre)
```

```{r}
# weight
pop.weight0 <- pew_pre$weight

head(unique(pop.weight0))
```

```{r}
# income (1-9 scale)
inc <- as.numeric(pew_pre$income)

# remove "dk/refused" income level
inc[inc==10] <- NA

levels(pew_pre$income)
unique(inc)
```

```{r}
# I believe these are the midpoints of the income levels
value.inc <- c(5,15,25,35,45,62.5,87.5,125,200)

# maximum income value
n.inc <- max(inc, na.rm = TRUE)
n.inc
```

```{r}
# party id
pid0 <- as.numeric(pew_pre[,"party"])
lean0 <- as.numeric(pew_pre[,"partyln"])

levels(pew_pre[,"party"])
levels(pew_pre[,"partyln"])
unique(pid0)
unique(lean0)
```

```{r}
# assigning new integers for party id
pid <- ifelse(pid0==2, 5,  # Republican
         ifelse(pid0==3, 1,  # Democrat
         ifelse(lean0==2, 4, # Lean Republican
         ifelse(lean0==4, 2, # Lean Democrat
         3)))) # Independent

pid.label <- c("Democrat", "Lean Dem.", "Independent", "Lean Rep.", "Republican")
n.pid <- max(pid, na.rm=TRUE)
```

```{r}
# ideology
ideology0 <- as.numeric(pew_pre[,"ideo"])

levels(pew_pre[,"ideo"])
unique(ideology0)
```

```{r}
# assign new integers for ideology
ideology <- ifelse(ideology0==2, 5, # Very conservative
              ifelse(ideology0==3, 4, # Conservative
              ifelse(ideology0==6, 1, # Very liberal
              ifelse(ideology0==5, 2, # Liberal
              3)))) # Moderate
ideology.label <- c("Very liberal", "Liberal", "Moderate", "Conservative", "Very conservative")
n.ideology <- max(ideology, na.rm=TRUE)
```

```{r}
# plot settings
par(mar=c(3,2,2.5,1), mgp=c(1.5, .7, 0), tck=-0.01)

# this creates an empty plot
plot(c(1,n.inc), c(0,1), xaxs="i", yaxs="i", type="n", xlab="", ylab="", xaxt="n", yaxt="n")

# add x-axis tick marks with empty string labels
axis(1, 1:n.inc, rep("", n.inc))

# add x-axis labels
axis(1, seq(1.5, n.inc-.5, length=3), c("Low income", "Middle income", "High income"), tck=0)

# add y-axis tick marks and labels
axis(2, c(0, 0.5, 1), c(0, "50%", "100%"))

center <- floor((1 + n.inc) / 2)

incprop <- array(NA, c(n.pid + 1, n.inc))

incprop[n.pid + 1,] <- 1


for (i in 1:n.pid) {
  for (j in 1:n.inc) {
    incprop[i, j] <- weighted.mean((pid<i)[inc==j], pop.weight0[inc==j], na.rm = TRUE)
  }
}

for (i in 1:n.pid){
  polygon(c(1:n.inc, n.inc:1), c(incprop[i,], rev(incprop[i+1,])), col=paste("gray", 40+10*i, sep=""))
  lines(1:n.inc, incprop[i,])
  text(center, mean(incprop[c(i,i+1),center]), pid.label[i])
}
mtext("Self-declared party identification, by income", side=3, line=1, cex=1.2)
```

```{r}
par(mar=c(3,2,2.5,1), mgp=c(1.5,.7,0), tck=-.01)
plot(c(1,n.inc), c(0,1), xaxs="i", yaxs="i", type="n", xlab="", ylab="", xaxt="n", yaxt="n")
axis(1, 1:n.inc, rep("",n.inc))
axis(1, seq(1.5,n.inc-.5,length=3), c("Low income", "Middle income", "High income"), tck=0)
axis(2, c(0,.5,1), c(0,"50%","100%"))
center <- floor((1+n.inc)/2)
incprop <- array(NA, c(n.ideology+1,n.inc))
incprop[n.ideology+1,] <- 1
for (i in 1:n.ideology){
  for (j in 1:n.inc){
    incprop[i,j] <- weighted.mean((ideology<i)[inc==j], pop.weight0[inc==j], na.rm=TRUE)
  }
}
for (i in 1:n.ideology){
  polygon(c(1:n.inc, n.inc:1), c(incprop[i,], rev(incprop[i+1,])), col=paste("gray", 40+10*i, sep=""))
  lines(1:n.inc, incprop[i,])
  text(center, mean(incprop[c(i,i+1),center]), ideology.label[i])
}
mtext("Self-declared political ideology, by income", side=3, line=1, cex=1.2)
```

I'll try to recreate the two plots using tidyverse and ggplot. I referenced [this article by the Pew Research Center which shows how to calculated weighted estimates.](https://www.pewresearch.org/decoded/2019/06/12/using-tidy-verse-tools-with-pew-research-center-survey-data-in-r/)

```{r}
pew_pre <- pew_pre %>%
  mutate(
    # create an integer column for income levels
    inc = as.numeric(income),
    # remove "dk/refuse" value for income
    inc = case_when(
      inc == 10 ~ NA,
      TRUE ~ inc
    ),
    # set political party integer column
    pid0 = as.numeric(party),
    # set lean integer column
    lean0 = as.numeric(partyln),
    # set ideology integer column
    ideology0 = as.numeric(ideo),
    # re-assign party values
    pid = case_when(
      pid0 == 2 ~ 5, # Repubican
      pid0 == 3 ~ 1, # Democrat
      lean0 == 2 ~ 4, # Lean Republican
      lean0 == 4 ~ 2, # Lean Democrat
      is.na(pid0) ~ NA,
      TRUE ~ 3 # Independent
    ),
    # reassign ideology values
    ideology = case_when(
      ideology0 == 2 ~ 5, # Very conservative
      ideology0 == 3 ~ 4, # Conservative
      ideology0 == 6 ~ 1, # Very liberal
      ideology0 == 5 ~ 2, # Liberal
      is.na(ideology0) ~ NA,
      TRUE ~ 3 # Moderate
    )
  )

# constants
n_inc <- max(pew_pre$inc, na.rm = TRUE)
n_pid <- max(pew_pre$pid, na.rm = TRUE)
n_ideology <- max(pew_pre$ideology, na.rm = TRUE)
```

```{r}
# calculate income proportions using population weight
inc_totals <- pew_pre %>% group_by(
  inc
) %>% summarize(
  total_inc = n()
)

pid_weighted_estimates <- pew_pre %>% dplyr::left_join(
  inc_totals
) %>% group_by(
  inc,
  pid
) %>% summarise(
  weighted_n = sum(weight)
) %>% mutate(
  weighted_group_size = sum(weighted_n),
  weighted_estimate = weighted_n / weighted_group_size
) %>% arrange(
  inc
)
```

```{r}
DT::datatable(pid_weighted_estimates)
```

```{r}
ggplot(pid_weighted_estimates, aes(x=inc, y=weighted_estimate)) + 
  geom_area(aes(group = pid, fill = pid), position = position_stack(reverse = TRUE), show.legend = FALSE) +
  annotate("text", x=5, y=.2, label="Democrat", color="white") + 
  annotate("text", x=5, y=.425, label="Lean Dem.", color="white") + 
  annotate("text", x=5, y=.55, label="Independent", color="white") + 
  annotate("text", x=5, y=.68, label="Lean Rep.") + 
  annotate("text", x=5, y=.87, label="Republican") + 
  scale_x_continuous("Average Income Level", breaks=c(1,5,9), labels = c("Low Income", "Middle Income", "High Income"), , expand = c(0, 0)) +
  scale_y_continuous("Proportion of Sample", breaks=c(0,.5, 1), labels = c("0%", "50%", "100%"), expand = c(0, 0)) +
  theme(
    plot.margin = margin(3, 3, 2.5, 1, "lines"),
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    plot.title = element_text(size=14,hjust = 0.5),
    axis.text = element_text(size = 10)
  ) + 
  ggtitle("Party Identification by Income Levels")
```

```{r}
# ideology data prep
ideo_weighted_estimates <- pew_pre %>% dplyr::left_join(
  inc_totals
) %>% group_by(
  inc,
  ideology
) %>% summarise(
  weighted_n = sum(weight)
) %>% mutate(
  weighted_group_size = sum(weighted_n),
  weighted_estimate = weighted_n / weighted_group_size
) %>% arrange(
  inc
)
```

```{r}
DT::datatable(ideo_weighted_estimates)
```

```{r}
ggplot(ideo_weighted_estimates, aes(x=inc, y=weighted_estimate)) + 
  geom_area(aes(group = ideology, fill = ideology), position = position_stack(reverse = TRUE), show.legend = FALSE) +
  annotate("text", x=5, y=.03, label="Very Liberal", color="white", size=3) + 
  annotate("text", x=5, y=.13, label="Liberal", color="white", size=3) + 
  annotate("text", x=5, y=.4, label="Moderate", color="white", size=3) + 
  annotate("text", x=5, y=.75, label="Conservative", size=3) + 
  annotate("text", x=5, y=.95, label="Very Conservative", size=3) + 
  scale_x_continuous("Average Income Level", breaks=c(1,5,9), labels = c("Low Income", "Middle Income", "High Income"), , expand = c(0, 0)) +
  scale_y_continuous("Proportion of Sample", breaks=c(0,.5, 1), labels = c("0%", "50%", "100%"), expand = c(0, 0)) +
  theme(
    plot.margin = margin(3, 3, 2.5, 1, "lines"),
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    plot.title = element_text(size=14,hjust = 0.5),
    axis.text = element_text(size = 10)
  ) + 
  ggtitle("Ideology by Income Levels")
```

Overall, I think the plots I made look pretty similar to the textbook's. The main bottleneck was learning how to calculate proportions based on weighted estimates. Once that took shape, I was able to figure out how to present the data visually in a way that resembles the authors' approach.

The figure "Ideology by Income Levels" shows that the proportion of political liberals, moderates, and conservatives is about the same for all income levels. The figure "Party Identification by Income Levels" shows a strong relation between income and Republican partisanship, at least as of 2008. The partisanship and ideology example is a reminder that even very similar measures can answer quite different questions.

### 2.2 Validity and reliability

Data analysis reaches a dead end if we have poor data.

Two reasons for discussing measurement:

-   we need to understand what our data actually mean, otherwise we cannot extract the right information

-   learning about accuracy, reliability and validity will set a foundation for understanding variance, correlation, and error, which will be useful in setting up linear models in the forthcoming chapters

The property of being precise enough is a combination of the properties of the scale and what we are trying to use it for.

In social science, the way to measure what we are trying to measure is not as transparent as it is in everyday life.

Other times, the thing we are trying to measure is pretty straightforward, but a little bit fuzzy, and the ways to tally it up aren't obvious.

Sometimes we are trying to measure something that we all agree has meaning, but which is subjective for every person and does not correspond to a "thing" we can count or measure with a ruler.

Attitudes are private; you can't just weigh them or measure their widths.

It can be helpful to take multiple measurements on an underlying construct of interest.

#### Validity

A measure is *valid* to the degree that it represents what you are trying to measure. Asking people how satisfied they are with some government service might not be considered a valid measure of the effectiveness of that service.

Valid measures are ones in which there is general agreement that the observations are closely related to the intended construct.

**validity:** the property of giving the right answer on average across a wide range of plausible scenarios. To study validity in an empirical way, ideally you want settings in which there is an observable true value and multiple measurements can be taken.

#### Reliability

A *reliable* measure is one that is precise and stable. We would hope the variability in our sample is due to real differences among people or things, and not due to random error incurred during the measurement process.

#### Sample selection

**selection:** the idea that the data you see may be a nonrepresentative sample of a larger population you will not see.

In addition to selection bias, there are also biases from nonresponse to particular survey items, partially observed measurements, and choices in coding and interpretation of data.

### 2.3 All graphs are comparisons
