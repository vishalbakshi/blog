---
title: My Top-5 Blog Posts I've Written this Year
date: "2025-09-03"
author: Vishal Bakshi
description: Inspired by a question on Twitter, I share my top 5 blog posts I've written this year, after surpassing my 2025 writing goal.
   - lightbox
lightbox: auto
---

I recently surpassed my goal of publishing 50 machine learning blog posts in 2025. I shared that on Twitter and got this interesting question from Skylar Payne:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Do you have one that is your favorite or one you are most proud of?</p>&mdash; Skylar Payne (@skylar_b_payne) <a href="https://twitter.com/skylar_b_payne/status/1963262456811205081?ref_src=twsrc%5Etfw">September 3, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

I browsed through the blog posts I've published this year, and I found it really hard to pick just one because each blog post either taught me something new (I learn through writing) or gave me an opportunity to try something new (writing style, blog post topic or technical project) that I was scared to do in public. They all served a purpose!

So instead I am picking my top 5 in no particular order. 

1. [RAGatouille/ColBERT Indexing Deep Dive](https://vishalbakshi.github.io/blog/posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/): This was a critical deep dive in my understanding of the ColBERT Library, and is the main reason why I am able to navigate the codebase today as a maintainer. 
2. [Memory Profiling raw ColBERT and RAGatouille](https://vishalbakshi.github.io/blog/posts/2025-02-14-RAGatouille-ColBERT-Memory-Profiling/): This was the first time I had done non-trivial memory profiling, and it also served as a great mechanism to better understand the ColBERT and RAGatouille codebases.
3. [Proof, Pricing, and Passion: Finding My Path in Machine Learning](https://vishalbakshi.github.io/blog/posts/2025-06-09-fireside-chat/): This was the first career-focused blog post I had published, which was a big step for me in sharing thoughts from a point of vulnerability publicly. 
4. [An Analysis of Batch Size vs. Learning Rate on Imagenette](https://vishalbakshi.github.io/blog/posts/2025-06-18-imagenette/): This was an important exploration as it built my fundamental intuition around the relationship between batch size, learning rate, and downstream performance (accuracy).
5. [Introducing portfolio-llm: A Professional Portfolio You Can Chat With](https://vishalbakshi.github.io/blog/posts/2025-06-26-portfolio-llm/): I don't consider myself an innovative applied AI thinker so this project was a breakthrough for me because I feel like it's the first interesting applied AI idea I've had. 

This was a fun exercise, and definitely something I'll do again at the end of the year and routinely moving forward. It gave me a moment to reflect and appreciate the progress that I've made in my ML journey. 

