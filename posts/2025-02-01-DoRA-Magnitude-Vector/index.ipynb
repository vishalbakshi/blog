{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: DoRA's Magnitude Vector\n",
        "date: \"2025-02-01\"\n",
        "author: Vishal Bakshi\n",
        "description: In this blog post I highlight a key difference I saw between Raschka's and peft's implementation of DoRA.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "    - machine learning\n",
        "    - LLM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYaBa-dY9lKw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NBGa8FUR9mWi"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "import transformers\n",
        "\n",
        "# the following imports are from dora.py\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from peft.utils.integrations import dequantize_module_weight, gather_params_ctx\n",
        "from peft.utils.other import transpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIISFIMIXk4B"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlCiU_MKXmIj"
      },
      "source": [
        "I am currently re-reading the DoRA (Weight-Decomposed Low-Rank Adaptation) [paper](https://arxiv.org/abs/2402.09353). I took a bit of a detour and worked through the fantastic article [Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch) by Sebastian Raschka (I am also reading his book [Building a Large Language Model (from scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch) as part of a fastai study group). The article is full of helpful diagrams and breakdowns of concepts as well as easily digestible implementation in code. One particular breakthrough for me while reading the article was his demonstration of the distributive law of multiplication:\n",
        "\n",
        "> **x.(W+ΔW) = x.W + x.ΔW**\n",
        ">\n",
        "> Similarly, we can write the following for LoRA:\n",
        ">\n",
        "> **x.(W+A.B) = x.W + x.A.B**\n",
        "\n",
        "Reading this made it click for me why and how LoRA adapters are such an efficient way of handling downstream tasks.\n",
        "\n",
        "I also took a deep dive into the peft library's implementation of DoRA. I recently made [a video](https://youtu.be/GE6jRudHhzY) of this deep dive.\n",
        "\n",
        "In this blog post I am going to compare Raschka's article's implementation with peft's and highlight a key difference that I found between them in how they implement the decomposition of a weight matrix into its magnitude and directional components.\n",
        "\n",
        "I'll start by reviewing both approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyeJEU87bAzr"
      },
      "source": [
        "## Raschka's Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpt6ee96bIHC"
      },
      "source": [
        "I want to add a caveat that this implementation I assume is by no means a \"final\" or \"production\" implementation, as I understand it to be more educational and illustrative.\n",
        "\n",
        "I'll start by copy/pasting relevant code: `LoRALayer` (DoRA uses LoRA to fine-tune the directional component) and `LinearWithDoRAMerged`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7N6h96Owa__V"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Qka94nq2cdYB"
      },
      "outputs": [],
      "source": [
        "class LinearWithDoRAMerged(nn.Module):\n",
        "\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "        self.m = nn.Parameter(\n",
        "            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
        "\n",
        "\n",
        "  # Code loosely inspired by\n",
        "  # https://github.com/catid/dora/blob/main/dora.py\n",
        "\n",
        "    def forward(self, x):\n",
        "        lora = self.lora.A @ self.lora.B\n",
        "        numerator = self.linear.weight + self.lora.alpha*lora.T\n",
        "        denominator = numerator.norm(p=2, dim=0, keepdim=True)\n",
        "        directional_component = numerator / denominator\n",
        "        new_weight = self.m * directional_component\n",
        "        return F.linear(x, new_weight, self.linear.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dST3s8IIz6XP"
      },
      "source": [
        "I'll also create a regular linear layer using one of the in/out feature values in the Raschka article:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxryBDBsz5aZ",
        "outputId": "5a631bc3-b6ef-49b4-c1df-1c32dab0254f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=128, bias=True)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear = nn.Linear(in_features=784, out_features=128, bias=True)\n",
        "linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5OZ21PQ0XM1",
        "outputId": "f7d1e34c-c79b-4135-e904-3d1cfa69f834"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearWithDoRAMerged(\n",
              "  (linear): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (lora): LoRALayer()\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dora_layer = LinearWithDoRAMerged(linear, 256, 512)\n",
        "dora_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Lju6bF0rHe"
      },
      "source": [
        "Here's the key value: the shape of the magnitude vector. In Raschka's code, it's 1 x 784, where 784 is the number of linear `in_features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hWu0DU_0doE",
        "outputId": "6dd100d0-55e6-42bc-c49a-9d0eecf0d5a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 784])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dora_layer.m.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LlmOGs31h8A"
      },
      "source": [
        "Looking at `LinearWithDoRAMerged.__init__`:\n",
        "\n",
        "```python\n",
        "self.m = nn.Parameter(\n",
        "            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
        "```\n",
        "\n",
        "The `norm` is taking over `dim=0`, which is the dimension of `out_features`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w85C9lhT0l79",
        "outputId": "c9bf9bab-831b-4c3a-ea4a-f0c54642b4a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 784])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear.weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH6pv4TI11ko"
      },
      "source": [
        "In other words, we end up with 1 magnitude value for each of the 784 input neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts-Rc85w18km"
      },
      "source": [
        "## `peft` Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWLnYZZV2Ieo"
      },
      "source": [
        "From [`src/peft/tuners/lora/dora.py`](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/dora.py):\n",
        "\n",
        "```python\n",
        "class DoraLinearLayer(nn.Module):\n",
        "    def __init__(self, fan_in_fan_out):\n",
        "        super().__init__()\n",
        "        self.fan_in_fan_out = fan_in_fan_out\n",
        "\n",
        "    def get_weight_norm(self, weight, lora_weight, scaling) -> torch.Tensor:\n",
        "        # calculate L2 norm of weight matrix, column-wise\n",
        "        weight = transpose(weight, self.fan_in_fan_out)\n",
        "        weight = weight + scaling * lora_weight\n",
        "        weight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)\n",
        "        return weight_norm\n",
        "\n",
        "    ...\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM5O8Dz52Snu"
      },
      "source": [
        "The very important attribute here is `fan_in_fan_out`. I found a few places in the peft codebase which documented it as follows:\n",
        "\n",
        "```\n",
        "Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
        "```\n",
        "\n",
        "- [src/peft/tuners/ia3/config.py](https://github.com/huggingface/peft/blob/0facdebf6208139cbd8f3586875acb378813dd97/src/peft/tuners/ia3/config.py#L79)\n",
        "- [src/peft/tuners/vblora/layer.py](https://github.com/huggingface/peft/blob/0facdebf6208139cbd8f3586875acb378813dd97/src/peft/tuners/vblora/layer.py#L122)\n",
        "- [src/peft/tuners/vblora/layer.py](https://github.com/huggingface/peft/blob/0facdebf6208139cbd8f3586875acb378813dd97/src/peft/tuners/vblora/layer.py#L122)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlrVoWnn_RHq"
      },
      "source": [
        "How I interpret this: if the weights are stored as (in, out), `fan_in_fan_out` is `True`, if stored as (out, in) `fan_in_fan_out` is `False`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDkOPDdR9Tor"
      },
      "source": [
        "Looking at an example, I'll peft-ify SmolLM2-135M:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Smb6DG1b1ve8",
        "outputId": "f2fdc27d-3a29-4b72-9c84-ce1d79ea74d9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'HuggingFaceTB/SmolLM2-135M'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_nm = 'HuggingFaceTB/SmolLM2-135M'\n",
        "model_nm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7hL6r2Y9yRx"
      },
      "outputs": [],
      "source": [
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_UK32Qxu9zpG"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(r=256, use_rslora=False, use_dora=True, target_modules=['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj2qVBeV956y",
        "outputId": "4166bf0b-8ce5-44df-bc5f-43ac8a7e4bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 78,307,200 || all params: 212,823,360 || trainable%: 36.7945\n"
          ]
        }
      ],
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GghOoI_A-Cxc"
      },
      "source": [
        "Looking at one of the layers which has a different number of input and output features, `k_proj`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2JERLwM96cM",
        "outputId": "c9b363b2-1042-4507-e625-7065bdaa5d89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "lora.Linear(\n",
              "  (base_layer): Linear(in_features=576, out_features=192, bias=False)\n",
              "  (lora_dropout): ModuleDict(\n",
              "    (default): Identity()\n",
              "  )\n",
              "  (lora_A): ModuleDict(\n",
              "    (default): Linear(in_features=576, out_features=256, bias=False)\n",
              "  )\n",
              "  (lora_B): ModuleDict(\n",
              "    (default): Linear(in_features=256, out_features=192, bias=False)\n",
              "  )\n",
              "  (lora_embedding_A): ParameterDict()\n",
              "  (lora_embedding_B): ParameterDict()\n",
              "  (lora_magnitude_vector): ModuleDict(\n",
              "    (default): lora.dora.DoraLinearLayer()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k_proj = model.base_model.model.model.layers[0].self_attn.k_proj\n",
        "k_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55AgEiua-Y5j"
      },
      "source": [
        "The base layer has 576 `in_features` and 192 `out_features`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_6h7Vaq-TEu",
        "outputId": "937ded35-e1fd-4b5c-d98b-4a4781871a2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([192, 576])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k_proj.base_layer.weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyknwS_x_NnT"
      },
      "source": [
        "The `fan_in_fan_out` attribute is `False` which checks out by looking at the shape above which is (out, in)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnkNGZz5_K_L",
        "outputId": "a2b509cf-8cba-425d-95ae-d29baf2d806c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k_proj.fan_in_fan_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIxm1KNZ_o5h"
      },
      "source": [
        "Why is `fan_in_fan_out` such a big deal to me? Well, because look at how `get_weight_norm` is written:\n",
        "\n",
        "```python\n",
        "def get_weight_norm(self, weight, lora_weight, scaling) -> torch.Tensor:\n",
        "    # calculate L2 norm of weight matrix, column-wise\n",
        "    weight = transpose(weight, self.fan_in_fan_out)\n",
        "    weight = weight + scaling * lora_weight\n",
        "    weight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)\n",
        "    return weight_norm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13JNsuMr_x7u"
      },
      "source": [
        "I'll walk through each line, starting with the base layers weight matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9QQMcyv_2Tz",
        "outputId": "b473b742-2715-4f33-d00b-259cf0805815"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([192, 576])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight = k_proj.base_layer.weight\n",
        "weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75yp2v8-AAqg"
      },
      "source": [
        "We then pass the weight and `fan_in_fan_out` to `transpose`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFuRZ-gm_-cy",
        "outputId": "d762eab8-6ab5-4627-c458-26759b37999c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([192, 576])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight = transpose(weight, k_proj.fan_in_fan_out)\n",
        "weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4vzOPsnAItb"
      },
      "source": [
        "It doesn't transpose it! That's because [in `tranpose`](https://github.com/huggingface/peft/blob/0facdebf6208139cbd8f3586875acb378813dd97/src/peft/utils/other.py#L559) if `fan_in_fan_out` is `False` it returns the `weight` as is:\n",
        "\n",
        "```python\n",
        "def transpose(weight, fan_in_fan_out):\n",
        "    if not fan_in_fan_out:\n",
        "        return weight\n",
        "\n",
        "    if isinstance(weight, torch.nn.Parameter):\n",
        "        return torch.nn.Parameter(weight.T)\n",
        "    return weight.T\n",
        "```\n",
        "\n",
        "This is absolutely critical in determining the shape of the magnitude vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAzCu32_AIAO",
        "outputId": "769830c0-55ff-49cd-85bc-399aa864a4f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([192])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight_norm = torch.linalg.norm(weight, dim=1).to(weight.dtype)\n",
        "weight_norm.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtYFuEtZAj-3"
      },
      "source": [
        "Note two things here:\n",
        "- they take the `norm` across `dim=1` (the number of `in_features`)\n",
        "- we end up with 1 magnitude value for each of the 192 _output_ neurons. Contrast this with Raschka's article where we had 1 magnitude value for each of the _input_ neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS2ehFXZAvK-"
      },
      "source": [
        "## Aside: Claude Conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGqAT7SMAxZr"
      },
      "source": [
        "While I was re-reading the DoRA paper, I found the emphasis on _column-wise_ vector norm interesting so I asked Claude about it:\n",
        "\n",
        "\n",
        "> **Prompt**: Why columns? What is special about doing this for columns and not rows. Is there something fundamental about columns in a matrix of pretrained weights?\n",
        "\n",
        "> **Response**: For neural networks, columns of weight matrices map features from one layer's dimension to another's. Each column represents all the connections to a single output neuron. So columns form natural functional units, making column-wise normalization more meaningful than row-wise for understanding and modifying the network's learned transformations.\n",
        "\n",
        "> **Prompt**: So suppose we have an input vector `x` which contains values corresponding to each input variable, the pretrained weight matrix would is d x k so `x` would be 1 x d. Each row in pretrained weight matrix would represent one input variable. Multiplying 1 x d with d x k gives us 1 x k where k is the number of activations (i.e. the number of neurons).\n",
        "\n",
        "> **Response**: Yes, that's correct. The output for a single neuron (one column of the weight matrix) is the dot product between the input vector and that column's weights. So when we do column-wise normalization, we're normalizing the set of weights that feed into each individual output neuron.\n",
        "This helps explain why column-wise normalization is natural - each column represents a complete \"unit\" in terms of how the network processes information to produce one output activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTRi6FnvBqRG"
      },
      "source": [
        "So, it seems like decomposing the weight matrix into a magnitude vector and direction matrix _corresponding to output neurons_ seems really important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEILIydcB6Ao"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or1w_ZiPB7BA"
      },
      "source": [
        "Explorations like this is why I'm grateful for open source code. The efforts of folks like HuggingFace and Raschka to democratize LLMs allows folks like me to curate insightful explorations for myself, and I'm obviously happy to share them out in this blog post. If you find something incorrect in my interpretation of Raschka's or peft's code, please at me on Twitter @vishal_learner.\n",
        "\n",
        "A future exercise that I want to do: train DoRA implementations with a column-wise norm and a row-wise norm and compare results. This would truly highlight whether it's important to align the decomposition with the output neurons of the weight matrix."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
