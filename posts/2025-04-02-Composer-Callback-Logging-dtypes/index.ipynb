{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Logging Data Types for Activations, Gradients, Weights, Optimizer States and Loss during Training with LLM-Foundry\n",
        "date: \"2025-04-02\"\n",
        "author: Vishal Bakshi\n",
        "description: I write a custom Composer callback (with lots of Claude's help!) to log data types of different entities during mixed precision LoRA fine-tuning. When the model is in fp32, all entities except activations are in fp32 (activations are in bf16).\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "    - LLM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a [previous blog post](https://vishalbakshi.github.io/blog/posts/2025-03-30-Composer-Callback/) I shared my first couple of iterations of custom Composer callback used to log data types of different entities (activations, gradients, weights, optimizer states, and loss) during training with LLM-Foundry. In this blog post I'll share my final callback iteration's code, some lessons I learned along the way (i.e. LLaMA's self-attention module doesn't have positional arguments!) and analyze the logging results to observe entity data types throughout the training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDzA45UL1QDO"
      },
      "source": [
        "## Composer Callback Walkthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tcTJWH_IwlM"
      },
      "source": [
        "The data types of entities (activations, gradients, weights, loss, and optimizer states) are logged during training with a custom Composer callback `DtypeLogger` passed to the Composer `Trainer`. This callback was built up and tested event-by-event using Claude. There is one event handler in the callback for each Composer event from `<FIT_START>` to `<BATCH_END>`:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8em2yePooqfr"
      },
      "source": [
        "```\n",
        "# <INIT>\n",
        "# <BEFORE_LOAD>\n",
        "# <AFTER_LOAD>\n",
        "# <FIT_START>\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # <EPOCH_START>\n",
        "    while True:\n",
        "        # <BEFORE_DATALOADER>\n",
        "        batch = next(dataloader)\n",
        "        if batch is None:\n",
        "            break\n",
        "        inputs, targets = batch\n",
        "        # <AFTER_DATALOADER>\n",
        "\n",
        "        # <BATCH_START>\n",
        "\n",
        "        # <BEFORE_FORWARD>\n",
        "        outputs = model.forward(inputs)\n",
        "        # <AFTER_FORWARD>\n",
        "\n",
        "        # <BEFORE_LOSS>\n",
        "        loss = model.loss(outputs, targets)\n",
        "        # <AFTER_LOSS>\n",
        "\n",
        "        # <BEFORE_BACKWARD>\n",
        "        loss.backward()\n",
        "        # <AFTER_BACKWARD>\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # <BATCH_END>\n",
        "    # <EPOCH_END>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SMpl3MRi7U4"
      },
      "source": [
        "There are four explicit logging functions:\n",
        "\n",
        "- `_log_model_weight_dtypes`\n",
        "- `_log_gradient_dtypes`\n",
        "- `_log_optimizer_state_dtypes`\n",
        "- `_log_loss_dtype`\n",
        "\n",
        "Additionally, activations are logged using `register_forward_hook` for all modules except self-attention (more on that below). Self-attention inputs are logged using a monkey-patched forward pass.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm4Tqodl1R1m"
      },
      "source": [
        "```python\n",
        "class DtypeLogger(Callback):\n",
        "    def __init__(self, save_path=\"/model-checkpoints/dtype_tracking\", log_interval=10):\n",
        "        self.save_path = Path(save_path)\n",
        "        self.dtype_logs = {'log': {}}\n",
        "        self.log_interval = log_interval\n",
        "        self.hooks = []\n",
        "        \n",
        "    def fit_start(self, state: State, logger: Logger) -> None:\n",
        "        self._log_model_weight_dtypes(state, \"fit_start\")\n",
        "        self._save_logs()\n",
        "        \n",
        "    def epoch_start(self, state: State, logger: Logger) -> None:\n",
        "        self._log_model_weight_dtypes(state, \"epoch_start\")\n",
        "        self._save_logs()\n",
        "    \n",
        "    def before_dataloader(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"before_dataloader\")\n",
        "            self._save_logs()\n",
        "            \n",
        "    def after_dataloader(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"after_dataloader\")\n",
        "            self._save_logs()\n",
        "            \n",
        "    def batch_start(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"batch_start\")\n",
        "            self._save_logs()\n",
        "            \n",
        "    def before_forward(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"before_forward\")\n",
        "            \n",
        "            # Clear old hooks\n",
        "            for hook in self.hooks:\n",
        "                hook.remove()\n",
        "            self.hooks = []\n",
        "            \n",
        "            # Get the model\n",
        "            model = state.model.model.base_model.model\n",
        "            transformer_model = model.model  # This is the transformer part\n",
        "            batch_id = state.timestamp.batch.value\n",
        "            \n",
        "            # Store original forward methods to restore later\n",
        "            self.original_forward_methods = {}\n",
        "            \n",
        "            def hook_fn(layer_name, module_name):\n",
        "                def _hook(module, inputs, outputs):\n",
        "                    # Log input activation dtype\n",
        "                    if isinstance(inputs, tuple) and len(inputs) > 0:\n",
        "                        self.dtype_logs[\"log\"][f\"forward:{module_name}:{layer_name}:activation_input\"] = str(inputs[0].dtype)\n",
        "                    \n",
        "                    # Log output activation dtype\n",
        "                    if isinstance(outputs, torch.Tensor):\n",
        "                        self.dtype_logs[\"log\"][f\"forward:{module_name}:{layer_name}:activation_output\"] = str(outputs.dtype)\n",
        "                    elif isinstance(outputs, tuple) and len(outputs) > 0:\n",
        "                        self.dtype_logs[\"log\"][f\"forward:{module_name}:{layer_name}:activation_output\"] = str(outputs[0].dtype)\n",
        "                return _hook\n",
        "            \n",
        "            # Monkey patch self-attention modules\n",
        "            for layer_idx, layer in enumerate(transformer_model.layers):\n",
        "                # Store the original forward method\n",
        "                original_forward = layer.self_attn.forward\n",
        "                self.original_forward_methods[layer_idx] = original_forward\n",
        "                \n",
        "                # Define a closure to capture the current layer_idx\n",
        "                def make_patched_forward(layer_idx, orig_forward):\n",
        "                    def patched_forward(self_attn, *args, **kwargs):\n",
        "                        # Log the hidden_states dtype\n",
        "                        if 'hidden_states' in kwargs and hasattr(kwargs['hidden_states'], 'dtype'):\n",
        "                            self.dtype_logs[\"log\"][f\"forward:self_attn:layer_{layer_idx}:activation_input\"] = str(kwargs['hidden_states'].dtype)\n",
        "                        \n",
        "                        # Call the original method as a bound method\n",
        "                        # This ensures 'self_attn' is correctly passed as 'self'\n",
        "                        return orig_forward.__get__(self_attn, type(self_attn))(**kwargs)\n",
        "                    \n",
        "                    return patched_forward\n",
        "                \n",
        "                # Replace the forward method\n",
        "                layer.self_attn.forward = make_patched_forward(layer_idx, original_forward).__get__(layer.self_attn, type(layer.self_attn))\n",
        "            \n",
        "            # Register hook for lm_head\n",
        "            if hasattr(model, 'lm_head'):\n",
        "                self.hooks.append(model.lm_head.register_forward_hook(hook_fn(\"output\", \"lm_head\")))\n",
        "            \n",
        "            # Register hook for embedding layer\n",
        "            self.hooks.append(transformer_model.embed_tokens.register_forward_hook(hook_fn(\"embeddings\", \"embed_tokens\")))\n",
        "            \n",
        "            # Register hooks for each transformer layer\n",
        "            for layer_idx, layer in enumerate(transformer_model.layers):\n",
        "                # Self-attention components - we still register hooks for outputs\n",
        "                self.hooks.append(layer.self_attn.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"self_attn\")))\n",
        "                self.hooks.append(layer.self_attn.q_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"q_proj\")))\n",
        "                self.hooks.append(layer.self_attn.k_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"k_proj\")))\n",
        "                self.hooks.append(layer.self_attn.v_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"v_proj\")))\n",
        "                self.hooks.append(layer.self_attn.o_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"o_proj\")))\n",
        "                \n",
        "                # MLP components\n",
        "                self.hooks.append(layer.mlp.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"mlp\")))\n",
        "                self.hooks.append(layer.mlp.gate_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"gate_proj\")))\n",
        "                self.hooks.append(layer.mlp.up_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"up_proj\")))\n",
        "                self.hooks.append(layer.mlp.down_proj.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"down_proj\")))\n",
        "                \n",
        "                # Layer norms\n",
        "                self.hooks.append(layer.input_layernorm.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"input_layernorm\")))\n",
        "                self.hooks.append(layer.post_attention_layernorm.register_forward_hook(hook_fn(f\"layer_{layer_idx}\", \"post_attention_layernorm\")))\n",
        "            \n",
        "            # Final layer norm\n",
        "            self.hooks.append(transformer_model.norm.register_forward_hook(hook_fn(\"final\", \"norm\")))\n",
        "            \n",
        "            self._save_logs()\n",
        "            \n",
        "    def after_forward(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"after_forward\")\n",
        "            \n",
        "            # Restore original forward methods\n",
        "            if hasattr(self, 'original_forward_methods'):\n",
        "                model = state.model.model.base_model.model\n",
        "                transformer_model = model.model\n",
        "                \n",
        "                for layer_idx, original_forward in self.original_forward_methods.items():\n",
        "                    transformer_model.layers[layer_idx].self_attn.forward = original_forward\n",
        "                \n",
        "                self.original_forward_methods = {}\n",
        "            \n",
        "            # Clear hooks\n",
        "            for hook in self.hooks:\n",
        "                hook.remove()\n",
        "            self.hooks = []\n",
        "            \n",
        "            self._save_logs()\n",
        "            \n",
        "    def before_loss(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"before_loss\")\n",
        "            self._save_logs()\n",
        "            \n",
        "    def after_loss(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"after_loss\")\n",
        "            self._log_loss_dtype(state, \"after_loss\")\n",
        "            self._save_logs()\n",
        "            \n",
        "    def before_backward(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            self._log_model_weight_dtypes(state, \"before_backward\")\n",
        "            self._save_logs()\n",
        "            \n",
        "    def after_backward(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            # Log gradient dtypes as before\n",
        "            self._log_gradient_dtypes(state, \"after_backward\")\n",
        "            \n",
        "            # Track weight dtypes before optimizer step\n",
        "            self._log_model_weight_dtypes(state, \"before_optim_step\")\n",
        "            \n",
        "            # Log optimizer state dtypes\n",
        "            self._log_optimizer_state_dtypes(state, \"optimizer_step\")\n",
        "            \n",
        "            self._save_logs()\n",
        "                    \n",
        "    def batch_end(self, state: State, logger: Logger) -> None:\n",
        "        if state.timestamp.batch.value % self.log_interval == 0:\n",
        "            # Track weight dtypes after optimizer step to detect precision changes\n",
        "            self._log_model_weight_dtypes(state, \"after_optim_step\")\n",
        "            self._save_logs()\n",
        "\n",
        "    def epoch_end(self, state: State, logger: Logger) -> None:\n",
        "        self._log_model_weight_dtypes(state, \"epoch_end\")\n",
        "        self._save_logs()\n",
        "        \n",
        "    def _log_model_weight_dtypes(self, state: State, event_name: str) -> None:\n",
        "        model = state.model\n",
        "        for name, param in model.named_parameters():\n",
        "            name = name.removeprefix(\"model.base_model.model.model.\")\n",
        "            self.dtype_logs[\"log\"][f\"{event_name}:{name}:weights\"] = str(param.dtype)\n",
        "\n",
        "    def _log_gradient_dtypes(self, state: State, event_name: str) -> None:\n",
        "        model = state.model\n",
        "        for name, param in model.named_parameters():\n",
        "            name = name.removeprefix(\"model.base_model.model.model.\")\n",
        "            if param.grad is not None: self.dtype_logs['log'][f\"{event_name}:{name}:gradients\"] = str(param.grad.dtype)\n",
        "            else: self.dtype_logs['log'][f\"{event_name}:{name}:gradients\"] = \"None\"\n",
        "    \n",
        "    def _log_loss_dtype(self, state: State, event_name: str) -> None:\n",
        "        if hasattr(state, 'loss') and hasattr(state.loss, 'dtype'):\n",
        "            self.dtype_logs[\"log\"][f\"{event_name}:loss\"] = str(state.loss.dtype)\n",
        "            \n",
        "    def _log_optimizer_state_dtypes(self, state: State, event_name: str) -> None:\n",
        "        if hasattr(state, 'optimizers') and state.optimizers is not None:\n",
        "            # Handle single optimizer or list of optimizers\n",
        "            optimizers = state.optimizers if isinstance(state.optimizers, list) else [state.optimizers]\n",
        "            \n",
        "            for opt_idx, optimizer in enumerate(optimizers):\n",
        "                # Get optimizer state dict\n",
        "                opt_state = optimizer.state_dict()\n",
        "                \n",
        "                # Check if 'state' exists in the optimizer state dict\n",
        "                if 'state' in opt_state:\n",
        "                    for param_id, param_state in opt_state['state'].items():\n",
        "                        for state_name, state_value in param_state.items():\n",
        "                            if isinstance(state_value, torch.Tensor):\n",
        "                                # Store dtype of optimizer state tensors (momentum buffers, etc.)\n",
        "                                key = f\"optimizer_{opt_idx}_param_{param_id}_{state_name}\"\n",
        "                                self.dtype_logs[\"log\"][f\"{event_name}:{key}:optimizer_states\"] = str(state_value.dtype)\n",
        "            \n",
        "    def _save_logs(self) -> None:\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "        log_file = self.save_path / \"dtype_logs.json\"\n",
        "        with open(log_file, 'w') as f:\n",
        "            json.dump(self.dtype_logs, f, indent=2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAtMBdmRIyZZ"
      },
      "source": [
        "The most involved event handler is `before_forward` which involves creating a hook function (`hook_fn`) passed to PyTorch's [`register_forward_hook`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) which exposes the positional inputs and outputs of a module's `forward` pass. The hook function modifies `self.dtype_logs` directly by storing the data type string of inputs and outputs. `hook_fn` is used for all modules except self attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9-uro39I3Mg"
      },
      "source": [
        "Self attention [cannot utilize `register_forward_hook`](https://github.com/huggingface/transformers/issues/29247#issuecomment-1965894085) because the [LlamaDecoderLayer](https://github.com/huggingface/transformers/blob/bf41e54fc8242dafa31bf6203e3d505bcb907119/src/transformers/models/llama/modeling_llama.py#L345) does not call self attention forward pass with any positional arguments:\n",
        "\n",
        "```python\n",
        "hidden_states, self_attn_weights = self.self_attn(\n",
        "    hidden_states=hidden_states,\n",
        "    attention_mask=attention_mask,\n",
        "    position_ids=position_ids,\n",
        "    past_key_value=past_key_value,\n",
        "    output_attentions=output_attentions,\n",
        "    use_cache=use_cache,\n",
        "    cache_position=cache_position,\n",
        "    position_embeddings=position_embeddings,\n",
        "    **kwargs,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quW9Z5yVI44x"
      },
      "source": [
        "Contrast this with how the forward pass of other modules are called with positional arguments only:\n",
        "\n",
        "```python\n",
        "# self attention sublayers\n",
        "query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "attn_output = self.o_proj(attn_output)\n",
        "\n",
        "# mlp sublayers\n",
        "down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "# non-self attention modules\n",
        "hidden_states = self.input_layernorm(hidden_states)\n",
        "hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "hidden_states = self.mlp(hidden_states)\n",
        "hidden_states = self.norm(hidden_states)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9QAZtaZI_y8"
      },
      "source": [
        "Since self-attention inputs can't be captured by a hook I had to monkey patch its forward pass to log its inputs' data type:\n",
        "\n",
        "```python\n",
        "for layer_idx, layer in enumerate(transformer_model.layers):\n",
        "    # Store the original forward method\n",
        "    original_forward = layer.self_attn.forward\n",
        "    self.original_forward_methods[layer_idx] = original_forward\n",
        "    \n",
        "    # Define a closure to capture the current layer_idx\n",
        "    def make_patched_forward(layer_idx, orig_forward):\n",
        "        def patched_forward(self_attn, *args, **kwargs):\n",
        "            # Log the hidden_states dtype\n",
        "            if 'hidden_states' in kwargs and hasattr(kwargs['hidden_states'], 'dtype'):\n",
        "                self.dtype_logs[\"log\"][f\"forward:self_attn:layer_{layer_idx}:activation_input\"] = str(kwargs['hidden_states'].dtype)\n",
        "            \n",
        "            # Call the original method as a bound method\n",
        "            # This ensures 'self_attn' is correctly passed as 'self'\n",
        "            return orig_forward.__get__(self_attn, type(self_attn))(**kwargs)\n",
        "        \n",
        "        return patched_forward\n",
        "\n",
        "    # Replace the forward method\n",
        "    layer.self_attn.forward = make_patched_forward(layer_idx, original_forward).__get__(layer.self_attn, type(layer.self_attn))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ688eOv3Jx5"
      },
      "source": [
        "\n",
        "`patched_forward` receives positional arguments `*args` (of which there are none) and keyword arguments `**kwargs` (all of the arguments to the self-attention forward) and logs the data types of the inputs to self-attention (`hidden_states`) as `self_attn_input` before returning the outputs of the original forward pass.\n",
        "\n",
        "A key line is `orig_forward.__get__(self_attn, type(self_attn))(**kwargs)`. As Claude's comment mentions, this is to avoid using `orig_forward(self_attn, **kwargs)` which was causing the following error because the first argument, `self_attn`, was being interpreted as `hidden_states` whereas it was intended to represent `self`:\n",
        "\n",
        "```\n",
        "TypeError: LlamaFlashAttention2.forward() got multiple values for argument 'hidden_states'\n",
        "```\n",
        "\n",
        "In short, when you call `__get__(obj, type)` on a function it will bind that function as a method to the given object, thus no longer requiring you to pass in `self` as an argument. This is critical because `self_attn.forward` _has no positional arguments_. We can then pass in the keyword arguments to the bound method `orig_forward.__get__(self_attn, type(self_attn))(**kwargs)`, and let the model continue using self-attention correctly. See the [Descriptor Guide in the Python docs](https://docs.python.org/3/howto/descriptor.html#functions-and-methods:~:text=To%20recap%2C%20functions%20have%20a%20__get__()%20method%20so%20that%20they%20can%20be%20converted%20to%20a%20method%20when%20accessed%20as%20attributes.%20The%20non%2Ddata%20descriptor%20transforms%20an%20obj.f(*args)%20call%20into%20f(obj%2C%20*args).%20Calling%20cls.f(*args)%20becomes%20f(*args).) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T89wTmFeWx9Y"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DtzseNn4hdki"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DIN4V390Xkbl"
      },
      "outputs": [],
      "source": [
        "def parse_index(string):\n",
        "    \"\"\"Extract structured information from parameter names\"\"\"\n",
        "    info = {\n",
        "        'layer_number': None,\n",
        "        'module': None,\n",
        "        'layer_name': None,\n",
        "        'lora_layer': None,\n",
        "        'training_step': None,\n",
        "        'entity': None\n",
        "    }\n",
        "\n",
        "    # layer = string.split(\":\")[1]\n",
        "    # info[\"layer\"] = layer\n",
        "\n",
        "    layer_number_match = re.search(r'layers\\.(\\d+)', string)\n",
        "    if layer_number_match: info['layer_number'] = int(layer_number_match.group(1))\n",
        "\n",
        "    modules = [\n",
        "        \"embed_tokens\",\n",
        "        \"input_layernorm\",\n",
        "        \"self_attn\",\n",
        "        \"post_attention_layernorm\",\n",
        "        \"mlp\",\n",
        "        \"norm\",\n",
        "        \"lm_head\"\n",
        "    ]\n",
        "\n",
        "    module_match = re.search(r'(mlp|self_attn|input_layernorm|post_attention_layernorm|embed_tokens|norm|lm_head)', string)\n",
        "    if module_match: info['module'] = str(modules.index(module_match.group(1))).zfill(2) + '_' + module_match.group(1)\n",
        "\n",
        "    layer_name_match = re.search(r'(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)', string)\n",
        "    if layer_name_match: info['layer_name'] = layer_name_match.group(1)\n",
        "\n",
        "    lora_match = re.search(r'(base_layer|lora_A|lora_B)', string)\n",
        "    if lora_match: info['lora_layer'] = lora_match.group(1)\n",
        "    else: info['lora_layer'] = \"Not a LoRA Layer\"\n",
        "\n",
        "    training_steps = [\n",
        "        \"fit_start\",\n",
        "        \"epoch_start\",\n",
        "        \"before_dataloader\",\n",
        "        \"after_dataloader\",\n",
        "        \"batch_start\",\n",
        "        \"before_forward\",\n",
        "        \"forward\",\n",
        "        \"after_forward\",\n",
        "        \"before_loss\",\n",
        "        \"after_loss\",\n",
        "        \"before_backward\",\n",
        "        \"after_backward\",\n",
        "        \"before_optim_step\",\n",
        "        \"optimizer_step\",\n",
        "        \"after_optim_step\"\n",
        "        ]\n",
        "\n",
        "    training_step = string.split(\":\")[0]\n",
        "    info['training_step'] = str(training_steps.index(training_step)).zfill(2) + '_' + training_step\n",
        "\n",
        "    info['entity'] = string.split(\":\")[-1]\n",
        "\n",
        "\n",
        "    return info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UbzyeMuj4jEd"
      },
      "outputs": [],
      "source": [
        "def _df(url):\n",
        "    dtype_data = json.loads(requests.get(url).text)\n",
        "\n",
        "    df = pd.DataFrame(dtype_data).reset_index()\n",
        "    df = df.rename(columns={\"index\": \"index\", \"log\": \"dtype\"})\n",
        "\n",
        "    parsed_info = df['index'].apply(lambda x: parse_index(x))\n",
        "\n",
        "    df['layer_number'] = parsed_info.apply(lambda x: x['layer_number'])\n",
        "    df['module'] = parsed_info.apply(lambda x: x['module'])\n",
        "    df['layer_name'] = parsed_info.apply(lambda x: x['layer_name'])\n",
        "    df['lora_layer'] = parsed_info.apply(lambda x: x['lora_layer'])\n",
        "    df['training_step'] = parsed_info.apply(lambda x: x['training_step'])\n",
        "    df['entity'] = parsed_info.apply(lambda x: x['entity'])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP5Wj-KsIs2Y"
      },
      "source": [
        "## Model in fp32 (`master_weights_dtype==None`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTikvdPpSDUQ"
      },
      "source": [
        "In this case, `master_weights_dtype` is not provided in the training YAML file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0A-ADZNnIjRK",
        "outputId": "73fb1c7d-f091-413b-90af-cf9fb0c6eb51"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10923,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10923,\n        \"samples\": [\n          \"after_dataloader:layers.8.self_attn.o_proj.lora_A.default.weight:weights\",\n          \"before_optim_step:layers.18.self_attn.k_proj.base_layer.weight:weights\",\n          \"before_forward:layers.12.mlp.up_proj.lora_A.default.weight:weights\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dtype\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"torch.int64\",\n          \"None\",\n          \"torch.float32\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"layer_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.655923954821418,\n        \"min\": 0.0,\n        \"max\": 29.0,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          27.0,\n          15.0,\n          23.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"module\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"00_embed_tokens\",\n          \"02_self_attn\",\n          \"05_norm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"layer_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"q_proj\",\n          \"k_proj\",\n          \"up_proj\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lora_layer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"base_layer\",\n          \"lora_B\",\n          \"Not a LoRA Layer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_step\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"09_after_loss\",\n          \"11_after_backward\",\n          \"00_fit_start\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"weights\",\n          \"activation_input\",\n          \"optimizer_states\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f6bb70e8-9110-47f0-85d1-46d907c311a6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>dtype</th>\n",
              "      <th>layer_number</th>\n",
              "      <th>module</th>\n",
              "      <th>layer_name</th>\n",
              "      <th>lora_layer</th>\n",
              "      <th>training_step</th>\n",
              "      <th>entity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fit_start:embed_tokens.weight:weights</td>\n",
              "      <td>torch.float32</td>\n",
              "      <td>NaN</td>\n",
              "      <td>00_embed_tokens</td>\n",
              "      <td>None</td>\n",
              "      <td>Not a LoRA Layer</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fit_start:layers.0.self_attn.q_proj.base_layer...</td>\n",
              "      <td>torch.float32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>q_proj</td>\n",
              "      <td>base_layer</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fit_start:layers.0.self_attn.q_proj.lora_A.def...</td>\n",
              "      <td>torch.float32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>q_proj</td>\n",
              "      <td>lora_A</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fit_start:layers.0.self_attn.q_proj.lora_B.def...</td>\n",
              "      <td>torch.float32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>q_proj</td>\n",
              "      <td>lora_B</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fit_start:layers.0.self_attn.k_proj.base_layer...</td>\n",
              "      <td>torch.float32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>k_proj</td>\n",
              "      <td>base_layer</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6bb70e8-9110-47f0-85d1-46d907c311a6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f6bb70e8-9110-47f0-85d1-46d907c311a6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f6bb70e8-9110-47f0-85d1-46d907c311a6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b878e7a4-6a3d-484e-8565-4873e2d61a8a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b878e7a4-6a3d-484e-8565-4873e2d61a8a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b878e7a4-6a3d-484e-8565-4873e2d61a8a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               index          dtype  \\\n",
              "0              fit_start:embed_tokens.weight:weights  torch.float32   \n",
              "1  fit_start:layers.0.self_attn.q_proj.base_layer...  torch.float32   \n",
              "2  fit_start:layers.0.self_attn.q_proj.lora_A.def...  torch.float32   \n",
              "3  fit_start:layers.0.self_attn.q_proj.lora_B.def...  torch.float32   \n",
              "4  fit_start:layers.0.self_attn.k_proj.base_layer...  torch.float32   \n",
              "\n",
              "   layer_number           module layer_name        lora_layer training_step  \\\n",
              "0           NaN  00_embed_tokens       None  Not a LoRA Layer  00_fit_start   \n",
              "1           0.0     02_self_attn     q_proj        base_layer  00_fit_start   \n",
              "2           0.0     02_self_attn     q_proj            lora_A  00_fit_start   \n",
              "3           0.0     02_self_attn     q_proj            lora_B  00_fit_start   \n",
              "4           0.0     02_self_attn     k_proj        base_layer  00_fit_start   \n",
              "\n",
              "    entity  \n",
              "0  weights  \n",
              "1  weights  \n",
              "2  weights  \n",
              "3  weights  \n",
              "4  weights  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = \"https://gist.githubusercontent.com/vishalbakshi/9ade8d501629d4c30e8aecfa1c6f67cf/raw/0c162e2305002fbe57fd2570ade302c3659140a1/dtypes_logs_1ba_fp32.json\"\n",
        "df = _df(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs4jhmXuO7JY"
      },
      "source": [
        "### Data Types by `lora_layer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S017vfmSIcM"
      },
      "source": [
        "All LoRA layer entities are in fp32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "c3HErY8LO8zQ",
        "outputId": "d3494bca-3fbd-4a42-ae1b-5087d3ef6618"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>dtype</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lora_layer</th>\n",
              "      <th>dtype</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Not a LoRA Layer</th>\n",
              "      <th>None</th>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>2339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.int64</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">base_layer</th>\n",
              "      <th>None</th>\n",
              "      <td>210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>2520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lora_A</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>2730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lora_B</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>2730</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lora_layer        dtype         \n",
              "Not a LoRA Layer  None                62\n",
              "                  torch.bfloat16     331\n",
              "                  torch.float32     2339\n",
              "                  torch.int64          1\n",
              "base_layer        None               210\n",
              "                  torch.float32     2520\n",
              "lora_A            torch.float32     2730\n",
              "lora_B            torch.float32     2730\n",
              "Name: dtype, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby(['lora_layer', 'dtype'])['dtype'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6oDiVpzhYu-"
      },
      "source": [
        "### Data Types by `entity` (Activations, Gradients, Loss, Optimizer States and Weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvywazAuSMhm"
      },
      "source": [
        "Every entity except activations are in fp32. Some parameters don't have gradients because we are training with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "4iMieCvpcUjv",
        "outputId": "78343b50-69b7-4e1e-f2eb-58f2a3fbc781"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>dtype</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entity</th>\n",
              "      <th>dtype</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">activation_input</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.int64</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">activation_output</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">gradients</th>\n",
              "      <th>None</th>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loss</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimizer_states</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>1260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>8304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "entity             dtype         \n",
              "activation_input   torch.bfloat16      60\n",
              "                   torch.float32      272\n",
              "                   torch.int64          1\n",
              "activation_output  torch.bfloat16     271\n",
              "                   torch.float32       62\n",
              "gradients          None               272\n",
              "                   torch.float32      420\n",
              "loss               torch.float32        1\n",
              "optimizer_states   torch.float32     1260\n",
              "weights            torch.float32     8304\n",
              "Name: dtype, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby(['entity', 'dtype'])['dtype'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzoTmCCQhio3"
      },
      "source": [
        "### Data Types by Composer Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "qrxZPiFvhCy0",
        "outputId": "98def9cf-d2c3-4003-a11c-1293b9879460"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>dtype</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training_step</th>\n",
              "      <th>entity</th>\n",
              "      <th>dtype</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>00_fit_start</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01_epoch_start</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02_before_dataloader</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03_after_dataloader</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>04_batch_start</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>05_before_forward</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">06_forward</th>\n",
              "      <th rowspan=\"3\" valign=\"top\">activation_input</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.int64</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">activation_output</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>07_after_forward</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>08_before_loss</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">09_after_loss</th>\n",
              "      <th>loss</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_before_backward</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">11_after_backward</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">gradients</th>\n",
              "      <th>None</th>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_before_optim_step</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_optimizer_step</th>\n",
              "      <th>optimizer_states</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>1260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_after_optim_step</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "training_step         entity             dtype         \n",
              "00_fit_start          weights            torch.float32      692\n",
              "01_epoch_start        weights            torch.float32      692\n",
              "02_before_dataloader  weights            torch.float32      692\n",
              "03_after_dataloader   weights            torch.float32      692\n",
              "04_batch_start        weights            torch.float32      692\n",
              "05_before_forward     weights            torch.float32      692\n",
              "06_forward            activation_input   torch.bfloat16      60\n",
              "                                         torch.float32      272\n",
              "                                         torch.int64          1\n",
              "                      activation_output  torch.bfloat16     271\n",
              "                                         torch.float32       62\n",
              "07_after_forward      weights            torch.float32      692\n",
              "08_before_loss        weights            torch.float32      692\n",
              "09_after_loss         loss               torch.float32        1\n",
              "                      weights            torch.float32      692\n",
              "10_before_backward    weights            torch.float32      692\n",
              "11_after_backward     gradients          None               272\n",
              "                                         torch.float32      420\n",
              "12_before_optim_step  weights            torch.float32      692\n",
              "13_optimizer_step     optimizer_states   torch.float32     1260\n",
              "14_after_optim_step   weights            torch.float32      692\n",
              "Name: dtype, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby(['training_step', 'entity', 'dtype'])['dtype'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bgYYU2_IwXR"
      },
      "source": [
        "## Model in bf16 (`master_weights_dtype==bfloat16`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL8-xb5SQd7-"
      },
      "source": [
        "I also logged data types after setting `master_weights_dtype` in the training YAML to `bfloat16`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PuWCNvsRnDTH",
        "outputId": "5b638922-40c0-42a9-9c0e-8b907d105abd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10923,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10923,\n        \"samples\": [\n          \"after_dataloader:layers.8.self_attn.o_proj.lora_A.default.weight:weights\",\n          \"before_optim_step:layers.18.self_attn.k_proj.base_layer.weight:weights\",\n          \"before_forward:layers.12.mlp.up_proj.lora_A.default.weight:weights\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dtype\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"torch.int64\",\n          \"None\",\n          \"torch.bfloat16\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"layer_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.655923954821418,\n        \"min\": 0.0,\n        \"max\": 29.0,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          27.0,\n          15.0,\n          23.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"module\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"00_embed_tokens\",\n          \"02_self_attn\",\n          \"05_norm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"layer_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"q_proj\",\n          \"k_proj\",\n          \"up_proj\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lora_layer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"base_layer\",\n          \"lora_B\",\n          \"Not a LoRA Layer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_step\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"09_after_loss\",\n          \"11_after_backward\",\n          \"00_fit_start\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"weights\",\n          \"activation_input\",\n          \"optimizer_states\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6f3e8532-20df-451d-8e93-caa52d279f3f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>dtype</th>\n",
              "      <th>layer_number</th>\n",
              "      <th>module</th>\n",
              "      <th>layer_name</th>\n",
              "      <th>lora_layer</th>\n",
              "      <th>training_step</th>\n",
              "      <th>entity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fit_start:embed_tokens.weight:weights</td>\n",
              "      <td>torch.bfloat16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>00_embed_tokens</td>\n",
              "      <td>None</td>\n",
              "      <td>Not a LoRA Layer</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fit_start:layers.0.self_attn.q_proj.base_layer...</td>\n",
              "      <td>torch.bfloat16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>q_proj</td>\n",
              "      <td>base_layer</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fit_start:layers.0.self_attn.q_proj.lora_A.def...</td>\n",
              "      <td>torch.bfloat16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>q_proj</td>\n",
              "      <td>lora_A</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fit_start:layers.0.self_attn.q_proj.lora_B.def...</td>\n",
              "      <td>torch.bfloat16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>q_proj</td>\n",
              "      <td>lora_B</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fit_start:layers.0.self_attn.k_proj.base_layer...</td>\n",
              "      <td>torch.bfloat16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>02_self_attn</td>\n",
              "      <td>k_proj</td>\n",
              "      <td>base_layer</td>\n",
              "      <td>00_fit_start</td>\n",
              "      <td>weights</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f3e8532-20df-451d-8e93-caa52d279f3f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6f3e8532-20df-451d-8e93-caa52d279f3f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6f3e8532-20df-451d-8e93-caa52d279f3f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-159646a6-3c88-49de-8770-5f4464ad1b49\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-159646a6-3c88-49de-8770-5f4464ad1b49')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-159646a6-3c88-49de-8770-5f4464ad1b49 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               index           dtype  \\\n",
              "0              fit_start:embed_tokens.weight:weights  torch.bfloat16   \n",
              "1  fit_start:layers.0.self_attn.q_proj.base_layer...  torch.bfloat16   \n",
              "2  fit_start:layers.0.self_attn.q_proj.lora_A.def...  torch.bfloat16   \n",
              "3  fit_start:layers.0.self_attn.q_proj.lora_B.def...  torch.bfloat16   \n",
              "4  fit_start:layers.0.self_attn.k_proj.base_layer...  torch.bfloat16   \n",
              "\n",
              "   layer_number           module layer_name        lora_layer training_step  \\\n",
              "0           NaN  00_embed_tokens       None  Not a LoRA Layer  00_fit_start   \n",
              "1           0.0     02_self_attn     q_proj        base_layer  00_fit_start   \n",
              "2           0.0     02_self_attn     q_proj            lora_A  00_fit_start   \n",
              "3           0.0     02_self_attn     q_proj            lora_B  00_fit_start   \n",
              "4           0.0     02_self_attn     k_proj        base_layer  00_fit_start   \n",
              "\n",
              "    entity  \n",
              "0  weights  \n",
              "1  weights  \n",
              "2  weights  \n",
              "3  weights  \n",
              "4  weights  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = \"https://gist.githubusercontent.com/vishalbakshi/ec91a59754633611fd8eb33b59031243/raw/5b83a7ebd5759cf6bd2db2369edf1c73e1fb67cf/dtypes_logs_1ba_bf16.json\"\n",
        "df = _df(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCPAHdZnOzFY"
      },
      "source": [
        "### Data Type by `lora_layer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4EGyqEBQlBf"
      },
      "source": [
        "Interestingly, setting `master_weights_dtype` makes all LoRA layers bfloat16 but some non-LoRA layers' entities are still in fp32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "8MDcr4iRO1bQ",
        "outputId": "b18ff791-dbf5-40a8-fcfe-402ed510d645"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>dtype</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lora_layer</th>\n",
              "      <th>dtype</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Not a LoRA Layer</th>\n",
              "      <th>None</th>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>2249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.int64</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">base_layer</th>\n",
              "      <th>None</th>\n",
              "      <td>210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>2520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lora_A</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>2730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lora_B</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>2730</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lora_layer        dtype         \n",
              "Not a LoRA Layer  None                62\n",
              "                  torch.bfloat16    2249\n",
              "                  torch.float32      421\n",
              "                  torch.int64          1\n",
              "base_layer        None               210\n",
              "                  torch.bfloat16    2520\n",
              "lora_A            torch.bfloat16    2730\n",
              "lora_B            torch.bfloat16    2730\n",
              "Name: dtype, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby(['lora_layer', 'dtype'])['dtype'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qt3XRgTI4DE"
      },
      "source": [
        "### Data Types by `entity` (Activations, Gradients, Loss, Optimizer States and Weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXnCzDwNQuFl"
      },
      "source": [
        "All floating point values are in bfloat16 except for the loss and some of the optimizer states. I'm not sure why some optimizer states are in bf16, even though it says in the [Composer docs](https://docs.mosaicml.com/projects/composer/en/latest/notes/numerics.html#automatic-mixed-precision-amp-training):\n",
        "\n",
        "> Store the weights and perform the optimizer step in single precision, enabling the weight update to be done more precisely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "8-PEOLy4I4UE",
        "outputId": "17283b63-7034-448d-f2a6-b51857b9d320"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>dtype</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entity</th>\n",
              "      <th>dtype</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">activation_input</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.int64</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>activation_output</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">gradients</th>\n",
              "      <th>None</th>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loss</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">optimizer_states</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>8304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "entity             dtype         \n",
              "activation_input   torch.bfloat16     332\n",
              "                   torch.int64          1\n",
              "activation_output  torch.bfloat16     333\n",
              "gradients          None               272\n",
              "                   torch.bfloat16     420\n",
              "loss               torch.float32        1\n",
              "optimizer_states   torch.bfloat16     840\n",
              "                   torch.float32      420\n",
              "weights            torch.bfloat16    8304\n",
              "Name: dtype, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby(['entity', 'dtype'])['dtype'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZAnkrGYI9UA"
      },
      "source": [
        "### Data Type by Composer Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "4KtjBw8PJA_U",
        "outputId": "168d15bc-1303-40c0-cc01-7a529ed5cb5d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>dtype</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training_step</th>\n",
              "      <th>entity</th>\n",
              "      <th>dtype</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>00_fit_start</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01_epoch_start</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02_before_dataloader</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03_after_dataloader</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>04_batch_start</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>05_before_forward</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">06_forward</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">activation_input</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.int64</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>activation_output</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>07_after_forward</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>08_before_loss</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">09_after_loss</th>\n",
              "      <th>loss</th>\n",
              "      <th>torch.float32</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_before_backward</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">11_after_backward</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">gradients</th>\n",
              "      <th>None</th>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_before_optim_step</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">13_optimizer_step</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">optimizer_states</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torch.float32</th>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_after_optim_step</th>\n",
              "      <th>weights</th>\n",
              "      <th>torch.bfloat16</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "training_step         entity             dtype         \n",
              "00_fit_start          weights            torch.bfloat16    692\n",
              "01_epoch_start        weights            torch.bfloat16    692\n",
              "02_before_dataloader  weights            torch.bfloat16    692\n",
              "03_after_dataloader   weights            torch.bfloat16    692\n",
              "04_batch_start        weights            torch.bfloat16    692\n",
              "05_before_forward     weights            torch.bfloat16    692\n",
              "06_forward            activation_input   torch.bfloat16    332\n",
              "                                         torch.int64         1\n",
              "                      activation_output  torch.bfloat16    333\n",
              "07_after_forward      weights            torch.bfloat16    692\n",
              "08_before_loss        weights            torch.bfloat16    692\n",
              "09_after_loss         loss               torch.float32       1\n",
              "                      weights            torch.bfloat16    692\n",
              "10_before_backward    weights            torch.bfloat16    692\n",
              "11_after_backward     gradients          None              272\n",
              "                                         torch.bfloat16    420\n",
              "12_before_optim_step  weights            torch.bfloat16    692\n",
              "13_optimizer_step     optimizer_states   torch.bfloat16    840\n",
              "                                         torch.float32     420\n",
              "14_after_optim_step   weights            torch.bfloat16    692\n",
              "Name: dtype, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby(['training_step', 'entity', 'dtype'])['dtype'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I absolutely loved this exercise. I learned a ton about callbacks, data types during mixed precision training, and Python fundamentals. Working with LLM-Foundry has opened up a whole universe of learning opportunities as I try to better understand what's going on under the hood. It's a gift that keeps giving! \n",
        "\n",
        "I'm trying to grow [my YouTube channel](https://www.youtube.com/@vishal_learner) so please give it a visit and subscribe if you want to stay in the loop."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
