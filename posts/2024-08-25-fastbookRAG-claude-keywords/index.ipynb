{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Generating Full Text Search Keywords using `claudette`\n",
        "date: \"2024-08-25\"\n",
        "author: Vishal Bakshi\n",
        "description: In this blog post, I use Answer.AI's `claudette` library to interface with the Claude-3.5 Sonnet API.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - RAG\n",
        "    - information retrieval\n",
        "    - LLM\n",
        "    - fastbookRAG\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWcYp_GNpJQ0"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptHFgXHapKXS"
      },
      "source": [
        "In this notebook, I'll use `claudette` to generate keywords from fastbook Questionnaire questions, which will then be used for SQLite full-text keyword search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C45REd_BOh2h"
      },
      "source": [
        "This notebook is part [a series of blog posts](https://vishalbakshi.github.io/blog/index.html#category=fastbookRAG) for a project I'm working on called **fastbookRAG** in which I'm building a hybrid search + LLM pipeline to answer questions from the end-of-chapter Questionnaires in the freely available [fastai textbook](https://github.com/fastai/fastbook/tree/master)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awrol8_jhyYR"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show imports\"\n",
        "!pip install claudette\n",
        "from claudette import *\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdoofmLBoLuG",
        "outputId": "76325170-2bc4-4b15-dc90-9521abbc6a28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('claude-3-opus-20240229',\n",
              " 'claude-3-5-sonnet-20240620',\n",
              " 'claude-3-haiku-20240307')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models # available in claudette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KBVTTgdO6_P"
      },
      "source": [
        "I'll be using the Claude-3.5 Sonnet API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KTpgth5poMFm",
        "outputId": "546ba965-88f9-4965-8cb3-c5d3ac60d7f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'claude-3-5-sonnet-20240620'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models[1]\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMAOVwDdpq3D"
      },
      "source": [
        "## Testing out the Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InBiRx9cp2Yk"
      },
      "source": [
        "I have already created keywords for the Chapter 1 Questionnaire questions, so I'll use a few of them as examples in my prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lngCNon9owqg",
        "outputId": "f97aa340-b4bb-442b-9bb8-9a9bc8a70ebe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "In: 0; Out: 0; Total: 0"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "chat.use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rk5qxcOho9O5"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show prompt\"\n",
        "prompt = \"\"\"I am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\n",
        "Do not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\n",
        "\n",
        "No yapping.\n",
        "\n",
        "Examples:\n",
        "\n",
        "question_text: Name five areas where deep learning is now the best in the world\n",
        "keywords: \"deep learning, state of the art, best, world\"\n",
        "\n",
        "question_text: Why is it hard to use a traditional computer program to recognize images in a photo?\n",
        "keywords: \"image, recognize, recognition, traditional, computer, program\"\n",
        "\n",
        "question_text: What were the two theoretical misunderstandings that held back the field of neural networks?\n",
        "keywords: \"theoretical, misunderstandings, held, back, field, neural network\"\n",
        "\n",
        "question_text: {question_text}\n",
        "keywords:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShVMGhoSq6vk",
        "outputId": "70c690e0-a525-4828-d72a-0e94f87d6135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\n",
            "Do not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\n",
            "\n",
            "No yapping. \n",
            "\n",
            "Examples:\n",
            "\n",
            "question_text: Name five areas where deep learning is now the best in the world\n",
            "keywords: \"deep learning, state of the art, best, world\"\n",
            "\n",
            "question_text: Why is it hard to use a traditional computer program to recognize images in a photo?\n",
            "keywords: \"image, recognize, recognition, traditional, computer, program\"\n",
            "\n",
            "question_text: What were the two theoretical misunderstandings that held back the field of neural networks?\n",
            "keywords: \"theoretical, misunderstandings, held, back, field, neural network\"\n",
            "\n",
            "question_text: Why is it hard to understand why a deep learning model makes a particular prediction?\n",
            "keywords:\n"
          ]
        }
      ],
      "source": [
        "formatted_prompt = prompt.format(question_text=\"Why is it hard to understand why a deep learning model makes a particular prediction?\")\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "hWSi54ZHq9GP",
        "outputId": "7fa3758d-a871-429c-c456-00e2e92ea35a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\"deep learning, prediction, understanding, model, interpretability\"\n",
              "\n",
              "<details>\n",
              "\n",
              "- id: msg_01GH67UgWfn8yTmufJt83Dyh\n",
              "- content: [{'text': '\"deep learning, prediction, understanding, model, interpretability\"', 'type': 'text'}]\n",
              "- model: claude-3-5-sonnet-20240620\n",
              "- role: assistant\n",
              "- stop_reason: end_turn\n",
              "- stop_sequence: None\n",
              "- type: message\n",
              "- usage: {'input_tokens': 229, 'output_tokens': 16}\n",
              "\n",
              "</details>"
            ],
            "text/plain": [
              "Message(id='msg_01GH67UgWfn8yTmufJt83Dyh', content=[TextBlock(text='\"deep learning, prediction, understanding, model, interpretability\"', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 229; Out: 16; Total: 245)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r = chat(formatted_prompt)\n",
        "r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zjyKEuflrsLN",
        "outputId": "50462667-9793-4b58-909a-4eef9232db73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"deep learning, prediction, understanding, model, interpretability\"'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F773ofoitKgq",
        "outputId": "0bc97b48-eb82-43c7-b4ea-83a84263dacf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "In: 229; Out: 16; Total: 245"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tam4OVEms8Uy"
      },
      "source": [
        "## Generating Keywords for One Chapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrKw5wuvs-sp"
      },
      "source": [
        "I'm always cautious when I use an API, as even with cheap per token costs, things can add up quickly. I'll first test out for one chapter's questions. A single question required a total of 245 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5AeOxQcrR08",
        "outputId": "50cd2c03-84c7-461b-9cf8-f6d687e92ac5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "In: 0; Out: 0; Total: 0"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "chat.use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1u4j6BQtdZc"
      },
      "source": [
        "The full set of questions (and \"gold standard\" answers, along with some metadata) is available in [this gist that I created](https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/0fb7f82221ca08cc17be065eb940a9704376beeb/fastbookRAG_evals.csv). Note that this only includes the chapters covered in Part 1 of the fastai course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gIRDieh7rVLy",
        "outputId": "80eb5826-66ca-4601-9740-33d134c17a7c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 220,\n  \"fields\": [\n    {\n      \"column\": \"chapter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 1,\n        \"max\": 13,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          2,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 1,\n        \"max\": 40,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          20,\n          17,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 220,\n        \"samples\": [\n          \"\\\"\\\"How do entity embeddings reduce memory usage and speed up neural networks?\\\"\\\"\",\n          \"\\\"\\\"Make a list of reasons why a model's validation set error might be worse than the OOB error. How could you test your hypotheses?\\\"\\\"\",\n          \"\\\"\\\"What is \\\"\\\"ReLU\\\"\\\"? Draw a plot of it for values from -2 to +2.\\\"\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"\\\"\\\"No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption.\\\"\\\"\",\n          \"\\\"\\\"\\u201cweight assignment\\u201d refers to the current values of the model parameters. Arthur Samuel further mentions an \\u201c automatic means of testing the effectiveness of any current weight assignment \\u201d and a \\u201c mechanism for altering the weight assignment so as to maximize the performance \\u201d. This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance.\\\"\\\"\",\n          \"\\\"\\\"In this case, we are not taking the dot product but instead concatenating the embedding matrices, so the number of factors can be different.\\\"\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_answerable\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-934f34ab-8e9f-41f6-8216-f87d0ad4f2df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chapter</th>\n",
              "      <th>question_number</th>\n",
              "      <th>question_text</th>\n",
              "      <th>answer</th>\n",
              "      <th>is_answerable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\"Do you need these for deep learning?nn- Lots...</td>\n",
              "      <td>\"\"Lots of math - False\\nLots of data - False\\n...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>\"\"Name five areas where deep learning is now t...</td>\n",
              "      <td>\"\"Any five of the following:\\nNatural Language...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>\"\"What was the name of the first device that w...</td>\n",
              "      <td>\"\"Mark I perceptron built by Frank Rosenblatt\"\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>\"\"Based on the book of the same name, what are...</td>\n",
              "      <td>\"\"A set of processing units\\nA state of activa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>\"\"What were the two theoretical misunderstandi...</td>\n",
              "      <td>\"\"In 1969, Marvin Minsky and Seymour Papert de...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-934f34ab-8e9f-41f6-8216-f87d0ad4f2df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-934f34ab-8e9f-41f6-8216-f87d0ad4f2df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-934f34ab-8e9f-41f6-8216-f87d0ad4f2df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3e1b9ad7-3245-4807-af11-33418a467516\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3e1b9ad7-3245-4807-af11-33418a467516')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3e1b9ad7-3245-4807-af11-33418a467516 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   chapter  question_number  \\\n",
              "0        1                1   \n",
              "1        1                2   \n",
              "2        1                3   \n",
              "3        1                4   \n",
              "4        1                5   \n",
              "\n",
              "                                       question_text  \\\n",
              "0  \"\"Do you need these for deep learning?nn- Lots...   \n",
              "1  \"\"Name five areas where deep learning is now t...   \n",
              "2  \"\"What was the name of the first device that w...   \n",
              "3  \"\"Based on the book of the same name, what are...   \n",
              "4  \"\"What were the two theoretical misunderstandi...   \n",
              "\n",
              "                                              answer  is_answerable  \n",
              "0  \"\"Lots of math - False\\nLots of data - False\\n...              1  \n",
              "1  \"\"Any five of the following:\\nNatural Language...              1  \n",
              "2    \"\"Mark I perceptron built by Frank Rosenblatt\"\"              1  \n",
              "3  \"\"A set of processing units\\nA state of activa...              1  \n",
              "4  \"\"In 1969, Marvin Minsky and Seymour Papert de...              1  "
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get all questions\n",
        "url = 'https://gist.githubusercontent.com/vishalbakshi/309fb3abb222d32446b2c4e29db753fe/raw/804510c62151142ea940faad9ce132c8c85585de/fastbookRAG_evals.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFU93ocf1Y7-",
        "outputId": "a884aa49-c57b-4dcc-dd95-9284f001b5c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(33, 5)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ch1_df = df.query(\"chapter == 1\")\n",
        "ch1_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "7JUsiV7208Ba"
      },
      "outputs": [],
      "source": [
        "keyword_results = []\n",
        "for row in ch1_df['question_text']:\n",
        "  formatted_prompt = prompt.format(question_text=row[2:-2])\n",
        "  r = chat(formatted_prompt)\n",
        "  keyword_results.append(r.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GymPPoEp4Sp9"
      },
      "source": [
        "These tokens add up fast! This got up to 100k+ tokens used. I think the issue is that each time the previous messages are included in the chat. I would expect the token count to be closer to 8000 (245 tokens used for one question times 33 questions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v6jdfkR1U62",
        "outputId": "dd20c37b-04a2-4fcc-e39d-f07466a5d83a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8085"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "245*33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D14W0ngV4WSi",
        "outputId": "067771d3-6cad-4868-ef8c-d5989d09f1bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"architecture, neural network, model structure, design\"',\n",
              " '\"segmentation, image processing, object detection, pixel-level classification\"',\n",
              " '\"y_range, output range, regression, model prediction\"',\n",
              " '\"hyperparameters, model configuration, tuning, machine learning\"',\n",
              " '\"AI implementation, failure prevention, organizational strategy, best practices\"']"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keyword_results[-5:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-sMPfjk5Ct1"
      },
      "source": [
        "I'll do chapter one again but this time I'll create a new `Chat` object for each question so I don't rack up the tokens so quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "qZs_9qMY4hiU"
      },
      "outputs": [],
      "source": [
        "keyword_results2 = []\n",
        "tokens = 0\n",
        "for row in ch1_df['question_text']:\n",
        "  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "  formatted_prompt = prompt.format(question_text=row[2:-2])\n",
        "  r = chat(formatted_prompt)\n",
        "  keyword_results2.append(r.content[0].text)\n",
        "  tokens += chat.use.total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIL390gP-2ZA"
      },
      "source": [
        "The token usage is much better!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq_ba0hy8F3i",
        "outputId": "d6ec0d74-1ba9-4021-99f9-917f20c01354"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8048"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f0ldw1_8frk"
      },
      "source": [
        "Although it definitely comes up with different keywords when it doesn't use the accumulated chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNg2tL3Y5q5V",
        "outputId": "bd326094-ee3d-469b-8663-bcd99cb5632a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"architecture, definition, structure, design\"',\n",
              " '\"segmentation, division, partitioning, classification\"',\n",
              " '\"y_range, purpose, usage, application\"',\n",
              " '\"hyperparameters, machine learning, model configuration, tuning\"',\n",
              " '\"AI failures, organization, best practices, risk mitigation, implementation\"']"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keyword_results2[-5:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LE9HMJ582E7"
      },
      "source": [
        "## Improving the Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHgZIotk851x"
      },
      "source": [
        "Something that I want to make sure Claude does is prefer single keywords with commas where possible. Currently, it is grouping words together like `\"AI failures\"` or `\"risk mitigation\"` (which I want as `\"AI, failures\"`, and `\"risk, mitigation\"`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "w6WseCvn8VcB"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show prompt\"\n",
        "prompt = \"\"\"I am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\n",
        "Do not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\n",
        "Try to use single-word keywords when possible.\n",
        "\n",
        "No yapping.\n",
        "\n",
        "Examples:\n",
        "\n",
        "question_text: Name five areas where deep learning is now the best in the world\n",
        "keywords: \"deep, learning, best, world\"\n",
        "\n",
        "question_text: Why is it hard to use a traditional computer program to recognize images in a photo?\n",
        "keywords: \"image, recognize, recognition, traditional, computer, program\"\n",
        "\n",
        "question_text: What were the two theoretical misunderstandings that held back the field of neural networks?\n",
        "keywords: \"theoretical, misunderstandings, held, back, field, neural, network\"\n",
        "\n",
        "question_text: {question_text}\n",
        "keywords:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNpt2ifq9UvT",
        "outputId": "de7150a0-7f81-4b91-c983-44bfdc6bde04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am working on a keyword search project and i need to create 3-6 keywords for each `question_text` that I provide you.\n",
            "Do not generate keywords that stray too far in meaning from the `question_text`. Only respond with the comma-separated list of keywords surrounded by double quotes.\n",
            "Try to use single-word keywords when possible.\n",
            "\n",
            "No yapping. \n",
            "\n",
            "Examples:\n",
            "\n",
            "question_text: Name five areas where deep learning is now the best in the world\n",
            "keywords: \"deep, learning, best, world\"\n",
            "\n",
            "question_text: Why is it hard to use a traditional computer program to recognize images in a photo?\n",
            "keywords: \"image, recognize, recognition, traditional, computer, program\"\n",
            "\n",
            "question_text: What were the two theoretical misunderstandings that held back the field of neural networks?\n",
            "keywords: \"theoretical, misunderstandings, held, back, field, neural, network\"\n",
            "\n",
            "question_text: What's the best way to avoid failures when using AI in an organization?\n",
            "keywords:\n"
          ]
        }
      ],
      "source": [
        "formatted_prompt = prompt.format(question_text=\"\"\"What's the best way to avoid failures when using AI in an organization?\"\"\")\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "yNPQC_1r-Ji2",
        "outputId": "583721b5-2fa3-4733-a26f-92187395b877"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\"AI, failures, avoid, organization, best practices\"\n",
              "\n",
              "<details>\n",
              "\n",
              "- id: msg_01FAMozL8CxeSu2ydhx1KdbC\n",
              "- content: [{'text': '\"AI, failures, avoid, organization, best practices\"', 'type': 'text'}]\n",
              "- model: claude-3-5-sonnet-20240620\n",
              "- role: assistant\n",
              "- stop_reason: end_turn\n",
              "- stop_sequence: None\n",
              "- type: message\n",
              "- usage: {'input_tokens': 236, 'output_tokens': 15}\n",
              "\n",
              "</details>"
            ],
            "text/plain": [
              "Message(id='msg_01FAMozL8CxeSu2ydhx1KdbC', content=[TextBlock(text='\"AI, failures, avoid, organization, best practices\"', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 236; Out: 15; Total: 251)"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "chat(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "Yp63l9EM-qQ8"
      },
      "outputs": [],
      "source": [
        "keyword_results3 = []\n",
        "tokens = 0\n",
        "for row in ch1_df['question_text']:\n",
        "  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "  formatted_prompt = prompt.format(question_text=row[2:-2])\n",
        "  r = chat(formatted_prompt)\n",
        "  keyword_results3.append(r.content[0].text)\n",
        "  tokens += chat.use.total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oian3YfoAg0x",
        "outputId": "c06bb584-fc2f-42f6-d6cf-20896bd87cfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8265"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em01YkPeAonX"
      },
      "source": [
        "The keywords look promising. Their true effectiveness will be tested when used in full-text search. I'll adjust the prompt as needed based on those results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3DnyQRhAhP0",
        "outputId": "83add14c-17f0-4304-863d-fa817e7af2e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"metric, loss, difference, measurement, evaluation\"',\n",
              " '\"pretrained, models, help, benefits\"',\n",
              " '\"head, model, neural, network\"',\n",
              " '\"CNN, layers, features, early, later\"',\n",
              " '\"image, models, photos, usefulness\"',\n",
              " '\"architecture, definition, structure, design\"',\n",
              " '\"segmentation, division, partition, categorization\"',\n",
              " '\"y_range, purpose, usage, necessity\"',\n",
              " '\"hyperparameters, machine, learning, parameters, model, configuration\"',\n",
              " '\"AI, failures, avoid, organization, best practices\"']"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keyword_results3[-10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6arnEtjJBKhk"
      },
      "source": [
        "So far I have used 48 cents in API credits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhQ5jgU3K4vV"
      },
      "source": [
        "## Generating Keywords for All Chapters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeTsxXt-MkXF"
      },
      "source": [
        "It took about 43 minutes and 20 cents to generate keywords for 220 questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "dCiSrKJfAjVK"
      },
      "outputs": [],
      "source": [
        "keywords = []\n",
        "tokens = 0\n",
        "for row in df['question_text']:\n",
        "  chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "  formatted_prompt = prompt.format(question_text=row[2:-2])\n",
        "  r = chat(formatted_prompt)\n",
        "  keywords.append(r.content[0].text)\n",
        "  tokens += chat.use.total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEqEs8ocCB3L",
        "outputId": "d4b1f6eb-54a3-43ad-9be2-12de424b5c22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55245"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buWamsUiPLoA",
        "outputId": "bfc664d8-a9e5-4c3a-d871-02382a4748e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "53900"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "245*220 # estimated tokens used for 220 questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SVCeaQ5KllS",
        "outputId": "c2a438d7-4a86-4c28-9888-d65ade92eaa1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "220"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD1_6nMmPWRE"
      },
      "source": [
        "Spot-checking the generated keywords and they look okay!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuVJZQA2PULm",
        "outputId": "e10e99a7-494a-4c2a-b30f-4cb859ce17e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"deep, learning, requirements, math, data, computers, PhD\"',\n",
              " '\"deep, learning, areas, best, world\"',\n",
              " '\"artificial, neuron, device, first, principle\"',\n",
              " '\"parallel, distributed, processing, PDP, requirements, book\"',\n",
              " '\"misunderstandings, neural, networks, theoretical, setbacks\"']"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDw0TR-rPRhS",
        "outputId": "ef9960e8-d757-44c6-e665-9c044b3fbef6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"column, pixels, color, dim, plot, represent\"',\n",
              " '\"bad, training, color_dim, why\"',\n",
              " '\"batch, normalization, trainable, parameters, layer\"',\n",
              " '\"batch, normalization, statistics, training, validation\"',\n",
              " '\"batch, normalization, generalization, models\"']"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[-5:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s09H-x0ONwX-"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA_FZ2olNyV2"
      },
      "source": [
        "Using `claudette` to generate keywords for my **fastbookRAG** eval questions was really straightforward. I'll use these keywords for full-text search to answer the questions and plan to revisit and refine the prompt based on the quality of the context retrieved.\n",
        "\n",
        "I hope you enjoyed this blog post! Follow me on Twitter [@vishal_learner](https://twitter.com/vishal_learner)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
