{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: Logit Divergence Between Models Differently Converted to `torch.bfloat16`\n",
        "date: \"2025-08-17\"\n",
        "author: Vishal Bakshi\n",
        "description: In this post I observe differences in intermediate and final outputs between HuggingFace models that are converted to `torch.bfloat16` differently (one model with `torch_dtype` specified in `from_pretrained` and the other with `.to(torch.bfloat16)` specified after model is loaded.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCQDHpQoZHtf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this blog post I'll illustrate a recent head-scratcher I came across---how to convert a model t `torch.bfloat16` changes the intermediate and final outputs. I don't know why this happens and not sure of a path to figure that out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In `model1` I specify `torch_dtype` in `AutoModelForCausalLM.from_pretrained`. In `model2`, I don't, and instead use `to(torch.bfloat16)` after the model is loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYrWS-Y7dFfF"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "device = \"cuda\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model1 = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n",
        "model2 = AutoModelForCausalLM.from_pretrained(checkpoint).to(device).to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cELFVsWx9h3h"
      },
      "source": [
        "## Comparing Logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gzemoxp9kTP"
      },
      "source": [
        "Given a set of input tokens, the output logits of the two models are not identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc06IhF7pj4g",
        "outputId": "80502c56-d418-4fba-dde2-a134dcd209db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[22007,  6463,   314]], device='cuda:0')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpzoNWbApv8D",
        "outputId": "782d67bf-c79b-45cc-fb92-9981b1528788"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[18.0000, 14.5625, 14.6875,  ..., 16.2500, 16.2500, 22.1250],\n",
              "         [15.6875, -0.4180, -0.3477,  ...,  8.2500, 12.1250,  7.3438],\n",
              "         [12.1875, -2.2812, -2.2031,  ...,  7.3750, 10.6875,  8.1875]]],\n",
              "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model1.eval()\n",
        "logits1 = model1(inputs).logits\n",
        "logits1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKRSKaGcp1BD",
        "outputId": "cf487415-91c9-4464-c064-a506c4a090cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[18.0000, 14.5625, 14.6875,  ..., 16.2500, 16.2500, 22.1250],\n",
              "         [15.7500, -0.2715, -0.2002,  ...,  8.4375, 12.2500,  7.5000],\n",
              "         [12.3125, -2.2188, -2.1406,  ...,  7.5000, 10.6875,  8.3125]]],\n",
              "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.eval()\n",
        "logits2 = model2(inputs).logits\n",
        "logits2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-6oRVPh9nwm",
        "outputId": "9c6a1f1e-6ce9-4a20-ee2d-ef92a3a9f82e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.allclose(logits1, logits2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXwcOoZ3Ak3S",
        "outputId": "1377752c-96df-4195-9d6f-052bbbe3cf7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3457, device='cuda:0')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(logits1 == logits2).float().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMG1xV4_Aq2X",
        "outputId": "41d1e426-3945-40e1-c97b-f2e20debb62e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0762, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.abs(logits1 - logits2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deoP-yjeA8pY"
      },
      "source": [
        "## Comparing Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzSkjO3JBAO8"
      },
      "source": [
        "A helper function to inspect a particular submodule in a particular layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJajlH70oKjY"
      },
      "outputs": [],
      "source": [
        "def _print(model1, model2, module, submodule, layer_idx):\n",
        "    w1 = getattr(getattr(model1.model.layers[layer_idx], module), submodule).weight\n",
        "    w2 = getattr(getattr(model2.model.layers[layer_idx], module), submodule).weight\n",
        "    print(f\"{module}.{submodule} torch.allclose: {torch.allclose(w1, w2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHab7KjqohsZ",
        "outputId": "2c8feb33-9f2e-4511-9518-0d313209b7d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self_attn.q_proj torch.allclose: True\n"
          ]
        }
      ],
      "source": [
        "_print(model1, model2, \"self_attn\", \"q_proj\", 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5t9lDMJBFUN"
      },
      "source": [
        "Looping through all weight matrices in state dicts, they are all identical---why are output logits not identical then? I would assume that something in the matrix ops is causing the divergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5JQAxofBuRE",
        "outputId": "6f87344e-7033-4bee-e12c-ff02b8bb7a89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(273, 273, 1.0)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n = 0\n",
        "d = 0\n",
        "for k in model1.state_dict().keys():\n",
        "    w1 = model1.state_dict()[k]\n",
        "    w2 = model2.state_dict()[k]\n",
        "    if torch.allclose(w1, w2): n += 1\n",
        "    d += 1\n",
        "n, d, n/d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iySuyP5CZRP"
      },
      "source": [
        "## Forward Hooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hooking the two models to track intermediate layer outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo-dE7qgxbNX"
      },
      "outputs": [],
      "source": [
        "model1.eval()\n",
        "model2.eval()\n",
        "outputs_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWaY4ayYCaCa"
      },
      "outputs": [],
      "source": [
        "def capture_output(name):\n",
        "    def hook_fn(module, input, output):\n",
        "        outputs_dict[name] = output[0].detach()\n",
        "    return hook_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9UYNkf0xk8Y"
      },
      "outputs": [],
      "source": [
        "hooks = []\n",
        "for i in range(30):\n",
        "    hooks.append(model1.model.layers[i].register_forward_hook(capture_output(f\"model1_{i}\")))\n",
        "    hooks.append(model2.model.layers[i].register_forward_hook(capture_output(f\"model2_{i}\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93j_hdpKIKyG"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model1(inputs)\n",
        "    model2(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQO4lgL1IQaA"
      },
      "outputs": [],
      "source": [
        "for h in hooks: h.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The difference in intermediate outputs diverges as you pass through the model. That smells of typical floating point precision error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AS9ul3CIQwR",
        "outputId": "6c7066d4-d24a-4f37-da3c-c396f78e69e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: mean diff = 0.0017547607421875\n",
            "Layer 1: mean diff = 0.005035400390625\n",
            "Layer 2: mean diff = 0.00830078125\n",
            "Layer 3: mean diff = 0.010986328125\n",
            "Layer 4: mean diff = 0.011962890625\n",
            "Layer 5: mean diff = 0.01251220703125\n",
            "Layer 6: mean diff = 0.01312255859375\n",
            "Layer 7: mean diff = 0.0137939453125\n",
            "Layer 8: mean diff = 0.015380859375\n",
            "Layer 9: mean diff = 0.0172119140625\n",
            "Layer 10: mean diff = 0.0189208984375\n",
            "Layer 11: mean diff = 0.0185546875\n",
            "Layer 12: mean diff = 0.01953125\n",
            "Layer 13: mean diff = 0.020751953125\n",
            "Layer 14: mean diff = 0.021728515625\n",
            "Layer 15: mean diff = 0.0234375\n",
            "Layer 16: mean diff = 0.026123046875\n",
            "Layer 17: mean diff = 0.0263671875\n",
            "Layer 18: mean diff = 0.0269775390625\n",
            "Layer 19: mean diff = 0.0301513671875\n",
            "Layer 20: mean diff = 0.03271484375\n",
            "Layer 21: mean diff = 0.036376953125\n",
            "Layer 22: mean diff = 0.044921875\n",
            "Layer 23: mean diff = 0.05322265625\n",
            "Layer 24: mean diff = 0.05810546875\n",
            "Layer 25: mean diff = 0.06689453125\n",
            "Layer 26: mean diff = 0.0771484375\n",
            "Layer 27: mean diff = 0.091796875\n",
            "Layer 28: mean diff = 0.1005859375\n",
            "Layer 29: mean diff = 0.1484375\n"
          ]
        }
      ],
      "source": [
        "metric = \"mean\"\n",
        "for i in range(30):\n",
        "    o1 = outputs_dict[f\"model1_{i}\"]\n",
        "    o2 = outputs_dict[f\"model2_{i}\"]\n",
        "\n",
        "    if not torch.allclose(o1, o2):\n",
        "        max_diff = (o1-o2).abs().max().item()\n",
        "        mean_diff = (o1-o2).abs().mean().item()\n",
        "        if metric == \"max\": print(f\"Layer {i}: max diff = {max_diff}\")\n",
        "        if metric == \"mean\": print(f\"Layer {i}: mean diff = {mean_diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The max difference in outputs reaches `6.0` by the 30th layer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPx9GggOIubI",
        "outputId": "d9b03a21-de97-48f9-919c-9c252f976f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: max diff = 0.0625\n",
            "Layer 1: max diff = 0.25\n",
            "Layer 2: max diff = 0.25\n",
            "Layer 3: max diff = 0.25\n",
            "Layer 4: max diff = 0.25\n",
            "Layer 5: max diff = 0.25\n",
            "Layer 6: max diff = 0.25\n",
            "Layer 7: max diff = 0.5\n",
            "Layer 8: max diff = 0.5\n",
            "Layer 9: max diff = 0.5\n",
            "Layer 10: max diff = 1.0\n",
            "Layer 11: max diff = 1.0\n",
            "Layer 12: max diff = 1.0\n",
            "Layer 13: max diff = 0.5\n",
            "Layer 14: max diff = 0.5\n",
            "Layer 15: max diff = 0.5\n",
            "Layer 16: max diff = 0.5\n",
            "Layer 17: max diff = 0.5\n",
            "Layer 18: max diff = 0.5\n",
            "Layer 19: max diff = 0.75\n",
            "Layer 20: max diff = 0.5\n",
            "Layer 21: max diff = 0.5\n",
            "Layer 22: max diff = 0.5\n",
            "Layer 23: max diff = 0.75\n",
            "Layer 24: max diff = 1.0\n",
            "Layer 25: max diff = 2.0\n",
            "Layer 26: max diff = 2.0\n",
            "Layer 27: max diff = 2.0\n",
            "Layer 28: max diff = 1.0\n",
            "Layer 29: max diff = 6.0\n"
          ]
        }
      ],
      "source": [
        "metric = \"max\"\n",
        "for i in range(30):\n",
        "    o1 = outputs_dict[f\"model1_{i}\"]\n",
        "    o2 = outputs_dict[f\"model2_{i}\"]\n",
        "\n",
        "    if not torch.allclose(o1, o2):\n",
        "        max_diff = (o1-o2).abs().max().item()\n",
        "        mean_diff = (o1-o2).abs().mean().item()\n",
        "        if metric == \"max\": print(f\"Layer {i}: max diff = {max_diff}\")\n",
        "        if metric == \"mean\": print(f\"Layer {i}: mean diff = {mean_diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reloading the models and inspecting the outputs of intermediate modules like `self_attn` and `mlp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12rosA0BhYPE"
      },
      "outputs": [],
      "source": [
        "model1 = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n",
        "model2 = AutoModelForCausalLM.from_pretrained(checkpoint).to(device).to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn8s3-d_etSZ"
      },
      "outputs": [],
      "source": [
        "modules = {\n",
        "    \"self_attn\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    \"input_layernorm\": [],\n",
        "    \"post_attention_layernorm\": []\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcqYDxCJegq1"
      },
      "outputs": [],
      "source": [
        "outputs_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61IWboA6fvtD"
      },
      "outputs": [],
      "source": [
        "for module in modules.keys():\n",
        "    if module == \"input_layernorm\" or module == \"post_attention_layernorm\":\n",
        "        hooks.append(getattr(model1.model.layers[0], module).register_forward_hook(capture_output(f\"model1_{module}\")))\n",
        "        hooks.append(getattr(model2.model.layers[0], module).register_forward_hook(capture_output(f\"model2_{module}\")))\n",
        "    else:\n",
        "        for submodule in modules[module]:\n",
        "            hooks.append(getattr(getattr(model1.model.layers[0], module), submodule).register_forward_hook(capture_output(f\"model1_{module}_{submodule}\")))\n",
        "            hooks.append(getattr(getattr(model2.model.layers[0], module), submodule).register_forward_hook(capture_output(f\"model2_{module}_{submodule}\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bsMhKRnemB1"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model1(inputs)\n",
        "    model2(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoEPyjNBftlp"
      },
      "outputs": [],
      "source": [
        "for h in hooks: h.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the intermediate attention outputs are identical but there's divergence in the outputs of the attention mechanism as it passes through `o_proj`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzGq68_5h0yu",
        "outputId": "ce0830e3-31bc-4554-ce6e-ba3d91c133fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self_attn.q_proj: 0.0\n",
            "self_attn.k_proj: 0.0\n",
            "self_attn.v_proj: 0.0\n",
            "self_attn.o_proj: 2.1457672119140625e-05\n",
            "mlp.gate_proj: 0.000476837158203125\n",
            "mlp.up_proj: 0.0003833770751953125\n",
            "mlp.down_proj: 0.00177764892578125\n",
            "input_layernorm: 0.0\n",
            "post_attention_layernorm: 1.8715858459472656e-05\n"
          ]
        }
      ],
      "source": [
        "for module in modules.keys():\n",
        "    if module == \"input_layernorm\" or module == \"post_attention_layernorm\":\n",
        "        o1 = outputs_dict[f\"model1_{module}\"]\n",
        "        o2 = outputs_dict[f\"model2_{module}\"]\n",
        "        diff = (o1-o2).abs().mean().item()\n",
        "        print(f\"{module}: {diff}\")\n",
        "    else:\n",
        "        for submodule in modules[module]:\n",
        "            o1 = outputs_dict[f\"model1_{module}_{submodule}\"]\n",
        "            o2 = outputs_dict[f\"model2_{module}_{submodule}\"]\n",
        "            diff = (o1-o2).abs().mean().item()\n",
        "            print(f\"{module}.{submodule}: {diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again I haven't dug into why these differences exist, but wanted to document that they do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
