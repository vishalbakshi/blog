{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: HuggingFace's Default KV Cache and the `flash_attn_varlen_func` Docstring\n",
        "date: \"2025-06-03\"\n",
        "author: Vishal Bakshi\n",
        "description: A deep dive into understanding `flash_attn_varlen_func`'s docstring's causal masks (for seqlen_q != seqlen_k) by exploring Hugging Face's KV Cache (`DynamicCache`) in `model.generate()` with hands-on Q/K shape inspection. Unravels \"bottom-right alignment\" and why `flash_attn_func` gets called.\n",
        "filters:\n",
        "   - lightbox\n",
        "lightbox: auto\n",
        "categories:\n",
        "    - python\n",
        "    - deep learning\n",
        "    - Flash Attention\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBmhhRtuuFtw"
      },
      "outputs": [],
      "source": [
        "!pip install -qq -U flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bAk6hsKoIFKS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSf7QQDaKoVJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Eu1uMqC8iS"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8EG03i0C9vY"
      },
      "source": [
        "I have recently been working on a research project which has required me to [better understand sequence packing and Flash Attention](https://vishalbakshi.github.io/blog/posts/2025-05-04-Understanding-Sequence-Packing/), and [eager attention as well](https://youtu.be/u_v6HHyv4No). However, I've found that both in terms of my interest, and for practical understanding, that hasn't been enough!\n",
        "\n",
        "Tangentially, I also recently started using Gemini Pro 2.5 (the chat UI) and have been thoroughly enjoying it after using a year of daily use of Claude Pro. This seemed like a great opportunity to test out Gemini's functionality to learn about a topic that is complex and currently outside of my comfort zone.\n",
        "\n",
        "I fed Gemini the Flash Attention GitHub repo and explained that I wanted to thoroughly understand `flash_attn_varlen_func` to the point where I could make a detailed video walkthrough with visuals. It provided me with this condensed timeline:\n",
        "\n",
        "- **Phase 1: Python Layer & Sequence Packing Concepts**\n",
        "    - Tasks: Understand `flash_attn_varlen_func`'s Python call stack, the role of `cu_seqlens`, `max_seqlen`, and `torch.autograd.Function`.\n",
        "    - Time: 1-2 Weeks\n",
        "    - Hours: Approximately 15-30 hours\n",
        "- **Phase 2: Core CUDA Kernel Deep Dive**\n",
        "    - Tasks: Study the FlashAttention research paper(s).\n",
        "    - Analyze the C++/CUDA dispatcher code (e.g., in `csrc/flash_attn/flash_api.cpp`).\n",
        "    - Dissect the core CUDA kernels for variable length forward and backward passes (e.g., in `csrc/flash_attn/src/` like `flash_fwd_kernel.h` and `flash_bwd_kernel.h`).\n",
        "    - Time: 6-12 Weeks\n",
        "    - Hours: Approximately 125-240 hours\n",
        "- **Phase 3: Content Creation (Video/Blog)**\n",
        "    - Tasks: Plan the structure for your content, create visuals, draft explanations, and prepare code snippets.\n",
        "    - Time: 2-3 Weeks\n",
        "    - Hours: Approximately 30-50 hours\n",
        "- **Total Estimated for CUDA Path**\n",
        "    - Overall Timeline: Roughly 2.5 - 4.5 months\n",
        "    - Total Focused Hours: Approximately 170 - 320 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6hkpXyeFiAd"
      },
      "source": [
        "This is obviously an amibitious goal and timeline, especially because of my limited C++/CUDA knowledge and experience. However, I do believe this is a case of aim-for-the-stars-land-on-the-moon, as I've already experienced growth and learning in the first steps of Phase 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOtHT4J3F87y"
      },
      "source": [
        "As I was reading through [`flash_attn_varlen_func`](https://github.com/Dao-AILab/flash-attention/blob/df1847a74ad0f9cee007ed186fab44f83fa03fad/flash_attn/flash_attn_interface.py#L1370) source code, I got stuck on the following piece of the docstring:\n",
        "\n",
        "```\n",
        "If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n",
        "For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n",
        "    1 1 1 1 0\n",
        "    1 1 1 1 1\n",
        "If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n",
        "    0 0\n",
        "    0 0\n",
        "    0 0\n",
        "    1 0\n",
        "    1 1\n",
        "If the row of the mask is all zero, the output will be zero.\n",
        "```\n",
        "\n",
        "I didn't have hands-on experience working with this concept, where the query length is different than the key and value length. Gemini helped me realize that this happens in the extremely common case of autoregressive generation---the next token (query length of 1) attends to the previous tokens (key/value length > 1). The concept of KV cache also came up in our conversation.\n",
        "\n",
        "I don't tend to understand things until I see them in code, so in this notebook, I'll inspect the shapes of Q, K and V during the HuggingFace `model.generate` call. I'll also peel back a couple layers and understand how HugginFace uses KV cache. After that exploration, I'll return back to the `flash_attn_varlen_func` docstring and walk through the logic behind how the causal mask is shaped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pZpK5uGr7Lo?si=znCX-qawKdcoX0V9\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suGvbWuEJI3y"
      },
      "source": [
        "## Understanding HuggingFace's Default KV Cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEugHzDXI2Ga"
      },
      "source": [
        "I'll start by understanding how HuggingFace uses KV cache (I was surprised to find that it uses it by default!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-EtcDRKTYrV"
      },
      "source": [
        "### Inspecting `model_kwargs` for Caching Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHFylGS3L3YO"
      },
      "source": [
        "Looking at the [`generate`](https://github.com/huggingface/transformers/blob/1094dd34f73dae1d9a91a6632635934516612490/src/transformers/generation/utils.py#L2481) source code, the first method call of interest when it comes to KV cache seems to be [`_prepare_cache_for_generation`](https://github.com/huggingface/transformers/blob/1094dd34f73dae1d9a91a6632635934516612490/src/transformers/generation/utils.py#L1981), which takes the following arguments: `generation_config`, `model_kwargs`, `assistant_model`, `batch_size`, `max_cache_length`, `device`. Going down the different elif statements, `_prepare_cache_for_generation` sets the following `model_kwargs` value:\n",
        "\n",
        "```python\n",
        "model_kwargs[cache_name] = (\n",
        "    DynamicCache()\n",
        "    if not requires_cross_attention_cache\n",
        "    else EncoderDecoderCache(DynamicCache(), DynamicCache())\n",
        ")\n",
        "```\n",
        "\n",
        "Where `cache_name` is defined earlier in that method as:\n",
        "\n",
        "```python\n",
        "cache_name = \"past_key_values\" if not is_hybrid_cache else \"cache_params\"\n",
        "```\n",
        "\n",
        "I want to inspect what `model_kwargs['past_key_values']` is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zABj8oDnR57O"
      },
      "source": [
        "[`_prepare_generation_config`](https://github.com/huggingface/transformers/blob/1094dd34f73dae1d9a91a6632635934516612490/src/transformers/generation/utils.py#L2369) is used in `generate` to produce `generation_config` and `model_kwargs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Ud8GlXIjl1",
        "outputId": "d4927381-46b1-42f6-aec8-9e94d36c16a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(GenerationConfig {\n",
              "   \"bos_token_id\": 0,\n",
              "   \"eos_token_id\": 0\n",
              " },\n",
              " {})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generation_config, model_kwargs = model._prepare_generation_config(None)\n",
        "generation_config, model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFKuaUCFTLxE"
      },
      "source": [
        "I can now pass those on to `_prepare_cache_for_generation`, which will internally modify `model_kwargs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "enjUJmn_HQNK"
      },
      "outputs": [],
      "source": [
        "model._prepare_cache_for_generation(generation_config, model_kwargs, None, 1, 8192, \"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMtpwt33INgb",
        "outputId": "533d54c0-fad1-4f4a-9b31-4fe867e16454"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'past_key_values': <transformers.cache_utils.DynamicCache at 0x78c3b80d9850>}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgqb25oNTRqc"
      },
      "source": [
        "I can see now that `model_kwargs` has a `'past_key_values'` key which has a `DynamicCache` value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g81vEjBRK8If"
      },
      "source": [
        "### How is `past_key_values` Used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WaidMC6NSmH"
      },
      "source": [
        "I think it makes sense to start by looking at [the forward pass of the LlamaAttention module](https://github.com/huggingface/transformers/blob/e8b292e35f331d3c3de85f7e5d3496b0e13d3d6f/src/transformers/models/llama/modeling_llama.py#L223):\n",
        "\n",
        "```python\n",
        "...\n",
        "\n",
        "key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "\n",
        "...\n",
        "\n",
        "if past_key_value is not None:\n",
        "    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQIeUwTFNu7J"
      },
      "source": [
        "The `hidden_states` pass through `k_proj` and `v_proj` to produce `key_states` and `value_states`, respectively, which are then passed to `past_key_value.update` to produce a new set of `key_states` and `value_states`. Looking at [`DynamicCache.update`]():\n",
        "\n",
        "```python\n",
        "# Update the cache\n",
        "if key_states is not None:\n",
        "    if len(self.key_cache) <= layer_idx:\n",
        "        # There may be skipped layers, fill them with empty lists\n",
        "        for _ in range(len(self.key_cache), layer_idx):\n",
        "            self.key_cache.append(torch.tensor([]))\n",
        "            self.value_cache.append(torch.tensor([]))\n",
        "        self.key_cache.append(key_states)\n",
        "        self.value_cache.append(value_states)\n",
        "    elif (\n",
        "        not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n",
        "    ):  # fills previously skipped layers; checking for tensor causes errors\n",
        "        self.key_cache[layer_idx] = key_states\n",
        "        self.value_cache[layer_idx] = value_states\n",
        "    else:\n",
        "        self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
        "        self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
        "\n",
        "return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilNgf2e-ToCe"
      },
      "source": [
        "Let's walk through each condition in the if-else block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu0apH1aUcm3"
      },
      "source": [
        "#### `if len(self.key_cache) <= layer_idx`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UMLgjFhUez3"
      },
      "source": [
        "A full `key_cache` is has `n_layers` number of elements. If its number of elements is less than or equal to the `layer_idx` that means that it does not contain `key_states` for that `layer_idx` yet (because python starts count from `0`). For example suppose `layer_idx` is `0`, our first layer. `if len(self.key_cache) <= layer_idx` is `True`, that means `len(self.key_cache)` is `0` and doesn't contain `key_states` for the first layer, as would be the case if you were generating the first token of a response. In this case you simply `append` the `key_states` to the cache.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59mwRyOEU_rV"
      },
      "source": [
        "If `layer_idx` is greater than `len(self.key_cache)` then it appends an empty tensor for the \"skipped\" layers. This would be a scenario where you were generating the first token of a response (`len(self.key_cache)` is `0`) but starting with `layer_idx` of `2`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRQdMThXjDJ"
      },
      "source": [
        "#### `elif not self.key_cache[layer_idx].numel()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecpdK-sFXpyX"
      },
      "source": [
        "If a layer was skipped and it has an empty tensor as its `key_cache` then this condition is triggered and it simply assigned `key_states` to that layer's `key_cache`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnPbbMrNX3OG"
      },
      "source": [
        "#### `else`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTkoaHdnX5S1"
      },
      "source": [
        "I think this is the most common case, used for autoregressive next-token generation. The `key_cache` contains a non-empty value for this layer so it concatenates the current value with the new `key_states`. In this way, the `key_cache` for this layer grows over the course of next token generation. Specifically, it's second to last dimension (sequence length) increases by 1 for each token processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gug4xOm0r9MP"
      },
      "source": [
        "#### `return self.key_cache[layer_idx], self.value_cache[layer_idx]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlVGqSLOsBUT"
      },
      "source": [
        "Finally, the concatenated `key_cache` and `value_cache` for the given layer are returned. The `update` step is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x6kcw2_r360"
      },
      "source": [
        "`key_states` and `value_states` after the `past_key_values.update` step are passed onto the `attention_interface` which we'll look at later in this blog post.\n",
        "\n",
        "```python\n",
        "attn_output, attn_weights = attention_interface(\n",
        "            self,\n",
        "            query_states,\n",
        "            key_states,\n",
        "            value_states,\n",
        "            attention_mask,\n",
        "            dropout=0.0 if not self.training else self.attention_dropout,\n",
        "            scaling=self.scaling,\n",
        "            **kwargs,\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqBzj7pbsa6G"
      },
      "source": [
        "### Visualizing the `DynamicCache.update`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkb4cugvtIms"
      },
      "source": [
        "To see how the cache update takes place during autoregressive language generation, I'll monkey-patch a `debug_update` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "svpsMKq3vqO_"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show `debug_update\"\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "def debug_update(\n",
        "    self,\n",
        "    key_states: torch.Tensor,\n",
        "    value_states: torch.Tensor,\n",
        "    layer_idx: int,\n",
        "    cache_kwargs: Optional[Dict[str, Any]] = None,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n",
        "\n",
        "    Parameters:\n",
        "        key_states (`torch.Tensor`):\n",
        "            The new key states to cache.\n",
        "        value_states (`torch.Tensor`):\n",
        "            The new value states to cache.\n",
        "        layer_idx (`int`):\n",
        "            The index of the layer to cache the states for.\n",
        "        cache_kwargs (`Dict[str, Any]`, `optional`):\n",
        "            Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n",
        "\n",
        "    Return:\n",
        "        A tuple containing the updated key and value states.\n",
        "    \"\"\"\n",
        "    # Update the number of seen tokens\n",
        "    if layer_idx == 0:\n",
        "        self._seen_tokens += key_states.shape[-2]\n",
        "\n",
        "    # Update the cache\n",
        "    if key_states is not None:\n",
        "        if len(self.key_cache) <= layer_idx:\n",
        "            print(f\"DEBUG: initializing cache for layer_idx {layer_idx}\")\n",
        "            for _ in range(len(self.key_cache), layer_idx):\n",
        "                self.key_cache.append(torch.tensor([]))\n",
        "                self.value_cache.append(torch.tensor([]))\n",
        "            self.key_cache.append(key_states)\n",
        "            self.value_cache.append(value_states)\n",
        "        elif (\n",
        "            not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n",
        "        ):  # fills previously skipped layers; checking for tensor causes errors\n",
        "            print(f\"DEBUG: filling empty cache for layer_idx {layer_idx}\")\n",
        "            self.key_cache[layer_idx] = key_states\n",
        "            self.value_cache[layer_idx] = value_states\n",
        "        else:\n",
        "            print(f\"DEBUG: updating/concatenating cache for layer_idx {layer_idx}\")\n",
        "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
        "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
        "\n",
        "    return self.key_cache[layer_idx], self.value_cache[layer_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "GX7ovG_qs8Jt"
      },
      "outputs": [],
      "source": [
        "from transformers.cache_utils import DynamicCache\n",
        "if 'ORIGINAL_DYNAMIC_CACHE_UPDATE' not in globals():\n",
        "    ORIGINAL_DYNAMIC_CACHE_UPDATE = DynamicCache.update\n",
        "    print(\"Stored original DynamicCache.update.\")\n",
        "\n",
        "DynamicCache.update = debug_update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY18H_xutBzk",
        "outputId": "3bbb53b2-9b89-4361-cb26-e980bb510959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: initializing cache for layer_idx 0\n",
            "DEBUG: initializing cache for layer_idx 1\n",
            "DEBUG: initializing cache for layer_idx 2\n",
            "DEBUG: initializing cache for layer_idx 3\n",
            "DEBUG: initializing cache for layer_idx 4\n",
            "DEBUG: initializing cache for layer_idx 5\n",
            "DEBUG: initializing cache for layer_idx 6\n",
            "DEBUG: initializing cache for layer_idx 7\n",
            "DEBUG: initializing cache for layer_idx 8\n",
            "DEBUG: initializing cache for layer_idx 9\n",
            "DEBUG: initializing cache for layer_idx 10\n",
            "DEBUG: initializing cache for layer_idx 11\n",
            "DEBUG: initializing cache for layer_idx 12\n",
            "DEBUG: initializing cache for layer_idx 13\n",
            "DEBUG: initializing cache for layer_idx 14\n",
            "DEBUG: initializing cache for layer_idx 15\n",
            "DEBUG: initializing cache for layer_idx 16\n",
            "DEBUG: initializing cache for layer_idx 17\n",
            "DEBUG: initializing cache for layer_idx 18\n",
            "DEBUG: initializing cache for layer_idx 19\n",
            "DEBUG: initializing cache for layer_idx 20\n",
            "DEBUG: initializing cache for layer_idx 21\n",
            "DEBUG: initializing cache for layer_idx 22\n",
            "DEBUG: initializing cache for layer_idx 23\n",
            "DEBUG: initializing cache for layer_idx 24\n",
            "DEBUG: initializing cache for layer_idx 25\n",
            "DEBUG: initializing cache for layer_idx 26\n",
            "DEBUG: initializing cache for layer_idx 27\n",
            "DEBUG: initializing cache for layer_idx 28\n",
            "DEBUG: initializing cache for layer_idx 29\n",
            "DEBUG: updating/concatenating cache for layer_idx 0\n",
            "DEBUG: updating/concatenating cache for layer_idx 1\n",
            "DEBUG: updating/concatenating cache for layer_idx 2\n",
            "DEBUG: updating/concatenating cache for layer_idx 3\n",
            "DEBUG: updating/concatenating cache for layer_idx 4\n",
            "DEBUG: updating/concatenating cache for layer_idx 5\n",
            "DEBUG: updating/concatenating cache for layer_idx 6\n",
            "DEBUG: updating/concatenating cache for layer_idx 7\n",
            "DEBUG: updating/concatenating cache for layer_idx 8\n",
            "DEBUG: updating/concatenating cache for layer_idx 9\n",
            "DEBUG: updating/concatenating cache for layer_idx 10\n",
            "DEBUG: updating/concatenating cache for layer_idx 11\n",
            "DEBUG: updating/concatenating cache for layer_idx 12\n",
            "DEBUG: updating/concatenating cache for layer_idx 13\n",
            "DEBUG: updating/concatenating cache for layer_idx 14\n",
            "DEBUG: updating/concatenating cache for layer_idx 15\n",
            "DEBUG: updating/concatenating cache for layer_idx 16\n",
            "DEBUG: updating/concatenating cache for layer_idx 17\n",
            "DEBUG: updating/concatenating cache for layer_idx 18\n",
            "DEBUG: updating/concatenating cache for layer_idx 19\n",
            "DEBUG: updating/concatenating cache for layer_idx 20\n",
            "DEBUG: updating/concatenating cache for layer_idx 21\n",
            "DEBUG: updating/concatenating cache for layer_idx 22\n",
            "DEBUG: updating/concatenating cache for layer_idx 23\n",
            "DEBUG: updating/concatenating cache for layer_idx 24\n",
            "DEBUG: updating/concatenating cache for layer_idx 25\n",
            "DEBUG: updating/concatenating cache for layer_idx 26\n",
            "DEBUG: updating/concatenating cache for layer_idx 27\n",
            "DEBUG: updating/concatenating cache for layer_idx 28\n",
            "DEBUG: updating/concatenating cache for layer_idx 29\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The quick brown\"\n",
        "input_ids, attention_mask = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\").values()\n",
        "outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, do_sample=False, return_dict_in_generate=True, max_new_tokens=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLThOq3IwgxU"
      },
      "source": [
        "As we can see by the printed output, for the first generated token `update` initializes cache with `self.key_cache.append(key_states)` and `self.value_cache.append(value_states)`. For the subsequent tokens, it updates the cache with `torch.cat`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyT_BOTcxGO8"
      },
      "source": [
        "I'll re-assign the original `update` to `DynamicCache` to avoid cluttering with print outs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vL8YY4GRxM6U"
      },
      "outputs": [],
      "source": [
        "DynamicCache.update = ORIGINAL_DYNAMIC_CACHE_UPDATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwdsJIEfThvG"
      },
      "source": [
        "## Inspecting `past_key_values` During `model.generate`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul-8HzGtxXHS"
      },
      "source": [
        "With an understanding of how KV cache is updated, I'll now turn my attention to the key and value cache contents during autoregressive generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPJMPGBF0Ar5"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XkxPQh1JTmAn",
        "outputId": "31e79d9e-72c0-4700-a6eb-7e6f795819b0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The quick brown'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"The quick brown\"\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4zqg9eCGKKs",
        "outputId": "9f2a2d84-afdd-465d-b95e-583cd7a8860a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 3]), torch.Size([1, 3]))"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids, attention_mask = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\").values()\n",
        "input_ids.shape, attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxCg55c6UJnF"
      },
      "source": [
        "By setting `return_dict_in_generate=True` we can retrieve `past_key_values`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELEYUlvqT6XY",
        "outputId": "8339ed0d-7ce1-45bd-8228-4e3a47f656af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateDecoderOnlyOutput(sequences=tensor([[  504,  2365,  6354, 16438, 27003,   690,   260, 23790]],\n",
              "       device='cuda:0'), scores=None, logits=None, attentions=None, hidden_states=None, past_key_values=<transformers.cache_utils.DynamicCache object at 0x78c4f80c4f50>)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, do_sample=False, return_dict_in_generate=True, max_new_tokens=5)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6CXNw8F2T9Ar",
        "outputId": "df742156-715e-4ca3-f32e-cacc3303d278"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The quick brown fox jumps over the lazy'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(outputs.sequences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkZVYqDP1u_a"
      },
      "source": [
        "We have 8 total tokens---3 from the original prompt and 5 new tokens generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dcJFZOiWGRs",
        "outputId": "4e26edc9-2958-47a7-eee0-7cc23ad3b7cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.sequences.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZBy8nBk10aD"
      },
      "source": [
        "Inspecting the values in the KV cache: there are 30 items in `key_cache` and `value_cache`, corresponding to the 30 layers in the model. For the last generated token (the 8th token) there were `7` `seen_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K3pEpfxVZh9",
        "outputId": "144271c6-c0d9-443f-b11c-537dd82f3a58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30, 30)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(outputs.past_key_values.key_cache), len(outputs.past_key_values.value_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb5Lkwn-UG1y",
        "outputId": "d2af89b4-8842-46d2-c6b5-78e8ad4010d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.past_key_values.seen_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZtz8BP52G2e"
      },
      "source": [
        "The `key_cache` tensors all have the same shape: batch size, num_heads, seen_tokens, head_dim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e7OFV1kWU41",
        "outputId": "df7616f9-08fe-4fa3-f527-615bf9f83a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n"
          ]
        }
      ],
      "source": [
        "for k in outputs.past_key_values.key_cache: print(k.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J7ty6xSWvU3",
        "outputId": "b16b73c8-950b-4443-dcea-4701436d9749"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9, 30, 3)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.num_attention_heads, \\\n",
        "model.config.num_hidden_layers, \\\n",
        "model.config.num_key_value_heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw8A8MKsW8HC",
        "outputId": "41b06ce8-7bfc-4bc6-8ade-8ce7084457ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "192"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.model.layers[0].self_attn.k_proj.out_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inzUBt-qYw55",
        "outputId": "316d31cf-cf8b-49e7-ebc7-1cb061de1468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "192"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "3*64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LlKx12LYgmH"
      },
      "source": [
        "The `value_cache` is similarly structured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8gF2qtlYVQd",
        "outputId": "f772dcaa-63fe-4315-e142-0cebb1c9e260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 7, 64])\n"
          ]
        }
      ],
      "source": [
        "for v in outputs.past_key_values.value_cache: print(v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAAHS3442WSn",
        "outputId": "c4db38cf-43f1-4172-ad94-706d347ecea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 7, 64])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "init_k = outputs.past_key_values.key_cache[0]\n",
        "init_k.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ZtW9i225-N"
      },
      "source": [
        "While the shapes of the `key_cache` tensors across layers are the same, their contents are not. This is because each layer has its own self attention module with its own `k_proj` and `v_proj` layers with their own learned weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "_Qr-aNkE2ZIX",
        "outputId": "09ec3082-2e2d-44ea-fa9a-1d7517d05b5f"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-f08f1c95a8ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for k in outputs.past_key_values.key_cache[1:]: assert torch.allclose(init_k, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLsja5xtZvZV"
      },
      "source": [
        "## Inspecting Intermediate Key/Value Cache Tensors During Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TnaCierY2xz"
      },
      "source": [
        "Now to understand how KV cache is used during generation: I want to inspect the shape of the key and value cache tensors as the prompt increases by one token at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fls85qM53pX4"
      },
      "source": [
        "To achieve this, I'll add a hook to the first layer's self attention module's forward pass using `register_forward_hook`. I came to an incorrect conclusion in a [previous video](https://youtu.be/4OBQkESiL0M?feature=shared&t=965) and [blog post](https://vishalbakshi.github.io/blog/posts/2025-04-02-Composer-Callback-Logging-dtypes/#composer-callback-walkthrough:~:text=Self%20attention%20cannot%20utilize%20register_forward_hook%20because%20the%20LlamaDecoderLayer%20does%20not%20call%20self%20attention%20forward%20pass%20with%20any%20positional%20arguments%3A) that you can't use `register_forward_hook` for the Llama attention module because it doesn't capture keyword arguments. What I didn't realize is that you can capture kwargs with `register_forward_hook` by setting `with_kwargs=True`, which I have done below.\n",
        "\n",
        "I wrapped `hook_fn` in `create_hook_fn` because I wanted to print out the `count` of total generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N2chU9_3ept",
        "outputId": "222ac421-f62f-444c-d90f-a65a283d8e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 torch.Size([1, 3, 3, 64])\n",
            "2 torch.Size([1, 3, 4, 64])\n",
            "3 torch.Size([1, 3, 5, 64])\n",
            "4 torch.Size([1, 3, 6, 64])\n",
            "5 torch.Size([1, 3, 7, 64])\n"
          ]
        }
      ],
      "source": [
        "def create_hook_fn():\n",
        "    count = 1\n",
        "    def hook_fn(module, args, kwargs, output):\n",
        "        nonlocal count\n",
        "        print(count, kwargs['past_key_value'].key_cache[0].shape)\n",
        "        count += 1\n",
        "    return hook_fn\n",
        "\n",
        "_hook_fn = create_hook_fn()\n",
        "attn_layer = model.model.layers[0].self_attn\n",
        "hook_handle = attn_layer.register_forward_hook(_hook_fn, with_kwargs=True)\n",
        "\n",
        "outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, do_sample=False, return_dict_in_generate=True, max_new_tokens=5)\n",
        "hook_handle.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTM05vma5goG"
      },
      "source": [
        "Let's parse this output:\n",
        "\n",
        "- The first new token generated sees only the 3 tokens in the prompt. The KV cache subsequently has a third dimension of `3`.\n",
        "- Each new token generated sees one more new token, so the third dimension (seen tokens) of `key_cache` and `value_cache` increases by `1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8peBT6m6Duo"
      },
      "source": [
        "I'll slightly modify `hook_fn` so it prints out the first few shapes of `key_cache`, allowing us to see what all layers' cache is storing from the perspective of `layer_idx=0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSpwBWL46OPa",
        "outputId": "779782e8-dc50-4d92-fc8a-0712826d3cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "torch.Size([1, 3, 3, 64])\n",
            "2\n",
            "torch.Size([1, 3, 4, 64])\n",
            "torch.Size([1, 3, 3, 64])\n",
            "torch.Size([1, 3, 3, 64])\n",
            "torch.Size([1, 3, 3, 64])\n",
            "torch.Size([1, 3, 3, 64])\n",
            "3\n",
            "torch.Size([1, 3, 5, 64])\n",
            "torch.Size([1, 3, 4, 64])\n",
            "torch.Size([1, 3, 4, 64])\n",
            "torch.Size([1, 3, 4, 64])\n",
            "torch.Size([1, 3, 4, 64])\n",
            "4\n",
            "torch.Size([1, 3, 6, 64])\n",
            "torch.Size([1, 3, 5, 64])\n",
            "torch.Size([1, 3, 5, 64])\n",
            "torch.Size([1, 3, 5, 64])\n",
            "torch.Size([1, 3, 5, 64])\n",
            "5\n",
            "torch.Size([1, 3, 7, 64])\n",
            "torch.Size([1, 3, 6, 64])\n",
            "torch.Size([1, 3, 6, 64])\n",
            "torch.Size([1, 3, 6, 64])\n",
            "torch.Size([1, 3, 6, 64])\n"
          ]
        }
      ],
      "source": [
        "def create_hook_fn():\n",
        "    count = 1\n",
        "    def hook_fn(module, args, kwargs, output):\n",
        "        nonlocal count\n",
        "        print(count)\n",
        "        for k in kwargs['past_key_value'].key_cache[:5]: print(k.shape)\n",
        "        count += 1\n",
        "    return hook_fn\n",
        "\n",
        "_hook_fn = create_hook_fn()\n",
        "attn_layer = model.model.layers[0].self_attn\n",
        "hook_handle = attn_layer.register_forward_hook(_hook_fn, with_kwargs=True)\n",
        "\n",
        "outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, do_sample=False, return_dict_in_generate=True, max_new_tokens=5)\n",
        "hook_handle.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccrBwgTc6cnY"
      },
      "source": [
        "Since we are capturing the `key_cache` shapes from the first layer (`layer_idx=0`), the other subsequent layer's cache tensors are 1 token \"behind\", since the new token's hidden states have not passed through the model yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbOZjVF16rpp"
      },
      "source": [
        "Ultimately, I want to tie this all back to the `flash_attn_varlen_func`'s dostring's causal mask example, so I'll take a look at the `query_states` shape, copying code from the [`LlamaAttention` forward pass](https://github.com/huggingface/transformers/blob/78d771c3c21922642fc9546ccb973cc7a182ab34/src/transformers/models/llama/modeling_llama.py#L232-L235). I'll also inspect the length of the `key_cache` and its shape, and the shape of `value_cache`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjXAy4rb3i_D",
        "outputId": "b70a2dd1-b26f-46f9-cf7e-311cccd3d82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 len(past_key_value): 1, query_states.shape: torch.Size([1, 9, 3, 64]), k.shape: torch.Size([1, 3, 3, 64]), v.shape: torch.Size([1, 3, 3, 64])\n",
            "2 len(past_key_value): 30, query_states.shape: torch.Size([1, 9, 1, 64]), k.shape: torch.Size([1, 3, 4, 64]), v.shape: torch.Size([1, 3, 4, 64])\n",
            "3 len(past_key_value): 30, query_states.shape: torch.Size([1, 9, 1, 64]), k.shape: torch.Size([1, 3, 5, 64]), v.shape: torch.Size([1, 3, 5, 64])\n",
            "4 len(past_key_value): 30, query_states.shape: torch.Size([1, 9, 1, 64]), k.shape: torch.Size([1, 3, 6, 64]), v.shape: torch.Size([1, 3, 6, 64])\n",
            "5 len(past_key_value): 30, query_states.shape: torch.Size([1, 9, 1, 64]), k.shape: torch.Size([1, 3, 7, 64]), v.shape: torch.Size([1, 3, 7, 64])\n"
          ]
        }
      ],
      "source": [
        "def create_hook_fn():\n",
        "    count = 1\n",
        "    def hook_fn(module, args, kwargs, output):\n",
        "        nonlocal count\n",
        "        input_shape = kwargs['hidden_states'].shape[:-1]\n",
        "        hidden_shape = (*input_shape, -1, module.head_dim)\n",
        "        query_states = module.q_proj(kwargs['hidden_states']).view(hidden_shape).transpose(1, 2)\n",
        "        print(count, f\"len(past_key_value): {len(kwargs['past_key_value'].key_cache)},\", f\"query_states.shape: {query_states.shape},\", f\"k.shape: {kwargs['past_key_value'].key_cache[0].shape},\", f\"v.shape: {kwargs['past_key_value'].value_cache[0].shape}\")\n",
        "        count += 1\n",
        "    return hook_fn\n",
        "\n",
        "_hook_fn = create_hook_fn()\n",
        "attn_layer = model.model.layers[0].self_attn\n",
        "hook_handle = attn_layer.register_forward_hook(_hook_fn, with_kwargs=True)\n",
        "\n",
        "outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, do_sample=False, return_dict_in_generate=True, max_new_tokens=5)\n",
        "hook_handle.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ooVbfQt7bR3"
      },
      "source": [
        "We see that there are 9 query heads, and 3 KV heads. The total hidden dimension for Q, K and V layers is 3 x 64 = 192.\n",
        "\n",
        "When the first token is being the generated, the length of the `key_cache` for `layer_idx=0` is `1`, because this is the first attention module's first forward pass. For subsequent tokens (2, 3, 4, 5) the length of the `key_cache` is `30`, as the cache has been instantiated for all 30 layers after the first token is generated.\n",
        "\n",
        "Finally, we see that the `key_cache` and `value_cache` shapes are equal, as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlqrO3uk8LGS"
      },
      "source": [
        "## Which Flash Attention Interface is Used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxRBdQ7G8NdZ"
      },
      "source": [
        "Since this exercise is part of my journey to understand the `flash_attn_varlen_func`, I was curious to confirm by visual inspection which Flash Attention interface function was being used. To achieve this, I wrote a \"debug\" version for the following three functions:\n",
        "\n",
        "- [`LlamaAttention.forward`](https://github.com/huggingface/transformers/blob/78d771c3c21922642fc9546ccb973cc7a182ab34/src/transformers/models/llama/modeling_llama.py#L223)\n",
        "- [`flash_attention_forward`](https://github.com/huggingface/transformers/blob/78d771c3c21922642fc9546ccb973cc7a182ab34/src/transformers/integrations/flash_attention.py#L14)\n",
        "- [`_flash_attention_forward`](https://github.com/huggingface/transformers/blob/78d771c3c21922642fc9546ccb973cc7a182ab34/src/transformers/modeling_flash_attention_utils.py#L284)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRFP25lMFG8A"
      },
      "source": [
        "How did I know which functions to modify? Well, largely because I [have done this exercise before](https://vishalbakshi.github.io/blog/posts/2025-05-04-Understanding-Sequence-Packing/) when I was trying to understand what triggered the use of `flash_attn_varlen_func`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1DnQAdxF9Lh"
      },
      "source": [
        "More concisely, I first inspected the forward pass of the attention module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTe4OroZGDiy",
        "outputId": "fc396455-d320-43da-9491-a9555f6118a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def forward(\n",
            "        self,\n",
            "        hidden_states: torch.Tensor,\n",
            "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
            "        attention_mask: Optional[torch.Tensor],\n",
            "        past_key_value: Optional[Cache] = None,\n",
            "        cache_position: Optional[torch.LongTensor] = None,\n",
            "        **kwargs: Unpack[FlashAttentionKwargs],\n",
            "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
            "        input_shape = hidden_states.shape[:-1]\n",
            "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
            "\n",
            "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "\n",
            "        cos, sin = position_embeddings\n",
            "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
            "\n",
            "        if past_key_value is not None:\n",
            "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
            "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
            "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
            "\n",
            "        attention_interface: Callable = eager_attention_forward\n",
            "\n",
            "        if self.config._attn_implementation != \"eager\":\n",
            "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
            "                logger.warning_once(\n",
            "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
            "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
            "                )\n",
            "            else:\n",
            "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
            "\n",
            "        attn_output, attn_weights = attention_interface(\n",
            "            self,\n",
            "            query_states,\n",
            "            key_states,\n",
            "            value_states,\n",
            "            attention_mask,\n",
            "            dropout=0.0 if not self.training else self.attention_dropout,\n",
            "            scaling=self.scaling,\n",
            "            **kwargs,\n",
            "        )\n",
            "\n",
            "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
            "        attn_output = self.o_proj(attn_output)\n",
            "        return attn_output, attn_weights\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from inspect import getsource\n",
        "print(getsource(model.model.layers[0].self_attn.forward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IneMn0nWGLB6"
      },
      "source": [
        "In there I saw the following lines of interest:\n",
        "\n",
        "```python\n",
        "if self.config._attn_implementation != \"eager\":\n",
        "    if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
        "        logger.warning_once(\n",
        "            \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
        "            'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
        "        )\n",
        "    else:\n",
        "        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnaiLQ3QGSIR"
      },
      "source": [
        "In our case, the `else` block would trigger and `ALL_ATTENTION_FUNCTIONS` would be accesssed. Looking at that constant directly we can see that for our model's `_attn_implementation` (`'flash_attention_2'`) the attention interface funtion is `flash_attention_forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wsZI2__AG3cu",
        "outputId": "8d40574b-2e73-4ad6-a6b0-7490e87e4108"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'flash_attention_2'"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config._attn_implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "K-17MEJBGqRt",
        "outputId": "fc2cd815-84be-44c3-8255-4b6a2f33e5c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.integrations.flash_attention.flash_attention_forward</b><br/>def flash_attention_forward(module: torch.nn.Module, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attention_mask: Optional[torch.Tensor], dropout: float=0.0, scaling: Optional[float]=None, sliding_window: Optional[int]=None, softcap: Optional[float]=None, **kwargs) -&gt; Tuple[torch.Tensor, None]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/transformers/integrations/flash_attention.py</a>&lt;no docstring&gt;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 11);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "<function transformers.integrations.flash_attention.flash_attention_forward(module: torch.nn.modules.module.Module, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attention_mask: Optional[torch.Tensor], dropout: float = 0.0, scaling: Optional[float] = None, sliding_window: Optional[int] = None, softcap: Optional[float] = None, **kwargs) -> Tuple[torch.Tensor, NoneType]>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
        "ALL_ATTENTION_FUNCTIONS[model.config._attn_implementation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "msBeFXBfS-zs"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show `_debug_flash_attention_forward\"\n",
        "from typing import Optional, Tuple\n",
        "import inspect\n",
        "from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
        "import torch\n",
        "import os\n",
        "from transformers.modeling_flash_attention_utils import is_flash_attn_greater_or_equal, fa_peft_integration_check, _upad_input, pad_input, prepare_fa2_from_position_ids\n",
        "\n",
        "_flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\n",
        "flash_241 = is_flash_attn_greater_or_equal(\"2.4.1\")\n",
        "deterministic_g = None\n",
        "\n",
        "def _debug_flash_attention_forward(\n",
        "    query_states: torch.Tensor,\n",
        "    key_states: torch.Tensor,\n",
        "    value_states: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    query_length: int,\n",
        "    is_causal: bool,\n",
        "    dropout: float = 0.0,\n",
        "    position_ids: Optional[torch.Tensor] = None,\n",
        "    softmax_scale: Optional[float] = None,\n",
        "    sliding_window: Optional[int] = None,\n",
        "    use_top_left_mask: bool = False,\n",
        "    softcap: Optional[float] = None,\n",
        "    deterministic: Optional[bool] = None,\n",
        "    cu_seq_lens_q: Optional[torch.LongTensor] = None,\n",
        "    cu_seq_lens_k: Optional[torch.LongTensor] = None,\n",
        "    max_length_q: Optional[int] = None,\n",
        "    max_length_k: Optional[int] = None,\n",
        "    target_dtype: Optional[torch.dtype] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "\n",
        "    if not use_top_left_mask:\n",
        "        causal = is_causal\n",
        "    else:\n",
        "        # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1.\n",
        "        causal = is_causal and query_length != 1\n",
        "\n",
        "    # Assuming 4D tensors, key_states.shape[1] is the key/value sequence length (source length).\n",
        "    use_sliding_windows = (\n",
        "        _flash_supports_window_size and sliding_window is not None and key_states.shape[1] > sliding_window\n",
        "    )\n",
        "    flash_kwargs = {\"window_size\": (sliding_window, sliding_window)} if use_sliding_windows else {}\n",
        "\n",
        "    if flash_241:\n",
        "        if deterministic is None:\n",
        "            global deterministic_g\n",
        "            if deterministic_g is None:\n",
        "                deterministic_g = os.environ.get(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n",
        "            deterministic = deterministic_g\n",
        "        flash_kwargs[\"deterministic\"] = deterministic\n",
        "\n",
        "    if softcap is not None:\n",
        "        flash_kwargs[\"softcap\"] = softcap\n",
        "\n",
        "    # PEFT possibly silently casts tensors to fp32, this potentially reconverts to correct dtype or is a no op\n",
        "    query_states, key_states, value_states = fa_peft_integration_check(\n",
        "        query_states, key_states, value_states, target_dtype\n",
        "    )\n",
        "\n",
        "    # Contains at least one padding token in the sequence\n",
        "    if attention_mask is not None:\n",
        "        batch_size = query_states.shape[0]\n",
        "        query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = _upad_input(\n",
        "            query_states, key_states, value_states, attention_mask, query_length\n",
        "        )\n",
        "        cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n",
        "        max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n",
        "        print(\"if attention_mask is not None: flash_attn_varlen_func is being used\")\n",
        "        attn_output_unpad = flash_attn_varlen_func(\n",
        "            query_states,\n",
        "            key_states,\n",
        "            value_states,\n",
        "            cu_seqlens_q=cu_seqlens_q,\n",
        "            cu_seqlens_k=cu_seqlens_k,\n",
        "            max_seqlen_q=max_seqlen_in_batch_q,\n",
        "            max_seqlen_k=max_seqlen_in_batch_k,\n",
        "            dropout_p=dropout,\n",
        "            softmax_scale=softmax_scale,\n",
        "            causal=causal,\n",
        "            **flash_kwargs,\n",
        "        )\n",
        "        attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n",
        "\n",
        "    # If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n",
        "    # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n",
        "    # Use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n",
        "    elif position_ids is not None and (\n",
        "        max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\n",
        "    ):\n",
        "        batch_size = query_states.size(0)\n",
        "\n",
        "        if cu_seq_lens_q is None or cu_seq_lens_k is None:\n",
        "            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = (\n",
        "                prepare_fa2_from_position_ids(query_states, key_states, value_states, position_ids)\n",
        "            )\n",
        "\n",
        "            cu_seq_lens_q, cu_seq_lens_k = cu_seq_lens\n",
        "            max_length_q, max_length_k = max_seq_lens\n",
        "\n",
        "        else:\n",
        "            query_states = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))\n",
        "            key_states = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n",
        "            value_states = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n",
        "\n",
        "        print(\"position_ids is not None: flash_attn_varlen_func is being used\")\n",
        "        attn_output = flash_attn_varlen_func(\n",
        "            query_states,\n",
        "            key_states,\n",
        "            value_states,\n",
        "            cu_seqlens_q=cu_seq_lens_q,\n",
        "            cu_seqlens_k=cu_seq_lens_k,\n",
        "            max_seqlen_q=max_length_q,\n",
        "            max_seqlen_k=max_length_k,\n",
        "            dropout_p=dropout,\n",
        "            softmax_scale=softmax_scale,\n",
        "            causal=causal,\n",
        "            **flash_kwargs,\n",
        "        )\n",
        "\n",
        "        attn_output = attn_output.view(batch_size, -1, attn_output.size(-2), attn_output.size(-1))\n",
        "\n",
        "    else:\n",
        "        print(\"flash_attn_func is being used\")\n",
        "        attn_output = flash_attn_func(\n",
        "            query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal, **flash_kwargs\n",
        "        )\n",
        "\n",
        "    return attn_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ucWZREvmRyss"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show `debug_flash_attention_forward\"\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "from transformers.modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n",
        "\n",
        "_use_top_left_mask = flash_attn_supports_top_left_mask()\n",
        "\n",
        "\n",
        "def debug_flash_attention_forward(\n",
        "    module: torch.nn.Module,\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    dropout: float = 0.0,\n",
        "    scaling: Optional[float] = None,\n",
        "    sliding_window: Optional[int] = None,\n",
        "    softcap: Optional[float] = None,\n",
        "    **kwargs,\n",
        ") -> Tuple[torch.Tensor, None]:\n",
        "    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n",
        "        print(\n",
        "            \"`flash_attention_2` does not support `output_attentions=True` or `head_mask`.\"\n",
        "            \" Please set your attention to `eager` if you want any of these features.\"\n",
        "        )\n",
        "\n",
        "    # This is before the transpose\n",
        "    seq_len = query.shape[2]\n",
        "\n",
        "    # FA2 uses non-transposed inputs\n",
        "    query = query.transpose(1, 2)\n",
        "    key = key.transpose(1, 2)\n",
        "    value = value.transpose(1, 2)\n",
        "\n",
        "    # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n",
        "    # therefore the input hidden states gets silently casted in float32. Hence, we need\n",
        "    # cast them back in the correct dtype just to be sure everything works as expected.\n",
        "    # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n",
        "    # in fp32. (usually our RMSNorm modules handle it correctly)\n",
        "    target_dtype = None\n",
        "    if query.dtype == torch.float32:\n",
        "        if torch.is_autocast_enabled():\n",
        "            target_dtype = torch.get_autocast_gpu_dtype()\n",
        "        # Handle the case where the model is quantized\n",
        "        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n",
        "            target_dtype = module.config._pre_quantization_dtype\n",
        "        else:\n",
        "            target_dtype = next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n",
        "\n",
        "    # FA2 always relies on the value set in the module, so remove it if present in kwargs to avoid passing it twice\n",
        "    kwargs.pop(\"is_causal\", None)\n",
        "\n",
        "    print(\"DEBUG: calling _flash_attention_forward\")\n",
        "    attn_output = _debug_flash_attention_forward(\n",
        "        query,\n",
        "        key,\n",
        "        value,\n",
        "        attention_mask,\n",
        "        query_length=seq_len,\n",
        "        is_causal=module.is_causal,\n",
        "        dropout=dropout,\n",
        "        softmax_scale=scaling,\n",
        "        sliding_window=sliding_window,\n",
        "        softcap=softcap,\n",
        "        use_top_left_mask=_use_top_left_mask,\n",
        "        target_dtype=target_dtype,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    return attn_output, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "X0k3NJJ-J6q3"
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show `debug_forward\"\n",
        "from typing import Callable, Optional, Tuple, Union\n",
        "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
        "from transformers.cache_utils import Cache, DynamicCache\n",
        "from transformers.processing_utils import Unpack\n",
        "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
        "from transformers.models.llama.modeling_llama import eager_attention_forward\n",
        "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
        "\n",
        "def debug_forward(\n",
        "    self,\n",
        "    hidden_states: torch.Tensor,\n",
        "    position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    past_key_value: Optional[Cache] = None,\n",
        "    cache_position: Optional[torch.LongTensor] = None,\n",
        "    **kwargs: Unpack[FlashAttentionKwargs],\n",
        ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "    input_shape = hidden_states.shape[:-1]\n",
        "    hidden_shape = (*input_shape, -1, self.head_dim)\n",
        "\n",
        "    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "\n",
        "    cos, sin = position_embeddings\n",
        "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "    if past_key_value is not None:\n",
        "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
        "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
        "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "    attention_interface: Callable = eager_attention_forward\n",
        "\n",
        "    if self.config._attn_implementation != \"eager\":\n",
        "        if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
        "            print(\n",
        "                \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
        "                'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
        "            )\n",
        "        else:\n",
        "            #attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
        "            attention_interface = debug_flash_attention_forward\n",
        "\n",
        "    attn_output, attn_weights = attention_interface(\n",
        "        self,\n",
        "        query_states,\n",
        "        key_states,\n",
        "        value_states,\n",
        "        attention_mask,\n",
        "        dropout=0.0 if not self.training else self.attention_dropout,\n",
        "        scaling=self.scaling,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
        "    attn_output = self.o_proj(attn_output)\n",
        "    return attn_output, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyhone4OtWUn"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz2Eoxr9Isjl"
      },
      "source": [
        "`types.MethodType` binds a function (`debug_forward`) as a method for a class (`attn_layer_instance`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "ye26Y5I2HMJA"
      },
      "outputs": [],
      "source": [
        "import types\n",
        "types.MethodType??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ARgZvKHgrQ"
      },
      "source": [
        "```\n",
        "Init signature: types.MethodType(self, /, *args, **kwargs)\n",
        "Docstring:      Create a bound instance method object.\n",
        "Type:           type\n",
        "Subclasses:    \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "r_GLdqegMAQN"
      },
      "outputs": [],
      "source": [
        "attn_layer_instance = model.model.layers[0].self_attn\n",
        "\n",
        "original_layer_forward = attn_layer_instance.forward\n",
        "\n",
        "attn_layer_instance.forward = types.MethodType(debug_forward, attn_layer_instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiLe1jMPbZkq",
        "outputId": "f9c3e320-4cc7-4467-bd12-727167ceb4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: calling _flash_attention_forward\n",
            "flash_attn_func is being used\n",
            "1 1 query_states.shape: torch.Size([1, 9, 3, 64]) k.shape: torch.Size([1, 3, 3, 64])\n",
            "DEBUG: calling _flash_attention_forward\n",
            "flash_attn_func is being used\n",
            "2 30 query_states.shape: torch.Size([1, 9, 1, 64]) k.shape: torch.Size([1, 3, 4, 64])\n",
            "DEBUG: calling _flash_attention_forward\n",
            "flash_attn_func is being used\n",
            "3 30 query_states.shape: torch.Size([1, 9, 1, 64]) k.shape: torch.Size([1, 3, 5, 64])\n",
            "DEBUG: calling _flash_attention_forward\n",
            "flash_attn_func is being used\n",
            "4 30 query_states.shape: torch.Size([1, 9, 1, 64]) k.shape: torch.Size([1, 3, 6, 64])\n",
            "DEBUG: calling _flash_attention_forward\n",
            "flash_attn_func is being used\n",
            "5 30 query_states.shape: torch.Size([1, 9, 1, 64]) k.shape: torch.Size([1, 3, 7, 64])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "def create_hook_fn():\n",
        "    count = 1\n",
        "    def hook_fn(module, args, kwargs, output):\n",
        "        nonlocal count\n",
        "        input_shape = kwargs['hidden_states'].shape[:-1]\n",
        "        hidden_shape = (*input_shape, -1, module.head_dim)\n",
        "        query_states = module.q_proj(kwargs['hidden_states']).view(hidden_shape).transpose(1, 2)\n",
        "        print(count, len(kwargs['past_key_value'].key_cache), f\"query_states.shape: {query_states.shape}\", f\"k.shape: {kwargs['past_key_value'].key_cache[0].shape}\")\n",
        "        # for k in kwargs['past_key_value'].key_cache: print(k.shape) # do this for v as well\n",
        "        count += 1\n",
        "    return hook_fn\n",
        "\n",
        "_hook_fn = create_hook_fn()\n",
        "attn_layer = model.model.layers[0].self_attn\n",
        "hook_handle = attn_layer.register_forward_hook(_hook_fn, with_kwargs=True)\n",
        "\n",
        "outputs = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, do_sample=False, return_dict_in_generate=True, max_new_tokens=5)\n",
        "print(outputs.sequences[0].shape)\n",
        "\n",
        "hook_handle.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn2YuAEYH6L4"
      },
      "source": [
        "From the print statements in my `_debug_flash_attention_forward` function, I can see that `flash_attn_func`, the non-variable-length interface, is being used for this generation. That makes sense because I only have 1 item in the batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAOuK8cbINmC"
      },
      "source": [
        "## Understanding the `flash_attn_varlen_func` Causal Mask Docstring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyngqlSeIRsE"
      },
      "source": [
        "A quick recap of what we've learned so far:\n",
        "\n",
        "- HuggingFace's `model.generate` uses KV cache by default (`DynamicCache`) stored as `past_key_values`.\n",
        "- For most scenarios, the `DynamicCache` is updated by concatenating the previous token's `key_cache` and `value_cache` with the `key_states` and `value_states` generated for the current new token.\n",
        "- As the next token is generated for a given prompt, `query_states` has a sequence length of `1`, whereas `key_cache` and `value_cache` tensors' sequence dimension increases by 1. This is directly relates to the `flash_attn_varlen_func` causal mask docstring example.\n",
        "- `model.generate` utilized the `flash_attn_func` interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As4hh9PRKV1K"
      },
      "source": [
        "Let's look at the `flash_attn_varlen_func` docstring snippet again:\n",
        "\n",
        "```\n",
        "If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n",
        "For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n",
        "    1 1 1 1 0\n",
        "    1 1 1 1 1\n",
        "If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n",
        "    0 0\n",
        "    0 0\n",
        "    0 0\n",
        "    1 0\n",
        "    1 1\n",
        "If the row of the mask is all zero, the output will be zero.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fANPZCGYK4ag"
      },
      "source": [
        "I'll annotate the causal mask examples a bit:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMncwNpLWHt"
      },
      "source": [
        "### `seqlen_q=2` and `seqlen_k=5`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN5JMPnLLbI9"
      },
      "source": [
        "||k_0|k_1|k_2|k_3|k_4|\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "|**q_0**|1|1|1|1|0\n",
        "|**q_1**|1|1|1|1|1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYu7TvWlPUba"
      },
      "source": [
        "The final query token (`q_1`) sees all 5 key tokens. The first query token (`q_0`) only sees the first four key tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTKrVhWpLRf6"
      },
      "source": [
        "### `seqlen_q=5` and `seqlen_k=2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGLfoqJhLVCi"
      },
      "source": [
        "||k_0|k_1\n",
        "|:-:|:-:|:-:|\n",
        "|**q_0**|0|0\n",
        "|**q_1**|0|0\n",
        "|**q_2**|0|0\n",
        "|**q_3**|1|0\n",
        "|**q_4**|1|1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H15Dl90PCo0"
      },
      "source": [
        "Again, the final query token (`q_4`) sees all key tokens. As a consequence, since there are only two key tokens, the first three query tokens do not see any key tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNEK722Zp1QH"
      },
      "source": [
        "In each example, we are offsetting the shorter sequence so that its last token aligns with the other sequences's last token. This is what the `flash_attn_varlen_func` docstring means by\n",
        "\n",
        "> `the causal mask is aligned to the bottom right corner of the attention matrix`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1x9ocgFmVbB"
      },
      "source": [
        "![Annotated casual masks](1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vml6aJyFdzEf"
      },
      "source": [
        "In the first case, the query sequence is shorter so we offset it by 3 positions to align with the last two tokens of the key sequence. The \"offset\" positions are 1s (this satisfies the rule of causality `j <= i`, query tokens can look back). In the second case, the key sequence is shorter so we offset it by 3 positions to align with the last two tokens of the query sequence. The offset positions are 0s (again, this satisfies causality, the query tokens have nothing to look back to)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GED1vq9xqO7T"
      },
      "source": [
        "The `model.generate` examples above are like the first case, where there are more key positions than query positions. The query token (the next-token being predicted) can look back at all key tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHqK894Qb7E6"
      },
      "source": [
        "### A Math-y Way to think About It"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sigB8u0hb-e7"
      },
      "source": [
        "For those of you who like to think through things with math."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0jD4B85VV7I"
      },
      "source": [
        "Causality (in language modeling) means that a query token vector at the i-th position can only see its own and previous tokens' key vectors. Having different sequence lengths for Q and K (5 and 2 or 2 and 5 in the `flash_attn_varlen_func` docstring example or 1 and 3-7 in my inspections above)  requires you to pick _how_ Q and K are aligned. In the case of `flash_attn_varlen_func` they choose to align Q and K such as _the last Q token vector is aligned with the last K token vector_. This becomes our \"present moment\" `P` with causality allowing access to previous tokens only.\n",
        "\n",
        "Let's define `i` as the position of query tokens and `j` as the position of key tokens. Causality is defined as token pairs that follow the inequality: `j <= i + (seqlen_k - seqlen_q)`.\n",
        "\n",
        "For the first causal mask example:\n",
        "\n",
        "|`j`|`i`|`j <= i + (seqlen_k - seqlen_q)`|\n",
        "|:-:|:-:|:-:|\n",
        "|0|0|0 <= 0 + 3 (`True`)|\n",
        "|1|0|1 <= 0 + 3 (`True`)\n",
        "|2|0|2 <= 0 + 3 (`True`)\n",
        "|3|0|3 <= 0 + 3 (`True`)\n",
        "|4|0|4 <= 0 + 3 (`False`)\n",
        "|0|1|0 <= 1 + 3 (`True`)|\n",
        "|1|1|1 <= 1 + 3 (`True`)|\n",
        "|2|1|2 <= 1 + 3 (`True`)|\n",
        "|3|1|3 <= 1 + 3 (`True`)|\n",
        "|4|1|4 <= 1 + 3 (`True`)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ToiEipNZOcp"
      },
      "source": [
        "Where does `j <= i + (seqlen_k - seqlen_q)` come from?\n",
        "\n",
        "Let `q_i` be a query that is `seqlen_q - 1 - i` steps before the end of the query sequence, and `k_j` be a key that is `seqlen_k - 1 - j` steps before the end of the key sequence. More concretely, for the example where `seqlen_q = 2` and `seqlen_k=5`:\n",
        "\n",
        "|q_i|Steps before end|`seqlen_q - 1 - i`\n",
        "|:-:|:-:|:-:|\n",
        "|q_0|1|2 - 1 - 0\n",
        "|q_1|0|2 - 1 - 1\n",
        "\n",
        "<br>\n",
        "\n",
        "|k_j|Steps before end|`seqlen_k - 1 - j`\n",
        "|:-:|:-:|:-:|\n",
        "|k_0|4|5 - 1 - 0\n",
        "|k_1|3|5 - 1 - 1\n",
        "|k_2|2|5 - 2 - 1\n",
        "|k_3|1|5 - 3 - 1\n",
        "|k_4|0|5 - 4 - 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdwUFtOoZqlB"
      },
      "source": [
        "By picking a \"present moment\" `P` (the last token in each sequence) have a unified timeline `p` such that causality is defined as: `p_j <= p_i`. `k_j` has a position on the timeline `p_j = P - (seqlen_k - 1 - j)` and `q_i` has a position on the timeline `p_i = P - (seqlen_q - 1 - i)`. Causality requires that `p_j <= p_i` on our \"unified timeline\". Writing that out:\n",
        "\n",
        "`P - (seqlen_k - 1 - j) <= P - (seqlen_q - 1 - i)`\n",
        "\n",
        "Cancelling out the `P`s and distributing the minus sign:\n",
        "\n",
        "`-seqlen_k + 1 + j <= -seqlen_q + 1 + i`\n",
        "\n",
        "Isolating `j` on the lefthand side:\n",
        "\n",
        "`j <= -seqlen_q + 1 + i + seqlen_k - 1`\n",
        "\n",
        "Simplifying + reordering:\n",
        "\n",
        "`j <= i + (seqlen_k - seqlen_q)`\n",
        "\n",
        "We can think of this `(seqlen_k - seqlen_q)` to be an \"offset\" term between the two sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkO8NXc7nCpq"
      },
      "source": [
        "Looking at this concretely for the second causal mask:\n",
        "\n",
        "||k_0|k_1\n",
        "|:-:|:-:|:-:|\n",
        "|**q_0**|0|0\n",
        "|**q_1**|0|0\n",
        "|**q_2**|0|0\n",
        "|**q_3**|1|0\n",
        "|**q_4**|1|1\n",
        "\n",
        "<br>\n",
        "\n",
        "|`j`|`i`|`j <= i + (seqlen_k - seqlen_q)`|\n",
        "|:-:|:-:|:-:|\n",
        "|0|0|0 <= 0 - 3 (`False`)\n",
        "|0|1|0 <= 1 - 3 (`False`)\n",
        "|0|2|0 <= 2 - 3 (`False`)\n",
        "|0|3|0 <= 3 - 3 (`True`)\n",
        "|0|4|0 <= 4 - 3 (`True`)\n",
        "|1|0|1 <= 0 - 3 (`False`)\n",
        "|1|1|1 <= 1 - 3 (`False`)\n",
        "|1|2|1 <= 2 - 3 (`False`)\n",
        "|1|3|1 <= 3 - 3 (`False`)\n",
        "|1|4|1 <= 4 - 3 (`True`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8460XB7WqbNY"
      },
      "source": [
        "## Closing Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz0Q4UWQqctX"
      },
      "source": [
        "Understanding `flash_attn_varlen_func` is going to require a sequence (pun intended) of such deep dives. It took me hours to just get through the docstring!! I'm also working on understanding ModernBERT's sequence packing implementation (to the point of explaining it with visuals) and I expect it to interweave with my Flash Attention study, especially when understanding how ModernBERT prepares and packs sequences and related artifacts in preparation of passing it through the attention mechanism, utilizing `flash_attn_varlen_func`. It's an exciting one-two punch for sure! I'm glad I'm working on them together.\n",
        "\n",
        "I'll end with listing out again what I've learned in this notebook/exercise, with a couple points added about the causal mask:\n",
        "\n",
        "- HuggingFace's `model.generate` uses KV cache by default (`DynamicCache`) stored as `past_key_values`.\n",
        "- For most scenarios, the `DynamicCache` is updated by concatenating the previous token's `key_cache` and `value_cache` with the `key_states` and `value_states` generated for the current new token.\n",
        "- As the next token is generated for a given prompt, `query_states` has a sequence length of `1`, whereas `key_cache` and `value_cache` tensors' sequence dimension increases by 1. This is directly relates to the `flash_attn_varlen_func` causal mask docstring example.\n",
        "- `model.generate` utilized the `flash_attn_func` interface.\n",
        "- The causal mask is aligned to the bottom-right of the attention matrix (the last tokens of the Q and K sequence are aligned).\n",
        "- Causality, when $Q_i$ and $K_j$ sequences are of different length, is satisfied by the equation `j <= i + (seqlen_k - seqlen_q)`.\n",
        "- When there are more query tokens than key tokens, the \"offset\" (needed to align the last token of each sequence) results in 0s in the mask as there are no key tokens to \"look back at\".\n",
        "- When there are more key tokens than query tokens, the \"offset\" results in 1s as the query tokens can look back at more key tokens.\n",
        "\n",
        "I'm trying to grow my YouTube channel this year so if you enjoyed this blog post, [please subscribe!](https://www.youtube.com/@vishal_learner)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
